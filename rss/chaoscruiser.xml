<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>强化学习理解的三重境界</title>
<link>http://www.jintiankansha.me/t/A5jNLvecGM</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/A5jNLvecGM</guid>
<description>&lt;p&gt;&lt;span&gt;想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。&lt;/span&gt;&lt;span&gt;面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定你的行为。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;强化学习有哪些实打实的应用呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;只要在问题里包含了动态的决策与控制，&lt;/span&gt; &lt;span&gt;都可以用到强化学习&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1， 制造业&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;强化学习之于制药业有一种天然的契合 ， 把强化学习翻个牌子换个叫法， 也可以叫做控制论， 学习控制机器手的精确动作， 比如让它自动的做比目前所能及的更复杂的事情， 强化学习在制造业的应用潜力是显然的 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6617283950617284&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8YBT4xfFfeIjiatN1CyyZZVqicS0kicw2faTLPZtvkZrkYV4mZ988CiaA1w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;405&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;2， &lt;/span&gt;&lt;strong&gt;&lt;span&gt;无人驾驶&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这就不用多说了， 开车本质是个控制问题, 　自动驾驶不仅需要模拟人类行为，　还需要对前所未遇的情况进行决策，　这需要强化学习。　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5612009237875288&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8BvCqcZ8G0tw81FlSBbLpUvJQkpvZ1WYK40jX7E00y9OknVic4PFNbYg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;433&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;3, &lt;/span&gt;&lt;strong&gt;&lt;span&gt;智能交通&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;智能交通， 显然这里包含了非常多的决策与控制问题， 就拿目前的共享汽车行业 ，滴滴和uber的派单系统时时都是一个动态的决策， 如何把正确的司机和乘客连接在一起， 如何让车辆调动到需求量最大的地方， 这些都要时时的考虑各种因素调整决策。我们说这里面既包含了效率的问题， 也包含了乘客的安全。比如这一次滴滴的事故如果修正强化学习的效用函数， 是有可能避免的。当然除了派单和调动问题， 在每个十字路口交通灯的控制等， 整个城市里的立体交通网络的协调， 本质都是强化学习问题， 所以强化学习在智能交通大有可为 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5841035120147874&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8aEGBiaEgrS7f5yM8cFRoHfwKfEKKmCZRuo2O6r45scgkGPjwbnnajicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;541&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;4，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;金融&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;金融的核心， 交易， 是一个动态控制问题， 即使你不能完全预测明天股市的涨跌， 你依然需要直到我今天要不要下单，下多少单， 这，就是一个强化学习的决策， 它可以影响明天的股市， 也会在非常长远的时间里让我收益或亏损。机器交易，本质是个强化学习问题。当然，金融里能够应用强化学习的绝不仅仅这一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4258720930232558&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8p7o3Dgj17Ygk1o805cWk8rfcYsGpjVTrtThibodXLq2rIrwy2md0gAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;688&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5, 智能客服&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;智能客服本质是个强化学习问题， 如果你把它处理成监督学习问题， 那个对话机器人只能照猫画虎， 不能够真正从顾客的好恶的角度出发来发言， 而如果用强化学习， 那么机器人学习的就是如何正确的决策， 每句话都是为了最终讨得顾客欢心。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562254259501966&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8TsUlYnticNY3Cd22ziaa2IEQ3vtQQ3eeibia7P9IMc0wsK6SLvhZ84ATuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;763&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6， 电商&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;电商的本事是如何吸引人买更多的东西， 因此我们买了一个东西后它总会在下面给我们推荐其它的东西。然后我们看到了一个新的东西， 又会点开下一个连接， 这样一步步的就买了一大堆东西， 这样在每一步给你展示不同东西吸引你上钩的过程， 也可以看作是电商系统的动态决策过程， 是一个强化学习问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5674044265593562&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8jydTibwaO24npwTStA4ftpBib5iaAWfnNib78yGGwsZvmpB98WAtHtS2uA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;497&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7， 艺术创作&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;艺术创作领域看起来与强化学习无关， 事实上它可以很灵活的把人类的好恶加在强化学习的过程里，通过强化学习， 机器作曲可以自发的得到取悦于人的风格，也就是范式。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.47791798107255523&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8ibb1jxhO04FDBz6DCPfIexGdKRsb3vYIIsRJGf4DibR1ZTTxDibxvJLtQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;634&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让机器来决策，首先体现在如何模仿人类的决策。对于决策这个问题，&lt;/span&gt; &lt;span&gt;我们来看人类决策都要解决哪些难题。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;强化学习最初的体现就是试错学习， 因此理解强化学习的第一个层次就是如何通过一个简单的机制在不确定的环境下进行试错， 掌握有用的信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3946830265848671&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8bDDGfnbVQmh1GEHdJu9H2RXBcjibpF1Lc9qDwvd5QzuUdwxtQiaQR61A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1467&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;在这个框架下，&lt;/span&gt; &lt;span&gt;我们需要掌握的只有两个基本要素，&lt;/span&gt; &lt;span&gt;一个是行为，一个是奖励。&lt;/span&gt; &lt;span&gt;在这个级别的强化学习，&lt;/span&gt; &lt;span&gt;就是通过奖励，强化正确的行为&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt; &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;所谓行为，行为的定义是有限的选项里选以恶搞，&lt;/span&gt; &lt;span&gt;所谓智能体的&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;，走哪一个都有正确的可能，但是我们预先不知道这个东西。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所谓奖励， 就是环境在智能体作出一个行为后， 给它的反馈。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;大家看到，如果这个奖励是已知的，那么也就没有了任何的游戏需要进行的可能了。&lt;/span&gt; &lt;span&gt;你为什么要学？&lt;/span&gt; &lt;span&gt;每个行为得到的后果是不知道的啊！&lt;/span&gt;  &lt;span&gt;奖励具有随机性，&lt;/span&gt; &lt;span&gt;同样的条件性，&lt;/span&gt; &lt;span&gt;有的时候我们可以得到奖励，&lt;/span&gt; &lt;span&gt;有时候没有，&lt;/span&gt; &lt;span&gt;因此，&lt;/span&gt; &lt;span&gt;它也是一个随机变量，&lt;/span&gt; &lt;span&gt;理解这一点非常重要，&lt;/span&gt; &lt;span&gt;因此才可以理解很多的后面的算法。&lt;/span&gt; &lt;span&gt;奖励可以是正向的，也可以是负向的（惩罚）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.69875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8AKZgMqIPKFjricunYfGg1bkaiaYA5g9khFDIwVa27VgyOaEka0GsJ5Cw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们迅速的切入一个强化学习的最小实用例子，&lt;/span&gt; &lt;span&gt;又被称为&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;多臂赌博机的例子&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;你去赌场玩的，&lt;/span&gt; &lt;span&gt;都知道这个机器的存在，&lt;/span&gt; &lt;span&gt;它的构造就是很多个长得一样的摇臂，每个对应不同的中奖概率，&lt;/span&gt; &lt;span&gt;你要玩&lt;/span&gt;N轮， 每次选择摇臂， 使得最终的收益最大。聪明的你一定可以设计一个方案， 让自己的收益最大， 怎么做呢？ &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;首先， 你能不能一下子找到这里的行为和奖励是什么呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你能否设计一个算法解决这个问题呢？首先， 注意， 我此处的题设是N。假定这个N是一次你会怎么做？10次呢？无穷多次呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6067146282973621&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8ZoLBicVgE11906kib5ODmUn9512MNlBfk4NpnpAGQ2xerLgSzdTF9fLg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1251&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;每次行动后带来不同的奖励&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;很自然的，我们就来到了这么一个问题，就是探索与收益的平衡，&lt;/span&gt; &lt;span&gt;只要&lt;/span&gt;N不是1或无穷， 你都会有如下的窘境。假如你开始就中了 ， 你会一直选择那个中奖的摇臂不放过吗？显然这可能是陷阱，因为还有收益更高的臂，或者只是这次的运气好。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;反过来， 你会不停的随机试下去吗？显然不会， 因为存在收益比较高的臂。所以， 你就需要设计一个策略， 在有效探索的同时加大在那个最有收益可能的臂的概率， 每次玩， 又都增加你的信息。这就是一个极为典型的应对不确定的策略。&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;而且，&lt;/span&gt; &lt;span&gt;可以直接用到产品设计上，&lt;/span&gt; &lt;span&gt;我们也可以把它看成一个特别有方向性的试错。&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这里你要形成的第一个观点就是：&lt;strong&gt;奖励是随机变量 为了量化奖励， 我们需要引入期望。&lt;/strong&gt;这，才是我们要优化的对象。我们所处的环境下，环境给我们的奖励分布是未知的，因此，你必须在一开始边测量， 边引入收益的机制。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;刚刚的游戏，&lt;/span&gt; &lt;span&gt;和真实世界的大多数问题相差甚远，&lt;/span&gt; &lt;span&gt;因为它每轮只有一步就可以看到奖励，而且这一次抽取，&lt;/span&gt; &lt;span&gt;和下一次抽取一点关联都没有。&lt;/span&gt; &lt;span&gt;而事实上世界上的大部分游戏是一个连续多步骤，步步相连的过程。比如各种棋类，扫地机器人（连续离散化）等，&lt;/span&gt; &lt;span&gt;这样的问题，&lt;/span&gt; &lt;span&gt;很快前面的问题就不管用了。我们需要完善我们的框架来改进前面的东西。&lt;/span&gt; &lt;span&gt;好了，假定你是在设计这个东西，&lt;/span&gt; &lt;span&gt;那么你要加入一个什么要素呢？&lt;/span&gt; &lt;span&gt;时间？&lt;/span&gt; &lt;span&gt; &lt;/span&gt;&lt;span&gt;步骤&lt;/span&gt;&lt;span&gt;?&lt;/span&gt;&lt;span&gt;   &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;No,  &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;我们说， 解决这类问题， 首先引入的是状态。在围棋里， 你需要看到的是每一步其实你需要决策的信息都在当时的棋盘布局里，&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;而在扫地的游戏里， 每一步的信息都在当下的位置里， 也就是说， 我们把这些某个时间步骤出现的所有有用信息或特征叫做一个状态。有了这个概念， 多步游戏就可以看成根据状态来决策的游戏。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么， 现在我所有的元素就是行为-状态-奖励，每一步， 我都要根据过去和当时的状态来决策此刻。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.8585365853658536&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop83Tz5VTWEvGO0sabfn3sVNC6JRnl8gPzTUSrrF3YycibRIBtmavic1zYw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;205&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这样我们就有&lt;/span&gt;state（observation）- action - reward 这样的一个组合。或者说环境给你一个state， 然后智能体得到一个action ， 这个action改变环境， 并且环境返回智能体一个reward，如此循环， 当然在真实的游戏下我们并没有这样机械的一步步的过程， 而是一个连续的整体， 这种机械的方法是为了让问题可以轻松的被一个程序解决。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样的思路和图灵最早提出的图灵机智能模型具有异曲同工之妙， 而图灵机被认为是智能产生的基本模型，因此你也可以理解为什么强化学习和强人工智能有关。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;从状态到行为&lt;/span&gt;action的函数，也就是刚刚提到的那个条件概率， 通常称之为&lt;/span&gt;&lt;strong&gt;&lt;span&gt;策略&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;，&lt;/span&gt; &lt;span&gt;犹如通常意义上说的战略，&lt;/span&gt; &lt;span&gt;也就是一个行为的指导方案。&lt;/span&gt; &lt;span&gt;当游戏结束的时候，&lt;/span&gt; &lt;span&gt;我们把所有环境给我们的奖励加在一起算分，&lt;/span&gt; &lt;span&gt;越好的策略得到的分数越高，&lt;/span&gt; &lt;span&gt;这就是强化学习的本质。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TD学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么如何得到一个好的策略呢？这就是强化学习的中心问题， 大家以看就知道这本质上还是一个优化问题。那么整个后面的篇章都围绕这个展开。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如何得到好的策略？在上个游戏里， 游戏没有很多步，而是一步就可以拿到奖励，这个游戏里， 游戏有很多步， 这里必然引入的一个基本问题就是， 如果我还有好多步才得到奖励， 那么根据奖励来强化学习，将是一个极为困难的事， 因为我今天的决策只能影响明天， 但是明天什么结果都看不到， 这将是一个十分令人绝望的事。因为还学习个什么啊， 没有奖励就没有强化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;你能否结合自己的生活设立一个解决方法呢？&lt;/span&gt;  &lt;span&gt;想想我们高中三年，都是为了高考，&lt;/span&gt; &lt;span&gt;但是我们其实中间有无数小目标啊，&lt;/span&gt; &lt;span&gt;比如各种期末考，期中考。再看金融的一个例子，如果你是一个潜力股，&lt;/span&gt; &lt;span&gt;你是可以在你得到最终的结果前贷到款的对吧？&lt;/span&gt; &lt;span&gt;也就是说，&lt;/span&gt; &lt;span&gt;虽然明天并没有任何实际的奖励，&lt;/span&gt; &lt;span&gt;我们可以引入一个虚构的量，它能够把未来的收益给量化，这就是价值函数。它代表的不是当下的收益，而是未来的收益。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;这里就有一个问题了，&lt;/span&gt; &lt;span&gt;如果这个奖励在一个月以后到达，&lt;/span&gt; &lt;span&gt;或者在一年之后到达，&lt;/span&gt; &lt;span&gt;这两种情况是否应该一视同仁呢？你可以以你的直观感受直接告诉我，&lt;/span&gt; &lt;span&gt;不可能的！&lt;/span&gt; &lt;span&gt;我们可以有一个心理学实验，就是马上给你&lt;/span&gt;50元和， 和一年后给你100元， 你愿意选择哪一个（举手）。好了， 这样的机制事实上有着非常强大的现实意义。它的隐含含义就是， 我们需要一个贴现因子， 来惩罚未来的收益。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;在大多数情况下，&lt;/span&gt; &lt;span&gt;我们都生活在这样一种情况里，&lt;/span&gt; &lt;span&gt;我们的游戏里有小的奖励有大的奖励，&lt;/span&gt; &lt;span&gt;有今天的奖励有明天的奖励，&lt;/span&gt; &lt;span&gt;而游戏是连续的。&lt;/span&gt; &lt;span&gt;比如扫地机器人，&lt;/span&gt; &lt;span&gt;你可以想象成每扫一小块，&lt;/span&gt; &lt;span&gt;它就得到一个奖励，&lt;/span&gt; &lt;span&gt;这个时候价值函数将变成一个不同时间点的奖励的求和。&lt;/span&gt; &lt;span&gt;那么，&lt;/span&gt; &lt;span&gt;你有没有发现，&lt;/span&gt; &lt;span&gt;这个贴现因子本身更深刻的含义&lt;/span&gt;? &lt;span&gt;你能用它解释艰苦忍耐的人生观和即使享乐的人生观吗&lt;/span&gt;?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;平衡当下和未来， 建立当下和未来的桥梁，正是强化学习的第二个基本矛盾。也因为如此，当下和未来收益的统一者，价值函数就成为了强化学习的核心概念。这个函数的定义方法是首先把当下的奖励和未来的奖励加在一起， 由于奖励本身就是随机变量且未来是不确定的， 我们要是把奖励都加在一起， 依然得到的是一个随机变量， 你要衡量一个随机变量的大小， 只能对它取期望。这样， 这个带着未来收益的期望， 就是我们对价值函数的最终定义。强化学习， 就变成了如何找到那么一个策略， 使得我们这个value函数达到最优。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;然后我们来说，&lt;/span&gt; &lt;span&gt;强化学习的核心，&lt;/span&gt; &lt;span&gt;策略。&lt;/span&gt; &lt;span&gt;所谓策略，&lt;/span&gt; &lt;span&gt;无非是把当下的这个对未来的估值，&lt;/span&gt; &lt;span&gt;我们也可以看作趋势，&lt;/span&gt; &lt;span&gt;和我们要的行动联系起来。&lt;/span&gt; &lt;span&gt;我们干脆把可能的行动也放在这个价值的条件里面去，&lt;/span&gt; &lt;span&gt;也就是，&lt;/span&gt; &lt;span&gt;我们定义目前每个行动下的值函数，&lt;/span&gt; &lt;span&gt;给它起个酷炫的名字，叫&lt;/span&gt;action value， 每个行动的价值。我问你， 如果我有了这个函数， 该如何求得策略？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;如果你告诉我这无非是找那个最高估值对应的行动，&lt;/span&gt; &lt;span&gt;恭喜你答对了。&lt;/span&gt; &lt;span&gt;你已经掌握了强化学习的精髓，&lt;/span&gt; &lt;span&gt;我们需要强化（选择）的行动，&lt;/span&gt; &lt;span&gt;就是那个使得整个走势，&lt;/span&gt; &lt;span&gt;也就是值函数最高的行动啊！&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;但是，同时我要告诉你没有完全答对。&lt;/span&gt; &lt;span&gt;为什么？&lt;/span&gt;  &lt;span&gt;有没有同学能发现我刚刚的逻辑漏洞？&lt;/span&gt;&lt;/span&gt;&lt;span&gt;如果你告诉我老师，&lt;/span&gt; &lt;span&gt;哪里来的值函数，&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;你怎么算出来的，&lt;/span&gt; &lt;span&gt;就是非常聪明。&lt;/span&gt; &lt;span&gt;刚刚讲了这么多，&lt;/span&gt; &lt;span&gt;我实际上是偷懒了啊，&lt;/span&gt; &lt;span&gt;我讲的都是一堆定义，&lt;/span&gt; &lt;span&gt;实际怎么操作却一点没讲。&lt;/span&gt; &lt;span&gt;你记得我说的，&lt;/span&gt; &lt;span&gt;我们求的是期望，就需要概率。&lt;/span&gt; &lt;span&gt;这个概率，&lt;/span&gt; &lt;span&gt;就是我的行为，&lt;/span&gt; &lt;span&gt;引起环境如何的改变的概率，&lt;/span&gt; &lt;span&gt;但是，&lt;/span&gt; &lt;span&gt;正如我们在赌博机里不知道每个臂的筹码，&lt;/span&gt; &lt;span&gt;这里我们也不会知道环境给我们反馈的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;对于这个问题的解决，&lt;/span&gt; &lt;span&gt;一个是笨办法，&lt;/span&gt; &lt;span&gt;一个是聪明办法。&lt;/span&gt;  &lt;span&gt;所谓笨办法，&lt;/span&gt; &lt;span&gt;就是不停的去实验，&lt;/span&gt; &lt;span&gt;从当下的状态和行为出发，&lt;/span&gt; &lt;span&gt;试它一万次测个平均，&lt;/span&gt; &lt;span&gt;所谓蒙特卡洛。&lt;/span&gt; &lt;span&gt;这个方法有着一个致命的弊病&lt;/span&gt; &lt;span&gt;，那就是你得等到游戏的结束才能更新一次&lt;/span&gt;v值，速度太慢了，而且想想有些时候你只能进行一次或几次游戏， 比如人生的游戏你只有一次， 你不能死了再回来， 所以这个方法就不那么给力了。怎么办呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然后我们看聪明方法，这个办法就是不等到游戏结束就更新。这个思维有点逆向。也就是你假定游戏已经结束了， 你从终局往回看整个游戏， 假定游戏结束的时候有一个终极的奖励刺激。这个时候， 你能马上更新的v函数一定是那个你离终局最近的状态。而如果你已经更新了这个状态， 那么它之前的那个状态呢？请你想一下， 我可不可以接着更新这个状态？当然， 我离终点奖励差两步， 所以我无非是把终点奖励乘以两次贴现。哦， 这样看可以， 同样的方法， 你可以不可以更新前三步， 前四步， 前5部， 前n步的状态？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;那么下一次游戏呢？&lt;/span&gt; &lt;span&gt;我是否还需要等到游戏的终局再更新呢？&lt;/span&gt;  No！在新的游戏里， 如果你走走走， 恰好达到了上次游戏里某个经过的状态， 你立刻会看到， 这里已经标记过了一个估值， &lt;span&gt;你可以怎么做？&lt;/span&gt; Ok， 你可以利用这个估值， 并用它来更新你之前的一步！ &lt;span&gt;因为你可以假定当你到大了这里，&lt;/span&gt; &lt;span&gt;一切的情景都是有过往经验支撑了，&lt;/span&gt; &lt;span&gt;如同你在一个城市里走走走，&lt;/span&gt; &lt;span&gt;误打误撞到了昨天走过的一条小路，&lt;/span&gt; &lt;span&gt;那么从这里，&lt;/span&gt; &lt;span&gt;一切都变成已知。&lt;/span&gt; &lt;span&gt;这个方法翻译成数学语言就是&lt;/span&gt;TD时间差分学习。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;数学上的表现就是估值函数v的迭代表达式， 你每次格按照v的迭代式定义来更新v函数，这样多步之后v也会趋于正确的值。好比当你在开车的时候， 你险些撞车， 游戏没有终止， 但是足以让你使用这个惊险来更新值函数。另外一个例子是你打公交车去上班， 每过一个站你看一个时间， 看和你的预计是否有差别， 这每一站的时间， 足以让你不停的调整对最终实现时间的预期。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看看这个故事背后的生物学故事。&lt;/span&gt;  &lt;span&gt;所谓的时间差分，&lt;/span&gt; &lt;span&gt;正是条件反射的基础。&lt;/span&gt; &lt;span&gt;你记得巴甫洛夫的狗吗？&lt;/span&gt; &lt;span&gt;摇铃和食物有什么关系？&lt;/span&gt; &lt;span&gt;食物就是我们说的终极的奖赏，&lt;/span&gt; &lt;span&gt;而终极奖赏之前，&lt;/span&gt; &lt;span&gt;摇铃是一个中间步骤，&lt;/span&gt; &lt;span&gt;由于大量的经历里，&lt;/span&gt; &lt;span&gt;摇铃都导致了食物，&lt;/span&gt; &lt;span&gt;所以，&lt;/span&gt; &lt;span&gt;狗就会学会，&lt;/span&gt; &lt;span&gt;摇铃就是那个终极奖赏前的状态，因此，&lt;/span&gt; &lt;span&gt;摇铃也应该得到一个更高的值函数。&lt;/span&gt; &lt;span&gt;这时候，&lt;/span&gt; &lt;span&gt;摇铃也就具有了某种奖励的性质，可以引起狗的流口水了。&lt;/span&gt; &lt;span&gt;因此你可以继续往前推，&lt;/span&gt; &lt;span&gt;你是否看到，&lt;/span&gt; &lt;span&gt;摇铃前还可以放置一个灯光，灯光之前还可以放一个声音呢？&lt;/span&gt;  &lt;span&gt;你是否看到了，&lt;/span&gt; &lt;span&gt;如果人生赢家是抱得美人归，&lt;/span&gt; &lt;span&gt;那之前的高考长夜，&lt;/span&gt; &lt;span&gt;之前的寒窗苦读，&lt;/span&gt; &lt;span&gt;之前的所有一切，&lt;/span&gt; &lt;span&gt;是不是都成为了我们趋之若鹜的奖励。&lt;/span&gt; TD学习， 是所有生物行为学习的基础。  &lt;span&gt;你可不可以用它的原理设计一个控制拖延症的方法呢？&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;有了&lt;/span&gt;TD学习， 我们可以随时更新每个状态下所有可能行为对应的值函数， 这样， 我们就可以选择那个当下估值最高的选项来行动， 所谓策略的更新（比起之前的估值下的行动）。  &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;由于我们每次更新的时候，&lt;/span&gt; &lt;span&gt;我们仅仅是比之前多了一点信息，我们的这个值函数依然是不完美的，&lt;/span&gt; &lt;span&gt;幸运的是，&lt;/span&gt; &lt;span&gt;每个行为最终都导致我们进入一个新的状态，&lt;/span&gt; &lt;span&gt;使得我们进一步的获得了上一个状态的信息，从而进一步的优化我们的值函数，&lt;/span&gt; &lt;span&gt;若干论之后，&lt;/span&gt; &lt;span&gt;我们将会得到一个最优的值函数和策略。这样的迭代过程，&lt;/span&gt; &lt;span&gt;值函数的更新紧跟着一个行为，&lt;/span&gt; &lt;span&gt;如同一组拉丁舞曲。这个算法就是大名鼎鼎的&lt;/span&gt;salsa和Q学习（此处不做区分&lt;span&gt;）的简化版。&lt;/span&gt;  &lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;这就是强化学习的第二个阶段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然后我们来看强化学习的第三个阶段， 所谓用想象和世界模型来填补的阶段。&lt;/span&gt;&lt;span&gt;首先，&lt;/span&gt; &lt;span&gt;我们刚刚遗漏了一个重要的&lt;/span&gt;&lt;span&gt;point， 这个世界难道真的是如刚描述的只有几个有限的状态吗？肯定不是。我们真实生活的状态是无限的。那么，人脑是如何由有限战胜无限的？我们有联想，有想象。&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt; &lt;/span&gt;&lt;span&gt;这，也是强化学习的第三个阶段。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，我们引入估值函数的近似解法。&lt;/span&gt; &lt;span&gt;刚刚说的值函数，&lt;/span&gt; &lt;span&gt;事实上如果永远都走不回之前的小路的时候，&lt;/span&gt; &lt;span&gt;你就不能用了。&lt;/span&gt;  &lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;但是，&lt;/span&gt; &lt;span&gt;如果你看到一个地方类似之前的走过，&lt;/span&gt; &lt;span&gt;你可不可以根据之前的经验判断呢？&lt;/span&gt; &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;所谓一朝被蛇咬，十年怕井绳。&lt;/span&gt; &lt;span&gt;你要引入一个东西，&lt;/span&gt; &lt;span&gt;具有从以往经验推测的能力&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt; &lt;span&gt;我们有没有学习过类似的东西？&lt;/span&gt; &lt;span&gt;所有的监督学习方法，&lt;/span&gt; &lt;span&gt;都是说这件事啊！&lt;/span&gt; &lt;span&gt;我们根据经验特征，从已知数据推知未知数据。&lt;/span&gt;  &lt;span&gt;由此我们得到值函数的近似算法。&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;有了值函数的近似算法， 虽然看起来我们的体系已经相当完整，但是依然有一个比较严重的弊端， 那就是最终学到的策略只能确定性的， 根据定义 ，这个最优化的策略是每一次选取Q（s,a)里最大的那一个。可是在这个不确定的世界， 我们的最优化策略本身就需要包含随机性，我们真正的策略， 恰好是如何设定这个随机性。我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果此时引入一个随机性的策略， 这个问题影刃而解，我无非子啊左右两个灰色的格子里制定左右各50%的策略， 这时候总是最终客户以达到宝藏，就是时间可能稍微长一点。这样的随机性的策略， 引入策略函数就可以可以很容易的学出来。这时候，我们的你策略及从一个状态下确定的行为函数， 变成了了状态的随机变量， 每个行为以一定的概率来取得。如果最初我们的Q学习里， 我们每一步选择那个动作状态的值函数最高的行为， 那么这里，我们就把这种选最大的行为变成一个称为softmax的函数， 它说的就是，我们依然倾向于选择那个值函数较高的行为， 但是， 其它的选项也是可以选择的， 选择的方法根据这个softmax函数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;有模型学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;刚刚讲的方法，通常称为无模型学习，所谓无模型，　就是当环境的动力学（也就是那两个条件概率，想象下棋的例子）不知道的时候，　我们通过直接抽样的方法来更新Ｑ（ａ，ｓ）和ｐｉ来进行控制住．这样做的一切基础在于环境是未知的，&lt;/span&gt; &lt;span&gt;我不知道环境是如何给我反馈，&lt;/span&gt; &lt;span&gt;假定你知道了环境反馈的方法，你会怎么做呢？&lt;/span&gt;  &lt;span&gt;显然，&lt;/span&gt; &lt;span&gt;你要做的是直接展开计算这个值函数而不是通过抽样更新！&lt;/span&gt; &lt;span&gt;这样，我们几乎每一步都会得到精确的值函数，&lt;/span&gt; &lt;span&gt;而游戏几乎会在瞬间得到最优策略！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;可是我们不知道这个世界给我们反馈的方法啊！&lt;/span&gt;  &lt;span&gt;这里，&lt;/span&gt; &lt;span&gt;我们可不可以在这个地方也引入一个机器学习的近似思想，&lt;/span&gt; &lt;span&gt;让我们通过在环境里取得的数据去近似这个世界模型呢？&lt;/span&gt; &lt;span&gt;当然可以的，&lt;/span&gt; &lt;span&gt;我们可以这么做！&lt;/span&gt; &lt;span&gt;所谓的环境反馈，&lt;/span&gt; &lt;span&gt;无非是下一步环境给我的状态，和给我的奖励，&lt;/span&gt; &lt;span&gt;我可以用监督学习的思想，&lt;/span&gt; &lt;span&gt;每走一步，&lt;/span&gt; &lt;span&gt;都收集环境给我的这两个数据，&lt;/span&gt; &lt;span&gt;然后，&lt;/span&gt; &lt;span&gt;我们就可以非常用我们的那些机器学习工具，&lt;/span&gt; &lt;span&gt;比如神经网络，&lt;/span&gt; &lt;span&gt;来计算它们和上一步环境状态，与我所做行为选择的关系。这样，&lt;/span&gt; &lt;span&gt;我就会得到一个可以学习的世界模型。&lt;/span&gt; &lt;span&gt;虽然它依然不是准确的，&lt;/span&gt; &lt;span&gt;却可以大大加速我得到准确的行动的性质。&lt;/span&gt;  &lt;span&gt;在此处，我们可可不可以学习创造一个世界模型，　来提高我们抽样学习的效率呢？　当然可以，　你不是由监督学习吗？&lt;/span&gt; &lt;span&gt;我们可以用类似监督学习的思路来把这个环境的动力学学出来啊！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;我们在这里就进入了有模型学习的范畴．　这个方法，又称为&lt;/span&gt;planning， 犹如一种做白日梦的能力，  &lt;span&gt;它可以通过自己构造的世界模型，&lt;/span&gt; &lt;span&gt;不停的想象某个步骤之上，&lt;/span&gt; &lt;span&gt;如果我采取某个行为后环境如何给我反馈，&lt;/span&gt; &lt;span&gt;得到如何奖赏，&lt;/span&gt; &lt;span&gt;结果，&lt;/span&gt; &lt;span&gt;我不需要不停试错，通过我脑子里的这个p&lt;/span&gt;lanning &lt;span&gt;，&lt;/span&gt; simulation &lt;span&gt;就可以获取关于某个行为该不该做的信息！&lt;/span&gt;  &lt;span&gt;虽然&lt;/span&gt;agent 并没有真正经历那些行为，　就好像经历过了一样，　这样agent 就如同获得了非常多的虚拟数据，　可以更准确的对未知的状态进行估值，　在数据极为稀缺高维诅咒极为明显的强化学习问题里，　这个效果是巨大的．好比正因为你瞻前顾后， 你才减少了很多错误！ &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;能够实用这个方法也是有条件的，&lt;/span&gt; &lt;span&gt;你觉得条件是什么呢？&lt;/span&gt; &lt;span&gt;如果你告诉我你必须确实能够掌握环境的套路，&lt;/span&gt; &lt;span&gt;给环境建立模型，&lt;/span&gt; &lt;span&gt;恭喜你答对了！&lt;/span&gt; &lt;span&gt;如果环境完全是不可琢磨的，&lt;/span&gt; &lt;span&gt;那么你最好的方法依然是直接试错的方法而非建模。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;有模型学习的最好应用例子就是阿法狗。&lt;/span&gt;  &lt;span&gt;所有棋类游戏，&lt;/span&gt; &lt;span&gt;都实用一个叫马尔可夫决策的框架，&lt;/span&gt; &lt;span&gt;这个框架里，&lt;/span&gt; &lt;span&gt;下一步都只和这一步的状态相关。&lt;/span&gt; &lt;span&gt;然后，&lt;/span&gt; &lt;span&gt;我们的环境是什么呢？&lt;/span&gt; &lt;span&gt;假定你手里是黑子，那么你的&lt;/span&gt; &lt;span&gt;对手白字就是下一步的状态。&lt;/span&gt; &lt;span&gt;显然，如果你能够知道在你下某个棋的时候白字如何落子，&lt;/span&gt; &lt;span&gt;你就可以建立所谓的世界模型&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;阿法狗所用的方法是，&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  先备棋谱在推理。&lt;/span&gt; &lt;span&gt;所谓的背诵棋谱，就是说直接模拟大师下棋的直觉，&lt;/span&gt; &lt;span&gt;我可以通过看很多大师的下法，&lt;/span&gt; &lt;span&gt;了解无论是黑还是白的套路，&lt;/span&gt; &lt;span&gt;这个东西有一个&lt;/span&gt;CNN来完成。之后， 我来看如何能够加入推理， 这个推理的成分， 有一个叫做蒙特卡洛树搜索的过程完成， 怎么个玩法？ &lt;span&gt;就是在&lt;/span&gt;CNN的直接基础上， 我在大脑里多下几步，形成一个模拟过的走子过程，如此，我们就可以知道真实下去这一步， 白字最有可能如何反应，我会如何应对， 白字又如何反应， 这种直觉加推理的方法， 就是阿尔法狗建立世界模型的方法。有了这个方法， 阿尔法狗就可以战胜围棋最大的挑战， 宇宙星辰般的搜索空间，战胜人类。这样的思维方法， 如果你学会， 也是不可小觑的啊！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqCgYN1dD3XAMuRjXZuW8LY5OKAgKPmsVSQzPpDldvFT9aVgpMWrl06g/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-w=&quot;800&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384317&amp;amp;idx=1&amp;amp;sn=329aeb82919ab6237bdb8f355e202311&amp;amp;chksm=84f3c7bcb3844eaab5197fecaba741c6cc97606ba41309729b18a2465e4357c53e81b523d2b2&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;5分钟读懂强化学习之Q-learning&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;铁哥的强化学习特训课&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 19 Apr 2019 15:02:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/A5jNLvecGM</dc:identifier>
</item>
<item>
<title>5分钟读懂强化学习之Q-learning</title>
<link>http://www.jintiankansha.me/t/SrT1aw8yLw</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/SrT1aw8yLw</guid>
<description>&lt;p&gt;强化学习的难点，在于其引入了时间这个维度，不管是有监督还是无监督学习，都是能获得即使反馈，但到了强化学习中，反馈来的没那么及时。在周志华的《机器学习》中，举过一个种西瓜的例子。种瓜有很多步骤，例如选种，浇水，施肥，除草，杀虫这么多操作之后最终才能收获西瓜。但是，我们只有等到西瓜收获之后，才知道种的瓜好不好，也就是说，我们在种瓜过程中执行的某个操作时，并不能立即获得这个操作能不能获得好瓜，仅能得到一个当前的反馈，比如瓜苗看起来更健壮了。因此我们就需要多次种瓜，不断摸索，才能总结一个好的种瓜策略。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.8597222222222223&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了应对时间带来的不确定性，就需要一个框架来量化时间的流逝对我们关心奖励有怎样的影响。按照最简单的线性模型，我们首先确认要引入那些特征，首先是前一个时间的得分，其次是新发生的事件对奖励的影响，由于我们对未来的奖励看的不如现在的重要，因此可以引入折线率，折现率越高，说明我们越处于游戏的早期，对未来的关注也越多，这道理就如同我们在年轻时更要做长久的规划。同时在更新策略时，也会有快慢之分，将其称为学习率。由此得出了时间差分学习(Temporal Difference)，简称TD方法的更新公式：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.14660831509846828&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatGHyEQuonYibg2174nHUCFhLdAGRQvSVal7TrHezmsKZ01ErVogJuYA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;457&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该公式描述了给定一个策略，该怎么去更新下一个时刻的估值函数，其中的V代表估值函数，下一个时刻的估值乘以折现率，再减去当前的差值，代表了一个策略的间接影响，可以看成是战略决策，再加上下一个时刻能立即获得的奖励，就是智能体（agent）应该关注的策略的影响，最后对此乘以学习率，用来控制随机性的影响，既要避免由于学习率过低导致的智能体学的太慢，也要避免学习率过高导致智能体矫枉过正。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;由于在当下，你并不知道下一时刻的估值函数，所以你要做的是对其有一个尽可能准确的估算，这个估算被称为Q value，对应的算法称为Q-learning。如果你是用神经网络得出对未来value的估算的，那你使用的算法框架就从强化学习变为了深度强化学习。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.7262357414448669&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaiaD8kfEVYfNkfJgLhYEbOzyOo6TqicKnb0NGgYfL5ib6Kkb8jOMt56xTg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;526&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是一个具体的例子，将一个结冰的湖看成是一个4×4的方格，&lt;span&gt;每个格子可以是起始块（S），目标块（G）、冻结块（F）或者危险块（H），目标是通过上下左右的移动，找出能最快从起始块到目标块的最短路径来，同时避免走到危险块上，（走到危险块就意味着游戏结束）为了引入随机性的影响，还可以假设有风吹过，会随机的让你向一个方向漂移。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.466893039049236&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaacrLdcvu8QwtBo9mvVzf3KZ3wZGMospic8OhNsuz9sdlNvfjjicP53hdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;589&quot; /&gt;&lt;/p&gt;
&lt;p&gt;左图是每个位置对应的Q value的表，最初都是0，一开始的策略就是随机生成的，假定第一步是向左，那根据上文公式，假定学习率是0.1，折现率是0.5，而每走一步，会带来-0.4的奖励，那么（1.2）的Q value就是 0 + 0.1 ×[ -0.4 + 0.5× (0)-0] = -0.04，为了简化问题，此处这里没有假设湖面有风。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4200626959247649&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaibSXpgch86ApiaPm5EBrQkr7KvJxmlPOoYxDa4icOJiajK8Oyl55fuhd7w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设之后又接着往右走了一步，用类似的方法更新（1，3）的Q value了，得到（1.3）的Q value还为-0.04&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4054878048780488&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaTtSt6Siceq5eg320L4ibrKfiaVFKloHuhiaAXS3ficPIheOXfH8W0Mskqcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;656&quot; /&gt;&lt;/p&gt;

&lt;p&gt;等到了下个时刻，骰子告诉我们要往左走，此时就需要更新（1，2）的Q-value，计算式为：V(s) = 0 +0.1× [ -0.4 + 0.5× (-0.04)-0) ]&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.46333853354134164&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaabAV71N9QPLA95sNdHmxw735yYwBd1JZ4fVxG4fMuYaHrlhLnDrnBcA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;641&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;从这里，智能体就能学到先向右在向左不是一个好的策略，会浪费时间，依次类推，不断根据之前的状态更新左边的Q table，直到目标达成或游戏结束。这就是TD learning的基本步骤，通过多次的实验，智能体掌握了在不同位置下，相应的策略的估值分，从而解决了将较远的未来映射到当下的对不同策略的激励这个强化学习的核心问题。&lt;/p&gt;

&lt;p&gt;根据是否亲自尝试不同的策略，Q learning可以分为在线和离线俩者，用学下棋来举例，前者是AI通过自己和人类选手下棋或者自我对弈来提升，而后者AI不操作只观察他人下棋的棋谱，下面看看再离线（off-line）的Q learning中，Q value更新的公式又有了怎样的改变。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.12110726643598616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa56EFEGaa5FzYynegbiaG0HHB5VXQericOGjxf1VyCWAGKjQyTyzeCPSQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;578&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和之前的公式对比，最大的不同是未来的Q值是所有行动/策略对应的未来Q值中最大的那一个，这代表着模型根据已有的知识，选择了局部最优的那个行动，通过不断的优化Q table，使得这样一个只考虑一步的最简单型启发规则，也能学到全局相对较优的策略。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.292626728110599&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatd4f6hZKGMOiasVOoUG86yF7sKictnZPX3LCNib7BPjlyMxJYAiabw8htA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;434&quot; /&gt;&lt;/p&gt;


&lt;p&gt;还是冰湖的案例，假设在训练的循环中，当前智能体已经学会了在（3，2）这个点上，向左和向右走对应的估值，此时模型要做的是去判定利用当前的知识，还是去探索未知策略的影响，探索是为了发现环境的更多信息，而当探索进行到了一定的程度，就需要根据已知信息去最大化奖励值，在Q learning中，通过一个0-1的参数来用随机性调控探索和开发的权衡。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.54&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaRggot5LiaL2n2jGVV7aOQAQp5L5WXiaqkvtl1CFlv4FLJJuGW0H3IZDA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设骰子告诉智能体应该选择探索，因此选择了向下走，左图代表的之前智能体的Q-table，现在要做的是根据公式，更新（3，2）这里的Q value，由于向下走的Q-value最低，假定学习率是0.1，折现率是1，那么（3，2）这个点向下走这个策略的更新后的Q value就是：&lt;/p&gt;

&lt;p&gt;Q( (3,2) down) = Q( (3,2) down ) + 0.1× ( 0.4 + 1 × max [Q( (4,2) action) ]- Q( (3,2), down））&lt;/p&gt;

&lt;p&gt;Q( (3,2), down) = 0.6 + 0.1× ( 0.4 + 1 × max [0.2, 0.4, 0.6] – 0.6)=0.64&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4582043343653251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaDiahblY26dmQgclaiac3B1IOojqD73sqjlEfqdpjYvgEZf4j0CEcNekw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;646&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在在线的Q learning算法下（称为State-Action-Reward-State-Action ，简称SARSA），Q table的更新公式变为了&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatj8T5icjPo7sDEOpshRUibnb0ldh2ic59JdNcGibaqwicpcicHjiaM8TyEKmA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;588&quot; data-cropy1=&quot;9.517985611510792&quot; data-cropy2=&quot;93.06474820143886&quot; data-ratio=&quot;0.14285714285714285&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa2dTIjl6icv7frMeMnyqIUPDhYeJ55RWKPxtoicbj5p0gpOB5kubo8DMQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;588&quot; /&gt;&lt;/p&gt;
&lt;p&gt; 此处不同的是没有了max，由于是智能体在亲自参与，这里也就没法像离线时那样，选择一个最优的策略。不管是在线还是离线，在训练的时候需要做经验回放，即存储当前训练的状态到记忆体中，等下一次训练时再调用。&lt;/p&gt;

&lt;p&gt;以上就是强化学习中最基础的Q learning，上诉的例子中不存在随机性，要引入随机性，可以需要通过蒙特卡罗的方法，来进行采样，同时引入对弈树，对其进行翦枝，这就是alpha zero的精髓。了解了Q learning的步骤，可以分析强化学习适用的领域所满足的假设，例如必须有能够清晰定义，事先已知且有限的策略，但现实生活中，真正重要的选择都是无限游戏，有无数种可能的选项，有前人根本不曾想到的选项，因此说强化学习不等价于强AI，只是通向强AI的一条必要选项。&lt;/p&gt;

&lt;p&gt;不同于人类的学习，是首先对坏境建模，之后再根据模型找到合适的启发式规则，Q learning框架是模型无关的，不管是什么样的问题，Q learning做的都是去更新状态对应的估值表，不管问题本身具有什么样的特点。和人类思维的另一个不同是Q learning中没有因果关系，学到的Q table只是反映了奖励和策略之间的相关性，而人类的学习则是受着因果关系指引的。关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;，其中有详细的论述。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384219&amp;amp;idx=1&amp;amp;sn=f396d027ea5a6074e0f0cda0aeb0cded&amp;amp;chksm=84f3c7dab3844ecc80be70e9b9e47cd12686624d71158caf69fb79de9b1067d1b548e31ee132&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;空间简史-人类认识空间的旅程与其对强化学习的启示&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 08 Apr 2019 18:01:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/SrT1aw8yLw</dc:identifier>
</item>
<item>
<title>模拟人类思维的机器学习算法</title>
<link>http://www.jintiankansha.me/t/EXOKZtMJtK</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/EXOKZtMJtK</guid>
<description>&lt;p&gt;假定你是一个数据科学家， 老板给了你一组数据， 你来让机器完成这个预测的任务。就假定一定老掉牙的任务吧， 房价。 这也是大家所最关心的东西了。&lt;/p&gt;

&lt;p&gt;用数据来预测的一个关键要素是寻找数据间的相关性，无论是谷歌搜索预测总统选举， 还是用数据推测买啤酒的用户是不是更容易买花生， 这些相关性都已经存在， 数据能够帮我们发现那些我们看不见的相关性，但是， 首先为前提的是， 相关性必须是已经存在的， 你不能从没有相关性的东西里预测另一个东西， 比如从今天天气的好坏预测汽车板块的股价。&lt;/p&gt;

&lt;p&gt;如果给相关性换一个词，那就是信息。你增加的每个数据， 必须让你所要预测的东西更加确定， 否则就是噪声。那么， 什么是最简单的数据预测方法呢？ 你所再熟悉不过的东西， 线性回归。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1 线性回归&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;回归这个词说的其实是一种由果索因的思维， regression拉丁语的原意， 是act of going back， 是回去，往回走的意思。 那么回的是哪里？ 我们说， 如果一组变量间存在相关性，比如A，B，C都与D相关。 那么我们就认为A，B，C可以确定D 。  预测D也就是由D回到A，B，C的过程。 线性回归， 就是给A，B，C三者每个变量一个重要性的打分，称为权重，然后按权重对三个有影响的变量做线性带权加和。&lt;/p&gt;

&lt;p&gt;让问题再简单一些，如果只保留A这一个影响因素， 你会发现D和A在一条直线上。 无论你一开始发现D的数据如何跳动， 你不停的收集数据， 它的平均数总是要回到那条直线上。&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5179738562091504&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaAsIYb93t9qxon2rXomhfa5KgjsELVEocXUaiaDpFAY2dQBCOyPNLguA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;612&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将线性回归的思路用在预测房产的价格，如果你打开链家的网站，  你会发现一个房子的价格决定于非常多的相关因素， 比如房子的地点， 房子的面积， 房子中卧室， 厕所，客厅的数量和面积， 周围环境的交通， 安静与否，有无学校等， 我们要从这些要素中知道房子的价格。如果你是老式的数据专家， 让你找到一个方法预测房价。你的方法一定是到各个地点做调查成交价，然后把它们画成一个表格。这个表格里， 前面是那些链接房产列出来的房屋属性， 最后一个是价格。 你会设法列出一个经验公式来， 比如刚刚的线性公式，然后你认为哪个因素比较重要， 就把前面的系数调大一点， 直到这个公式和你表里的公式符合的比较好。 &lt;/p&gt;

&lt;p&gt;这个老派数据专家的做法， 换一个专业的术语叫做拟合。 只要是学过中学物理的人对这个东西都不会陌生。 记得我们要求得一些重要参数吗， 比如重力加速度， 还是水的密度。 你的做法都是取不同体积的水， 计算它们的质量，然后把它们画在一张纸上，每一个点对应一次测量， 一般情况下， 你会发现所有的数据点在一条直线来拟合， 而这条直线的斜率，可以帮你计算出水的密度来。  这就是拟合， 也就是老派房价研究者的方法了。 你把房子的大小，屋子的数量算作显著因素， 然后把不同地区的房价平均值作为基准， 就可以给每个地方的房子拟合出这样一个曲线。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.806970509383378&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaldvybSbvTibjyN5fRjZguicW3L8faOAJhgvBg0sQfXdEZ9FO0IWpf4sg/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;373&quot; /&gt;&lt;/p&gt;

&lt;p&gt;好了， 把这门直线拟合的技术交给电脑， 就是机器学习了， 你可以用python完成，也可以用excel。 就是这么简单。 事实上很多人也是这么说的， 它们管机器学习叫做拟合。线性回归一定不像你想的那么简单。  首先， 当下的数据， 早已不是以前那种两个或三个特征的数据。 如果你打开链接， 你可能发现近百个和房价有关的特征。&lt;/p&gt;

&lt;p&gt;如果你是百度预测广告链接的人， 你会发现你的用户的特征成千上万。难道你不能挑出那些重要的吗？ 问题是， 没有人能预先清楚哪些要素重要。&lt;/p&gt;
&lt;p&gt;如果给一千个因素列出一个线性回归的公式， 然后把它们的系数解出来， 你看看这件事还容不容易呢 ？ 即使是交给电脑， 也不是一两下可以算出。&lt;/p&gt;

&lt;p&gt;更关键的是， 你其实预先并不知道你的这些特征有还是没有用， 我们要设计的算法， 要能从这些信息中首先识别， 然后学习价格的pattern（模式），然后得到这个关系。 在此之前， 老派人士的专业知识可以很好的帮我们滤清这个关系， 比如你可以很清晰的甄选出房子的大小和卧室数量很重要。 但是， 一旦特征多起来， 我想就会超出任何人的专业知识， 比如你是否能判断一个地区的男女比例对房价的影响呢？然后我们的数据也很多， 在这样多的数据下， 任何专家都会眼花缭乱。  &lt;/p&gt;

&lt;p&gt;所以， 我们要建立一个新的思维框架， 来让机器自己完成这个特别繁琐的任务。首先， 我们 把之前说的因素，属性换以一个新的词汇-特征。特征， 就是那些我们认为可以用来预测的因子， 无论它最终是否有作用。一个特征， 可以用一个数表示，更好的理解是它是空间里的一个坐标，就如同下x，y，z。 特征的个数我们通常称之为维度。 &lt;/p&gt;

&lt;p&gt;这样， 刚刚说的一个问题的特征很多， 我们就换作了一个新的词， 就是维度很高， “高维”， 问题越高维， 就是通常说的越复杂。 维度本身同时决定我们可能需求的数据量多少， 高维度意味着我们需要求解问题的信息量也成比例的增长。由于你我都很难想象高维空间的事情， 我们就拿二维来讲解， 以后， 机器学习就称为在平面画曲线的过程。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7491525423728813&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaGY2VBFfJnNKHfNQPpkcaslrpIgK5LdIhiaibTZiaYEpH6vISYKq8rjAHg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;295&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们来说算法，特征如何决定预测对象， 哪些特征有用， 哪些没用，用处大小， 就是算法的事情。算法也可以看作机器学习模型，它就像一个机器， 这个机器输入进去数据， 输出我们要预测的对象， 听起来像不像一个函数呢？ 但是它不是一个函数， 对机器学习模型更好的理解是一个框架，或者说一大类函数的综合。 &lt;/p&gt;

&lt;p&gt;比如刚刚我们说的线性回归。 作为机器学习模型的线性回归，可以看作包含了所有的这类很多因素相加的函数， 那些刚刚说的决定每个特征重要性的参数，叫权重。 每一组权重都对应一个特定的函数， 我们也叫它假设。 机器学习与函数的区别在于， 你所拥有的是这样一台机器，你把数据扔进去，它自动给你吐出一个最好的假设， 它能够最好的解释你的数据。   &lt;/p&gt;
&lt;p&gt; &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;为什么叫模拟人类思维的机器学习模型？  因为这些假设的提出是直接从我们的思维转化的。 比如决定房价的方法就是不同的因素按照一定权重组合。最终得到这组最匹配数据的假设的好处是什么？  机器就可以自动做预测了。 我们不再需要房产专家， 我们用有限驾驭无限。 因为你用来得到这个假设的是你收集的有限的数据， 而你要预测的是无限的情况。&lt;/p&gt;

&lt;p&gt;现在我们来说如何求得这个假设， 或者说最佳参数， 这个过程通常称为训练。训练用一句话总结，就是在错误中改进和学习， 假定你是一个房价预测元， 你的评估也许开始是按照自己的经验来是房子的面积比较重要还是地点比较重要， 但是之后他很快被上级打的一塌糊涂，之后在一步步调整权重。机器，也是一样， 我们要写一个程序，模拟小孩挨揍学习的过程。&lt;/p&gt;
&lt;p&gt;和人学习的过程不同的是， 我们要从统计的角度， 设计一个自动调节的过程，之前所说的，机器学习模型就是一个框架，我们开始选择的假设（参数）一定是错误的， 而每个数据， 都按照给定的算法一步步的调整模型， 最终得到正确的算法。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4856687898089172&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaah0xvfbyOwrm6c3TsCOH80ctVxjecUhbKRUGlc28sDFIeA9sDHoK4yg/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;628&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们来看如何得到这样一个训练算法， 首先， 你会发现由于我们手里有很多特征， 很容易找到一组参数得到完全准确的价格， 假定你有一个待回归的， 有三个特征， 每个特征都有对应的权重，从无数个特征对应权重的组合中找出对数据拟合最好的那个，就是我们的目标 。你也许会说， 这不就是解n元一次方程组吗， 比如你有三个数据，三组特征， 这就是一个三元一次方程组， 然后只要是符合一定条件， 它是有唯一解的。 但你手里不可能只有三个数据， 假定你有几十个几百个特征， 你的数据通常是这个特征的若干倍， 你只基于某几个数据求得的精确解一定会死的很惨。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;机器学习训练的根本思维方法就是，我要根据所有的数据求解， 而使得所有数据都拟合的解一定不存在，我要做的是，首先量化模型造成的错误， 然后， 让这个错误最小， 这就是优化的思维。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;首先说说量化错误， 记住， 我说的统计的观点， 我们必须针对全体数据衡量这个模型的错误， 这个东西， 通常叫cost function， 你可以理解为由于模型较差引起的花销， 或者是一笔罚款。 在这里， 我们可以把它简单定义为所有数据点预测误差的平方和。&lt;/p&gt;

&lt;p&gt;一个要牢牢记住的观点是cost函数是参数的函数， 而不是特征的函数， 记得刚讲过的我们的预测由特征决定的吗？ 这里， 我们的代价函数确是由参数决定的， 对应每一个参数， 或者每个假设， 它由一个值。 那些比较符号真实的假设，这个值就越低。你要找的那个模型， 就是错误最少的， 就是最接近真实的。 你可以用只有一个参数的情况做个实验，这个时候我们得到一个抛物线， 最优解就是那个初中你就会算的抛物线的谷底。 这， 就是最优点，在这里， 你就走上了人生巅峰。&lt;/p&gt;

&lt;p&gt;寻找这个人生巅峰（谷底）的方法由很多， 这就是数学里的优化理论。 如果你是一个傻子， 你可以用遍历的方法找 ， 也就是把所有情况试一遍， 人生不值得，这是一个很值得的事情。 但是， 如果你要讲点效率， 通常我们会换个思维。 ，实际操作中的方法称为梯度下降， 这个想法的核心是， 我不知道那个人生巅峰在哪里， 但是我知道当下每一步我能够做的最好的事情，就是沿着让我自己的cost代价函数下降最快的方向走一步， 就如同水总要向低留，而不是反过来。具体走多远， 步子不要太大别把巅峰给错过了， 也不要太小走的太慢了， 具体自己调一下。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7326732673267327&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa8dEibPqvJ8yGMa2sbZNNKfQSsLpwVSZ6Er7icIqBSE1BGriasgDB9d1dg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;303&quot; /&gt;&lt;/p&gt;
&lt;p&gt;好了， 假定我们已经在这个人生巅峰， 你觉得是不是万事大吉了呢？ 有没有一种可能， 是你的数据误导了你， 你发现海淀的房价很低， 但是事实上你手里只有一个海淀的数据 ， 而这个房子还是凶宅呢？ 这个闹鬼的屋子导致它的价格异常的低， 结果海淀区这个特征被学到了一个极低的房价， 你还自以为你处在人生巅峰。一旦换了下一个五道口的房子， 你就输惨了。&lt;/p&gt;

&lt;p&gt;你的模型很强大， 它可以抓住海淀这个特征， 但是恰恰数据有问题， 结果， 你被舞蹈， 这个现象， 构成一个机器学习的核心概念， 过拟合。 它所引出的， 是机器学习和简单的函数拟合的根本区别， 机器学习要的不是最好的拟合一条线， 而是举一反三的泛化能力。 &lt;/p&gt;

&lt;p&gt;如果你拟合的很好， 而在新数据面前变成弱鸡， 那就一定不是好模型。过拟合最好的例子是星座， 比如你认为有几个星星， 你就想出这像一个大熊。 事实上它可能还是仙鹤， 还是汽车。 无论哪个， 你的观测都不至于让你推断它真的和任何一个动物神似， 你确偏偏从这极少的数据中推到一个结论。 可以想象的是， 如果这些星星真的是某个动物的一部分， 你从这里得到的模型换一组星星一定错 。如果说机器学习预测是模式识别， 过拟合就是一种走火入魔， 从噪声里读出了模式。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4743935309973046&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaxArQJKsUsmRia566PJAUxBib1r8XuEJHiaG0qr5cMQbv4Q2ficsPyk9jyw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;371&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这也告诉我们数据的重要性， 要不你的数据很干净， 没有那些鬼屋一类的脏数据。要不，就是你的数据很多， 有几个脏数据会被很多的好数据纠正。 当然我们也可以从模型角度入手，让它更聪明的去除噪声。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;二 做分类的几种方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;分类和回归的区别仅在于预测的数据类型不同， 一种预测的目标是如价格， 人的身高， 国家GDP这种连续的值，后一种通常是预测类别，比如男人女人， 有病没病。 但是在数学上这会造成很大的区别， 所以被分入到两个大类里面。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;如果你翻开机器学习的字典， 分类问题独具鳌头， 几乎占领了大部分算法的席位。为什么分类如此重要， 依然类比人类的智能， 我们日常生活中， 大部分都在做是和否的判断，一个东西能不能吃， 一个地方能不能去，  这些大大小小的判断， 都可以看作是分类问题。 因此， 掌握分类， 也成为让机器掌握智能的关键。&lt;/p&gt;

&lt;p&gt;设想一个最简单的问题， 莫过于分辨物体， 一个机器学习特别俗套的问题就是分辨鸢尾花儿， 这种十分美丽的植物有很多品种但都相似，有一个著名的植物学家收集了所有这类花的数据，然后构建了一个数据集， 这个数据集后来被机器学习专家用作了一个经典的试验所， 你像做一个自动的分类器， 就先拿它来试。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.576837416481069&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaZFudKl24IaVcbzt27AqqEDR6P4A9VIF5lgIXvHdpuicmpBjQfjyBYsQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;449&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果让你构建这样一个分类器， 什么是第一要素呢？ 刚刚我们讲了模式隐藏在特征之中， 与你预测相关性越高的那些特征作用就越大。  此处， 构建分类器的第一步也就是抓取关键特征， 当然这一步已经被植物学家完成， 一般来说， 这类花最大的分别度在于花瓣和萼片的长度， 这也正是植物学家要提取的特征。 大家注意这一步是传统机器学习的关键， 在深度学习以前， 它决定了学习的成败， 而特征提取， 也特别需要专业知识。  &lt;/p&gt;

&lt;p&gt;好了， 有了关键特征， 我们如何进一步让机器来辨识花儿？ 想象下人的情况， 你最常用的思维方法是什么？ 做比较！ 我要预测华清嘉园的房子价格， 你最可能做的是看看旁边和它差不多的展春园的价格怎么样， 就可以估测的八九不离十。 &lt;/p&gt;

&lt;p&gt; 那么机器也是一样的 我们完全可以寻找那些和我目前数据最相近的数据，然后由它们确定我的数据的类别。 这个方法， 就是机器学习的KNN-K最近邻法。 当然，我们这里所说的最近， 不是那个地理的最近， 而是特征空间里得最近。说到近， 就要有距离的概念。 我们可以用初中学习的勾股定理计算欧式距离， 也可以用其它非常多的距离计算方法。比如拿两个特征得情况， 这个时候， 我们有一个特征平面， 那个这个距离就和我们所说的平面上两点的距离没有区别。总之， 你要量化这个数据间的差别大小 ， 然后， 就可以通过距离做预测。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.841726618705036&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa6LRO9hja2Gob8X4ehShLNM2BeOGOllY6qA3uicSfFXG18lQGFpgiaZnA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;278&quot; /&gt;&lt;/p&gt;

&lt;p&gt;金融投资在评估企业价值的时候有一个价值比较法。首先， 你要得到一组决定公司估值的特征， 然后， 你要按照这个特征公司放到这个空间里， 最后，寻找与你要估值公司最近的公司经过一系列校准进行估值。&lt;/p&gt;

&lt;p&gt;KNN依然可以用于分类，也可以用于回归。 只要你有足够多的数据点， 而且特征不太多， 就不可以用。 为什么说特征不太多呢？ 因为你我都不擅长思考高维空间的几何。 但是数学家可以证明， 在极高维度的空间里， 随着数据变的稀疏，距离这个概念并不是特别有用， 因为大家的距离其实都差不多。&lt;/p&gt;

&lt;p&gt;这个时候， 建立在距离之上的KNN就没有那么好用了。  但是这样的例子实际上非常之多， 尤其是在自然语言识别上。比如一类经典的问题就是考证一些经典的作者：&lt;/p&gt;

&lt;p&gt;1787年5月，美国各州（当时为13个）代表在费城召开制宪会议；1787年9月，美国的宪法草案被分发到各州进行讨论。一批反对派以“反联邦主义者”为笔名，发表了大量文章对该草案提出批评。宪法起草人之一亚历山大·汉密尔顿着急了，他找到曾任外交国务秘书（即后来的国务卿）的约翰·杰伊，以及纽约市国会议员麦迪逊，一同以普布利乌斯（Publius）的笔名发表文章，向公众解释为什么美国需要一部宪法。他们走笔如飞，通常在一周之内就会发表3-4篇新的评论。1788年，他们所写的85篇文章结集出版，这就是美国历史上著名的《联邦党人文集》。&lt;/p&gt;

&lt;p&gt;《联邦党人文集》出版的时候，汉密尔顿坚持匿名发表，于是，这些文章到底出自谁人之手，成了一桩公案。1810年，汉密尔顿接受了一个政敌的决斗挑战，但出于基督徒的宗教信仰，他决意不向对方开枪。在决斗之前数日，汉密尔顿自知时日不多，他列出了一份《联邦党人文集》的作者名单。1818年，麦迪逊又提出了另一份作者名单。这两份名单并不一致。在85篇文章中，有73篇文章的作者身份较为明确，其余12篇存在争议。&lt;/p&gt;

&lt;p&gt;像这样一个问题， 在没有机器学习的时代， 可以耗费一个考据学家10年20年也不一定能有结果。 但是用机器学习一个叫朴素贝叶斯的方法， 就可以解开。（参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384272&amp;amp;idx=1&amp;amp;sn=ea54b08f3493f367a0ac029cd70d363a&amp;amp;chksm=84f3c791b3844e87d15170cee081f3df8042b8515c7707fde7fcc6160500c540892d80869051&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;贝叶斯推理实用入门&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;我们来看这个方法是怎么样的，首先， 不同的作者写作风格的差异很大体现在用词上， 因此， 我们可以找到一组两个作者都会使用但是和使用频率不同的关键词作为特征。 我们可以想象， 这个特征的数量很大。  然后， 我们可以统计在作者身份明确的稿子里这些词出现的频率，  已经身份明确的稿子的总量有了这个信息后。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4299835255354201&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaiaEM24pvFywdyKwIhQdv7olvWUPTDZicQYdfMLAFYWYzsjuicRCay9EpQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;607&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们就可以展开贝叶斯公式， 直接计算某个稿子是某人的概率， 当然， 这里出现了一组包含很多特征的条件概率， 朴素贝叶斯的核心， 就是假定特征之间是独立的吗， 这样，我们就可以展开成概率连乘的形式， 在这个连乘的形式里，我们看到了我们刚刚计算的那些频率， 如此， 我们就可以直接计算出谋篇未知文章的概率。 神奇的是， 朴素贝叶斯可以对一些从未出现过的特征组合做预测，只要那些特征之前出现过就好。&lt;/p&gt;

&lt;p&gt;关于多个特征下的分类， 事实上最常用的方法是一大类被称为树模型的家族， 而最基本的称为决策树。 它能体现好几个人类理性的基本要素。&lt;/p&gt;

&lt;p&gt;决策树是一个极为典型的模拟人类逻辑的思路，简单的说， 决策树就是分而治之。按照特征把事物分成很多小组，然后对所有的情况分开判断， 如果A符合某条件就是是， 不符合就是否。我在简史课里给大家讲过这个分而治之最大的问题是要列举的可能太多了， 假定你有N个特征， 每个特征两个情况，你就有2的N次方个组合的情况， 要做到天网恢恢疏而不漏， 那得看到什么时候。&lt;/p&gt;

&lt;p&gt;而决策树得核心智慧， 就是优先级算法， 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心，事实上， 我们永远在抓轻重缓急，在抓主要矛盾， 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 &lt;/p&gt;

&lt;p&gt;很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8430034129692833&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaabKGCsJ6hExKoRjMDqiaM4orECfEQkF4qHCD7BIMmAJN3QM3dGiaV92Xw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;586&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么从机器算法的角度看，算法是完全类似的， 你可以把整个问题看成一个树，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 如此，就会形成一个树的结构。&lt;/p&gt;

&lt;p&gt;构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？ 这我们又回到数学物理一个古老而重要的概念 – 信息。 记得我说的信息是特征与预测的相关性吗？  这里， 同样的套路换一个思路， 那就是， 你每增加一个问题，我的预测从模糊变得清晰了吗？ 变清晰了多少？ 如果你是一个人的话，就是你比先前更肯定了多少？ 变得清晰的越多， 变得肯定的越多， 这个问题就是能够给我们带来更多的信息。 能够增加最多信息的特征，就是优先特征。 我们可以做一个思想实验，还是人们用举决策树相亲的例子， 假定你要找一个男生，  告诉你它不会做菜， 这个一般情况下， 对你判定要不要见他影响不大，因为大部分男生都不会做菜 ，而且一般你也都会叫外卖。它在数字上的表现是什么？ 就是来了10个男生，5个被你pass， 5个被你保留， 但是两边的人里做菜的数量和不会做菜的数量都差不多。 &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5601023017902813&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaavkhg8fEGAZehbFMdbbPD7ia4ewO4ImZ2MJP62OyahaY0UnUNhHPEW7A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;782&quot; /&gt;&lt;/p&gt;

&lt;p&gt;用数学的角度看， 这就是信息熵， 你问了这个问题， 信息熵不变，这就是一个很糟糕的问题。而如果你问的是年龄呢？ 年薪呢？ 显然两边的yes or no的比例会非常不同。  因此，年龄或年薪带来的信息增益就更多。 建立决策数的一种经典方法所谓ID3， 就是把描述的这些变成程序了。刚刚说的过程非常容易的称为一个迭代的过程。 迭代的起点是哪个信息量最大的问题也就是父问题， 然后， 根据回答的yes or  no把样本分堆， 每个堆里会出现一个最终预测标签的分布， 根据同样方法选择子问题， 在子问题之下样本继续分堆， 预测标签重新分化， 我的要求就是， 每一次分化， 结果的分布都更纯净， 每个问题回答的yes or no下， 都是同一个标签。 最终达到稳定后过程停止。   &lt;/p&gt;

&lt;p&gt;这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;三 降维与聚类&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚讲过的几种简单的回归与分类方法， 事实上都建立在一个基础上， 就是你的数据必须包含连个部分， 一部分是特征， 一部分是预测对象的值。 这对于数据的要求其实还挺严格的。 但是， 如果我手里拿到一大批数据， 但是这批数据偏偏没有要预测的值呢？ 我们是不是就无能为力了呢？  不是这样的，   还是以刚刚的房价数据为例， 如果我把那些房子的价格都抹掉， 你可以做什么呢？&lt;/p&gt;

&lt;p&gt;事实上，机器学习模型依然可以帮助你发现数据的内在结构。 即使这些结构不能帮助你预测价格， 却可以对你产生重大的启示。&lt;/p&gt;

&lt;p&gt;还记得我们刚刚说过的那个问题， 现在的数据经常具有非常多的特征， 我们称之为高维吗？  但是你要知道， 真实的数据里通常没有那么大的信息量。 比如刚刚的房价数据， 你看到的那个车库的面积其实和房屋的总面积是高度正相关的， 这就说明， 信息里存在大量冗余。 这种冗余信息， 有时候对你分析和理解数据没有好处。&lt;/p&gt;

&lt;p&gt;我们会介绍一个方法叫PCA。什么是PCA？ PCA- 其名称 principle component analysis 就是一种能够从高维度数据里提取信息的方法。PCA让我们假设数据的特征分布符合高斯分布，但是特征不是独立的，而是存在大量线性相关的特征， 线性相关的特征使得数据的表现维度高于真实维度， 这将加深维度灾难，使得训练缓慢。&lt;/p&gt;

&lt;p&gt;你想把这些冗余数据去掉， 你想给老板画出一份漂亮的数据分析报告， 直接告诉它数据里的那些特征是最主要的， 或者你想把一些比较表面的特征化城一个更本质的合成指数，比如房屋综合质量指数？  这时候， 我们可以用PCA。PCA， 其实做的是一个坐标变换， 把线性相关的特征合成一个， 重新合成一组正交的特征。听着有点晕，可以画图看一下。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.420863309352518&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaLHhOCZoFOiaNGUa05L0VgGpI5nPBvSM46cO0JSZXkHJrsib9lhBnzbvw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们知道，要显示两个不同的特征是否线性相关的做法一般是求解其谐方差， 如果我们所有特征的两两相关系数放到一个矩阵里，这个时候得到一个矩阵叫谐方差矩阵， 这已经是对我数据内部的相关性的最好的表示。我们知道，要抽取一组特征里的相关性。&lt;/p&gt;

&lt;p&gt;我们想象，如果把特征看成一个向量空间，我们第一段描述的那个事情，无非是要找到那么一组基底，让原来的数据在新的空间里线性无关，换句话也就是说原来的数据在经过一个线性变换后， 数据可以被投影到一组正交的特征上， 新的特征之上， 每个特征尽量独立。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8561643835616438&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaakzcL5LP9gWrvCg6hGGYH0owHIdvwQkSjRDQeX4y1tGibNAWicAQpiaZGw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;438&quot; /&gt;&lt;/p&gt;
&lt;p&gt; &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;这里我给我们的PCA流程加入一个细节， 就是在求解特征根前不仅要减去均值，还要让每个方向的方差值相等，也就是说， 进入处理之前得到的矩阵对角线是相等的。这个过程叫做归一化，归一化对我后面要说的一件事至关重要。&lt;/p&gt;

&lt;p&gt;我们得到的这个E矩阵是个对角阵，每个值代表的是新特征的方差， 我们把方差从大到小排列，并且重整其特征向量的顺序得到最终的坐标变换矩阵。那么我要问大家，为什么一开始的特征方差是相等的，而后面不同不同了？正是因为存在线性相关的特征，而新的特征进行了特征的重组之后把线性相关的特征进行了重新组合，使得原先均等的方差变得不等了， 你可以理解为如果存在大量变化趋势相同的更向量，使得数据的方差在某个放向加强， 比如说在一个股市大盘里很多股票是正向关联变化的， 这样的方向我们通常称之为主成分。排在头上的特征又被称作第一主成份。 如果是股市，我们可以想象为它是引领股市变化的板块。&lt;/p&gt;

&lt;p&gt;如果我们的数据是很多篇文章，每个文章是一个数据点，特征是一组关键词在文章的频率， 如果我们做这样一个PCA的工作， 就会实现对一些类似文章中常出现的文章被聚集在一个新特征上。新特征就是这样类似单词的组合， 每个新特征， 都代表了一系列文章。 比如政治类很可能被聚成一类， 而其它也如此。   &lt;/p&gt;

&lt;p&gt;PCA的本质，就是寻找数据内的冗余维度， 一言以蔽之， 就是找核心矛盾。 在众多特征里找到最重要的特征。 PCA可以实现对复杂特征的降维处理，大家注意， 这也是我们接触的第一个无监督算法。因为你的数据无需包含你要预测的变量， 比如刚刚说的文章的例子， 你并不需要有文章类别有关的信息，PCA还是可以帮你把你数据库的主成分抓出来。&lt;/p&gt;

&lt;p&gt;因此用PCA你也可以实现压缩， 比如你电脑里的电影，大型游戏，和照片，大多存储称为JPDEG压缩格式， 它们的本质其实也是降维， 把原始数据格式的照片， 用一套新特征，给编码出来， 这套新特征， 比之前的数据要少很多， 但是保持大部分信息。由于保存了大部分信息， 你打开一张Jpeg照片和原始格式照片并不能区分很大。  &lt;/p&gt;

&lt;p&gt;而PCA，则是一个最简单的压缩方法。 在刚说的机器学习方法里，压缩了的数据往往更加好用， 这里面教给我们的， 一个是数据不等同于信息，你的数据量大， 不一定信息就多， 事实上数据的维度表现很高， 而真正的信息其实往往没有那么多维度。 而以PCA为代表的降维工具， 就是让我们实现这种对信息的浓缩， 让我们节省大量计算机硬盘还是我们大脑资源的同时， 得到最难主要的信息。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  当你手里的数据特征太多维度灾难发生的时候，你的电脑根本跑不动，这个方法就十分有用。一类是你缺乏带标注的数据， 这种情况十分常见， 因为数据的标注是最昂贵的， 这种昂贵的情况最好的解决方法就是先用没有标注的数据提取它的信息。 而无监督学习是极好的工具。用PCA对大量无标注信息做训练， 然后得到的新特征用于少量标注数据的有监督学习，会省掉不少麻烦。&lt;/p&gt;

&lt;p&gt;大家注意任何模型的都是都是错的，有些是有用的。 什么时候有用， 取决于我们的假设。 我们对PCA的假设是数据符合高斯分布， 当然，实际中数据可能不符合这种假定， 但并不代表模型不能用， 因为所有模型都是错的， 只是它的有用性会打个折扣。&lt;/p&gt;

&lt;p&gt;降维不仅在机器学习里， 而且在日常生活里是一个重要的思维方法，人脑能够处理的信息量极少，降维，抓住主要矛盾，所谓擒贼先擒王，是人生的制胜法宝。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5613496932515337&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaftS9fTsePiavicX2RYCkt0CtRdLVaNLa4oBI3mq8BjO78UpaXVXM82LQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;978&quot; /&gt;&lt;/p&gt;


&lt;p&gt;最后提一句聚类，所谓聚类算法， 也是在没有预测标签的时候我们可以玩的一个方法， 只不过， 这个时候我们是要根据特征的相似度， 把数据分成一撮一撮的， 说不定， 这样我们就可以发现一个新物种， 或者一个具有特别商业价值的客户群。这里常用的方法K-means也是基于数据点在特征空间上距离的，类似之前提到的KNN。先随机给出K个簇的中心点，再一步步的根据数据调整当前的中心簇的位置，从而使数据能够分开。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384253&amp;amp;idx=1&amp;amp;sn=ae80db08ac2b72167a6ea07d9ec05ac9&amp;amp;chksm=84f3c7fcb3844eeaceee8b6e8744480df7faef7792ae05f0f8745ffbedd5b0a0c99b8deb3da1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;与不确定性作战-从物理模型到特征提取&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;




</description>
<pubDate>Sun, 07 Apr 2019 03:52:44 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/EXOKZtMJtK</dc:identifier>
</item>
</channel>
</rss>