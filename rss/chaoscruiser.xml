<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>机器学习高维数据分析中那些一定可以避开的坑！</title>
<link>http://www.jintiankansha.me/t/pyTQCuYVcR</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/pyTQCuYVcR</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7311258278145696&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;755&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZoycAWPNzQsu5CcQWJjv6zsVdGkD5qicgIo3Aqhspync7IxJx3ISqzr3A/640?&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1258535&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1172402&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section readability=&quot;4.2717815344603&quot;&gt;&lt;section readability=&quot;8.5435630689207&quot;&gt;&lt;p&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;《Nature》11月28日推出的一篇&lt;span class=&quot;&quot;&gt;comments&lt;/span&gt;文章指出了使用高维数据的机器学习中常见的“坑”，以及避免方法，从而帮助该领域的小白能够客观的评价他们结果是否靠谱。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;由于和生物有关的&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247488566&amp;amp;idx=1&amp;amp;sn=5be07345c82c154d0bb69e8266d8c4bd&amp;amp;chksm=e8944ebbdfe3c7ad3da3d3759f83f7d95d89fb0bd417e8c7c9a68ce0897d1503c0a445cd2582&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;组学数据&lt;/span&gt;&lt;/a&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;都是典型的高维数据，例如蛋白组，脂质组，转录组（RNA），基因组（DNA）等，在这个&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247493647&amp;amp;idx=1&amp;amp;sn=4d75f9734f1562bd0d849a45261f2845&amp;amp;chksm=e897b282dfe03b942747ba35bc95b0bf7a1299066f747e86388443e5a8f05fd221a6c37389ab&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;人工智能和医疗结合&lt;/span&gt;&lt;/a&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;愈发深入的年代，基于高维数据的预测模型将会变得越来越重要。因此对于相关从业者，深入了解其中的方法论愈加不可或缺。另外，高维数据不止出现在生物相关的组学数据中，在材料，气象等领域也会有类似的数据集，故这篇“避坑指南”不仅仅适用于与生物相关的数据挖掘中&lt;/span&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;blockquote helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;span&gt;论文题目：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;论文地址：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.nature.com/articles/s41563-018-0241-z&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;



&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;过拟合与维度灾难&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;文章背景&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为了搞清楚那些因素影响我们的身体健康，人们对自身从多个角度进行了观测。随着测序成本的降低，从基因到转录出RNA再到合成蛋白质，积累了越来越多的数据。关于一个人的数据项，没有百万也有数十万，这其中的每一项数据，可以看成数据集的一个维度。而我们关心的是&lt;/span&gt;&lt;span&gt;这些分子层面的数据如何与宏观的表型通过统计模型关联起来&lt;/span&gt;&lt;span&gt;&lt;span&gt;，&lt;/span&gt;例如血压，血糖，尿酸等数据变化与人体的健康状况的联系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;过拟合指的是将表型之间本来是随机的变化错视为统计显著的关联，错误地和某一个维度建立了联系，即假阳性。&lt;/span&gt;&lt;span&gt;假设有一百万维的数据，如果单独来看，那么就需要判定一百万次是否统计相关，而每次独立的判定假设有5%的几率将随机的误差当成是相关性的信号，&lt;/span&gt;&lt;span&gt;那一百万次的判定，不知会导致多少次假阳性。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此需要将P值的显著性判定值进行校正，最严格的就是&lt;/span&gt;&lt;span&gt;用0.05除以数据的维度&lt;/span&gt;&lt;span&gt;。但是由于不同数据间本身具有相关性，很多维度反映了身体相同的调控机制，所以简单地除以数据本身的维度，也会带来假阴性的问题。而且这些维度之间是有相互影响的。这两项原因，使得机器学习的方法逐渐流行起来。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;最近有文献指出，在&lt;/span&gt;&lt;span&gt;已发表的神经科学类论文中，有&lt;/span&gt;&lt;span&gt;50%&lt;/span&gt;&lt;span&gt;的文章统计学方法有疏漏[2]&lt;/span&gt;&lt;span&gt;，在其他的分子生物学领域也是类似的，毕竟该领域的数据暴增是最近十年间才发生的。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;高维度低样本数据的维度诅咒&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;（&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;The curse of dimensionality&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;下图代表我们的数据集，其中有n个sample，比如有500个，但是每个人的基因数据，却可能有几万的维度，即p&amp;gt;&amp;gt;n，最上面的表型是不同颜色代表分类的标签，比如是否患某种疾病。但这里的基因数据是随机生成的高斯噪音。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6525547445255474&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;685&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZoFaj2e4xdEkrckZ9bW9ngdpjiacS69iamIF3PTKOGZ6T1AJs4m0SibWGMg/640?&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;但是当使用SVM分类器，将原始投影在三维平面上时，却可以几乎完美地分开，这就是高维度低样本数据的维度诅咒（The curse of dimensionality）。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;引入惩罚项的常规步骤&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;机器学习中面对过拟合的常见方法，是引入惩罚项，如果模型越来越复杂，就在要优化的损失函数中加上对应的惩罚，这样是不是就能够解决问题了？&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;惩罚项也有很多种类，该加那一类惩罚项这个问题也需要通过数据才能回答。&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;2.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;在理想情况下，有足够多的样本，能够将样本分成三部分，一个训练集，一个测试集，还有一个数据集用来确定模型的复杂度，这三个集合是完全隔离的；&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;先在训练集上使用不同的方法和惩罚项的组合训练一组模型及进行特征选择（选出哪些数据项对预测任务更有效）；&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;之后在橙色的模型选择集合上选择一个最优模型，即图中曲线的最低点；&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;最后在测试集合上判定准确性。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.22927100723427934&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1797&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZooNAES2m0RTopMnZhXIxs6B7l1ckL7YVo9mBX3DL29PqzHeUO05V74g/640?&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;就像在下图中，先在测试数据集上训练了多个模型，之后在橙色的第三幅图中确定三次项的模型是最好的。最后再去测试数据集上看模型的泛化误差。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2932745314222712&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1814&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZoibvBFicW5QoDlOiaXhslicYjnsib0RIeoictyg2h2y0RJuLwg2PrTs11q6TQ/640?&quot;/&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;非理想情况下的数据集分类：交叉验证&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;      &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;可是真实情况下，组学的数据往往本身就不会有那么多样本，不同标签间的比例也不一定均一，所以不能像理想中那样将数据集分成三类。实际上的做法&lt;/span&gt;&lt;span&gt;类似下图的&lt;/span&gt;&lt;span&gt;交叉验证&lt;/span&gt;&lt;span&gt;，将数据分为N份，每次拿其中一份做测试集，剩下的做训练集。之后将每次实验验证集的误差汇总或取平均值，当做模型的泛化误差。由于每一份数据在都有机会被用做了验证集，因此对模型泛化误差的估计是无偏的。而模型选择的过程则是通过在不同模型上进行交叉验证完成的。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3413705583756345&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1576&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZorhwtyXjUVjP1Awzr4nfH65L0TE82ghBs4aSEMCfkaK7ZvMvgwW9xtA/640?&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;极端情况下，n是数据集的样本个数（leave-one-out CV ），即只拿出一个样本来，剩下的都用来做训练集，从而保证模型有足够的样本量。那在做交叉验证的时候，需要注意什么了？&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;交叉验证的目的是为了避免训练出的模型过拟合，因此可以训练一组模型，之后将这些模型给予不同数据维度的权重进行平均及排序，从中根据模型复杂度的惩罚项来选出多少项对预测结果影响最大的特征。或者通过选择不同数量的模型，确定一个最优的惩罚项，之后再用全部的数据来训练这个加上了预估的最优惩罚项的模型[3,4,5]。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果已经进行了特征选择，比如选出了我们关注的表型和这几百个基因最相关，那在做交叉验证的时候需要注意什么？&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;要在全部的数据项上进行交叉验证。使用降维后的数据以及反复的特征选择，会带来偏差项的提高。因此要区分CV用来判定整个模型的泛化能力的交叉验证外层循环以及用来对具体这个模型参数调优的内部循环，从而将模型选择和模型优化分离，从而避免过拟合。&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;未知的混淆因素&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;数据维度不足有什么不良后果？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;数据量不足之外，生物相关的数据还受制于数据本身的维度不足的影响，比如你收集的数据不包括生活习惯，或者做实验时用到的试剂的批次等，但这却会对这个人是否患病有显著的影响，或影响数据本身的分布。这种情况具体是怎样的？&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如下图所示，本来的数据集中一个没有被记录下的特征，将其称为X，如图中灰色和黑色的那一列，而在对应的表型上，这个X变量并不是均匀分布的，这导致在交叉验证时，不论在测试集还是验证集上，黑色对应的哪一列用肉眼就能看出其很特殊，这导致模型学到的&lt;/span&gt;&lt;span&gt;其实不是判定一个样本是红色还是蓝色这个预设的目标，反而“偷懒”去判定样本是灰色还是黑色&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这导致的结果是不管训练集本身的交叉验证还是测试集，其结果都不差，但到了完全不同的一份独立数据集上，模型的表现就差得和随机乱猜差不多了。见右下方的ROC曲线，ROC 接近0.5，就意味着模型完全没有预测效力。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6510948905109489&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;685&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZot2aFjKX7df7owpnibVcvIGWM77ibfy70hfIzGr0Ga1LTFDxOfe6FIHDA/640?&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;     &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此当前判定一个模型是否靠谱的金标准，都是将你模型在其他实验室使用相同的实验方法对相近或相同样本观测得出的独立数据集上跑一下。&lt;/span&gt;&lt;span&gt;通过一个独立的验证集，能够看到模型本身是不是受到未知的干扰因素的影响&lt;/span&gt;&lt;span&gt;。那如果发现了有未知因素的影响，又应该怎么办呢？&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;对于未知混淆因素&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;有些好排除，例如人种，年龄之间的差异，可以通过统计上的校正解决，对于年龄，由于年老对身体的影响不一定是线性而可能是指数的，因此还会将年龄的平方项和立方项作为控制因素[6]。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;但更多的混合因素则是未知的，例如那一天做的实验，用的那一台机器等。虽然目前已有相关的统计方法来解决这一问题，但这些模型都假设未知的因素满足相应的分布，但现实中却往往不是这样的，这导致即使校正后也有残余的未知因素影响。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;当你的模型在独立数据集上无法重复好结果的时候，也不应该当成是你的模型的末日审判，而应该去找可能的原因和解释。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;比如用于验证的独立数据测量的数据项不如原始的用于交叉验证的数据项丰富，或者用来验证的数据来自一个不同的人种，从而使得你的模型不适用。尤其在医疗领域，为了保护隐私及伦理要求，并不是所有的数据都是公开的。这使得评价用于验证的独立数据集是否恰当变得很困难。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这个问题对于那些方法类的“虐前任”型创新尤其严重。很多文章宣称自己通过整合很多组学数据，提出了一种比之前所有模型更准确的预测模型。但由于有残留的未知因素，当你引入新的数据项的时候，模型不做改进，就有可能效果比前人的好，这并不代表着你做出了方法学上的改进与创新。因此现在严谨的做法是&lt;/span&gt;&lt;span&gt;如果你要证明你的方法有所长进，至少需要在五到六份的独立数据集上展示你的方法比前人的有显著提升。&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section data-mid=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;非监督学习中也有过拟合&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在非监督学习中，也会也出现过拟合的现象呢？&lt;/span&gt;&lt;span&gt;例如在对特征进行聚类时，即使每一个数据项都和待研究的表型统计相关性并不显著，但它们之间的相关性却会使它们聚在一起，从而导致数据项聚成簇，但这样的结果却不能用于指导特征选择，否则就会导致过拟合。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;非监督学习的另一个常见应用是数据降维，这中间也会也导致过拟合。&lt;/span&gt;&lt;span&gt;这篇论文中举出了一个具体的例子，如下图。图中的红色和绿色是两个样本在不同基因区域上的对应特征，红色的是有病的，绿色的是没病的。在图a中，使用数据的均值作为降维后的特征，会导致对数据分类时效果变差，但使用数据的方差则不会。而在b图中，由于数据本身的分布不同，导致相反的结果，使用方差会导致分类时效果变差。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4399776661083194&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1791&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZolN45063qwoicaPIa5Pj09jbC7cdia6h07YBSOBYMOVG2hfd1I82oGOvg/640?&quot;/&gt;&lt;span&gt;     &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;虽然取方差和取均值不是现实中用到的数据降维的方式，但上面的例子展示了如果只根据少量数据选择的降维模型不适合新的数据，即导致过拟合。这方面有一个工具可以用来评估，可以进一步学习。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;       &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.3487762237762238&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1144&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZogaWKRXC63s4ibqHlz8SiaXTAC89y1v4FjHE3RLF3ynSjFVzkjSNE2KicA/640?&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;该方法借鉴了交叉验证，用来判定非监督模型是否过拟合。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;论文题目：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;MOVIE: Multi-Omics VIsualization of Estimated contributions&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;论文地址：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.biorxiv.org/content/early/2018/07/29/379115&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;     &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;例如下面的图片中，哪一个过拟合了？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;      &lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0685805422647527&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;627&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZoSmdMFM7JuicnJr0NZkQs6qJZ1QdU1ic3Okgwodf3xd6AtoqzzbtWoAicQ/640?&quot;/&gt;&lt;span&gt;   &lt;/span&gt;&lt;span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;小结&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这篇文章总结了高维数据的三个常见问题，一是没有用好交叉验证，导致预测模型过拟合。二是忽略了未知的干扰因素，导致模型在独立数据集上表现糟糕，三是在非监督学习中忽略了过拟合，导致特征选择时丢失关键信息，从而影响预测模型的效果。针对这三个问题，作者给出了当前行业内共识的常见解决建议，虽然这些问题都没有完全解决，但避免前人踩过的坑，是必不可少的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;10.5&quot;&gt;&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;Nieuwenhuis, S., Forstmann, B. U. &amp;amp; Wagenmakers, E. J. Nat. Neurosci. 14, 1105–1107 (2011)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;Simon, R., Radmacher, M. D., Dobbin, K. &amp;amp; McShane, L. M. J. Natl Cancer Inst. 95, 14–18 (2003)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;Varma, S. &amp;amp; Simon, R. BMC Bioinform. 7, 91 (2006).&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;Teschendorf, A. E. et al. Genome Biol. 7, R101 (2006)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;https://permalinks.23andme.com/pdf/23-12_predictivemodel_methodology_02oct2015.pdf&lt;/span&gt;&lt;/p&gt;

&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;
&lt;p&gt;&lt;span&gt;作者：郭瑞东&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;编辑：王怡蔺&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;
&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;推荐阅读&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247490693&amp;amp;idx=1&amp;amp;sn=171c4800ce8d7eb72e7b7e24d4fc6806&amp;amp;chksm=e8944608dfe3cf1eed89347b3227a7125887ab97132a31a22bc89b63844a0065da69de7e09de&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;从拓扑数据分析到压缩感知&lt;/a&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247488037&amp;amp;idx=1&amp;amp;sn=802627d033d5f724930938eff55df8f6&amp;amp;chksm=e89448a8dfe3c1be6babd37a031f8b265052a13bce6cfea294008d1980a01711ab5d29711670&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;如何捕捉&lt;/a&gt;&lt;strong&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247488037&amp;amp;idx=1&amp;amp;sn=802627d033d5f724930938eff55df8f6&amp;amp;chksm=e89448a8dfe3c1be6babd37a031f8b265052a13bce6cfea294008d1980a01711ab5d29711670&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;关系数据结构？&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247490249&amp;amp;idx=1&amp;amp;sn=8f494f8da307913910f51523ea218482&amp;amp;chksm=e8944044dfe3c952c975ebfa26c36c827bb20d957ec693a928c929bf9dcfe1855cf8aa631adf&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;混乱中的秩序 | Kolmogorov复杂度&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247491991&amp;amp;idx=1&amp;amp;sn=b866da720abca815123eb2d6024b6cde&amp;amp;chksm=e897bb1adfe0320ca533cb6bca860148a61d538fd77122117001a830163b2f4399db3eaaea70&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;大数据知道你更想和谁约会&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247487778&amp;amp;idx=1&amp;amp;sn=c2e77ec93213c4c63f57a777ff10e368&amp;amp;chksm=e8944bafdfe3c2b9d66544dafe7403159473e8c94fd3bc513f5300c353bcec49c0c0b69797af&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;加入集智，一起复杂！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;title&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;
&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;推荐课程&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWLer16ibicMVplicymdJ2rLpaOIcZa3TWmowp7tWE4HematypZyuO7eMQLJFA4M82bf62EE3DwptH1iaw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;1280&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;580.3584229390681&quot; data-ratio=&quot;0.453125&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWINE7dHffUN1wQZG6icOIpZo217bHkjLXud50FYVCNwq6o7h6nG4erXgLbS6YQpboIysbjhc61xEgQ/640?&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1398939&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;PC观看地址：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://campus.swarma.org/gpac=10406&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;
</description>
<pubDate>Tue, 04 Dec 2018 09:43:00 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/pyTQCuYVcR</dc:identifier>
</item>
<item>
<title>[原创]用算法在图像中画重点-浅谈物体识别</title>
<link>http://www.jintiankansha.me/t/rUFL39iTvc</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/rUFL39iTvc</guid>
<description>&lt;p&gt;物体检测是计算机视觉中的最常见应用之一，有极为广泛的用途。例如识别体育影像，确定哪个是运动员那个是篮球。或者你在找在桌上丢失钥匙时，就是在用人眼做物体识别，这时如果能通过机器来帮你完成，该多好啊。又比如交警通过物体识别，能够判定视频中的哪些车辆违纪。&lt;/p&gt;

&lt;p&gt;物体识别这个问题经过近三十年的发展，从深度学习之前的人工特征提取，到之后逐步使用卷积神经网络进行特征提取和分类以及候选框的确定，直到端对端的模型使用一个网络完成所有任务，从而做到了更快的速度，更低的资源消耗，最终达到了实时的物体检测。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6767241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWSVZnsefdmrnX2AQmNq1AcHFLcdqbPwRW8ZQIWKd7w0w7xnLAkylrwg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;232&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不同与图像分类任务，物体识别要逐层的在图像中画出一个个框框，比如先识别出这是一只鸡，再识别出鸡的脑袋，翅膀等。之后对框中的图像进行分类，框框中的图像要尽可能的完整的包含待识别的物体。（如上图所示）&lt;/p&gt;

&lt;p&gt;物体识别最简单粗暴的做法是在图片上滑动切割大大小小的框框，然后对每个框中的结果进行分类，从中选择那些分类可信度高的，但这样实在太慢了。在深度学习席卷计算机图像学之后，出现了一个系列的RCNN，包括基础板，加速板，升级加速版，其中的R代表regional。这里对其逐个进行详细介绍，从其进化中试图总结深度学习算法改进的一般道理。RCNN家族虽然已不是物体识别领域最新最好的方法，但其思路仍值得借鉴。&lt;/p&gt;

&lt;p&gt;ps. 对卷积神经网络不熟悉的，可以看看&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt; 复习下。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5930555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd7XTEuknZ8Iamwwa4E4a4N4tc1kjibEibfyKyDVVFpXM7xTboH4tPa3F9icp13wV9TBjVE6sk8NYsibg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;先说RCNN，这个最简单，就算将手动的特征提取换成了由卷积神经网络去做，其他的和传统模型类似，也是先生成所有可能的框，之后对每个大小形状不同的子图缩放到同样的尺寸，最后对每个框由SVM来判定是不是待识别的物体。但就是这个创新，在不改变分类器的情况下，就能够比之前最好的模型提升任务的准确度50%，下图是13年RCNN文章中给出的结果，其中红色的31.4%的准确率相比之前最好的22.6%进步很大。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/6iajibNtiaBKUzFOP3waCd0ic5WA2BRC8uCd3dage4vwic2zbqjcyMDBpS01nHEozfnj7GvW9qKp0SwJOY0EyD4ur4A/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.7912087912087912&quot; data-w=&quot;637&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从这里看出深度学习的最大优点，就是其能够比之前的方法更好的提取图像的特征，这是卷积神经网络的结构决定的，局部的信息提取以及通过pooling来实现信息的汇总。然而RCNN的分类器依旧是线性的二分类SVM，因此分类的效果不好，而且RCNN要暴力的从一幅图中生成2000张大小不同的被裁剪的图片，再根据要识别的物体种类对每一幅子图片通过SVM进行01分类，判定这幅图片是每一个待识别物体的概念，因此运行时速度很慢。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;RCNN之所以慢，是由于对于2000个候选框中每一个，都要通过卷积层来提取特征，这注定会有很多重复的计算，如果能避免这其中的重复，就可以对算法显著的加速。具体来说，利用了CNN中的池化操作，先对整张图片过卷积层进行信息提取，之后将每一个候选框内的特征进行池化操作。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5915317559153176&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd7XTEuknZ8Iamwwa4E4a4NEk51KPKdSR9MlPx4Kxic30HZicl3HqiabIWLiaATBbrickHBHibAssrgjM6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;803&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里的池化操作，为了考虑到不同颗粒图像带有的不同信息，也是层次化进行的，具体是想将一幅图中的全部特征中选出一个，再将这幅图等分成4个小正方形，从每一个中得出一个特征，最后再将每个子图再切分，最后从16个小图中每个得到一个特征。如此在池化层对不同的候选框中的信息进行汇总，达到提高效率的目的，使RCNN时一张图40-50秒的判别时间变到了2秒&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/6iajibNtiaBKUzFOP3waCd0ic5WA2BRC8uCd96hn6aOWVsqqNV4HTFwtLoYzoE1DmQ1P3eObXgcRibIJwFWpppIMMSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.37703016241299303&quot; data-w=&quot;862&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;但是fast RCNN仍然有候选框的选择这一步，而这种机械化的操作将花费很多时间，faster RCNN对此进行了改进，通过引入神经网络替代了候选框筛选（selective search）。将一整张图片通过卷积神经网络提取特征，之后通过Region proposal network对整张图中确定哪一部分对应着相对应的区域。之后通过卷积层将不同大小形状的子图的特征变为相同长度。最后再通过softmax函数进行分类，使用线性回归去微调筛选框的具体位置。&lt;/p&gt;

&lt;p&gt;Region proposal network通过在一张图片中按照给定的形状，设置k个锚点（anchor box），从而针对每个区域的子图，先预判待识别的物体在这个框中的可能性，在通过训练更新每个框的可能性的估计，从而之关注哪些最有可能性的位置。每个框一开始并没有标注待识别的是什么物体，这里优化的只是物体本身是不是在这个框中的概率，之后的网络将负责分类这个框里究竟是什么。相比于Fast RCNN的2秒，Faster RCNN的识别速度达到了0.2秒。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5398230088495575&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd9HzPsnZh98MmnamCQFayC1wDxbsHW1Nmjz0jXB9Zicbs7DOX7N3qCaAgichLE3opZIibplhf2rhlZg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;339&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Faster RCNN也有自身的问题，比如一幅图像仍然要经过多个筛选框的反复处理。另外Faster RCNN在特征提取的时候采取了训练好的成熟模型，例如Inception或Residual net等，如果待识别的物体和这些网络本身训练的东西类似还好，如果不是，那模型的特征提取就会效果不佳。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结一下。由于物体识别在商业上的广泛应用，其发展很快。RCNN家族已经早已不是最新的模型，YOLO和RetinaNet等端对端的模型通过使用一个网络完成全部任务，实现了实时的物体识别，能够在视频中使用。但RCNN的发展过程，就是神经网络一步步替代掉传统模型的过程。从最初的只替代特征提取，到用softmax多元分类替代多个二元分类，再到使用Region proposal network替代机械化的筛选框穷举。模型运行时间上取得了数量级上的进步。&lt;/p&gt;

&lt;p&gt;要想做到数量级层面的性能优化，一定要来源于一个突破性的创新。从RCNN的发展过程中，我们可以学到将具体任务拆解的方法，以及如何使用学习而不是穷举的方式来减少计算量。智能的标志就是能用最少的计算量达到相近的信息提取能力，而端对端的网络通过将一切步骤整合在一起，从而进一步提升了性能。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383465&amp;amp;idx=1&amp;amp;sn=e579b06baa00207e66f8668a0e161a23&amp;amp;chksm=84f3c8e8b38441fe71ff5765963224016e511aff653e4e5e04b61ce1c679cec5a520030764e7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;放养的深度学习-浅谈自编码器&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Sun, 02 Dec 2018 19:43:31 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/rUFL39iTvc</dc:identifier>
</item>
<item>
<title>[原创]认识基因的复杂性-读《潘多拉的种子》</title>
<link>http://www.jintiankansha.me/t/GI8RGsWa3S</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/GI8RGsWa3S</guid>
<description>&lt;p&gt;这也是一篇17年6月旧文。因为最近的热点新闻，翻出来。借着这本书，说说人类基因的复杂性。对人的基因进行编辑，虽然我是反对的，但也意识到，一旦有了工具，哪怕不成熟，有风险，也如同伊甸园的苹果，越禁忌越诱惑，是迟早会被摘下的。读过《自私的基因》的朋友，可以将这一发明看成是某些基因延续其自身的一种手段。对技术进行监管，只能延缓而不能阻止人们编辑自身的基因。&lt;strong&gt;与其一味的咒骂，不如加速技术民主化的进程。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比如所有讨论这件事的人，首先要自己&lt;strong&gt;做好科普&lt;/strong&gt;，比如了解什么是脱靶，什么是嵌合体，那个被修改的CCR5基因又是做什么的？除了这些和这件事有关的，还要了解更大的背景，比如基因是什么？有什么影响？为何会是现在这样？其中有怎样的相互作用？其次要&lt;strong&gt;限定讨论的范围&lt;/strong&gt;，比如不要在科学的讨论中加入宗教词汇；不要将个人的贪婪和野心当成了一个群体的，不要将道德的评判加到无辜的孩子以及孩子家长身上。最后还要&lt;strong&gt;避免因噎废食&lt;/strong&gt;，避免因为这件不幸的事件让万千正在进行中的合规科研躺枪，不要认为每一个学生物的童鞋都是弗兰肯斯坦，不要忘记如同任何技术，基因编辑本身是中立的，这项技术可以治病，可以拓展人类的知识边界。&lt;/p&gt;

&lt;p&gt;最后推荐四本基因相关的书：前俩本没有中文版&lt;/p&gt;
&lt;p&gt;1）&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382424&amp;amp;idx=1&amp;amp;sn=7c21a08d694b098e08822f38fb2b5ce0&amp;amp;chksm=84f3ccd9b38445cf18fd9c0f449396f9bc446444df4b89f6ae20e2744c3b4e1678f9073acc9c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;远离基因决定论，正视基因检测-读《DNA is not your destiny》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2）&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382790&amp;amp;idx=1&amp;amp;sn=7158c6d6c32dce96e2aacc394f19ee37&amp;amp;chksm=84f3ca47b3844351da6cb4fedfde5cdc30a1c0918d3ae260d070c49e5873a29680f4723e6127&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;新书速读：关于基因魔剪的伦理问题&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3）基因社会 （介绍基因基因之间是如何合作与对立的）&lt;/p&gt;
&lt;p&gt;4）第四本就是今天介绍的《潘多拉的种子》&lt;/p&gt;



&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设你太爷爷是个成功的商人，现在你从你太爷爷的太爷爷手里拿到了他当年经商的秘籍，然后你打算按照这个秘籍的指导开始创业，从此走上人生的巅峰。但等你拿到了秘籍一看，太爷爷的时代，和当下的魔幻现实，完全不是一个样，你傻眼了。&lt;/p&gt;

&lt;p&gt;这样的事情，其实发生在现代的每个人身上。我们每个人的DNA，都是我们游牧时代超级成功的祖先留给我们的生存秘籍。只是这些秘籍已经不适合我们当下的环境了，这才造成了诸多问题。在今天介绍的书《潘多拉的种子-人类文明的种种代价》中，我们将看到&quot;基因地理计划”（Genographic Project）的负责人斯宾塞·韦尔斯是如何系统化的去论述这个话题的。读完这本书，你会明白为什么《未来简史》中描述的反直觉的事实，即当今“死于超重的多于死于营养不良的”，以及在发达国家“死于自杀的多于死于谋杀的”。&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;1.4215686274509804&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccSVIIbASdUWupNA2ZbZXGfFiaWHOfbqSmNdIAGGBiasJLZFeiafsCAPC5iaibdNRyx5qR96S6SicGZA01w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;306&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这本书的作者从事遗传学及人类迁徙方面的研究，在讲座之后，他常常会被问到这样的问题：你的研究工作有什么更广泛的价值与意义？这个问题是作者试图用这一本书的篇幅去回答的问题。&lt;/p&gt;

&lt;p&gt;要想回答好这个问题，需要做的不是去直接的去解释研究人类迁徙过程中留下的遗传密码有什么用，例如向这个充满纷争的世界呈现在DNA的层次，不同种族之间差异有多么微不足道；基本上人类是完全相同的。或者诉诸好奇心，举出如果我们碰上了某个来自地外行星的智慧生命，那么在这种千载难逢的机会里，我们要跟对方谈的究竟是最新的电视游戏如何操作的单调细节呢？还是专注于我们这两个高度演化的物种是如何变成现今这个模样的问题？&lt;/p&gt;

&lt;p&gt;上述的两种回答方式，都是线性的去考虑问题，好的答案，要将提问者引到自己的主场，这需要引出一个异常现象，最好以问题的形式引出。这个问题要相关，要影响广泛。例如随着惊人科技进步而来的，慢性疾病在西化社会中史无前例的大幅上升，在印度与中国这样的发展中国家，心脏病、糖尿病以及单纯的肥胖病例也不断增加，像抑郁与焦虑等心理疾病也逐渐增多。是否在西方文化与我们的生物组成之间有某种致命性的不搭调，而让我们生病？如果说这种搭配不当确实存在，那我们目前的文化当初又如何取得了优势？&lt;/p&gt;

&lt;p&gt;回答这个问题，需要用到DNA中留下的痕迹，这涉及到一个堪比人类基因组计划的大研究课题，人类单倍型计划。将DNA上的每一个碱基想象成一颗珠子，突变会给一个物种的基因池里引进了额外的变异，这些改变随着时间而逐渐累积，使得大约每隔一千个珠粒就会出现一个变异。而父母在给子女的那一条染色体，则会因为重组，即染色体被打断再重新洗牌，从而产生如下图一样的多种组合。&lt;/p&gt;

&lt;p&gt;&lt;img align=&quot;bottom&quot; border=&quot;0&quot; data-ratio=&quot;1.998003992015968&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf1qg0qqxx5aaaiat1o1XPA139aT3zvicbFiamMoM3T69UEOtDMx4m7bXhuajL9sNaHibSozyymW5xY7Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;501&quot; height=&quot;1001&quot; width=&quot;501&quot; /&gt;&lt;/p&gt;

&lt;p&gt;经由检视非洲人、欧洲人以及亚洲人的基因，可以估算这些珠粒在不同的人类族群当中重组的情况，人类单倍型计划因此能推算出珠粒没有遭到更动的珠链平均长度为何。这个长度与该族群存在的时间、随时间而改变的平均族群大小，以及其他有助于决定珠链上何处出现重组的因子相关。随着时间过去以及经过许多世代的重组，每个族群的珠链结构都产生了某种“识别标志”，也就是可用来区分不同族群的珠链排列形态；那是因为住在同一区域的人要比住在地球其他区域的人，有更多共同的祖先。&lt;/p&gt;

&lt;p&gt;然而为什么不同种族的基因会有所区别，其原因在于不同的环境。比如欧洲人的祖先养牛羊，于是90%的欧洲人都有消化乳糖的基因，而90%的亚洲和非洲人，在成年之后，则会逐渐消失吸收乳糖的能力，或多或少的患上乳糖不耐。基因上的差别，和考古发现的研究对的上，这显示了跨学科研究的魅力。&lt;/p&gt;

&lt;p&gt;农业革命显著的改变了人类的生活方式，读过《人类简史》这本书的人都不会陌生。而农业也会在人类的基因组中留下印记。科学家发现，那些人类为了适应农业而改变的基因，其中多半和当下的复杂疾病，例如高血压，二型糖尿病有关。当年使人体适应谷物的基因，如今正在使我们生病。在食物不充裕的时候，进化偏爱那些“今朝有酒今朝醉”的暴饮暴食者，而如今吃的太多则对健康没有什么好处。进化训练了我们偏好甜的东西，而如今工业化的食品生产则使得自己舌头上味蕾的成为自己的主人。&lt;/p&gt;

&lt;p&gt;人类的心智也被农业以及之后的精细分工改变了。人类不是脑容量最大的，尼安德特人有比人类更大的脑容量。但人类成功的秘诀在于会讲故事，会想象未来，即表现在创造艺术上。&lt;/p&gt;

&lt;p&gt;创新是复杂的过程，但究其根本，则包括想出解决问题的新方法并加以执行。第一步，是需要类似反映在艺术创作的想象力，就像我们在古人类聚居的岩洞人看到的那些画上说展示的。其次，是需要向其他人解释其创新的方法。想象出全新可能性的过程，可由文化交流而加速，一如早期农人将山区各地演化出来的不同品种小麦、稻米及玉米予以杂交，以制造出他们想要的特征。这种尝试错误的过程（通常用上看似疯狂的突发想法），加上更好的沟通，就成了人类创新的模式，也是人类头一回演化出成功解决问题的系统。上旧石器时代人类行为的改变，就只能以这两种能力的合作无间得到解释。&lt;/p&gt;

&lt;p&gt;现代的狩猎采集族提供了这种行为绝佳的研究案例。一天结束后，每个人都围坐在火边，说故事、嬉笑，以及讨论当天发生的事。有些故事成了族群神话的一部分（像是叙述一回特别成功的捕猎行动），有的则是用来测试及淬炼新想法。这有点像某种“创新智库”，其中成员在讨论、分析以及决定关于他们生活的口述报告时，进行的是思维实验。这种口语叙述的精炼，与人脑将短期记忆转变成长期记忆的内在细胞过程，并无不同；后者是经由故事重述来强化神经联结，好让它说出我们希望听到的故事版本。&lt;/p&gt;

&lt;p&gt;事实上经由演化，现代人变成了能够产生并淬炼想法的社会性动物；这一点或许能够解释管理学上的发现：目标集中的小团体似乎是运作最良好的，因为这样的环境可以让这种古老狩猎采集族的做法发挥出来。人类演化出这种快速创新的模式，有助于解释为什么现代人出现在欧洲的那一刻，尼安德塔人就注定灭亡了。让人类善于创新的改变，也让人变得更好奇，还可能创造出一种喜欢到处游荡的癖好，以及让人类迅速因应新的状况，而改变自身的文化。&lt;/p&gt;

&lt;p&gt;但到了现代，一度在旧石器时代无穷大的疆域任意驰骋的自由心智，如今无论在广度及深度都受到了限制。根据人类学家撒林斯（Marshall Sahlins）的说法，人类从生活在“原始富饶社会”的狩猎采集族，拥有自由与时间从事看似无用的活动，变成了一群面对一堆工作时限的工蜂。人开始跟机器变得密不可分，一辈子都做着重复的工作；虽说这种方式能够制造大量标准化且价廉的产品，但却有效地剥夺了一般工人的个体性与创造力。而这正是当下越来越多的抑郁的深层原因。那些曾经带给人类优势的基因，再一次成为了当下的负担。&lt;/p&gt;

&lt;p&gt;类似的故事还体现在我们面对气候问题上，进化没有想到我们要去面对全球变暖这样一个长期的全球性问题。我们的心智还没有准备好，问题已迫在眉睫。&lt;/p&gt;

&lt;p&gt;赫胥黎（Aldous Huxley）说过：“所有的神都是人造的，是我们拉着线让它们产生动作，同时，我们也给了它们力量来拉动我们身上的线。” 。在众声喧哗、令人头晕目眩的现代世界里，渴望找到一方小天地，并不是新鲜事，这是人类一向在做的事，就算是在最现代的所在。不管是社交网络上的亚文化，还是沉迷于游戏的青年，亦或是更危险的伊斯兰原教旨主义。都是在追求一片只属于自己的空间。而这是写在我们基因中最深的渴望，一种对归属感和意义的不可抑制的渴求，这些让我们祖先成功的基因，如今正在使人类疏离。&lt;/p&gt;

&lt;p&gt;以上种种，不应让我们感到意外。&lt;strong&gt;有益或是有害，本来就相对于环境所决定。环境变了，策略就要变。&lt;/strong&gt;人不是基因的牵线木偶。苏格兰哲学家休谟曾指出：“实然”并不等于“应然”；你“能够”做某件事，并不代表你“应该”去做。然而今日情况正好相反：如果我们“能够”做某件事，似乎就提供了去做的“正当理由”。我们学会做更多的事，也就要求更多；这是个恶性循环，由于人类误以为资源是无限的，而使得该循环不断进行。这个过程从六万年前人类这个物种离开非洲老家、向全球扩张时就开始了，到了新石器时代革命过后，速度则突然加快。我们已然看到，随农业而来的是制造更大问题的能力，这种能力是狩猎采集族从来无法想象的；在背后推动的力量，绝大部分来自贪婪。贪婪，这种曾经使人类从非洲大陆到踏上月球的心智模式，正在成为当下最大的负担。&lt;/p&gt;

&lt;p&gt;回到这本书的副标题“人类文明的种种代价”，总结起来，一共有五种：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1    农业带来的基因上的改变使得当今人类容易超重，患上种种慢性病&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2    分工使得人类无法自由的探索，造成当今人面对的种种心理问题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3    缺乏长远计划和想象力，从而在气候问题上不作为&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4    对归属感的追求以及信息的过载，使得人群隔离成诸多相互分离的小群体&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5    贪婪，永远不停下的消费主义，使得人类消耗了过多的资源&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在这本书的结尾，作者给出了很有诗意的一个建议。这里摘录出来：&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;2003年1月22日，美国航天总署（NASA）接到了来自先锋十号宇宙飞船传回的最后一则讯息。先锋十号于1972年发射，是人类制造的头一个试图离开太阳系的对象。先锋十号的外壳上，贴有一块带给其他智慧生物讯息的牌子。上头除了有地球在银河系当中位置的说明外，还画有一男一女：两人都赤身露体，男子并抬起右手做出友善的欢迎姿态。关键是，牌子上并无文字，没有提到国家或政治人物，也没有宗教或物质财富的描述。作为我们这个物种对地外生物的头一个“招呼”，我们选择了强调自己的生物本质。&lt;/p&gt;

&lt;p&gt;在人类面临历史关键的此刻，必然会面对由生物与文化的不兼容所造成的问题，我们已经拥有部分解决的工具：&lt;strong&gt;想要拯救自己，需要人类要接受而非压抑人的天性；需要重新评估人类文化对扩张、占有以及完美化的重视；&lt;/strong&gt;需要向与人类过去的生活方式有所联系的族群学习，基本上人类在整个演化史上都是以那种方式生活。这么做，或许能让人类撑过接下来的两百万年。&lt;br /&gt;&lt;/p&gt;



&lt;p&gt;扩展阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382537&amp;amp;idx=1&amp;amp;sn=865911f3691e5b6ed7b65a880e7f93c3&amp;amp;chksm=84f3cd48b384445effd6e1e6158b3a867b6d6ca0ba488356fd7f73fe96e5f2b513595bbec682&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;自私的基因 读书笔记&lt;/a&gt;&lt;/p&gt;










</description>
<pubDate>Thu, 29 Nov 2018 06:32:22 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/GI8RGsWa3S</dc:identifier>
</item>
<item>
<title>[原创]机器学习的本质： 理解泛化的新观点</title>
<link>http://www.jintiankansha.me/t/dkkqYSYgi3</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/dkkqYSYgi3</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;人工智能的主题是学习， 从简单的机器学习到深度学习， 我们始终在头疼的一个事情就是过拟合。 对于过拟合， 我们有很多说法， 过拟合对应的是机器死记硬背， 没有能够举一反三的情况。 关于什么是泛化能力， 我们管它叫机器在新数据面前的预测水平。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.25277777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWbrfH9SBticH79gXibnay0dEzkMoMbau2Ct1Mq01mZycycg5ebVGwhLJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1280&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWFVb6UE9vk27UYjUINHKicY2sQ4kHhdVhXnzgkwHqer5ktuGzIICcsfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;923&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用一个简单的方法理解过拟合，如果你手中的数据有限，比如说星空里观测到的三个星星，　你可以想象出任何一个复杂的图形穿过那三个点，也是你的想象力丰富多彩，你就可以做出越多这样的图形。事实上，我们知道这样的想象不具备任何预测能力，如果天空中出现第四颗星，我们一定就不能确定它是否在该在的位置上。　&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWwbsAEtP3LYFQCFAlPyA5EyzgokiaNiaxnUEgPrgZHricKtWlJb8hmDIfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;400&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;过拟合的反面， 就是泛化， 应该说，它就是学习的本质。 否则， 整个机器学习就是一门拟合而已， 深度学习就是比较复杂的拟合。学习的最高境界，是在纷繁的现象里总结出简单的定理，比如看到大量物体运动的轨迹，总结出牛顿定律: F=ma . 但是它的预测能力几乎是无限的。学习，　本来就是在总结规律，　而不是复制数据。　　　　　　　　&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而翻开机器学习的典籍，关于泛化和过拟合的理解，却非常零碎，表面化。首先，我们回顾，我们看来自不同学派的不同观点和做法：&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 贝叶斯学派的最简单模型： 学习是从先验到后验概率的转化， 一个好的学习算法， 最重要的事情是用最少的假设，得到对数据出现最大概率的解释 ，每多一个假设 ，整个观测成立的概率就乘以一个小于1的因子， 假设解释理论的概率迅速减少。 这个理念翻译为白话就是“如无必须， 勿添实体”。 来自贝叶斯的一个直接方法论是引入最大后验概率和正则化的概念， 贝叶斯派的核心观点是我们需要简单的模型。 模型的参数直接体现复杂度， 因此贝叶斯观点下参数多而数据少， 就是过拟合的直接原因。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.549718574108818&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWVS7ibj6QA3wtUnWdNzo5Mvyb23iahGtIgPkb7I70eoPkG0cshqUx3vQA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;533&quot; width=&quot;533&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.43194444444444446&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWYL2bcflcOecu6ZAQ0uPoVCrwFrgqdLCOOWEiblNH6xeWj59QPenIQcA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1045&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 几何派机器学习的风险最小化： 泛化误差的来源是数据的量不够大， 新的没有见过的数据代表着看不见的额风险， 因此我们要泛化风险最小化， 如果是一个分类问题， 意味着你要找一个分类界限，叫已知的数据点离分类边界足够远， 这个想法， 就是maximum margin solution 支持向量机的源泉。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.7335243553008596&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWrZ1lIuR1OA3eicZcEibrWgdfDzyeg7SCnuIzd8LNKnAk4fQfWGia1uW4g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;349&quot; width=&quot;349&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3， 统计机器学习的模型平均： 过拟合的源泉是数据量不够大而模型的参数过多，引起对于数据集的轻微变化， 模型参数剧烈变化， 从而在测试集的表现就是高方差， 训练集稍微一变，结果就变了。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何抵抗过拟合的一个方法就是做平均，每个模型假设可能会犯错误， 把它们平均起来减少了模型在新数据上的方差。 比如说我们最喜欢的决策树， 我们如果把一棵树变成一片森林， 过拟合的风险大大减少， 因此我们得到随机森林。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWqEUSAdpKWadZjb5TITmib67kMYpBrSorqicfZYN4aBxMwBiaTBCWexVmQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;592&quot; width=&quot;592&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3， 连接主义学派， 连接主义学派认为， 过拟合的风险来自于神经网络具有无与伦比的拟合能力， 那么如果要削弱过拟合的风险， 我们就加入一个东西， 叫dropout， 我们在训练的时候随机的筛选掉一些连接， 然后如果网络还能做正确分类他就是具有鲁棒性， 这就是连接主义的做法。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.4968553459119497&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWVvU45gfib2fwr4IMc2lG6BDWf2bicsJyuiawNzARvcARyL0edQbTibnQJQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;318&quot; width=&quot;318&quot; /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;应该说，这些说法不仅有些杂乱，甚至某些时候是矛盾的。　如果仔细思考，我们会立刻发现下面几个问题：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;１，在机器学习时代，我们希望在同等数据下参数越少越好，而在深度学习时代　，我们发现模型的范化能力随着参数的增加而增加，层数多了，&lt;span&gt;反而泛化误差的能力更好&lt;/span&gt;。更有甚者，　深度学习动辄使用参数的数量大于数据的量，但是模型却不会过拟合。　&lt;br /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;２，即使机器学习传统的正则方法，也看起来矛盾重重，比如很多模型都没有使得模型更简单而是更复杂，却同样能够减少范化误差。　　　　　　　　　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;一种更好的理解方法是，机器学习里说的泛化误差和物理里的熵最大有一种深刻的联系。如果有一个实际观测，有一个模型空间， 这个模型空间的每个模型都代表着我们对真实数据生成过程的一种模拟。 那么我们希望符合实际观测的模型在这个模型空间里越多越好，而不仅仅只有一个， 而且从符合观测到不符合观测的过度要比较均匀，这样我们就可以从容的容纳最多的随机性，这样找到的模型，也就是我们要的泛化能力最高的解。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;简单的来说，机器学习的过拟合，　根源于它是一种反向工程。　一组输入，　经过真实世界的变化得到一组输出，比如说人脑看到一个图片就知道是个苹果。机器只是得到了这些输入和输出，就要产生一个模型，跟那个真实世界的产生过程一样。这其实是不可能的，顶多是类似。你所拥有的数据也仅仅是一些特例而已。我们的机器学习黑箱，无论如何学，只是提供可能的关系中的一个版本。你的训练集上的准确度再高，这仅仅是一个巧合而已。　一个经常被用到的比喻就是，如果一个机器学习模型死记硬背的能力超强，它几乎可以把所有的训练集的标签都记下来，然后一个不差的背出（想象一个神经元数量和数据量一样多的神经网络），但是只要数据哪怕改变一点点，它一定出错。　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何抵抗这样的问题呢？　其实一个很好的方法就是让这个求得的解不那么凑巧。假如你用不同的方法改变你的数据集，　最终得到的结果是一致的。当年的机器学习经典教材里管这个叫低方差，就是说数据变一变，　结果不太变。 如果你一味追求低方差，　你会发现最后模型就傻了，所以，你还要权衡一下模型的准确度。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们再看下物理里熵的概念： 对于一个复杂的系统，比如一群分子，一个人群，你要测量其中的某个性质， 比如分子的动能， 人的身高， 这个东西首先具有的性质就是不确定性，由一个概率分布表达。熵用来衡量这个分布的不确定性大小。一个分布具有的可能性越多， 概率分布越均匀，熵就越大。 物理上一个系统往往趋于熵最大的状态， 熵越高系统越稳定。但是真实情况下， 我们只关心在一定限制条件下的熵， 比如体积温度压强，这个时候熵不能随意的增大而是受到限制条件的最大化， 我们通过在熵最大化公式里引入拉格朗日乘子法解决这个问题，也就是受限条件下的最大化问题。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;在同样能拟合数据的情况下， 我们需要找到模型空间里那个熵最大的解。 学习器永远不是只有一组参数w，而是一个很多参数组成的空间。 每组参数代表一个假设。什么是好的模型？　　一方面你要能给我生成特别多不一样的假设，　也就是模型的容量要足够大，　足够有层次（比如加入深度），这样你具有足够强的拟合能力。　另一方面，　要让符合数据观测的假设足够多， 可能的分布足够均匀， 这也正是熵最大的含义所在。 你找到的解不是那么凑巧得到的， 也就是熵很大，最终， 你在这组解里取一个平均，也就取得了泛化误差最小的解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;一方面模型尽量复杂，一方面有大量行的通的解， 这样就是最好的。 这就把那些特别复杂的模型从潘多拉的魔盒解放了出来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用这样的观点来看，机器学习和深度学习不同视角下的正则化就不会那么多矛盾：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;１，　机器学习，　这个观点在支持向量机算法上体现的淋漓尽致。所谓的max margin solution, 　也就是最大间隔，也就是说这个时候两组点之间的隔离带最胖，两组点被分的最开，这个时候，所有出于隔离带里的直线本质都可以把两组点分开，如果一个直线代表模型的一种可能，这个时候就有最多的模型可以解释数据，而我们选择中央的那一条，是因为这个解是所有那些可能的解的平均，因而也就是最安全的。　我们最终把这个解释为结构风险最小。　&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.697265625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWP0k6ajRBXycoUK2fO6yFGt3t0HH3icRANXsYMLh3rB1U1Szbu91jZmg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;512&quot; width=&quot;512&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看逻辑斯蒂回归，整个逻辑斯蒂回归都可以从熵最大里面推出。事实上，通过交叉熵找到的最后的逻辑斯蒂回归的解，　和支持向量机的解具有一致性。　我们的逻辑斯蒂回归就是加了概率分布的最大间隔解。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;２，关于一范数和二范数：　如果有一组数据ｘ，ｙ同时可以被模型ｗ解释，那么我们偏好w较小的（这基于你假定预测数据的分布符合高斯）。 事实上如果你做一个统计实验你会发现，ｗ的模长越小，你在你选择的解周围就有越多差不多也符合观测的解。　相反，如果你的ｗ模长很大，你会发现，你稍微改变ｗ以后，　模型的解释力就很差了。　一范数的道理是类似的，　只不过你假定数据的分布符合拉普拉斯而非高斯分布，同时引入稀疏性。 　　　　　　　　　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;３　进入到深度学习的世界， 我们看到之前说过的那个矛盾不见了，　因为，增加模型参数不再和过拟合有必然联系，增加模型参数只是增大了可能的模型空间。&lt;span&gt;但是，如果这里面有很多解是符合观测的，那增大的模型空间不但不是诅咒，还是一种福祉。&lt;/span&gt;　比如如果你增加了很多层，　但是通过dropout这样的随机减枝操作，你的解不受影响，也就说明有大量的解都符合观测，　也就是说这一组解其实都是泛化能力比较好的。　　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们的网络不是宇宙里最特别的那个， 它只是无数个这样的网络里的一元，而最后我们就把这些网络做个平均就可以了， 这正是dropout的道理所在！ 一个网络越接近一个随机的网络， 越是在毫不刻意的情况下发现数据里的规律， 就越有可能是泛化能力最好的模型。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看各类隐式的防止过拟合的手段，比如随机梯度下降，SGD, 　随机梯度下降是说每次取一个数据（或一个小批量数据），　由于取数据的这个过程是随机的，带来的参数改变的信息也就有限，也就是带来一个噪声。我们说，这个过程就像物理里的含有一个漂移项的布朗运动，一方面它在往更正确的方向运动，一方面它也在做一个随机游走。Tshiby信息瓶颈的论文指出到了训练后期，我们几乎完全做的是一个随机游走（我们看到训练误差变化不大）。这件事和泛化的联系就很显然了，一个随机游走的过程又不允许训练误差增大，那么它只能寻找那个周围有很大的自由度，或者说附近的解都比较正确的那个参数区域，　这就是泛化的根本。　&lt;br /&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;４，　回看Tshiby的信息瓶颈理论，其实，它里面有新意的东西已然不多了，它所说的，深度网络实现一个信息压缩，也就是一个降维的过程，它符合公式：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.29765013054830286&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpW2mHKczutwcsQQTQYtm1ZWOFGEBta0zjI1zrTj9YRZWzKqV0uG3hrUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;383&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;而在这其中， 我们也看到深度学习和传统机器学习的潜在不同， 它出现了一个介导T， 也就是我们说的representation。 输入信息X， 先被翻译成表征T， 再达到Y ， 从T到Y的过程。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里说的是，　表征就是被转化出来的特征Ｔ， 需要具有两重性， 一方面， 它需要尽量少的含有Ｘ的信息， 另一方面， 它们需要尽量多的含有标签Y的信息。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;尽量少的含有Ｘ的信息，　也就是你要尽可能的不care X里的变化。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果对这个公式进行处理：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.16195372750642673&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWggyWb3NsKoHysZvmdyo8Zcznctfocc0LLecIN71TT1rXfw6Z1ClCjA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;389&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里的 H(T|H) 就是条件熵，数据本身分布的熵 H(X) 是先天确定的，那么最小化互信息，就是最大化这个条件熵。也就回到了我们刚刚说的，要有尽可能多的模型符合观测数据的同时保存对Ｙ的预测力，也正是泛化的本质，也是深度学习模型能够在变得越来越复杂的同时保有抵抗过拟合能力的关键。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;作者简介&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;
&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 28 Nov 2018 04:43:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/dkkqYSYgi3</dc:identifier>
</item>
</channel>
</rss>