<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>推荐 | 一个 AI 方向的优质公众号</title>
<link>http://www.jintiankansha.me/t/fmUebDqQ2Z</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/fmUebDqQ2Z</guid>
<description>&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1620483&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;&amp;#x6536;&amp;#x85CF;&quot;&gt;&lt;section&gt;&lt;section/&gt;&lt;section mpa-is-content=&quot;t&quot;&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;公众号内容&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;该公众号的内容非常的丰富，已经原创发表 150 余篇高质量文章，共 10 多个技术专栏，覆盖以下内容：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1，《深度学习从零进阶到高手》，经过多年经验以及参考若干资料独创一条初学者的学习路线，以计算机视觉领域为例，粗分为：白身，初识，不惑，有识，不可知 5 个境界，讲述从编程，图像基础到深度学习理论和实践，一步一步晋升。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2，《方向综述》，覆盖图像分类，分割，目标检测，图像降噪，GAN，可视化，损失函数等等内容，对计算机视觉或者深度学习的某一个研究方向进行深入全面的解读。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3，《开源框架》，覆盖 caffe，tensorflow，pytorch，mxnet 等十余个框架。每一个开源框架，从简介，到数据的处理，模型的自定义，模型的训练，结果的可视化，模型的测试等进行讲述，麻雀虽小，五脏俱全。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;4，《数据理解》，覆盖各个领域数据集的介绍，数据整理与获取，数据增强，数据可视化等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;5，《模型结构》，对深度学习中的各类模型的结构进行剖析，对其适用的场景进行分析，同时也即将涵盖模型优化等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;6，《AI-1000问》，串讲 AI 技术中的一些非常小而重要，但是又容易被人忽视的问题，以实现知识的查漏补全。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;7，《深度学习理论》，细致地讲述深度学习中的基础理论，让大家更深刻的理解原理，跟踪前沿的发展，激发思考。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;8，《模型训练》，从手动调参到自动调参，应有尽有。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;9，《行业发展》，介绍深度学习在各个应用领域的发展现状，技术在其中的使用前景。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;10，《就业机会》，介绍各家公司的就业机会，剖析其关键产品和技术。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;11，《杂谈》，什么都聊一点，可能是学习习惯，可能是某个特别有意思的文章，或者一些心得体会。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;总之，内容非常丰富详尽，非常推荐！虽然目前多是 CV 技术，但是以后也会有语音和 NLP。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对了，他们还招人。实习也欢迎的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;amp;mid=2649031845&amp;amp;idx=1&amp;amp;sn=3a5b6c04bd107eda5d12ceb3fa9bad47&amp;amp;chksm=8712bad8b06533ce24cabb6be7ad64313dc2620976d87e2bcfac88d543d17f2465de89251803&amp;amp;token=1434981917&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;有三AILab成立 | 寻找合适的你&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;520&quot; data-backw=&quot;510&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/e4kxNicDVcCFW8qHIFeCDA9GDSIYCnVNlcK9kicAnVsnqaE6I5QpQKPjIiaLib6MZ7q8KdJf69e3n8qibJMkQZzNP4Q/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.01953125&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/e4kxNicDVcCFW8qHIFeCDA9GDSIYCnVNlcK9kicAnVsnqaE6I5QpQKPjIiaLib6MZ7q8KdJf69e3n8qibJMkQZzNP4Q/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 15 Mar 2019 14:53:25 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/fmUebDqQ2Z</dc:identifier>
</item>
<item>
<title>AI就能解决食品安全问题吗</title>
<link>http://www.jintiankansha.me/t/kigw41CD7S</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/kigw41CD7S</guid>
<description>&lt;p&gt;十年之后，我们怀念过去的时候，就像我们如今回忆99年或者08年时，会怎样谈论这个三月了？也许会说起波音的事故是人类打响从AI抢回控制权的第一枪，在这场人机大战中，人类飞行员无法战胜一个有bug的程序。站在十年的角度来看，能够多少让我们更多的看到制度的问题。但我今天不想说这个话题，有一件让人心寒到骨子里的事，不吐不快。&lt;/p&gt;

&lt;p&gt;在一个信奉民以食为天的国度，成都七中的食堂被曝光出现了显而易见的食品安全问题。受害者不是幼儿园，而是小学生。小学校园中有六年级的孩子，所以我本以为是孩子们参与了真相的曝光，但看了报道，才知道并没有。小学的时候看小鬼当家这个电影，看完我想美国的孩子真厉害，能自己动手来解决问题，而在9012年，遇到坏人，我们的家长还只是习惯性的跪下来。这是何等的悲哀。&lt;/p&gt;

&lt;p&gt;问了几个朋友，为何不就成都七中的事情写点东西，回答都是累了，觉得写了也没用，食品安全喊了这么久，只能靠跑路解决问题。但米国已经选择了America first，老欧洲则更加封闭和民粹，等过几年大概率的成了gdp第一，担起普世价值的大旗，不想承担也躲不开。再说真要学欧美那样，工业化的食品精加工，高盐高脂高糖，未来还不满街的超重人群。&lt;/p&gt;

&lt;p&gt;接着有朋友说AI未来能解决这个问题，AI是不会被阿伦特的平庸之恶困扰的，现实中那么多做饭的洗碗的买菜的，他们在这件事被曝光后可以说自己上有老，下有小只是为了混口饭吃，自己不用承担什么责任。在创客展上，我见过机器人厨师做的三明治，汉堡，也吃过无人售卖机卖的面，它们的确不会罔顾良心，而做这些机器人的公司，也会爱惜羽毛，不要机器人做出违背阿西莫夫三原则的事情来。过不了几年，也许私立小学的食堂会引入机器人，然后这里会成为家长们参观的必要打卡地，会成为孩子们科学实践课的场所。&lt;/p&gt;

&lt;p&gt;或者不需要等未来的机器人技术成熟，现在只要有人能找到足够多霉变食物做的菜的图片，用图像识别就能够做一个分类器，以后每天手机扫一扫，就能知道自己吃的饭是否卫生了，这目前已是很成熟的技术了。但我们真的要依靠AI来解决人类自己作的恶吗？过度的依赖技术，就会像波音的飞机那样，陷入人机大战中。做饭机器人可能会被供货商黑掉其用来做食物指控的模块，而通过图像来识别食品是否卫生，也会让某种食物更容易被误判的。人类自身的问题，需要自身的制度建设来解决。&lt;/p&gt;

&lt;p&gt;判断一个学校的食堂好不好，就看老师或校长是不是也在这里吃饭，判断一个单位的食堂好不好，就看大老板是不是也会食堂吃饭或招待客户，街边的苍蝇馆子是否干净，就看这家店的伙计是不是各个都吃的胖胖的，这就算所谓的skin in the game，要让做这件事的人来承担因这件事做错带来的损失，一定要具体到个人，这次成都七中的事情，要想杜绝下一次类似事情的重演，就要追究到个人，所有在这个时间段，在该小学食堂工作的人，不管是帮厨还是掌勺，通通很长的一段时间，按老赖处理，限制坐飞机上高铁，派出所重点照顾，芝麻信用分什么的通通清零，良心上欠了债，不在脸上打上终身的红字，就已经是社会进步的红利了。&lt;/p&gt;

&lt;p&gt;除了寄希望事后的惩罚，还不如用好事前的开放，有一个方法，可以不需要立法的改变，就能避免未来再出现类似的事。现在直播这么普遍，为何不能要求每个学校的食堂开一个直播间，将手机固定起来，从买来的食物原材料，到最后的上菜，每一步都直播出来，有了这样的直播间，家长那怕给直播平台交流量费，也肯定会看的。类似的，外卖平台也可以要求加盟商户直播自家的厨房。这不需要新的立法，不需要新的技术，只需要有人用现有的技术，只需要有人愿意去做，愿意去用脚投票。&lt;/p&gt;

&lt;p&gt;最后的最后，本想放一首歌，周云蓬的中国孩子，但加上了，这篇文多半会404的，想听的百度就能找到。我既不想写会404的文，更不想写十万加的文，会刻意的避免那些会让文章火起来的元素，这篇也不例外。真正持久的改革，不是靠几篇十万加就促成的，只要少数的人从心理认定了一件事坚持下去，就足够了。昨天看美剧The good doctor中的一句台词，深深的打动了我，”Whenever people want you to do something they think is wrong, they say it's reality&quot;,世上有那么多的学校，那么多的食堂，总有一所会有一些害群之马，就像飞机的代码那么多，总会有些bug，这就是现实。从统计来看，飞机是安全的，食物的无害的，只是我们说起这样的现实的时候，是低着头的，而当说起我们改变了的现实的时候，肯定会自豪的扬着头的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9571428571428572&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcchv3KUuweZXe6fsWE5ibGU1ia4EK7z4k4428aicv80iaX9TmDgibur2w2ibe7KjtWmGMFjbHsNibFbe3XDA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;420&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ps 最后放一张图，很喜欢这句话，也很应和这篇的主题，希望通过AI来解决社会自身的问题，就是年轻和幼稚的表现，通过发明或者应用一个黑科技，立刻见到立竿见影的成果，但人的问题，应该由人来解决。机器不应该为人的平庸之恶买单，机器只会以指数级的效率，去放大人的平庸之恶，甚至是人无心做的恶，只要自己先做的端正，才能避免ai把我们带到反乌托邦。&lt;/p&gt;


&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383677&amp;amp;idx=1&amp;amp;sn=b1aa1ab4453286c02b7fedcc28a9b391&amp;amp;chksm=84f3c93cb384402ae4482f878a696616f168f13e9f1a6a74481f892b0265ef087de6f7d1901a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当你年近九十，是否还有心气，向人类史上最困难的百万美金发起挑战？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383412&amp;amp;idx=1&amp;amp;sn=b3d8537651bb02abd3ebfa759e5e1db3&amp;amp;chksm=84f3c835b38441239a0cc7412f1e0873a8ff7dfdff3f7044c4a998c9706f444a43cbaf55fbcd&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;《Skin In The Game》塔勒布由风险管理引出的一碗毒鸡汤&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;





</description>
<pubDate>Fri, 15 Mar 2019 14:53:24 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/kigw41CD7S</dc:identifier>
</item>
<item>
<title>空间简史-人类认识空间的旅程与其对强化学习的启示</title>
<link>http://www.jintiankansha.me/t/9DKU2tIReQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/9DKU2tIReQ</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;9nmgo-0-0&quot;&gt;本文是对okeefe 1978(栅格细胞发现者， 2014诺贝尔奖得主)的论文 cognitive map  的总结和延申。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4mvhc-0-0&quot;&gt;一  空间的先验与后验之争&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5fivr-0-0&quot;&gt;对于我们在其中生存和繁衍的空间， 是如何在我们的心理世界表达的， 这是一个争论了几百年， 也依然没有完全清楚的问题。 如果你不去仔细思考， 你可能觉得这是一个很简单的问题。 而一旦较真， 你就会发现几乎所有的哲学家， 物理学家， 心理学家所纠结过的那些问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fut2f-0-0&quot;&gt;首先， 什么是空间？  最早探讨它的是物理学， 从亚里士多德到牛顿。 牛顿的物理学在&lt;strong&gt;绝对空间&lt;/strong&gt;基础上存在，所谓绝对空间， 可以简化为一个欧式直角坐标系， 世间的所有有行实体都可以在这个坐标系里寻找到一个坐标。有了空间和时间， 我们就可以相当准确的描述和预测发生在时空里的运动，并且进行大距离的迁徙（比如大航海）。 想象一下没有地图和坐标， 哥伦布即使偶然到达美洲也不可能回去了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.902542372881356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5eoVMy7tN5ia4Gf32QqzbydCZP5h3zYLmY0j8BnBqfomuju42nfRvSibg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;236&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.68&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pqRgj6fWpib4icGP6F9cIQLjTXk4jibGmuGvW5HQiaW8Uo4SMBp3UibuCAw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在古代， 星空是人类航海重要的坐标， 我们通过判断星辰间的指向， 知道茫茫大海自己的去向， “陪你一起看看星星” 绝非为了浪漫， 而是关乎生存。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5df0s-0-0&quot;&gt;虽然物理学家从不怀疑真实空间的存在， 然而有一个问题确没法解决。 我们的感知是含糊的，柔软的，既缺乏像尺规一样的绝对空间度量， 也没有绝对的方向度量。 我们对距离的描述经常是或近或远这样的模糊语言，也不擅长想象一个超大空间的地图（受到训练之前）。 那么， 那个物理学家关心的刚性的欧式度量的空间是从哪里来的呢？ 我们为什么能够产生这样的概念？ 是什么使我们能够产生这样的概念？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9umse-0-0&quot;&gt;换句话说， 空间如果存在， 它到底在哪里？ 它是怎么在我们脑海里形成的？ 它是通过某种先天的“结构” 得来 ， 还是通过感知基础， 在后天的学习和思考基础上形成的?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6ren9-0-0&quot;&gt;应该说对这些问题的回答绝非容易， 我们一开始解决这些问题的方法是哲学， 而后面才从生物学的认知基础上讨论。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eh1l2-0-0&quot;&gt;最早对这个问题进行阐述的人包括贝克莱和康德， 它们分别代表了两种截然不同的观点。贝克莱和我们熟悉的休谟和洛克一样是英国经验主义哲学的代表人物， 强调一切认知的基础， 无非是大量经验的总结， 它否定物理上的绝对空间，认为这是人的认知造成的一种幻觉。首先在空间认知的事情上，他认为存在等同于被感知， 而所谓的空间， 无非是我们被感知到的大量的触觉，视觉， 和肌肉运动之间的某种关联。 因而绝对空间这个东西， 根本就是子虚乌有。 大家想下大卫休谟的那句话：&lt;strong&gt;只要闭上眼睛就没有悬崖&lt;/strong&gt;， 就会理解他的观点的深刻含义。 感知所构成的大量经验集合是第一性的， 绝对的物理空间是第二性的， 是一种方便性的考量。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3454545454545455&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5ia4BdTYBLNibv3hicicIUOiaS4QCKBkNRU8eTMqLphD0zfibupQiaCyXuWhyw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;220&quot; /&gt;&lt;/p&gt;
&lt;p&gt;具有经验主义传统的英国， 出产了贝克莱和休谟这样的哲学家。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2i12a-0-0&quot;&gt;这样的对空间的认知， 与牛顿的物理学存在本质的冲突， 而另一个派别， 是结合了理性主义和经验主义的康德提出的理论， 他认为绝对空间存在，而它依赖的恰不是外部的物理世界， 而是人类先天的认知基础，一种与这种绝对空间相对应的脑组织，它是我们认知外部世界的基石。   &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2i12a-0-0&quot;&gt;康德的时空观是起纯粹理性批判的基础。康德的观点既不同于贝克莱也不同于牛顿。 首先他认同绝对欧式空间的存在， 其次他认为这个空间不存在于物理世界恰恰在我们的心理， 第三这个先验的结构是我们其它感知的基础。 &lt;strong&gt;我们的对物体的感知， 都要放到这个空间结构里得到认识。&lt;/strong&gt; 应该说这里的第一性和第二性的顺序与经验主义恰好相反。 康德的理念里， 没有了时空这样的先验， 经验毫无意义（联想以下当下 数据-经验 驱动的AI所遇到的缺乏逻辑推理能力的瓶颈， 我们无疑在某种程度回归康德的问题）。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.4984984984984986&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5q2ydezSg0X819ribLFic7zSJUSbib4wv1CqpYu3pzEmAvXwmfREJAGM8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;333&quot; /&gt;&lt;/p&gt;
&lt;p&gt;康德认为经验的认知需要在先验存在的时间和空间之上， 这也是康德思想体系的基础之一。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2s0ur-0-0&quot;&gt;在康德之后， 这两个派别分别发展出Empiricist（经验主义）,  和Natist （先天认知）两个基础流派，&lt;/span&gt;&lt;span data-offset-key=&quot;2s0ur-0-1&quot;&gt;经验主义者强调所有有关绝对空间的认知都是后天学习得到的大量感知之间的联系。 而先天主义者则认为需要有一个先验而非习得的空间结构，这个结构是后来学习的基础。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt;在后面的整个世纪里，两边各站着一批各自的哲学家，分别寻找证据阐述各自的理由。 一个比较标示性的任务是20世纪初的庞家莱。 这个时期的物理学发生了天翻地覆的变换。 爱因斯坦的相对时空开始取代牛顿的绝对时空。 而黎曼几何的出现代表我们之前深信不疑的欧式空间无非是受到了我们经验的局限。黎曼几何成为广义相对论的基础。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt;而从电动力学和量子力学衍生的场论更是刷新了人们的三观 。庞家莱在这个基础回到了贝克莱的经验主义，就没有特别奇怪。庞家莱首先认为空间无非是无数经验的集合， 这些经验主要是由人在移动时候视觉的变换构成的。 我们对不同物体的距离的感知， 也无非是让一个虚拟的自己经历一个从A物体到B物体的过程而认识到的。大量 经验上学到的位移与视野变换的对应关系可以用平移算子和群表示。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt; 而这样的群最大的特质是存在一个逆运算可以让当下的状态和先前的状态完全一致（联想一下时间就没有这样的对称性， 不存在一个时间平移逆运算让你回到时间的原点）。  位置的概念隐含在这种平移算子的对称性里 。庞家莱的理论不难找到同时代的相对论和场论的影子， 而他的思想标志了经验主义的新高度。&lt;/span&gt; &lt;span data-offset-key=&quot;1sfen-0-1&quot;&gt;我们在不停的变化的经验积累中得到了变化中的不变性（数学规则）， 而这些数学规则就是空间的本质。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5HseU1MBGiaNOp3ptQ909N9utJQbyghIY3wc2mORoDKIO3tloIqrL0xQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pblIushnqyL4lmicj1glPd1KhDWVbFEDP7AR175TbR1sEqyrLuiawebg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;黎曼几何， 打破了欧式直角坐标系，同样的也是对于日常经验的一个突破。 因为我们常见直线， 不说明它是真实的。事实上爱因斯坦的广义相对论指出光线被引力弯曲沿曲线传播。 黎曼几何成为广义相对论的数学基础。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9g6ei-0-0&quot;&gt;注： 爱因斯坦的狭义相对论的建立过程体现了对牛顿绝对时空的突破。事实上正是爱因斯坦看到了牛顿的绝对时空是受到了我们经验的局限才能够打破它。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4033333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5otsAHscJQBT7jGhR0JbJkhYZWQzvuIOAxHASjMrTvKuL9PyKHEM37A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;狭义相对论认为，我们的绝对时间的观点， 正是受制于我们自身的经验，因为我们从来不以接近光速运行。 而得到真实的物理规律， 事实上需要突破这种经验。 狭义相对论以光速（电磁学规律）为绝对不变， 而放弃时间的绝对流逝， 当物体的运动速度变换，其时钟也相对静止坐标系进行调整。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;egr3d-0-0&quot;&gt;而继续把场论的思维进行深化的，是Kohler等人提出的Gestalt（格式塔）理论。 Gestalt理论比庞家莱进一步的指向了空间感知的神经基础， 他把大量神经元的同时放电看做是一种场的形成， 不同的神经元组（网络）代表不同的场， 两种最基本的和空间导航有关的场一种叫做 地理场（geography field）， 一种叫做 行为场（behavior field）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egr3d-0-0&quot;&gt;地理场主要用来表征外部的物理世界-空间关系， 而行为场用于赋予各种外部刺激（感知）以意义，估值，和反射行为（这就是强化学习理论的预演，行为场可以看做强化学习的值函数），这两个场互相配合产生空间有关的概念和行为。 从外部的刺激通过神经组织合成出各种合适的“场”来表征外部特征的思想已经像极了今天的深度学习， 不难看出我们今天的科技和前人的思想的联系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a4jvn-0-0&quot;&gt;在此基础上， 1936年Lewin提出了空间拓扑结构和所谓行为场的关系， 使得Gestalt的理论变得更为坚实， 之前的行为场的一个问题是不知道它如何组织和形成， 而Lewin则提出了它的基础是各种各样的和行为有关的空间拓扑结构， 比如边界，连接， 等等。  也就是说你先建立一个空间的拓扑场， 后面可以就容易建立一个行为场。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f780f-0-0&quot;&gt;二 来自动物行为的证据&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk4an-0-0&quot;&gt;好了，再fancy的问题 ，最终还要回到空间认知的本质是个生物问题 ，它需要特定的生物载体 。  那么研究动物对空间的认知就是一个几乎不可避开的问题。 动物是不会说话的，本质上了解动物的空间认知必须要从行为入手，与空间有关的行为就是导航。 像鸟类，小鼠， 蝙蝠都具有极为发达的空间导航能力（甚至比人还厉害），那么它们是怎么在复杂的空间里穿行，或者经过几千公里回到自己的家的呢？ 从观察这些行为入手， 我们也可以得到空间认知的本质。 我们说， 如果一个概念对行为和动物的生存并无意义 ，那也就是失去了任何行为的基础。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5387755102040817&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5CGFiauhV0R2yIqFLibFg6VprTlUia5m6RVBIck4N3Qbibtz5qj8MQRenbA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;经典的小鼠走迷宫任务。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a92uh-0-0&quot;&gt;关于小鼠的导航问题的实验的问题，我们看到小鼠很容易在非常复杂的迷宫里找到食物，关于这个现象基本的假设解释， 一种是小鼠没有空间的概念，但是它可以记住一系列的动作 。这就好比一个很长的条件反射，比如左左右右左左右。 这就好比在现实生活中， 当你完成一个动作系列到达了星巴克， 你再执行另一个动作序列到达肯德基。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;aliv-0-0&quot;&gt;而另一个假设是小鼠有关于空间的概念 ，根据在大脑里生成的地图来决定每个时候的走向找到目标。 所谓地图，是指你和周围的物体（地标）以及周围的物体（地标）之间相对位置的几何。 在一个地图上， 所有的地标都获得了一个绝对的坐标， 即使你没有去过那个地方， 这个坐标依然告诉你它在什么位置。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ktiv-0-0&quot;&gt;为了研究相应的问题，我们可以把真实的空间去掉， 让小鼠在一个“时间迷宫”里（这个任务里缺乏固定的空间结构），单纯记住“左左右右左左右” 这样的动作序列来解决这个问题。 事实上小鼠这个时候已经很难完成这个任务。 这一系列的实验结果支持地图学说， 导致Tolman在1948年提出了Cognitive map的概念。 那就是 空间 或者 地图的概念在小鼠的大脑里是存在的， 成为其导航学习的基础。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e3g3n-0-0&quot;&gt;对于同样的实验现象， Hull为代表的人提出了一套截然不同的解释，可以看作刚刚说的动作序列的高级版本，解决刚刚的矛盾 。 那就是看似复杂的空间导航，无非是一个多级的，组合式的条件反射。这就和我们日常大多数习惯的获得没有区别。 只是，在空间导航的学习里， 你学到的不是一个从起点到终点的方法， 而是一个系列的能够从起点到终点的动作系列（对应同一效果的不同的轨迹），这样也就不会受困于某个特定的行为序列。这个理论与庞家莱的群论的含义是一致的。 也就是我们学到的不是一个轨迹， 而是一个行为的集合， 具有同样的最终效果（一个群）， 这其实说的就是当今机器学习的泛化能力。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f5hiq-0-0&quot;&gt;多级条件反射和认知地图均能够解释现象， 但是背后的眼里却非常不同， 这也成为后面一系列的工作的起点。&lt;/span&gt;多级条件反射， 与心理学的一个重要的流派-行为主义流派不谋而合。它的主要代表人斯金纳用非常复杂的条件反射来解释语言和思考在内的所有认知现象（把语言符号也看作一种刺激），因此在那个年代也很占优势。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7283333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5TWERwC5F7fF6lQib49o26DWyOMCVfOl8Il8icUEeusZjF1JvvEjjib8Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;斯金纳箱， 操作性条件反射的实验装置。 小鼠做出正确的动作后可以得到食物。 操作性条件反射在斯金纳的时代被认为是智能的基础。 也是强化学习理论的基础。通过多级条件反射， 小鼠不仅可以把当下的刺激和奖励联系起来， 还可以把之前的行为和刺激和当下的刺激联系起来&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7bdb1-0-0&quot;&gt;注： 稍微用心的研究者不难发现组合条件反射与深度强化学习的关系 ，我们一次又一次回归前人思想的轨迹。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rlgf-0-0&quot;&gt;而认知地图的支持者后来者居上， 一个重要的根据在动物导航行为的研究。 研究者发现在诸如鸟类这样的动物里 ，当你把鸟从一个地方移动到它所从未见过的地方， 它依然有能力找到到回家的路。 按照多级条件反射的说法， 鸟需要根据自己熟悉的地标， 记住一系列动作， 或者一个方向， 然后才能达到目的地。 而如果一个地方是完全陌生的， 那么鸟根本不可能能够根据习得的一套方案回巢（事实上这个逻辑并不严密）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9383333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5f4WicAHicGB4f4RpRStXvTwiboEJb4LhMbXibeU2GIymibkn1a8GVCw5L0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;鸟类天然擅长长途迁徙&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2r51e-0-0&quot;&gt;另一个重要的支持在于寻找捷径， 比如你回家的路上发现平时需要绕过的公园多了一条小路 你可能没有走过， 但是你依然可能会直接穿越回去到家。 寻找捷径的能力类似于强化学习里的有模型学习， 你需要建立一个最小的世界模型， 才能知道当下某个从没有见过的地标和你熟悉的地标（家）之间的联系。认知地图的支持者认为这个模型正是由认知地图提供的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ou86-0-0&quot;&gt;这些都成为认知地图作为一个先验结构早已存在于脑海中的实验支持， 不仅如此， 这个地图需要的样子是一个绝对的欧几里得坐标系，而不是你根据自己的位置为中心，设立的一张相对你而言周围物体分布的地图。 正是因为有这样一个绝对的欧式坐标系，你才知道周围物体相对周围物体， 门子相对窗子， 马路相对公园的位置， 你才能根据你的空间想象做出决策 ，不是走A路而是走B路，即使你从来没有见过A路，或者到了一个完全陌生的城市。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rids-0-0&quot;&gt;如何构建这样一个地图？ 你的大脑里的某个部位需要能够精确的进行路径积分， 并把每个看到的地标放置到这个精确积分的大脑平面图里。如果整个周围环境是固定的， 一旦出现一个新的物体， 你就很快可以想象出它和之前所有出现过的物体的相对位置， 在这个世界里， 每个物体的表示都是一个位置向量。 如果你想做一个能够行走的机器人， 不难想象也会构建一个类似的概念。&lt;/span&gt;这样的观点构成认知地图的基础， 我们通过大脑里的一个先验的绝对空间的概念载体， 而使得复杂的空间计划和导航学习成为可能。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a2m4q-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  三 Place Cell 和 Grid Cell的发现&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eete9-0-0&quot;&gt;这样的想法非常合理， 唯一的问题是我们的大脑里真有这样的结构吗？  这个观点在一组大名鼎鼎的细胞， grid cell和place cell之后可谓是登峰造极， 成为了科学的主流。 而它的发现者O'Keefe 和 Moser也获得了2014年的诺贝尔奖。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5dp4k-0-0&quot;&gt;这组细胞， 仿佛就是cognitive map的生物载体。所谓place cell的含义非常简单， 就是当你不停的经过某个同样的地点，同一个细胞会放电。  而所谓Grid cell， 其特征是其感受野对空间进行周期性的放电，它可以把一个二维平面表现成一个密集堆积的六边形结构， 不同的grid-cell具有不同的空间周期。 认知地图的支持者认为，这个Grid cell正是那个先验的大脑里的欧式坐标系的载体。如果你对空间里的一个狄拉克函数（一个空间质点的表示）做傅里叶变换你会得到一系列不同周期频率的波函数， 反过来， 这群函数或许可以作为一组表达不同物体位置的基函数。 而Grid cell如果是对应了这群函数， 那么它将可以非常灵活的表达生物体在一个绝对坐标系里的位置，即使生物体运动到了一个完全陌生的环境。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.995&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pmcLxcOm2D3WGCjia2RAPZePreTiaeWUbd0tiadOgibULxbEhKHl03icv3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4340175953079179&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5A7P7peaOS064K3TQjVAfjDe2fDqUSneiaceuDgDcApVYUpMXInCCuZw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;341&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.555&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5npnXSEAguVVZ319icKVmAmCIE4DiaQs3TA9BWgl88lHggjicusq7wePGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不仅在小鼠， 蝙蝠的大脑里也存在Grid Cell， 与三维空间相对应, 参见 Grid cells without theta oscillations in the entorhinal cortex of bats Nature&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;egu7f-0-0&quot;&gt;在Grid cell和Place cell发现之后，认知地图的理论奠定了统治地位，空间学习需要一个先验的神经空间坐标系成为了共识。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a2nid-0-0&quot;&gt;四 人工智能时代的续篇&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eciud-0-0&quot;&gt;在人工智能时代，我们越来越发现这些早期认知科学争论过的核心主题， 事实上对发展从狭义到通用的人工智能都非常重要。你要先理解智能，才能做出人工智能， 否则做出的东西只有“人工”没有“智能” 。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rrte-0-0&quot;&gt;在DeepMind去年发表的一篇和空间导航有关的论文里， 它们也确实把这种和空间有关的结构- Grid Cell 引入到了它们的网络架构里，而非常有趣的是， 如同当年的认知科学家所阐述的， 这个空间坐标结构的引入， 使得导航出现了类似于直接利用捷径这样的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1pob1-0-0&quot;&gt;而与空间结构的先验学派不同的是，DeepMind的这个Grid Cell 结构， 事实上是从利用监督学习进行引导的。  DeepMind 让人工“小鼠” 在方格空间里乱跑并预测其位置，在这个过程里， 如果适当的引入dropout这样的条件，它们表明就可以出现类似于Grid的细胞结构。 而这个结构正是刚刚说的寻找捷径行为的基础。论证的方法也和生物实验相同， 就是去掉这些细胞观测， 寻找捷径的行为消失了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6866666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5121JlNk1abM39bE8hurQ6uP0uVKoeKEia4xA2ibQwpdYvWydLIas8T6w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Vector-based navigation using grid-like representations in artificial agents  Nature&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ej61m-0-0&quot;&gt;这篇文章在专业圈子引起了很多批判，很多学者不认可这样形成的Grid Cell就是生物学的Grid Cell。&lt;/span&gt;另外一种可能是Grid Cell只是许多对空间探索有利的结构的一种，而这种结构恰恰是无论是自然训练还是人工训练都非常容易找到的一种， 可能对应某个自然界的最小作用原理（事实上六边形是周期性的布满一个二维空间的最经济方法）。因此DeepMind的这个作品也就没有那么神奇了。&lt;/p&gt;

&lt;p&gt;在思考这个问题的时候， 我个人依然觉得到庞加莱等人的经验主义思想具有极高的借鉴价值。 虽然用认知地图方便好用， 但是它是否是最基本的东西？  我们大脑里的那个空间概念最根本的东西究竟是什么？ 或许背后更本质的东西依然是几条抽象的数学规则，而我们大脑的神奇在于利用这个规则得到地图这类方便的概念。Deepmind按照人们已经预期设定的理论找到了同样的结果， 虽然促进了AI的进步， 但是对于我们理解这个问题却是有限的。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9md34-0-0&quot;&gt;五 关于空间任务之外的启示&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9lp2a-0-0&quot;&gt;不管认知地图是否最终成立，生物学的研究，还是人工智能的研究，都在指向的一个共同点，就是我们学习需要预先存在的特定“结构”，而不是简单的多级条件反射可以得到， 虽然在深度强化学习时代，多级条件反射给我们展示的可能性比我们想的多很多。 而AI的研究在告诉我们， 这样的先验结构， 是可以通过大量的预训练得到的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9lp2a-0-0&quot;&gt;如何预训练， 怎么设计预训练流程， 可能是未来的一个极为重要的方向。Karl Friston所说的预测误差最小，最新的大量关于好奇心的研究，甚至最近的语言模型Bert，可能都在提示我们怎样设计这样的流程。  同时，这样的研究或许也在启发我们如何更好的设计婴儿的早期教育 ，使得后期的学习效果更好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8il2h-0-0&quot;&gt;对于空间的思考本身， 对于非空间的很多任务也极有启发。 比如我们常说的语言。 我们知道，语言代表了我们使用和控制符号的能力，  而“符号” 和空间“位置”的关系是什么？ 是否存在一种隐喻， 正是由于我们发展出了对抽象的“空间” 和 “位置”的认知能力， 才引领我们走向了更广义的形成和使用“符号”的能力？ 在一个抽象的“符号” 地图里， 运动不再是欧式空间里从一点到另一点的轨迹， 而可能是一种逻辑思维的流动？  这些都将是未来人工智能极为需要回答的问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.74&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5bKc1g86RibzW2OmNLVicH3kQMLTvr6iaGSrPTnt5iaB5kP7uYozTuzMgsQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Organizing Conceptual Knowledge in Humans with a Grid-like Code  Science   一个惊人的实验， 在人类进行对不同形状的关联（把一种形状的鸟对应到另一个形状上）的时候， 类似的Grid的神经表示出现&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://www.cognitivemap.net&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://www.cognitivemap.net/HCMpdf/HCMComplete.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383843&amp;amp;idx=1&amp;amp;sn=41e82163f76edfe5ffe31a8518d5bafa&amp;amp;chksm=84f3c662b3844f7430b27f82522dd9414d6c481f6e63822d99deb8baf281be8681ea5c4413a7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑的自由能假说-兼论认知科学与机器学习&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 13 Mar 2019 11:44:39 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/9DKU2tIReQ</dc:identifier>
</item>
<item>
<title>如何让让神经网络开口说话</title>
<link>http://www.jintiankansha.me/t/o5DC8CEtwb</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/o5DC8CEtwb</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;人类简史说，人类的进步大大依靠与人的语言，人在灵长类中具有最高级的语言能力，并且能够编故事，从而把越来越多的人组织在一起产生了文明，这才是人类社会进化的根本。&lt;/span&gt;那么，神经网络能否掌握人类的语言，听懂人类的故事呢？ 相信朋友们都知道谷歌翻译，谷歌助手，机器写诗， 它们实现了令人惊艳的性能，并在某些具体任务里让人真假难辨。这里的技术核心，就是&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;-&lt;/span&gt;&lt;span&gt;我们常说的传说中的循环神经网络。&lt;/span&gt;RNN&lt;span&gt;可以称得上是深度学习未来最有前景的工具之一。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.65379113018598&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNw72RI3PwDHibjxNr7t1oNQLSv5FLvmPR6GDkZTg0F5aJ9WutibYapUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;699&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;时间序列和&lt;/span&gt;RNN&lt;span&gt;引入&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让我们从基础学起，&lt;span&gt;首先&lt;/span&gt;什么是时间序列，&lt;span&gt;我们又&lt;/span&gt;为什么需要&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6744730679156908&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k663I0G9glMsglxZ9z9S1YH5lmWibkRWLeaj0kU2MwM4WGibDickz81R0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6227848101265823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kwibz1UiaqqF3Aqtee5PF7wZJAFfRAVgWGoGHicAfb7XibGyhibKztNtay6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;395&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;时间序列，&lt;span&gt;是&lt;/span&gt;一个随时间变化的过程，&lt;span&gt;我们&lt;/span&gt;把它像一个数列一样排列下来，&lt;/span&gt; &lt;span&gt;&lt;span&gt;序列&lt;/span&gt;里的数字往往&lt;span&gt;看起来&lt;/span&gt;在随机波动。&lt;/span&gt; &lt;span&gt;&lt;span&gt;一定&lt;/span&gt;程度，&lt;span&gt;我们&lt;/span&gt;可以把它看做以一个一维的图像或向量，&lt;span&gt;这个&lt;/span&gt;图像不停的向前滚动。&lt;span&gt;比如说文字，就是一个典型的时间序列。&lt;/span&gt;处理和时间有关的信息，我们再次回到我们的大脑，看我们的大脑是怎么处理这一类问题的。&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;实验发现，大脑越靠近感官的区域就越像CNN的结构，它的最本质特征是前馈，也就是每一次神经信息都是从感官向大脑的深层一步步推进的，而每层网络之间是没有联系的。而到了深层，这一切发生了变化，大脑内开始出现一些神经网络，这些网络在层间出现了很多的连接，这意味着什么呢？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8511705685618729&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQek5FIxZefqx6QJemfLiceYcbaLAG6P7V3JV0ibsxH67IUb3yAOLSO2A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;span&gt;&lt;span&gt;我们说，我们需要引入时间这个维度才能完全理解这件事，这些层内的链接意味着当下的神经信息会在下一个时刻被层内的其它神经元接受，而这个接受的结果呢？&lt;/span&gt; &lt;span&gt;当下的信息会传给这些层内的神经元，&lt;/span&gt; &lt;span&gt;从而使得这个信息在网络内回响一段时间，就好像社交网络里人们互相发送消息或分享朋友圈一样一个事件发生，就会引发一系列的社交网络动作使得信息的影响停留很久。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个典型的例子是如果人和人彼此链接形成回路，我发出的信息可能会在若干时间又传递给我，从而让我自己直接看到我的过去历史，这个信息停留的效应是什么？大脑处理时间相关信息的关键正是记忆。 最典型的在一个对话过程里，你要记住此时此刻所说的话， 还要关联前面很久的话，否则就会发生歧义。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;引入了记忆， &lt;span&gt;这些问题就好解决了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;记忆分为不同的种类， &lt;span&gt;你在一个对话里可能回忆起很久以前的事情，&lt;/span&gt; &lt;span&gt;但是这和刚刚说的话的重要性显然不一样，&lt;/span&gt; &lt;span&gt;所以我们大脑就把这些记忆分成了长时的记忆和短时的记忆（工作记忆）。这个短时记忆，&lt;/span&gt; &lt;span&gt;正是通过互相高度连接成回路的网络之间的神经元互相喊话造成的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;一个神经网络里神经元之间彼此互相传递信号，形成循环往复的回路结构，就可以让过去的信息保持下来，把&lt;/span&gt;这个&lt;span&gt;原理转化成人工神经网络就是&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;，&lt;span&gt;翻译出来就是循环神经网络，这个循环，正是彼此链接形成回路的意思。我们看怎么把模糊的想法一步步变成一个数学模型。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5401174168297456&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kI3cicAeAs54DqNJF4toeQm7fEB9FGnQ0MINuiauBAydgoGdhvrnTJ67g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1022&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们把这个网络内部神经元互相链接的方式用一个矩阵Wij表达， 也就是从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;i&lt;/span&gt;到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;j&lt;/span&gt;的连接强度表示。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wij&lt;/span&gt;就是神经元间彼此沟通的方法，&lt;/span&gt; &lt;span&gt;我们看rnn的方程，一方面网络要接受此刻从外部进来的输入，另一方面网络接受自己内部的神经元从上一个时间时刻通过&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wi&lt;/span&gt;进来的输入，这一部分代表过去的历史 ，决定网络此刻的状态。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23720136518771331&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kAiaMKQFH5unWWVxrqymUcsCoMmTxMaVhNnsI8TZic9CdBicyhhib9Z51Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;586&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;深度学习的核心就是特征提取。用一句话说，&lt;/span&gt;RNN&lt;span&gt;具备的素质就是把整个过去的时间信息转化成此刻的一组特征，然后让网络做预测或者决策。 比如，此刻股市的价格是所有之前和公司有关的信息一点一滴积累起来的。公司每个时间点的信息， 就是输入向量，&lt;/span&gt; &lt;span&gt; 那么神经元所干的事情是什么呢？&lt;/span&gt;  &lt;span&gt;它把所有过去的信息转化为当下的神经元活动， 而这些活动，就是一组由过去历史信息组成， 决定当下预测的特征。在股价的情况下，你可以想象成， 这些神经元就在描绘人们的信心指数， 而信心是所有过去点信息汇集的结果，RNN把每个时间步的信息通过神经元之间的互相喊话Wij，压缩成当下的一个状态向量hi，它包含了所有和我此刻决策有关的历史。 数学上你可以推到， 这个hi是所有过去输入的一个函数.&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;学说话&lt;/span&gt;&lt;/span&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;什么叫预测说话，这是一个形式上开起来有点傻的例子， 就是给你一个序列的字母或单词让你猜下一个，我想你一定玩过报纸上的填词游戏。 那个每个时间步骤的输入就是一个字母，然后我要输出最后一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;一个处理这个问题的经典模型叫N-gram&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;它是说我可以把这个猜词的游戏看成一系列条件概率来处理，&lt;span&gt;用这个模拟过去的字母对当下字母的影响。因为我们知道语言本身存在非常清晰的规律，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这个规律就是字母都是成对出现的，&lt;/span&gt; &lt;span&gt;如果全面我给你的字母是&lt;/span&gt;hell&lt;span&gt;， 且还有一个字母，那么你基本就知道是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;o&lt;/span&gt;&lt;span&gt;了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;这个现象我们可以用字母共现概率表达，也就是说衡量一些字母在一起的概率&lt;/span&gt;P&lt;span&gt;（&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;w&lt;/span&gt;&lt;/span&gt;1, w2,w3…&lt;span&gt;&lt;span&gt;）&lt;/span&gt;,&lt;/span&gt; &lt;span&gt;&lt;span&gt;那些经常在一起出现的字母概率会很大，&lt;/span&gt; &lt;span&gt;而其他很小，&lt;/span&gt; &lt;span&gt;我们可以用经典的条件概率公式来表达，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;这个事情。&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.03968253968253968&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQz8rY74Za5wiaIaMLlVu9rZfE2QY8lKzQ2WOQoPnQAzJ1LFGdOY2DUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1260&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个公式的意思非常清楚， 就是字母和字母之间不是互相独立的， 就是刚刚说的后面的字母极强的依赖于前面的字母， 这种关系通过条件概率体现。 这件事我们用一个图画出来，就是当下是所有过去的作用， 而一旦字母多了以后，这件事就特别复杂。我们用物理学家惯用的加入假设来简化降维，就是我当下的字母与过去相关，这种相关却是有限的， 比如只与前面n-1个相关。&lt;/span&gt; &lt;span&gt;这个模型就是n&lt;/span&gt; &lt;span&gt;gram&lt;/span&gt;  &lt;span&gt;。&lt;/span&gt;  &lt;span&gt;比如说 只与前面一个相关，这就是2-gram，&lt;/span&gt; &lt;span&gt;而这个模型就是标准的马尔科夫链。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;那么，这里我们换一个模型来，&lt;span&gt;我们用&lt;/span&gt;rnn&lt;span&gt;来做。具体怎么干？ 每个字母给他编个号码（独热编码），我一个一个的输入给这个网络，网络每一刻的状态取决于此刻的输入和上一刻的网络内部状态，而上一个网络内部状态又取决于过去的，输入，这样当我整个单词输出完毕，每个字母的信息可以看作都包含了在了神经元的状态里。我们要&lt;/span&gt;把整个输入切分成小块，&lt;/span&gt; &lt;span&gt;用一个卷积核把&lt;span&gt;它们&lt;/span&gt;卷入到一个隐层网络里，&lt;span&gt;只不过此处的&lt;/span&gt;&lt;/span&gt;Wij&lt;span&gt;&lt;span&gt;代替了&lt;/span&gt;CNN&lt;span&gt;的卷积核，把历史卷入到一个隐层里。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们用一段小巧的&lt;/span&gt;python&lt;span&gt;代码让你重新理解下上述的原理：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.31364031277150306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kpLMkok673hoAia1JmwT3BCCtSajFRyeRxvM9R2KMeRahYMlNZxULcAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1151&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个结构，&lt;span&gt;非但&lt;/span&gt;优雅，&lt;span&gt;而且&lt;/span&gt;有效。&lt;span&gt;一个非常重要的点是，&lt;/span&gt; &lt;span&gt;我们不必在假定那个&lt;/span&gt;n-gram&lt;span&gt;里的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;n&lt;/span&gt;&lt;span&gt;，这时候，因为原则上&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;是所有历史输入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;I1&lt;/span&gt;&lt;/span&gt;…It &lt;span&gt;&lt;span&gt;的函数，&lt;/span&gt; &lt;span&gt;这个过程，&lt;/span&gt; &lt;span&gt;也就是说我们具有一个&lt;/span&gt;&lt;/span&gt; &lt;span&gt;infinit&lt;/span&gt; – &lt;span&gt;gram&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;&lt;span&gt;你来思考一下这是真的吗？&lt;/span&gt;No&lt;/span&gt; &lt;span&gt;，&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;你知道信息的传播是有损耗的，&lt;/span&gt; &lt;span&gt;如果把&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;展开，它事实上相当于一个和历史长度一样长的深度网络，信息随着每个时间步骤往深度传播，这个传播的信息是有损耗的，到一定程度我就记不住之前的信息了，当然如果你的学习学的足够好，&lt;/span&gt;Wij&lt;span&gt;还是可以学到应该学的记忆长度。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2611683848797251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kFoiaJw1g1Q2BhwvicUsrbnibR4o1JDmfnOIGU1uY1cvSwkongmI6DmOcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;582&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图：循环的本质&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;刚刚的词语预测模型，我们最终的目标是每次预测出现的下一个字母，我们的方法是在隐层之上加入一个读出权重，这个矩阵的作用如同我们之前讲的感知机，是在有效的特征基础上加一个线性分类器， 再加一个softmax函数得到一个向量每个数值代表出现某个字母的概率。然后我们希望我们的向量能够完全预测真实字母出现的概率，因此我们把真实数据作为输入不停的让他预测这个字母，直到这个概率和真实是最匹配的，我们就得到了训练好的模型， 然后我们就可以让他生成一段文本了！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8045325779036827&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kt9lGY722ic5zh0p4qugEHz7MXuAGUH9HJmXaB1ibksd7iadjJSvjLTAHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;706&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上面介绍的过程是语言介绍的最基本的部分，这里所说的对字母进行独热编码这件事，如果进入到文本的世界，你要预测的不是字母而是单词， 这时候，我们通常不再采用独热编码，而你自己思考这件事显然是不合理的，因为词语之间相互关联，如果你把词语本身作为完全独立的东西来编码，你事实上就是丧失了这种本来的语义结构的信息，因而我们用更复杂的word2vec来替换，这个方案的核心依然是编码，只不过这套编码也是从神经网络里学过来的，具体来说，这套编码可以把语言内在的语法或语义信息包含在编码向量的空间结构里， 从而使用这部分语言有关的先验信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9627749576988156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kcnRq0wZJO8FLnRkeIrAmTWoF1W7Idhuz1lS0Lp00fbqySqScyGSx1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;591&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;的物理与工程理解&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们说把对&lt;/span&gt;RNN&lt;span&gt;的理解&lt;span&gt;在&lt;/span&gt;抬升一个层次，RNN&lt;span&gt;在物理学家眼里是一个动力系统，&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;&lt;span&gt;循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖&lt;/span&gt;-&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;路径依赖&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。另一个角度看也符合著名的图灵机原理。&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;即此刻的状态包含上一刻的历史，又是下一刻变化的依据。&lt;span&gt;工程上看，&lt;/span&gt; 这其实&lt;span&gt;就是&lt;/span&gt;可编程神经网络的概念，即当你有一个未知的过程，但你可以测量到输入和输出，&lt;/span&gt; &lt;span&gt;你假设当这个过程通过&lt;/span&gt;RNN&lt;span&gt;的时候，&lt;span&gt;你要设计一个程序来完成&lt;/span&gt;这样的输入输出规律，&lt;span&gt;那么这个程序可以被&lt;/span&gt;RNN&lt;span&gt;学出来&lt;/span&gt;。 &lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.071278825995807&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMUht40XmvRwwC2zjpTEgjibqDiakMScEicDqfDYEz4tnGmlrU44Nf0hQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;477&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;总的来说&lt;/span&gt; &lt;span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;无论作为&lt;/span&gt;一个非线性动力系统&lt;span&gt;还是程序生成器&lt;/span&gt;的&lt;/span&gt;RNN&lt;span&gt;, &lt;span&gt;都需要依然数据背后本身是有规律可循的，也就是它背后真的有某种“&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;program&lt;/span&gt;&lt;span&gt;”而非完全随机&lt;/span&gt;。&lt;/span&gt; &lt;span&gt;如果一旦&lt;/span&gt;RNN&lt;span&gt;学习到了&lt;span&gt;真实&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  数据背后的&lt;span&gt;动力系统&lt;/span&gt;的性质，&lt;span&gt;它也就&lt;/span&gt;掌握了过程中复杂的路径依赖，从而能够对&lt;span&gt;过去&lt;/span&gt;和&lt;span&gt;现在&lt;/span&gt;进行建模。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;生成诗歌&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;诗歌的生成，方法类似于句子，区别在于，诗歌是有主题的，我们都知道主题的重要性，同一个句话在不同主题下含义完全不同，如何把这个诗歌的主题输入给rnn呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;事实上，我们可以把这种主题或者背景看作是一种上下文关系，比如说一首诗歌有四行，在生成第一行的时候，我可以输入开头一个关键词，然后让rnn自动生成一行，虽然这个过程还有一定随机性，但是这一行内容无疑确定了诗歌整体的基调，因此，我们需要把这种信息编码成一个包含上下文含义的内容向量，这个向量作为整个网络的输入。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3133535660091047&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4klibJGiaACy2FrV6BFJ7WN70mlv5gI20U41fV8hQD4v7FKrqEpZicuPhJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1318&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们采取的方法是逐行生成诗歌。已经生成的行就作为后面生成行的基调，每行诗歌的生成都使用和之前一样的RNN，但是，它的输入要加上一个主题向量。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如何提取主题向量呢？首先，它是一种由所有之前生成行构成的宏观信息，那么，我们也可以用一个RNN来提取它，由于宏观， 这个RNN的输入是每行诗歌。而我们要用一个方法， 直接把行编码， 而不像刚刚是把字母或单词编码。 一行诗歌本身每个子都是一个向量，整行诗歌构成一个二维的图像（字数一定， 图像尺寸一定）。记得什么东西最好处理图像吗？我们引入一个CNN网络，把这些同样尺存的“图片”，压缩一个特征向量，最后被这个被称为主题RNN的RCM卷成一个主题向量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4824766355140187&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kCiaqcL6B6pl0WAZxmnxWK42KjPXt6sN79tkHTyicHqrpXt5BpxNBFsog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;856&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个主题向量作为一个先验输入交给RGM， 这个普通的RNN， 作为这行诗词生成的关键。由此生成的诗歌不仅是押韵的，而且可以构成一个完整的意思。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kzn6aYicN4L5a7symMoAvt8vusq6QLdcCzYdSgXJRzks7DgfhDrNibMKQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1001&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;引入长短记忆&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;RNN虽然看起来好用，&lt;/span&gt; &lt;span&gt;&lt;span&gt;而且似乎&lt;/span&gt;能够模拟任何一个&lt;span&gt;动力过程或程序&lt;/span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;实际&lt;/span&gt;中，&lt;/span&gt; &lt;span&gt;&lt;span&gt;却&lt;/span&gt;并没有那么容易。&lt;/span&gt; &lt;span&gt;&lt;span&gt;为什么&lt;/span&gt;？&lt;/span&gt; &lt;span&gt;RNN&lt;span&gt;的&lt;/span&gt;强大&lt;span&gt;功能&lt;/span&gt;，&lt;span&gt;体现在&lt;/span&gt;能够学习&lt;span&gt;过去&lt;/span&gt;时间点对现在的影响这件事，&lt;span&gt;但是，我们刚刚说了&lt;/span&gt;RNN&lt;span&gt;相当于于一个无限深的深度网络， 而传播是有损失的， 假定每次这个损失是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.99&lt;/span&gt;&lt;span&gt;， 经过&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;100&lt;/span&gt;&lt;span&gt;层后也是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.36&lt;/span&gt;&lt;span&gt;， 这种信息传递的递减， 会导致&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;无法学到长时间的信息之间的关联性&lt;/span&gt;。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的一个重要的解决这个问题的技巧是&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;引入一个能够学习多个时间尺度的改进版&lt;/span&gt;RNN&lt;/span&gt; – &lt;span&gt;LSTM（&lt;/span&gt;Long short term memory&lt;span&gt;）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6776556776556777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koTXDZNjRYuG90Osb9NhnCzCCkCCq8FTCC50WSkObHsKBQ1bJEXQf0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;819&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;首先，什么是时间尺度，宇宙间最神秘的概念莫过于时间，但是绝对的时间毫无意义，一个时间的长短，一定是根据你所描述的过程，&lt;/span&gt; &lt;span&gt;&lt;span&gt;比如你是描述一个人一生的变化过程，还是描述一次化学反应，&lt;/span&gt; &lt;span&gt;还是生物进化，各自的时间尺度可以有级数之差。我们对时间尺度最数学的理解来自原子的半衰期。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于大脑神经尺度，处理动态过程最麻烦的东西就是时间尺度， 因为生活中的事情往往是多时间尺度的，比如你的一个决策今天晚上吃不吃饭，可能既受到刚刚是不是饿了的影响，又受到这个月你是不是有减肥计划的影响，还受到你长期养成的饮食习惯的影响，因此， 你的大脑需要有对复杂时间尺度的处理能力。也就是说，同时对各个不同的时间尺度变化的信息保持特异性和敏感度， 这和我们图像识别里需要对图像的局部和整体反应是类似的。一个有意思的电影inception描述要改变一个人的意念，我们需要一步步的走进他思想的最深层 ，逐层的改变它对某个东西的认知，而每个层里的时间尺度又有不同，就是对这件事最好的体现。事实上，类似于&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;inception&lt;/span&gt;的描述，我们的确发现在我们的大脑里，有着不同时间尺度的处理， 越浅层，我们就对越近的东西敏感，而进入到大脑的深层，我们开始对慢过程敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;LSTM&lt;span&gt;，所谓的长短记忆机，这是对这个过程的模拟。我们来看它是怎么对&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;进行改进的&lt;/span&gt;，&lt;span&gt;这个道理非常简单，首先，我们加入一个叫做记忆细胞的概念，进入到记忆细胞的信息，可以永久不被改变，&lt;/span&gt; &lt;span&gt;但也可以根据一定触发条件修改，实现的方法是我们加入一些&lt;/span&gt;控制&lt;span&gt;信息流动&lt;/span&gt;的阀门&lt;span&gt;在这些记忆细胞之间&lt;/span&gt;，&lt;span&gt;这个&lt;/span&gt;阀门随着输入和隐层状态决定，&lt;/span&gt; &lt;span&gt;&lt;span&gt;如果是&lt;/span&gt;1&lt;span&gt;，我们让过去的记忆完全进入到当下，信息丝毫不衰减，如果阀门的值是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;，就彻底的遗忘，如果是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;1&lt;/span&gt;&lt;span&gt;就是在一个时间段里记住这个值就是一个时间尺度。只要&lt;/span&gt;控制好这个&lt;span&gt;阀门&lt;/span&gt;，&lt;span&gt;我们就得到了一个动态的可以学习的时间尺度。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5616161616161616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kibwMIicLVK1VNiclzdj2JchRw3DLa25DdPhEuuVRmEXyfqVSib7N9762qA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;495&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;刚刚讲的阀门被称为&lt;/span&gt;遗忘门，&lt;span&gt;为了配合它，我们还加上输入门和输出们，&lt;/span&gt; &lt;span&gt;控制有多少新的信息流入，有多少输出，&lt;/span&gt; 这三层门&lt;span&gt;整体构成一套&lt;/span&gt;信息的闸门，门的形式都是可微分的&lt;/span&gt;sigmoid&lt;span&gt;函数，确保可以通过训练得到最佳参数。根据这一原理，我们可以抓住本质简化&lt;/span&gt;lstm&lt;span&gt;，如&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;或极小&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;span&gt;。其实我们只需要理解这个模型就够了，而且它们甚至比&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;span&gt;更快更好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看一下最小&lt;/span&gt;GRU&lt;span&gt;的结构：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42168674698795183&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kVcbiaYKKbxIBINvz2ISUyxxTcDHga7tFPdTwAdI3jhg7HVN974FPsEg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2278719397363465&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kkKhNMtqduPrfiaLqFiaHqUncdD0kH3v9iax33BgFxU4Y4eZdDHyfTiaABg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;531&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;第一个方程&lt;/span&gt;f&lt;span&gt;即遗忘门，第二方程如果你对比先前的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;会发现它是一样的结构，只是让遗忘门&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;f&lt;/span&gt;&lt;span&gt;来控制每个神经元放多少之前信息出去（改变其它神经元的状态），第三个方程描述&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;“&lt;/span&gt;&lt;span&gt;惯性&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;”&lt;/span&gt; &lt;span&gt;，即最终每个神经元保持多少之前的值，更新多少。这个结构你理解了就理解了记忆体&lt;/span&gt;RNN&lt;span&gt;的精髓。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Attention&lt;span&gt;版&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;LSTM&lt;/span&gt;&lt;span&gt;与宋词生成&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然，我还可以把这个多时间记忆的东西玩到极致，最终我们可以得到一个升级版本的诗词处理器，&lt;/span&gt; &lt;span&gt;我们要加入几个新的概念：&lt;/span&gt; 1&lt;span&gt;， 双向编码&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;/span&gt; 2&lt;span&gt;，&lt;span&gt;多层&lt;/span&gt;lstm&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;3&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为什么双向，因为语言也可以倒过来念啊，甚至很多时候后面的内容越重要越能决定主题， 比如一句话一旦出现“但是”一定是但是后面的内容更中重要有决定性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.35340314136125656&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kTd77wRVfeOS7WzAsLiaibGGIic5BlVrbUlicVLQ6JaibBC6XdiaS8ibOSacOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;764&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么要多层， 记得我刚刚讲过的大脑结构吗？ 如果每层的lstm学习的事件尺度敏感性不同， 会更好的处理多事件尺度新消息， 比如从词语到句子到段落。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;最后，也是最关键的，看看我们如何引入&lt;/span&gt;a&lt;/span&gt;ttention.&lt;span&gt;。&lt;/span&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Google&lt;/span&gt;&lt;span&gt;这一次&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2016&lt;/span&gt;&lt;span&gt;寄出的大法，是在其中加入了&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;机制 ，&lt;span&gt;首先大家理解人脑的注意力模型，人脑的注意力机制的特点是在认知资源有限的情况下，我们只给那些最重要的任务匹配较多的认知资源。&lt;/span&gt; &lt;span&gt;这个机制实现的方法正是&lt;/span&gt;attention&lt;span&gt;。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，在引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;之前，我们的想法是既然我们最终的决策想利用好所有的历史信息，而每个时间的隐层状态都是对那个时刻时间状态的总结，我们完全可以把所有时间点的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;作为一个特征使用， 这一点， 而不只是最后的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;，这点在文章分类这类任务里特别好理解，但是每个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;动辄上百维度，所以当我们把所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;合成在一起的时候 ，我们就会得到几十万个维度，我们再次抛出降维度找重点的思维， 加入一个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;来吸取和当下预测最重要的信息。&lt;/span&gt;&lt;/span&gt; A&lt;span&gt;ttention&lt;span&gt;又称为&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;content&lt;/span&gt;&lt;/span&gt; &lt;span&gt;base&lt;/span&gt; addressing&lt;span&gt;&lt;span&gt;，因为过去哪个东西比较重要，往往取决于我现在要预测的信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.834070796460177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kr774SyBt7bMVY4qVm4sNr1olrWH3nAz6Nfc4XO4dgPGpBUYva0VR2g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有了这个机制，我们可以抛出我们唐诗生成器的改进版，宋词生成器，宋词的生成，确实是比唐诗更复杂的一个东西，因为宋词更长， 更多变，句子长短不同，也更需要多时间尺度。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8047244094488188&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kUlL5A2z6dcIxib9cCsOFiaGaibVDVrnQepcrmpvHqD2MYiaF43ia7EDDy5Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这一次，我们可以把宋词的结构看作是一种对话体，&lt;/span&gt; &lt;span&gt;我们用一个把问题编码，&lt;/span&gt; &lt;span&gt;然后直接从这个基础上预测对上一句的回答，这里，我们生成第一句词的技巧和之前是类似的，&lt;/span&gt; &lt;span&gt;从这个生成的第一句词开始，我们用编码器&lt;/span&gt;LSTM&lt;span&gt;来把这一行编码作为下一句的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;， 然后解码器把这个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;转化为下一行词。为什么我们要用&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;呢？ 因为上一句的所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;被叠加成一打，刚刚说到的，这时候会造成信息过大，所以我们引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;的机制来注意相应的信息， 这样我们就可以找到上一行和下一行之间精细的相关性。&lt;/span&gt;&lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.9306451612903226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k2zxW29QBMksaJTicu0H9yQibxviapVtlRJSVSHvTctMCyibxLkt3HxeWtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;大家可以比较和唐诗生成的时候， 我们的结构不仅更简洁，而且能够处理更难的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;模拟人类对话&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最终， 我们已经接近了让神经网络听懂故事的境界， 你是否认为类似的结构可以用作和我们对话呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kJ6ZIVojPFnRal0gAGQFFaSicQKYYTQmaJzSTJlYhTWslB3yWOKA6uFA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;476.75359712230215&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;491.23201438848923&quot; data-ratio=&quot;1.0294117647058822&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kqgbTa8IAQ8ofH5QVGuYaKLrsdrItYENGJ2hm9hKBDdxOx6bUicmKXfQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;476&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;因为你我在对话时候有大量的背景知识，&lt;/span&gt; &lt;span&gt;而机器人是没有的，&lt;/span&gt; &lt;span&gt;但是有没有补救的方法？&lt;/span&gt; &lt;span&gt;当然有，&lt;/span&gt; &lt;span&gt;谷歌的问答系统，&lt;/span&gt; &lt;span&gt;已经把很多重要的背景知识放入了神经网络，&lt;/span&gt; &lt;span&gt;这种加入很长的背景知识的结构，&lt;/span&gt; &lt;span&gt;被我们称为记忆网络，&lt;/span&gt; &lt;span&gt;事实上，它是对人类的长时间记忆的模拟。&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这些长时间记忆，&lt;/span&gt; &lt;span&gt;包含知识，或者地图等。用它做的聊天机器人，可以成为你电话里的助手，但是，&lt;/span&gt; &lt;span&gt;你是否认为这样的结构已经具备了真正理解语言的能力呢？&lt;/span&gt;   &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;参考文献&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The unreasonable effective RNN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Minimal Gated Unit for Recurrent Neural Networks&lt;/span&gt;&lt;/p&gt;







</description>
<pubDate>Fri, 08 Mar 2019 14:53:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/o5DC8CEtwb</dc:identifier>
</item>
</channel>
</rss>