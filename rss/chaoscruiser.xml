<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>《人类网络》读书笔记-让连接造福人类</title>
<link>http://www.jintiankansha.me/t/BCyZJ1p6nE</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/BCyZJ1p6nE</guid>
<description>&lt;p&gt;介绍一本19年的新书，英文名是《The human network》，中信近期要出的中文版翻译为《人类网络》，作者MATTHEW O. JACKSON，从他之前写的书《Handbook of social economic》就可以看出作者的背景偏于经济，对于复杂网络不了解的读者，可以从该书了解该领域的基础概念，例如中心节点，最短路径，网络的极化等，还能从书中举的例子中，直观的感受到网络分析的威力。不过对于见惯了用网络结构来预测从艺术家科学家，再到初创企业乃至一个国家兴衰的笔者，这本书的价值不在于具体的例子，而在于其帮助我打通了”结构-特征-功能“这个思考网络相关问题的三角基座。有了这个基座，如果让我来设计一门和复杂网络相关的课程，我就能够将材料放到合适的位置上。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.49453125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfJt3cfJXL7XOGcnoryPFVpRCHxyCzPw7DjV8ccmQqDwdn4Vmr0RibIvP1ibjic7djL5DwtnknLKicd6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不管是人组成的网络，例如社交网络，还是人造物组成的网络，例如网页之间的相互联系，一篇文章中的句子之间，商品或者货币的网络，都可以用连接矩阵来表示，用下面的图来可视化，所谓结构，就是给这个网络按照全局的特征贴上标签，以便于人类理解和分析，例如上面是随机生成的网络结构，下面是社交网络，你说不出这俩附图有什么不同，但就是觉得这俩不应该被归在一类上。这里的归类，自然不是一个非黑即白的，而是描述网络变化的趋势在朝向那个方向。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.27890625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfJt3cfJXL7XOGcnoryPFVpbQ4kGSSw3mEVJ8X9c04ejCmpToQXFVJK4ktLCTsXRWcagrLUBqldAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了极端的或典型的网络结构，就能够自然而然的推出该网络具有的性质，例如下图是08年金融危机时美国银行间的相互联系，看到这幅图，牵一发而动全身的道理真是不言自明了。&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.8294829482948295&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfJt3cfJXL7XOGcnoryPFVpWKiajnudQdgW0Y0Sga1vD4rW02ScFHbgg7v7VnAredYyeial24p01ib4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;909&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而下面这幅图是米国一个高中里有过性关系的相互网络，看了这幅图，你多半会想到“一个老鼠害了一锅粥”，网络的结构之所以有趣，就是由于不同的结构带来了信息传递效率的巨大差别。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.484375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfJt3cfJXL7XOGcnoryPFVpGLJkbR844XYN4kX9yXicFViaocQrCuX9S6SvOSlib5gcH5jbZYQfr2bOQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但结构只是直觉的描述，要科学的进行分析，需要从网络中提取出特征来定量的比较，这就是所谓的性质。比如比较网络中每个节点的重要性，就可以有多种方式，例如degree centrality只是靠数人头来衡量流行程度，Eigenvector Centrality还考虑了连接的点本身的质量，而diffusion centrality则是从新信息传播的可行性上去考虑，而betweeness centrality则是从如果这个节点消失后的影响评价重要性的，不同的性质对应着不同的计算方法，在只有几个节点的模型网络（toy network）上，能直觉的看到不同的计算方法，对同一节点的重要性给出不同的评价。&lt;/p&gt;

&lt;p&gt;性质是对网络结构的抽象，其目的则涉及用网络建模想做什么，是找出网络中关键的节点，从而更快速的在网络中传递消息，或者预测这个网络在面临随机扰动或定点攻击时是否稳健。复杂网络近二十年来的研究，已经积累了很多人类提取的性质，当前图卷积神经网络（GCN）盛行，我觉得就像让DNN预测量子物理的相关数据，DNN能从中推出薛定谔的波动方程，如果针对特定的应用场景，训练GCN去对网络中节点的重要性做回归，那这个GCN预期也会如人类一样，去从网络的连接关系中，提取出对应的centrality计算方式来。毕竟所有的信息抽象都不是为了求真，而是为了求存，尤其对于在模拟进化的GCN尤其如此。&lt;/p&gt;

&lt;p&gt;回到“结构-特征-功能”的思考三角，结构是最直觉的概述，针对一个我们关注的情境，网络的结构可以分为俩个极端，大多数日常生活中的网络，处于俩个极端之中。我们能说看出来08年的金融网络很脆弱，但却说不出不脆弱的网络应该是怎样的。这时加上了多种数量化描述的特征，就可以去比较出细微的差异了。而基于网络提取的特征，就可以做预测，而这也是所有科学理论想要达到的终极目标。&lt;/p&gt;

&lt;p&gt;举个例子，例如你要对一篇文章进行缩写，提取摘要，这是功能。用网络的语境去翻译，就是文中的每个句子是一个节点，句子和句子之间的联系密切程度是边，而要预测的是网络中那些重要的节点，如此就要权衡，到底用那个centrality指标来度量重要性。这个问题的答案也不是一成不变的，取决于网络的结构。体育新闻这样的文章，其句子与句子之间的网络结构，就和散文随笔不一样，正是这种结构上的区别，使得上述的方式，对生成体育新闻的摘要效果不错，而却无法为《瓦尔登湖》这样的随笔或游记生成像样的摘要。如此就完成了功能-&amp;gt;特征-&amp;gt;结构-&amp;gt;功能的循环。&lt;/p&gt;

&lt;p&gt;用一句话来总结《人类网络》这本书的核心观点，可以用一句中学政治课讲过的“人是其社会社会关系的总和”来概括，人所在网络中的位置，即使过去的果，也是未来的因。了解社交网络中和你相邻的节点，可以预测你的收入，未来是否更成功，有什么样的爱好，但这不是否认个人的能动性，只是所处的网络是一个人创新或者改变的基础，为了之后的发展确定了基线和锚点，不了解所处网络中的结构，以及自己在网络中，根据定量的特征，在人群中排在什么位置，就无法实事求是的去以此为前提去选择那些需要改变，从而实现自己设定的目标(功能)。&lt;/p&gt;

&lt;p&gt;《人类网络》关心的另一个话题是网络结构的变化带来的宏观效应，这也可以用“功能-特征-结构”去理解，你需要解释为何会这样，为此你要监控对应网络中的某些特征的变化曲线，从而说明网络正在走向某个特征。例如你想要解释为何在高校中HIV会流行，你可以去看不同时间点，高校学生性关系的网络是怎样变化的。你要解释贫富差距的增大，以及阶级固化，也可以去看求职者，推荐者与面试者组成的社交网络。&lt;/p&gt;

&lt;p&gt;《人类网络》这本书还指出当前的社交网络，面临的最大问题是连接越来越多，但连接的可能性的增加速度超过了人类适应连接的能力，人们没有因为更多的连接而变得更为幸福，更为睿智，而是变得暴躁，越来越不宽容，越来越否认多元化的价值，从而减少了整个社会的social capital，使得人类整体在面临全球性的问题时无法全力应对。该书的第九章讲全球化对网络的影响，这时网络改变的特征就不是针对某个节点或者边，而是网络的分型维数，节点之间度数的分布落在那条幂律曲线上，网络中聚簇形成的簇与簇及不同簇之间的连接的比例等和人群整体有关的特征，而能够做的改变则是至上而下，通过技术去改变或助推（nudge)子网络的生成与演化规则，通过改变每个基层网络的结构，从而影响最宏观的指标，从而做到本文标题所说的“让连接造福人类&quot;。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383791&amp;amp;idx=1&amp;amp;sn=8d64cb59ba4d52cad925858badc4cdb3&amp;amp;chksm=84f3c9aeb38440b8856c467faa2c0786c7b97168f38e9e47b5aa92221a6d2dfa256f1bcfff9d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;11&quot; data-linktype=&quot;2&quot;&gt;Science重磅：“要想成功，快抱大腿！”&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=206380507&amp;amp;idx=1&amp;amp;sn=93ec8fd22d3c9935188b58f55f5d9c0e&amp;amp;chksm=16b6339a21c1ba8c8aaffc05ab53de12efcdc4e8e2e270c20ac51c172f3e3a67c5a1f145aae4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;社交网络的信息进化&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 16 Jun 2019 02:20:19 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/BCyZJ1p6nE</dc:identifier>
</item>
<item>
<title>如何让有监督学习变得有解释性</title>
<link>http://www.jintiankansha.me/t/7rtMhp85bX</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/7rtMhp85bX</guid>
<description>&lt;p&gt;机器学习目前面临的一个大问题是模型缺少解释性，当别人问你，为何我要相信你的模型的时候，大多模型无法给我一个可靠的解释，仅仅提升准确率，并不能解决问题，这限制了在诸如金融，医疗等模型必须解释性，应用机器学习。今天介绍的模型LIME来自17年的NIPS会议，对应的python/R包能够对目前包括图像，自然语言，以及对数据表的传统分类及回归模型给出模型为何为何做出这样分类的解释。这篇小文先讲原理，再展示效果，最后介绍代码。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4110787172011662&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJzib8RRXTXTGe3Nr9p7UHjGp7XBCCAZONQu3UI7wmiavWNia6CicoZF3tCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;686&quot;/&gt;&lt;/p&gt;

&lt;p&gt;LIME的全称是“&lt;span&gt; Local Interpretable Model-Agnostic Explanations.”，局部性假设一个数据点被分类模型做了标记，LIME只会针对这个数据点，利用该点本身的特征对其进行解释，而Model-Agnostic指的是给出的解释不会与你用的是什么样的模型有关，不会涉及模型如何做出分类，这使得LIME具有通用性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;LIME的原理很简单，下如代表的一个二分类器，模型拟合的函数很复杂，而要待分类的数据点X是图中的粗线条红色十字，LIME在X周围随机生成一些数据点，让分类器进行分类，之后按照随机生成的位点和原位点X的距离，来最优化出图中的虚线，这里虚线的斜率代表了该模型针对数据X分类时，各个特征的重要性，以及当数据点发生变化时分类结果会怎么变化，从而解释了模型为何对X进行如下的分类。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6220614828209765&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ9JIlOiaWDfXz3TTWZo8vm1bDhbDXHd2KCGOictxY139IQmtqAd3I4b2Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;553&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;接下来看分类的代码&lt;/span&gt;&lt;/p&gt;
&lt;pre liberation=&quot;&quot; mono=&quot;&quot; menlo=&quot;&quot; courier=&quot;&quot; monospace=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; auto=&quot;&quot; start=&quot;&quot; readability=&quot;26&quot;&gt;
library(&lt;span class=&quot;pl-smi&quot;&gt;caret&lt;/span&gt;)&lt;br/&gt;library(&lt;span class=&quot;pl-smi&quot;&gt;lime&lt;/span&gt;)&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Split up the data set&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_test&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;]&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[&lt;span class=&quot;pl-k&quot;&gt;-&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;), &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;]&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_lab&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[[&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;]][&lt;span class=&quot;pl-k&quot;&gt;-&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;)]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Create Random Forest model on iris data&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; train(&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;iris_lab&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;rf&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Create an explainer object&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; lime(&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;model&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Explain new observation&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-smi&quot;&gt;explanation&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; explain(&lt;span class=&quot;pl-smi&quot;&gt;iris_test&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;explainer&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;n_labels&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;explanation&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;&lt;span&gt; And can be visualised directly&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;plot_features(&lt;/span&gt;&lt;span class=&quot;pl-smi&quot;&gt;explanation&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/pre&gt;

&lt;p&gt;这里的数据集是鸢尾花数据集，用随机森林做了分类，测试时选择了5个数据，需要解释的是最后三句，explain函数将分类器，待解释的特征作为参数，而最后对解释器进行可视化，得出下面的图：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.8895265423242468&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJt1iacBPvNPicW4oK6FYGc2ddxrujia6FFhibqEI4SMoF9jdxiadUCkBKIiaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;697&quot;/&gt;&lt;/p&gt;
&lt;p&gt;针对每个测试案例，给出了一个柱状图，图中给出了原特征中的四个的俩个，柱状图中横轴的那句话代表该特征需满足的条件，柱状图的长度代表该特征在满足该条件对模型做出该分类的重要性，柱状图的上方给出该数据点的分类结果。可以看出，对于分类为setosa，最重要的是花瓣的宽度小于0.4，由于4个数据点中都指出该条件最重要。&lt;/p&gt;

&lt;p&gt;对于回归模型，也能给出类似的结果，下图是波士顿房价数据集，用sklearn的中的随机森林，来做回归后，对模型解释性给出的可视化，这里每个柱状图上的条件按是否对房价提升和降低进行了打分，这可以看成对模型预测结果的解释。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJesToTib9l8eZb1olDUbMsibYzAxMhZr52eE6VHI0fw3OxNicDpMLWQx6Q/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;811&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;192.53956834532374&quot; data-ratio=&quot;0.2379778051787916&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJUoDrOZfeO0sxViaEDFCibRX12hzKImjicxx3yJLcVL2U9b5F2GdPhYlVg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;811&quot;/&gt;&lt;/p&gt;


&lt;p&gt;而对图像数据，LIME可以可以标出图像中那个部分导致了模型导致了分类结果，例如下如被分类为草莓，LIME标出的分类原因正好对应是图片中俩个草莓，&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.3266129032258065&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJBZx3JsRaOwcEYVFcibqicqkOEldRG2nH7ZPJFz8WOX4lpm0f4ACHiaib6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;248&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而下图被谷歌的inception从高到低按可能的概率给出不同的分类，LIME可以解释不同的标签为何将图片看成是电吉他，木吉他或者拉布拉多狗。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.38896746817538896&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ1icqKia7fTMonN0v64IicguwXowW0ibic8KWOvX7Cobf8gMbibKuz9fHsNeA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;707&quot;/&gt;&lt;/p&gt;

&lt;p&gt;这里展示的是文本主题分类中，对新闻主题分类进行解释的可视化，这里柱状图的横轴是待分类文本的关键词，这段文本的主题被分类为无神论，而不是基督教，究竟是哪些关键词导致这样的分类了，这里不同的关键词，他们支持的分类结果及对应权重。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.31432360742705573&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ665C8B2UxJGckDXGcgrL4YzuLF59V6PPasOP7qDXuc6tpUHADl6ejw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;754&quot;/&gt;&lt;/p&gt;

&lt;p&gt;LIME还针对文本分类，做了可视化的交互工具Skiny，来互动的展示文本中对分类影响大的关键词。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.7311715481171548&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJSYzorojegCELibibNLTgUc7qVNm0ulTwyp9m1YTq5YfOxOaO9lWLlBbA/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;956&quot;/&gt;&lt;/p&gt;

&lt;p&gt;总结一下，之前提升模型解释性的尝试，只能依靠树模型给出的特征重要性，或者看去掉了那些特征，模型的准确性变差较多，或者看那些特征的相对梯度较大，LIME提供的通用框架，对促成可解释的机器学习，有所帮助，找到了关键特征，对建模时的缺失值补全，数据清洗，以及数据增强，都会有所助益。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384403&amp;amp;idx=1&amp;amp;sn=ab94d9446b1bec4d6ecb3954aeaee7c2&amp;amp;chksm=84f3c412b3844d048b4eff968fa92d2791bd6870b14fc1fa5c9cde9d765e9ef25008c9d92021&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;图像分类中的隐式标签正则化&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384338&amp;amp;idx=1&amp;amp;sn=ef349fadd766aca33d0499ea72425258&amp;amp;chksm=84f3c453b3844d457c19473e8299647b320a292c3544505380345f9f530d6fd83d1c1d5b14fa&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;GAN的五个神奇应用场景&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Jun 2019 03:18:04 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/7rtMhp85bX</dc:identifier>
</item>
<item>
<title>对神经网络进行翦枝-让深度学习变的可以训练</title>
<link>http://www.jintiankansha.me/t/91d8x3ee9U</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/91d8x3ee9U</guid>
<description>&lt;p&gt;今天这篇文章算是ICRL（International Conference on Learning Representations）中标题最有趣的一篇了，也是对实际训练很有指导意义的一篇文章。这篇文章的主要观点一点都不难懂，也没有多少复杂的公式推导。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.2976694915254237&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccI2O8uL63WlDfTDccwGsGVZANnBeamNt6ud90yCwP8JPOr7sKcQeXBmKYsJVzNYnpaFA36OvUmjw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;944&quot; /&gt;&lt;/p&gt;


&lt;p&gt;对训练好的神经网络进行翦枝，去除其中不必须的权重项，可以去除网络中90%的权重项，同时不会影响网络的在分类等问题上性能。通过网络翦枝，可以减少网络的大小，减少计算所需的能耗及计算时间，从而提高使用网络做推理时的效率。&lt;/p&gt;

&lt;p&gt;但问题在于，为何要在训练完成后再去对网络进行翦枝，去除那些&lt;span&gt;当网络收敛到一定程度的时候对网络做分类任务时贡献度较小的权重，而不是直接用一个更小的网络架构来训练。传统上对这个问题的回答是，如果一开始训练时的网络架构就相对较小，模型的容量有限，会导致最终训练好的模型效果相比翦枝后的模型相对较差，因此要先去训练一个大的网络，在通过翦枝对网络进行加速。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而今天介绍的研究则是问，如果在训练的时候就开始对网络随机去除一些权重，而且在训练过程中一次次的去做翦枝，那么网络在分类精度，以及训练所需的时间上，有什么区别了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.54627539503386&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9x53m91FCBnjup6t8ic8W2qW3QsmEkMLpP8gTvibjjbNkxIckTWLq0aFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;443&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上图展示的是针对MINST数据全连接网络（红色），以及用于分类的卷积网络的CIFAR（绿色与黄色），在训练过程中，在什么那个epoth需要停止训练，以避免过拟合，用该值来代表训练所需的计算时间，横轴是随机去掉多少比例的权重，不同的线型代表训练完成后和训练进行中进行翦枝。相比翦枝前的稠密网络，翦枝后的网络训练所需的轮数（epoth）普遍更小。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.44719101123595506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9WCEP9gHRd5Vbt2xk3T1RuHUpNufldWib2ERrHCpClWatgKYo9qKGGhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;445&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上图展示了针对上述俩个网络，比较了分类精度的变化，注意横轴最极端情况下，进行翦枝去除了99%以上的权重。但模型在分类准确度上，以及分类所需的epoth上，都没有出现特别明显的下降，但如果训练中迭代式的对随机选择权重进行翦枝，和训练完一次性的去除对分类帮助不大的权重，在分类的精度上，就存在着10%-20%的差距。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.31385281385281383&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9HaPQGR7Z84o7cuyicIEibWqyXiav87Lj1aAC5uc4c9ddibianIy6289cH1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;924&quot; /&gt;&lt;/p&gt;
&lt;p&gt;本文用到的网络的具体参数与大小&lt;/p&gt;

&lt;p&gt;&lt;span&gt;由此作者给出了神经网络中的”lottery tickect“彩票猜想，即对于一个稠密的网络，其中一定包含一个子网络，当该子网络单独训练时，在分类准确性和训练时间上和原网络近似。这样的子网络作者拟人化的称为”彩票中奖者“。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;那图中另一种线条代表的训练过程是怎样的了？首先是随机生成一个稠密网络结构，然后做反向传播来对权重进行优化，再去除优化后网络中权重较小的部分，之后对剩下的网络结构中的权重随机，不断重复上面的四个步骤，迭代式的找出”中了彩票“的最优子网络。&lt;/p&gt;

&lt;p&gt;假设要去除的网络权重为p%，去除过程包括n次迭代，那每次去除 网络中p^(1/n) %的权重项，而不是像传统中在训练后一次性全部去除。由于迭代式的翦枝，在每一次的训练过程中，都需要重新对网络的权重进行随机初始化，正是这个操作，导致了模型的准确性不如训练后一次性翦枝，这反过来说明了在深度学习中，对网络权重的初始化，对提升网络的精度影响很大，不应该简单的用随机值来初始化网络权重。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;该实验引申出的另一个推论是翦枝剩下的网络结构本身和训练数据强相关，从“中了彩票”的网络结构中，甚至可以推出到底是训练数据本身的规律，至少这个网络结构展示了训练数据中存在的偏向性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;综合训练精度和训练时间，如果一开始训练的网络结构本身是一个相对稠密的，包含更多权重的，那在模型的准确性上要好，也更容易训练，因为从一个密集连接的网络结构中，找出最优子网络的几率更大。如果先通过翦枝，找到原网络结构里“中彩票”的子网络，那么模型的训练所需时间会相对较短。&lt;/p&gt;

&lt;p&gt;该研究的猜想如果在更多的数据中重现，那可以用其来降低神经网络训练所需的计算资源（在训练过程中进行权重翦枝），也可以用来提升模型的精度（通过对最优子网络的研究，指导设计新的网络结构及初始化策略，或者将一类数据对应的最优子网络结构用在其他类型的数据上），还可以对加深对深度学习背后的理论基础的研究，例如确认随机梯度下降的优化策略是不是等价于迭代式的选择子网络对其进行优化。&lt;/p&gt;

&lt;p&gt;人类大脑的发育，在幼儿时期，先让神经元之间自由的构建连接，到了童年，再去除多达80%以上的多余连接。神经网络和大脑发育呈现出了相同的趋势，这在AI中并不是特例，更多这方面的进展，参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该研究并不是百分百完成了的，其中考虑的神经网络，和当前研究用到的网络相比还是比较小的，不确定该文的猜想是不是适合更大规模的神经网络。同时该文也只考虑了计算机视觉，对于时间序列数据以及NLP相关的任务，该文的猜想也没有相应的验证。另外该文中对网络进行权重翦枝的方法，只是基于当个权重的大小一刀切，对于新的翦枝方法，例如对网络中的一个局部整体进行翦枝，不基于权重大小的翦枝选择策略，该文都没有涉及。随机生成的网络要想有效，需要足够大，本文考虑的网络大小，对比两者网络翦枝策略，其实不是一个公平的起跑线。对如何从翦枝最优网络结构中如何提取与训练数据有关的特征，本文也没有进行讨论。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384403&amp;amp;idx=1&amp;amp;sn=ab94d9446b1bec4d6ecb3954aeaee7c2&amp;amp;chksm=84f3c412b3844d048b4eff968fa92d2791bd6870b14fc1fa5c9cde9d765e9ef25008c9d92021&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;图像分类中的隐式标签正则化&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384360&amp;amp;idx=1&amp;amp;sn=9e7a1c88f8dc373730bc5933abd1c616&amp;amp;chksm=84f3c469b3844d7ff3e6cd1d3f9c728f583063c0a95c6ad21596d2d66a2dfd8a2bdc205eadad&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;随机网络中的智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 08 Jun 2019 17:22:20 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/91d8x3ee9U</dc:identifier>
</item>
<item>
<title>图像分类中的隐式标签正则化</title>
<link>http://www.jintiankansha.me/t/fnyrIit0Tv</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/fnyrIit0Tv</guid>
<description>&lt;p&gt;正则化是深度神经网络中必不可少的一环，传统的正则化是在损失函数中加上一项与标签本身无关的惩罚项，去阻止模型变得过于复杂。在这周的Science Robitics上，刊登了一篇只有俩页的小文，概述了一种全新的正则化方式，并指出其俩者具体的实现方式。好文章不必长篇大论，说清楚突破点，指出进一步阅读的方向即可。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.3263246425567704&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqnvs1PUr6Bb9dNeVAsQNIfQHT4WzTEOyYcvxdbmOuiaB8HOPOLEnSKL2w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1189&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正则化的目地是为了提升网络的泛化能力，而过拟合的源泉来自模型对已有数据的死记硬背。而在现实生活中，你给一个幼儿园的小孩子一堆动物的照片，然后给Ta一张豹子的照片，让其从中选择十张和豹子最类似的照片，一个孩子拿来了九张豹子的，还有一个大黄狗的照片，另一个孩子拿来了8张豹子的和一张狮子以及一张老虎的照片。你觉得这俩个孩子哪一个更聪明，对豹子的概念理解更深入。如果单看选对的数目，那第一孩子获胜，但是由于第二个孩子选的的都是猫科动物，因此可以说第二个孩子对豹子的本质有更深刻的掌握，Ta不是在死记硬背，下次遇到白色的雪豹，第二个孩子也更有可能将其归为豹子的一种。&lt;/p&gt;

&lt;p&gt;将这个例子用到有监督学习的语境下，就得到了下图：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.36655405405405406&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqn0Kfh3CoqvZWQ3Fa4XlFBYrtaSnDnlCjE5rgfYqnPxMcMVKBZKoboXA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1184&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;摘自&lt;span&gt;Toward principled regularization of deep networks—From weight decay to feature contraction&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;左图展示的是传统的损失函数下卷积网络训练完成的结果，图中的数字代表模型给出的不同标签的置信度以及待训练真集对应标签的置信度，可以看到图中正确的结果豹子的值最大，而猫这个和豹子是一类有些类似的标签和其他错误的标签一样，其置信度都很低。而在右图中，训练好的模型给了猫这一标签更大的置信度，这会导致模型在训练时更多的犯错。但正如Hinton所说，不同错误之间的相对比例能传递很多的信息。同样是对照片进行错误的分类，将豹子错判成猫的模型比将豹子错判成汽车的模型从常识上就更靠谱，因而也具有更强的泛化能力，这正是本文提出的隐式正则化。&lt;/p&gt;

&lt;p&gt;具体怎么做，直接的方法是将训练的真集（ground truth）中不同标签的置信度进行改变，这等价于告诉模型，对豹子的图片进行分类时，要押八分宝认为上图中的豹子是豹子，还要押俩分认为图中的豹子其实是只猫。这样做不用改变模型本身，但找到一个合适的真集置信度，来提升模型的泛化能力，却不是一件容易的事情。大规模的数据不应该依赖手动的标注，而应该训练神经网络，找出怎样的真集能提高分类网络的泛化能力。&lt;/p&gt;

&lt;p&gt;另一条路是在损失函数中增加一项(例如不同标签间的交叉熵)，来使得模型训练后的预测具有更大的信息熵，在上图中，对比俩张图给出的预测结果，左图知道了模型预测的是豹子的置信区间是0.95，那其他的信息的重要性就很低了，因为这意味着次高的置信区间也只有0.05，而在右图中，知道了图片的标签的是豹子的置信区间只有0.75，那其他的标签的置信区间还是有一定的信息量的。通过在损失函数引入新的组成部分，以惩罚那些信息熵较低的模型，最终使得模型训练后倾向于右图。由于引入的惩罚项不是为了避免模型过于复杂，而是为了避免模型钻牛角尖走极端，因此和传统的正则化方法有本质区别。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.2759170653907496&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqngJaNjdCLKXqpmhr9qhGhyWJSBpvwTOicRLiaEbUKEoX4FCaVT0Elv9aw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;627&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;摘自 REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;这里展示了不同的正则化方法下，在MINST数据集下不同置信区间的概率密度，相对来看越均一，说明模型泛化能力越好，这里拿来和常见的dropout对比的是之前提到的在真集标签上进行的smothing与对过于“自信”的模型施加惩罚项之后的效果。其中效果最好的confidence panelty。&lt;/p&gt;

&lt;p&gt;另一个正则化的方法则是在神经网络每层的每一个特征中加入L2正则项，这可以形象的称为“特征缩水（Feature contraction）”，这样做的好处是对网络中每一个隐藏层的特征都加以缩减，不管是正值还是负值，只要这个特征本身过大，那就会通过正则项让这一特征缩水，从而最终让模型判定的标签不像之前那么自信。下图展示了MINST数据集上不同正则化方法的效果，可以看出特征缩水的效果是最佳的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.6717241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqnwIia2qM9ldjPe2XRiaJJhyxUOibUhr41DmcFfNOUK8lZ38SBNWRCmpOlA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;725&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源：https://github.com/VladimirLi/feature_contraction_example&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这篇文章的标题是走向一个有原则（principled）的正则化模型，文中不止介绍了上述的正则化方法，还指出了例如权重衰减在本质上，也是和上述的方法要达到同一个目标，即避免模型对一个分类标签孤注一掷。从优化的角度来看，如果训练的最终目标是类似图一中左边那样的模型，那其梯度在最终下降时会减慢，而让模型的训练结果类似右图，则可以提高模型优化过程中的梯度，这也解释了为何上图在训练epoth不多时，特征缩水的效果就优于其他的方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;顺着这个思路去扩展新的问题，在对于深度模型，总会有针对其的攻击样本，仅仅通过改变图像中几个像素，就可以改变图片的分类，在模型中引入隐式的关于不同分类之间的知识，能否部分避免对模型的攻击了？能否在除了监督学习的领域，例如在强化学习中的奖励函数的设计时，考虑到不同策略的相似性和从属关系，或者在评价分监督学习聚类的结果时，按层次对不同类型的聚类错误给予不同的权重，大类都错误的聚成一簇的给予更大的罚分，而只是在同一大类中聚混了则不那么严重。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最后想说几句无关的话。对于神经网络中的正则化，曾写过一篇&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384281&amp;amp;idx=1&amp;amp;sn=48c0f5a02ea2c2f8b0582fc9356ed668&amp;amp;chksm=84f3c798b3844e8e759a9b253b8d3fc34ad86089ed9ed2135365499ee12a61ba3dba0cdb7e0f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;预测神经网络预测准确性的普遍理论&lt;/a&gt;，也是偏理论的文。但正则化的种种技术，在生活中反映出来，都是让我们学着谦虚，比如最常用的drop out，大白话来说就是每个人都是社会中的一个螺丝钉，离了谁都照样转，不要把自己想的太该死的重要，比如weight decay，说的是你在向你的上级传递信息时，不要让自己显得过于权威（减少你输出特征的权重），而本文提到的&lt;/span&gt;Feature contraction，则是让你把收到信息中那些极端的部分不要听风就是雨。近来舆论界俩级分化，各国皆是如此，信息茧房，也是越演越烈。这正是整个社会层面的过拟合啊，只关注眼前的利益和斗争，需要的正是有更多的人，懂得上面所说的三条正则化技术背后对应的哲理。&lt;/p&gt;

&lt;p&gt;今天儿童节，所谓的找回初心，可不对应的是正则化方法中另一个常见的early stopping吗？而另一种所有神经网络都会用到的正则化方法随机梯度下降及其改进版则让我想到最近流行的一句话：“鸿蒙初辟原无姓，打破顽冥须悟空”，这话太文雅了，技术一点看，就是全局最优解本没有固定的位置，要想走出局部最优，需要领悟到空（即随机）的价值，粗一点的话是刘欢那首歌中唱的：“心若在，梦就在，人生不过从头再来”。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384360&amp;amp;idx=1&amp;amp;sn=9e7a1c88f8dc373730bc5933abd1c616&amp;amp;chksm=84f3c469b3844d7ff3e6cd1d3f9c728f583063c0a95c6ad21596d2d66a2dfd8a2bdc205eadad&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;随机网络中的智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384326&amp;amp;idx=1&amp;amp;sn=b8f75655a8cfe79b67ce7263c50f1bba&amp;amp;chksm=84f3c447b3844d513b51c1efbef9f067d089584f6acf114b0956a353d153d9b12aeaeff3da52&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当机器学习遇上进化算法&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;







</description>
<pubDate>Wed, 05 Jun 2019 01:02:44 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/fnyrIit0Tv</dc:identifier>
</item>
</channel>
</rss>