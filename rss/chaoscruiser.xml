<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>让神经网络看懂图像</title>
<link>http://www.jintiankansha.me/t/C8XQvUfjg6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/C8XQvUfjg6</guid>
<description>&lt;p&gt;视觉的重要性毋庸置疑， 你可以想象，我们平时的生活， 从识图辨物， 到读书看电脑， 哪一个离不开视觉。 所谓的互联网信息大爆炸， 你看看我们手机空间里的大部分图片是什么， 一定是照片。 所以， 我们说视觉占领了我们信息的主体。这背后深层的原因是视觉相对听觉或触觉对真实世界的信息效率大的多， 一个图片可能包含很长一段文字的信息， 这点是其它渠道所不能比拟的。生物进化出视觉而有了寒武纪大爆发， 那么让机器拥有视觉能力， 一定是让它变得更聪明的第一步。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.37558062375580625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koFQJ0KyYo2twh0dBzVDic9bb8quJZRv5c5pBP6oEFiafTobNPhstZugw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1507&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;人脑对图像的认知：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;电脑记录下来的图像是由一个个像素构成的，每个像素又分为r，g，b三个通道（可以理解为垂直排列的三个像素），这三个通道起到复现整个光场的作用。而事实上物理里的真实的图象， 是一个由无数光子组成的电磁场， 这个电磁场在我们的视网膜上振动， 从而形成了我们对图象的感知。 因此， 归根到底 ，我们是用大脑， 而不是用双眼来感知图象的， 也许我们永远无法知道真实世界是什么样， 但是是我们的大脑赋予了它形象， 一个很好的例子就是你分不清猪的美丑， 但我想猪是可以的， 这正说明了所谓的相由心生， 你不关心， 就看不到。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;那么什么是我们大脑处理图象的神经基础呢？  多少代的科学家研究这个问题， 最终有了一个比较完备的答案。   一个眼睛正常的人不一定能够产生对视觉的知觉， 有一种叫视觉认知症的人： visual agnosia，   它们虽然可以看见物体， 却无法区分一个物体时什么， 课件， 看到， 不等于知道。 事实上， 大脑对视觉的感知主要时通过视觉回路实现的， 这个视觉回路的概念 ， 主要是通过视皮层V1-V4完成的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5879120879120879&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kb3jh8KiboexvaqpcbeXAAWHYPVGfrYdGsW44hGIrAbGegiadpFme1RqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;910&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 这个v1到v4的视觉回路，  本质起到的作用是一级一级的筛选视觉特征。  我们之前讲过， 每个细胞都相当于一个小的特征检测器， 而我们事实上发现， 这些小的特征选择器所检测的目标是不同的， 有的对简单的特征敏感， 比如桌子的轮廓边角， 有些对复杂的特征敏感， 比如桌子的腿或边角， 一个重要的假设是复杂细胞形成的基础正是简单细胞的组合， 很多简单细胞的输入构成了复杂细胞。  而最“复杂”的一些细胞， 居然会对那些抽象的人名，物体概念敏感。  为了表达这种极端的特性， 我们把这类极为复杂的细胞称为“祖母细胞”就好像每个人的脑子里都有那么一个细胞对自己的祖母是反应的， 它就是祖母的代言。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0314465408805031&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kOlUDudCyB33jMJ5wnvGPibNKORj7lBVRqKJp2zNgicp8RBjwEIPibGia0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;636&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;这种表面的“简单”， “复杂”其实可以被一个称为为层级编码假设的理论解释， 说的是比较底层的细胞先得到从视网膜传来的视觉信号（类似数码相机的图象）进行处理， 然后所谓的“复杂”细胞， 无非是把最底层的特征拼接组合起来， 得到比较了比较复杂的特征。而最终当我们得到的祖母的头， 鼻子， 或眼睛这些特征的时候， 在最后进行一次综合就得到了“祖母细胞”这种复杂概念的对应物。当然， 这只是粗浅版本的视觉编码机制。 很多人认为除了层级特征， 视觉编码还需要具有集群编码的特性， 也不一定存在那么一个特定的祖母细胞， 而是概念被一个细胞发放的集体模式所表达。 这些我们就不一一详述了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模拟人脑的CNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如何把图象让机算计处理呢？ 我们可以在深度学习兴起以前， 这是一个超级超级难的问题。 我们就拿机器视觉最简单的例子： 图象分类来说。我们前两节课讲过应鸢尾花的识别， 在这个例子里， 我们看到的实际状况是花的照片来了， 然后我们的植物学家告诉我们花瓣的长度和宽度是重要的特征， 它可与把鸢尾花分为三类， 这样，我们的计算机就可以用前面讲过的KNN把花分成三类。这个方法里， 计算机事实上接受的一个表格数据， 也就是花的特征总结， 而得到一个分类的结论。 非常可惜的是， 这和真正的图象识别相差甚远。 因为真正的图象识别，意味着我们直接把图象，也就是我们看到的原始数据给计算机处理。 或者说， 计算机需要自己找出像花的长度和宽度这样的特征。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7663197729422895&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKiadibV0TFibYrLIgxtiapoYGNARzgOMy6SiaiaTUa01jNaakulibgMCDo9bA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1057&quot;/&gt;&lt;/p&gt;

&lt;p&gt;刚刚说了， 计算机眼里的图象是一个巨大的矩阵， 首先图象由像素组成， 每个像素就是一个数字， 它代表我们对信息的采样。 像素组成的图象是黑白的， 然后我们需要对不同波段的光波分别形成这样一个黑白图， 然后把它们拼接在一起，得到我们最后的彩色照片，比如我们拿一个日常的3x256x256的图像看， 那个像素就是256x256个，然后有三个色彩通道。 如此组成了一张图片。最终这个图象这样的信息维度是巨大的。 远非机器学习的常见问题可以比拟。&lt;/p&gt;

&lt;p&gt;让机器来直接看图，这个在过去看似不可能的技术，被一个叫卷积网络的东西给解决掉了，在2012 ，它超过了所有的视觉算法， 并在随后几年在很大的数据集上赶超人类。这个卷积网络正是对刚说的生物神经网络的直接模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是卷积&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;你要理解卷积， 只需要理解一个东西叫滤镜， 人类在处理图像问题的时候， 最有名的发明莫属photoshop了， 在ps里你可以把图片调整各种各样的色调，模糊，锐化， 这些东西统统是一个叫做滤镜的东西做出的。&lt;/p&gt;
&lt;p&gt; 滤镜这个玩应， 你可能想到镜头前的镜片，事实上，它所做的事情是把图像转化为 一个另一个图片。 它是怎么做到的呢？ 数学上的操作，正是今天讲的卷积。数学上， 这些操作对应的运算都有一个特点， 就是对局部的信息进行综合 ，得到一个新的信息。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5936352509179926&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNMkicsrDnx2xgaJqavCTW5TiaicO92Sj5mIUn7SZ58zAFTs18pHkyicdPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;817&quot;/&gt;&lt;/p&gt;

&lt;p&gt;看看卷积的数学操作，卷积，顾名思义， “卷”有席卷的意思，“积“ 有乘积的意思。 卷积实质上是用一个叫kernel的矩阵，从图像的小块上一一贴过去，一次和图像块的每一个像素乘积得到一个output值， 扫过之后就得到了一个新的图像。我们用一个3*3的卷积卷过一个4*4的图像。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6256627783669141&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kiafx1W2wMDMpcpjib6VfT1xRjogT5ptswDhOPM6X16H3TTcteENjmpcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;943&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积网络的基础正是这样的卷积， 我们说通过一个滤镜我们可以提取一个图象的特征， 那么为什么我们要采取看起来这么笨拙的一个方法呢？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图像识别与降维&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其实要让神经网络告诉某两个照片是香蕉还是苹果， 并不是那么难， 但是图像识别的根本目标是你要识别整个世界的香蕉或苹果， 这个问题背后的核心是我们之前讲的泛化。  也就是让它理解苹果这个概念。当然你可能会想到苹果是红色， 圆形这种具体的特征，这些特征变化了，它就不是苹果了。 但是我今天要说的是， 你要让计算机来学到这个东西， 你要想的是反过来， 那就是， 什么特征变化了， 它还是一个苹果？&lt;/p&gt;

&lt;p&gt;首先，我们想到的是， 一张图象是跟苹果还是香蕉， 首先一定不取决于它所处在图象中的位置。 这个东西叫位置不变性， 或者叫&lt;strong&gt;平移不变性&lt;/strong&gt;。我们把这个特性直接写到神经网络里， 就是卷积。 什么意思， 卷积就是拿着一个恒定不变的小型矩阵， 一行行的扫过整个图像， 这样得到一个特征图。 你的苹果无论出现在什么位置， 对应我的卷积扫描这个行为， 事实上得到的结果都是一样的， 数学上说， 就是你的图像如果移动了5个格， 它在特征图上也做同样的一个移动， 别的什么都不变。 能够满足这种条件的运算-就是卷积。&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicEJJke9nugZB031A0zcCJglstLuNPuGjxZahlHk4qFzGDJ4aqSj2vw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;362&quot;/&gt;&lt;/p&gt;
&lt;p&gt;卷积被当成先验信息写入，每次卷积都对应一个神经元对图像的一个小块进行信息提取， 而每个神经元与输入的连接系数均是一致的，这个特性叫做&lt;strong&gt;权值共享&lt;/strong&gt;。不要小瞧这样一个简化，有了这样一个简化，我们的神经网络得到正确的解就好了很多。用一个术语就是， 我们把问题的维度减少了。 抓住一个不变性， 你就可以把需要解决问题的维度指数级别的减少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;激活函数：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;事实上完成局部特征检测这一步，我们还需要一个东西，就是激活函数， 这个我摸嗯上节课已经讲过了，一般这里用的激活函数是relu，它的作用是把一个信号里为负的部分变成0，你可以把这看成特征提取的实现，更本质的说，如果没有激活函数，我们的神经网络将是一个巨大的线性回归而已。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7770034843205574&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k3LIxRYO5HB5yXEjicsicJ68fOBCvKOMSqnqQBZFvuMW6LOyZV1a1mibJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;287&quot;/&gt;&lt;/p&gt;

&lt;p&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;ReLU函数是小于0是为0，大于0时为自身&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是通道：&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;我们以一个手写数字的历程为例，讲讲我们还需要什么，首先我们说， 一层卷积对应一个特征， 但是，显然一个特征是不够识别的。 就以识别数字为例子降价给你这个问题， 比如你要识别10个数字，仅以1和7为例子。显然识别它们的核心方法就是条纹走向。横线是一个特征，竖线是一个特征。如果一个卷积对应一个特征，那么我们其实需要两个卷积，让一个卷积核可以识别横向条纹， 另一个卷积核识别纵向条纹， 这个操作就可以。&lt;/p&gt;

&lt;p&gt;这样的操作，使用如下的3x3卷积就可以了： -1，1， -1  ，  这样的算子具有和之前提取梯度的运算差不多的样子。只要两种卷积核可以做到这点， 然后，如果我们把这两个卷积核组成一个小组扫描一个特征， 那么我们就会知道每个图像小块上的横竖情况。&lt;/p&gt;

&lt;p&gt;比如这时候我们得到每个图像小块的一个特征编码，一共有四种情况 （0，0），（0，1），（1，0）（1，1），横线对应（1，0）竖线对应（0，1）， 你是不是可以把整个数表看成一个新的图像 ？ 而这个新的图像里的变化从（1，0）到（0，1）是否相当于一个角度呢？ 这就是比条纹走向更高级的一个特征。 怎么提取它？如果你的答案是再放入一层卷积， 恭喜你答对了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4789272030651341&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kxd2XRKOic2My0D0qappe2qgI8L3KYvLk6khicEElAict1x1SKCCQWzXxg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;回顾整个过程，我们要做的无非是在第一卷积层的位置上， 放两个并行的卷积核， 一个核处理横向条纹，一个处理纵条纹， 得到两组不同的特征， 最终我们在前面的两个特征之上读取这组新生成的特征图之上的特征。 下一层卷积寻找的上一组卷积的特征组合。这个操作对应的是在两张并列的图层之上，在它们的同一位置识别信息， 如果两个警报器均响了，说明夹角存在，  我们依然可以用一个3x3卷积网络来完成这个操作，这个新的卷积建立在之前的纵横两组卷积之上，对原先的横纹和纵纹组成的特征空间进行操作（因为这里的维度是2x3x3，最单纯的情况我们也可以用一个2x1x1的一个矩阵综合两个特征）。  因为这个时候， 对之前平行卷积的结果做一个综合， 以及形成一个特征之特征， 即横向和竖线交叉的特征。&lt;/p&gt;

&lt;p&gt;这样的方法无论手写数字出现在什么位置，  我都给你找出来。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;从两层到多层：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚说 kernel就是通过计算小区域内像素的关系来提取局部特征，而最常用的卷积核大小是3x3， 那么这里的一个问题是， 为什么要这么小， 为什么要提取一个局部信息？我们说因为图像这个东西里包含的信息具有以下特点： 最底层的信息，比如边角轮廓， 都存在于局部之中， 只有更上层的信息，比如物体的概念， 才会用到更多部分的信息， 而这种跨度又是逐步发生的。那么，如果实现这种跨度呢？  答案： 多层。局部特征，在更高层上被组合， 会变成整体特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42369186046511625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKqsX2xB7m1ZnmrTCNuF12RThK0PURyTC5mgzPrDDw2PZpEcS3SF0vw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1376&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  首先，我们把每个神经特征所提取的特征区域， 叫感受野，如果我们始终只能用的其实都是3x3这样的小卷积核， 我们能不能让感受野扩大呢？ 答案是， 可以。 这里的关键是一个叫池化的造作。&lt;/p&gt;

&lt;p&gt;最大池化所做的是事情，是把每四个相邻神经元得到的数值取一个最大的， 其它全部扔掉。每次卷积后如果经过这样一个操作，那么图像就会缩小到原先的四分之一，而再次之上的相邻四个像素， 对应了原始的16个像素， 从而使得感受野迅速扩大。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5582706766917294&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kA57IKwRcHrqUjq6SVb77Sd3ibgSvvcDxNOzRHgaBrmiazv7e1Yrb209g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1064&quot;/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Pooling的本质依然是降维，或者过滤冗余信息，这个就是pooling。背后能够这样做的理由是，局域特征特征是大量冗余的 ，经过条纹提取的数字一定在大量临近区域里的数值都一样。 冗余踢去后， 经过pooling， 上层细胞得到更大的感受野，也就抽取了更高层次的特征。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积层 ，激活函数，pooling帮我完整的特征提取到剔除冗余的过程 ， 这可以称为卷积网络的三明治， 把这个结构不停迭代，我们可以构建一个很深的网络。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.363479758828596&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMARlAiaha7Tp0OAyibiclNr6c5R7PS7uCicOZjRjCkZP35apJUxgq2rJnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1161&quot;/&gt;&lt;/p&gt;

&lt;p&gt;深度意味着什么？  我们想一下， 要正确的识别一个图像，你不可能只看边，也不可能只看角， 你要对图像的整体有认识才知道张三李四。 也就是说我们要从局部关联进化到全局关联， 真实的图像一定是有一个全局的，比如手我的脸， 只有我的眼镜，鼻子耳朵都被一起观察时候才称得上我的脸，一个只要局部，就什么都不是了。如何提取全局特征？ 从一个层次到另一个层次的递进， 通常是对上一层次做横向以及纵向的整合（图层间的组合或图层之内的组合或两者）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4612005856515373&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicPBaImgyWqzuJNtkEKMbrIrkzPSdp6UWZyq9NgShHPJb3P6MtCmibjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot;/&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚讲了CNN如何找到边角的过程， 但是它的下一层会是什么？再下一层会是什么？ 我们头脑中的想象力已经不够了。我们只能做让学习得到结构，然后去观测。我们可以把每组卷积网络看做一组基，我们在这组基上重构我们的信息， 就和线性代数里坐标变换相似，只不过非线性更复杂。 每一级别的网络都是一组新的基底，我们把刚刚的全局换一个词叫抽象。深度卷积赋予了神经网络以抽象能力。 这样的一级级向上卷积做基变换的过程，有人说叫搞基（深度学习就是搞基），深一点想叫表征， 和人的思维做个比喻就是抽象。 抽象是我在很深的层次把不同的东西联系起来，CNN教会了我们实现抽象的一种物理方法，  他也体现了在一个空间尺度上我们所能够达到的特征工程。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最终分类：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里我们还差最后一步没讲， 整个CNN网络如同一个等级社会里，最上层的，就是君王。 而这个君王，与直接其下的一层（议会）的关系，事实上往往是全连接网络。为什么，因为这时候君王要做的是最终决策， 它不在“搞基”提取特征了。一个非常复杂的问题，已经在此时变成了线性可分的简单问题。 决策 – 就是做一个线性分类， 得到我最想要的结果。  我们要做的是返回一组最终可能结果的概率。如果得到可能结果的一组概率？ 我们搬出基于最大熵模型的softmax  gate ，这也是正是CNN网络做分类的最后一层。 至此，我们可以得到众多识别物体的抽象信息。 那个概率最大的，即使我想要的结果。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5389435989256938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kF6KG6kGBVeH4Zwib3bCIeFwOrMQygTJ8ckUDyfZHkLTCEKfHmgbfSlQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1117&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7400318979266348&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kByPbeQrjLRAFZMt2UBqXLDompmohkqQN7fjYMqSlaeWfxI0jZaQ2og/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;627&quot;/&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;你所不能不知道的CNN&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作者简介：微信号：ironcruiser 法国巴黎高师物理硕士 ，以色列理工大学计算神经科学博士，巡洋舰科技有限公司创始人, 《机器学习与复杂系统》纸质书作者。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 02 Mar 2019 11:40:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/C8XQvUfjg6</dc:identifier>
</item>
<item>
<title>基于一张“规则表”的人工智能</title>
<link>http://www.jintiankansha.me/t/WW3EirRLSj</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/WW3EirRLSj</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;4ds8l-0-0&quot;&gt;我在过去对人工智能简史的描述中，把人工智能的整个历史描述成围绕一张规则表， 本文是基于这一想法的总结和扩展。我们说，早期的AI发展史围绕如何人为构建这样一个规则表解决复杂问题， 而当下的AI则围绕如何让它在复杂的现象中自己归纳出这个规则表。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bkglj-0-0&quot;&gt;我们说，上帝通过制定规则从简单演绎出复杂。最初的原始人类在黑暗中摸索， 在众多的现象不知所措， 只能通过设立各色大神小神来缓解自己对不确定性的恐惧。 从多神宗教到一神宗教的跨度体现了一个从复杂中寻找简单的跨度， 这可能是基于一种隐隐的直觉，就是现象虽然多样， 但是背后的法则不应该如此复杂。 到了科学的时代， 这种思维在物理学里淋漓尽致起来，四大力学， 把分子原子间的作用力统一到电磁力， 把宏观物体的作用统一到引力和经典的动力方程， 已经是极致。 而后面的对这两者的统一构成了从广义相对论后的现代物理主线。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibembXL81evOfRdy0cTz2qqulnss6iaFGmXLmdjuRu9GiayK86XIM1TVSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图: 简单规则生成复杂的极好例子， 元胞自动机， 每一步细胞的繁殖和扩散方法一定（左图黑格表述的， 从上一行到下一行的变化法则）， 它最后形成的图案就定了， 规则可以很简单， 图案可以很复杂。 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;早期的智能：  制定规则表 - 迭代&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1j7g-0-0&quot;&gt;另一方面，这样的思想从智能科学诞生之出，也贯穿出来。它在早期的可计算性中， 通过图灵机的构建。它认为存在这样的通用机器，能够和人类一样解决问题， 即使过程非常复杂， 你无非需要四个要素： &lt;strong&gt;1， 输入 2， 中间状态 3， 规则表 4， 输出。&lt;/strong&gt; 并在时间上进行大量迭代， 就可以实现这个过程。 通过这个过程， 我们可以把一个输入转化为一个想要的输出。 如果我们能够在在有限的步骤里将一个输入转化为一个输出，据此解决一个实际问题， 那么这个问题就是可计算的。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6114081996434938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBnC8zkibScsiccAOwoSH4Ld9vdiamljakNVOX5TKF69ywm3Qq2ic3IdhUw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;561&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图： 图灵纸袋， 一个在纸袋上根据一定规则表行走的机械昆虫， 机械昆虫具有一个内部状态， 并接受外部的输入，最终通过规则表找到对应的下一步输出。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c00nl-0-0&quot;&gt;而冯诺依架构让它变成一个技术现实。 它通过可以存储程序的机器， 让人们通过把这些图灵规则表的指令变为计算机二极管的开合代码， 而让图灵纸袋的思想成为了一个每个工程师可以设计的现实，从此有了程序和程序员。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;一个计算机程序， 最基本的部分包括一些简单的形式逻辑， 包括逻辑与或非， if else， for 循环这些。 其实本质上， if else 所描述的就是规则表， 规则里面通常涉及简单的逻辑， 最终通过for循环， 我们就可以得到我们要的东西。 比如一个中学生都会的排序算法， 我们无非需要做的是前面和后面的数比较大小， 然后一个if else进行换位， 最后一个for循环， 多长的序列都可以瞬间搞定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.40556900726392253&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibibEBCIwsAHiaGwDkFcYUeGLqHsuPP0ibBA7ibPX7OxPPq87Jr0NAtFjSibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;826&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b1ct2-0-0&quot;&gt;这就是用程序解决问题的核心思维， 给你一个再复杂手忙脚乱的问题， 只要这个问题可计算， 那么我们只需要设定好我们需要的规则表， 在有限的步骤里迭代， 最终机器总会给你解决。&lt;/span&gt;比如魔方问题， 一般的聪明小孩都很难在短时间解决问题， 但是， 事实上解决魔方问题有一套非常整齐的规则表（你想象打乱一个魔方其实比较容易的， 把它弄整齐是打乱的逆运算，但是破镜重圆总是难的）。 如果按照这个规则表执行若干步， 再困难的魔方也给你整出来。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibz3zliaRoh4oEWf9QQOX3VMpbMFylumcHY7novvRnic1j7aN7R7pohRHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;676&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44333333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBqmPPsMKMLBUUyKWYnVsiaibg6hmZP9cADL9iak5yxuibvmosIG5ZUWbuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e13hv-0-0&quot;&gt;我们说规则表， 加上迭代等操作的思路可以解决大量的工程问题。我们曾经认为按照这样的思路我们可以解决整个智能的问题。 只是填入一张越来越大的表格。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;但是它在通向智能的关键位置， 却停住了， 这个元凶 -就是- &lt;strong&gt;不确定性&lt;/strong&gt;。日常生活中很多东西无法轻易的总结出规则表来， 因为细小的规则实在太多了。 你可以想象我们有无数尺寸和规格各不相同的螺钉螺母。 每一种规格我们都要想一条if else，可悲的是这些螺钉和螺母几乎没有哪两对完全相同， 穷尽一个程序员一生也写不完这些程序。现实生活中的大部分问题属于这一类问题， 比如你无法轻易的写出一段程序来判断A男和B女是否合适结婚。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中期的智能： 让机器学会归纳规则表&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;9mrot-0-0&quot;&gt;统计机器学习 - 机器判断规则&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6pfmt-0-0&quot;&gt;这个问题的解决方法十分自然又十分了不起：  能不能让机器自己学会这个表格， 而不是认为设定它呢？  这就是整个智能问题的第二步 - 学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eeh10-0-0&quot;&gt;整个学习问题的基石其实是古希腊人提出的归纳法和演绎法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-0&quot;&gt;伟大的希腊哲学家早就对学习的本质展开过探讨，它们把学习分类为&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-1&quot;&gt;归纳法和演绎法&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;。所谓演绎法， 就是从用一定规则进行推理的过程。 苏格拉底是人，人都是会死的， 因此苏格拉底会死。这就是三段论， 或者称为演绎法的根基。 而真正学习的过程，是这个演绎法的逆过程。我们先知道一个特例，然后通过特例，得到这个“人都是会死的” 知识，再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibmSX7rgCyJJfTKEhApOyARbIowfAnPvZVr0gFicQTQYD7WgfkLMcPbZw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;让机器实现归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6nhn2-0-0&quot;&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。 我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？ 组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibeB6eia8YghicHCagGO02XX9WcmpmicrD1ya9UNmpAlIqbbygWzLKiarAVQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;1bdeq-0-0&quot;&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！比如机器收集到一个苏格拉底死了， 那么苏格拉底是什么？ 男性，白种人， 哲学家， 于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！ &lt;strong&gt;我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibCChMBjNz3U3sMtU9kp7SpWibY4ocjf7OhopFnViaibcnR1aYIDpYKLaVg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;整个有关统计的机器学习， 都可以看成让机器学习有效归纳的方法， 从数据里得到规则表， 再用规则表进行判断。前面的过程叫训练， 后面的过程叫测试。 &lt;/strong&gt;&lt;span data-offset-key=&quot;9nnub-0-0&quot;&gt; 如果这些规则是有关一个是非的命题， 它就是一个分类器， 如果它是一个连续数值的预测， 就是回归。 但是规则表的本质是不变的， 它就是让你填表，表格的横排和竖排已经有了， 一个叫特征， 一个叫实例。 特征是人为归纳好的， 而实例是我们人为收集的， 表格中有些地方是空的，  就是我们想要判断的东西， 需要机器来填的部分。 比如给你一百幸福和不幸的人的案例， 让你判断第101个人的情况。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7663197729422895&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibGeQKfN4oT43Gf7qPQGibnceOkVZRMtllFgcAHBwVIocWdibkMkFB9V1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1057&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图：机器从实例中学到分类的方法： 机器学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;81s85-0-0&quot;&gt;刚刚的那个例子你应该已经体会到， 这个命题验证过程其实是一个组合爆炸的问题。我们把关于这个世界的互相矛盾的假设都丢尽机器。即使最简单的问题也会有无穷多的情况要判断 （特征的n次方）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib6XNRA36VEFMtlPlZQ2RpPXtla5u66LIcDmhkLUQuPecqg0IVicuibIwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ajopm-0-0&quot;&gt;而机器学习的各个算法， 让我们通过加入更多的假设， 来偷懒解决这个问题， 此处没有比决策树更典型的， 它的高阶版本xgboost成为机器学习竞赛的杀手锏。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;而决策树得核心智慧就是优先级算法简化命题数量。 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心。事实上， 我们永远在抓轻重缓急，在抓主要矛盾， 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。 每次pass， 就减少掉了一半需要判定的命题。 通过这种预设的二叉树逻辑， 一个本来需要2的n次方的步骤解决的事情， 可能只要n步了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7453703703703703&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibq5oBakMtQKgHb2Akq4jNYicdNNniarAhbtJNpWaJ7qOkbJDDKpTTB9MQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;具体如何来学，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;如此，就会形成一个树的结构。构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？我的要求就是， 每一次分化，我们都希望取得最多的信息，如分叉后一个树杈全是yes，一个全是no就是最好的效果， 如果达不到， 也让它尽可能接近这个效果。  这样一个一个问题问下去， 最终达到稳定后过程停止。  这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk0ce-0-0&quot;&gt;另一个得到这样的一个规则表的方法是线性假设。 线性分类器通过假定特征之间的相互独立， 使得命题的成立与否可以通过一个加权求和的关系表达， 即f=wx+b 。最后f如果大于0就是是， 小于0就是否。 线性分类器也是一种特别符合人认知习惯的模型：一般人在决策时候做的事情就是加权平均，比如你平时做分类（决策）， 你最想的一种状态是什么？你要把几个核心的要素放到一起， 按照他们的重要性加和，比如你今天要不要去看电影，可能取决于你的女朋友free否， 下不下雨和电影好不好看， 这个时候，我们可以把这些因素加权在一起， 在和一个我们给定的阈值做比较，大就去， 不大就不去， 这正权衡得失的做法， 就是线性分类器。线性分类器看上去是一个数学公式， 本质还是一个规则表， 只不过这里要学习的规则无非是每个特征给多少权重。最后在表格最后一列得到yes or no。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9387096774193548&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibm99R31Z3g5uTib3k3mrK8kME4XDCicJ6HgXW6a31dQaFa9RABR3z7VHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图： 线性分类器， 一条直线代表一组权重， 把两组数据分的越开越好。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;具体学习的过程， 我们从实例里归纳出每个特征对应的权重参数，然后进行判断。 只要参数都确定了， 也就是一次解决了所有的问题。 线性分类器的高级版本SVM已经超越了线性假设。 也是小数据下生成有效规则的大杀器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;近期的智能： 让机器生成有效的规则 &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8te9f-0-0&quot;&gt;连接主义机器学习， 产生规则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;fk6j-0-0&quot;&gt;刚刚说的那一套， 有一个问题你有没有注意到？ 我们最先提出的问题是让机器产生一个规则表， 而刚刚说的统计机器学习里， 更多的是让机器根据定好的特征收集数据进行命题判断。 这其实离我们说的让机器自己得到规则表只进行了0.5步。&lt;strong&gt;大家想象下， 在真正的实践活动里， 你无法一开始就设定出一堆特征让它进行逻辑判断，在这个情况下如何得到我们所说的“规则”呢？ &lt;/strong&gt; 如何让机器自己生成战胜“复杂”的程序呢？ 连接主义机器学习在一定程度解决了这个问题。  因为， 人类认识事物，生成规则， 其实是通过“&lt;strong&gt;概念&lt;/strong&gt;”来的， “概念”是一个浓缩的信息载体， 通过它我们能够进行任何更复杂的推理。 那么“概念”是如何生成的呢？ 它的载体正是下面说的联结主义的代言人神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5gsn-0-0&quot;&gt;神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dijog-0-0&quot;&gt;首先，神经网络是由神经细胞组成的。  一个神经细胞就是一个最小的认知单元， 何为认知单元，就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习，它就是刚刚说的线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cr2lq-0-0&quot;&gt;正确的分类，是认知的基础，我们对事物的感知比如色彩，物体的形状等，其实都是离散的，而物理信号是连续的，比如光波，声波。这里面的中间步骤就是模数转化，把连续的信号转化成离散的样子，这正是一个分类器干的事情。一个单个神经元可以执行一个简单的基于感知信号的if else语句。 先收集一下特征做个加和，if大于一个值我就放电，小于我就不放电，就这么简单。晶体管当然也在干这个事情。 神经细胞与晶体管和计算机的根本区别在于可塑性。或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib2NvE4HCXevrTz3VAnXzfIxaOZibibe1YnYGN84NF1XWqyKpNdsISR2aA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3f6fb-0-0&quot;&gt;我们这个简化出来的模型，正是所有人工神经网络的祖母－感知机。从名字可以看出，&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-1&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-2&quot;&gt; 它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型， 它的创始人 R 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2b1gb-0-0&quot;&gt;单个的感知机并不能比传统的机器学习多做一丁点的事情， 还要差一些。 但是把很多个感知机比较聪明的联系起来，就发生了一个质变。&lt;/span&gt;首先， 每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示，左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。所以， 多个神经之后，我们还要在头顶放一个神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总，&lt;strong&gt; 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，把哪些被直线分开的四分五裂的块，就可以得到一个非常复杂的判决结果。 &lt;/strong&gt;你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。 其实这里面做的事情， 正是基础的逻辑运算， 一个简单的一层神经网络可以执行与或非这些基本的逻辑操作。事实上它的本质就是把简单的特征组合在一起形成一些原始的概念。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5758323057953144&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibuABrESypc6M08wlHD6dia0vL7GRLUKEQiby0Bicc47icreRupKWw5IHVVQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;811&quot; /&gt;&lt;/p&gt;
&lt;p&gt;它是怎么做到的呢？ 学习。 生物神经网络的学习， 是通过一种叫做hebbian可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是， 我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了一个简单的“概念”。 上述过程被总结hebian学习的一个过程。  我们可想象，一个两层以上的神经网络， 就可以表述香蕉， 苹果， 菠萝这些水果了， 它们无非是底层特征颜色，形状的不同组合而已。 而这些不同水果的概念， 就可以帮助我们形成更加复杂的规则表 ，比如让它根据客户的信息帮它推荐一个水果拼盘。 由此可见， 神经网络通过与或非进行简单特征的组合 ，再通过if esle进行判断选择合适的特征得到概念， 再通过下一层迭代得到概念有关的命题。 就可以生成比之前的传统机器学习复杂的多的规则表。而且我们可以想象出来， 迭代的层数越多，它生成的“概念”和“规则”就越复杂。  &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;当然真实训练中我们用到的不是模仿生物版本的hebbian学习， 而是强大的多的反向传播算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.6111111111111112&quot; data-w=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span data-offset-key=&quot;dmb6s-0-0&quot;&gt;事实上为了让这种生成“概念”得到“规则”的方法更加有效， 我们会加入一些无比强大的先验假设。 其中最有名的一组，  就叫CNN，它所做的，其实是针对于图像这类巨大无比而局部特征不断重复的数据形式， 你可以写一个循环， 来让你的程序更有效。 循环里的每一步都对图像的局部特征进行提取， 由一个可以共用的卷积核实现。 卷积核一点点的卷过图像上的每个小块， 也就是循环的总体。 卷积核在每个图像局部做的， 事实上都是一个小的if esle 语句。 if像素之间符合某个关系，就是yes，否则No。这个结果， 最后被综合出来， 给下一层合成更复杂的图像特征。我们事实上通过学习的过程，让机器自动补全了循环每一步的这个if else语句。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7298850574712644&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibJ7GaCXDzw2dMW8QiaAPh96svf1lqjLico2MoDlVUCfFZgIHkysrSaRibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4vk9r-0-0&quot;&gt;好了，到目前为止， 说的都是和时间无关的规则。 而一开始讲到的真实的图灵机， 是和时间有关的规则。 那么如何得到一个和时间有关的规则表呢？ 如果要处理和时间相关的信息， 你必须要引入记忆，引入内部状态， 而和刚刚说的一样， 这些含时间的规则要是可以学习的，用数学的语言说， 就是要有一个连续可微的载体， 这个东西就是RNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;72qm9-0-0&quot;&gt;def step&lt;/span&gt;&lt;span data-offset-key=&quot;72qm9-0-1&quot;&gt;(self, x):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;di032-0-0&quot;&gt;# update the hidden state&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4f218-0-0&quot;&gt;self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;243tl-0-0&quot;&gt;# compute the output vector&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;591p4-0-0&quot;&gt;y = np.dot(self.W_hy, self.h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a1b57-0-0&quot;&gt;return&lt;/span&gt;&lt;span data-offset-key=&quot;a1b57-0-1&quot;&gt; y&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;g9on-0-0&quot;&gt;以上是RNN的python程序定义。 它说的无非是你有一个刚刚说的线性分类器组成的单隐层神经网络，但是这一回，神经网络的输出，要作为输入，重新回到神经网络的隐层里， 这个关键的增加， 就使得它具有了处理复杂时间信息的能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5qh71-0-0&quot;&gt;这个结构，非但优雅，而且有效。一个非常重要的点是， 你知道信息的传播是有损耗的， 如果把RNN展开， 它事实上相当于一个和历史长度一样长的深度网络， 信息随着每个时间步骤往深度传播， 这个传播的信息是有损耗的， 到一定程度我就记不住之前的信息了， 当然如果你的学习学的足够好， Wij还是可以学到应该学的记忆长度。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26161790017211706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibPf1BYBmsfuHXyspOv0uwulVbuy0UibqOib2grXtp4XpvWU4b6Pj3402Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;事实上叫做“循环神经网络”  循环的本质是什么呢？   它其实正是你的程序里的for循环啊！ RNN的本质是， 在每个时间步里进行同样的操作， 这个操作无非是， 当下的输入， 和神经网络的状态两部分特征的逻辑组合（与或非）然后， 这个组合的结构进行一个if else的逻辑判断， yes or no， 根据这个，生成一个输出的结果， 这个结果， 要回传给神经网络隐层， 生成下一个隐层状态。 大家看这其实就是图灵机的定义啊， 而RNN的本质， 就是一个可以通过微分方法学习的图灵机啊。 虽然每个步骤的规则和执行足够简单， 但是只要步数足够多， 却可以产生非常复杂的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;RNN学习的本质， 就是给你那个足够复杂的结果， 让你反演出那个足够简单的规则， 然后让它在新的环境下再去做预测与决策。 我们可以看到， 这已经非常接近智能的本质了。 那么RNN有没有可能学到真正类似人类的抽象思考能力， 具备人类类似的生成规则的能力呢？ 这可能才是后面真正的问题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;让机器生成有关未来的规则-强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt; &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;果说监督学习的基本框架已经是在生成用于判断（分类）的规则表， 那么强化学习， 就是生成一套直接用于行动的规则表， 这套语言的元素包括&lt;span&gt;状态， 行为， 观测， 奖励&lt;/span&gt;。 事实上， 强化学习所做的事情是从成功或失败的经历里去归纳行为的准则。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;我开头讲的解魔方的问题， 如过让机器自己找到最短时间完成它的方法， 这就是强化学习所做的事情。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;首先看&lt;span&gt;状态&lt;/span&gt;s， 状态是什么呢？ 它指的是智能体（agent）所在的环境里所有和游戏有关的信息， &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;再来看行为，所谓行为，是指智能体的&lt;/span&gt;&lt;span&gt;决策&lt;/span&gt;&lt;span&gt;，某种情况下我们可以认为它就是监督学习要求的那个y， 或者预测， 但一个决策与预测不同的是，我们并不能马上取得一个信号告诉我们这个决策对不对， 只有在游戏的最后 ，我们才能从整个游戏的收益反观当时的决策好坏。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;寻找到从根据当下的状态s行动的一张规则表， 让我有最好的机会拿到奖励， 就是强化学习在做的事情。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;而深度强化学习呢？  它就是把刚说的连接主义通过概念生成规则的方法，和此处的决策联系起来得到的框架。 &lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;我们还差很远&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;说到所有上述的东西， 你会觉得我们已经无所不能， 既然机器能够自己填写程序， 自己生成规则表来适应多变的世界， 那我们还差什么呢？  我们说都是生成规则， 不同的规则效力差距万千，而目前的AI在此处也就是个小学生。 亚里士多德和牛顿都观测力学现象， 亚里士多德看到的是轻的气体向上飘，重的东西向下沉。 牛顿看到的是受力与加速度的关系。 这两种规则的归纳即使都能解释现象， 但是它们的泛化能力确是千差万别， 一个可以解释全宇宙， 另一个也就适应一些物体吧。 如同下图所示， 坏的规则总结只能解释数据实例周边一丁点的地方， 而好的规则呢， 它可以把点连城片！  &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7445945945945946&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4XExqvasWticuD9ehaBQuFeiazL5LEfoMicN72hz0eQJEZpp4IqichpMVAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;我们说人类总结的规则解释力最强的地方是物理 ，因为物理里描述了客观实体间作用的因果联系，而非简单的相关性。 这恰恰是目前AI所不足的。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5j932-0-0&quot;&gt;有关物理的世界和智能的世界&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;di5qc-0-0&quot;&gt;上面的这些思考无疑开始让人们想象我们所说的包含了逻辑推理， 情感，甚至意识的问题与物理世界的关系到底是什么。 我们说物理的世界里， 主宰一切的是微分方程。 一切因果关系， 都由微分方程所承载。 你有了不同不同微观粒子电磁力的描述，把它们放入薛定谔和狄拉克方程， 你就可以推出原子的不同性质。  这其实可以说是因果推理的极致了。 它甚至导致了机械的宿命论思想。当一切初始的原因输入系统， 那么它就回归于一个必然的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dl784-0-0&quot;&gt;到了非线性动力学的时代看似这点被混沌打破了，亚马逊的蝴蝶引起北美的飓风， 让通俗科学爱好折重新燃起了不可知论的希望，事实上并没有。  所谓的混沌， 无非是一种确定性下的不确定， 或者已知中的未知。 混沌的系统依然在一个被方程高度确定的洛伦兹吸引子里。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span data-offset-key=&quot;af64u-0-0&quot;&gt;到此处， 我认为微分方程依然是描述因果关系最精密的所在， 它可以在输入很少信息的时候， 得到最多的预测产出。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;在看刚刚的智能问题， 我们说， 整个智能问题， 到目前为止其实还是在围绕那张规则表， 只是我们的思路由制定规则表， 到了学习规则表。和物理比较， 目前的机器， 需要输入进去大量的数据， 才能生成一点十分简单的规则。 当然你可以举阿法狗下围棋的例子说明所生成的规则并没有那么简单， 可惜的是， 那些规则只适用于一些非常封闭而特定的领域。 而不像牛顿定律放之四海而皆准。  那么神经网络可不可以观测大量物体坠落的过程把万有引力定律给推出来呢？ 目前看是不能的。其实牛顿引力定律的得出是含有了大量的人类推理。 我们需要先知道物体运动改变和受力的关系， 然后通过观测物体的轨迹得到大量物体的受力情况，再在这些手里情况下得出某一种共同的作用力形式， 这是一个多么复杂的思维链条。 这对于目前统计的巨人， 而只懂得浅显的形式逻辑的神经网络，还是比较困难的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;如果我们可以用神经网络加上强化学习，诱导它掌握特别抽象而复杂的如受力，运动这样的概念，那将是不可想象的， 目前我们也并不清楚有没有那一天。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;另一方面， 人类是通过符号组成的语言思考的， 目前AI总结和归纳符号的能力， 同人类的语言相比依然天差地别。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dsh1-0-0&quot;&gt;有关语言代表的符号世界&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;讨论智能的问题离不开语言。从乔姆斯基开始，人们就开始研究不同语言背后的共同语法基础。 其实如果深究语言问题， 我们会看到它和刚刚说的程序的联系。 语言无非是对世界的符号化，类似于给每个刚刚说的概念赋予一个符号。而语言其实很像程序， 它就是对概念之间关系的表述。 当然数学也是一套伟大的符号系统。  语言和数学以及程序的区别可能在于语言更加模糊， 但是它对付不确定性的能力远远大于程序。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;事实上， 人类语言很少有那种特征精确的关于几何关系和量值的描绘， 而似乎更多定性成分， 比如美丑，近， 远， 大，小。 我们可以想象在一个充满变化的世界那种特别精确的描述不一定十分有用。 恰恰因为这种模糊性，让它具备了更好的适应能力， 和泛化能力。 可以表述那些用程序难以描述的事情。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;既然本质上语言无非是一个现有概念的符号体系，以往的深度学习NLP其实是走了一条南辕北辙的路，我们把不同的词汇和句子压缩成词向量， 句向量喂给神经网络学习， &lt;strong&gt;而事实上神经网络对这些符号背后的实体概念却一无所知。&lt;/strong&gt;虽然词向量也能稍微的带有一点不同词语之间的语义距离， 但是这和真实世界所含有的信息量，也依然是差异巨大。 目前用图卷积网络解决NLP的思路，算是一个进步， 因为它更好的涵盖了整个符号世界的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;语言， 好比一个巨大的人类经验和逻辑的宝库， 这个符号世界几乎就是真实世界的最好压缩体， 如果一天神经网络真正被赋予了语言的power，也就是能够真正理解这个符号世界，或许离通用人工智能也就不远了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;有关物理世界和语言世界的打通&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;刚刚说的精确的物理方程的世界， 和能够应付更多不确定性的模糊的语言， 之间又有哪些联系呢？  我的想法是， 物理的杀手锏微分方程， 当构成了一个非线性的动力学系统， 却可以通过它内在的定点， 极限环，吸引子等概念， 去接近那个模糊性的语言， 就好比在非线性动力学的世界里， 我们往往不再那么关于一个系统如何发展的暂态，很多不同的系统都归一于一个吸引子， 那么它们背后的逻辑可能就是类似的。 这或许会架起一座物理世界和语义世界的桥梁？  &lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;如何让机器学习符号&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;假定语言这样的高效符号系统是大脑产生的，那么我们是否可以根据脑科学的启发把这种能力赋予神经网络，来加强它的推理能力呢？ 如此可以延申的思考还很广很广。&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 26 Feb 2019 00:50:52 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/WW3EirRLSj</dc:identifier>
</item>
<item>
<title>基于一张规则表的人工智能</title>
<link>http://www.jintiankansha.me/t/q56XOgLetH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/q56XOgLetH</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;4ds8l-0-0&quot;&gt;我在过去对人工智能简史的描述中，把人工智能的整个历史描述成围绕一张规则表， 本文是基于这一想法的总结和扩展。我们说， 早期的AI发展史围绕如何人为构建这样一个规则表解决复杂问题， 而当下的AI则围绕如何让它在复杂的现象中自己归纳出这个规则表&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bkglj-0-0&quot;&gt;我们说， 上帝通过制定规则从简单演绎出复杂。最初的原始人类在黑暗中摸索， 在众多的现象不知所措， 只能通过设立各色大神小神来缓解自己对不确定性的恐惧。 从多神宗教到一神宗教的跨度体现了一个从复杂中寻找简单的跨度， 这可能是基于一种隐隐的直觉，就是现象虽然多样， 但是背后的法则不应该如此复杂。 到了科学的时代， 这种思维在物理学里淋漓尽致起来，四大力学， 把分子原子间的作用力统一到电磁力， 把宏观物体的作用统一到引力和经典的动力方程， 已经是极致， 而后面的对这两者的统一构成了从相对论到杨米尔场的现代物理主线。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibembXL81evOfRdy0cTz2qqulnss6iaFGmXLmdjuRu9GiayK86XIM1TVSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图: 简单规则生成复杂的极好例子， 元胞自动机， 每一步细胞的繁殖和阔算方法一定， 它最后形成的图案就定了， 规则可以很简单， 图案可以很复杂。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1j7g-0-0&quot;&gt;另一方面， 这样的思想从智能科学诞生之出，也贯穿出来。 它在早期的可计算性中， 通过图灵机的构建。它认为存在这样的通用机器，能够和人类一样解决问题， 即使过程非常复杂， 你无非需要四个要素： 1， 输入 2， 中间状态 3， 规则表 4， 输出 并在时间上进行大量迭代， 就可以实现这个过程。 通过这个过程， 我们可以把一个输入转化为一个想要的输出。 如果我们能够在在有限的步骤里将一个输入转化为一个输出，据此解决一个实际问题， 那么这个问题就是可计算的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6114081996434938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBnC8zkibScsiccAOwoSH4Ld9vdiamljakNVOX5TKF69ywm3Qq2ic3IdhUw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;561&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c00nl-0-0&quot;&gt;而冯诺依架构让它变成一个技术现实。 它通过可以存储程序的机器， 让人们通过把这些图灵规则表的指令变为计算机二极管的开合代码， 而让图灵纸袋的思想成为了一个每个工程师可以设计的现实。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;一个计算机程序， 最基本的部分包括一些简单的形式逻辑， 包括逻辑与或非， if else， for 循环这些。 其实本质上， if else 所描述的就是规则表， 规则里面通常涉及简单的逻辑， 最终通过for循环， 我们就可以得到我们要的东西。 比如一个中学生都会的排序算法， 我们无法需要做的是前面和后面的数比较大小， 然后一个if else进行换位， 最后一个for循环， 多长的序列都可以瞬间搞定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.40556900726392253&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibibEBCIwsAHiaGwDkFcYUeGLqHsuPP0ibBA7ibPX7OxPPq87Jr0NAtFjSibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;826&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b1ct2-0-0&quot;&gt;这就是用程序解决问题的核心思维， 给你一个再复杂手忙脚乱的问题， 只要这个问题可计算， 那么我们只需要设定好我们需要的规则表， 在有限的步骤里迭代， 最终机器总会给你解决。&lt;/span&gt;比如魔方问题， 一般的聪明小孩都很难在短时间解决问题， 但是， 事实上解决魔方问题有一套非常整齐的规则表（你想象打乱一个魔方其实比较容易的， 把它弄整齐是打乱的逆运算，但是破镜重圆总是难的）。 如果按照这个规则表执行若干步， 再困难的魔方也给你整出来。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibz3zliaRoh4oEWf9QQOX3VMpbMFylumcHY7novvRnic1j7aN7R7pohRHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;676&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44333333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBqmPPsMKMLBUUyKWYnVsiaibg6hmZP9cADL9iak5yxuibvmosIG5ZUWbuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e13hv-0-0&quot;&gt;我们说规则表， 加上迭代等操作的思路可以解决大量的工程问题。我们曾经认为按照这样的思路我们可以解决整个智能的问题。 只是填入一张越来越大的表格。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;但是它在通向智能的关键位置， 却停住了， 这个元凶 -就是- &lt;strong&gt;不确定性&lt;/strong&gt;。日常生活中很多东西无法轻易的总结出规则表来， 因为细小的规则实在太多了。 你可以想象我们有无数尺寸和规格各不相同的螺钉螺母。 每一种规格我们都要想一条if else，可悲的是这些螺钉和螺母几乎没有哪两对完全相同， 穷尽一个程序员一生也写不完这些程序。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9mrot-0-0&quot;&gt;统计机器学习 - 机器判断规则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6pfmt-0-0&quot;&gt;这个问题的解决方法十分自然又十分了不起：  能不能让机器自己学会这个表格， 而不是认为设定它呢？  这就是整个智能问题的第二步 - 学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eeh10-0-0&quot;&gt;整个学习问题的基石其实是古希腊人提出的归纳法和演绎法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-0&quot;&gt;伟大的希腊哲学家早就对学习的本质展开过探讨，它们把学习分类为&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-1&quot;&gt;归纳法和演绎法&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;。所谓演绎法， 就是从用一定规则进行推理的过程。 苏格拉底是人，人都是会死的， 因此苏格拉底会死。 这就是三段论， 或者称为演绎法的根基。 而真正学习的过程，是这个演绎法的逆过程。 我们先知道一个特例， 然后通过特例，得到这个“人都是会死的” 知识， 再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6nhn2-0-0&quot;&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。 我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？ 组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1bdeq-0-0&quot;&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！ 比如机器收集到一个苏格拉底死了， 那么苏格拉蒂是什么？ 男性，白种人， 哲学家， 于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！ 这样的思维范式，就是归纳法，由于我们列举的假设依然用到了人类已有的知识， 因此我们得到的这个机器，事实上是最接近规则机器的一台学习机， 我们可以称之为规则为主体的归纳法。我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nnub-0-0&quot;&gt;整个有关统计的机器学习， 都可以看成让机器学习有效归纳的方法， 从数据里得到规则表， 再用规则表进行判断。前面的过程叫训练， 后面的过程叫测试。  如果这些规则是有关一个是非的命题， 它就是一个分类器， 如果它是一个连续数值的预测， 就是回归。 但是规则表的本质是不变的， 它就是让你填表，表格的横排和竖排已经有了， 一个叫特征， 一个叫实例。  特征是人为归纳好的， 而实例是我们人为收集的， 表格中有些地方是空的，  就是我们想要判断的东西， 需要机器来填的部分。 比如给你一百幸福和不幸的人的案例， 让你判断第101个人的情况。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;81s85-0-0&quot;&gt;刚刚的那个例子你应该已经体会到， 这个命题验证过程其实是一个组合爆炸的问题。我们把关于这个世界的互相矛盾的假设都丢尽机器。即使最简单的问题也会有无穷多的情况要判断 （特征的n次方）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib6XNRA36VEFMtlPlZQ2RpPXtla5u66LIcDmhkLUQuPecqg0IVicuibIwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ajopm-0-0&quot;&gt;而机器学习的各个算法， 让我们通过加入更多的假设， 来偷懒解决这个问题， 此处没有比决策树更典型的， 它的高阶版本xgboost成为机器学习竞赛的杀手锏。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;而决策树得核心智慧就是优先级算法简化命题数量。 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心，事实上， &lt;strong&gt;我们永远在抓轻重缓急，在抓主要矛盾，&lt;/strong&gt; 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。 每次pass， 就减少掉了一半需要判定的命题。 通过这种预设的二叉树逻辑， 一个本来需要2的n次方的步骤解决的事情， 可能只要n步了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7453703703703703&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibq5oBakMtQKgHb2Akq4jNYicdNNniarAhbtJNpWaJ7qOkbJDDKpTTB9MQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;具体如何来学，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 如此，就会形成一个树的结构。构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？我的要求就是， 每一次分化，我们都希望取得最多的信息，如分叉后一个树杈全是yes，一个全是no就是最好的效果， 如果达不到， 也让它尽可能接近这个效果。  这样一个一个问题问下去， 最终达到稳定后过程停止。  这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk0ce-0-0&quot;&gt;另一个得到这样的一个规则表的方法是线性假设。 线性分类器通过假定特征之间的相互独立， 使得命题的成立与否可以通过一个加权求和的关系表达， f=wx+b 。最后f如果大于0就是是， 小于0就是否。 线性分类器也是一种特别符合人认知习惯的模型：一般人在决策时候做的事情就是加权平均，比如你平时做分类（决策）， 你最想的一种状态是什么？你要把几个核心的要素放到一起， 按照他们的重要性加和，比如你今天要不要去看电影，可能取决于你的女朋友free否， 下不下雨和电影好不好看， 这个时候，我们可以把这些因素加权在一起， 在和一个我们给定的阈值做比较，大就去， 不大就不去， 这正权衡得失的做法， 就是线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;具体学习的过程， 我们从实例里归纳出每个特征对应的权重参数，然后进行判断。 只要参数都确定了， 也就是一次解决了所有的问题。 线性分类器的高级版本SVM已经超越了线性假设。 也是小数据下生成有效规则的大杀器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8te9f-0-0&quot;&gt;连接主义机器学习， 产生规则&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk6j-0-0&quot;&gt;刚刚说的那一套， 有一个问题你有没有注意到？ 我们最先提出的问题是让机器产生一个规则表， 而刚刚说的统计机器学习里， 更多的是让机器根据特征进行命题判断。 这其实是只进行了0.5步。 大家想象以下， 在真正的实践活动里， 你无法一开始就设定出一堆特征让它进行逻辑判断，在这个情况下如何得到我们所说的“规则”呢？ 如何让机器自己生成战胜“复杂”的程序呢？ 连接主义机器学习在一定程度解决了这个问题。  因为， 人类认识事物，生成规则， 其实是通过“概念”来的， &lt;strong&gt;“概念”是一个浓缩的信息载体， 通过它我们能够进行任何更复杂的推理。&lt;/strong&gt; 那么“概念”是如何生成的呢？ 它的载体正是下面说的联结主义的代言人神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5gsn-0-0&quot;&gt;神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dijog-0-0&quot;&gt;首先，神经网络是由神经细胞组成的。  一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习，它就是刚刚说的线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cr2lq-0-0&quot;&gt;正确的分类，是认知的基础，我们对事物的感知比如色彩， 物体的形状等，其实都是离散的， 而物理信号是连续的， 比如光波， 声波。这里面的中间步骤就是模数转化， 把连续的信号转化成离散的样子， 这正是一个分类器干的事情。  一个单个神经元可以执行一个简单的基于感知信号的if else语句。 先收集一下特征做个加和， if大于一个值我就放电， 小于我就不放电，就这么简单。 晶体管当然也在干这个事情。 &lt;strong&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。&lt;/strong&gt;或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib2NvE4HCXevrTz3VAnXzfIxaOZibibe1YnYGN84NF1XWqyKpNdsISR2aA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3f6fb-0-0&quot;&gt;我们这个简化出来的模型，　正是所有人工神经网络的祖母　－　感知机。　从名字可以看出，&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-1&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;3f6fb-0-2&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 R 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;单个的感知机并不能比传统的机器学习多做一丁点的事情， 还要差一些。 但是把很多个感知机比较聪明的联系起来，就发生了一个质变。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ui0b-0-0&quot;&gt;首先， 每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有那个头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  其实这里面做的事情， 正是基础的逻辑运算， 一个简单的一层神经网络可以执行与或非这些基本的逻辑操作。事实上它的本质就是把简单的特征组合在一起形成一些原始的概念。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;它是怎么做到的呢？ 学习。  生物神经网络的学习， 是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是， 我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了一个简单的“概念”。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;上述过程被总结H&lt;/span&gt;ebian学习的一个过程。  我们可想象，一个两层以上的神经网络， 就可以表述香蕉， 苹果， 菠萝这些水果了， 它们无非是底层特征颜色，形状的不同组合而已。 而这些不同水果的概念， 就可以帮助我们形成更加复杂的规则表 ，比如让它根据客户的信息帮它推荐一个水果拼盘。 由此可见， 神经网络通过与或非进行简单特征的组合 ，再通过if esle进行判断选择合适的特征得到概念， 再通过下一层迭代得到概念有关的命题。 就可以生成比之前的传统机器学习复杂的多的规则表。而且我们可以想象出来， 迭代的层数越多，它生成的“概念”和“规则”就越复杂。  当然真实训练中我们用到的不是模仿生物版本的Hebian学习， 而是强大的多的反向传播算法。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.6111111111111112&quot; data-w=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dmb6s-0-0&quot;&gt;事实上为了让这种生成“概念”得到“规则”的方法更加有效， 我们会加入一些无比强大的先验假设。 其中最有名的一组，  就叫CNN，它所做的，其实是对于图像这类巨大无比， 而局部特征不断重复的信息形式， 其实你可以写一个循环， 来让你的程序更有效。 循环里的模块每一步是可以共用的， 也就是卷积核。 卷积核一点点的卷个图像上的每个小块， 也就是循环的总体。 卷积核在每个图像局部做的， 事实上都是一个小的if esle 语句。 if像素之间符合某个关系，就是yes，否则No。这个结果， 最后被综合出来， 给下一层合成更复杂的图像特征。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7298850574712644&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibJ7GaCXDzw2dMW8QiaAPh96svf1lqjLico2MoDlVUCfFZgIHkysrSaRibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4vk9r-0-0&quot;&gt;好了， 到目前为止， 说的都是和时间无关的规则。 而一开始讲到的真实的图灵机， 是和时间有关的规则。 那么如何得到一个和时间有关的规则表呢？ 如果要处理和时间相关的信息， 你必须要引入记忆， 引入内部状态， 而和刚刚说的一样， 这些含时间的规则要是可以学习的， 用数学的语言说， 就是要有一个连续可微的载体， 这个东西就是RNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;72qm9-0-0&quot;&gt;def step&lt;/span&gt;&lt;span data-offset-key=&quot;72qm9-0-1&quot;&gt;(self, x):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;di032-0-0&quot;&gt;# update the hidden state&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4f218-0-0&quot;&gt;self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;243tl-0-0&quot;&gt;# compute the output vector&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;591p4-0-0&quot;&gt;y = np.dot(self.W_hy, self.h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a1b57-0-0&quot;&gt;return&lt;/span&gt; &lt;span data-offset-key=&quot;a1b57-0-1&quot;&gt;y&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;g9on-0-0&quot;&gt;以上是RNN的python程序定义。 它说的无非是你有一个刚刚说的线性分类器组成的单隐层神经网络， 但是这一回，神经网络的输出， 要作为输入，重新回到神经网络的隐层里， 这个关键的增加， 就使得它具有了处理复杂时间信息的能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5qh71-0-0&quot;&gt;这个结构，非但优雅，而且有效。一个非常重要的点是， 你知道信息的传播是有损耗的， 如果把RNN展开， 它事实上相当于一个和历史长度一样长的深度网络， 信息随着每个时间步骤往深度传播， 这个传播的信息是有损耗的， 到一定程度我就记不住之前的信息了， 当然如果你的学习学的足够好， Wij还是可以学到应该学的记忆长度。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26161790017211706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibPf1BYBmsfuHXyspOv0uwulVbuy0UibqOib2grXtp4XpvWU4b6Pj3402Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  事实上叫做“循环神经网络”  循环的本质是什么呢？    它其实正是你的程序里的for循环啊！ RNN的本质是， 在每个时间步里进行同样的操作， 这个操作无非是， 当下的输入， 和神经网络的状态两部分特征的逻辑组合（与或非）然后， 这个组合的结构进行一个if else的逻辑判断， yes or no， 根据这个，生成一个输出的结果， 这个结果， 要回传给神经网络隐层， 生成下一个隐层状态。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;大家看这其实就是图灵机的定义啊， 而RNN的本质， 就是一个可以通过微分方法学习的图灵机啊。 虽然每个步骤的规则和执行足够简单， 但是只要步数足够多， 却可以产生非常复杂的结果。  &lt;strong&gt;RNN学习的本质， 就是给你那个足够复杂的结果， 让你反演出那个足够简单的规则， 然后让它在新的环境下再去做预测与决策。&lt;/strong&gt; 我们可以看到， 这已经非常接近智能的本质了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5j932-0-0&quot;&gt;有关物理的世界和智能的世界&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;di5qc-0-0&quot;&gt;上面的这些思考无疑开始让人们想象我们所说的包含了逻辑推理， 情感，甚至意识的问题与物理世界的关系到底是什么。 我们说物理的世界里， 主宰一切的是微分方程。 一切因果关系， 都由微分方程所承载。 你有了不同不同微观粒子电磁力的描述，把它们放入薛定谔和狄拉克方程， 你就可以推出原子的不同性质。  这其实可以说是因果推理的极致了。 它甚至导致了机械的宿命论思想。当一切初始的原因输入系统， 那么它就回归于一个必然的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dl784-0-0&quot;&gt;到了非线性动力学的时代看似这点被混沌打破了，亚马逊的蝴蝶引起北美的飓风， 让通俗科学爱好折重新燃起了不可知论的希望，事实上并没有。  所谓的混沌， 无非是一种确定性下的不确定， 或者已知中的未知。 混沌的系统依然在一个被方程高度确定的洛伦兹吸引子里。&lt;/span&gt;到此处， 我认为微分方程依然是描述因果关系最精密的所在， 它可以在输入很少信息的时候， 得到最多的预测产出。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;在看刚刚的智能问题， 我们说， 整个智能问题， 到目前为止其实还是在围绕那张规则表， 只是我们的思路由制定规则表， 到了学习规则表。和物理比较， 目前的机器， 需要输入进去大量的数据， 才能生成一点十分简单的规则。 当然你可以举阿法狗下围棋的例子说明所生成的规则并没有那么简单， 可惜的是， 那些规则只适用于一些非常封闭而特定的领域。 而不像牛顿定律放之四海而皆准。  那么神经网络可不可以观测大量物体坠落的过程把万有引力定律给推出来呢？ 目前看是不能的。其实牛顿引力定律的得出是含有了大量的人类推理。 我们需要先知道物体运动改变和受力的关系， 然后通过观测物体的轨迹得到大量物体的受力情况，再在这些手里情况下得出某一种共同的作用力形式， 这是一个多么复杂的思维链条。 这对于目前统计的巨人， 而只懂得浅显的形式逻辑的神经网络，还是比较困难的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dsh1-0-0&quot;&gt;有关语言&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;讨论智能的问题离不开语言。从乔姆斯基开始， 人们就开始研究不同语言背后的共同语法基础。 其实如果深究语言问题， 我们会看到它和刚刚说的程序的联系。 &lt;strong&gt;语言无非是对世界的符号化，类似于给每个刚刚说的概念赋予一个符号。&lt;/strong&gt;而语言其实很像程序， 它就是对概念之间关系的表述。  我想语言和程序的区别可能在于语言更加模糊， 但是它对付不确定性的能力远远大于程序，因为这种模糊性， 让它具备了更好的适应能力， 可以表述那些用程序难以描述的事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt; 但是本质上， 语言无非是一个现有概念的符号体系， 描绘概念和概念间的关系。这样看以往的深度学习NLP其实是走了一条南辕北辙的路， 我们把不同的词汇和句子压缩成词向量， 句向量喂给神经网络学习， 而事实上神经网络对这些符号背后的实体概念却一无所知。虽然词向量也能稍微的带有一点不同词语之间的语义距离， 但是这和真实世界所含有的信息量，也依然是差异巨大。 目前用图卷积网络解决NLP的思路，算是一个进步， 因为它更好的涵盖了整个符号世界的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;语言， 好比一个巨大的人类经验和逻辑的宝库， 这个符号世界几乎就是真实世界的极好压缩体， 如果一天神经网络真正被赋予了语言的power，也就是能够真正理解这个符号世界， 或许离通用人工智能也就不远了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;有关物理世界和语言世界的打通&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;刚刚说的精确的物理方程的世界， 和能够应付更多不确定性的模糊的语言， 之间又有哪些联系呢？  我的想法是， 物理的杀手锏微分方程， 当构成了一个非线性的动力学系统， 却可以通过它内在的定点， 极限环，吸引子等概念， 去接近那个模糊性的语言， 就好比在非线性动力学的世界里， 我们往往不再那么关于一个系统如何发展的暂态，很多不同的系统都归一于一个吸引子， 那么它们背后的逻辑可能就是类似的。 这或许会架起一座物理世界和语义世界的桥梁？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 18:31:05 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/q56XOgLetH</dc:identifier>
</item>
</channel>
</rss>