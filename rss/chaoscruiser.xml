<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何让让神经网络开口说话</title>
<link>http://www.jintiankansha.me/t/o5DC8CEtwb</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/o5DC8CEtwb</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;人类简史说，人类的进步大大依靠与人的语言，人在灵长类中具有最高级的语言能力，并且能够编故事，从而把越来越多的人组织在一起产生了文明，这才是人类社会进化的根本。&lt;/span&gt;那么，神经网络能否掌握人类的语言，听懂人类的故事呢？ 相信朋友们都知道谷歌翻译，谷歌助手，机器写诗， 它们实现了令人惊艳的性能，并在某些具体任务里让人真假难辨。这里的技术核心，就是&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;-&lt;/span&gt;&lt;span&gt;我们常说的传说中的循环神经网络。&lt;/span&gt;RNN&lt;span&gt;可以称得上是深度学习未来最有前景的工具之一。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.65379113018598&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNw72RI3PwDHibjxNr7t1oNQLSv5FLvmPR6GDkZTg0F5aJ9WutibYapUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;699&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;时间序列和&lt;/span&gt;RNN&lt;span&gt;引入&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让我们从基础学起，&lt;span&gt;首先&lt;/span&gt;什么是时间序列，&lt;span&gt;我们又&lt;/span&gt;为什么需要&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6744730679156908&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k663I0G9glMsglxZ9z9S1YH5lmWibkRWLeaj0kU2MwM4WGibDickz81R0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6227848101265823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kwibz1UiaqqF3Aqtee5PF7wZJAFfRAVgWGoGHicAfb7XibGyhibKztNtay6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;395&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;时间序列，&lt;span&gt;是&lt;/span&gt;一个随时间变化的过程，&lt;span&gt;我们&lt;/span&gt;把它像一个数列一样排列下来，&lt;/span&gt; &lt;span&gt;&lt;span&gt;序列&lt;/span&gt;里的数字往往&lt;span&gt;看起来&lt;/span&gt;在随机波动。&lt;/span&gt; &lt;span&gt;&lt;span&gt;一定&lt;/span&gt;程度，&lt;span&gt;我们&lt;/span&gt;可以把它看做以一个一维的图像或向量，&lt;span&gt;这个&lt;/span&gt;图像不停的向前滚动。&lt;span&gt;比如说文字，就是一个典型的时间序列。&lt;/span&gt;处理和时间有关的信息，我们再次回到我们的大脑，看我们的大脑是怎么处理这一类问题的。&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;实验发现，大脑越靠近感官的区域就越像CNN的结构，它的最本质特征是前馈，也就是每一次神经信息都是从感官向大脑的深层一步步推进的，而每层网络之间是没有联系的。而到了深层，这一切发生了变化，大脑内开始出现一些神经网络，这些网络在层间出现了很多的连接，这意味着什么呢？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8511705685618729&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQek5FIxZefqx6QJemfLiceYcbaLAG6P7V3JV0ibsxH67IUb3yAOLSO2A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;span&gt;&lt;span&gt;我们说，我们需要引入时间这个维度才能完全理解这件事，这些层内的链接意味着当下的神经信息会在下一个时刻被层内的其它神经元接受，而这个接受的结果呢？&lt;/span&gt; &lt;span&gt;当下的信息会传给这些层内的神经元，&lt;/span&gt; &lt;span&gt;从而使得这个信息在网络内回响一段时间，就好像社交网络里人们互相发送消息或分享朋友圈一样一个事件发生，就会引发一系列的社交网络动作使得信息的影响停留很久。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个典型的例子是如果人和人彼此链接形成回路，我发出的信息可能会在若干时间又传递给我，从而让我自己直接看到我的过去历史，这个信息停留的效应是什么？大脑处理时间相关信息的关键正是记忆。 最典型的在一个对话过程里，你要记住此时此刻所说的话， 还要关联前面很久的话，否则就会发生歧义。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;引入了记忆， &lt;span&gt;这些问题就好解决了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;记忆分为不同的种类， &lt;span&gt;你在一个对话里可能回忆起很久以前的事情，&lt;/span&gt; &lt;span&gt;但是这和刚刚说的话的重要性显然不一样，&lt;/span&gt; &lt;span&gt;所以我们大脑就把这些记忆分成了长时的记忆和短时的记忆（工作记忆）。这个短时记忆，&lt;/span&gt; &lt;span&gt;正是通过互相高度连接成回路的网络之间的神经元互相喊话造成的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;一个神经网络里神经元之间彼此互相传递信号，形成循环往复的回路结构，就可以让过去的信息保持下来，把&lt;/span&gt;这个&lt;span&gt;原理转化成人工神经网络就是&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;，&lt;span&gt;翻译出来就是循环神经网络，这个循环，正是彼此链接形成回路的意思。我们看怎么把模糊的想法一步步变成一个数学模型。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5401174168297456&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kI3cicAeAs54DqNJF4toeQm7fEB9FGnQ0MINuiauBAydgoGdhvrnTJ67g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1022&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们把这个网络内部神经元互相链接的方式用一个矩阵Wij表达， 也就是从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;i&lt;/span&gt;到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;j&lt;/span&gt;的连接强度表示。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wij&lt;/span&gt;就是神经元间彼此沟通的方法，&lt;/span&gt; &lt;span&gt;我们看rnn的方程，一方面网络要接受此刻从外部进来的输入，另一方面网络接受自己内部的神经元从上一个时间时刻通过&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wi&lt;/span&gt;进来的输入，这一部分代表过去的历史 ，决定网络此刻的状态。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23720136518771331&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kAiaMKQFH5unWWVxrqymUcsCoMmTxMaVhNnsI8TZic9CdBicyhhib9Z51Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;586&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;深度学习的核心就是特征提取。用一句话说，&lt;/span&gt;RNN&lt;span&gt;具备的素质就是把整个过去的时间信息转化成此刻的一组特征，然后让网络做预测或者决策。 比如，此刻股市的价格是所有之前和公司有关的信息一点一滴积累起来的。公司每个时间点的信息， 就是输入向量，&lt;/span&gt; &lt;span&gt; 那么神经元所干的事情是什么呢？&lt;/span&gt;  &lt;span&gt;它把所有过去的信息转化为当下的神经元活动， 而这些活动，就是一组由过去历史信息组成， 决定当下预测的特征。在股价的情况下，你可以想象成， 这些神经元就在描绘人们的信心指数， 而信心是所有过去点信息汇集的结果，RNN把每个时间步的信息通过神经元之间的互相喊话Wij，压缩成当下的一个状态向量hi，它包含了所有和我此刻决策有关的历史。 数学上你可以推到， 这个hi是所有过去输入的一个函数.&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;学说话&lt;/span&gt;&lt;/span&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;什么叫预测说话，这是一个形式上开起来有点傻的例子， 就是给你一个序列的字母或单词让你猜下一个，我想你一定玩过报纸上的填词游戏。 那个每个时间步骤的输入就是一个字母，然后我要输出最后一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;一个处理这个问题的经典模型叫N-gram&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;它是说我可以把这个猜词的游戏看成一系列条件概率来处理，&lt;span&gt;用这个模拟过去的字母对当下字母的影响。因为我们知道语言本身存在非常清晰的规律，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这个规律就是字母都是成对出现的，&lt;/span&gt; &lt;span&gt;如果全面我给你的字母是&lt;/span&gt;hell&lt;span&gt;， 且还有一个字母，那么你基本就知道是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;o&lt;/span&gt;&lt;span&gt;了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;这个现象我们可以用字母共现概率表达，也就是说衡量一些字母在一起的概率&lt;/span&gt;P&lt;span&gt;（&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;w&lt;/span&gt;&lt;/span&gt;1, w2,w3…&lt;span&gt;&lt;span&gt;）&lt;/span&gt;,&lt;/span&gt; &lt;span&gt;&lt;span&gt;那些经常在一起出现的字母概率会很大，&lt;/span&gt; &lt;span&gt;而其他很小，&lt;/span&gt; &lt;span&gt;我们可以用经典的条件概率公式来表达，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;这个事情。&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.03968253968253968&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQz8rY74Za5wiaIaMLlVu9rZfE2QY8lKzQ2WOQoPnQAzJ1LFGdOY2DUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1260&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个公式的意思非常清楚， 就是字母和字母之间不是互相独立的， 就是刚刚说的后面的字母极强的依赖于前面的字母， 这种关系通过条件概率体现。 这件事我们用一个图画出来，就是当下是所有过去的作用， 而一旦字母多了以后，这件事就特别复杂。我们用物理学家惯用的加入假设来简化降维，就是我当下的字母与过去相关，这种相关却是有限的， 比如只与前面n-1个相关。&lt;/span&gt; &lt;span&gt;这个模型就是n&lt;/span&gt; &lt;span&gt;gram&lt;/span&gt;  &lt;span&gt;。&lt;/span&gt;  &lt;span&gt;比如说 只与前面一个相关，这就是2-gram，&lt;/span&gt; &lt;span&gt;而这个模型就是标准的马尔科夫链。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;那么，这里我们换一个模型来，&lt;span&gt;我们用&lt;/span&gt;rnn&lt;span&gt;来做。具体怎么干？ 每个字母给他编个号码（独热编码），我一个一个的输入给这个网络，网络每一刻的状态取决于此刻的输入和上一刻的网络内部状态，而上一个网络内部状态又取决于过去的，输入，这样当我整个单词输出完毕，每个字母的信息可以看作都包含了在了神经元的状态里。我们要&lt;/span&gt;把整个输入切分成小块，&lt;/span&gt; &lt;span&gt;用一个卷积核把&lt;span&gt;它们&lt;/span&gt;卷入到一个隐层网络里，&lt;span&gt;只不过此处的&lt;/span&gt;&lt;/span&gt;Wij&lt;span&gt;&lt;span&gt;代替了&lt;/span&gt;CNN&lt;span&gt;的卷积核，把历史卷入到一个隐层里。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们用一段小巧的&lt;/span&gt;python&lt;span&gt;代码让你重新理解下上述的原理：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.31364031277150306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kpLMkok673hoAia1JmwT3BCCtSajFRyeRxvM9R2KMeRahYMlNZxULcAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1151&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个结构，&lt;span&gt;非但&lt;/span&gt;优雅，&lt;span&gt;而且&lt;/span&gt;有效。&lt;span&gt;一个非常重要的点是，&lt;/span&gt; &lt;span&gt;我们不必在假定那个&lt;/span&gt;n-gram&lt;span&gt;里的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;n&lt;/span&gt;&lt;span&gt;，这时候，因为原则上&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;是所有历史输入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;I1&lt;/span&gt;&lt;/span&gt;…It &lt;span&gt;&lt;span&gt;的函数，&lt;/span&gt; &lt;span&gt;这个过程，&lt;/span&gt; &lt;span&gt;也就是说我们具有一个&lt;/span&gt;&lt;/span&gt; &lt;span&gt;infinit&lt;/span&gt; – &lt;span&gt;gram&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;&lt;span&gt;你来思考一下这是真的吗？&lt;/span&gt;No&lt;/span&gt; &lt;span&gt;，&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;你知道信息的传播是有损耗的，&lt;/span&gt; &lt;span&gt;如果把&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;展开，它事实上相当于一个和历史长度一样长的深度网络，信息随着每个时间步骤往深度传播，这个传播的信息是有损耗的，到一定程度我就记不住之前的信息了，当然如果你的学习学的足够好，&lt;/span&gt;Wij&lt;span&gt;还是可以学到应该学的记忆长度。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2611683848797251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kFoiaJw1g1Q2BhwvicUsrbnibR4o1JDmfnOIGU1uY1cvSwkongmI6DmOcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;582&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图：循环的本质&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;刚刚的词语预测模型，我们最终的目标是每次预测出现的下一个字母，我们的方法是在隐层之上加入一个读出权重，这个矩阵的作用如同我们之前讲的感知机，是在有效的特征基础上加一个线性分类器， 再加一个softmax函数得到一个向量每个数值代表出现某个字母的概率。然后我们希望我们的向量能够完全预测真实字母出现的概率，因此我们把真实数据作为输入不停的让他预测这个字母，直到这个概率和真实是最匹配的，我们就得到了训练好的模型， 然后我们就可以让他生成一段文本了！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8045325779036827&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kt9lGY722ic5zh0p4qugEHz7MXuAGUH9HJmXaB1ibksd7iadjJSvjLTAHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;706&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上面介绍的过程是语言介绍的最基本的部分，这里所说的对字母进行独热编码这件事，如果进入到文本的世界，你要预测的不是字母而是单词， 这时候，我们通常不再采用独热编码，而你自己思考这件事显然是不合理的，因为词语之间相互关联，如果你把词语本身作为完全独立的东西来编码，你事实上就是丧失了这种本来的语义结构的信息，因而我们用更复杂的word2vec来替换，这个方案的核心依然是编码，只不过这套编码也是从神经网络里学过来的，具体来说，这套编码可以把语言内在的语法或语义信息包含在编码向量的空间结构里， 从而使用这部分语言有关的先验信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9627749576988156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kcnRq0wZJO8FLnRkeIrAmTWoF1W7Idhuz1lS0Lp00fbqySqScyGSx1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;591&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;的物理与工程理解&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们说把对&lt;/span&gt;RNN&lt;span&gt;的理解&lt;span&gt;在&lt;/span&gt;抬升一个层次，RNN&lt;span&gt;在物理学家眼里是一个动力系统，&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;&lt;span&gt;循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖&lt;/span&gt;-&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;路径依赖&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。另一个角度看也符合著名的图灵机原理。&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;即此刻的状态包含上一刻的历史，又是下一刻变化的依据。&lt;span&gt;工程上看，&lt;/span&gt; 这其实&lt;span&gt;就是&lt;/span&gt;可编程神经网络的概念，即当你有一个未知的过程，但你可以测量到输入和输出，&lt;/span&gt; &lt;span&gt;你假设当这个过程通过&lt;/span&gt;RNN&lt;span&gt;的时候，&lt;span&gt;你要设计一个程序来完成&lt;/span&gt;这样的输入输出规律，&lt;span&gt;那么这个程序可以被&lt;/span&gt;RNN&lt;span&gt;学出来&lt;/span&gt;。 &lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.071278825995807&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMUht40XmvRwwC2zjpTEgjibqDiakMScEicDqfDYEz4tnGmlrU44Nf0hQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;477&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;总的来说&lt;/span&gt; &lt;span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;无论作为&lt;/span&gt;一个非线性动力系统&lt;span&gt;还是程序生成器&lt;/span&gt;的&lt;/span&gt;RNN&lt;span&gt;, &lt;span&gt;都需要依然数据背后本身是有规律可循的，也就是它背后真的有某种“&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;program&lt;/span&gt;&lt;span&gt;”而非完全随机&lt;/span&gt;。&lt;/span&gt; &lt;span&gt;如果一旦&lt;/span&gt;RNN&lt;span&gt;学习到了&lt;span&gt;真实&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  数据背后的&lt;span&gt;动力系统&lt;/span&gt;的性质，&lt;span&gt;它也就&lt;/span&gt;掌握了过程中复杂的路径依赖，从而能够对&lt;span&gt;过去&lt;/span&gt;和&lt;span&gt;现在&lt;/span&gt;进行建模。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;生成诗歌&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;诗歌的生成，方法类似于句子，区别在于，诗歌是有主题的，我们都知道主题的重要性，同一个句话在不同主题下含义完全不同，如何把这个诗歌的主题输入给rnn呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;事实上，我们可以把这种主题或者背景看作是一种上下文关系，比如说一首诗歌有四行，在生成第一行的时候，我可以输入开头一个关键词，然后让rnn自动生成一行，虽然这个过程还有一定随机性，但是这一行内容无疑确定了诗歌整体的基调，因此，我们需要把这种信息编码成一个包含上下文含义的内容向量，这个向量作为整个网络的输入。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3133535660091047&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4klibJGiaACy2FrV6BFJ7WN70mlv5gI20U41fV8hQD4v7FKrqEpZicuPhJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1318&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们采取的方法是逐行生成诗歌。已经生成的行就作为后面生成行的基调，每行诗歌的生成都使用和之前一样的RNN，但是，它的输入要加上一个主题向量。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如何提取主题向量呢？首先，它是一种由所有之前生成行构成的宏观信息，那么，我们也可以用一个RNN来提取它，由于宏观， 这个RNN的输入是每行诗歌。而我们要用一个方法， 直接把行编码， 而不像刚刚是把字母或单词编码。 一行诗歌本身每个子都是一个向量，整行诗歌构成一个二维的图像（字数一定， 图像尺寸一定）。记得什么东西最好处理图像吗？我们引入一个CNN网络，把这些同样尺存的“图片”，压缩一个特征向量，最后被这个被称为主题RNN的RCM卷成一个主题向量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4824766355140187&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kCiaqcL6B6pl0WAZxmnxWK42KjPXt6sN79tkHTyicHqrpXt5BpxNBFsog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;856&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个主题向量作为一个先验输入交给RGM， 这个普通的RNN， 作为这行诗词生成的关键。由此生成的诗歌不仅是押韵的，而且可以构成一个完整的意思。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kzn6aYicN4L5a7symMoAvt8vusq6QLdcCzYdSgXJRzks7DgfhDrNibMKQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1001&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;引入长短记忆&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;RNN虽然看起来好用，&lt;/span&gt; &lt;span&gt;&lt;span&gt;而且似乎&lt;/span&gt;能够模拟任何一个&lt;span&gt;动力过程或程序&lt;/span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;实际&lt;/span&gt;中，&lt;/span&gt; &lt;span&gt;&lt;span&gt;却&lt;/span&gt;并没有那么容易。&lt;/span&gt; &lt;span&gt;&lt;span&gt;为什么&lt;/span&gt;？&lt;/span&gt; &lt;span&gt;RNN&lt;span&gt;的&lt;/span&gt;强大&lt;span&gt;功能&lt;/span&gt;，&lt;span&gt;体现在&lt;/span&gt;能够学习&lt;span&gt;过去&lt;/span&gt;时间点对现在的影响这件事，&lt;span&gt;但是，我们刚刚说了&lt;/span&gt;RNN&lt;span&gt;相当于于一个无限深的深度网络， 而传播是有损失的， 假定每次这个损失是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.99&lt;/span&gt;&lt;span&gt;， 经过&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;100&lt;/span&gt;&lt;span&gt;层后也是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.36&lt;/span&gt;&lt;span&gt;， 这种信息传递的递减， 会导致&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;无法学到长时间的信息之间的关联性&lt;/span&gt;。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的一个重要的解决这个问题的技巧是&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;引入一个能够学习多个时间尺度的改进版&lt;/span&gt;RNN&lt;/span&gt; – &lt;span&gt;LSTM（&lt;/span&gt;Long short term memory&lt;span&gt;）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6776556776556777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koTXDZNjRYuG90Osb9NhnCzCCkCCq8FTCC50WSkObHsKBQ1bJEXQf0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;819&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;首先，什么是时间尺度，宇宙间最神秘的概念莫过于时间，但是绝对的时间毫无意义，一个时间的长短，一定是根据你所描述的过程，&lt;/span&gt; &lt;span&gt;&lt;span&gt;比如你是描述一个人一生的变化过程，还是描述一次化学反应，&lt;/span&gt; &lt;span&gt;还是生物进化，各自的时间尺度可以有级数之差。我们对时间尺度最数学的理解来自原子的半衰期。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于大脑神经尺度，处理动态过程最麻烦的东西就是时间尺度， 因为生活中的事情往往是多时间尺度的，比如你的一个决策今天晚上吃不吃饭，可能既受到刚刚是不是饿了的影响，又受到这个月你是不是有减肥计划的影响，还受到你长期养成的饮食习惯的影响，因此， 你的大脑需要有对复杂时间尺度的处理能力。也就是说，同时对各个不同的时间尺度变化的信息保持特异性和敏感度， 这和我们图像识别里需要对图像的局部和整体反应是类似的。一个有意思的电影inception描述要改变一个人的意念，我们需要一步步的走进他思想的最深层 ，逐层的改变它对某个东西的认知，而每个层里的时间尺度又有不同，就是对这件事最好的体现。事实上，类似于&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;inception&lt;/span&gt;的描述，我们的确发现在我们的大脑里，有着不同时间尺度的处理， 越浅层，我们就对越近的东西敏感，而进入到大脑的深层，我们开始对慢过程敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;LSTM&lt;span&gt;，所谓的长短记忆机，这是对这个过程的模拟。我们来看它是怎么对&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;进行改进的&lt;/span&gt;，&lt;span&gt;这个道理非常简单，首先，我们加入一个叫做记忆细胞的概念，进入到记忆细胞的信息，可以永久不被改变，&lt;/span&gt; &lt;span&gt;但也可以根据一定触发条件修改，实现的方法是我们加入一些&lt;/span&gt;控制&lt;span&gt;信息流动&lt;/span&gt;的阀门&lt;span&gt;在这些记忆细胞之间&lt;/span&gt;，&lt;span&gt;这个&lt;/span&gt;阀门随着输入和隐层状态决定，&lt;/span&gt; &lt;span&gt;&lt;span&gt;如果是&lt;/span&gt;1&lt;span&gt;，我们让过去的记忆完全进入到当下，信息丝毫不衰减，如果阀门的值是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;，就彻底的遗忘，如果是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;1&lt;/span&gt;&lt;span&gt;就是在一个时间段里记住这个值就是一个时间尺度。只要&lt;/span&gt;控制好这个&lt;span&gt;阀门&lt;/span&gt;，&lt;span&gt;我们就得到了一个动态的可以学习的时间尺度。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5616161616161616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kibwMIicLVK1VNiclzdj2JchRw3DLa25DdPhEuuVRmEXyfqVSib7N9762qA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;495&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;刚刚讲的阀门被称为&lt;/span&gt;遗忘门，&lt;span&gt;为了配合它，我们还加上输入门和输出们，&lt;/span&gt; &lt;span&gt;控制有多少新的信息流入，有多少输出，&lt;/span&gt; 这三层门&lt;span&gt;整体构成一套&lt;/span&gt;信息的闸门，门的形式都是可微分的&lt;/span&gt;sigmoid&lt;span&gt;函数，确保可以通过训练得到最佳参数。根据这一原理，我们可以抓住本质简化&lt;/span&gt;lstm&lt;span&gt;，如&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;或极小&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;span&gt;。其实我们只需要理解这个模型就够了，而且它们甚至比&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;span&gt;更快更好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看一下最小&lt;/span&gt;GRU&lt;span&gt;的结构：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42168674698795183&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kVcbiaYKKbxIBINvz2ISUyxxTcDHga7tFPdTwAdI3jhg7HVN974FPsEg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2278719397363465&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kkKhNMtqduPrfiaLqFiaHqUncdD0kH3v9iax33BgFxU4Y4eZdDHyfTiaABg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;531&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;第一个方程&lt;/span&gt;f&lt;span&gt;即遗忘门，第二方程如果你对比先前的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;会发现它是一样的结构，只是让遗忘门&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;f&lt;/span&gt;&lt;span&gt;来控制每个神经元放多少之前信息出去（改变其它神经元的状态），第三个方程描述&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;“&lt;/span&gt;&lt;span&gt;惯性&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;”&lt;/span&gt; &lt;span&gt;，即最终每个神经元保持多少之前的值，更新多少。这个结构你理解了就理解了记忆体&lt;/span&gt;RNN&lt;span&gt;的精髓。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Attention&lt;span&gt;版&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;LSTM&lt;/span&gt;&lt;span&gt;与宋词生成&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然，我还可以把这个多时间记忆的东西玩到极致，最终我们可以得到一个升级版本的诗词处理器，&lt;/span&gt; &lt;span&gt;我们要加入几个新的概念：&lt;/span&gt; 1&lt;span&gt;， 双向编码&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;/span&gt; 2&lt;span&gt;，&lt;span&gt;多层&lt;/span&gt;lstm&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;3&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为什么双向，因为语言也可以倒过来念啊，甚至很多时候后面的内容越重要越能决定主题， 比如一句话一旦出现“但是”一定是但是后面的内容更中重要有决定性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.35340314136125656&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kTd77wRVfeOS7WzAsLiaibGGIic5BlVrbUlicVLQ6JaibBC6XdiaS8ibOSacOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;764&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么要多层， 记得我刚刚讲过的大脑结构吗？ 如果每层的lstm学习的事件尺度敏感性不同， 会更好的处理多事件尺度新消息， 比如从词语到句子到段落。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;最后，也是最关键的，看看我们如何引入&lt;/span&gt;a&lt;/span&gt;ttention.&lt;span&gt;。&lt;/span&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Google&lt;/span&gt;&lt;span&gt;这一次&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2016&lt;/span&gt;&lt;span&gt;寄出的大法，是在其中加入了&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;机制 ，&lt;span&gt;首先大家理解人脑的注意力模型，人脑的注意力机制的特点是在认知资源有限的情况下，我们只给那些最重要的任务匹配较多的认知资源。&lt;/span&gt; &lt;span&gt;这个机制实现的方法正是&lt;/span&gt;attention&lt;span&gt;。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，在引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;之前，我们的想法是既然我们最终的决策想利用好所有的历史信息，而每个时间的隐层状态都是对那个时刻时间状态的总结，我们完全可以把所有时间点的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;作为一个特征使用， 这一点， 而不只是最后的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;，这点在文章分类这类任务里特别好理解，但是每个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;动辄上百维度，所以当我们把所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;合成在一起的时候 ，我们就会得到几十万个维度，我们再次抛出降维度找重点的思维， 加入一个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;来吸取和当下预测最重要的信息。&lt;/span&gt;&lt;/span&gt; A&lt;span&gt;ttention&lt;span&gt;又称为&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;content&lt;/span&gt;&lt;/span&gt; &lt;span&gt;base&lt;/span&gt; addressing&lt;span&gt;&lt;span&gt;，因为过去哪个东西比较重要，往往取决于我现在要预测的信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.834070796460177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kr774SyBt7bMVY4qVm4sNr1olrWH3nAz6Nfc4XO4dgPGpBUYva0VR2g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有了这个机制，我们可以抛出我们唐诗生成器的改进版，宋词生成器，宋词的生成，确实是比唐诗更复杂的一个东西，因为宋词更长， 更多变，句子长短不同，也更需要多时间尺度。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8047244094488188&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kUlL5A2z6dcIxib9cCsOFiaGaibVDVrnQepcrmpvHqD2MYiaF43ia7EDDy5Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这一次，我们可以把宋词的结构看作是一种对话体，&lt;/span&gt; &lt;span&gt;我们用一个把问题编码，&lt;/span&gt; &lt;span&gt;然后直接从这个基础上预测对上一句的回答，这里，我们生成第一句词的技巧和之前是类似的，&lt;/span&gt; &lt;span&gt;从这个生成的第一句词开始，我们用编码器&lt;/span&gt;LSTM&lt;span&gt;来把这一行编码作为下一句的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;， 然后解码器把这个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;转化为下一行词。为什么我们要用&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;呢？ 因为上一句的所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;被叠加成一打，刚刚说到的，这时候会造成信息过大，所以我们引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;的机制来注意相应的信息， 这样我们就可以找到上一行和下一行之间精细的相关性。&lt;/span&gt;&lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.9306451612903226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k2zxW29QBMksaJTicu0H9yQibxviapVtlRJSVSHvTctMCyibxLkt3HxeWtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;大家可以比较和唐诗生成的时候， 我们的结构不仅更简洁，而且能够处理更难的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;模拟人类对话&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最终， 我们已经接近了让神经网络听懂故事的境界， 你是否认为类似的结构可以用作和我们对话呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kJ6ZIVojPFnRal0gAGQFFaSicQKYYTQmaJzSTJlYhTWslB3yWOKA6uFA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;476.75359712230215&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;491.23201438848923&quot; data-ratio=&quot;1.0294117647058822&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kqgbTa8IAQ8ofH5QVGuYaKLrsdrItYENGJ2hm9hKBDdxOx6bUicmKXfQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;476&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;因为你我在对话时候有大量的背景知识，&lt;/span&gt; &lt;span&gt;而机器人是没有的，&lt;/span&gt; &lt;span&gt;但是有没有补救的方法？&lt;/span&gt; &lt;span&gt;当然有，&lt;/span&gt; &lt;span&gt;谷歌的问答系统，&lt;/span&gt; &lt;span&gt;已经把很多重要的背景知识放入了神经网络，&lt;/span&gt; &lt;span&gt;这种加入很长的背景知识的结构，&lt;/span&gt; &lt;span&gt;被我们称为记忆网络，&lt;/span&gt; &lt;span&gt;事实上，它是对人类的长时间记忆的模拟。&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这些长时间记忆，&lt;/span&gt; &lt;span&gt;包含知识，或者地图等。用它做的聊天机器人，可以成为你电话里的助手，但是，&lt;/span&gt; &lt;span&gt;你是否认为这样的结构已经具备了真正理解语言的能力呢？&lt;/span&gt;   &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;参考文献&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The unreasonable effective RNN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Minimal Gated Unit for Recurrent Neural Networks&lt;/span&gt;&lt;/p&gt;







</description>
<pubDate>Fri, 08 Mar 2019 14:53:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/o5DC8CEtwb</dc:identifier>
</item>
<item>
<title>让神经网络看懂图像</title>
<link>http://www.jintiankansha.me/t/C8XQvUfjg6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/C8XQvUfjg6</guid>
<description>&lt;p&gt;视觉的重要性毋庸置疑， 你可以想象，我们平时的生活， 从识图辨物， 到读书看电脑， 哪一个离不开视觉。 所谓的互联网信息大爆炸， 你看看我们手机空间里的大部分图片是什么， 一定是照片。 所以， 我们说视觉占领了我们信息的主体。这背后深层的原因是视觉相对听觉或触觉对真实世界的信息效率大的多， 一个图片可能包含很长一段文字的信息， 这点是其它渠道所不能比拟的。生物进化出视觉而有了寒武纪大爆发， 那么让机器拥有视觉能力， 一定是让它变得更聪明的第一步。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.37558062375580625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koFQJ0KyYo2twh0dBzVDic9bb8quJZRv5c5pBP6oEFiafTobNPhstZugw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1507&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;人脑对图像的认知：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;电脑记录下来的图像是由一个个像素构成的，每个像素又分为r，g，b三个通道（可以理解为垂直排列的三个像素），这三个通道起到复现整个光场的作用。而事实上物理里的真实的图象， 是一个由无数光子组成的电磁场， 这个电磁场在我们的视网膜上振动， 从而形成了我们对图象的感知。 因此， 归根到底 ，我们是用大脑， 而不是用双眼来感知图象的， 也许我们永远无法知道真实世界是什么样， 但是是我们的大脑赋予了它形象， 一个很好的例子就是你分不清猪的美丑， 但我想猪是可以的， 这正说明了所谓的相由心生， 你不关心， 就看不到。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;那么什么是我们大脑处理图象的神经基础呢？  多少代的科学家研究这个问题， 最终有了一个比较完备的答案。   一个眼睛正常的人不一定能够产生对视觉的知觉， 有一种叫视觉认知症的人： visual agnosia，   它们虽然可以看见物体， 却无法区分一个物体时什么， 课件， 看到， 不等于知道。 事实上， 大脑对视觉的感知主要时通过视觉回路实现的， 这个视觉回路的概念 ， 主要是通过视皮层V1-V4完成的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5879120879120879&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kb3jh8KiboexvaqpcbeXAAWHYPVGfrYdGsW44hGIrAbGegiadpFme1RqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;910&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 这个v1到v4的视觉回路，  本质起到的作用是一级一级的筛选视觉特征。  我们之前讲过， 每个细胞都相当于一个小的特征检测器， 而我们事实上发现， 这些小的特征选择器所检测的目标是不同的， 有的对简单的特征敏感， 比如桌子的轮廓边角， 有些对复杂的特征敏感， 比如桌子的腿或边角， 一个重要的假设是复杂细胞形成的基础正是简单细胞的组合， 很多简单细胞的输入构成了复杂细胞。  而最“复杂”的一些细胞， 居然会对那些抽象的人名，物体概念敏感。  为了表达这种极端的特性， 我们把这类极为复杂的细胞称为“祖母细胞”就好像每个人的脑子里都有那么一个细胞对自己的祖母是反应的， 它就是祖母的代言。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0314465408805031&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kOlUDudCyB33jMJ5wnvGPibNKORj7lBVRqKJp2zNgicp8RBjwEIPibGia0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;636&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;这种表面的“简单”， “复杂”其实可以被一个称为为层级编码假设的理论解释， 说的是比较底层的细胞先得到从视网膜传来的视觉信号（类似数码相机的图象）进行处理， 然后所谓的“复杂”细胞， 无非是把最底层的特征拼接组合起来， 得到比较了比较复杂的特征。而最终当我们得到的祖母的头， 鼻子， 或眼睛这些特征的时候， 在最后进行一次综合就得到了“祖母细胞”这种复杂概念的对应物。当然， 这只是粗浅版本的视觉编码机制。 很多人认为除了层级特征， 视觉编码还需要具有集群编码的特性， 也不一定存在那么一个特定的祖母细胞， 而是概念被一个细胞发放的集体模式所表达。 这些我们就不一一详述了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模拟人脑的CNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如何把图象让机算计处理呢？ 我们可以在深度学习兴起以前， 这是一个超级超级难的问题。 我们就拿机器视觉最简单的例子： 图象分类来说。我们前两节课讲过应鸢尾花的识别， 在这个例子里， 我们看到的实际状况是花的照片来了， 然后我们的植物学家告诉我们花瓣的长度和宽度是重要的特征， 它可与把鸢尾花分为三类， 这样，我们的计算机就可以用前面讲过的KNN把花分成三类。这个方法里， 计算机事实上接受的一个表格数据， 也就是花的特征总结， 而得到一个分类的结论。 非常可惜的是， 这和真正的图象识别相差甚远。 因为真正的图象识别，意味着我们直接把图象，也就是我们看到的原始数据给计算机处理。 或者说， 计算机需要自己找出像花的长度和宽度这样的特征。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7663197729422895&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKiadibV0TFibYrLIgxtiapoYGNARzgOMy6SiaiaTUa01jNaakulibgMCDo9bA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1057&quot;/&gt;&lt;/p&gt;

&lt;p&gt;刚刚说了， 计算机眼里的图象是一个巨大的矩阵， 首先图象由像素组成， 每个像素就是一个数字， 它代表我们对信息的采样。 像素组成的图象是黑白的， 然后我们需要对不同波段的光波分别形成这样一个黑白图， 然后把它们拼接在一起，得到我们最后的彩色照片，比如我们拿一个日常的3x256x256的图像看， 那个像素就是256x256个，然后有三个色彩通道。 如此组成了一张图片。最终这个图象这样的信息维度是巨大的。 远非机器学习的常见问题可以比拟。&lt;/p&gt;

&lt;p&gt;让机器来直接看图，这个在过去看似不可能的技术，被一个叫卷积网络的东西给解决掉了，在2012 ，它超过了所有的视觉算法， 并在随后几年在很大的数据集上赶超人类。这个卷积网络正是对刚说的生物神经网络的直接模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是卷积&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;你要理解卷积， 只需要理解一个东西叫滤镜， 人类在处理图像问题的时候， 最有名的发明莫属photoshop了， 在ps里你可以把图片调整各种各样的色调，模糊，锐化， 这些东西统统是一个叫做滤镜的东西做出的。&lt;/p&gt;
&lt;p&gt; 滤镜这个玩应， 你可能想到镜头前的镜片，事实上，它所做的事情是把图像转化为 一个另一个图片。 它是怎么做到的呢？ 数学上的操作，正是今天讲的卷积。数学上， 这些操作对应的运算都有一个特点， 就是对局部的信息进行综合 ，得到一个新的信息。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5936352509179926&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNMkicsrDnx2xgaJqavCTW5TiaicO92Sj5mIUn7SZ58zAFTs18pHkyicdPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;817&quot;/&gt;&lt;/p&gt;

&lt;p&gt;看看卷积的数学操作，卷积，顾名思义， “卷”有席卷的意思，“积“ 有乘积的意思。 卷积实质上是用一个叫kernel的矩阵，从图像的小块上一一贴过去，一次和图像块的每一个像素乘积得到一个output值， 扫过之后就得到了一个新的图像。我们用一个3*3的卷积卷过一个4*4的图像。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6256627783669141&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kiafx1W2wMDMpcpjib6VfT1xRjogT5ptswDhOPM6X16H3TTcteENjmpcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;943&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积网络的基础正是这样的卷积， 我们说通过一个滤镜我们可以提取一个图象的特征， 那么为什么我们要采取看起来这么笨拙的一个方法呢？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图像识别与降维&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其实要让神经网络告诉某两个照片是香蕉还是苹果， 并不是那么难， 但是图像识别的根本目标是你要识别整个世界的香蕉或苹果， 这个问题背后的核心是我们之前讲的泛化。  也就是让它理解苹果这个概念。当然你可能会想到苹果是红色， 圆形这种具体的特征，这些特征变化了，它就不是苹果了。 但是我今天要说的是， 你要让计算机来学到这个东西， 你要想的是反过来， 那就是， 什么特征变化了， 它还是一个苹果？&lt;/p&gt;

&lt;p&gt;首先，我们想到的是， 一张图象是跟苹果还是香蕉， 首先一定不取决于它所处在图象中的位置。 这个东西叫位置不变性， 或者叫&lt;strong&gt;平移不变性&lt;/strong&gt;。我们把这个特性直接写到神经网络里， 就是卷积。 什么意思， 卷积就是拿着一个恒定不变的小型矩阵， 一行行的扫过整个图像， 这样得到一个特征图。 你的苹果无论出现在什么位置， 对应我的卷积扫描这个行为， 事实上得到的结果都是一样的， 数学上说， 就是你的图像如果移动了5个格， 它在特征图上也做同样的一个移动， 别的什么都不变。 能够满足这种条件的运算-就是卷积。&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicEJJke9nugZB031A0zcCJglstLuNPuGjxZahlHk4qFzGDJ4aqSj2vw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;362&quot;/&gt;&lt;/p&gt;
&lt;p&gt;卷积被当成先验信息写入，每次卷积都对应一个神经元对图像的一个小块进行信息提取， 而每个神经元与输入的连接系数均是一致的，这个特性叫做&lt;strong&gt;权值共享&lt;/strong&gt;。不要小瞧这样一个简化，有了这样一个简化，我们的神经网络得到正确的解就好了很多。用一个术语就是， 我们把问题的维度减少了。 抓住一个不变性， 你就可以把需要解决问题的维度指数级别的减少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;激活函数：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;事实上完成局部特征检测这一步，我们还需要一个东西，就是激活函数， 这个我摸嗯上节课已经讲过了，一般这里用的激活函数是relu，它的作用是把一个信号里为负的部分变成0，你可以把这看成特征提取的实现，更本质的说，如果没有激活函数，我们的神经网络将是一个巨大的线性回归而已。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7770034843205574&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k3LIxRYO5HB5yXEjicsicJ68fOBCvKOMSqnqQBZFvuMW6LOyZV1a1mibJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;287&quot;/&gt;&lt;/p&gt;

&lt;p&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;ReLU函数是小于0是为0，大于0时为自身&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是通道：&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;我们以一个手写数字的历程为例，讲讲我们还需要什么，首先我们说， 一层卷积对应一个特征， 但是，显然一个特征是不够识别的。 就以识别数字为例子降价给你这个问题， 比如你要识别10个数字，仅以1和7为例子。显然识别它们的核心方法就是条纹走向。横线是一个特征，竖线是一个特征。如果一个卷积对应一个特征，那么我们其实需要两个卷积，让一个卷积核可以识别横向条纹， 另一个卷积核识别纵向条纹， 这个操作就可以。&lt;/p&gt;

&lt;p&gt;这样的操作，使用如下的3x3卷积就可以了： -1，1， -1  ，  这样的算子具有和之前提取梯度的运算差不多的样子。只要两种卷积核可以做到这点， 然后，如果我们把这两个卷积核组成一个小组扫描一个特征， 那么我们就会知道每个图像小块上的横竖情况。&lt;/p&gt;

&lt;p&gt;比如这时候我们得到每个图像小块的一个特征编码，一共有四种情况 （0，0），（0，1），（1，0）（1，1），横线对应（1，0）竖线对应（0，1）， 你是不是可以把整个数表看成一个新的图像 ？ 而这个新的图像里的变化从（1，0）到（0，1）是否相当于一个角度呢？ 这就是比条纹走向更高级的一个特征。 怎么提取它？如果你的答案是再放入一层卷积， 恭喜你答对了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4789272030651341&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kxd2XRKOic2My0D0qappe2qgI8L3KYvLk6khicEElAict1x1SKCCQWzXxg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;回顾整个过程，我们要做的无非是在第一卷积层的位置上， 放两个并行的卷积核， 一个核处理横向条纹，一个处理纵条纹， 得到两组不同的特征， 最终我们在前面的两个特征之上读取这组新生成的特征图之上的特征。 下一层卷积寻找的上一组卷积的特征组合。这个操作对应的是在两张并列的图层之上，在它们的同一位置识别信息， 如果两个警报器均响了，说明夹角存在，  我们依然可以用一个3x3卷积网络来完成这个操作，这个新的卷积建立在之前的纵横两组卷积之上，对原先的横纹和纵纹组成的特征空间进行操作（因为这里的维度是2x3x3，最单纯的情况我们也可以用一个2x1x1的一个矩阵综合两个特征）。  因为这个时候， 对之前平行卷积的结果做一个综合， 以及形成一个特征之特征， 即横向和竖线交叉的特征。&lt;/p&gt;

&lt;p&gt;这样的方法无论手写数字出现在什么位置，  我都给你找出来。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;从两层到多层：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚说 kernel就是通过计算小区域内像素的关系来提取局部特征，而最常用的卷积核大小是3x3， 那么这里的一个问题是， 为什么要这么小， 为什么要提取一个局部信息？我们说因为图像这个东西里包含的信息具有以下特点： 最底层的信息，比如边角轮廓， 都存在于局部之中， 只有更上层的信息，比如物体的概念， 才会用到更多部分的信息， 而这种跨度又是逐步发生的。那么，如果实现这种跨度呢？  答案： 多层。局部特征，在更高层上被组合， 会变成整体特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42369186046511625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKqsX2xB7m1ZnmrTCNuF12RThK0PURyTC5mgzPrDDw2PZpEcS3SF0vw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1376&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  首先，我们把每个神经特征所提取的特征区域， 叫感受野，如果我们始终只能用的其实都是3x3这样的小卷积核， 我们能不能让感受野扩大呢？ 答案是， 可以。 这里的关键是一个叫池化的造作。&lt;/p&gt;

&lt;p&gt;最大池化所做的是事情，是把每四个相邻神经元得到的数值取一个最大的， 其它全部扔掉。每次卷积后如果经过这样一个操作，那么图像就会缩小到原先的四分之一，而再次之上的相邻四个像素， 对应了原始的16个像素， 从而使得感受野迅速扩大。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5582706766917294&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kA57IKwRcHrqUjq6SVb77Sd3ibgSvvcDxNOzRHgaBrmiazv7e1Yrb209g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1064&quot;/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Pooling的本质依然是降维，或者过滤冗余信息，这个就是pooling。背后能够这样做的理由是，局域特征特征是大量冗余的 ，经过条纹提取的数字一定在大量临近区域里的数值都一样。 冗余踢去后， 经过pooling， 上层细胞得到更大的感受野，也就抽取了更高层次的特征。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积层 ，激活函数，pooling帮我完整的特征提取到剔除冗余的过程 ， 这可以称为卷积网络的三明治， 把这个结构不停迭代，我们可以构建一个很深的网络。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.363479758828596&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMARlAiaha7Tp0OAyibiclNr6c5R7PS7uCicOZjRjCkZP35apJUxgq2rJnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1161&quot;/&gt;&lt;/p&gt;

&lt;p&gt;深度意味着什么？  我们想一下， 要正确的识别一个图像，你不可能只看边，也不可能只看角， 你要对图像的整体有认识才知道张三李四。 也就是说我们要从局部关联进化到全局关联， 真实的图像一定是有一个全局的，比如手我的脸， 只有我的眼镜，鼻子耳朵都被一起观察时候才称得上我的脸，一个只要局部，就什么都不是了。如何提取全局特征？ 从一个层次到另一个层次的递进， 通常是对上一层次做横向以及纵向的整合（图层间的组合或图层之内的组合或两者）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4612005856515373&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicPBaImgyWqzuJNtkEKMbrIrkzPSdp6UWZyq9NgShHPJb3P6MtCmibjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot;/&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚讲了CNN如何找到边角的过程， 但是它的下一层会是什么？再下一层会是什么？ 我们头脑中的想象力已经不够了。我们只能做让学习得到结构，然后去观测。我们可以把每组卷积网络看做一组基，我们在这组基上重构我们的信息， 就和线性代数里坐标变换相似，只不过非线性更复杂。 每一级别的网络都是一组新的基底，我们把刚刚的全局换一个词叫抽象。深度卷积赋予了神经网络以抽象能力。 这样的一级级向上卷积做基变换的过程，有人说叫搞基（深度学习就是搞基），深一点想叫表征， 和人的思维做个比喻就是抽象。 抽象是我在很深的层次把不同的东西联系起来，CNN教会了我们实现抽象的一种物理方法，  他也体现了在一个空间尺度上我们所能够达到的特征工程。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最终分类：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里我们还差最后一步没讲， 整个CNN网络如同一个等级社会里，最上层的，就是君王。 而这个君王，与直接其下的一层（议会）的关系，事实上往往是全连接网络。为什么，因为这时候君王要做的是最终决策， 它不在“搞基”提取特征了。一个非常复杂的问题，已经在此时变成了线性可分的简单问题。 决策 – 就是做一个线性分类， 得到我最想要的结果。  我们要做的是返回一组最终可能结果的概率。如果得到可能结果的一组概率？ 我们搬出基于最大熵模型的softmax  gate ，这也是正是CNN网络做分类的最后一层。 至此，我们可以得到众多识别物体的抽象信息。 那个概率最大的，即使我想要的结果。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5389435989256938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kF6KG6kGBVeH4Zwib3bCIeFwOrMQygTJ8ckUDyfZHkLTCEKfHmgbfSlQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1117&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7400318979266348&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kByPbeQrjLRAFZMt2UBqXLDompmohkqQN7fjYMqSlaeWfxI0jZaQ2og/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;627&quot;/&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;你所不能不知道的CNN&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作者简介：微信号：ironcruiser 法国巴黎高师物理硕士 ，以色列理工大学计算神经科学博士，巡洋舰科技有限公司创始人, 《机器学习与复杂系统》纸质书作者。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 02 Mar 2019 11:40:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/C8XQvUfjg6</dc:identifier>
</item>
</channel>
</rss>