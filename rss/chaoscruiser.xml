<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>深度学习背后的基础-神经网络揭秘</title>
<link>http://www.jintiankansha.me/t/aeBEgAR2Il</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/aeBEgAR2Il</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;j108-0-0&quot;&gt;最近， 深度学习三杰获得了计算机界最重要的图灵奖， 它们的贡献都集中在对深度学习的根据神经网络的理论突破。 今天我们看到的所有和人工智能有关的伟大成就， 从阿法狗到自动驾驶， 从海量人脸识别到对话机器人， 都可以归功于人工神经网络的迅速崛起。那么对于不了解神经网络的同学如何入门？ 神经网络的技术为什么伟大， 又是什么让它们这么多年才姗姗走来?  我们一一拆解开来。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;一  引入&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们说人工智能经历了若干阶段， 从规则主导的计算模型到统计机器学习。传统统计机器学习不乏极为强大的算法， 从各种高级的线性回归，SVM到决策树随机森林，它们的共同特点是把人类的学习过程直接转化为算法。 但是沿着直接模拟人类学习和思维的路线， 我们是否可以走向人工智能的终极大厦呢？&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2bfp0-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6180555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFox5Ffs67icfICgmGB0BORtBlhpWxzG2YkfgTSQlcem8vibJuLkJZ4gTjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;答案是否定的。 基于统计， 模拟人类思维的机器学习模型， 最典型的是决策树， 而即使决策树， 最多能够提取的无非是一种数据特征之间的树形逻辑关系。 但是显然我们人的功能， 很多并不是基于这种非常形式化的逻辑。 比如你一看到一个人， 就记住了他的面孔。 比如你有情感， 在你愤怒和恐惧的时候击退敌人。 比如你一不小心产生了灵感， 下了一手妙棋或者画出一幅名画。 这些显然都与决策树那种非常机械的逻辑相差甚远。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bspib-0-0&quot;&gt;人类的智慧根据， 是从感知， 到直觉， 到创造力的一系列很难转化为程序的过程。  那我们如何才能真正模拟人类的智能？ 我们回到人的组成人的智能本身最重要的材料。 大脑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4s0b-0-0&quot;&gt;如果说统计机器学习的故事是一个模拟人类思维的过程， 那么神经网络的故事就是一个信息加工和处理的故事， 我们的思维将一步步接近造物， 接近 - “信息”， 这个一切认知和智能的本源。 看它该如何流动， 才能产生人的思维。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二 神经元&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;favf8-0-0&quot;&gt;生物神经元&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bb2cs-0-0&quot;&gt;首先， 神经网络的灵感来自生物神经网络。 那么生物神经网络是怎么组成的？ 神经元。 神经元的基本结构是树突， 胞体和轴突（如上图）。&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-1&quot;&gt;这样一个结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-3&quot;&gt;就像因特网上的一台电脑，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-5&quot;&gt;它有一部接受器-树突，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-7&quot;&gt;每一个树突的顶端是一个叫突出的结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-9&quot;&gt;上面布满各种各样的传感器（离子通道），&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-11&quot;&gt;接受外界物理信号，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-13&quot;&gt;或其它“电脑”给它发来的脉冲，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-15&quot;&gt;一部发射器，轴突，则以化学递质的形式放出自身的信号。&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-16&quot;&gt;只不过树突和轴突是有形的生物组织， 他们象树枝，根须一般延伸出去，&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-17&quot;&gt;不停的探知外界的情况并调整自己。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;btp85-0-0&quot;&gt;神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8temh-0-0&quot;&gt;我们把生物神经元进行数学抽象， 就得到人工神经元。如何抽取它的灵魂？ 简单的说， 每一个神经元扮演的角色就是一个收集+传话者。树突不停的收集外部的信号，大部分是其它细胞传递进来的信号，也有物理信号， 比如光。 然后把各种各样的信号转化成胞体上的电位， 如果胞体电位大于一个阈值， 它就开始向其它细胞传递信号。 这个过程非常像爆发。 要不是沉默， 要不就疯狂的对外喊话。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.25833333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFotBFVTVGWWuOicpIJ9FfxDqzbRudXnowYgTM3zUD6KoE6zBVSndhic42w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;sejk-0-0&quot;&gt;我们说， 一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习， 就是分类器，不停的对外界数据进行处理。为什么要做分类？  因为正确的分类， 是认知的基础， 也是决策的基础。 因为认识任何概念， 往往都是离散的分类问题， 比如色彩， 物体的形状等等。因此， 神经细胞做的事情， 就像是模数转化， 把连续的信号转化成离散的样子。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6611111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo7IYjoHvwolKM6LJDhufUmWuRIibXQzcpJBuGib3dkFE4gdxT1nOAVE6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
突触：生物神经元对话的媒介， 信号在这里实现转化

&lt;p&gt;&lt;span data-offset-key=&quot;7ggij-0-0&quot;&gt;如果说把神经元看成这样一个信息转化的函数， 那么生物神经元的这副样子，使它能够极好的调节这个函数。 信号的收集者树突， 可以向外界生长决定探测哪些信号， 然后通过一个叫离子通道的东西收集信号，每个树突上这种通道的数量不一样。 有的多一点， 有的少一点， 而且是可以调控的。 这个调控的东西， 就是对外界信号的敏感度。 这样的结构， 可以说得到了最大的对外界信息的选择性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dbmjj-0-0&quot;&gt;什么是外界信号？  这里我们用上次课将的一个东西替代， 特征， 如果把一切看作树， 神经元在不停观测外界， 每个树突都在收集某个特征。而这个离子通道的数量， 就是线性回归里面的那个权重。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibji-0-0&quot;&gt;它在干什么事情呢？ 收集特征， 作为决策证据！  当某些信息积累到一定地步， 它就可以做决策了， 如果把这个功能进一步简化， 我们就可以把这个决策过程描述成以单纯的阈值函数， 要么就干， 否则就不干。 就这么简单的。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4263888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFobf2KiaJPeiamU74V5JkPq7iaicufS8yaZy5iaE0YnT6ssye8F4MPjibNp5dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

从生物神经元到数学神经元


&lt;p&gt;&lt;span data-offset-key=&quot;as9ns-0-0&quot;&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;79bjr-0-0&quot;&gt;我们这个简化出来的模型，正是所有人工神经网络的祖母－感知机。　从名字可以看出，人们设计这个模型的最初目的就是像让它像真实的生物神经元一样，做出感知和判断。　并且从数据中学习。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59a57-0-0&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;59a57-0-1&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 Rosenblatt 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36527777777777776&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9icsW6iaibhrGtXQyKPGibkNHia4LA7Ooia8vymNVBVMYbVC4dn2DLnRj6zA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

把数学神经元变成机器

&lt;p&gt;&lt;span data-offset-key=&quot;27jc1-0-0&quot;&gt;你要是想理解这个过程。最好的方法是几何法。你仔细观察， 这个感知机的方程， 如果只有两个特征的时候， 描述的就是一个x1和x2坐标的平面， 中间有一条直线w1x1+w2x2=0，直线的左边是一类， 右边是二类 。感知机的学习过程就是调整这条分类直线的位置，我们测量错分点到这条线的距离之和，调整线的位置， 直到两个类的点乖乖分布到直线两边，我们就实现了分类过程。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9387096774193548&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod3fSJrcicN5uibzI90zF1ZH6slmflRuaOmYO7gmv6ibiaGY3K6SFbCYEwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

一个神经元相当于一个分类器

&lt;p&gt;&lt;span data-offset-key=&quot;25nko-0-0&quot;&gt;这其实就是一个神经元的偏好。 比你让一个神经元帮你确定是否去看电影的例子。它根据今天的温度，一个是电影的长度来判断， 当然一开始它不了解你， 但是它可以先帮你做做决策， 然后根据你每次给他的反馈意见它调整自己，如果你注重温度，它就加大温度的权重，总之找到你看重的东西， 这就是感知机训练的方法白话版， 如果你给他肯定意见，他就不做改变， 你不满意， 他就变一变。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8kn9j-0-0&quot;&gt;最初的感知机采用阈值函数作为神经元的决策（激活）函数。后来这个函数逐步被调整为sigmoid函数。表现上看， 这个函数把阈值函数进行了软化， 事实上， 它使得我们不是仅仅能够表达非黑即白的逻辑， 而是一个连续变化的概率。 而这个函数的扩展softmax则可以帮助我们实现多个分类的选择。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.665625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoImiazM4ib27dfAwMs1nux9HcuB7LVzJcmp6szTWqoK7E1OndTQuGnAbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;320&quot; /&gt;&lt;/p&gt;

sigmoid 函数

&lt;p&gt;&lt;span data-offset-key=&quot;2om41-0-0&quot;&gt;我们说， 这一类模型开始被人们寄托重大希望， 不久却落空。原因在于， 它真是太简单了。 在几何上， 无论怎么变它都只是一个线性的模型。 而事实上， 这个世界却是非线性的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;三 神经网络降伏非线性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c5d5g-0-0&quot;&gt;什么是非线性， 我们举一个经典的机器学习的房价的例子，首先这是一个经典的线性问题。 我们通常用线性模型描述房价的问题。 把各个影响因子线性的累加起来。比如我们把问题简化为卧室数量， 面积 ，地段， 最后求成交价。 我们用线性模型求解价格。 然而， 着一定不是真实的情况， 这背后的关键假设是，我们的卧室数量，面积， 地段都会独立的影响房价， 这就好像说， 如果你有几个不同的因素， 某个要素变化， 不影响其它要素最终决定房价的方式，我们具体来说， 还是房屋单价只受地段， 距离地铁远近，卧室数量三个要素影响，假定三者的权重是w1，w2, w3. 如果你去改变地段，那么w2，和w3必须是不变的。 就拿海淀和朝阳来说，对这两个区， w2 和w3 是一致的。 事实上呢？  显然这三个要素互相制约，或许海淀的人都做地铁， 朝阳的有车的比较多， 它们对距离地铁远近的敏感就不同， 或者说海淀的单身汉比较多， 朝阳的成家的比较多， 朝阳的就更喜欢卧室多的大户型。 如何表现这种特征之间的相互依赖关系呢？ 一个天然的解决方法，就是：&lt;/span&gt; &lt;span data-offset-key=&quot;c5d5g-0-1&quot;&gt;网络模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.43333333333333335&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoX3xicajmjMl3ZNViabf3Ew7wgqhoBLqOr4C4nx5MwQ38CAAVwlqm3uag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;crcl9-0-0&quot;&gt;看看人类可能是怎么处理这些信息的，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-2&quot;&gt;人对于这种复杂的信息，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-4&quot;&gt;往往采用的是陪审团模式，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-5&quot;&gt;我们可能有好几个房价专家， 就自己了解的方向去提供意见， 最后我们再根据每个人的情况综合出一个结果。虽然这个时候我们失去了简单的线性， 但是通过对不同意见单元的归纳， 和集中的过程， 就可以模拟更复杂的问题。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c7vnh-0-0&quot;&gt;再看看怎么把它变成数学处理房价问题。 首先把他表示成公式， 他其实是说的是&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5736111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFosfnkfkAqIFjIzNq3CxGpbYNyDtXzl2HXIibTSBKQC2XJU8icjEn9baDg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7aatl-0-0&quot;&gt;系数变成了和x有关的，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-1&quot;&gt;仔细想一下，这不就是增加了一个新的层次吗，&lt;/span&gt;&lt;span data-offset-key=&quot;7aatl-0-3&quot;&gt;我们需要再原始特征x1和x2的基础上，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-5&quot;&gt;增加一个新的处理层次，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-7&quot;&gt;让不同的特征区域，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-9&quot;&gt;享受不同的权重&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-10&quot;&gt;。 就好比我们有好几位房价评估师，每一位都是针对某种情况下房价的专家，最后有一个人， 根据专家的特长给出最终的综合结果。这与我们的灵感是相符的。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vqde-0-0&quot;&gt;我们把他表示成图示方法。  首先， 每个专家都仿佛是一个小的线性模型， 它们具有唯一的一组权重， 显示它们再所熟悉的区域的权威。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8126843657817109&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9ibmk3wbb7DoRNJpAnrg2P3qqUEAd7E3mdXW4JGVpR4kMvAPO3XMmBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;678&quot; /&gt;&lt;/p&gt;

图片改编自网络博文Machine Learning is Fun!  by Adam Geitgey

&lt;p&gt;&lt;span data-offset-key=&quot;afvtt-0-0&quot;&gt;然后，我们把它们汇总起来， 得到下面这个图， 这个图和之前我们讲解线性回归的图是一样的， 但是我们增加了一个中间的层次，这就是刚刚说的陪审团， 哪些中间的位置， 事实上就是表达了特征之间复杂的互相影响。而最终我们把它们的意见综合起来， 就是最终的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo2E1HdEBTguA3uM8kic3riaibTQgYib212lbLemDibKD4q4oet6ID2sQCnMw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4bjac-0-0&quot;&gt;你换个角度思考， 那些中间的绿色圆圈， 也就是说每个专家，不就是神经元吗？ 而那个黄色圆圈， 是最终决策的神经元。 这样的一个组织， 而不是单个神经元， 就是神经网络的雏形。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;as5n6-0-0&quot;&gt;当然， 房价的问题是个回归问题， 而这个世界的大部分机器学习问题是分类， 每个神经元也是一个小小的分类器， 所以我们把每个神经元的角色变成线性分类器，再套用刚刚陪审团的逻辑， 看看我们得到了什么： &lt;/span&gt;每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有一组头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  我们说， 这就是一个一层的神经网络所能干的事情。 它可以做出一个复杂的选择， 每个神经元都代表着对特征的一个组合方法，最后决策神经元把这些重组后的特征已经刻画了不同特征之间的关系， 就可以干掉认识现实世界复杂特性-非线性的能力， 特征之间的关系很复杂， 我也可以学出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;所谓神经网络的近似定理， 是说一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，&lt;/span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;它可以以任意的精度来近似任何&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-1&quot;&gt;从一个有限维空间到另一个有限维空间&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-2&quot;&gt;的&lt;/span&gt;Borel 可测函数。  这是关于神经网络最经典的理论了，简称万能函数逼近器&lt;/strong&gt;。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5416666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod4vIAibhcvVQNWJFiaS8q4IGwG6LORjWzfhcPUcGicRbFDAdtGQdGO0OQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2r5k6-0-0&quot;&gt;另外的一种理解是，神经网络具有生成非常复杂的规则的能力， 如果你可以让神经网络很好的学习， 他就可以自发的去做那些与或非， 和逻辑运算，不用你自己写程序就解决非线性问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;四 关于神经网络的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1nvpl-0-0&quot;&gt;是什么阻止了神经网络从出现之后很快的发展， 事实上是它们非常不好学习。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;28o13-0-0&quot;&gt;我们说神经网络能够成为一种机器学习的工具关键在于能够学习，那么如何学习的呢？ 如果说单个神经元可以学习的它的偏好， 通过调整权重来调整自己的偏好。 那么神经网络所干的事情就是调整每个神经元和神经元之间的联系， 通过调整这个连接， 来学习如何处理复杂的特征。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7h3c6-0-0&quot;&gt;说的简单， 这在数学上是一个非常难的问题。 我们通常要把一个机器学习问题转化为优化问题求解。 这里， 神经网络既然在解决非线性问题， 事实上和它有关的优化又叫做非线性优化。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;em7k3-0-0&quot;&gt;先看生物神经网络的学习， 它是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是，我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了对高级特征的认知。 上述过程被总结hebbain学习的一个过程。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4708333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoodYXesR7lOnwbA0FpvdW1hbl8Zxw6KaDCNicI4F7gDSlD09AzSQqHFg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44722222222222224&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo4ibY5gFmA50Wd4zhXO4TaPibTmGgVhwGKqCR6HyEZzGeYHYnRuLA32Dw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;coje1-0-0&quot;&gt;从本质上说， 刚刚说的过程是一种无监督学习，你接受输入引起神经活动， 它慢慢的调整。 如果用这样的方法训练人工神经网络，恐怕需要跑一个世纪。  人工神经网络的训练依赖的是监督学习，一种有效的结果导向的思维，如果我要它判断香蕉和苹果， 我就是要一边给它看图片， 叫它告诉我是什么 ，然后马上告诉它对错， 如果错了，就是寻找那个引起错误的神经连接。 然而这个过程在数学上特别的难以表达， 因为一个复杂网络的判断， 引起错误的原因可能是任何中间环节 ，这就好像调节一大堆互相关联在一起的参数，更加可怕的是， 神经网络往往有很多层， 使得这些参数的相关性更加复杂， 如果你一个个去试探， 那个感觉就是立即疯。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6ogvv-0-0&quot;&gt;最终， 这个问题被BP算法的提出解决。 BP反向传播算法， 是一种精确的根据最终实现的目标，然后通过比较当下输出和最终目标的差距， 然后一级反推如何微小的改变各级连接权重以减少这种误差的方法。 这其实就是梯度下降结合复合函数求导法则的一个更复杂的形式。 我们通常把一组刚刚到来的数据， 扎成一捆喂给神经网络， 让它计算出一个输出， 这个输出当然会错的很离谱， 然后我们把这个结果和我们真正需求的比较得到一个误差信号， 这个误差信号会一级一级的改变所有的连接权重。 每一捆数据， 被称为一个tensor， 都像一个个子弹一样塑造着整个网络。 对于这个方法的理解， 最好的办法是使用一套由tensorflow提供的可视化工具。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9h3f9-0-0&quot;&gt;当然这仅仅是一个神经网络的训练的简短小节， 当你在神经网络坚守的Hinton这样的大神，事实上提出过对学习算法的无数改进细节， 才有了它今天的成功。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;五 关于多层神经网络，深有什么好处？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5jrv-0-0&quot;&gt;刚刚说的方法你可以理解一个一层的神经网络， 那更多层的神经网络呢？深层的神经网络究竟有什么好处？事实上深度学习的深，就是指神经网络之神，可见这是奥妙的门道。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3fkq2-0-0&quot;&gt;用一句化说，一层的神经网络可以对特征做一次变换，就如同得到了一组新的特征， 这种特征的变换， 用数学家的眼光就是做了一个坐标变换， 兑换到一个新的空间。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;63aiu-0-0&quot;&gt;我举一个最小的例子让你理解。 你记得高中学过的直角坐标系和极坐标系吧， 这两个坐标系之间存在一个经典的坐标变换公式。 这就可以看作一个坐标变换。 我们举一个例子看，如果要做一个分类器， 把一些居住在圆形中心的点和它的外周分离开来， 那么这个坐标变换就有奇效。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图片来源： Medium Support vector machine explained  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ej38h-0-0&quot;&gt;多层网络， 就代表一个多次连续的特征变换，最后会把数据从一种性质变到另一个性质，从一个空间拉到另一个空间。最终总有一个空间， 这些数据呆着最合适。这种行为，简称为表征学习（representative learning）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.475&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoAPpjHdFGtYzf774rsmY2Jr9iaZuWXCiayVr3rF2WKFV6eOnSTY24DTPg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7lhu9-0-0&quot;&gt;表征学习顾名思义，是要一种特征转化为另外的特征，因为简单的特征一旦发生关系就可以构成复杂的特征，刚刚那个处在领头的大法官和司令再次变成士兵，给上层的决策者提供信息,隐层神经网络所作的事情，我们说其实就是对特征进行重组， 然后得到一组新的特征的坐标变换， 只不过这个变换的形式是可以学习的。 我们说，越限制， 就越自由，为了更好的学习， 我们会限制神经元对信息的处理只由两部分组成， 一个是用一定的权重组合上层的特征， 另一个是通过某个样子的激活函数把这个总和变一变。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;agvf3-0-0&quot;&gt;多层的神经网络， 通过多次的基变换， 把特征一次次重组，得到越来越复杂的新的特征， 这就是深度神经网络作用的机制。 某种程度上， 它把这种层级特征强加给了它要识别的事物， 但是假定事物本身也是这种按照一层层的方法搭建起来， 那么神经网络就会取得奇效。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8jijp-0-0&quot;&gt;哪些事物是有这种层级结构的呢？ 我们说，所有的感知信号都是， 从视觉， 到听觉，到语言。  &lt;/span&gt;假定你看着一个屏幕， 你真正看到的是每个像素点， 而最终你要组合成为一个可以认知的图象。 假定你听着的是声音， 你最终需要分辨出一个个单词 ，句式，直到语言。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;85nga-0-0&quot;&gt;认知科学里说，抽象使得不同的事物可以在更深的层次上被联系起来， 的就是说， 不同层次的函数迭代，产生的新的空间（某种程度也是降维，比如图形处理中），使得原先的距离概念被打破，具体怎样被打破。 这正是人脑处理信息的本质。 实验表明，越靠近感官的神经元处理的信息就具体， 比如颜色，亮度， 而经过多级处理，达到皮层之后， 我们的神经元就可以对比较抽象的实体，如人名，物体的种类敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5125&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoHicmAzdVLQ0RZUTnlhTqwNic1WWOAj06A4VYNTXvUuiaTrjpGjmtczuhA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c2jbo-0-0&quot;&gt;当然，这只是最形象的一个描述，关于增加深度相比增加宽度对神经网络功能的好处，是整个深度学习问题的焦点问题之一。因此引出了几个不同的数学观点， 有机会给大家总结。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7k452-0-0&quot;&gt;训练深度神经网络是困呐的，由于神经网络的训练里有大量的复杂求导运算， 我们是不是函数要写死为止？ 其实不是， 我们有一个强大的框架， 就是tensorflow，让你的整个求导过程十分容易。你只需要写出你的目标函数cost function，然后简简单单的调用tensorflow内部的optimisor就可以了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;六 神经网络动物园&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5oi27-0-0&quot;&gt;神经网络经过这些年的进化，已经是一个大家族。  我们来看几个具体的神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e2saf-0-0&quot;&gt;Auto-encoder 自编码器：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;15eeh-0-0&quot;&gt;最早成功的一种深度神经网络，自编码器意如其名， 其实做的就是对数据进行编码， 或者压缩。  这种网络通常具有的特点是输出的长度或者说大小远小于输入，但是神奇的是， 这个输出却保留了数据种的大部分信息。 你是不是想到了上节课介绍的PCA了呢？ 是的， 这就是一种降维的模型。 这个模型的实质， 也就是一个复杂， 或者说是非线性的降维操作，  比如说你把一张图象压成很简单的编码。 由于这种输入多输出少的性质， 自编码器的形状形如一个下夸上窄的漏斗， 数据从夸的一面进来， 出去的就是压缩的精华。 自编码器之所以在神经网络的进化历史里举足轻重， 是因为它在回答一个很核心的问题， 那就是在变化的信息里那些是不变的， 这些不变的东西往往也就是最重要的东西。比如说人脸， 你把1万张不同的人脸输入到网络里， 这些人类分属于十个人， 那么按说， 很多照片无非是一个人的不同状态，它的不变性就是这个人本身。 这个网络能够把数据最终一步步的集中到十个维度就够了， 这样， 最终可能用一个十位数的数字就可以压缩整个数据集。 而这， 正是神经网络， 甚至是人类理解事物的关键， 那就是找出纷繁变化事物里的不变性， 它对应的正是一个概念。 抓住了概念， 就抓住了从具象到抽象的过程， 抓住了知识，抓住了人认知的关键所在。 深度学习大师Hinton早期的核心工作，正是围绕自编码器。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4847222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoGuozHjTbiakzdWwM1x10geKvNhZQ9dpDib3yUSGcC0eLfog4Tk0ruWvA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dtmn8-0-0&quot;&gt;卷积神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bmp0a-0-0&quot;&gt;卷积神经网络就是模拟人类是视觉的一种深度神经网络。这个网络的特点是能够把图象数据，像photoshop 里的滤镜一样， 被过滤成某种特征图，比如纹理图， 这些低级的特征图， 将再次被过滤， 得到一个新的特征图， 这个特征图的特点就是更抽象， 还有更多的刚刚讲过的概念性的特征。  比如一个物体的形状轮廓。 直到最后一层， 得到对某类物体概念的认知。 这就是人类视觉知识形成的过程。  也是我们下次课的重点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5c9i-0-0&quot;&gt;含时间的神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1df5j-0-0&quot;&gt;神经网络不仅能够描述静态的各个特征之间的关系， 而且能够描述特征（这里更好叫因子）之间在时间上的复杂相互作用关系， 一个神经网络的做出的含时间过程， 最好的例子就是含有证据积累的决策过程。 即使是单个神经元， 也可以把不同时间的信息积累起来做个决定， 而动态的神经网络， 就可以更好的把这些证据总体的汇集起来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ceh5e-0-0&quot;&gt;总的来说 ， 几个神经元组成的网络， 可以像一个信息的蓄水池一样， 通过互相喊话， 把过去的信息在自己人之间流传起来， 从而产生类似于人的记忆的效应， 这些通过特定方法连接在一起的神经元， 就可以形成人的工作记忆或内隐记忆， 而同时， 也可以帮我们设定出处理和时间有关信号的神经网络工具， 这就是RNN – LSTM家族， 以及其它的长时间记忆网络。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7430555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoC2jGqlma1D0hyq3XB24GdAQgQLNEvE8CicBysZhbiaP8WohAknicu3GRw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在深度时间序列处理种扮演重要核心角色的LSTM，其创始人schmidhuber却无缘此次图灵奖。&lt;/p&gt;

</description>
<pubDate>Sun, 31 Mar 2019 12:22:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/aeBEgAR2Il</dc:identifier>
</item>
<item>
<title>贝叶斯推理实用入门</title>
<link>http://www.jintiankansha.me/t/EY2XU24xDA</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/EY2XU24xDA</guid>
<description>&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;什么是贝叶斯推理， 我早在过去的文章里分析过有关贝叶斯概率的知识， 例如&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381931&amp;amp;idx=1&amp;amp;sn=2eb7ab8b5dadda70d74ae6289ef361f8&amp;amp;chksm=84f3ceeab38447fc736fd8a9e6494b663c12dfa3c347b37054d926751141bbe0ed61f0890d11&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;朴素贝叶斯之实践篇&lt;/a&gt;，这次融入纽约大学weijima的方法论教程（http://www.cns.nyu.edu/malab/index.html），给大家一个更实用的版本。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;一，什么是贝叶斯概率， 它于经典概率由什么关系.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e3m1u-0-0&quot;&gt;谈贝叶斯首先是用概率量化问题。 概率这件事大家都觉得自己很熟悉， 叫你说概率的定义 ， 你却不一定说的出。经典的概率， 说的是事件发生的可能性。 我们中学课本里说概率这个东西表述是一件事发生的频率， 这个频率就代表某件事发生的可能大小。 或者说这叫做客观概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;948n2-0-0&quot;&gt;而贝叶斯框架下的概率理论确从另一个角度给我们展开了答案，&lt;/span&gt; &lt;span data-offset-key=&quot;948n2-0-1&quot;&gt;他说概率是我们个人的一个主观概念， 表明我们对某个事物是否发生的相信程度&lt;/span&gt;&lt;span data-offset-key=&quot;948n2-0-2&quot;&gt;。 如同Pierre Lapalace说的: Probability theory is nothing but common sense reduced to calculation. 这正是贝叶斯流派的核心，换句话说，它解决的是来自外部的信息与我们大脑内信念的交互关系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6jnh-0-0&quot;&gt;两种对于概率的解读区别了频率流派和贝叶斯流派。同时我们不难看出两者之间的联系， 你对一件事情发生的可能性估计正是基于某种频率的统计。 但是它们的区别在哪里呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bc3bn-0-0&quot;&gt;首先，给你下面的事件， 假定你带着孩子去看月亮， 然后孩子说月亮的属性是块奶酪， 你会跟它怎么说呢？  首先， 你一定知道月亮是一个石头的星球而非奶酪， 那么这件事你要如何去跟孩子说呢？ 首先我们生活在概率的世界， 你要和它说的是你可以认为月亮是石头或者奶酪， 但是你不要相信任何一个， 既然不相信， 你把它们称为假设1 和假设2，  然后你给它们各自一个数字来代表可能性的大小， 这就是概率。 然后我们看频率观和贝叶斯的区别&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4h102-0-0&quot;&gt;1， 频率观的家长： 到天空做一些测量， 看看奶酪和石头的比例， 然后算出假设1和假设2的概率。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bj860-0-0&quot;&gt;2，  贝叶斯的家长：  孩子我们去不了天空， 但是我们可以想象下我们生活中的经验， 然后查看一下教科书。 首先， 教科书里说， 到目前为止，天空中发光的99.99%是石头。   然后，  再联想下生活经验， 如果是奶酪， 那么它确实是黄灿灿的发光， 因此生活证据显示， 月亮是奶酪的假设并不违和。 那么把两个综合一下， 通过一系列后面会说的公式， 你给出孩子它的观测结合书里的知识的合理性概率： 月亮是石头的概率99.9%。 贝叶斯通过承认我们自身的无知，给不同的假设以调整空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.05&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Er0r4hHxCTIVzGB4kdGibZPHZDLhufbpAYHZGMGIAKO3cGxYZ1ddJkg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f45d1-0-0&quot;&gt;哪个过程更合理？ 哪个方法更正确？ 你自行去分析。这里要说的是，&lt;/span&gt; &lt;span data-offset-key=&quot;f45d1-0-1&quot;&gt;在真实世界里， 我们所做的往往是把现象的经验推理， 和某种先验结合， 去估计事物的可能性，这正是贝叶斯的思路 . 没有人会对每件重要的事做无限的测量， 也不是所有事件都可以重复（分手不可以， 股灾不可以），这是我们唯一可以做的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7kf2h-0-0&quot;&gt;贝叶斯的数学公式十分简单， 一， 你要有先验概率P（A），二， 似然性  P（B|A）， 最终得到后验概率P（A|B）。这三者构成贝叶斯统计的三要素。&lt;/span&gt;似然性实用条件概率表达， 后验也用条件概率来表达， 基于此的贝叶斯定律数学方程极为简单：&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.30434782608695654&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6KfJ3pWuOyaw1gf77HOwiaPIQef4gxPOfw3UcicvlTZB44OlFxQalW4lw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;184&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-0-0&quot;&gt;套用月亮的例子，P(A)代表月亮是奶酪的假设， P(B|A)代表现象黄色发光。 &lt;/span&gt;&lt;span data-offset-key=&quot;dc4dh-2-0&quot;&gt;即月亮是奶酪的先验概率，&lt;/span&gt; &lt;span data-offset-key=&quot;dc4dh-4-0&quot;&gt;是如果月亮是奶酪， 那么它是黄色发光的概率， 你得到前两者，就可以根据公式算出结合了证据之后的月亮是奶酪的后验概率。 这里比较难计算的是 P（B）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-6-0&quot;&gt;事实上对它的计算你要把所有可以给出这个结果的假设都包含进来， 用概率的marginal law 展开每个假设之下观测到现象的概率， 比如这个问题里， 你就要把月亮是奶酪和石头两个假设都包含进来， 分别计算各自假设下发光现象的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9l6h-0-0&quot;&gt;有一个非常有趣的现象是如果我们的先验概率审定为1或0（即肯定或否定某件事发生）， 那么无论我们如何增加证据你也依然得到同样的条件概率（此时P（A）=0 或 1 ， P（A|B）= 0或1） 这告诉我们的第一个经验就是不要过早的下论断， 下了论断你的预测也就无法进化了， 或者可以称之为信仰。&lt;/span&gt; &lt;span data-offset-key=&quot;a9l6h-0-1&quot;&gt;你如果想让你的认知进步，就要给各种假设留一点空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8vg85-0-0&quot;&gt;贝叶斯分析的思路对于由证据的积累来推测一个事物发生的概率具有重大作用， 它告诉我们当我们要预测一个事物， 我们需要的是首先根据已有的经验和知识推断一个先验概率， 然后在新证据不断积累的情况下调整这个概率。整个通过积累证据来得到一个事件发生概率的过程我们称为贝叶斯分析。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9df1h-0-0&quot;&gt;贝叶斯的数学计算主要考察对条件概率的实用。 但是有时候我们也不理解条件概率， 比如著名的辛普森案， 为了证明辛普森有杀妻之罪，检方说辛普森之前家暴的历史，而辩护律师说，美国有400万女性被丈夫或男友打过，而其中只有1432人被杀，概率是2800分之一。 这其实就是误用了条件概率， 因为辩护律师用的条件是家暴，用来推测的事件是男友杀人， 而事实上这里的条件是被杀而且有家暴，而要推测的事件是凶手是男友（事实上概率高达90%），这才是贝叶斯分析的正当用法， 而辩护律师却把完全在混淆条件与要验证的假设。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7256637168141593&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Mz8k7jElRSnzK5icqYic43ZDLRlJ104tBLOmufISqAIKOn4AcSK26t6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二， 把贝叶斯概率工具用来建模 &lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;4mrm0-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blf3r-0-0&quot;&gt;贝叶斯概率是非常基础的统计知识， 有的人只把它当成统计， 而它在心理学，经济学， 神经科学等领域具有巨大潜力。&lt;/span&gt;为什么？  因为这类问题的研究对象往往具有极高的不确定性， 是由大量较低一级单元组成的复杂系统。 这就造成直接用物理学搞定分子结构解薛定谔方程的思路是不行的， 你不能把人的行为像氢原子光谱一样求解出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;68f6s-0-0&quot;&gt;那么怎么办？ 纯统计？ 你可以做量表， 去统计所有可能的人的属性和和它们的行为之间的联系， 然后求一个皮尔森系数。 但是这样的方法虽然可以用， 但在量化和机器学习发展急速的今天还是naive了一点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;83o4i-0-0&quot;&gt;一个折衷的方法？ 贝叶斯建模。 贝叶斯建模非常善于处理“黑箱” 问题，对付这种不好精确预测但有些用到一点建模的东西很关用。&lt;/span&gt;贝叶斯建模可以很快的把实验数据和理论做一个结合。 而且据说我们的大脑处理信息也确实符合贝叶斯框架。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;34nj4-0-0&quot;&gt;你只要有假设先验， 有观测， 有似然性， 就可以用一个贝叶斯。这里的似然性，经常是由我们的理论模型提供的，而贝叶斯的框架可以把这个模型的参数迅速的推到出来。&lt;/span&gt;这同时也意味着，果你手里有几个不同的模型假设，有一些数据，贝叶斯会迅速告诉你哪个比较合理。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59t31-0-0&quot;&gt;比如你有两个截然不同的假设解释一种心理现象，贝叶斯方法迅速告诉你哪个更合理。&lt;/span&gt;我们通过下面的几个例子说明， 刚刚说了， 我们哟啊建立一个模型， 然后用贝叶斯把它转化为一个预测机器， 模型可以到多简单？ 请看下面的例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1， 多个运动物体例子&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7189655172413794&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6G8nBbLmOjpRibvdBTUicIziaTCyph1aqj5rmRibhOepcyjSMRbqwImFUoQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;580&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.27166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6PIWDfviat9RUnpEKZWGTeyFickUCWI3Y4a87gWWEybbH0iaicDoWOSxqew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6cvit-0-0&quot;&gt;如果看到一组一起移动的物体， 比如上图， 人往往会倾向于认为它们是一个整体。这个现象被格式塔心理学解释为一宗天然的心理倾向。 而解释同样的现象， 你只需要搭建一个简单的贝叶斯概率模型：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7s7nn-0-0&quot;&gt;1， 找到两种可能的假设和现象， A 上面的五个物体是独立的， 刚好一起向上运动  B  上面的5个物体是一个整体  。 现象：  五个物体一起向上运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3frem-0-0&quot;&gt;2， 找到A和B的先验概率：  先验可以。 基于知识或者大量过去的观测， 那么平时生活经验或者书本都会告诉你， 两种情况可能差距不大&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7d852-0-0&quot;&gt;3，A和B得到现象的概率， 事实上它测量假设到现象的关联， 在这个情况下， A几乎一定得到现象， B， 如果每个物体向上或向下的概率是0.5， 那么你应该已经求出来了： 1/32&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e7tph-0-0&quot;&gt;4， 合成后验概率 ：  A压倒B。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4hcon-0-0&quot;&gt;所以， 我么倾向于认为A是对的，即使A和B都有成立的可能。 由此得到的推论是人有把一起移动的物体看成一个整体的趋势， 这符合格式塔原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;49ij9-0-0&quot;&gt;2， 运动眩晕&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;du63j-0-0&quot;&gt;类似的方法可以解释很多我们日常生活中的一些现象，比如我们一个非常常见的现象， 晕船。&lt;/span&gt;关于晕船的一个重要的进化心理学理论说， 这是祖先的一个毒物排出反应， 因为祖先在尝到毒物之后会引起眩晕， 而这个时候呕吐可以排出毒物。 进化心理学用这个例子说明我们事实上生活在祖先的记忆感觉里。&lt;/p&gt;

&lt;p&gt;那么， 这个非常简单的模型假设成立的可能是多大呢？ 如果用贝叶斯方法来分析这个问题会有个很清楚的框架。在此处我们先预设眩晕确实是我们的大脑根据现象对世界做出了预测产生的反应， 我们在船舱里产生了眩晕， 我们有三个可能的模型：&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8v2n-0-0&quot;&gt;A， 我们的大脑检测到我们自己的运动， 是我们自己的运动导致我们的眼睛和前庭（vestibular） 的感觉&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dfcs9-0-0&quot;&gt;B， 我们的大脑检测到了地面的运动， 我们自己的运动（摇摆）导致了我们的视觉感知， 而前庭（vestibular）则感觉到了船舱和地面的相对运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2hlva-0-0&quot;&gt;C   我们的大脑检测到了我们吸入毒物。 毒物的作用导致了你自己的运动， 以及你所感知到的地面的剧烈晃动（幻觉）。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;el956-0-0&quot;&gt;贝叶斯的分析框架告诉我们， A， 似然性为0， 因为因果关系是错的， 我们自己的运动只能解释我们的视觉感知。   B,  先验为0， 除非世界末日，我们的祖先几乎不会在车船这类快速运动的物体上活动  C， 这种情况确实会出现在祖先的生活里，先验不为0， 而一旦吸入毒物， 那么确实有可能产生幻觉， 因此似然性不为0 .  所以相对前两者， C最有可能。  当然细心的你会发现这里还是做了太多的假设，尤其对先验， 但是这无疑是一个相对合理的框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;1g3p9-0-0&quot;&gt;3， 颜色误差&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6B65coY3d19voMDpZe4orr4LVMC10w50ehMFn4T9nvEBCyoyyic0w5eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6487455197132617&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6JYRicIOshSnHSQbdibGYCkYMBZNvyGNAcpLkibDKhBZaiaCCgs13sB9W8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;279&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;88rvr-0-0&quot;&gt;你有没有印象这篇刷爆朋友圈的文章，  这个裙子的颜色是黑色还是金色？ 有的人猜是黑色， 有的人猜是金色，而它到底是什么颜色的？ 没有人知道。 这是不是说明客观世界是不存在的？ 还是说我们发现了一个检测乐观主义和悲观主义者的方法？  如果你在思考前面两个，那么你不懂贝叶斯。  事实上， 这个问题的实质是， 这条裙子的颜色确实是不确定的。 而我们在现实世界中对颜色的判断， 本来就是一个贝叶斯推断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;我们来看为什么，颜色事实上光谱决定的， 也是不同频率光的成分大小。 这里我们做个简化，我们只有黑白灰。 大家知道， 其实真实世界的物体本身谈不上颜色， 它只是在反射， 而入射光乘以反射率决定了我们看到的样子。 黑色的物体代表反射率为0，   白色的物体是1， 而中间就是灰色。 但是， 你记住， 你的研究只能检测反射光强，这个反射光强等于反射率乘以入射光强。 如果你的眼睛检测到一个反射光强， 而我们的物体识别问题实际上正是想找到反射率这个特征（它才与颜色相关）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;也就是说，我们的研究遇到一个两难处境。 它所收集的资料反射光强， 既包含反射率， 又包含入射光的信息。 我们得到的是一组反射率和入射光的组合， 那么我们究竟为什么会看到黑白灰的色彩呢？  原因是， 我们的大脑根据先验和似然性， 做了一个贝叶斯推断。  首先， 这里的先验是什么？ 我们在自然界中， 往往会根据时间现场的光线强度等对于入射光强做一个估计， 这个经验数值就是我们的先验（在这里最好把这个经验数值想成一个以最可能的值为中心的高斯分布）。 因为日常生活吗， 总归是在那几种光线下。然后根据刚刚的乘法法则（这个相当于似然性， 你有了反射度和入射光强， 可以完全确定你眼睛的检测光强，一个狄拉克函数）， 你可以推出反射度的后验分布， 这个分布的峰值， 正是你最可能看到的颜色。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.3416666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6dF1FsW8e2icYA47C4OPNm2uWRYH9XnhVrE14vHuticpl98qE5mBL6hLw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;41g06-0-0&quot;&gt;这个实验解释了那个裙子的颜色问题， 你是看到黑色还是金色， 和你日常经验里对现场光强的先验有关， 看来酒吧里的DG和阳光下的建筑工程师的想法应该不太一样。  而这也在告诉我们， 我们看到的东西永远并非真实，由于我们接受的信息总是有限，我们在不自觉的做大量的脑补， 这些脑补， 组成了我们最终看到的世界。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cor57-0-0&quot;&gt;你估计还记得旋转舞女的实验吧， 如果你理解了刚刚的颜色问题， 那么这个问题很容易解释。  你看到她是向右还是向左旋转？  不是有人引用来解释左脑还是右脑型人？  你还相信吗？ 旋转舞女是一个典型的信息不全， 而可以容纳不同的解释的问题。虽然说一些八卦的说法并不可靠， 如果人和人之间在对这类问题的回答上真有差异， 说不定会告诉我们一定所从未想到的东西（比如是什么导致了我们的先验？）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bntb2-0-0&quot;&gt;你可以举出无数这类脑补的例子，比如为什么一篇英语文章每个单词都只保留首尾字母你还能猜到一些？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3318181818181818&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6NHqeGib34vQPm3ACQCqyCr9YpBwTyg4lr4xd5FnNlQ7KLSZxN2sQXog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;220&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span&gt;4， 和时间有关的因子预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;贝叶斯方法很擅长解决的一个问题就是和时间有关的因子预设。 假设你经常去以加喜爱的餐厅吃饭， 某一天你突然发现这家餐厅的菜突然就好吃了， 这可能是怎么回事呢？  是不是厨师换了呢？ 然而你没法进到餐厅里去看， 这个时候你会开始回想前几天吃的是不是味道也变化了你没有留意。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;其实， 这里你已经开始进行一个贝叶斯推断过程了。 假定你每天来吃一次饭， 你想推测某个点开始厨师坏了的概率， 这就是一个经典的点推测问题。  这个问题之所以有难度， 是因为如果你把每次吃饭看作一次测量， 那么测量本身是有噪声的，这使得你比较难做出决断，到底是那天厨师心情不好做坏了饭，还是换了厨师。  这个问题的实用性不用多说， 无论是在医疗健康诊断问题里， 还是某段人际关系的变化（好好一段感情突然就变了？ No， 所有的突然变化都是潜在的蓄谋已久）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt;那么具体如何做呢?  事实上这已经开始涉及到很复杂的数学，回到贝叶斯本质， 先验在哪里， 似然性在哪里？ 在很多贝叶斯问题， 先验充满主观性， 这里也不例外， 在所有主观里最客观的就是假定它是一个常数， 也就是厨师变化的概率随时间是均匀的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt; 然后， 似然性呢 ?  似然性就是一个生成模型。 也就是给你一个内在的过程，比如厨师的变化， 然后推导出菜的味道的变化。 一个最简单可以放进去的模型， 就是转化概率随时间独立的马尔科夫过程。 有了这个生成模型， 你还会得到系列不同时间厨师变化假定下吃菜味道的分布。 这就完成了生成模型部分， 后面的工作很简单， 只要按照贝叶斯把它反过来，你就得到了给定观测下， 潜在因子（厨师）在不同时间段发生变化的概率分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;&lt;span&gt;三 &lt;span&gt;贝叶斯推理究竟告诉我们什么&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;udk1-0-0&quot;&gt;传统的深度学习的特点是大量标注数据驱动的黑箱， 不太考虑概率分布。 而到了深度生成模型的时代，我们必须考虑概率分布， 因此深度生成模型和贝叶斯有着深刻的内在联系。 同时，贝叶斯框架通过结合有效的先验，可以做到用更少的数据达到更好的泛化效果， 也极为的符合深度学习的需求。 两者在网络训练的结合请参考深度贝叶斯&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;贝叶斯分析里， 你会发现， &lt;strong&gt;你始终要有一个先验， 一个似然性， 而似然性事实上是某种简单的模型&lt;/strong&gt;（也可以很复杂！）。 事实上我们在我们的思维过程， 主动或被动， 正确或不正确的运用着贝叶斯，你所认可的事实里， 很多是你的推断。同样的客观数据面前，先验或似然性不同的人， 可以得出完全相反的结论。  教条的人可能给予了某个假设一个无限强的先验。 而容易被忽悠的大多数可能用到了过于简单的似然性模型， 比如用好人和恶棍解释很复杂的社会现象。 而自做聪明的人呢？ 可能用了一个对自己有利的解释模型， 而忽略了其它可能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383354&amp;amp;idx=1&amp;amp;sn=133519d77356bdae60ad9388c1f53cb5&amp;amp;chksm=84f3c87bb384416d488770175c676b9061c8168f7c84e10822f186fa8934009c67502279ba72&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;贝叶斯大脑&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382202&amp;amp;idx=1&amp;amp;sn=75d481a667221f328ed44ab76f241451&amp;amp;chksm=84f3cffbb38446edadae25d079a91e5f30592cf67a24e6bf60cf7314f3f60a5860e87cb91240&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;趣味贝叶斯推理&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 26 Mar 2019 16:02:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/EY2XU24xDA</dc:identifier>
</item>
<item>
<title>让神经网络变得透明-因果推理对机器学习的八项助力</title>
<link>http://www.jintiankansha.me/t/7xSXhTttBM</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/7xSXhTttBM</guid>
<description>&lt;p&gt;如今的AI已经能够在Dota2这样的对战游戏中战胜人类了，但游戏主播和解说却不需要担心自己失业，因为当前的神经网络还无法解释自己为何做出决定。我们可以训练另一个神经网络，对前一个神经网络做出的每一个决定，来预测人类会给予其什么样的解释。还拿Dota2举例子，可以训练一个神经网络，对操作游戏的AI点的每一个技能，买的每一个装备，去猜测人类会给予怎么解释。但当AI进化出人类想不到的策略时，就像围棋中Alpha Go已经能够走出人类棋手想不出的套路，上述策略就不行了。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceE1piajBpf3TqrYxHmeibImWckebOwWJ4fsd31zBMibQVet43icCpicq509m0AAGXoDATR6LAql1KIfEQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;1.4970059880239521&quot; data-w=&quot;334&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本文是《Possible Mind》（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;点击查看相关的读书笔记&lt;/a&gt;）系列读书笔记的第三篇，围绕因果推理的创始人，&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383568&amp;amp;idx=1&amp;amp;sn=fb2a6857f18cf4de917111406ef9bd4f&amp;amp;chksm=84f3c951b3844047e23a00d5aca0f9acac4aa27580dabfa7096401a166b9ad3196b34de9d251&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;《The book of why》&lt;/a&gt;的作者Judea Pearl的题为“The limitation of opaque machine learning”，延伸而写成。当前神经网络缺少解释性，其成功的原理是如魔法一般的黑盒子。当前提升模型解释性的方法，无论是注意力机制，对模型进行压缩，用线性的浅层的模型来模拟深层的模型，还是对隐藏层的权重可视化展示，都只是隔靴搔痒，无法带来质变。本文基于Judea Pearl的随笔以及他去年12月的一篇论文整合而成，先说明可解释的重要性，再讨论因果推理带来的本质改变，再详述因果推理对深度学习带来的八大助力。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.5215736040609137&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFoSUOFGsoGM4fGOgfiaU7oP7SBm1qzg8Ic3sicYT0ZVll1OKdKK5e584A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;788&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当扫地机器人遇到上图的情况，卡住了，如果其内部的AI能够正确的给出为何会卡住的解释，那下次机器人就会避免不平的环境，不管造成不平的是地毯还是掉在地上的衣物，这就提高了模型的可扩展性。能够给出解释，还可以帮助人更好与AI协作，假设每次扫地机器人能够告诉人自己今天因为那种原因卡住了，从而多花了多少分钟才扫完屋子，那这家的主人就可以下一次避免让家里出现地面不平的情况，从而让机器人能够更高效的工作，节约能源。而机器具有因果推理后，还会学到是自己清扫地毯的角度，使得地毯打折从而使得地面不平的，从而下次用其他的角度去清扫地毯覆盖的地方。&lt;/p&gt;

&lt;p&gt;这些都是实际的好处，而在《Possible mind》这本书中，借用哲学家Stephen Toulmin在1961年的书《Foresight and Understanding》中的对比，引入了巴比伦和雅典科学的区别，同样是预测天体的运行，四季的节律，巴比伦的预测在精确程度和一致性上都好过同时代的雅典人，但雅典的预测背后有神话去提供解释，并且当一种解释比另一种解释更符合时，前者会战胜后者，巴比伦式的精准预测，没有带给这个文明天文学，而古希腊则孕育了现代科学。&lt;/p&gt;

&lt;p&gt;类似的还有李约瑟之问，为何古代中国没有发展出科学，尽管其很长的一段时间中技术是领先全球的。比如勾股定理，明明中国人早在《周髀算经》中就有所记录，但西方人却称之为毕达哥拉斯定理。这里的区别在于前者只是从经验的层面记录了这个现象，而后者是对此给予了普遍化的证明。用机器学习的视角来看，前者无法确定这个规律的可扩展性如何，而后者保证了在所有的宇宙中，该规律都是适用的。&lt;/p&gt;

&lt;p&gt;借用这个对比，当前的深度学习，尽管取得了突飞猛进的进展，但由于其缺少可解释性，不透明，还是属于巴比伦式的。即使其在所有的游戏上，都战胜了人类，都需要花更少的能源即时间去训练，也不能算是能匹敌人脑的强人工智能。雅典的天文学者，可以根据自己的理论，去设计一个实验，来估算地球的半径，还和真实的结果相差不多，而巴比伦式的不透明的“天文知识”，则根本不会问出这样的问题。当今的大数据，不会用虚假的概念来讲故事，而根据《人类简史》中的论述，正是想象出的共同体，使得人类走进了文明时代。&lt;/p&gt;

&lt;p&gt;按照Judea Peral的分类，&lt;span&gt;认识世界分三个层次&lt;/span&gt;，最低的是关联，只需要观察就好，例如那些症状告诉医生这个人患病了，那些行为告诉我这个人容易被促销打动；再上一层是干预，也就是去通过行为去改变世界之后，看会发生什么，这个层次要回答的问题是吃药能不能治病，而最高的层次是反事实的推理，要达到这一层，需要想象力，需要反思，要回答的问题是如果我之前多一些锻炼，现在是不是就不会生病。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4044526901669759&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFxWUOzYB4LibnMGkdNSe2zn74SnJzHMnRvGCtZ9226cutBZ2S1qeJYGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1078&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当前的有监督学习，是站在了认知阶梯的第一层，强化学习由于和坏境有所互动，是站在了第二层上，通过无监督学习，去预测下一秒会发生什么，根据类比来推测位置的情况，也是介于第一层和第二层之间的。不论是那种学习范式，都是基于统计的，得出的结论是概率性的。正如同不懂得证明勾股定理，永远也无法百分之百打包票说这个规律是普世的，当前基于统计的机器学习，如同三体中被智子锁死的地球科技，看似进步神速，但总会碰到天花板。&lt;/p&gt;

&lt;p&gt;Judea Pearl给出的解药是他发明的公理化式的因果推理图，想要详细了解的推荐下面的免费课程，&lt;span&gt;edx&lt;/span&gt;&lt;span&gt;平台上的，名为&lt;/span&gt;&lt;span&gt;Causal Diagrams: Draw Your Assumptions Before Your Conclusions。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.46193265007320644&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceE1piajBpf3TqrYxHmeibImWH4Zs7x6s6ZVY1ODPsY9vzs4zTme2ibBCM1aqicmicl4jM1p0ERNa3OiaTw/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那如何机器学习在中加入因果推理了？回答是结构化因果模型（Structural Causal Models，下文简称SCM），通过有向图的形式，对常识中的因果作用方向的假设做结构化的建模，例如下图中x代表是否采取实验疗法，y代表从癌症中康复，z代表性别，那么对世界的模型就是性别可以决定一个人是否更可能会选择实验疗法，是否更容易从癌症中康复，但反方向的因果却是不现实的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFkmIdVhnicKHGQYTyub3mlicxY5lTTz2ibIZ5BVOoYXQ6DSEdtjuDv1UZA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;364&quot; data-cropy1=&quot;13&quot; data-cropy2=&quot;124&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFel6zNBrX1FuaFPE8xN15j0oer3GKTicFFpmtm7M3J0J0uD4POqaURPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;364&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了因果图，那SCM就可以根据提问，来计算一个和因果推理的问题，究竟有多少数据的支持，不管这个问题位于上述的认知阶梯的第几层，都能够给予回答。这里的&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFWq1RlL2E9ySJjEkseRibSshqKcKKTjmTmguvnRBPhUtIRNriauX7npCw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;209&quot; data-cropy1=&quot;6&quot; data-cropy2=&quot;35&quot; data-ratio=&quot;0.13875598086124402&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BF1skbnL0pdd07gt5DyDqecNgQE8uGdoFfPgZ2oFaj0e6wcokLm8cnOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;209&quot; /&gt;，也就是考虑不同性别下是否采取实验疗法对癌症康复的概率，而根据深度学习的模型，可以估算出真实的状况下的概率，如果发现俩者的差距很大，那因果推断的模型就可以对实验疗法导致癌症康复这个因果论断给予反驳，说明这是没有事实依据的，而不是只说明实验疗法和癌症治愈缺少统计显著性。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.5781893004115226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFhdzw631DGKqudv649xN39sTpUam05u3XianZjiaxkAuPdX4KhZetEoAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;486&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;那这对于解决当前机器学习缺少解释性的问题，又什么本质的改变了？这里列出Judea Pearl的八条回答：&lt;/p&gt;

&lt;p&gt;首先是让机器做的假设以人类容易理解的方式（因果图）呈现出来，从而&lt;span&gt;让模型更加透明&lt;/span&gt;，也让测试模型的推论的后人能够更精准的去检验模型的鲁棒性。可以说只有能够解释清楚自己每一步的推理的逻辑，模型才算具有了可证伪性，否则即使有一个不符合模型预测的事件，由于不确定其和模型的因果假设有什么关系，又该怎么区分这究竟是应该被去除的噪音还是能推翻整个认知模型的反例。&lt;/p&gt;

&lt;p&gt;第二点是通过因果推断，&lt;span&gt;去除混杂因素的影响&lt;/span&gt;。当前的机器学习，重要先进性数据清洗，特征提取，往往花在特征工程上的时间占到了全流程的大头。有了因果推断，就不必人，来根据常识去掉那些可能影响相关性的混杂因素，从而在更复杂的坏境下，做到端对端的学习。这使得模型能够超越建模者的认知局限，从而模拟真实坏境更复杂的相互作用。&lt;/p&gt;

&lt;p&gt;第三点是算法化的&lt;span&gt;回答反事实的问题&lt;/span&gt;。人类能够区分出充分条件和必要条件，能区分cause of effect与effect of cause，例如小明酒后游泳溺水而死，游戏是小明死亡的必要而不是充分条件，要回答这样的问题，就需要进行反事实的思考，去幻想如果小明没有游泳会怎样，如果游泳时小明没有喝酒会怎样。如果机器能够做这样的思考，那AI思考的模块化程度就会进一步提高，需要的训练数据也会减少，对于跨领域的迁移学习也会有所助力。&lt;/p&gt;

&lt;p&gt;第四个助力是&lt;span&gt;区分直接和间接的诱因&lt;/span&gt;，如果只有关联分析，那在较长的时间尺度上，就会面临如何区分是否之前的决定带来了奖励的问题，但如果能够将因果关系描述出来，并且根据数据来评价每一条因果链条的坚固程度，那就能够去解决强化学习中在较长的时间尺度上，该如何分配奖励的问题。区分了直接的诱因与通过第三方作用间接的影响，就能够判定数据中那些异常点处在间接影响的链条上，受到未知因素的影响，属于噪音，而对于处于直接因果链条上的，则异常不应该被视作是噪音，而是可以证伪模型的“黑天鹅”。&lt;/p&gt;

&lt;p&gt;第五个助力是&lt;span&gt;模型具有跨领域的适用性&lt;/span&gt;，还能够通过其他领域来验证该模型的鲁棒性。如果一个通过强化学习的智能体在一种游戏中表现优异，那预期换一个游戏，该模型也不会表现的太差，这种能力被称为domain adaptation，人类就有这个能力，例如dota玩的好的人，玩英雄联盟也不差。如果智能体是通过因果推理，来决定下一回合的policy，那这个思考过程就更像人类做决定时的所思所想，由此类比推出，智能体也会具有更好的domain adaptation。&lt;/p&gt;

&lt;p&gt;第六个助力&lt;span&gt;避免sampling bias&lt;/span&gt;，正如人类的认知偏见会让人丢掉那些对支持自己结论不适合的数据，人类在对机器建模时，也会展现出类似的认知偏见。如果机器具有了公理化的因果推理，那通过反事实的问题，就可以指出人类可能受到了采样偏见的影响。这指出了人机协作的新的可能性，不是机器只懂得找出相关性，从而指数级的放大人的认知偏见，而是机器根据人对世界的建模，去帮助人做人类不擅长的用数据找出偏见，从而带来一个更公平的模型。&lt;/p&gt;

&lt;p&gt;第七个助力是通过因果模型，来&lt;span&gt;判定数据集中是否存在数据缺失的问题&lt;/span&gt;。例如建模者以为女性不擅长数学，那用来训练该录取那个学生的分类器时女性申请者的样本就会很少，从而使得基于统计相关的模型预测女生不应该录取到数理相关的专业。但对于基于因果判定的模型，那只要对世界的假设中包含性别会影响是否报名数理相关专业这个因果联系，那模型就能够根据数据判定出这里存在着可能的数据缺失，从而提醒建模者注意。&lt;/p&gt;

&lt;p&gt;第八个助力是去&lt;span&gt;发现因果关系&lt;/span&gt;，例如遗传学中的孟德尔随机，就是利用了基因在有性生殖中自然会发生重组，来区别到底基因的差距与人身体表现出来的胖瘦高矮这样的表型到底是因果性还是相关性。现实中存在着诸多类似孟德尔随机的自然形成的与随机双盲实验等价的场景，通过让模型具有因果推断能力，就能够发现未知的因果关系。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.4863498483316481&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFjuZl8uM1icZrPsLAef5gOxzIcwGK0GEkmibArREh39o98G0ibEztsXWVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;989&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;附图是孟德尔随机的示意图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;总结全文，当前的深度学习，这样model free的模式，无法达到人类水平的智力，只有增加了因果推断，才能像科学的精神孕育出持续不断的发现那样，让机器学习走到之前无法企及之地，例如上文列出的八种具体任务，都需要更高层次的思考。当前的深度学习，模型缺少解释性，可迁移性，也不够鲁棒，会由于一个像素的改变而彻底改变分类的结果。&lt;span&gt;所有的指数级增长都会有尽头，当前深度学习取得的成就，大多&lt;/span&gt;只是依赖计算资源和训练数据的指数化增加，&lt;span&gt;只有不断站在更高的认知阶梯，通过&lt;/span&gt;自我指称获得的递归式的思考，才是无限的增长模式。因此将因果模型引入下一代人工智能中，是充分且必要的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;需要注意的是，因果推断的模型，依赖与建模者去根据常识或本领域背景，给定对因果关系运行的方向，如果这个假设有问题，那模型是无法从数据中发现人类建模者犯了因果倒置的问题的，这说明不管多么先进的模型，都需要建模者的智慧参与其中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参考文献：&lt;span&gt;The Seven Tools of Causal Inference with Reflections on Machine Learning&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384252&amp;amp;idx=1&amp;amp;sn=bdc733e516f25b44a1b7bdfb6104d2b5&amp;amp;chksm=84f3c7fdb3844eeb9f82b2f7e0590f3514f5b9887c279ccdd5919004b59a8686353a85670742&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;信息的俩种定义&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384244&amp;amp;idx=1&amp;amp;sn=b7ea9482daef936f6f433719ad419097&amp;amp;chksm=84f3c7f5b3844ee35fb7a98ae317ed19ea32a5e79f1529a8d0c7da2bfb361545443b03f0f5ba&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;如何让神经网络具有好奇心&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;读《Possible Mind》，看25位大咖谈AI&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 23 Mar 2019 11:56:59 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/7xSXhTttBM</dc:identifier>
</item>
</channel>
</rss>