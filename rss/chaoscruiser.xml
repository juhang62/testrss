<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>AI会完美的执行我们设定的目标，但这不是一个好消息</title>
<link>http://www.jintiankansha.me/t/VIB1nHCP8L</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/VIB1nHCP8L</guid>
<description>&lt;p&gt;&lt;span&gt;导读： &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;科普网站quanta magazine上一篇名为 Artificial Intelligence Will Do What We Ask. That’s a Problem的文，是关于AI与人际交互时，如何满足更好的理解人类需求。该文的核心观点是：通过教机器理解我们真实的欲望，科学家希望避免让它们做我们所要求的事情可能带来的灾难性后果。本文先详述（部分翻译并用自己的话重述）一篇AI伦理相关的文章，之后会根据这篇文章的观点，对当前最热门的时政，进行简要的延伸和议论。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1）当今以目标为导向的人工智能存在局限性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;牛津大学哲学家Nick Bostrom在2003年提出了一个现在已经成为经典的思想实验，来说明这个问题。设想一个超级智能机器人，其编程目标看似无害，仅仅是制造回形针。这个机器人最终把整个世界变成了一个巨大的回形针工厂。&lt;/p&gt;

&lt;p&gt;现实中，Youtube 为了最大化浏览时间，部署了基于 ai 的内容推荐算法。两年前，计算机科学家和用户开始注意到，YouTube 的算法似乎通过推荐越来越极端和阴谋论的内容来达到目的。有关素食主义的视频导致了有关素食主义的视频。关于慢跑的视频导致了关于跑超级马拉松的视频。” 因此，研究表明，YouTube 的算法一直在帮助人们极化和激进化，传播错误信息，只是为了让我们观看。&lt;/p&gt;

&lt;p&gt;人类常常不知道给我们的人工智能系统设定什么目标，因为我们不知道我们真正想要的是什么。“如果你问街上的任何人，‘你想让你的无人驾驶汽车做什么? ’ 他们会说，‘避免碰撞，”’但你会意识到，不仅仅是这样，人们还有很多偏好。” 超级安全的自动驾驶汽车行驶得太慢，刹车太频繁以至于乘客生病。&lt;/p&gt;
&lt;p&gt;当程序员试图列出机器人汽车应该同时兼顾的所有目标和偏好时，这个列表不可避免地以不完整告终。 说起她在旧金山开车的时候，她经常被一辆停在街上的自动驾驶汽车卡住。正如程序员告诉它的那样，它可以安全地避免与移动的物体接触---- 但这个物体就像一个塑料袋 。&lt;/p&gt;

&lt;p&gt;AI研究者Stuart Russell认为，当今以目标为导向的人工智能最终还是有局限性的，因为它在完成特定任务方面的成功，比如在危险边缘和围棋中击败我们，然而通过将机器的目标设定为最优化一个“奖励函数”(对某些目标组合的一丝不苟的描述) ，将不可避免地导致人工智能失调，因为奖励函数不可能包括并正确衡量所有目标、无法理清主要与次级目标、同时无法应对例外和警告，甚至不可能知道哪些是正确的目标。给自由漫游的“自主”机器人设定目标，随着它们变得越来越智能，风险也会越来越大，因为这些机器人会无情地追求自己的奖励功能，并试图阻止我们关闭它们。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2）对人类有益的AI应该是怎样的&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Russell在最近出版的新书《Human Compatible》中给出了3条“对人类有益的AI应遵守原则”，呼应著名的的机器人三定律，分别是：&lt;/p&gt;
&lt;p&gt;1 机器的唯一目标是最大限度地满足人类的偏好&lt;/p&gt;
&lt;p&gt;2 机器最初不能确定这些偏好是什么&lt;/p&gt;
&lt;p&gt;3 关于人类偏好的最终信息来源是人类行为&lt;/p&gt;



&lt;p&gt;接下来指出这篇文章的核心观点，&lt;span&gt;AI不应该去试图实现最大限度地提高观看时间或回形针产量这样的目标，它们应该只是试图改善我们的生活。 只有一个问题: “如果机器的终极目标是试图最大化人类经验到的快乐的总和，AI究竟如何知道那是什么? ”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;这个问题的难度在于，人类一点也不理性: 我们不可能计算出在任何特定的时刻，哪些行动会给我们的长期未来带来更好的结果;Russell认为，我们的决策是层次化的ーー&lt;span&gt;我们通过较为清晰的中期目标，以追求相对模糊的长期目标，同时最关注眼前的情况，从而逼近理性的决策&lt;/span&gt;。他认为，AI需要做类似的事情，或者至少了解我们是如何运作的。&lt;/p&gt;

&lt;p&gt;在强化学习，AI通过外界环境的反馈，来优化它的奖励函数，比如它在游戏中的得分; 当它尝试各种行为时，那些增加奖励功能的行为会得到强化，并且更有可能在未来发生。由Stuart Russell和 Andrew Ng提出的“逆向强化学习（ inverse reinforcement learning）”系统不会像强化学习一样试图优化某个奖励函数; 相反，它试图了解人类正在优化的奖励函数是什么。 &lt;/p&gt;

&lt;p&gt;强化学习系统会计算出实现目标的最佳行动，而逆向强化学习系统则会在给定一系列行动时破译潜在目标。如果计算机不知道人类喜欢什么——“它们可以做一些逆向强化学习来学习更多知识。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages js_insertlocalimg&quot; data-ratio=&quot;0.66640625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdRRQe9Mra6ppsDOpYkwT1w6x2VicEwM1y7wWfj435yyzj7wP5Bh0fcJia5Vib92DkDf0y4FNdJkL6AA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Russell进一步提出了“合作逆向强化学习” ，在这种模式中，机器人和人类可以一起工作，在各种各样的“辅助博弈”（代表真实世界、部分知识情况的抽象场景），中了解人类的真实偏好。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3）偏好的不确定性与AI的关机问题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1951年，阿兰 · 图灵在BBC的一次广播讲座上提出，也许可以“让机器处于从属地位，例如在关键时刻关闭AI的电源” 研究人员现在发现这种观点如今显得有些简单化。 如何阻止智能代理禁用它自己的关闭开关，或者更广泛地说，忽略停止增加其奖励功能的命令？ &lt;/p&gt;

&lt;p&gt;在《Human Compatible》一书中，罗素写道，关机问题是“智能系统控制问题的核心”。如果我们不能关掉一台机器，因为它不让我们关，我们就真的有麻烦了。如果我们能做到，那么我们或许也能够以其它方式控制它。”&lt;/p&gt;

&lt;p&gt;关于我们偏好的不确定性是这个问题的核心难点，例如在下面的具体案例中，机器助理罗比正在决定是否代表使用者哈丽特做决策，例如，是否为她预&lt;/p&gt;
&lt;p&gt;订一个漂亮但昂贵的酒店房间，但罗比不确定她会喜欢什么。罗比估计哈丽特的回报可能在 -40到 + 60之间，平均 + 10(罗比认为她可能会喜欢那间豪华的房间，但不确定)。无所事事的收益是0。 &lt;/p&gt;

&lt;p&gt;但还有第三种选择: 罗比可以询问哈丽特是否希望罗比继续为她做决策，或者更愿意“关掉它”——也就是说，让罗比退出酒店预订决定。如果她让机器人继续工作，哈丽特的平均预期收益将大于 + 10。所以 罗比会决定和哈丽特通过商量决定，如果她愿意，就让罗比关机。一般来说，除非罗比完全确定哈丽特本人会做什么，它会更倾向于让她来决定。“事实证明，&lt;span&gt;对目标的不确定性对于确保我们能够关闭机器至关重要，”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;4）合作逆向强化学习面临的挑战&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;合作逆向强化学习，在其提出者Russell看来，面临两大挑战。“其中是，我们的行为远非理性，因此很难重建我们真正的潜在偏好，”他说。&lt;span&gt;人工智能系统需要理清长期、中期和短期目标的层次结构&lt;/span&gt;——我们每个人都被锁定在无数的偏好和承诺中。如果机器人要帮助我们(并避免犯严重的错误) ，它们需要知道如何绕过我们的潜意识信念和未明确表达的欲望这张模糊的网。&lt;/p&gt;

&lt;p&gt; 第二个挑战是人类偏好的改变。我们的思想会随着我们生活的进程而改变，它们也会随着我们的心情或者机器人可能难以理解的环境的改变而改变。&lt;/p&gt;

&lt;p&gt; 此外，我们的行为并不总是符合我们的理想。人们可以同时持有相互冲突的价值观。机器人应该优化哪一个？  为了避免迎合我们最坏的冲动(或者更糟糕的是，放大这些冲动，从而使它们更容易满足，就像 YouTube 算法那样) &lt;/p&gt;

&lt;p&gt; 机器人可以学习Russell所说的我们的元偏好: “描述关于什么样的偏好改变是可以接受的偏好。” 即我们对自己感觉上的变化有什么感觉？&lt;/p&gt;

&lt;p&gt;像机器人一样，我们也在试图弄清我们的偏好，它们是什么，我们希望它们是什么，以及如何处理模糊和矛盾。 像我们一样，人工智能系统可能会永远停留在问问题上，走不出不确定性的带来的局部认知上，因为不确定而无法提供帮助。&lt;/p&gt;

&lt;p&gt;然而，还有第三个主要问题没有出现在罗素的关注列表中: 坏人的偏好是什么？如何阻止机器人为了满足邪恶主人的邪恶目的而工作？人工智能系统倾向于找到绕过禁令的方法，就像富人在税法中找到漏洞一样，简单地禁止他们犯罪可能不会成功。&lt;/p&gt;

&lt;p&gt;或者，更黑暗的是: 如果我们都是坏人怎么办？例如一直在努力修正自己，能够捕捉到无处不在的人类冲动的推荐算法，是否在通过满足人类短期的需求，而让人类忽略了气候变化这样长期的威胁了？&lt;/p&gt;

&lt;p&gt;尽管如此，Russell还是感到乐观。尽管还需要更多的算法和博弈论研究，但他表示，他的直觉是，有害的偏好可能会被程序员成功地赋予更低的权重，&lt;/p&gt;
&lt;p&gt;而且&lt;span&gt;同样的方法甚至可能“在我们培养孩子和教育人等方面”很有用 换句话说，在教机器人做好人的过程中，我们可能会找到一种教会自己的方法&lt;/span&gt;。他补充说，“我觉得这也许是一个机会，可以引导事情朝着正确的方向发展。”&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;5）结合时政的评论&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;这篇文说的虽然是AI，但也适合人的决策。湖北潜江市因为提前预警，导致病患数目是最少的，而武汉由于拖延而没有控制疫情。这背后的对比，就在于前者不止是遵守规范，完成上级设定的目标，而是对人民真正需要什么，有过深度的思考和理解，从而能够做出一些不那么听话，但有用的举措，即要敢于打违抗命令的胜仗。今天看到一篇名为“如果武汉市长是李云龙”的帖子，看到这个标题，你应当能猜到答案，如果是这样，情况不会像现在这样。公众号西西弗评论的帖子“艰难的决策与领导的担当”其中谈到类似的观点。要想了解人民想要什么，需要借助新技术，实时了解一线的情况，之后不拘一格的，敢于承担风险的做出非常规决定，而不是官气十足把开会作为第一要务，为开会而开会。&lt;/p&gt;

&lt;p&gt;如果一个人只能按照上级要求的去完成任务，而不能真正理解上级需要的是什么，那么这个人在职场多半只能做办事的人，而不能成为管理者。之所以需要管理，就是需要专门有人来厘清，分解出一个组织真正的目标是什么？客户的潜在偏好与渴望在那里？通过本文对AI算法的分析，可以启发读者思考，自己该如何和家人，同时协作解决生活中的”合作逆向强化学习”。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;张爱玲说写作的诀窍无非是，写别人想写的或者写别人想听的。这其中的意味，就在于写作者要解决本文提到的”逆向强化学习“问题，但作为读者，要明白既然大部分文章本身是如此写出来的，我们明白，写作者的目标和我们自身的目标很大程度上是不一致的。个人需要的优化自己长期的福祉，即要通过奋斗获得个人的成功，又要通过对自我的承诺获得人际间的尊重。为了达到这个目标，就要听到和自己观点不同的，特别的对其加以重视，并争取用自己的观点复述出来；而听到和自己观点一样的，则要对其重点加以批判，看看是否存在逻辑谬误。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;点击阅读原文，查看 Artificial Intelligence Will Do What We Ask. That’s a Problem 的英文原文&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.quantamagazine.org/artificial-intelligence-will-do-what-we-ask-thats-a-problem-20200130/&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384867&amp;amp;idx=1&amp;amp;sn=31bfe19e88dfc82a16f063c59677b252&amp;amp;chksm=84f3c262b3844b74c063ff456290e739f8df94054707d41b8633fc0e122f7a9dcc7222b56ce3&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;鲁棒特征与该怎么更好这一生&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383943&amp;amp;idx=1&amp;amp;sn=ec5b8c9495b422df0e6d63a6a8409468&amp;amp;chksm=84f3c6c6b3844fd01fe985fddb6fb8447c1faad19af538bbf1cf5b2365e1891175fa67ad1726&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;自由能最小说的是什么？又该怎么批判性的去看？&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384973&amp;amp;idx=1&amp;amp;sn=7c009a6e10ed6c5287266988b4ce3c83&amp;amp;chksm=84f3c2ccb3844bdaea4dce536365e5cd90924a029a4ba4d24232430f489341293899b13db9c8&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;疫病期间的反思II - 网络效应的威力&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;



</description>
<pubDate>Tue, 11 Feb 2020 01:02:14 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/VIB1nHCP8L</dc:identifier>
</item>
<item>
<title>从时间序列表示到层级化的强化学习</title>
<link>http://www.jintiankansha.me/t/7WLPYLwSt7</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/7WLPYLwSt7</guid>
<description>&lt;section&gt;CNN对图像的表示我们熟悉是从细节纹理到轮廓到物体部分到概念的层级表示关系。那么神经网络对时间序列可以建立什么样的表示呢？要知道时间序列是比图像更普遍的一种数据类型。我们知道最广泛使用的也是最高级的时间序列是语言。那么什么是时间序列的一般表示呢？&lt;/section&gt;
&lt;p&gt;这篇文章从神经科学的角度从五个层次解析了时间序列的表示问题。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees. Dehaene, Stanislas, et al. &quot;The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees.&quot;&lt;/span&gt;&lt;span data-offset-key=&quot;ffrcq-0-1&quot;&gt;Neuron &lt;/span&gt;&lt;span&gt;88.1 (2015): 2-19.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;首先， 对于一个一般的时间序列， 你可以想象以声音为例，当声音进入到耳朵里， 它本来仅仅是一些频率音量不同的振动，它怎么就变成了音律，变成了语言？这就说明我们的神经系统不是在机械的接受这些时间序列， 而是不停提取和序列有关的信息， 最终合成成为有意义的东西。这个过程， 事实上类似于图像的层级化处理， 也是一个有层次的过程。这种层次， 被分为如下图的5步：&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.6958333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYPqZpn3XNoxafxbrJsU3ukhLukUEwTiaBN8eKskvc6Ij9vDInyWic3VPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;

&lt;p&gt;第一层， 神经网络可以提取和序列中的时间信息，比如不同音符间的间隔，每个音符持续的时间等，以及这些音符是如何转换的。或者说在这个层次，神经网络主要care的维度就是时间，而尽量忽略其它维度。如果用比较数学的语言来说，&lt;span&gt;神经网络会对时间序列下一步的输入做预测， 这种预测的主要依据是一个从过去的输入到当下的条件概率。&lt;/span&gt;&lt;span&gt;&lt;img data-ratio=&quot;1.2&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFEvRvxKqE3SKsziagcjPlviaIhJYEsVsibAyj4KmxAWic3Ypmf9UIn8Dd4ibaYlgPOmtoC8kcD3fSBstUr7iatcZw1HD/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;5&quot; height=&quot;6&quot; width=&quot;5&quot;/&gt;&lt;img data-ratio=&quot;0.07936507936507936&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFEvRvxKqE3SKsziagcjPlviasL9b7Z8ibnrDmZzlMgyiaDKOTJSdurgQhOaqoMSQZwzY9pIiaRHeT5QTOwG8AqAafa1/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;252&quot; height=&quot;20&quot; width=&quot;252&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;生物神经网络的本性，根据时间对未来的刺激做预测， 比如你给它一个周期刺激， 当某个周期相应的刺激没有来，就会发放一个缺失信号表示惊讶。&lt;/p&gt;
&lt;p&gt;然后， 神经网络可以对序列进行分块， 好比切割成由一列列车厢组成的而火车， 这样， 序列就不再仅仅依赖于时间，分出的块好比一个个物体， 或者item，这就好比CNN对图像进行了边缘分割。具体如何做分块切割呢？通常一个过程里会出现自然的分割节点， 比如你在左顾右看的时候， 视觉信号是输入时间序列， 你看到左边的某个边界或右边的边界，就是时间序列的自然分割点。&lt;/p&gt;

&lt;p&gt;再后，神经网络可以抽取这些item之间的顺序信息， 而无视每个块的具体时间序列， 如时间长度一类，也就是此时的神经网络可以区分abc和acb的区别，而忽略时间或单个块的区别。&lt;/p&gt;

&lt;p&gt;如果前面的看起来和智能关系不大， 后面就厉害了， 因为生物神经网络还可以抽取块与块之间的代数模式， 比如cocolith， co-co 连续出现了两次，这就是模式。这种模式本身代表了一种统计规律， 被我们称为regularity 。我们可以看做神经网络可以在复杂的音乐中识别出模式，识别出和弦。&lt;/p&gt;


&lt;p&gt;&lt;img data-ratio=&quot;0.9569444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYKibiauqrIialrD5wUuUqibq49RmEibdMYCA46LmrkswL63UicJ5B7eloHBkw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;

在这个阶段，神经网络关注不同输入间的某种统计相关性而不是具体输入的类型， 可以称为抓取序列的某种抽象结构知识 （TurnTurnTurnTurn = PushPushPushPush != TurnTurnPushPush）

&lt;p&gt;最后， 被作者称为人类独有的， 是抽取序列里由一定符号规则生成的“语法”，这种规则可以反复使用实现层级结构，也就是树结构。&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5653923541247485&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYZPSo0uNWrOpoY1j74zQCALRE5YqclESQ8ggvCKSDWzGt6LmUiaLmj1w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;497&quot;/&gt;&lt;/p&gt;

对树结构的解析模式：从词语到短语到句子的语法树结构，这是对时间序列的最高级表示

&lt;p&gt;到最后一个级别， 我们可以看到这已经不再是机械的时间序列处理， 而几乎是自然语言处理了。你有没有想到一个重大的问题也就是语言的起源呢？你有没有想到乔姆斯基的通用语法规则呢？&lt;/p&gt;

&lt;p&gt;除了上面介绍的方法，从时间序列到空间结构， 从时间序列到因果图， 都可以看做是很高级的序列表示生成方式。&lt;/p&gt;

&lt;p&gt;如果这里看去， 好像没有什么意思的神经科学文章， 那么后面我们会看到这样一个思想如何有助于解决一个AI问题。&lt;/p&gt;

&lt;p&gt;强化学习本质处理的就是时间序列， 由输入观测的时间序列， 我们要得到一组动作的时间序列， 最终得到我们要的奖励。每个动作都需要通过学习到的策略得到。这是经典的强化学习方法， 但是这种一个个动作学习的方法未免过于繁琐，而且需要遍历的动作总数犹如组合爆炸（想一想即使每个回合只有四个基本动作， 一百回合就是4的100次方种组合）。一篇新的文章给我们一个完全不同的思维角度。&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.4888888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYEcb53IyrA16FUoaSe58mQbQqvqlcmay2qLOLgiabITjk9zGSuQDPQCA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;强化学习通常用如上框图实现， 即agent会根据环境给与的reward调整action的一个反馈系统， 最终实现利益最大化， 难点在于agent的行为通常改变环境，而环境影响行为策略。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Christodoulou, Petros, et al. &quot;Reinforcement Learning with Structured Hierarchical Grammar Representations of Actions.&quot;&lt;span data-offset-key=&quot;e0gr8-0-1&quot;&gt;arXiv preprint arXiv:1910.02876&lt;/span&gt;(2019).&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;这篇文章的核心思想是， 如果把动作看做一组时间序列，我们可以对动作的集合本身建立一套语法规则， 得到一组macro-action，也就是由若干动作组成的动作的宏，它们可以作为新的基本单位被学习。这样，我们变可以把动作 打包来学习。这样一个个基本动作就可以构造出一个层级嵌套的结构 ，大大减少需要学习的动作组合数。这种方法在20个atari game里19取得了 性能的进步。&lt;/p&gt;

&lt;p&gt;想一想这和人类的基本学习习惯是类似的。你去学习做饭，不会去学习如何用一个个基本的运动手臂的动作构成倒盐，加油，烧水的一连串动作，菜谱只会告诉你热锅之后先煎西红柿后放入鸡蛋即可，也就是说我们默认了即使做菜的种类很多，有几种基本行为是类似的，那些基本行为又由更基本的动作构成，但是我们不需要提起那些更基本的动作，就可以告诉你怎么做菜。这里提到的方法是类似的， 而唯一不同的是每个基本行为是要学习的，&lt;span&gt;我们需要知道如何从更基本的动作构建基本的行为。&lt;/span&gt;&lt;span&gt; 事实上这个方法是实现层级强化学习的一种方式。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.4097222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYds87qkySTK8RpmzhJj2DLc7TGQyLpU8UMV8Itfw6HyguzENWk5hNibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;

勃艮第牛肉的制作过程为例看层级化的强化学习。每个动作由更基本的动作组成，又成为更复杂的动作的基础。

&lt;p&gt;&lt;img data-ratio=&quot;0.33055555555555555&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYQJpjWiaBGicqWlW3jKeYaobibzaIcZ1wjB0ORJiadZVdtn84iakfbHXVqCg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;
&lt;p&gt;那么如何去学习这些动作的宏-基本行为呢？我们可以看到， 此处对动作时间序列的处理， 刚刚所说的一般时间序列表示，本质是一样的，&lt;span&gt;都是通过先打包，再整和包和包之间的语法规则，得到动作的层级结构&lt;/span&gt;。如果神经网络可以对时间序列实现如上表示。那么它就可以通过观测自己的动作，得到如何组合这些动作得到其语法结构。&lt;/p&gt;

&lt;p&gt;算法的具体实现如下：&lt;/p&gt;

&lt;p&gt;agent需要在探索游戏并记录自己在游戏中的历史，与一般的强化学习流程不同，这里的一些经历是没有加入噪声的，这些经历被单独放入到一个叫Sequitur的程序里进行action的语法生成， 在这个过程里， 一些经常重复的行为片段比如ababab，会被替代为ccc ， c =ab， 这个新生成的c， 被称为marco-action，生成符号的标准是新符号可以减少描述整个序列需要的字符数（符号的本质？）如此我们进行一级级的迭代，就得到action的整个语法树，这里的方法像不像是刚刚讲过的从序列中生成代数结构和树结构的方法？&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;img data-ratio=&quot;0.6444444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYATxsccBMfTtLYp8FawOKIdToEkkkv8GN0FI8yRhkcrh5fEmpic2Tz1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.4777777777777778&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd2IeOuh82slhAlyWzrOcsYQmaicFJpSg2o7QLAbpy5TJ0jmelzWF27ribfGD5ncBp0jO7N1gbJIW8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;

通过回放机制方便生成新的宏


&lt;p&gt;然而我们也可以从这里联系到技术的起源。有一本书叫技术的本质， 它提出技术是一种进化的生命体， 它可以看做是由更基本的单元技术组成，这些不同的单元技术构成技术的模块，组成更复杂的技术。这或许帮助我们理解为什么近代最先进的技术出现在欧洲而不是中国。欧洲尤其是地中海地区，地理位置上很容易汇聚从中东，非洲，小亚细亚和欧洲本地的技术，从而交汇组合不同的技术元素，组成更大的技术的“宏”，促进更新技术的发展。&lt;/p&gt;

&lt;p&gt;由此我们想到如果把技术本身看成某种强化学习可以学习的动作集合， 那么技术不就对应这种层级化的强化学习吗？&lt;span&gt;较高级的技术由较低级的技术模块组合而成， 这些较低级的技术模块本身因为经常被重复使用就被打包起来&lt;/span&gt;。 &lt;/p&gt;
&lt;p&gt;从这一点看， 我们或者可以联想如何可以通过强化学习帮助我们生成新的技术。&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdibJvzibQknhAFzcIKEffcMJu3vsSBgZiaqyOzqT4jM3ENRAfRtb8lrLDMLeYZmia01KF9iblFMxgG7xg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot;/&gt;&lt;/p&gt;

&lt;section/&gt;
&lt;p&gt;整个这篇文章说明了，时间序列的抽象表示是如何帮助我们解决一个非常困难的强化问题的。本质上&lt;span&gt;越是能够从具象的输入信息里抽象出结构性知识，就越能从降低维数灾难， 也越能够让学习变得简单容易&lt;/span&gt;&lt;span data-offset-key=&quot;bkfqv-0-0&quot;&gt;（从更小的集合里寻找最优解），得到的解也越容易具有鲁棒性（策略建立在一般结构之上而不是细节上）。因此从这点上看， 抽象的意义是巨大的。那些最聪明的大脑，往往也是最擅长从细节里寻找一般性结构的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384881&amp;amp;idx=1&amp;amp;sn=0faf30bbeb5e4f7b386c9ba08aab8ebb&amp;amp;chksm=84f3c270b3844b6636333248db8e75e3e348517bed5ba3ad53851f78e5dd9317a186294a499e&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;多巴胺引领下的分布式强化学习&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384652&amp;amp;idx=1&amp;amp;sn=6f9d524b432673822e4c5da4e188522e&amp;amp;chksm=84f3c50db3844c1bb4f6a29b280c517467933c203a4fbeec529f229173e311431944012adc4e&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;站在AI与神经科学交叉点上的强化学习&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 09 Feb 2020 01:02:36 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/7WLPYLwSt7</dc:identifier>
</item>
</channel>
</rss>