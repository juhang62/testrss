<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>从神经网络和强化学习的角度来看人类的决策系统</title>
<link>http://www.jintiankansha.me/t/yl7WYKt9Em</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/yl7WYKt9Em</guid>
<description>&lt;strong&gt;
                &lt;a href=&quot;http://www.jintiankansha.me/about&quot; class=&quot;dark&quot; target=&quot;_self&quot;&gt;关于&lt;/a&gt;
                 
                &lt;a class=&quot;go-mobile&quot; href=&quot;javascript:;&quot;&gt;移动版&lt;/a&gt;
                  &lt;span class=&quot;snow&quot;&gt;·&lt;/span&gt;  
                &lt;a tar=&quot;&quot; href=&quot;https://www.sov5.cn&quot; class=&quot;dark&quot; target=&quot;_blank&quot;&gt;SoV5搜索&lt;/a&gt;
                  &lt;span class=&quot;snow&quot;&gt;·&lt;/span&gt;  
                &lt;a tar=&quot;&quot; href=&quot;http://www.python88.com&quot; class=&quot;dark&quot; target=&quot;_blank&quot;&gt;Python社区&lt;/a&gt;
                  &lt;span class=&quot;snow&quot;&gt;·&lt;/span&gt;  
                &lt;a href=&quot;http://link.sov5.cn/&quot; class=&quot;dark&quot; target=&quot;_blank&quot;&gt;Link管理&lt;/a&gt;
                  &lt;span class=&quot;snow&quot;&gt;·&lt;/span&gt;  
                &lt;a href=&quot;http://www.3daima.com/&quot; class=&quot;dark&quot; target=&quot;_blank&quot;&gt;三行代码&lt;/a&gt;
            &lt;/strong&gt;

            
            今天看啥 - 让阅读更高品质

            
            &lt;span class=&quot;f12 gray&quot;&gt;
                &lt;a href=&quot;http://www.miibeian.gov.cn/&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;© 2019 ~ 沪ICP备11025650号&lt;/a&gt;&lt;/span&gt;
            
        </description>
<pubDate>Wed, 25 Mar 2020 05:05:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/yl7WYKt9Em</dc:identifier>
</item>
<item>
<title>强化学习书籍与课程推荐</title>
<link>http://www.jintiankansha.me/t/t3mDMe8GmF</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/t3mDMe8GmF</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;人工智能有关决策的核心是强化学习。让机器来决策，首先体现在如何模仿人类的决策。对于决策这个问题， 对于人类是困难的， 对于机器就更难。而强化学习， 就是一套如何学习决策的方法论。当下的强化学习逐步过渡到深度强化学习主导的时代， 从打游戏，逐步向机器人，无人驾驶等领域扩散。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而强化学习的入门门槛却比其它人工智能领域要高，其中一个原因是强化学习课程不像深度学习和机器学习一样琳琅满目， 铁哥在此给大家推荐几个经典的强化学习课程和书籍。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;经典书籍：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;由于这是一个80年代才发展起来的新兴学科，其经典著作也非常稀少。如果硬要推一本， 那首推Rechard Sutton的强化学习经典书籍， 首先Rechard老爷子是强化学习的发明人， 这也就为Reinforcement Learning: An Introduction. 这本书从强化学习是什么开始引入，然后把内容分为表格化方法（Tabular method）, 近似方法（Apporximative method） 和 强化学习前沿（主要讲解强化学习和心理学的关系）三个方面。而Tabular method又是其中最大的一块。所谓的Tabular方法，指的是在任务的所有状态都已知的情况下，强化学习问题等同为一个把每个状态的未来收益都列出来的状态表， 唯一要做的就是把表里的每个数值都通过各种不同的经历都列出来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个写法的根基在于Sutton老人家的得意之作在于TD学习的方法， 这种利用迭代方法boostrap对未来收益的估计的方法特别好体现在表格方法里，后面的TD-lambda等高阶的TD方法也在这里一并讲出。表格化方法还有一个优点是容易引入动态规划（dynamic programming）的整体框架， 这是人工智能领域比较少见的非常优雅成系统的数学方法。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而本书毕竟成书较早， 虽然理论功底十分扎实， 但是对强化学习当下的应用， 尤其是深度强化学习， 涉猎较少， 对于学以致用的中国学生显然无法满足要求。因此这本书只建议作为基础入门之选，可以让你从根上熟悉强化学习理论是怎么一步步生长出来的。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;以下推荐经典课程：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， Deepmind 强化学习课程系列&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Deepmind的课程高度尊重了Sutton书籍的经典性， 可谓是其书籍的现代版。这个课程偏向强化学习的理论框架搭建，应该说比较好的延续了Rechard Sutton偏向于值学习为基础的框架，甚至可以看作Sutton书籍的视频版 。本书从强化学习的基本概念入手，引入值函数， 引入TD学习 和蒙特卡洛抽样学习逐层引入强化学习的基本概念，因此课程的理论框架偏向于值函数学习这一套， 这也是和Sutton最初的框架一脉相承。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Deepmind采用这样的课程体系也不难理解， 因为Deepmind 在强化学习的第一桶金深度强化学习攻克Atari game就是DQN（深度Q学习）的巨大功劳， 值学习和值函数学习在状态给定清晰的游戏里也具有最高的效率。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;课程优点是理论框架和数学&lt;/span&gt;推导&lt;span&gt;极为扎实，而且讲解和比喻非常清晰有趣， 缺点是实践性内容比较依然稀缺， 当然你可以去Deepmind的网页查找论文和对应的github。&lt;/span&gt;&lt;span&gt;DeepMind课程在Youtube上可以找到两个不同的品种，一个是David Silver亲自讲的，一个是更年轻的教授的较新版本，两个课程事实上非常相似。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Deepmind的课程缺少实践的内容本身也和其公司注重打游戏而非把算法用在工业实践有关， 而另一个课程则可以看成极好的对其补充。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2，Berkeley· deep RL 285&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个课程较少为人所知， 但是确是一个真正的干货满满的深度强化学习课程。它的内容一开始就跳开了Sutton老爷子的动态规划框架，而是从一个更接地气的角度模仿学习入手。一开始学生就可以接触到自动驾驶这样的非常实际的问题， 这和Berkeley在机器人领域功底深厚密不可分。因此如果你喜欢研究深度强化学习的偏实践问题， 那么我强烈推荐这个课程。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从理论上看有意思的是， 它的介绍角度偏向于policy gradient作为框架的基础。课程一开始以模仿学习引入， 然后指出模仿学习的不足， 无法轻易的泛化到人类经验之外，从而开始引入学习策略和策略梯度，以及最核心的Actor-Critic算法。而在所有其它教材里浓墨重彩的值学习方法， 却仅仅是作为Actor-Critic一个去掉Actor的特例讲解。这与工业控制领域更多依靠策略梯度而很难应用值函数法是密切相关的（因为工业领域一般面对较为连续的动作空间，比如机器手的移动）。同时这个课程最大的优势在于引入了元强化学习，分布式强化学习， 多任务学习， 生成式模型等领域前沿概念，仅仅看课表就可以感受到其内容之新颖。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个课程的主讲人是Sergey Levine， 也是这个领域的前沿研究者。&lt;/p&gt;
&lt;img data-ratio=&quot;0.43333333333333335&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceTw4J0RSWJyJb30T8XQw5OZzgoSZibmZco7zjzCZpApPf3kDwDfDCAlMyGOIsSiahLJfGp65ywcGOA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1305&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;作为总结补上强化学习的学科地图。我们看到解决一般性的强化学习有两大不同的流派，一个叫做策略优化（左），旨在利用策略梯度直接优化行为，得到最后的奖励。一个叫做动态规划（右）， 旨在通过假定存在一个马尔可夫状态链，迭代式的求解每个状态下的未来收益，侧重先评估再改进行为。Deepmind的书籍课可以看作从右向左的过程， 而伯克利的课程则是从左向右。经典的理论注重从右向左， 当下强化学习实践注重从左向右， 此处就是其关键所在。&lt;/p&gt;
&lt;img data-ratio=&quot;0.37777777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceTw4J0RSWJyJb30T8XQw5OqbSvA8YZG6p1BqicrF3CfCK093iaxNYdXL5IEJicj192fghBvEu7xn7Bw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;720&quot;/&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;最后分享一个铁哥3月30号的 live 讲座 ICLR论文看脑科学如何助力人工智能：&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span data-text=&quot;true&quot;&gt;ICLR论文看脑科学如何助力人工智能&lt;/span&gt;&lt;span&gt;www.zhihu.com&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;img data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceTw4J0RSWJyJb30T8XQw5OgeSOJPWkos0l0lZBAKGMMtTkFq8YRJ7juo9W6iaBpvXic4xlYQZxwOKg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;100&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从中你可以了解如何用强化学习构建一个适应各种不同环境任务的导航系统，制造一个“聪明”的人工小鼠。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;

&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651385192&amp;amp;idx=2&amp;amp;sn=40df3b79609f6abe54b60e62148b1c9d&amp;amp;chksm=84f3c329b3844a3fff1c767d53a9934741df23b0b3b740313e09932f364a2fbd98dfd8db1a4b&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;铁哥知乎live讲座-从导航看AI的未来&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384652&amp;amp;idx=1&amp;amp;sn=6f9d524b432673822e4c5da4e188522e&amp;amp;chksm=84f3c50db3844c1bb4f6a29b280c517467933c203a4fbeec529f229173e311431944012adc4e&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;站在AI与神经科学交叉点上的强化学习&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384881&amp;amp;idx=1&amp;amp;sn=0faf30bbeb5e4f7b386c9ba08aab8ebb&amp;amp;chksm=84f3c270b3844b6636333248db8e75e3e348517bed5ba3ad53851f78e5dd9317a186294a499e&amp;amp;scene=21#wechat_redirect&quot; data-itemshowtype=&quot;0&quot; tab=&quot;innerlink&quot; data-linktype=&quot;2&quot;&gt;多巴胺引领下的分布式强化学习&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 24 Mar 2020 03:28:57 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/t3mDMe8GmF</dc:identifier>
</item>
<item>
<title>铁哥知乎live讲座-从导航看AI的未来</title>
<link>http://www.jintiankansha.me/t/1YZVcrBmzb</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/1YZVcrBmzb</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;铁哥3月30号要通过知乎live给大家讲解一个有趣的故事。&lt;/span&gt;故事的起因，是当下的深度学习日子越来越不好过，自动驾驶，智能对话都在陷入一种人工智能不智能的怪圈， 图像识别被加入的微小噪声糊弄，自然语言处理照猫画虎不理解语言，强化学习海量数据学习条件反射。统统这些问题的背后，是我们没有理解我们自身大脑智能的本质，因此无法把这种智慧运用到AI中。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所以讲座的第一个部分就是对比人工智能和人类智能两大系统寻找两者的区别和联系，再此知道人工智能当下弱在哪。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwR5Yu4jtVZIug2kric2DlcWTVWQJPrDiaZCQoJICH30fEoXNFXwPSuhTRg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;既然是这样，有些人可能开始觉得幸灾乐祸，看来让机器打败人类的时间尚早，我的工作也一时无法被取代，但是，作为一个人工智能研究者你一定不满意，那么这一段就开始寻找和探讨一些可能的缩小人工智能和生物智能系统差距的方法。不同的学科人士想法其实众说纷纭：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRf9QE2yOdMGsLhLFfFiaxqM8QMYvxTcYk8LsmUTh7IrLbkeV2zPEzWGA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;此后我们会聚焦在仿生学派（因为我就是这一派），这里的关键是如何理解生物智能到底是怎么来的。然而我要用的是物理学家的方法而不是生物方法。因为大脑是个复杂动力学系统。何为动力学系统，事实上大脑不像电脑 ，拔掉电源就不再工作，仅有输入来有一个输出。而是一个不停演化的动力系统 。最简单的动力学系统是一个单摆系统，有简单的动力学方程确定。那么被牛顿爱因斯坦使用的物理思维和模型就可以用来撸人工智能的羊毛了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRegsmvk3r19zS65qdslxibdpbHqoN79jcR29AOMV96IQhQ7SROGXFfibQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;讲座的下一部分聚焦在如何研究脑内的动力学，如果用生物学一个个分解研究的思维是很困难的，那么一个很好的方法是以毒攻毒，以复杂攻克复杂，我们使用人工神经网络，来拟合脑，但目的不是拟合，而是找到脑功能动力学的关键。这个机器学习工具是什么呢?  他就是RNN。对RNN的理解可以分为三个层次：机器学习，动力学和图灵机。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRGfV6sOrvdr9pMy3Z1nrJykFRvXFASkh1E8JicSytpTYLbAI90gD0TaA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;讲座的第三部分就是最精华的部分，铁哥新晋顶会（ICLR）论文为例，说明如何把生物智能的智慧通过RNN得到阐述， 再用来改善我们的人工智能。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个任务就是导航 。没错，就是解决你在商场总迷路的问题的哪个导航，自动驾驶的关键导航，而且是不依靠GPS的导航。 也许有人立刻想到了自动制图和定位SLAM。但是文章的后面将告诉你地图不是故事的全部，因为作为生物 ，导航有时候需要的其实仅仅是一系列头脑中的行为指令，而不需要任何关于地理位置的概念，简称行为主义的智能法则。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRCeWgR3x2UyG0RFc8pRwia9ncHnIiaLibNok9p18hnd9KCk0JlGq0FfXEg/640?wx_fmt=png&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; block=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; visible=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRaS9vrtUU5OZK0zJACxmGDcfIiccOfk2jMTHeHzupickUa2ghnZia72Xiag/640?wx_fmt=png&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; block=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; visible=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;故事最精彩的地方在于如何告诉你通过一个RNN循环神经网络来把这两种不同的生物的智慧放到一个人工智能系统里，告诉你不仅可以通过智能地图，还可以通过行为流派的方法来完善导航能力。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRHOSjXgPsXw7THXKt4ePMQicJk3HmPKl2UkJUTaibpbP3Fj0CDiaNDFZLA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;



&lt;p&gt;&lt;span&gt;而且与主流的端到端深度学习不同，文中模拟生物进化和发育的历程，得到如同生物多样性的RNN们，如同不同能力的小鼠，比如最典型的地图鼠，随机鼠，和行为流派鼠（对应刚刚的不同生物智慧）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRxanqxkLPmQxAn32KFkFpUSGgrDlxvXgXCP29z7eJpibW9rnbTqexeqg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;



&lt;p&gt;&lt;span&gt;为这些不同的“小鼠”我们建立多姿多彩的任务空间，体现我们考虑的是多任务而非单任务学习能力，也更加全面的考察不同小鼠的能力，如同IQ测验。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRY56jsQlkzkEhCyvibTznR3HDgcVoUHgy4GjB3qXA2TyjX8aBABU4xkA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;最终通过强化学习，我们会得到不同的小鼠对于同样的任务，也喜欢使用不同的策略，而这导致泛化能力千差万别。有的喜欢使用地图，有的擅长用行为流派的方法来做事，虽然它们的动作有时看起来比较机械。有意思的是这些看似机械的行为流派小鼠并非一无是处，在某一类任务上，它们大大优于掌握地图的小鼠。 &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5625&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc1XgXu8jfRiawWM4cIqPfwRASGP8XNWxASibYgMgU6LofUs3iaPU6tic3uDtfhpJJw5wU5XYAXSeJ9Xg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;故事的最终我们将研究这些策略背后的不同神经表示。人工小鼠背后的核心概念RNN及其所依赖的动力系统。而一个更加神秘陌生的概念，RNN的吸引子成为这一部分故事的关键。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;故事的最终我们用一个巧妙的方法如何把不同能力的小鼠结合在一起，从而构建一个模块化的超级小鼠，灵活在不同策略直接切换，是真正的全能鼠。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;故事的最终我们将展望脑科学和计算神经科学在人工智能方面的更多潜能。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然这一个小文只是抛砖引玉，要想真正理解文中故事的来龙去脉，3月30，铁哥与你在知乎Live不见不散。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;https://www.zhihu.com/lives/1220364052704129024&lt;/p&gt;

</description>
<pubDate>Mon, 23 Mar 2020 02:20:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/1YZVcrBmzb</dc:identifier>
</item>
</channel>
</rss>