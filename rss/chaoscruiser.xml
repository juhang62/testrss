<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>当深度学习握手脑科学-圣城会议归来</title>
<link>http://www.jintiankansha.me/t/c4o5askVfm</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/c4o5askVfm</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;6760a-0-0&quot;&gt;耶路撒冷号称三教圣地， 而它的牛逼之处绝不仅在于宗教， 如果你深入了解， 你会发现它的科学，尤其是理论创新也同样牛逼， 尤其是在脑科学和人工智能方向。 当然神族人不是特别关心最接地气的问题， 而是更关注形而上的理论框架。  耶路撒冷的脑与深度学习会就是这样一个杰出的体现。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b298c-0-0&quot;&gt;深度学习有关的核心会议， 从NIPS到ICLR 我们都不会陌生， 这些会议对深度学习在人工智能的应用极为相关。 耶路撒冷的这个会议与之不同的是， 它非常关注深度学习与脑的交叉领域， 关注它们背后共同的指导理论， 在这点上也算是独树一帜。&lt;/span&gt; &lt;span data-offset-key=&quot;b298c-0-1&quot;&gt;因为在大家忙于做应用主题的时候， 其实更需要有一些人其思考背后的理论，即使这样的思考在一个时间里不会马上促进应用， 但是在更长远的时间里， 却可能把应用推向一个远高于现在的平衡点。&lt;/span&gt; &lt;span data-offset-key=&quot;b298c-0-2&quot;&gt;就像人类在了解牛顿定律以前就能够建造各种各样的桥梁。有人可能会说我们不需要牛顿定律， 而实际上他们没有看到我们有了牛顿定律后所造的桥根本不是一种桥， 不是石拱桥，或者独木桥，而是跨海大桥。 好了，我们直接来说正事， 来总结下会议里一些有趣的内容。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c18r0-0-0&quot;&gt;脑与深度学习的关系本来就是一个高度双向的主题， 这个会议围绕以下几个核心问题：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;5kv4m-0-0&quot;&gt;1 深度学习的基础理论， 深度学习为何work又为何不work？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;acvn6-0-0&quot;&gt;2 如何从心理学和认知科学的角度归纳当下深度学习的不足？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;einic-0-0&quot;&gt;3 如何用深度学习促进对人脑的理解，包含感知（视觉为主）， 认知与记忆。 反过来如何促进AI？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e7ev5-0-0&quot;&gt;会议最大的一个板块， 在于对深度学习理论的剖析， 这个板块可谓大牛云集， 从信息瓶颈理论的创始人Tshiby 到 MIT的 Tomaso Poggio，   从牛津的Andrew Saxe到MIT的Daniel Lee， 都表达了自己的核心观点， 问题围绕的一个主线就是深度学习的泛化能力 。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;36nnk-0-0&quot;&gt;我们把这个问题分成两个子问题：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;cr9s2-0-0&quot;&gt;深度学习的泛化能力为什么那么好？&lt;/span&gt; &lt;span data-offset-key=&quot;cr9s2-0-1&quot;&gt;大家知道深度学习理论的第一个谜团就是一个大的网络动辄百万参数， 而能够泛化的如此之好, 这是非常不符合贝卡母剃刀原理的（解决同样的问题简单的模型更好），更加作妖的是， 这种泛化能力往往随着参数的继续增加而增强。 这到底是为什么?    几个不同的流派从不同的角度回答了这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vbcr-0-0&quot;&gt;1， 信息流派：&lt;/span&gt; &lt;span data-offset-key=&quot;vbcr-0-1&quot;&gt; 从信息论的角度分析深度学习， Tshiby是该流派的集大成者，也是此次的发言者。 他的核心观点是从把深度网络理解为一个信息管道， 数据， 就是入口的原油 ，里面富集了我们可以预测未来的信息， 那么这个深度网络， 就是首先要把输入数据里那些相关性最高的成分给把握住， 然后再一步步的把我们与预测信息无关的东西给剔除， 最后得到一个与预测对象而非输入数据极为相关的表征。 深度学习的泛化能力， 在于层数越深， 这种对无关信息的抽离的效率就越高， 因为随机梯度下降的训练过程， 每层的网络权重都在做一个随机游走， 越高的层 ，就越容易忘记那些与预测无关的特征， 层数越多， 这个过程其实就越快，我们能够在控制梯度消失的同时拥有更多的层， 会使我们越快的发现那个与预测相关的不变的特征本质。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHibvVQ6ufSAgBdap1gJJYuZibv8u59MexXCib8PKqoapJAtpHG6LqHwrKQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;信息瓶颈理论， 深度网络作为信息抽取的管道。&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;4bd1p-0-0&quot;&gt;2， 几何流派：&lt;/span&gt; &lt;span data-offset-key=&quot;4bd1p-0-1&quot;&gt; 这是Daniel D Lee 的talk 。从Manifold learning的角度理解 ， 深度学习的“类&quot; 对应一个在高维空间里得到一个低维流形，。这一个高， 一个低， 就是深度能力泛化能力的源泉。 这个观点的核心起源可以追溯到SVM的max margin solution。 在SVM的世界， 首先我们可以用增加维度的方法把两堆在低维世界混合分不开的点投影到高维空间， 它们就清楚的分割开来。 然后我们用最大间隔来做限制，让这两堆点分的尽可能开， 就可以避免过拟合。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4bd1p-0-1&quot;&gt;这个做法的本质首先用维度增加增强模型的容量， 然后在模型有了更高容量后我们当然也更容易过拟合。但是我们可以用最大间隔尽可能把数据”打“到一起， 事实上让每个类数据分布的维度尽可能低，这就可以避免过拟合。在深度学习的世界里， 我们每层网络都把之前的数据映射到一个新的流型里， 最简单的假设就是一个球体。比如猫和狗的分类， 就是两个球体， 一个猫星， 一个狗星。 在一个同样的高维空间里， 这两个球的维度越小， 半径越小， 就越容易把它们分开，而且可以分的类越多。  随着深度网络的层数变深， 这个趋势恰恰是每个球的维度越低，半径越小。 如果不同类型的图像对应不同的球，层数越深， 就越容易给它们分开。这个观点的内在事实上和Tshiby的信息瓶颈有异曲同工处， 大家体会下， 那个小球的维度越低是不是在抓取数据里的不变性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHlotz92qJfLtTVBgXq2iah0yn0ww38w5SwUNnaiajQqicOkbhIYyMpyqrw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;几何学派， 猫星和狗星的分离&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHJ1m2vvrv6wz2YUZg0ojdV1EM8wic1XVicKiacEJ4icNUrAw8y7X2ib74auQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHiaPE0uG8SviaXTEyI5oJBck5QdW3qQtkiabWqYiayTX7S1c3ia9SXIopoCA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;几何流派， 高维空间的低维流型随着层数变深的变化&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6topo-0-0&quot;&gt;3， 动力学流派 ：&lt;/span&gt; &lt;span data-offset-key=&quot;6topo-0-1&quot;&gt;高维空间非线性优化的本质是这种优化随着维度增长效率增加。 这是牛津那位仁兄Andrew  Saxe的talk 。 牛津例来是深度学习的阵地， 理论当然当仁不让。 这个talk从非线性优化的角度揭示了深度学习泛化的本质。  网络训练的过程， 事实上是高维空间上一个寻找动力学定点（全局最优）的过程， 每时每刻，梯度下降的方向是由当下x和y的相关性和x和x的自相关性决定的。 当优化进行到定点（最优点）附近时候， 这个相关性信息开始减少， 网络开始对数据里的噪声敏感，  因此我们需要早停法来减少过拟合。 但是， 如果我们的网络足够大，甚至这个早停都不必要我们无需提防这种拟合噪声带来的过拟合。 取得这个结论需要非常复杂的线性代数， 同学们可以参考论文&lt;/span&gt;&lt;span data-offset-key=&quot;6topo-0-2&quot;&gt;High-dimensional dynamics of generalization error in neural network&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f7tf0-0-0&quot;&gt;会议的另一个部分talk，围绕深度学习的泛化能力为何如此之差， 这不是互相矛盾吗？此泛化非彼泛化也。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b8vb-0-0&quot;&gt;1，  先天的偏见与推理的无知&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7af77-0-0&quot;&gt;先验误差导致的失灵:  &lt;/span&gt;&lt;span data-offset-key=&quot;7af77-0-1&quot;&gt;希伯来大学的Shai Shalev  深度网络可以战胜围棋这样牛逼的游戏， 然而你想不想的到， 它可能在学习乘法表的时候都会出错？  这个talk讲解了让深度网络学习并泛化一个乘法表， 然后看在测试集上它是怎么表现得。 非常有趣的是 ，虽然深度网络在训练集上表现完美， 在测试集上出现了让人耻笑的系统误差， 说明它还真的不如一个小孩子的学习能力。  这突出了反应了深度统计学习依然无法绕过统计学习固有的缺陷， 就是缺少真正的推理能力。 而这种系统误差背后的原因， 是网络内在的inductive bias， 这就好像网络自己就带着某种先天的偏见， 我们却对它茫然无知。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHyT9nICG23D5H9JLZGbaHCFWqSv0lhWmIUpIFqUnvrtVAeBDZdnhaCA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;深度学习学乘法出现的难以忍受的系统误差&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;buboe-0-0&quot;&gt;另一个惊人的talk来自于Montreal University的Anron Courville。 他围绕一个深度学习的当红应用领域&lt;/span&gt;&lt;span data-offset-key=&quot;buboe-0-1&quot;&gt;VAQ -视觉看图回答问题&lt;/span&gt;&lt;span data-offset-key=&quot;buboe-0-2&quot;&gt;展开。 这个框架的核心在于让深度网络看图， 回答一个有关图像的问题， 比如图像里有几把桌子几把椅子这种。 我们关键考察那些需要一点推理能力才能回答的问题，  比如回答完了图像里有几个桌子，有什么颜色的椅子后， 问它图像里有什么颜色的桌子。 如果这个网络真的有泛化能力， 它就会回答这个问题。 事实上是我们所设计的超复杂的由CNN和LSTM组成的巨型网络在这个问题面前举步维艰。 它可以找到3张桌子或5张桌子， 但是很难把什么颜色的椅子里学到的东西迁移到桌子里正确回答出灰色的桌子。之后我们从工程学的原理设计了一个全新的结构把这种推理能力人为的迁移进去， 会使问题稍稍好转。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggH9K9c1gAS4dzQJ3TSyhibUXrG7LLZTRHInpT0uXLk8M1BOBibjNJct6Gw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;视觉看图回答问题&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a1a8m-0-0&quot;&gt;2， 你不知道的CNN那些缺陷：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egj4s-0-0&quot;&gt;1 CNN真是平移不变的吗？&lt;/span&gt; &lt;span data-offset-key=&quot;egj4s-0-1&quot;&gt; Yair Weiss  希伯来大学计算机系的Dean给大家讲解了CNN网络最大的根据-平移不变性是错误的。 我们知道CNN网络建立的基础是它模仿生物感受野的原理，建立了一个共享权值的网络系统 ，这样不同位置的图像部分， 会共享同一种特征偏好， 你的鼻子出现在图像的顶端或下面都是鼻子。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;84nbd-0-0&quot;&gt;而Yair Weiss却想了一个方法， 证实了CNN， 哪怕你把图像向上移动了一个像素， 都可能造成它整个看法（分类）的变化。 这和那个在动物脸上加噪声看成其它动物的实验类似， 证明了CNN的脆弱性，同时动摇了平移不变的基础。 一开始我也觉得是天方夜谭， 但是看了他的整个试验后开始稍稍信服。 事实上它证实了对于最早期的CNN-neocognitron ， 平移不变的确是成立的。 但是对于”现代“CNN， Alexnet， VGG， ResNet， 这个性质却不再成立。 因为现代CNN在整个网络结构里，加入了大量的降采样，比如池化， 这些在空间上离散的降采样操作， 导致了一种惊人的脆弱性，就是平移不变的丧失。 当然， 在实际应用中， 它不够成那么大的问题， 因为你永远可以通过数据增强的方法， 来强化网络里的这些不变性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHGdibGlwIra1qJxRmsLWMMbKQVK2l5QpAATYdcMe3YonklibDhGdBIVzg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN居然不是平移不变的&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0dss-0-0&quot;&gt;2，CNN对细节的敏感与对轮廓的忽视。&lt;/span&gt; &lt;span data-offset-key=&quot;a0dss-0-1&quot;&gt; 我们本来相信CNN对不同尺度的图像特征，从细节纹理到图像轮廓， 都会同样器重并做出判断。 而事实上， 来自德国Tubingen的Matthias Bethe, 给我们展示了CNN事实很可能把自己90%的判断依据，放在了细节和纹理上。 也就是说， 它也许可能精确的识别狗和猫，但是它或许真正基于的是狗毛和猫毛的区别做出的判断。 如果你联想一下那么在图像里加入噪声， CNN就可以把熊猫看成长颈鹿的实验， 就觉得这个想法还挺合理的。  它通过它的实验验证了它的这个理论。也就是用那套图像特征迁移的网络， 把一个个图片的纹理抽取， 或者更换掉， 虽然还是猫或者狗， 里面的纹理变了， 那个CNN就彻底傻掉了。 同时它还对比了人的认知测试，看到了CNN的巨大差距。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHFLk2jgwm7orxXGCyKo3QmXHw7Yuibia8lnudYYyUibJibb6TSGn7MJl8SQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN难道只对细节感兴趣？&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bqh4d-0-0&quot;&gt;以上这些研究都暴露了CNN和人脑的区别。 即使是图像识别这个目前AI做的最好的领域， 这个”人工智能“ 也显得太”人工“ 了， 而与”智能“差距甚远。&lt;/span&gt;&lt;span data-offset-key=&quot;bqh4d-0-1&quot;&gt; 当然Matthias通过强化对轮廓的训练识别， 可以让它变得更像人一点， 可以识别一定的整体特征， 然而这个时候对总体数据集的识别度会变得更差。&lt;/span&gt;到这里，可以说是从深度学习多么好，到了深度学习多么差， 我们毕竟还没有掌握智能最核心的东西，包括符号推理这些， 也没有具备真正的”泛化能力“ ，  此处之后的几个talk，就是围绕这个智能的真正核心，探讨人脑有多牛逼了。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;914ei-0-0&quot;&gt;脑科学与心理学角度的智能：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b9ge6-0-0&quot;&gt;1， 有关表征学习:  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;t196-0-0&quot;&gt;来自Princeton的Yael Niv讲解了智能科学的核心-表征学习的几个关键问题：首先什么是表征学习， 表征学习的本质概念是学习一个真实世界的神经表示。它可能是从真实世界抽离出来的一些核心特征， 或者我们说的对真实世界的抽象， 而这里面，却可以帮助我们大大增强我们举一反三的学习能力。 比如说你被蛇咬了， 下一次出现运动的细长生物你知道避开。 另一方面， 我们可以把任务根据当下情景在大脑中重构出来， 比如都是讨价还价， 你碰到辣妹可能就没有那么用力了，而是开始谈笑风声起来。我们可以把从相似的任务里学到的经验整合， 或者同一个经验里学到的东西和不同的新的任务结合。   &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;t196-0-0&quot;&gt;这些都依赖于我们大脑中一套灵活的对不同任务和事物的表征系统。 这个系统我们可以管它叫任务表征系统。Yael 讲了这个任务表征系统的一些基本特性， 比如说贝叶斯证据整合，证据如何互相关联和启发（召唤）， 并把这些研究和大量心理学测试联系在一起。  这种对任务的极强的迁移学习能力， 可以从一个任务中的经验，关联到一大堆任务的能力， 是得到更好的泛化能力， 甚至走向通用人工智能的一个关键步骤。 如何能够通过学习得到&lt;/span&gt;&lt;span data-offset-key=&quot;t196-0-1&quot;&gt;这种可以迁移的任务表征&lt;/span&gt;&lt;span data-offset-key=&quot;t196-0-2&quot;&gt;也将成为重中之重。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHPpOXgnXAxicz97Y86ziaMXaSgzFrmvEIWr6W7KibBbFKBImjSGCn6FDHw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;表征学习-智能的核心&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2oc25-0-0&quot;&gt;2，  有关人类记忆的研究：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9vlfk-0-0&quot;&gt;来自哈佛医学院的Anna Schapiro  讲解了海马记忆的两个根本机制。 我们知道， 海马是人和小鼠短期记忆， 情景记忆的载体。 在海马体内有两种不同的记忆模式。 一个事短期的快速的记忆， 每个记忆由相互独立的神经元基团表达， 另一种是长期的稳固的， 某几个记忆根据它们的共性共享大量的神经元基团。 在夜晚睡眠的时候， 我们白天记住的东西一部分会从短期转向长期，另一部分则会被遗忘。 有意思的是 ， 谁会被遗忘， 谁会被增强呢？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9vlfk-0-0&quot;&gt;事实上Anna的研究表明人脑有一种非常灵活的机制， 可以把那些重要的记忆筛选出来，从短期区域走向长期区域， 而一些不重要的就像被水冲过一样遗忘掉。 这个机理可以由海马体的一个网络动力模型理解。   同时这个研究还一定程度解开人类神经编码的方式。 那些长期记住的事物为什么要共享神经元基团？ 这是为了更有效的泛化， 一些类似的事物，或任务，通过共享神经元， 可以更好的提取共性， 预测和它们类似的东西。 反过来这也表明我们大脑内的记忆很多可能是错误的， 类似的东西之间会”相互污染“  ，这就是我们为什么经常会记混或记串。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHxibOQKOELGGicNwp9icTKzINUNibn7xfUT6Op0iaxz1tia2AUHIVYl6VuibzA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;两种记忆承载的模式， 一种很独立， 一种有重合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHH0DdfziaUiaKrm0wJy2nq1gXhsj5eCvso5cwclskNjx8XZR3sfVV8agg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;海马模型&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8i75-0-0&quot;&gt;最后一个模块，就是围绕人脑和深度学习的关系， 虽然我们的最终梦想是把让人脑牛逼的算法迁移到AI系统， 但是第一步最容易实现的恰好是反过来， 如何借助深度学习这个崛起的工具更好的挖掘人脑的原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bmt4v-0-0&quot;&gt;对于这块，来自斯坦福的Daniel L K Yamins 提出了一个非常酷的研究框架， 就是用reverse eigeerneering（逆向工程， 正是我导师的领域） 研究人脑的感知系统（视觉或听觉皮层）。 对人脑视觉或听觉回路进行建模是我们一直的梦想 ，整个计算神经科学， 围绕如何用数学建模来理解这些现象 ，建立实验数据之间的联系。然而建立这样的模型异常复杂， 需要考虑的生物细节极为繁琐。 现在， 深度学习的网络给我们提供了极佳的工具去理解这些现象。我们的一个想法是用这些深度学习模型去学习具体任务，等到它学会了我们再想法来理解它。 那我说你不还是搞一些toy model 给我吗？ 谁信？ 没关系， 不是有实验数据吗， 我们先让它能做任务， 再用它来拟合我们的实验数据， 比如你先训练一个CNN来做图像识别， 同时训练好后， 你想法让这个CNN里的神经元活动能够匹配从大脑视觉皮层得到的实验数据， 这样你就得到“生物版” CNN。为了确定它是一个真正的科学， 而不是一种“形似”的骗术， 我们会用这个生物版本的CNN提出一些新的现象预测， 可以拿回到实验检验， 如果真的成立了， 这个用深度学习“构建出来”的模型， 就可以得到一个我们目前阶段最接近真实生物系统的模型。 你可以理解我们做了一个机器猫，它不仅能够捉老鼠，而且各项生理指标也和真猫差不多。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHIWgL4Pm9FEemgcJlAx4zGcNqLo1Qibz39OlTHmp8PJX0ruPbOOibICgQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;让深度网络和动物看同样的图像，并把它们的内部活动联系起来！&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3mrkj-0-0&quot;&gt;具体可以见Nature论文Using goal-driven deep learning models to understand sensory cortex。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c8gdu-0-0&quot;&gt;这一类的工作还有一个talk是如何构建一个CNN网络理解人类的视网膜系统，同样的，这个网络既有视觉信息的处理能力， 同时还能够描述生物的神经活动， 甚至可以预测一些生物视网膜特有的现象（如对未来运动信息的预测）。这一类工作可以说打通了生物与工程， 虽然人工神经网络无论在单个神经元还是在功能层面和神经元活动层面都获取了类似于真实生物系统的特性， 我们又有多大可能认为我们用这个方法理解大脑的真正工作机理， 这依然是一个仁者见仁 ，智能见智的过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4gvnv-0-0&quot;&gt;&lt;strong&gt;最后， 关于所有人的梦想， 把大脑的牛逼算法迁移到AI， 有一个talk颇有启发。 它来自于斯坦福的Surya Ganguli&lt;/strong&gt;，&lt;/span&gt;&lt;span data-offset-key=&quot;dgb9o-0-0&quot;&gt;如何让深度网络生成语义结构：&lt;/span&gt; &lt;span data-offset-key=&quot;dgb9o-0-1&quot;&gt;一个AI最根本的问题是如何沟通统计主义， 连接主义和符号主义的人工智能， 统计机器学习与深度学习代表了前两者的巅峰， 而早期活跃的符号主义目前只保留了知识图谱这样的果实残留。 事实上， 如果不能让符号主义的思维重新以某个方式进入到深度学习， 真正的AI将很难到来。 而这个方向的第一个步骤就是如何得到语义结构的神经表示。 人类的语言，可以用几千个单词表达十万百万的事物， 由于组合规则和树结构。那些共用特征的概念会被放在一个树枝之上，  而另一些则会放在其它树枝上。 这种特征层级结构， 使得人类的概念学习极为有效率， 只要直接把一个新概念放到它应该在的枝桠上， 有些该有的就都有了。 那么， 基于统计和连接主义的神经网络可不可以再现这种树结构呢？Ganguli 的研究给这个方向提示了可能， 它把学习和非线性系统在高维空间的运动联系起来，训练，就是不同的概念根据其间相似度互相分离的过程。  通过分叉等结构， 把概念的树结构和动力学空间联系在了一起。 详情请见论文： A mathematical theory of semantic development.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.41833333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHYWAp5zVM8kUDibZeiaz6Md0f8bQBibKL98OZyvS1DTlcTyg9vric4WyQpw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A mathematical theory of semantic development deep neural networks。 学习过程里的概念分离&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;这个会议， 可以说对于深度学习和脑科学未来的发展， 意义都非常深刻。 我看到的是， 尽管人们都怀揣着统一两个领域的梦想， 但现实的差距还非常遥远， 双方的沟通依然艰难。而这也更突出了这类会议的难能可贵。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 02 Feb 2019 07:35:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/c4o5askVfm</dc:identifier>
</item>
<item>
<title>机器学习中的8种常见疏忽以及对应的认知偏差</title>
<link>http://www.jintiankansha.me/t/R4lCgiTS99</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/R4lCgiTS99</guid>
<description>&lt;p&gt;随着机器学习工具的“傻瓜化”，越来越多的人在标准数据集上训练出了效果还不错的分类器，但这并不意味着到了真实的应用场景下，可以从头到尾做完一个项目，真实的环境中会出现数据太小，数据的特征不明晰，以及大量的缺失值等很多在标准数据集下预先不到的问题。针对具体的项目，需要了解行业的背景知识，和本领域的专家当面聊，从而有针对性的做出调整。但总有一些共通的要避免的坑，而这些坑则和我们个人的认知偏差也是有对应关系的。（本文主要针对监督学习）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 采样偏差&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要做训练，先要有训练数据。而数据集的选择中，如果取样有偏，那就会导致你训练出的模型到了真实的环境中无法泛化。比如你的观测仪器会不会本身没有校准，你判定是否为合适样本的规则是否会带来某类数据无法被采用。你可以通过检查你数据的分布，是否符合预期即实际情况，来判定是否采样有偏。但做项目之前，首先&lt;strong&gt;要对什么是理想的数据集要有一个清晰准确全面的定义&lt;/strong&gt;，不然即使你以为你是随机的选取数据，你也无法避免采样偏差。&lt;/p&gt;

&lt;p&gt;采样偏差来自于我们大脑与生俱来的选择性偏差（Selection Bias），比如你会表现的比实际情况展示的更加乐观，你会忽略那些和你观点不一致的证据等。在日常生活中，你无法预先设定什么是理想的数据集，因此要维护Ta发声的权利。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2引入无关的特征&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;特征并非是越多越好，在训练数据不足的时候，引入过多无关的特征，只会导致模型过拟合。你可以根据实际问题，选择你认为相关的特征；也可以使用特征选择，选取那些对模型预测贡献度大的特征；还可以使用PCA进行数据降维，使用降维后的数据做为特征。在日常生活中，当你过分关注细节，而缺少大局观时，你就被无关的特征干扰了；自闭症可以看成是一种极端情况，大脑选取了过多的细节，最终导致其无法做出有用的预测。而面对决策时的拖延和犹豫不决，也是大脑中了收集更多无关特征的毒。而特征选择中的方法，也可以借用到生活中，比如你选择是否接受一个工作时，可以列出这项工作的优缺点，再进行比较，但在此之前，你需要先筛选一下你列出的优缺点是否对你的成长与幸福有关系，然后去掉那些关系不大的项。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 数据泄漏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果你在预测是时候，选取的特征包含了待预测标签的变种，例如你的目的是预测一个人是否感冒，然后你选取的特征中包含Ta是否正在吃感冒药，那你的预测模型结果会出乎意料的好，但这样的模型在实际中是没用的。在特征选择时，要避免那些能够直接导出待预测标签的特征。现实生活中的数据泄露对应的是因果倒置。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4 错误的补全缺少的数据项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当数据项存在缺失时，常见的方式是用平均值去补齐，或者填入某一个和已有数据相近的随机值。但如果出现数据项缺失的原因并非是随机出现的，这样做就会带来问题。例如如果在数据的持续记录过程中，有些参与者会退出实验（比如医疗数据中患者死亡），从而造成数据项的缺失。这时就不应该使用缺失值补齐或数据插值。而应该选取那些能够包容缺失值的算法。或者将对包含缺失的及不包含缺失的数据分别训练一个分类器。日常生活中，当我们面对缺失的数据时，想当然的补上一个我们最常看见的值，这样大多数还不如补上平均值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 忽略对数据进行正则化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有些模型，例如KNN或支持向量机，对于极端的数据点十分敏感，如果使用原始数据来处理，那么模型关注的就只是那些包含极端大值的特征，但这些特征并不是对预测最有效的，在真实情况下，极端的值不一定会出现，从而影响模型的泛化能力。为此要将数据进行正则化，使其具有相同的方差和均值。日常生活里，我们也容易被极端值迷惑，比如飞机明明是最安全的交通方式，但由于飞机失事后的影响很大，导致人们高估的飞行的危险，因此你要做的是将全部数据拿出来，统一来看，而不是只关注一个数据点。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6 忽略离群点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在清洗数据时，你会发现有极少数数据点很反常，这时你会想将这些离群点拿掉，应该不会有多少影响。这时你要注意的是分析数据点离群的原因，是由于测量仪器的bug，导致了出现了明显不可能出现的情况，还是这些数据就可能会有一些离群点。如果是前者，那离群点应该去掉，但如果是后者，那在训练数据集中去掉离群点，训练出的模型到了包含离群点的真实数据中，还是会难以泛化的。不同的模型对于离群点的容忍程度也是不同的，adaBoost会对离群点给予超高的权重，而决策树对是否包含离群点则不那么在乎，因此当你无法确定为何会出现离群点时，要选择那些对离群点更容忍的分类器。日常生活中，我们总会遇到那些特立独群的人，听到那些离经叛道的观点，这时我们该不该选择性的忽略，需要你来判断这个离群点到底是因何而生，完全的包容或排斥非主流的观点，都不是应选择的道路，你要做的除了甄别异想天开的背后是否有证据支持，还要让自己的价值观不那么容易被极端观点说左右。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7 忽略特征间的多重共线性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果训练数据中的特征存在多重共线性，例如特征A和特征B与C相关，特征B又与特征C,D和E相关，那么一个值的细微变化，就会导致众多特征的变化，从而使得训练数据中的由测量不准确带来误差被指数级的放大，从而使得模型变得对误差敏感，从而影响泛化能力。为此要检查训练数据的特征间是否俩俩存在着相关性，如果存在严重的多重共线性，要去除那些冗余的特征，或者将相关的特征组合起来，作为一个特征来分类。日常生活中的多重共线性是人群中的人云亦云，你以为你是听从多数的意见，但如果你听到的信息追诉起来是来自一个人，那这个信息就不应该被当成是群体智慧。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8 选择了错误的评价指标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于分类器，评价的指标有很多，常见的例如查全（recall），查准（precision）已经ROC，F1等。但对于具体的应用场景，应该选择不同的评价指标，有时假阳性是不可容忍的，有时则是一定要保证假阴性足够的低。如果你没有选择适合应用场景的评价指标，训练好的模型也会由于缺少优化目标，而无法实用化。而日常生活中，在不同的阶段，就该有不同的奋斗目标，不然你将无法确定自己是否成功。&lt;/p&gt;

&lt;p&gt;总结一下，本文总结了8种监督学习者的常见疏忽及解决方法，针对每种，引申了其在日常生活中对应的认知偏差，供读者自查自勉。这8种疏忽分别是：&lt;/p&gt;
&lt;p&gt;1 采样偏差&lt;/p&gt;
&lt;p&gt;2引入无关的特征&lt;/p&gt;
&lt;p&gt;3 数据泄漏&lt;/p&gt;
&lt;p&gt;4 错误的补全缺少的数据项&lt;/p&gt;
&lt;p&gt;5 忽略对数据进行正则化&lt;/p&gt;
&lt;p&gt;6 忽略离群点&lt;/p&gt;
&lt;p&gt;7 忽略特征间的多重共线性&lt;/p&gt;
&lt;p&gt;8 选择了错误的评价指标&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383835&amp;amp;idx=1&amp;amp;sn=c937dbe49d0f6320fed53d727bf63071&amp;amp;chksm=84f3c65ab3844f4cbcc834b0d881069254a193a0f3799282f583c256920bb3582c173b29d863&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习高维数据分析中那些一定可以避开的坑！&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 02 Feb 2019 00:03:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/R4lCgiTS99</dc:identifier>
</item>
<item>
<title>你会不会花两千块，去买一本数学书？</title>
<link>http://www.jintiankansha.me/t/x3KXN6BVNO</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/x3KXN6BVNO</guid>
<description>&lt;p&gt;&lt;span&gt;编者的话：公子是老朋友了，如今他换了ID，自己开了公众号，从中选了一篇，发出来。之后说说我（Peter）对这篇文的一些看法&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;昨天领导发了几本书给我，你们感受一下价格。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.287962962962963&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF699tBXvJiaugQtAByuP85mGDu4wwD57V3LoVcrBh07pPmibdFdLfz4hsaoy3Sia2rm8EGuv6PtdmwarOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1907407407407407&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF699tBXvJiaugQtAByuP85mGDuUTDwL95mQJVqchXKjrwaEEkV1tFBo7ESfia9EY4ibZHhg7ibfM8WSh7gg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.2861111111111112&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF699tBXvJiaugQtAByuP85mGDuhldrEv6ox8BDXgbDcgdibyj6ovBBZmibHfasmO38aCUmfaib9iadKnibYUg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;要明白为什么一本《群论在化学中的应用》能卖两千多，而一本《人类简史》只要&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;68&lt;/span&gt;，我们需要了解经济学中的两个概念：弹性，竞争性垄断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;首先，弹性描述的是当价格发生变动时，供求的变化程度。比如当一瓶可乐从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;3&lt;/span&gt;元跌到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2&lt;/span&gt;元、一包烟从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;8&lt;/span&gt;元涨到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;10&lt;/span&gt;元，需求会怎样变化？如果一个产品是“富有弹性的”，它的需求变动会很明显，比如奢侈品：它不是生活必须品，价格上涨的时候，销量会有明显的下跌，所以你看&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;4S&lt;/span&gt;店的车只有降价活动，没有涨价活动。而如果一个产品是“缺乏弹性的”，它的需求变动就很小，比如毒品和天价药：因为它是特定需求的必须品，没有代替品，即便涨价也不会让销量暴跌。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而图书市场是垄断性的，因为版权，出版社对每一本书都有定价权，他们是市场上的价格制定者。尽管多印一本书的边际成本只用&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;5&lt;/span&gt;元，每本书还是可以卖到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;50&lt;/span&gt;元。但同时，图书市场又是竞争性的，因为每个作者都有很多出版社可以选择，出版社不仅要与同行争取作者，还要在市场上争取读者。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;是什么让出版社决定一本书的价格，该卖&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;20&lt;/span&gt;还是&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2000&lt;/span&gt;？答案不是纸的材质和数量，是弹性。比如《人类简史》，只要识字就能看懂，受众范围非常广，背两页就能满足在茶会上装逼的需求，这种书就“比较富有弹性”。富有弹性的商品，价格一般不会太高，它要在海量需求和消费能力间找到一个“均衡点”，即“利润最大时的最大销售量”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;反观《群论在化学中的应用》，这种你连目录都看不懂的书，显然是没有大市场的。但要注意一点：市场大不大，不代表需求刚不刚。有时越是细分市场，越有刚性需求。而越刚需的产品，就越缺乏弹性，越容易定高价。比如数学，身为学科鄙视链顶端的专业，数学的从业人员显然比金融、法律、计算机都少。而这个专业每上一个&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;level&lt;/span&gt;，人数都会断崖式下跌，所以可供参阅的越数学书只会更少。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但如果不买这些书，你的课题和论文就很难进行下去了，搞不好还要丢饭碗。所以，当一个普通人面对《人类简史》时，他可买可不买；而当一个数学教授面对《线性代数》时，他多半不得不买。如果这本书恰好能解决他卡住的课题，价格再翻一倍都得买。这就是为什么，小众专业书籍能定高价的原因。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;抛砖引玉，我们来看一个更普世的问题：&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2019&lt;/span&gt;年后，经济会继续下行吗？世界将会怎样？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9953271028037384&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF699tBXvJiaugQtAByuP85mGDu67PgrmVgpMStwMibQ6NSzqkGVhlVaWQPhicfW5Vk5icRNT0VzVzhDWeBw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;428&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;以我的观察看，至少在人口基数最大的民用市场，供给已经饱和了。这里说的供给，是指各方各面的供给，包括且不限于实体商品、虚拟产品、技术、服务、模式、玩法&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;……&lt;/span&gt;是的，已经太多太多了，远远超过现代人所能想到的需求。这个市场上，只要你想得到的，只要去查都有，无非买不买得起的问题。而这“买不买得起”的问题，便是未来几年的风暴之眼：库存过饱和的前提下，怎么解决分配问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;翻译成新闻联播的语言，就是供给侧结构性改革。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;刚刚说到弹性，在市场过饱和的前提下会发生什么？答案是弹性将变得过于敏感，稍微一点价格波动，都会使供求曲线严重倾斜。因为市场上有无数的商家在提供数不完的商品，现在这个均衡点上，企业的经济利润基本为&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;（注：经济利润&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;&amp;gt;&lt;/span&gt;会计利润，因为包含机会成本），稍微一点点涨价，都会让经营难以为继。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;你也许为奇怪，为什么现在主城区那么多铺面退租转让，开得很好的店突然关门不做了？因为今年起强制企业给员工上五险一金，经营成本就突破盈亏平衡点了。以前一个餐馆服务员月薪只要&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2000&lt;/span&gt;，现在每个人头又多出好几百元，十个服务员就足以让经营成本翻倍，当然是关门止损了，开着就是纯赔。不幸的是，这个经济周期在原理上可以预见，现实中却无能为力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6776556776556777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF699tBXvJiaugQtAByuP85mGDuSsCayQBfbZqfH7jbz7E24qaXYXmp3UfuFdA9JBCZmib48uu9xLCg4wQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;273&quot; /&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;根本原因是啥？滥印钞票？我觉得不是，印钞是纸币发明后就不可消亡的行为。根本原因，是我们的底层技术没有新的突破，现有生产力已经达到瓶颈了。也就是说，理论上验证可行、现实中能够量产的商品和服务，已经没有我们造不出来的了。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;5G&lt;/span&gt;、比特币、互联网经济、共享单车、都是这个时代的怪物产出。这是近&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;100&lt;/span&gt;年来技术大爆炸的结果，我们解决了粮食问题，我们攻克了物种瘟疫，大家都沉浸在商品海洋的福利和资本积累的欲望中，而生产关系滞后带来的矛盾，终于到现在开始凸显。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;底层技术的核心是什么？是基础科学，是数学，是天体物理，是材料科学，是能源转化率。我们为什么如此关注黎曼猜想有没有被证明，为什么花上百亿经费找基本粒子，为什么建那么大一个射电望远镜，为什么一定要去月球的背面？如果在这些事情上没有突破，人类的生产力将被锁死在当前。如果不去追问这个世界终极和真相，我们只有在一个漫长不知终点的周期内，不断消耗自己。黑暗的新世纪，将很快到来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当年的苹果为什么伟大，现在的苹果为什么暴跌，都是这个原因。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;iPhone 6&lt;/span&gt;都出到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;iPhone XS MAX&lt;/span&gt;了，你不还在用同一个软件点外卖么？而送来的那一碗小锅米线，不还是小锅煮出来的么？如果能量转化率没有突破，如果我们的活动范围还限制在大气层内，这些使用习惯都不会再改变。再过&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;50&lt;/span&gt;年，等&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;iPhone&lt;/span&gt;都出到第&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;60&lt;/span&gt;代时，我们就会像赛博朋克的电影里那样，用快到无法想象的网速，继续等一份送餐半小时的外卖。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所以你现在告诉我，为什么一本数学书要卖&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2000&lt;/span&gt;块？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于为什么专业书这么贵的原因，我觉得还有一点是书的价值不止取决于你花了多钱买的，还取决于你花了多少时间在阅读思考这本书之上。《人类简史》这样的书为什么只会卖几十块，是由于看书的人花几小时看完了书，这本书的价值就结束了，你多半不会再读一遍，最多像我这样给这本书写篇读书心得。但专业书就不同了，如果是课本，你每堂课都会用，考试前复习时要用，等到之后实践中用到还能当参考书。因此对于每小时你花费的金钱，《人类简史》这样的书也许更贵。&lt;/p&gt;

&lt;p&gt;关于前文中提到基础科学的发展的重要意义，我觉得人们对看似无用的基础科学无知但盲信，以致于形成了所谓的科学拜物教。做基础科学，本就是为自己无法享用的树荫去种树。之前我一直想就权健的事情写一篇，但没有成文。下面是当时写的一部分：&lt;/p&gt;

&lt;p&gt;对死亡的畏惧自古皆然，但古人相信的是跳大神的神婆，如今是什么负离子鞋垫，纳米卫生巾。当下科学的地位很高，但对科学本身的理解太少，于是科学成了新的宗教，新兴的科学术语就成了神像，如果今年的诺贝尔奖给了量子力学，那就会有很多量子开头的保健品，而前些年害死了魏泽西的免疫治疗，也是用新概念包装的骗局。这些骗局之所以成功，是由于人们习惯了科技的飞速进步与高深莫测。所谓科学拜物教，就是相信科学却又对其不求甚解。拜物教信徒还会选择性的接收信息，通过对现实带有偏见的采样来减少认知不一致。而当下的主题是民族复兴，于是古老中药经过现代科技加持的叙事就特别有效，所谓的火疗，来自自然的保健品等就是典型的例子。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;不止老年人有科学拜物教，年轻人也有。前年过年回家看到高三的堂弟在背化学方程式，背物理题的解法，我问他想不想知道为什么不同的元素有的活泼有的很难发生反应，他说考试不会涉及就不需要知道。回想起我高中时好奇为什么电磁力和引力的公式那么相似，不过同时我也清楚的知道这样的问题不该问老师，不该自己花时间找资料。不知这是悲哀还是成熟。如今想来，我们自我驯化，养成了对科学问题不求甚解，不追求终极因的习惯。这带来的后果反映在当下的科普文章中，其中介绍原理的部分大多不会吸引人，而强调研究的结果和引申影响，殊不知这加剧科学拜物教。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;本来接下来想就这李约瑟之问，去探寻东方文化能否接棒源自于古希腊的西方哲科思维。虽然只看每年发表论文的数量，我们迟早能到第一名，但能否做出突破性的进展，却是另一回事。科学追求的尽可能通用的定量的在一定条件下成立的终极解释。我觉得好的科普一定要带上科学史，讲述认识自然过程中持续的自我批判，要将事情的原理和假设说清楚，至于研究的意义，也要通过例子来说明为什么可能产生应用价值，如何和其他的成果结合。读书要读经典，比如欧几里得的《几何原本》，作为西方哲科思维的起始，这本书才是所有真正想弄懂科学是什么的人必读的书，书看似无用，但肯定有趣。&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 15:23:01 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/x3KXN6BVNO</dc:identifier>
</item>
<item>
<title>谷歌大脑的“世界模型”简述与启发</title>
<link>http://www.jintiankansha.me/t/ppyxZ1iWM6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ppyxZ1iWM6</guid>
<description>&lt;p&gt;我们的视觉看到什么，部分取决于大脑预测未来会看到什么，例如下图中，如果你预计要看到突出的球体，那也许你就会看到，如果让机器也具有了这样的能力，会带来什么了？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkCLP1ia37xna8aoibHPOOBCLFQTOzxuLjqpcN9xVEbSUSRn1VPiaXqOTHg/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;205&quot; data-cropy1=&quot;13&quot; data-cropy2=&quot;218&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmO13hGkUN4v5e3Eol7Rh0t5ccR205fP1HDW3YDsTMzeXsalwAfjgicg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;205&quot;/&gt;&lt;/p&gt;

&lt;p&gt;18年谷歌大脑提出“世界模型”(World Models)可以在复杂的环境中通过自我学习产生相应的策略，例如玩赛车游戏。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7259615384615384&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmuuDIugmtDicLnq0dNH7YmfVBpw6FUnghDlVMd1PlwYYqibu2f7v7mBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;416&quot;/&gt;&lt;/p&gt;

&lt;p&gt;下面是世界模型的整体架构:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7060185185185185&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkPwWNeqEkezB3PJqGoXtBmibptFBfYIoE0aw4bXxmDySfZcz7DCG8xyA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;432&quot;/&gt;&lt;/p&gt;

&lt;p&gt;整个模型分为3个组件：视觉组件（V），记忆组件（M），控制组件（C）。视觉组件V用来压缩图片信息到一个隐变量z上（其实只是一个VAE编码解码器）：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.6&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkrbBKiaPqBkmPQf3bAYrmU37icPH8icpIrskk3GDib2YUNUnv6iaIz7PibAUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;记忆组件M的输入是一帧帧的游戏图片（论文中的一帧图像似乎叫一个rollout），输出是预测下一帧图像的可能分布，其实就是比一般LSTM更高级一些的MDN-RNN：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4832&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkT0zvQibp5s1TH0RPNUfNfaM6x7602Lfw2kibx0aXn9N2mqh5rR0na1kA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;625&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后控制组件C的目标，就是把前面视觉组件V和记忆组件M的输出一起作为输入，并输出这个时刻智能体agent应该做出的动作（action）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;在所谓的“世界模型”，其中的组件模型几乎没有是谷歌大脑自己创新研制的。但世界模型会很大提高强化学习训练稳定性和成绩 从而使其与其他强化学习相比有一些明显优势，如下表所示;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4444444444444444&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkicKDOBicjWeKZEElnsCzm1qzbbOuBfl5shc6gTfyWAbFOCe5ZQRjsjHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;558&quot;/&gt;&lt;/p&gt;

&lt;p&gt;世界模型有如下的3个特点&lt;/p&gt;
&lt;p&gt;1. 模型拼接得足够巧妙，这个巧妙的拼接模型做到所谓的世界想象能力，就是模型在学习时，自身对环境假想一个模拟的环境，甚至可以在没有环境训练的情况下，自己想象一个环境去训练。其实就是我们人类镜像神经元的功能。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8233532934131736&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkpsQ5KpvR3byIp6ibS2QHKg40F6oRiaMXtLpglgFbz009kBiby8qibdHFug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;

&lt;p&gt;2. 抓住了一些“强视觉”游戏的“痛点”。记忆组件M中的RNN是生成序列的能手，所以根据之前游戏图像再“想象”一些图像帧应该不成问题（RNN生成一些隐变量z，再根据隐变量z，由视觉组件VAE的decode生成的图像帧即可）。所以对于“强视觉”的游戏，把RNN的记忆能力用在视觉预测和控制上是个好主意 。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkMOk09Pib2ia2DDq3HBhuZWqX4NEpsuzbvOx3wDDs0P9kCkVWZBxhEqZw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;334&quot; data-cropy1=&quot;9&quot; data-cropy2=&quot;251&quot; data-ratio=&quot;0.7245508982035929&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkfDoibG4whRIFVy2NAKAibSORrc71CBLG15xdf9WTv9BjOUWbvaxuugPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;


&lt;p&gt;3 不同于我们常见的“不可生”智能算法，例如遗传算法和进化策略只是强调了基因的“变异”与在解空间中进行搜索，神经网络只是固定网络结构；而生物界的基因却可以指导蛋白质构成并且“生长”。如果基因可以构造自身个体，外部环境和个体情况也可以反过来影响基因，而我们的模型都太固定呆板了，模型结构不能随内部隐变量改进，当然最佳的设计形式也许谁也不知道。而世界模型做到了让在内部”幻想“的环境中产生的策略转移到外部世界中。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8567073170731707&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrksicaWK7Ls5EJMsF6hU4iatAd6MG7orOxwHCIZAumfzWvyhjRwWlJU1Dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;328&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;最后简单看一下&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;世界模型&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;的训练过程：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5024752475247525&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkLmQXpD9K5Ic89FCTQT7LA9RlljB2QmHj2E10dv56IWNjqszIhA6MpQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;404&quot;/&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;world models代码&lt;/span&gt;&lt;span&gt;基于&lt;/span&gt;&lt;span&gt;chainer&lt;/span&gt;&lt;span&gt;计算框架，步骤如下:&lt;/span&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1. 准备数据集，随机玩游戏生成训练帧（rollouts意思应该就是多少帧）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python random_rollouts.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --num_rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;10000&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;2. 训练视觉组件V，即前面提到的VAE：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python vision.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;3. 训练记忆组件M，即前面提到的RNN：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python model.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;4. 训练控制组件C，即前面提到的CMA-ES算法（其实就是支持更复杂输入和更新的ES）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python controller.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --lambda_&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mu&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--trials&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--target_cumulative_reward&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;900&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--cluster_mode&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;5. 测试训练结果：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python test.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--record&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://arxiv.org/pdf/1803.10122.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://github.com/AdeelMufti/WorldModels&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文经作者授权，转载自David9的个人博客，著作权属于“David 9的博客”原创，&lt;strong&gt;如需转载，请联系微信: david9ml&lt;/strong&gt;。原文地址： http://nooverfit.com/wp/谷歌大脑的世界模型world-models与基因学的一些思考/#comment-3444&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383931&amp;amp;idx=1&amp;amp;sn=5349aed7549713d893e08f07d0d44859&amp;amp;chksm=84f3c63ab3844f2cb0fbc01efe877031bbcbe25de23d6637f4e35e8e775b39806940e7084b4a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;多任务学习的未来之路&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383923&amp;amp;idx=1&amp;amp;sn=d717ecdc31f055bdcf7d8381c9e8b5c9&amp;amp;chksm=84f3c632b3844f243769de14d893e09d7db3fd4c43367a3eb61ed54ab0720aa2134750b4922a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Nature子刊机器智能综述-通过神经进化（neuroevolution）设计神经网络&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 28 Jan 2019 03:38:08 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ppyxZ1iWM6</dc:identifier>
</item>
</channel>
</rss>