<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何让神经网络具有好奇心</title>
<link>http://www.jintiankansha.me/t/pB5uZcxwFe</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/pB5uZcxwFe</guid>
<description>&lt;p&gt;&lt;strong&gt;一 为何强化学习要有好奇心&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;聪明人都珍视好奇心这个品质，好奇心驱使我们探索未知，从而带来了人类历史上所有重大的科学发现，从而带领人类走上了蓝星生态链的顶级。而对于那些需要在虚拟的游戏环境中称霸的AI来说，游戏环境越复杂，就越需要在强化学习中加入好奇心。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.8597222222222223&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图是强化学习的框架，AI要根据奖励和当下的情况来调查下一个阶段的行动，最终在最后的阶段能够让奖励最大化。但在最后的结算日之前，agent需要一个能够代表当前场上局势的估值函数，对于象棋军旗，人们还能根据场上的形式去确定这个函数大概章什么样子，例如象棋中当前的局势是当前你的车马炮各自的价值的加和，人们可以写出这样的函数应该具有的形式，由程序去学到该如何为函数中参数值赋值，但对于更加复杂的游戏，例如围棋，这样的方式就不可行，也就是说，手动确定奖励函数形式的方法，不具备可扩展性（scalable）。&lt;/p&gt;

&lt;p&gt;强化学习要解决的问题，要面临的另一个问题是激励来的太晚，比如智能体的最终目标赢得星际争霸的游戏，但为了让智能体在自我对弈中学会如到制胜的策略，需要一个函数来告诉智能体，是那些决策导致了获胜，而这对于很多策略类的游戏来说，开局的决定就能够影响最终获胜的概率。而这会让奖励函数在时间尺度上变的稀疏&lt;/p&gt;

&lt;p&gt;而有了好奇心之后，智能体就不需要由程序员来定义奖励函数了，而可以自己对自己之前的行为给予奖励，从而指导下一轮的行动。奖励函数对于强化学习中的智能体，可以类比为人的情绪，情绪告诉我们现在所处的情况对我们的生存是好是坏，在没有内在好奇心的时候，你的情绪全部由外人的批评或表扬决定，而有了好奇心这个机制，你就可以自己调控情绪，从而通过情绪的变化指导你调整自己的行为，让你更好的活下去。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;二 好奇心指的是什么&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;古龙对好奇心说过这样的话，“人们对他们不了解的人，总是会生出一种特别强烈的好奇心，这份好奇心，往往又会引起许多种别的感情。”，这句话反映出好奇心的原本是为了应对人际的互动中的陌生情况，如果陌生人和你的交往如你心里所想，那你会生出喜悦的情绪，而如果出乎意料，你也会在负面情绪的指导下调整自己的认知模型。&lt;/p&gt;

&lt;p&gt;将这句话中的道理用稍微数学一些的语言来表示，就得出了强化学习中的好奇心，即根据当前的形式，最小化对智能体的行为（action）的后果的预测误差，这等价于基于场上当前的局势及自己本回合的策略，预测未来一个回合的会发生什么。智能体在上述的“好奇心”的指导下，为了最小化预测误差，会主动采取那些会降低对未来不确定性有帮助的策略，例如智能体预测到下一回合敌人要进攻了，而这个行为会导致自己对未来的预测有很大的不确定性，于是本回合就会积极备战，从而降低不确定性。&lt;/p&gt;

&lt;p&gt;而智能体为了能进行上述的思考，就需要其对周围的环境构建出一个更抽象的表征，从而在类似的坏境下，也能用到之前的经验。而当一个模型具有了从环境中提取更精炼的表征（representation）的能力后，也能更容易的用同样的模型在不同的游戏中取得成功。这也是为什么说好奇心在各类强化学习任务中都有通用性的原因。&lt;/p&gt;

&lt;p&gt;既然是熟悉的预测任务，那就要面对过拟合的问题。在真实的而不是像围棋那样简化后情境下，智能体预测的环境中，既包含了会受到智能体行为影响的部分，也一定会包含和智能体行为毫无关系的部分，例如对于自动驾驶来说，要关注的不是路边的行道树，而是其他的车与行人，下图展示了对于自动驾驶程序，好奇心模块需要关注的部分。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7185501066098081&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1Ivf3op5EdJWIiaADwNeyibBlz7Y9jJYDjjNf7mAGLyqXzP3AEBQicjDWMDuQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;469&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面的例子展示了强化学习中的好奇心应具有的一个普遍规律，即预测的目标不应该是环境本身，例如星际争霸游戏中屏幕上的每一个像素，而应该是对环境的映射（embedding），或者是环境中的一部分，这个映射应该维度足够的低，同时保留了全部受智能体影响及会影响智能体决策的特征，同时在短时间内保持相对的稳定，否则会导致模型难以瞄准一个变化幅度太大的目标。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;三 好奇心的具体实现&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;好奇心是在2017年的一篇论文中第一次引入的，在今年，该组又此基础上发表了最新的结果。下面是好奇心模型已经征服的54种游戏中的截图&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5729166666666666&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfW71aFpickcQz2Yr3QwicawYsPTRN4mbyUmnF0OdajptDLmzrmUb1icX3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;576&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;这个模型的缩写ICM，指的是Intrinstic Curiosity Model。引入好奇心之后，不需要由人来告诉程序目标是什么，也不需要指出什么条件下游戏结束或者胜利，仅仅通过好奇心，就可以让AI学会怎么通关超级马里奥。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7983870967741935&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvffzSCByNarCBY7b8ISwHWZF4PSia1Yx06bWNnEviaT3jHOUQ0bKiciagbMg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;496&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里蓝色的部分是好奇心具体实现中的self-supervise模块，给定当前的状态St以及St+1，先从中提取那些和智能体有关的特征，通过神经网络，去最小化对行为at造成的影响的预测。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8236301369863014&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfuhnWOG0Yczt4FEWqCa1wOEfjJcRINAWxJ5JRcwY87xBLmfU7GeKYGQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;584&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而模型中红色的前馈神经网络，则是利用前面的ICM提取出的特征，最终实现预测下一时刻的坏境的目标，这里的待优化函数类似图像领域常见的MSE误差，&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7661870503597122&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfehJfRlZqTDOeO9n9iaXftJLQBjPicwQ2Il73Y6ytDBHgmLV8LyoUXYRA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6868686868686869&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfGskBfdZv3O1ZKTmfsCVUrkJ1j6qPTKczTBTPRozQ17rVtutiaiczZllQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;495&quot; /&gt;&lt;/p&gt;
&lt;p&gt;有了前馈网络提供的对下一时间点的环境中特征的预测结果，就可以从导出好奇心用数学表达的形式，下图中的n类似折线系数，代表了模型有多看重对更遥远未来的预测准确性。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.43548387096774194&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfZf7aiakBiaUPSSZr7MLQn7UmKeibCrHNHeSib0zl1vn8oerIhVHcXnTVpQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;558&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后要做的是用一个公式，将上文提到的俩个神经网络的预测目标结合起来，从而使得模型有动力去探索那些自己不熟悉的或者很复杂的环境，因为对未知的探索，哪怕只知道一点点，也能够极大的降低对未来的预测误差，同时要促使模型能够自主的学到那些特征是和智能体的行动有关系。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3830104321907601&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1IvfzgSuBCpAMGypDCs9hiaaroDKL4yz3ic8mbwRfae7ubj3ic79eppdjcR6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;671&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3549222797927461&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcck4r46wZsTRCpQFI9K1Ivfv3N1VmcRMXdCjcXgu5uDdTAzxsn09dofvBriaXLYrczTJiadQibDDo8Lw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;772&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四 总结&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Lecun一直以来推崇的半监督学习，就是根据视频的前一帧去预测下一帧，从而使得模型具有更强的表征抽取能力。而在强化学习中，由于要考虑智能体自身行为的影响，这个过程变得更为复杂，需要一个独立的模块来判断环境中的那些是与智能体的行动有关的（包括限制条件和对智能体的行为有效应的部分），从而避免随机噪音的影响。通过在强化学习中引入好奇心，可以使模型在面对噪音时的表现更加稳定（这在原论文中有细致的论述），还能使模型具有通用性，从而接近强人工智能所要达到的。好奇心的引入还使得数据标注不是必须，从而能够让更多的数据集得以被利用。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384219&amp;amp;idx=1&amp;amp;sn=f396d027ea5a6074e0f0cda0aeb0cded&amp;amp;chksm=84f3c7dab3844ecc80be70e9b9e47cd12686624d71158caf69fb79de9b1067d1b548e31ee132&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;空间简史-人类认识空间的旅程与其对强化学习的启示&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://pathak22.github.io/large-scale-curiosity/&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://pathak22.github.io/noreward-rl/&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 17 Mar 2019 14:37:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/pB5uZcxwFe</dc:identifier>
</item>
</channel>
</rss>