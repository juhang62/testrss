<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>图像分类中的隐式标签正则化</title>
<link>http://www.jintiankansha.me/t/fnyrIit0Tv</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/fnyrIit0Tv</guid>
<description>&lt;p&gt;正则化是深度神经网络中必不可少的一环，传统的正则化是在损失函数中加上一项与标签本身无关的惩罚项，去阻止模型变得过于复杂。在这周的Science Robitics上，刊登了一篇只有俩页的小文，概述了一种全新的正则化方式，并指出其俩者具体的实现方式。好文章不必长篇大论，说清楚突破点，指出进一步阅读的方向即可。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.3263246425567704&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqnvs1PUr6Bb9dNeVAsQNIfQHT4WzTEOyYcvxdbmOuiaB8HOPOLEnSKL2w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1189&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正则化的目地是为了提升网络的泛化能力，而过拟合的源泉来自模型对已有数据的死记硬背。而在现实生活中，你给一个幼儿园的小孩子一堆动物的照片，然后给Ta一张豹子的照片，让其从中选择十张和豹子最类似的照片，一个孩子拿来了九张豹子的，还有一个大黄狗的照片，另一个孩子拿来了8张豹子的和一张狮子以及一张老虎的照片。你觉得这俩个孩子哪一个更聪明，对豹子的概念理解更深入。如果单看选对的数目，那第一孩子获胜，但是由于第二个孩子选的的都是猫科动物，因此可以说第二个孩子对豹子的本质有更深刻的掌握，Ta不是在死记硬背，下次遇到白色的雪豹，第二个孩子也更有可能将其归为豹子的一种。&lt;/p&gt;

&lt;p&gt;将这个例子用到有监督学习的语境下，就得到了下图：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.36655405405405406&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqn0Kfh3CoqvZWQ3Fa4XlFBYrtaSnDnlCjE5rgfYqnPxMcMVKBZKoboXA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1184&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;摘自&lt;span&gt;Toward principled regularization of deep networks—From weight decay to feature contraction&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;左图展示的是传统的损失函数下卷积网络训练完成的结果，图中的数字代表模型给出的不同标签的置信度以及待训练真集对应标签的置信度，可以看到图中正确的结果豹子的值最大，而猫这个和豹子是一类有些类似的标签和其他错误的标签一样，其置信度都很低。而在右图中，训练好的模型给了猫这一标签更大的置信度，这会导致模型在训练时更多的犯错。但正如Hinton所说，不同错误之间的相对比例能传递很多的信息。同样是对照片进行错误的分类，将豹子错判成猫的模型比将豹子错判成汽车的模型从常识上就更靠谱，因而也具有更强的泛化能力，这正是本文提出的隐式正则化。&lt;/p&gt;

&lt;p&gt;具体怎么做，直接的方法是将训练的真集（ground truth）中不同标签的置信度进行改变，这等价于告诉模型，对豹子的图片进行分类时，要押八分宝认为上图中的豹子是豹子，还要押俩分认为图中的豹子其实是只猫。这样做不用改变模型本身，但找到一个合适的真集置信度，来提升模型的泛化能力，却不是一件容易的事情。大规模的数据不应该依赖手动的标注，而应该训练神经网络，找出怎样的真集能提高分类网络的泛化能力。&lt;/p&gt;

&lt;p&gt;另一条路是在损失函数中增加一项(例如不同标签间的交叉熵)，来使得模型训练后的预测具有更大的信息熵，在上图中，对比俩张图给出的预测结果，左图知道了模型预测的是豹子的置信区间是0.95，那其他的信息的重要性就很低了，因为这意味着次高的置信区间也只有0.05，而在右图中，知道了图片的标签的是豹子的置信区间只有0.75，那其他的标签的置信区间还是有一定的信息量的。通过在损失函数引入新的组成部分，以惩罚那些信息熵较低的模型，最终使得模型训练后倾向于右图。由于引入的惩罚项不是为了避免模型过于复杂，而是为了避免模型钻牛角尖走极端，因此和传统的正则化方法有本质区别。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.2759170653907496&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqngJaNjdCLKXqpmhr9qhGhyWJSBpvwTOicRLiaEbUKEoX4FCaVT0Elv9aw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;627&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;摘自 REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;这里展示了不同的正则化方法下，在MINST数据集下不同置信区间的概率密度，相对来看越均一，说明模型泛化能力越好，这里拿来和常见的dropout对比的是之前提到的在真集标签上进行的smothing与对过于“自信”的模型施加惩罚项之后的效果。其中效果最好的confidence panelty。&lt;/p&gt;

&lt;p&gt;另一个正则化的方法则是在神经网络每层的每一个特征中加入L2正则项，这可以形象的称为“特征缩水（Feature contraction）”，这样做的好处是对网络中每一个隐藏层的特征都加以缩减，不管是正值还是负值，只要这个特征本身过大，那就会通过正则项让这一特征缩水，从而最终让模型判定的标签不像之前那么自信。下图展示了MINST数据集上不同正则化方法的效果，可以看出特征缩水的效果是最佳的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.6717241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceUtx1ysrSqbhB2FvMia7bqnwIia2qM9ldjPe2XRiaJJhyxUOibUhr41DmcFfNOUK8lZ38SBNWRCmpOlA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;725&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源：https://github.com/VladimirLi/feature_contraction_example&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这篇文章的标题是走向一个有原则（principled）的正则化模型，文中不止介绍了上述的正则化方法，还指出了例如权重衰减在本质上，也是和上述的方法要达到同一个目标，即避免模型对一个分类标签孤注一掷。从优化的角度来看，如果训练的最终目标是类似图一中左边那样的模型，那其梯度在最终下降时会减慢，而让模型的训练结果类似右图，则可以提高模型优化过程中的梯度，这也解释了为何上图在训练epoth不多时，特征缩水的效果就优于其他的方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;顺着这个思路去扩展新的问题，在对于深度模型，总会有针对其的攻击样本，仅仅通过改变图像中几个像素，就可以改变图片的分类，在模型中引入隐式的关于不同分类之间的知识，能否部分避免对模型的攻击了？能否在除了监督学习的领域，例如在强化学习中的奖励函数的设计时，考虑到不同策略的相似性和从属关系，或者在评价分监督学习聚类的结果时，按层次对不同类型的聚类错误给予不同的权重，大类都错误的聚成一簇的给予更大的罚分，而只是在同一大类中聚混了则不那么严重。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最后想说几句无关的话。对于神经网络中的正则化，曾写过一篇&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384281&amp;amp;idx=1&amp;amp;sn=48c0f5a02ea2c2f8b0582fc9356ed668&amp;amp;chksm=84f3c798b3844e8e759a9b253b8d3fc34ad86089ed9ed2135365499ee12a61ba3dba0cdb7e0f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;预测神经网络预测准确性的普遍理论&lt;/a&gt;，也是偏理论的文。但正则化的种种技术，在生活中反映出来，都是让我们学着谦虚，比如最常用的drop out，大白话来说就是每个人都是社会中的一个螺丝钉，离了谁都照样转，不要把自己想的太该死的重要，比如weight decay，说的是你在向你的上级传递信息时，不要让自己显得过于权威（减少你输出特征的权重），而本文提到的&lt;/span&gt;Feature contraction，则是让你把收到信息中那些极端的部分不要听风就是雨。近来舆论界俩级分化，各国皆是如此，信息茧房，也是越演越烈。这正是整个社会层面的过拟合啊，只关注眼前的利益和斗争，需要的正是有更多的人，懂得上面所说的三条正则化技术背后对应的哲理。&lt;/p&gt;

&lt;p&gt;今天儿童节，所谓的找回初心，可不对应的是正则化方法中另一个常见的early stopping吗？而另一种所有神经网络都会用到的正则化方法随机梯度下降及其改进版则让我想到最近流行的一句话：“鸿蒙初辟原无姓，打破顽冥须悟空”，这话太文雅了，技术一点看，就是全局最优解本没有固定的位置，要想走出局部最优，需要领悟到空（即随机）的价值，粗一点的话是刘欢那首歌中唱的：“心若在，梦就在，人生不过从头再来”。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384360&amp;amp;idx=1&amp;amp;sn=9e7a1c88f8dc373730bc5933abd1c616&amp;amp;chksm=84f3c469b3844d7ff3e6cd1d3f9c728f583063c0a95c6ad21596d2d66a2dfd8a2bdc205eadad&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;随机网络中的智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384326&amp;amp;idx=1&amp;amp;sn=b8f75655a8cfe79b67ce7263c50f1bba&amp;amp;chksm=84f3c447b3844d513b51c1efbef9f067d089584f6acf114b0956a353d153d9b12aeaeff3da52&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当机器学习遇上进化算法&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;







</description>
<pubDate>Wed, 05 Jun 2019 01:02:44 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/fnyrIit0Tv</dc:identifier>
</item>
<item>
<title>《The Creativity Code》读书笔记-AI距离有创造力还有多远</title>
<link>http://www.jintiankansha.me/t/Em6W0LWrW6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Em6W0LWrW6</guid>
<description>&lt;p&gt;&lt;span&gt;这本书是今年4月份出版的，感谢亚马逊，让我能第一时刻看到这本书，书的作者是数学家和科普书作者&lt;/span&gt;Marcus du Sautoy，他之前的几本书也很值得一看。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.5015015015015014&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce4AM65XzsjZAbNhENpCTJ2w8EpnIWjIoB7KgdQJ7icNPdaydzMhkgiaxSD8DB8686UwXFhicI8xlJew/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;333&quot; /&gt;&lt;/p&gt;

&lt;p&gt;书中首先定义了创造力的三个要素，新的，意外的，有价值的。刘慈欣的“诗云”中的列出了所有诗歌可能组成的外星文明，按照这样的标准，不算在诗歌创作中展现出了创造力，因为列出了所有的可能，就没有意外了。而价值这个主观的评价，说明了创造力不是随意的涂涂抹抹，而是带着枷锁的舞蹈，价值的多少会随着人的喜好而改变，而人的喜好又可以在不自知的情况下被操纵，例如操作的米国大选的剑桥分析。&lt;/p&gt;

&lt;p&gt;书中接着将创造性分为了三种，组合型，探索型与革命型。狮身人面像是组合型的创意，读武侠小说，总会有人争吵郭靖和张无忌谁的武功更高，再YY一段，这就是探索型的创作，而革命（transformative）型的创意，则是创作出一种全新的艺术流派。当前AI与创造力相关的进展，更多集中在前俩者上，但人们往往对正在发生的范式改变一无所知，直到变化彻底取代了前一个范式，有谁能说，现在AI在创造力上取得的成绩，本身不是一场革命（transformative）型的巨变了。&lt;/p&gt;

&lt;p&gt;人们判断机器在某一个领域，是否具有创造性，传统上采取的是类似图灵测试的方式，看其的表现是否能够模拟人类的专业人员，例如如果职业画家都分不出这是不是机器的画作，大多数人听不出这首曲子是算法创作的，或者一篇新闻稿是不是机器写的，那就说明机器在该领域表现出了创造力。在书中，作者指出，关于创意，不应该采取这样人类中心的做法，例如在AlphaGo和李世石的第二局棋中，alphaGo走出的一步是人类棋手根本想不到的，无法通过上述的图灵测试，事后却被证明是制胜一子。根据之前提出的创造性的三个要素，这其实是最高级的革命性的创意。读这本书的一大收获，就是改变了我们对何为创意的判定方式。&lt;/p&gt;

&lt;p&gt;书中作者在先讲了推荐系统，搜索引擎这些我们日用而不自知的系统背后原理，以此来论述在操纵人们的所思所想方面，机器不过是执行编程的人的意志，是一个更自动化的打字机，而不是会自己创作的神笔。之后分绘画，音乐，文学三方面讲述了几十年间各种不同方式的尝试，由于作者的主业是数学家，书中很大的篇幅在论证机器是否能在数学上展示出创意，书中写的最精彩，信息量最大的，就是这几个章节。&lt;/p&gt;

&lt;p&gt;先看绘画，首先是分形艺术，这是只有机器能够产生的复杂图像（参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=204448071&amp;amp;idx=1&amp;amp;sn=152c7e9c5db9c06881db31508925fef9&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;16张gif图带你认识分形（fractal）&lt;/a&gt;），其次是用对抗生成网络去学习某位大艺术家的风格，去批量的生成模仿类的作品，例如风格迁移（参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383188&amp;amp;idx=1&amp;amp;sn=ec8d1090fe46741c14ccf7cc02b57c2c&amp;amp;chksm=84f3cbd5b38442c33ef70b698dd07f5b7aa954623fe3724e9f83b8e10a44b86a3777054395a4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;怎么样用深度学习取悦你的女朋友（有代码）&lt;/a&gt;），更有趣的是让机器去在已有的绘画风格上进行改变，好的艺术作品，会让人感到恰当好处的偏离传统，即不是对过去一成不变的模仿，也不是完全的颠覆已有的规范，通过将艺术风格提取成高维的数量表征，在对其进行细微的扰动，算法能够产生新的艺术风格，而不只是模仿前人的作品。&lt;/p&gt;

&lt;p&gt;在音乐上，最简单的做法是用马尔可夫链，根据之前的音符来预测之后的音符，而用神经网络来做预测，则能够让机器模拟出巴赫的作曲风格，创造出音乐学院的大学生都无法分辨的作品。书中还介绍了根据对已有音乐的特征提取，将音乐所表达的情绪分为8类，从而使的机器做的曲子能够对应特定的情绪。另一个有趣的尝试是算法的生成个性化的古典乐曲，将一段钢琴曲打成几秒钟的碎片，通过随机的组合，生成一段只属于你的钢琴曲，书中作者称之为量子作曲。这方面的商业应用很有前景，广告中，电视剧或者游戏中的背景音，不需要特别好，而且需要很长的时间，相近的风格，完全可以由机器来生成。&lt;/p&gt;

&lt;p&gt;在文学创作上，微软小冰已经出版了一本诗集了，IBM的waston很早就赢得Jeopardy的冠军了，但随着作者的描述，我们看到了这些成绩背后的原理，其本质还是搜索与优化。Waston先将问题分类，之后提取关键词以减少搜索范围，之后生成关于答案的200个可能的假设（及其对应的回答），最后根据其从维基百科中的数据对不同假设进行排序。而生成诗歌，只需要遵守诗词的格律，根据前人的诗词作品，用之前的字来预测之后应该是那个字。中文屋的思想实验指出，仅仅能够在行为上模仿人类，不代表真正对语言的理解。&lt;/p&gt;

&lt;p&gt;如果当前AI真的理解语言，那waston这些年间就应该可以全面开花，而不必为了某个特定的应用场景专门进行优化和调整。而小冰也应该能够续写冰与火之歌，而不是出版诗集。而当前AI的文字创作，还完全无法写出一个逻辑通顺的故事，用写诗歌的预测喜爱一个词的做法，出来的只是零星的片段，书中介绍的有希望的方式是让机器去问一系列如果这样会发生什么的问题，再自问自答，从而构造出情节张力。目前成熟的，是让AI写不包含感情的公文，例如体育新闻的报道，财务报表的解读，但AI看人下菜碟，创造假新闻的能力，让这个领域蒙上了阴影。但凡涉及到人的喜怒哀乐，哪怕是量产鸡汤文，在完全理解人类意识和情感之前，AI都不可能完成。&lt;/p&gt;

&lt;p&gt;而在数学领域，当前的很多证明，都依赖计算机的参与，例如地图只需要四色就能够保证相邻国家的颜色不出现重合，随着证明的越来越长，人们也需要机器来验证证明是否是正确的。书中作者将数学定理的证明和下棋归为了一类，都是按照给定的规则，从一个固定的起点出发，一步步得到最后的结果。这其中的创造性，不是来自与暴力的尝试所有可能的分叉，从而证明出所有可能的定理。而是找出有价值的联系，将俩个看似不同的领域联系起来，例如通过几何的方法，用一张图，来证明连续的奇数之和是一个平方数。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce4AM65XzsjZAbNhENpCTJ2VfX8hBtF5KLFuibVBtQP5BQsSoQRRN62fbuRibFOLUerWYLvqqsRXmMw/0?wx_fmt=gif&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;507&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;191&quot; data-ratio=&quot;0.3747534516765286&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce4AM65XzsjZAbNhENpCTJ2JZBfqXh8CQX60Lzeru9PbGFyug7gsLqLJmRh9wDlejL83KMu6iaeO6A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;507&quot; /&gt;&lt;/p&gt;

&lt;p&gt;机器不会发挥失常，一旦机器能够达到莫扎特的水平，就会如同莫扎特永生了一样，不停的进行创造。这会不会让人类产生对机器在艺术及创造力上的依赖，就像人类正在变得依赖推荐系统搜索引擎提供的智能外包，异或正由于有无穷的创造力，莫扎特的音乐变的不那么有价值了。人们创造出艺术音乐文学与数学，是为了相互沟通，从而体会做一个他者是什么感觉，从而变得有同理心，进而才有了人类的文明。当我们担心AI带来的反乌托邦时，从AI创造的艺术中，我们可以看到AI是怎么运作的，从而产生有知觉的理解。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381695&amp;amp;idx=1&amp;amp;sn=4cb6459c2a63f658e8370b9bde652177&amp;amp;chksm=84f3f1feb38478e81108f04c987a95133dab9c61894d6bd00830bb82c0755d9d4acad8d4972c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;人工智能会取代艺术家的工作吗？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384351&amp;amp;idx=1&amp;amp;sn=d4b5827c09b80dd7a5c7811bfe111636&amp;amp;chksm=84f3c45eb3844d4848134ba1209e14a0a17be3eacaac36310922cf450caa9e782a5d7f274698&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;AI中的幂率法则-通过Scaling来看AI的未来&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 05 Jun 2019 01:02:42 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Em6W0LWrW6</dc:identifier>
</item>
</channel>
</rss>