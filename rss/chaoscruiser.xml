<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>2019计算与系统神经科学大会Cosyne 前沿研究汇总</title>
<link>http://www.jintiankansha.me/t/MJ6IvWrnum</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/MJ6IvWrnum</guid>
<description>&lt;p&gt;&lt;strong&gt;&lt;span&gt;0 背景&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;计算神经科学是一门超级跨学科的新兴学科，几乎综合信息科学，物理学， 数学，生物学，认知心理学等众多领域的最新成果。关注的是神经系统的可塑性与记忆，抑制神经元与兴奋神经元的平衡。计算神经科学在做的事情是先主动设计这个一个系统，看看如何做到需要的功能（自上而下），然后拿着这个东西回到生物的世界里去比较（由下而上）。人工智能和计算神经科学具有某种内在的同质性， 唯一的区别可能是人工智能可以不必拘泥生物的限制，或者也是为什么他最终或许会比生物网络表现更好。更多内容参考 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=207945043&amp;amp;idx=1&amp;amp;sn=9b6799ff0de34bae7f141ff95492e090&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;为什么你需要计算神经科学（上）&lt;/a&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=208053222&amp;amp;idx=1&amp;amp;sn=5a7feead10fa4210c3d160bc66c82b87&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;为什么你需要计算神经科学（下）&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;今年的计算与系统神经科学大会 - Cosyne 在葡萄牙结束。 这个会议和nips都是神经网络与计算方面的最重要盛会， 而方向上一个更偏深度学习， 一个更偏和生物有关的计算。而近两年的趋势是， 两个会议的交叉主题越来越多。 对于会议涵盖的几个方面， 做一个小的总结，也算涵盖了计算神经科学的主要方面。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1： 前馈网络： 如何让深度学习工作更像人脑&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在这个session， Yann Lecun 作为邀请演讲人， 总结了CNN受生物神经网络启发的历史， 并提出他最近的核心方向 - learning predictive model of world（学习建立预测性的模型）。 指出深度学习的未来在于以建立预测性模型为核心的半监督学习， 这样可以弥补普通的监督学习或model free reinforcement learning（无模型强化学习）的巨大缺陷-缺乏稳定的先验模型。 比如你要做一个视频有关的处理， 让他看完youtube上的视频并不停的预测视频下一帧的状态， 这样预训练后再去进行任何任务都会更方便。 yann认为这是dl的未来方向。&lt;/p&gt;

&lt;p&gt;一个目前突出的成就是大量预训练产生的NLP模型Bert在各大任务上都破了记录。 关于如何进行半监督学习， auto-encoder和对抗学习都是方向。 在此处无监督，监督， 和强化学习的界限已经接近。 强化学习不再只是蛋糕上的樱桃， 无监督学习也不再是难以操作的暗物质。 预测性学习用的是监督学习的方法， 干的是无监督学习的事情， 而最后被用于强化学习。 不难看出， 这个方法论和计算神经科学领域的predictive coding间的联系， 和好奇心的联系。 整个工作都符合Karl Friston 关于自由能最小的理论框架。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6402777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcQfcc4GaA4fzQqZWYyQ4sDBCR6LgFxGZsibHSUQNCvL2fwCsWLzicthaA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;720&quot;/&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcEDNLia7lGDVuvnkSPh4PNRAicAszS7DCJNsk6HJrYZMLNQavBULRr7Vg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;4032&quot;/&gt;&lt;p&gt;有关前馈网络和计算神经科学的交叉， 另外几个speaker 着重在于研究生物系统如何实现类似反向传播算法的过程。 反馈的神经信号和local的Hebbian rule等的结合， 可以实现类似于反向传播的修正，也就是说大家在寻找反向传播的生物基础，而且还非常有希望。&lt;/p&gt;

&lt;p&gt;当然比较CNN不同层次的representation 和生物视神经的表示已经是老课题， 目前imagenent上预训练的网络经常被用来和生物神经网络的活动比较， 逐步被作为一种衡量生物神经网络表达复杂度的标尺， 也是一个有意思的方法。&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;span&gt;&lt;strong&gt;2，多巴胺（Dopamine）在学习回路中的作用（Ilan Witten， Princeton）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;多巴胺神经元和回路是计算神经科学和强化学习的热点问题， 它与我们的一切行为有关， 影响我们的喜乐哀愁。 dopamine的经典理论被认为传递对未来奖励的预期信息和真实奖励的差距， 这恰好对应强化学习理论的TD误差。 后来人们发现这个想法太简单了。 一些新的结果指出dopamine神经元作为一个数量巨大的群体， 编码的信息不仅包括奖励信号， 还有和奖励有关的的信号特征，比如颜色，物体的运动方向。生物系统为什么这样选择自己的强化学习算法， 非常值得探讨。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bckQcP8rm6KibbWibmQxVkJJ8U0DNiaqXr5qEQCLcUZjicfl7ibU4nv1iaOSdA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;4032&quot;/&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcpG503hJwyt7ZjouIwtT8MUXaP7A8yB1oM2Y3VmibaicBDJXUFh5hYmHw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1440&quot;/&gt;
&lt;p&gt;另一些工作围绕dopamine和强化学习的研究通过实验验证dopamine的数学理论， 模型结合实验的方法可以很好的test这方面的idea。 人们一直在争论dopamine对应value function本身还是TD误差， 你能不能设计一个研究的方法很好的区分了前者和后者？ 事实上真实情况永远比理论模型复杂的多。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;3. 神经编码的本质： 高维vs低维（Kenneth Harris， UCL）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;大家都知道人脑有1000亿个神经元， 近似于我们说的无穷多，为什么？为什么要这么多？&lt;/p&gt;

&lt;p&gt;神经编码的本质属性是维度， 大部分时候， 当我们对世界的理解抽离到最后， 就只剩下维度。 首先神经编码必须是高维的， 这对应我们的大千世界信息是丰富的。 同时我们又不希望神经编码的维度太高，我们们希望在能够表达现实世界的丰富信息的时候， 这个表征流行的维度越低越好， 反过来说，就是我们希望在某种限制条件下尽可能充分的表达真实世界的信息。 其背后的合理性是什么？&lt;/p&gt;

&lt;p&gt;这组实验让小鼠不停的观测从自然环境中随机抽取的图像样本（nature image）， 然后我们记录视皮层的神经活动， 并通过PCA等降维手段来观测神经表征里的维度。 首先， 我们最终得到的结果是我们的生物神经网络确实具有无穷多（和图像的总数一样多）的维度（所以需要无穷多神经元表达）, 这是由于自然环境中的物体太丰富了，自然信息的维度可以是接近正无穷， 这也是为什么我们的脑内需要这么多的神经细胞。&lt;/p&gt;

&lt;p&gt;然后， 我们发现并非每个维度都是均衡的， 每个PC维度所刻画的信息量均匀的下降， 而且这个下降呈现的衰减符合一个幂律分布。 而这个幂律的数值非常关大。 我们知道这个数值越小， 衰减就越慢，幂律就越接近肥尾， 这背后对应的是什么呢？ 如果我们用流型的思维看， 这个指数大小正对应流行曲面的形状（你可以想象一下极限情况， 如果我们只有两个PC，后面的数值均是0，我们的流型是一个平面） 。 越小的指数， 代表高维的成分越显著，流型维度大到一定程度， 就会出现分型结构（连续但不可导）。 一个高维的分型结构意味着，每个样例可能都占据着一个高峰， 而稍微一离开， 就是波谷。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.3411458333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bchyXDmvico1Z7oDLIypYvcZCqPuoKufZu4QXEKBiblHxVM9IyTl5fYXqA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;384&quot; width=&quot;384&quot;/&gt;从低维流型到高维分型&lt;p&gt;这在机器学习里，恰恰意味着泛化能力很差， 如果你稍微移动一下这个曲面， 分类就可能变化。 如果指数比较大呢？ 指数比较大， 意味着高维成分衰减很快， 这个时候， 我们会得到更为平滑的流行曲面，从而得到更好的泛化能力。那么指数可不可以尽量高呢？ 答案是不行， 因为那样导致的表征维度过低， 刚刚已经说了很多遍，那样我们就失去了对丰富世界的表达能力（维度越高越好做分类，可以容纳更多互相正交的分类， 模型容量高）。&lt;/p&gt;

&lt;p&gt;总结一下这个幂律的指数值有两个关键点， 当指数比较小的时候流形都是刚刚讲的分型结构， 第一个关键点是从分型到平滑， 而第二个关键点是神经全息成像， 当衰减速度快到一定程度（低维到一定程度）， 我们就会得到类似全息成像的现象，此时神经信息处处是冗余， 你随便找一组神经元都可以得到整个外部世界的信息。&lt;/p&gt;

&lt;p&gt;自然界用高维冗余的非线性系统表达低维的表征，来对现实世界降维。 这是神经科学和深度学习恒久不变的主题。 一般情况下高维会增加分类的效率和模型的容量(正交性)， 而低维则有利于泛化（平滑性， 把相关类别的编码放到一起）。 而在当下的深度学习里， 我们恰恰缺乏这种能力， 用同样的指数实验测量CNN的信息压缩特性， 我们发现， 它的指数衰减明显的慢于小鼠，也就是依然保留了更多高维成分， 这使得它对高维信息（往往在空间上意味着高频）极为敏感。 当你在已经识别很好的图像加一点噪声（高频信息）它就认错了。&lt;/p&gt;

&lt;p&gt;这个讲话解释了很多困扰我的谜团， 比如为什么需要那么多神经元， 深度学习的泛化问题等等， 同时把学习算法和幂律巧妙的联系在了一起。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcoFDCp6UTpryVtvbAJic5LF7btvcet7tunNzgp1PXemEtn4zCOUJlv4A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;4032&quot;/&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bc3zWn2Y9licKdY4WOOiaFsgylycMeEbymgTXMQEPKDLFJ1N8d4REMkEog/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;4032&quot;/&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcCg6eYiau0iaC4iafibLxdHESeltSsIrrdibhmpUnkX9uJcCeDA91th5cTtg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;4032&quot;/&gt;自然图像与神经活动中的幂律
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;strong&gt;4， 寻找RNN的动力学维度（Eric Shea-Brown， Univesity of Washington）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;另一个研究指出用RNN解决任务时候自身动力学维度与任务维度的匹配关系。 如何预测RNN所表征的系统维度？ 首先维度取决于背后的动力学， 然后网络的动力学取决于结构， 我们可以用一套启发于物理学的方法来从结构推出动力学维度。 这个方法通过定位神经网络里的motif来预测其维度， 可以说和费曼的场论异曲同工。&lt;/p&gt;

&lt;p&gt;然后这个维度有什么意义？ 我们说这个维度与我们要执行任务本身的复杂度高度相关。如果换一个在平面上的简单分类， 我们不需要实用自身动力学维度很高的系统做， 而如果这个分类就是高维的， 那么具有高维动力学的系统往往优于低维的。 这揭示了网络动力学与真实世界动力学的内在联系。 &lt;strong&gt;而事实上， 一般在混沌状态的网络动力学维度更高， 这无形中揭示了，混沌没有看上去混乱，它可能恰恰是我们强大认知能力的基础。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;5， 生物导航Navigation （Edward Moser， Kavli institute, grid cell诺贝尔奖得主）&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;导航与空间运动相关的问题一直是计算神经科学的热点主题。 grid cell实现所谓的物体位置编码，可以把空间里的核心物体位置编码成一组向量。 这种能力是如何一点点随学习和发育产生的? 这是一个非常大的主题，也是无数计算神经科学家的目标。&lt;/p&gt;

&lt;p&gt;Navigation的一个核心主题是cognitive map 的理论。 它说的是在大脑中存在一个空间表示的神经载体。 你我都存在在这个认知空间里， 它独立于你我而存在。 根据Okeefe的理论， 这个空间是hippocampus的grid cells 和place cell 作为基础提供的。 grid cell类似于一个巨大的坐标系统， 而place cell 可以在每个不同的空间里重新编码（remapping）。 这个十分有魅力的理论至今其实很多问题依然是悬案。&lt;/p&gt;

&lt;p&gt;在这次的会议上， grid cell 理论的创始人Moser给了key speech， 他主要描述了这种空间的神经编码应该以对空间的物体进行向量编码为基础， 每个物体对应一个向量编码。 同时， 他讲解了提供这种空间结构的基础网络是如何从发育阶段一点点形成的。 从发育阶段理解一个复杂问题通常可以把这个问题简化。&lt;/p&gt;

&lt;p&gt;围绕这一主题的其它讲话里有几个来自以色列的研究特色鲜明。维兹曼研究所的 Alon Rubin 揭示出我们所认为的认知地图即使对应同一个环境也不仅有一个，在同一个房间运动的小鼠可以解码若干地图， 这一点让我们不仅思考这些地图到底是干什么的， 显然它们与不只对应我们所认识的绝对空间， 因为绝对空间只有一个。&lt;/p&gt;

&lt;p&gt;另一个来自以色列的Gily吗Ginosar 则展示了如何寻找蝙蝠头脑里的grid cell， 并揭示出它符合一个三维空间的密堆积周期结构 。 因为蝙蝠的生活空间是三维的， 所以显然它的空间表征也要是这个维度。 这点让我们不禁想象， 如果存在4维和5维的空间，这个表示是什么样的？ 到底是我们的认知确定了我们的世界， 还是我们的世界决定了我们的认知？&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  另外一个核心问题是我们头脑里的认知地图是egocentric(自我中心) 还是allocentric（外部环境中心），所谓以自我为中心（以上下左右表达整个世界，自我就是坐标原点）， 还是以一个外界的坐标系（如不同的地标）为中心。 经典的认知地图模型是allocentric的外部坐标表示， 然而事实上很多研究指出， 自我为中心可以找到很多实验证据 。因此两个派别进行了激烈的辩论。&lt;/p&gt;

&lt;p&gt;当然也有些会议上的报告讨论了place cell的真实性“它们可能仅仅是一些依照时间序列依次发放的神经集团” 来自MIT的Buffalo指出。&lt;/p&gt;

&lt;p&gt;最后， 这个方向的讨论还包含了这种能力是否能够提供空间之外的推理能力？来自马普所的教授进行了很好的开拓性发言，它认为空间的grid cell 可以作为我们的其它推理能力的一种基础形式。（更多内容 参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384219&amp;amp;idx=1&amp;amp;sn=f396d027ea5a6074e0f0cda0aeb0cded&amp;amp;chksm=84f3c7dab3844ecc80be70e9b9e47cd12686624d71158caf69fb79de9b1067d1b548e31ee132&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;空间简史-人类认识空间的旅程与其对强化学习的启示&lt;/a&gt;）&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcKME1kReXjtJ2TibPVgR9XzfA9xD1BXGUzfCJjYo2bpqVLDrD6bXP1QQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1440&quot;/&gt;&lt;p&gt;&lt;span&gt;蝙蝠的三维grid cell&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;6 对不确定性的神经编码（Maneesh Sahani, Gatesby Unit UCL）&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;神经系统如何通过大量的神经元编码周边信号的不确定性是一个很重要的课题，一个有意思的主题是集群编码（population coding）。这方面的研究和机器学习里variational auto-encoder （VAE）密切相关。 因为你要决策， 不仅要依靠确定性的信息， 还要靠不确定的信息， 比如distribution。 神经网络被认为具有这种编码不确定性的能力。 同时这也是机器学习的核心主题， 贝叶斯学习基础的神经网络-深度贝叶斯学习正在占据越来越大的研究空间。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;7 贝叶斯学习(Weiji Ma, New york University)&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;贝叶斯学习和上面的不确定性密切相关。 贝叶斯相关的模型可以迅速的建立同时包含数据和假设的模型。贝叶斯概率是非常基础的统计知识， 有的人只把它当成统计， 而它在神经科学的巨大潜力在于， 它可以非常好的解释行为， 以及大量之前模棱两可的现象。 把实验数据和理论做一个极好的结合。 因为通过贝叶斯方法， 你可以把现有的实验数据迅速的通过似然性转化为一个预测性模型，验证你的假设。&lt;/p&gt;

&lt;p&gt;贝叶斯模型有别其它更基础的模型，可以直接在行为上建模。你只要有先验， 有似然性， 就可以建立一个贝叶斯模型。比如你有两个截然不同的假设解释一种心理现象， 贝叶斯方法让你直接把先验和似然性（可以通过数据检测或者直接推理得到）组合在一起解决一个问题。同时， 贝叶斯方法和自编码器有很多灵活的结合， 不少新的工作围绕如何在高斯假设之外实现变分自编码器。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;8 强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;强化学习相关的主题（如果包含多巴胺）几乎占据了会议的半壁江山， 这些理论可以揭示动物的行为和决策后面的大量算法基础。 神经科学方面， 大家围绕stratum, amygdala, basal ganglia是如何配合实现这一算法展开了大量研究。 算法方面， 一些研究把小鼠海马在空间导航学习中的预演“（preplay）和&quot;回放“（replay）进行了对比。 预演很像有模型学习中的计划和模拟部分， 而回放可以对应到TD lambda算法的值函数回传， 这些算法， 都可以很好的对应到现代的深度强化学习里， 但并不是每一个AI里的强化学习算法都有很好的神经对应， 比如策略梯度。或许未来我们会发现两者是一致， 或许不一致的部分正好可以指导我们改进AI。&lt;/p&gt;

&lt;p&gt;最后， 一个有趣的研究（David Reddish）把强化学习和神经经济学（neural economics）联系起来， 让小鼠在不同的选择中权衡， 我们可以很轻易的控制每个奖赏的属性（如时间， 获取难度）， 看它怎么选择。 有趣的是， 从小鼠中得到的现象居然可以直接和人类进行对比。&lt;/p&gt;

&lt;p&gt;这让我想到， 目前的大量心理学理论，甚至经济学理论，可以通过强化学习， 与计算神经科学和AI联系起来。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;9， data inference &amp;amp; latent dynamics&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;模型分成两种， 一种叫做机理模型， 一种叫做数据模型。 所谓机理模型的核心是用第一型原理推出现象， 理解现象，比如神经细胞放电的Hodykin-Hukly模型，平衡神经网络模型， 这些往往是传统的计算神经科学模型。 而数据现象模型， 是力图用最少的参数解决复杂的现象， 似乎理解了现象，然而实际只是拟合而已，但是这样的模型有时候具有泛化能力， 它就是好的预测模型，几乎所有的机器学习模型都可以进入这一类。&lt;/p&gt;

&lt;p&gt;然而对于想理解大脑的人第二类模型是不靠谱的， 因为你又不是做股票 ， 你是想理解现象。而你确实希望让第一种模型具有第二个的能力， 因为如果一个机理模型可以预测现象或数据， 你就更加确定它是合理的，甚至可以给出更靠谱的预测。 而现在，有一些方法可以把两个模型合成成一种。其中的一大类方法基于贝叶斯推理， 因为贝叶斯可以把一个”生成模型“通过贝叶斯公式，和观测数据结合起来， 得到一个模型参数的后验概率， 事实上相当于你用数据而不是其它的拟合了你的机理模型。 然后我们可以把这个机理模型带去预测新的现象 ，验证它靠不靠谱。&lt;/p&gt;

&lt;p&gt;而贝叶斯方法经常面临的问题是先验不好给出， 似然性不好求解。 一个更加fancy的方法是直接上机器学习里的神经网络来做参数估计。 首先我们用我们”不靠谱“的机理模型通过模拟， 得到大量的结果。 每个模型参数，都得到一大类模拟结果。 这些模拟结果和参数， 就称为了神经网络的输入和labels， 不过可能和你想的反过来， 模拟的结果是输入， 而参数是输出， 这个神经网络所做的正是贝叶斯里的推测后验概率，只不过先验和似然性被包含在了模型里。由此训练好的模型， 我要输入给它最终测量到的真实数据， 它就会得到一组最后我想要的模型参数了。你也可以理解为它很像一个GAN的结构，机理模型在这里扮演了生成器的角色， 而神经网络是一个判别器。 最终生成器生成的数据要和真实数据完全一致， 一个拟合就完成了。 由此你就得到了最具有预测力的机理模型。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bccc9hay3QRibYDduvM0qry79hibkctKLCPzB7wiaPVEWEWomOqGnvyuTZw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1440&quot;/&gt;
&lt;p&gt;另一些讨论围绕RNN，本次会议提到了一个GOLD模型 (Daniel O'shea Stanford)。用RNN可以学习执行一个任务，比如决策，但是以往我们不知道RNN的神经元活动和真实的关系。 现在， 执行任务的同时我们用类似刚刚的方法让它拟合真实的实验数据（神经元活动）， 由此我们认为，得到的RNN就是我们脑网络的缩影，可以分析出大脑信息流动的基本原理。 这类工作应该对构建大规模脑网络非常有帮助。&lt;/p&gt;

&lt;p&gt;Gold 模型实际用到的结构类似一个自编码器， 一个编码RNN把和任务有关的信息， 初始条件都压缩成神经编码， 而另一个解码RNN， 则在所有这些信息基础上做出最后的决策，并拟合真实数据。这一类数据反推得来的模型， 可以帮助我们寻找数据背后的神经活动本质，这一类认为又称为Inference of latent dynamics.&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdOvdjsluQH8rtYV5xDo0bcic8XBKianDxBnGqZcrdRP9FBAW5F7qwtcHrQw6AXOAWUA9Ffe4VicPSCw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1440&quot;/&gt;GOLD模型
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;10， 寻找真实神经网络模型的神经连接&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;一些好的计算模型， 可以帮助我们找到两个真实脑网络模块之间的连接， 让我们知道它们是怎么被连在一起的。 这也是计算和实验非常紧密在联系在一起的一块。 比如这次的会议一个talk讲了初级视皮层V1区和V2区之间的功能连接可以如何通过数据推理出来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;总结：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;这次会议展示了计算神经科学的巨大魅力和潜力，以及研究的挑战。我们看到，火爆发展的机器学习的思想和方法， 已经渗入了计算神经科学的所有角度， 而对计算神经科学的理解， 也在帮助我们制定发展通用人工智能的潜在方法。 当然， 计算神经科学的作用远不止这些， 它和所有的心理学，认知科学， 生物神经科学的关系犹如理论物理和物理的关系一样紧密。 我经常惊叹某个计算理论可以如何让我们联想到一些心理现象， 这个学科的发展与神经医学的联系也是不言而喻的。&lt;/p&gt;

&lt;p&gt;然而进入这个学科的难度还是很大的， 真正要在这个领域做好研究， 需要精通数学里的高等代数和微积分， 机器学习和深度学习的所有理论， 物理里的非线性动力学和一部分统计物理知识， 要求不可谓不高。&lt;/p&gt;

&lt;p&gt;最重要的，还要有极好的思辨能力。 因为这个学科不同于机器学习的是， 你不是光得到一个benchmark分数很高的模型预测性能就可以了， 而是要真正理解一个机理的， 本质性的东西。 你的模型永远来源于真实， 又远远抽象于真实， 如何知道你的东西不是一个toy model， 而是包含了这种本质的东西？ 这种思辨力可能才是这个学科最有门槛的东西， 也是最有魅力之处吧。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383980&amp;amp;idx=1&amp;amp;sn=a519606db4453f1f0b7ff4073c2011a4&amp;amp;chksm=84f3c6edb3844ffb0925bbf29973629420751ea4637768b70c0fb0dba281070cb371d0dfbd35&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当深度学习握手脑科学-圣城会议归来&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 20 Mar 2019 00:48:50 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/MJ6IvWrnum</dc:identifier>
</item>
<item>
<title>推荐 | 一个 AI 方向的优质公众号</title>
<link>http://www.jintiankansha.me/t/fmUebDqQ2Z</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/fmUebDqQ2Z</guid>
<description>&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1620483&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;&amp;#x6536;&amp;#x85CF;&quot;&gt;&lt;section&gt;&lt;section/&gt;&lt;section mpa-is-content=&quot;t&quot;&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;公众号内容&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;该公众号的内容非常的丰富，已经原创发表 150 余篇高质量文章，共 10 多个技术专栏，覆盖以下内容：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1，《深度学习从零进阶到高手》，经过多年经验以及参考若干资料独创一条初学者的学习路线，以计算机视觉领域为例，粗分为：白身，初识，不惑，有识，不可知 5 个境界，讲述从编程，图像基础到深度学习理论和实践，一步一步晋升。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2，《方向综述》，覆盖图像分类，分割，目标检测，图像降噪，GAN，可视化，损失函数等等内容，对计算机视觉或者深度学习的某一个研究方向进行深入全面的解读。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3，《开源框架》，覆盖 caffe，tensorflow，pytorch，mxnet 等十余个框架。每一个开源框架，从简介，到数据的处理，模型的自定义，模型的训练，结果的可视化，模型的测试等进行讲述，麻雀虽小，五脏俱全。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;4，《数据理解》，覆盖各个领域数据集的介绍，数据整理与获取，数据增强，数据可视化等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;5，《模型结构》，对深度学习中的各类模型的结构进行剖析，对其适用的场景进行分析，同时也即将涵盖模型优化等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;6，《AI-1000问》，串讲 AI 技术中的一些非常小而重要，但是又容易被人忽视的问题，以实现知识的查漏补全。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;7，《深度学习理论》，细致地讲述深度学习中的基础理论，让大家更深刻的理解原理，跟踪前沿的发展，激发思考。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;8，《模型训练》，从手动调参到自动调参，应有尽有。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;9，《行业发展》，介绍深度学习在各个应用领域的发展现状，技术在其中的使用前景。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;10，《就业机会》，介绍各家公司的就业机会，剖析其关键产品和技术。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;11，《杂谈》，什么都聊一点，可能是学习习惯，可能是某个特别有意思的文章，或者一些心得体会。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;总之，内容非常丰富详尽，非常推荐！虽然目前多是 CV 技术，但是以后也会有语音和 NLP。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对了，他们还招人。实习也欢迎的。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;amp;mid=2649031845&amp;amp;idx=1&amp;amp;sn=3a5b6c04bd107eda5d12ceb3fa9bad47&amp;amp;chksm=8712bad8b06533ce24cabb6be7ad64313dc2620976d87e2bcfac88d543d17f2465de89251803&amp;amp;token=1434981917&amp;amp;lang=zh_CN&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;有三AILab成立 | 寻找合适的你&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;520&quot; data-backw=&quot;510&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/e4kxNicDVcCFW8qHIFeCDA9GDSIYCnVNlcK9kicAnVsnqaE6I5QpQKPjIiaLib6MZ7q8KdJf69e3n8qibJMkQZzNP4Q/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.01953125&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/e4kxNicDVcCFW8qHIFeCDA9GDSIYCnVNlcK9kicAnVsnqaE6I5QpQKPjIiaLib6MZ7q8KdJf69e3n8qibJMkQZzNP4Q/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 15 Mar 2019 14:53:25 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/fmUebDqQ2Z</dc:identifier>
</item>
<item>
<title>空间简史-人类认识空间的旅程与其对强化学习的启示</title>
<link>http://www.jintiankansha.me/t/9DKU2tIReQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/9DKU2tIReQ</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;9nmgo-0-0&quot;&gt;本文是对okeefe 1978(栅格细胞发现者， 2014诺贝尔奖得主)的论文 cognitive map  的总结和延申。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4mvhc-0-0&quot;&gt;一  空间的先验与后验之争&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5fivr-0-0&quot;&gt;对于我们在其中生存和繁衍的空间， 是如何在我们的心理世界表达的， 这是一个争论了几百年， 也依然没有完全清楚的问题。 如果你不去仔细思考， 你可能觉得这是一个很简单的问题。 而一旦较真， 你就会发现几乎所有的哲学家， 物理学家， 心理学家所纠结过的那些问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fut2f-0-0&quot;&gt;首先， 什么是空间？  最早探讨它的是物理学， 从亚里士多德到牛顿。 牛顿的物理学在&lt;strong&gt;绝对空间&lt;/strong&gt;基础上存在，所谓绝对空间， 可以简化为一个欧式直角坐标系， 世间的所有有行实体都可以在这个坐标系里寻找到一个坐标。有了空间和时间， 我们就可以相当准确的描述和预测发生在时空里的运动，并且进行大距离的迁徙（比如大航海）。 想象一下没有地图和坐标， 哥伦布即使偶然到达美洲也不可能回去了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.902542372881356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5eoVMy7tN5ia4Gf32QqzbydCZP5h3zYLmY0j8BnBqfomuju42nfRvSibg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;236&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.68&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pqRgj6fWpib4icGP6F9cIQLjTXk4jibGmuGvW5HQiaW8Uo4SMBp3UibuCAw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在古代， 星空是人类航海重要的坐标， 我们通过判断星辰间的指向， 知道茫茫大海自己的去向， “陪你一起看看星星” 绝非为了浪漫， 而是关乎生存。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5df0s-0-0&quot;&gt;虽然物理学家从不怀疑真实空间的存在， 然而有一个问题确没法解决。 我们的感知是含糊的，柔软的，既缺乏像尺规一样的绝对空间度量， 也没有绝对的方向度量。 我们对距离的描述经常是或近或远这样的模糊语言，也不擅长想象一个超大空间的地图（受到训练之前）。 那么， 那个物理学家关心的刚性的欧式度量的空间是从哪里来的呢？ 我们为什么能够产生这样的概念？ 是什么使我们能够产生这样的概念？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9umse-0-0&quot;&gt;换句话说， 空间如果存在， 它到底在哪里？ 它是怎么在我们脑海里形成的？ 它是通过某种先天的“结构” 得来 ， 还是通过感知基础， 在后天的学习和思考基础上形成的?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6ren9-0-0&quot;&gt;应该说对这些问题的回答绝非容易， 我们一开始解决这些问题的方法是哲学， 而后面才从生物学的认知基础上讨论。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eh1l2-0-0&quot;&gt;最早对这个问题进行阐述的人包括贝克莱和康德， 它们分别代表了两种截然不同的观点。贝克莱和我们熟悉的休谟和洛克一样是英国经验主义哲学的代表人物， 强调一切认知的基础， 无非是大量经验的总结， 它否定物理上的绝对空间，认为这是人的认知造成的一种幻觉。首先在空间认知的事情上，他认为存在等同于被感知， 而所谓的空间， 无非是我们被感知到的大量的触觉，视觉， 和肌肉运动之间的某种关联。 因而绝对空间这个东西， 根本就是子虚乌有。 大家想下大卫休谟的那句话：&lt;strong&gt;只要闭上眼睛就没有悬崖&lt;/strong&gt;， 就会理解他的观点的深刻含义。 感知所构成的大量经验集合是第一性的， 绝对的物理空间是第二性的， 是一种方便性的考量。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3454545454545455&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5ia4BdTYBLNibv3hicicIUOiaS4QCKBkNRU8eTMqLphD0zfibupQiaCyXuWhyw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;220&quot; /&gt;&lt;/p&gt;
&lt;p&gt;具有经验主义传统的英国， 出产了贝克莱和休谟这样的哲学家。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2i12a-0-0&quot;&gt;这样的对空间的认知， 与牛顿的物理学存在本质的冲突， 而另一个派别， 是结合了理性主义和经验主义的康德提出的理论， 他认为绝对空间存在，而它依赖的恰不是外部的物理世界， 而是人类先天的认知基础，一种与这种绝对空间相对应的脑组织，它是我们认知外部世界的基石。   &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2i12a-0-0&quot;&gt;康德的时空观是起纯粹理性批判的基础。康德的观点既不同于贝克莱也不同于牛顿。 首先他认同绝对欧式空间的存在， 其次他认为这个空间不存在于物理世界恰恰在我们的心理， 第三这个先验的结构是我们其它感知的基础。 &lt;strong&gt;我们的对物体的感知， 都要放到这个空间结构里得到认识。&lt;/strong&gt; 应该说这里的第一性和第二性的顺序与经验主义恰好相反。 康德的理念里， 没有了时空这样的先验， 经验毫无意义（联想以下当下 数据-经验 驱动的AI所遇到的缺乏逻辑推理能力的瓶颈， 我们无疑在某种程度回归康德的问题）。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.4984984984984986&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5q2ydezSg0X819ribLFic7zSJUSbib4wv1CqpYu3pzEmAvXwmfREJAGM8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;333&quot; /&gt;&lt;/p&gt;
&lt;p&gt;康德认为经验的认知需要在先验存在的时间和空间之上， 这也是康德思想体系的基础之一。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2s0ur-0-0&quot;&gt;在康德之后， 这两个派别分别发展出Empiricist（经验主义）,  和Natist （先天认知）两个基础流派，&lt;/span&gt;&lt;span data-offset-key=&quot;2s0ur-0-1&quot;&gt;经验主义者强调所有有关绝对空间的认知都是后天学习得到的大量感知之间的联系。 而先天主义者则认为需要有一个先验而非习得的空间结构，这个结构是后来学习的基础。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt;在后面的整个世纪里，两边各站着一批各自的哲学家，分别寻找证据阐述各自的理由。 一个比较标示性的任务是20世纪初的庞家莱。 这个时期的物理学发生了天翻地覆的变换。 爱因斯坦的相对时空开始取代牛顿的绝对时空。 而黎曼几何的出现代表我们之前深信不疑的欧式空间无非是受到了我们经验的局限。黎曼几何成为广义相对论的基础。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt;而从电动力学和量子力学衍生的场论更是刷新了人们的三观 。庞家莱在这个基础回到了贝克莱的经验主义，就没有特别奇怪。庞家莱首先认为空间无非是无数经验的集合， 这些经验主要是由人在移动时候视觉的变换构成的。 我们对不同物体的距离的感知， 也无非是让一个虚拟的自己经历一个从A物体到B物体的过程而认识到的。大量 经验上学到的位移与视野变换的对应关系可以用平移算子和群表示。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1sfen-0-0&quot;&gt; 而这样的群最大的特质是存在一个逆运算可以让当下的状态和先前的状态完全一致（联想一下时间就没有这样的对称性， 不存在一个时间平移逆运算让你回到时间的原点）。  位置的概念隐含在这种平移算子的对称性里 。庞家莱的理论不难找到同时代的相对论和场论的影子， 而他的思想标志了经验主义的新高度。&lt;/span&gt; &lt;span data-offset-key=&quot;1sfen-0-1&quot;&gt;我们在不停的变化的经验积累中得到了变化中的不变性（数学规则）， 而这些数学规则就是空间的本质。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5HseU1MBGiaNOp3ptQ909N9utJQbyghIY3wc2mORoDKIO3tloIqrL0xQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pblIushnqyL4lmicj1glPd1KhDWVbFEDP7AR175TbR1sEqyrLuiawebg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;黎曼几何， 打破了欧式直角坐标系，同样的也是对于日常经验的一个突破。 因为我们常见直线， 不说明它是真实的。事实上爱因斯坦的广义相对论指出光线被引力弯曲沿曲线传播。 黎曼几何成为广义相对论的数学基础。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9g6ei-0-0&quot;&gt;注： 爱因斯坦的狭义相对论的建立过程体现了对牛顿绝对时空的突破。事实上正是爱因斯坦看到了牛顿的绝对时空是受到了我们经验的局限才能够打破它。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4033333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5otsAHscJQBT7jGhR0JbJkhYZWQzvuIOAxHASjMrTvKuL9PyKHEM37A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;狭义相对论认为，我们的绝对时间的观点， 正是受制于我们自身的经验，因为我们从来不以接近光速运行。 而得到真实的物理规律， 事实上需要突破这种经验。 狭义相对论以光速（电磁学规律）为绝对不变， 而放弃时间的绝对流逝， 当物体的运动速度变换，其时钟也相对静止坐标系进行调整。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;egr3d-0-0&quot;&gt;而继续把场论的思维进行深化的，是Kohler等人提出的Gestalt（格式塔）理论。 Gestalt理论比庞家莱进一步的指向了空间感知的神经基础， 他把大量神经元的同时放电看做是一种场的形成， 不同的神经元组（网络）代表不同的场， 两种最基本的和空间导航有关的场一种叫做 地理场（geography field）， 一种叫做 行为场（behavior field）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egr3d-0-0&quot;&gt;地理场主要用来表征外部的物理世界-空间关系， 而行为场用于赋予各种外部刺激（感知）以意义，估值，和反射行为（这就是强化学习理论的预演，行为场可以看做强化学习的值函数），这两个场互相配合产生空间有关的概念和行为。 从外部的刺激通过神经组织合成出各种合适的“场”来表征外部特征的思想已经像极了今天的深度学习， 不难看出我们今天的科技和前人的思想的联系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a4jvn-0-0&quot;&gt;在此基础上， 1936年Lewin提出了空间拓扑结构和所谓行为场的关系， 使得Gestalt的理论变得更为坚实， 之前的行为场的一个问题是不知道它如何组织和形成， 而Lewin则提出了它的基础是各种各样的和行为有关的空间拓扑结构， 比如边界，连接， 等等。  也就是说你先建立一个空间的拓扑场， 后面可以就容易建立一个行为场。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f780f-0-0&quot;&gt;二 来自动物行为的证据&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk4an-0-0&quot;&gt;好了，再fancy的问题 ，最终还要回到空间认知的本质是个生物问题 ，它需要特定的生物载体 。  那么研究动物对空间的认知就是一个几乎不可避开的问题。 动物是不会说话的，本质上了解动物的空间认知必须要从行为入手，与空间有关的行为就是导航。 像鸟类，小鼠， 蝙蝠都具有极为发达的空间导航能力（甚至比人还厉害），那么它们是怎么在复杂的空间里穿行，或者经过几千公里回到自己的家的呢？ 从观察这些行为入手， 我们也可以得到空间认知的本质。 我们说， 如果一个概念对行为和动物的生存并无意义 ，那也就是失去了任何行为的基础。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5387755102040817&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5CGFiauhV0R2yIqFLibFg6VprTlUia5m6RVBIck4N3Qbibtz5qj8MQRenbA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;经典的小鼠走迷宫任务。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a92uh-0-0&quot;&gt;关于小鼠的导航问题的实验的问题，我们看到小鼠很容易在非常复杂的迷宫里找到食物，关于这个现象基本的假设解释， 一种是小鼠没有空间的概念，但是它可以记住一系列的动作 。这就好比一个很长的条件反射，比如左左右右左左右。 这就好比在现实生活中， 当你完成一个动作系列到达了星巴克， 你再执行另一个动作序列到达肯德基。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;aliv-0-0&quot;&gt;而另一个假设是小鼠有关于空间的概念 ，根据在大脑里生成的地图来决定每个时候的走向找到目标。 所谓地图，是指你和周围的物体（地标）以及周围的物体（地标）之间相对位置的几何。 在一个地图上， 所有的地标都获得了一个绝对的坐标， 即使你没有去过那个地方， 这个坐标依然告诉你它在什么位置。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ktiv-0-0&quot;&gt;为了研究相应的问题，我们可以把真实的空间去掉， 让小鼠在一个“时间迷宫”里（这个任务里缺乏固定的空间结构），单纯记住“左左右右左左右” 这样的动作序列来解决这个问题。 事实上小鼠这个时候已经很难完成这个任务。 这一系列的实验结果支持地图学说， 导致Tolman在1948年提出了Cognitive map的概念。 那就是 空间 或者 地图的概念在小鼠的大脑里是存在的， 成为其导航学习的基础。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e3g3n-0-0&quot;&gt;对于同样的实验现象， Hull为代表的人提出了一套截然不同的解释，可以看作刚刚说的动作序列的高级版本，解决刚刚的矛盾 。 那就是看似复杂的空间导航，无非是一个多级的，组合式的条件反射。这就和我们日常大多数习惯的获得没有区别。 只是，在空间导航的学习里， 你学到的不是一个从起点到终点的方法， 而是一个系列的能够从起点到终点的动作系列（对应同一效果的不同的轨迹），这样也就不会受困于某个特定的行为序列。这个理论与庞家莱的群论的含义是一致的。 也就是我们学到的不是一个轨迹， 而是一个行为的集合， 具有同样的最终效果（一个群）， 这其实说的就是当今机器学习的泛化能力。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f5hiq-0-0&quot;&gt;多级条件反射和认知地图均能够解释现象， 但是背后的眼里却非常不同， 这也成为后面一系列的工作的起点。&lt;/span&gt;多级条件反射， 与心理学的一个重要的流派-行为主义流派不谋而合。它的主要代表人斯金纳用非常复杂的条件反射来解释语言和思考在内的所有认知现象（把语言符号也看作一种刺激），因此在那个年代也很占优势。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7283333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5TWERwC5F7fF6lQib49o26DWyOMCVfOl8Il8icUEeusZjF1JvvEjjib8Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;斯金纳箱， 操作性条件反射的实验装置。 小鼠做出正确的动作后可以得到食物。 操作性条件反射在斯金纳的时代被认为是智能的基础。 也是强化学习理论的基础。通过多级条件反射， 小鼠不仅可以把当下的刺激和奖励联系起来， 还可以把之前的行为和刺激和当下的刺激联系起来&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7bdb1-0-0&quot;&gt;注： 稍微用心的研究者不难发现组合条件反射与深度强化学习的关系 ，我们一次又一次回归前人思想的轨迹。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rlgf-0-0&quot;&gt;而认知地图的支持者后来者居上， 一个重要的根据在动物导航行为的研究。 研究者发现在诸如鸟类这样的动物里 ，当你把鸟从一个地方移动到它所从未见过的地方， 它依然有能力找到到回家的路。 按照多级条件反射的说法， 鸟需要根据自己熟悉的地标， 记住一系列动作， 或者一个方向， 然后才能达到目的地。 而如果一个地方是完全陌生的， 那么鸟根本不可能能够根据习得的一套方案回巢（事实上这个逻辑并不严密）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9383333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5f4WicAHicGB4f4RpRStXvTwiboEJb4LhMbXibeU2GIymibkn1a8GVCw5L0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;鸟类天然擅长长途迁徙&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2r51e-0-0&quot;&gt;另一个重要的支持在于寻找捷径， 比如你回家的路上发现平时需要绕过的公园多了一条小路 你可能没有走过， 但是你依然可能会直接穿越回去到家。 寻找捷径的能力类似于强化学习里的有模型学习， 你需要建立一个最小的世界模型， 才能知道当下某个从没有见过的地标和你熟悉的地标（家）之间的联系。认知地图的支持者认为这个模型正是由认知地图提供的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ou86-0-0&quot;&gt;这些都成为认知地图作为一个先验结构早已存在于脑海中的实验支持， 不仅如此， 这个地图需要的样子是一个绝对的欧几里得坐标系，而不是你根据自己的位置为中心，设立的一张相对你而言周围物体分布的地图。 正是因为有这样一个绝对的欧式坐标系，你才知道周围物体相对周围物体， 门子相对窗子， 马路相对公园的位置， 你才能根据你的空间想象做出决策 ，不是走A路而是走B路，即使你从来没有见过A路，或者到了一个完全陌生的城市。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rids-0-0&quot;&gt;如何构建这样一个地图？ 你的大脑里的某个部位需要能够精确的进行路径积分， 并把每个看到的地标放置到这个精确积分的大脑平面图里。如果整个周围环境是固定的， 一旦出现一个新的物体， 你就很快可以想象出它和之前所有出现过的物体的相对位置， 在这个世界里， 每个物体的表示都是一个位置向量。 如果你想做一个能够行走的机器人， 不难想象也会构建一个类似的概念。&lt;/span&gt;这样的观点构成认知地图的基础， 我们通过大脑里的一个先验的绝对空间的概念载体， 而使得复杂的空间计划和导航学习成为可能。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a2m4q-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  三 Place Cell 和 Grid Cell的发现&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eete9-0-0&quot;&gt;这样的想法非常合理， 唯一的问题是我们的大脑里真有这样的结构吗？  这个观点在一组大名鼎鼎的细胞， grid cell和place cell之后可谓是登峰造极， 成为了科学的主流。 而它的发现者O'Keefe 和 Moser也获得了2014年的诺贝尔奖。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5dp4k-0-0&quot;&gt;这组细胞， 仿佛就是cognitive map的生物载体。所谓place cell的含义非常简单， 就是当你不停的经过某个同样的地点，同一个细胞会放电。  而所谓Grid cell， 其特征是其感受野对空间进行周期性的放电，它可以把一个二维平面表现成一个密集堆积的六边形结构， 不同的grid-cell具有不同的空间周期。 认知地图的支持者认为，这个Grid cell正是那个先验的大脑里的欧式坐标系的载体。如果你对空间里的一个狄拉克函数（一个空间质点的表示）做傅里叶变换你会得到一系列不同周期频率的波函数， 反过来， 这群函数或许可以作为一组表达不同物体位置的基函数。 而Grid cell如果是对应了这群函数， 那么它将可以非常灵活的表达生物体在一个绝对坐标系里的位置，即使生物体运动到了一个完全陌生的环境。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.995&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5pmcLxcOm2D3WGCjia2RAPZePreTiaeWUbd0tiadOgibULxbEhKHl03icv3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4340175953079179&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5A7P7peaOS064K3TQjVAfjDe2fDqUSneiaceuDgDcApVYUpMXInCCuZw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;341&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.555&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5npnXSEAguVVZ319icKVmAmCIE4DiaQs3TA9BWgl88lHggjicusq7wePGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不仅在小鼠， 蝙蝠的大脑里也存在Grid Cell， 与三维空间相对应, 参见 Grid cells without theta oscillations in the entorhinal cortex of bats Nature&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;egu7f-0-0&quot;&gt;在Grid cell和Place cell发现之后，认知地图的理论奠定了统治地位，空间学习需要一个先验的神经空间坐标系成为了共识。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a2nid-0-0&quot;&gt;四 人工智能时代的续篇&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eciud-0-0&quot;&gt;在人工智能时代，我们越来越发现这些早期认知科学争论过的核心主题， 事实上对发展从狭义到通用的人工智能都非常重要。你要先理解智能，才能做出人工智能， 否则做出的东西只有“人工”没有“智能” 。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2rrte-0-0&quot;&gt;在DeepMind去年发表的一篇和空间导航有关的论文里， 它们也确实把这种和空间有关的结构- Grid Cell 引入到了它们的网络架构里，而非常有趣的是， 如同当年的认知科学家所阐述的， 这个空间坐标结构的引入， 使得导航出现了类似于直接利用捷径这样的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1pob1-0-0&quot;&gt;而与空间结构的先验学派不同的是，DeepMind的这个Grid Cell 结构， 事实上是从利用监督学习进行引导的。  DeepMind 让人工“小鼠” 在方格空间里乱跑并预测其位置，在这个过程里， 如果适当的引入dropout这样的条件，它们表明就可以出现类似于Grid的细胞结构。 而这个结构正是刚刚说的寻找捷径行为的基础。论证的方法也和生物实验相同， 就是去掉这些细胞观测， 寻找捷径的行为消失了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6866666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5121JlNk1abM39bE8hurQ6uP0uVKoeKEia4xA2ibQwpdYvWydLIas8T6w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Vector-based navigation using grid-like representations in artificial agents  Nature&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ej61m-0-0&quot;&gt;这篇文章在专业圈子引起了很多批判，很多学者不认可这样形成的Grid Cell就是生物学的Grid Cell。&lt;/span&gt;另外一种可能是Grid Cell只是许多对空间探索有利的结构的一种，而这种结构恰恰是无论是自然训练还是人工训练都非常容易找到的一种， 可能对应某个自然界的最小作用原理（事实上六边形是周期性的布满一个二维空间的最经济方法）。因此DeepMind的这个作品也就没有那么神奇了。&lt;/p&gt;

&lt;p&gt;在思考这个问题的时候， 我个人依然觉得到庞加莱等人的经验主义思想具有极高的借鉴价值。 虽然用认知地图方便好用， 但是它是否是最基本的东西？  我们大脑里的那个空间概念最根本的东西究竟是什么？ 或许背后更本质的东西依然是几条抽象的数学规则，而我们大脑的神奇在于利用这个规则得到地图这类方便的概念。Deepmind按照人们已经预期设定的理论找到了同样的结果， 虽然促进了AI的进步， 但是对于我们理解这个问题却是有限的。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9md34-0-0&quot;&gt;五 关于空间任务之外的启示&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9lp2a-0-0&quot;&gt;不管认知地图是否最终成立，生物学的研究，还是人工智能的研究，都在指向的一个共同点，就是我们学习需要预先存在的特定“结构”，而不是简单的多级条件反射可以得到， 虽然在深度强化学习时代，多级条件反射给我们展示的可能性比我们想的多很多。 而AI的研究在告诉我们， 这样的先验结构， 是可以通过大量的预训练得到的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9lp2a-0-0&quot;&gt;如何预训练， 怎么设计预训练流程， 可能是未来的一个极为重要的方向。Karl Friston所说的预测误差最小，最新的大量关于好奇心的研究，甚至最近的语言模型Bert，可能都在提示我们怎样设计这样的流程。  同时，这样的研究或许也在启发我们如何更好的设计婴儿的早期教育 ，使得后期的学习效果更好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8il2h-0-0&quot;&gt;对于空间的思考本身， 对于非空间的很多任务也极有启发。 比如我们常说的语言。 我们知道，语言代表了我们使用和控制符号的能力，  而“符号” 和空间“位置”的关系是什么？ 是否存在一种隐喻， 正是由于我们发展出了对抽象的“空间” 和 “位置”的认知能力， 才引领我们走向了更广义的形成和使用“符号”的能力？ 在一个抽象的“符号” 地图里， 运动不再是欧式空间里从一点到另一点的轨迹， 而可能是一种逻辑思维的流动？  这些都将是未来人工智能极为需要回答的问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.74&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf6UicViaMRj6HicuSic8LUiajm5bKc1g86RibzW2OmNLVicH3kQMLTvr6iaGSrPTnt5iaB5kP7uYozTuzMgsQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Organizing Conceptual Knowledge in Humans with a Grid-like Code  Science   一个惊人的实验， 在人类进行对不同形状的关联（把一种形状的鸟对应到另一个形状上）的时候， 类似的Grid的神经表示出现&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://www.cognitivemap.net&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://www.cognitivemap.net/HCMpdf/HCMComplete.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383843&amp;amp;idx=1&amp;amp;sn=41e82163f76edfe5ffe31a8518d5bafa&amp;amp;chksm=84f3c662b3844f7430b27f82522dd9414d6c481f6e63822d99deb8baf281be8681ea5c4413a7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑的自由能假说-兼论认知科学与机器学习&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 13 Mar 2019 11:44:39 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/9DKU2tIReQ</dc:identifier>
</item>
</channel>
</rss>