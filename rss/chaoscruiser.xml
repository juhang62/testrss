<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>关于幸福的老实话</title>
<link>http://www.jintiankansha.me/t/Hbe5jFhY2P</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Hbe5jFhY2P</guid>
<description>&lt;p&gt;&lt;span&gt;编者的话：新年快乐，但快乐是什么了？本次推送整理了之前发的三篇关于快乐的文，其中有俩篇读书笔记，让你能在祝福别人新年快乐时，对幸福多一分思考。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;看到一个朋友的状态“孤独是人类痛苦最大的来源之一。个体的&lt;/span&gt;&lt;span&gt;孤独，往往因为心无所系。简单的排解方式是让更多的情绪来冲淡这份孤独，另一种的排解方式是找到自己的羁绊将自己的心交在他处安放。即如此，那么一个人在默默地做着自己的事情，有一份牵挂在心底，便该知足感到幸福了。”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从神经生物学家的角度看，人是很容易饱和的。人脑的快感很多来自多巴胺的奖惩机制。当一个事物超乎你的预期这种东西开始分泌让你兴奋。而当一个事物长期存在习以为常，它就逐步失去这种效力。所以人是极容易饱和的东西。有的人说吃饱饭是幸福，但是天天吃饱饭的人一定不觉。有的人说性爱是快乐，常常做的人也和吃饱饭无差别吧。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;用以前阿哲的话，人是很容易饱和的。你只要看到一种场景另你心向往之，那么你是幸福的。因为说明还有东西没有饱和，你因憧憬而兴奋因初尝禁果而兴奋。 而过早游历了名山大川的孩子不知道是幸福还是不幸。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因此你要清楚的是人的幸福从来不是绝对的拥有而是当下的充实感结合未来的希望。人生其实无外乎寄托情感，对我觉得就是创造与爱，所谓心的寄托。当他们未能够得到填补人是饥渴的痛苦的，得到成果自然不错，但是真正high还是正在创造+正在去爱，你既在今天的行动中满足又在明天的希望中满足，所谓双倍奖励。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那么想想同学的那句话就觉得很有味道了。&quot;默默的做一份事情，有一个人牵挂&quot; 仔细品味，它们都是进行时。如果我把他改成完成时 “功成名就，美人相伴” 你会觉得怎样呢？ 或者“ 不知道做什么，没有人去爱” 的未进行时你会怎么样呢？ 头一个是新月，而第二个是满月，第三个是虚空。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;满月所有最多但内心饱和，无月或有朦胧的希望但没有充实的当下与之对接，新月才是心灵的顶点。新月指的是学习的心态，人一生永恒重要的只有一件事-学习。尽力吸收自己所在环境的信息，学习所有能学的，尤其是观察每一个可以接触到的人的智慧和视野，并且不停的向所有可以触及的外界转化传递这些信息，当一个环境饱和的时候，就用尽策略转化。地点和环境给人带来的视野，实在太重要了。形成你命运转机的要素， 经常是那些你平时没有目的性学习到的。这些平时的综合积累，形成你观察问题的眼力，并在不经意间形成你命运的转机。因此，请把握每一个学习的机会，因为上帝所揭示的每一细节皆有其玄机。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;人都生活在对未来的期盼和担忧中，而幸福生活的法则，是此刻的行动与对未来的期望和担忧建立一种内在的协调一致。 而这种一致性来源于人的信念。信念是什么？相信此刻行动和未来愿景存在必然联系。这种联系不是受眼下得失影响而是终将得来。&lt;/span&gt;&lt;span&gt;当信念取得，没一点符合信念的行动都将得到快感。人的信念要具备一定的结构。每一种信念都与人最基本的欲望相联系。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;生活在对未来的期盼和担忧中，一个优秀赌徒的必要素质是在每一次错误中吸取经验，扩大已知世界的范围（贝叶斯学习）。 职业赌徒恰恰不是鲁莽之辈，他们甚至会在比赛中故意犯几个错误， 来暴露对方的信息，如果形势不妙则及时收场， 看重一次输赢的人永远赢不了，因为人生不是短跑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最高的信念来自死亡。人都是要死的，只是死期未知。假设有一个使者告诉你要在某年某月死，会对你的生活有影响吗？ 假设你得到了通知，那么那一夜你一定会仔细思考的，你感到最想要的，最怕失去的。那个时候你就得到了你的根本信仰。也许你想到你要享受肉体的青春，不叫它白白流失，那么你的信念就是寻欢作乐，那么莫使金樽空对月。如果你想到的是你要让世人彰显你的才华，为世界所追捧而不是流落草莽之间，那么你要做的是在你才华处奋力一搏，以前暂时的苦都没什么。如果你想到的是得一人终老，子女成行环绕左右，那么你的信念是找到并尽一切可能珍惜真爱。如果你从小穷极受尽欺负，你想死前一刻扬眉吐气翻身做主，那么你的信念就是要不择手段往上爬，而你不怕失去其他东西。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;胜利不是它最终有什么而是它有的是不是填补他的怨念。&lt;/span&gt;&lt;span&gt;来自生死的信念是最强大最根本的信念，和它联系的行动也是无敌的行动，发生任何什么都无所悔恨。有了高层信念，它就可以赋予下层的信念。比如得小A的最高信念是找到真爱建立家庭，他的下层信念就可以是为那个合适的人作一切可能的努力，他的再下层信念就可以是为保护那人修炼可以营生的技能而非任由自己的天赋发展。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然一个人的最高信念不可能是一成不变，他甚至一定经常变动，但是总有一些在较长时间里稳定的东西。如果你能每时每刻都信的很深，做到对自己的诚实，那么你的人生已经成功了。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 02 Feb 2019 20:02:34 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Hbe5jFhY2P</dc:identifier>
</item>
<item>
<title>正视幸福的迷雾，决绝拒绝那些通向持久幸福的百忧解</title>
<link>http://www.jintiankansha.me/t/KeTRAei49J</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/KeTRAei49J</guid>
<description>&lt;p&gt;&lt;span&gt;幸福不会因为荣誉，快乐，理性或任何其他事情被我们选择，它只因自己被我们选择。亚里士多德这样说。但这句话应该怎么理解了。假设幸福等于荣誉，那么我们会过分的追求荣誉，把幸福，这一人生的终极目标定义为一个单独的指标，只会让我们的生活陷入过拟合的状态，你得到了你的战术目标，却在你想要达到的目标上差的越来越远。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么幸福是否是越简单越好了。这取决于你得到的信息的质量，如果你能得到准确的信息，拥有充分的知识储备，那么更复杂的幸福指标意味着你可以去探索更广阔的幸福。而如果你接受的都是三手四手的扭曲的信息，那么对于你来说，幸福的确是越简单越好。所以鸡汤的盛行，对于那些不那么聪明的人来说，是一个理性的选择。他们一直把权威放在最大的最舒服的圈里。他们用比较来衡量自己的幸福，从而不自觉的使用了别人的标准来判断自己的幸福，从而耗费了太多的能量。让他们停止比较，回归简单，是让他们停止拿着哈哈镜看自己。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在&lt;/span&gt;&lt;span&gt;《拉克斯》这部柏拉图最明确探讨勇敢的对话中&lt;/span&gt;&lt;span&gt;，苏格拉底实际通过问题想指出的是勇敢既不等同与知识，也不完全是一种超越了情景的灵魂的状态。两种对勇敢的评价都会有overfitting的风险。前者无力区分智慧与知识，使他无法解释何以明智或愚蠢的坚持都不是勇敢；后者由于对不确定性的恐惧，而寄望于一种无所不能的、基于超自然力量的知识。实际上，真正的勇敢与节制、正义和智慧密不可分。勇敢之所以不能被定义，正是因此我们的德育才要有教无类，因材施教。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1990年的诺贝尔经济学奖得主&lt;/span&gt;&lt;span&gt;亨利·马克威茨&lt;/span&gt;&lt;span&gt;的成果是最优资产组合理论。但在说起他自己的资产组合时，他选择将一半的资金投入到固定收益的产品，另一半投入到股市。这样做是为了减少自己的后悔。他之所以不按照自己的最优理论去选择自己的投资组合，是因为他觉得自己无法精确的掌握股票市场的平均收益率和波动率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;达尔文在他的日记中纪录是否要结婚时。在一张纸上列出了结婚的好处与坏处，但是当这张纸被写满，他决定不在写下去。这类似机器学习中的early stopping。于其考虑过于复杂的模型，不如早点停下来，避免我们花费无意义的时间，去解释那些不可避免的观测误差。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;人们的味觉告诉我们甜的东西是好的，高脂肪的食物是能帮我们活下去的。于是我们过分的关注这些指标，于是现代的工业化食品工业造成了太多的大胖子。我们健身减肥的时候只关注体脂率和肌肉的大小，这些指标的确和心血管疾病的风险降低相关，但这也只是身体健康的不完美的代表。公司的期权激励鼓励短视现象，于是CEO便不在乎公司的长期发展。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;只有看到了普遍存在的overfitting 才明白幸福本身为什么不能被其他事物所选择。也才能明白罗素所说的“参次多态乃是幸福的本源”。你最好有多个幸福的指标，这样才可以做cross validation。有了多种判断幸福的维度，你在大部分时间里追求一个维度，但时不时换一个维度去看看自己在意的维度上的成绩是否造成了其他维度上的断崖式下跌。若没有，你获得了内心的平静，于是你懂得为什么说内心的平静是幸福最根本的判断标准。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;正是由于幸福是所有人的终极目标。因此不会存在统一的幸福，否则人类就会overfitting这个幸福的代理。就如同各个宗教都警惕对圣物的迷信会让教徒忘记了教义本身，没有人有权强迫我们认为可以使用他人获得幸福的方式来追求自己的幸福。相反的，我们每个人都有权用一种适合他自己的方式来追求自己的幸福。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;但这样做的前提是你在追求自己的幸福时，不能损害他人为追求自己的目的而奋斗的自由。为了达到这两个相互矛盾的目标。资本主义给出的答案是用一种欲望反对另一种欲望，以害处较小的激情抑制害处较大的激情。赚钱被认为是一种温和的不易冲动的欲望。商人们相信和气生财，不会有政治野心，不会追有求乌托邦的宏伟计划。因为他们要赚钱，所以他们必须和气，理性，灵活。这就是经济的文明化影响。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;经济学家赫希曼说道“在某种意义上，就像很多暴君的胜利一样，资本主义的胜利在很大程度上归功于人们普遍拒绝认真对待它，或归功于人们普遍拒绝相信它能一展宏图或取得重大成就”。只有这个社会里不充斥着拜金的思潮，大家都不是绝对的唯利是图的人，资本主义才有可能兴盛。也就是说，如果你变成了物质生活的奴隶，你就无法创造出更多的财富。这也可以去解释南北美经济发展的差异。南美的大庄园的主人把庄园看的太该死的重要了，而北方的新教徒把追求财富看成是对上帝的责任。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;《撞上幸福》&lt;span&gt;的作者吉尔伯特将幸福区分为简单的幸福（hedonic happiness）和复杂的幸福（synthetic happiness），那是一种更综合的感受，需要结合特定的情形来评估。复杂幸福远比简单幸福持续的更久。复杂的幸福同时不会收到边际效应递减的诅咒。不过要想达到复杂的幸福。首先要消除亚当斯密所说的“人类的混乱和不幸的一个重要原因就在于高估两种永久状态间的差异。例如，穷人往往高估富人的幸福感。如同安全，金钱，权利，亲情，友谊，创造，平静等能带来幸福感的抽象价值，必须放在具体的情景下，才能影响我们的情感，才能释放多巴胺。而我们的头脑，我们不完美的认知模型，只能以不确定性涵盖未来可能发生的无数情景。于是聪明人会因为知识而幸福，而信息过载则会让那些心智带宽本来就不够的人更加不幸福。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;所以幸福究竟是否是简单点好，最终还是取决与我们得到的信息的质量。从机器学习处得到的模型迁移后的猜测应和了心理学的研究。关于幸福，应有的态度不应该是”我想幸福，因为幸福能让我成名成家“。幸福是在一段旅程中体验到的情感而非物质的事件。构成幸福的一系列情感事件，正是每日生活中基本需求的满足，是指向他人的社会生活中的情感投资。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;韦伯悲观的预测”人类社会，古往今来，必定要在具有卡里斯马魅力的精神领袖和具有官僚化倾向的管理者之间徘徊。“这不是我们的诅咒，而历史的进展就应该是这样，长时间的小碎步的摸索，偶尔来个大的跃迁，从而跳出之前陷入的局部最优。站在这样大的视角来看幸福，就不该用画眉细笔来描摹什么会带来幸福，从而陷入对细节的overfitting，而应该用画写意画的手法去勾画大的轮廓，去降低分辨率，去迫使自己关注大的图景。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;正是由于这个原因，在回答什么样的生活是值得追求时。1983年哲学家Agnes Heller的答案值得在最后被强调，美好人生的三大核心要素是”基本安全保证后人的自然禀赋的充分发展，正义和人与人之间深度的情感交流“。对于需求层次处在基本安全和自我实现的普通人，实现幸福的最好方法也许是最求自由，实践正义。拒绝那些通向幸福的百忧解，拒绝用别人的标准丈量自己的幸福。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 02 Feb 2019 20:02:33 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/KeTRAei49J</dc:identifier>
</item>
<item>
<title>跌跌撞撞的幸福之路 -读《撞上幸福》</title>
<link>http://www.jintiankansha.me/t/CYpoQXnPoY</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/CYpoQXnPoY</guid>
<description>&lt;p&gt;关于幸福的书很多，《stumbling on happiness》不是一本积极心理学自助手册，而更像一本解毒书，它不会告诉你什么会让你幸福。读过这本书，引用书中的话，你会发现，你就像一个大多数人那样，认为你注定不会是大多人（If you are like most people, then like most people, you don't know you're like most people.） ，你会认为你比平均人更聪明，更有魅力，可实际上你不是这样的。&lt;br /&gt; &lt;br /&gt;可是知道这些为什么能使我快乐了？这本书的一个核心观点是，人类是唯一一种会预测未来的生物，人类相比与猿猴扩展的上脑门里装着的前额叶就是用来计划未来的，当今的神经科学也发现，人类脑部的默认模式网络（Default Mode Network）是与我们对未来信息的加工有关的。之所以是默认模式的，是指其在哪怕我们无所是事，做白日梦时特别活跃。正因为此，积极心理学之父 马丁 塞利格曼把人称之为”期望人（homo prospecteos).&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;只是我们计划未来的能力经常给我们带来不快乐，所谓苏轼在超然台记中所写“美恶之辨战乎中，而去取之择交乎前。则可乐者常少，而可悲者常多。是谓求祸而辞褔。夫求祸而辞褔，岂人之情也哉？”，这里的美恶之辩，从效用主义的道德观来看，需要倚重我们预测未来的能力，去取之择，则更加直接需要我们依据对未来的预测而做出选择。苏轼在这里觉得可乐者常少，而可悲者常多，那不怪别的，只怪他对未来的预测出了问题。等到他改变后，他便在黄州过的怡然自得。&lt;br /&gt; &lt;br /&gt;本书的另一个观点是，每一个人都有一个相对稳定的”幸福基础值“，哪怕是地震这样的天灾，几个月之后，你的辛福感也会回到自己的幸福基础值上去。为了让读者更生动的记住这个结论，这里举书中的一个例子，在电影《卡塞布兰卡》中，如果女主脚最终留下来了，那么过不了多久，她也会过得蛮好的。同样的，在苏轼在”超然台记“的结尾，也回归了他在扬州的”乐哉游乎“。&lt;br /&gt; &lt;br /&gt;这个结论在笔者大一时初看此书时，很是震撼。时隔多年，觉得这是一个本该如此的事。影响幸福感的因素，一部分是基因，这个是先天因素，一部分取决于三观，这主要靠早年的教育和你阅读的积累；而大多数人以为的幸福的来源，则多半是外界因素，比如升迁，结婚，赚大钱。这么想的人，正是苏子所说的”彼游于物之内，而不游于物之外。“，而这么做的后果，正是”自其内而观之，未有不高且大者也。彼挟其高大以临我，则我常眩乱反复，如隙中之观斗，又焉知胜负之所在“，这里翻译过来，就是我们对这些外在因素改变对我们幸福感的影响有了过高的估计。在这本书里，作者举出了数十个不同的实验来证明这一观点，苏轼则通过自我反省得出了这一结论。&lt;br /&gt; &lt;br /&gt;只把我们对未来的错误预期当成是一种既成事实，而不去思考其背后的原因，未免有些浅尝即止。这里本书的作者虽然没有点明，但借用《自私的基因》中提出的模因（meme），我们对物质的追求不是为了让我们自身幸福，而是为了让物质能让我们幸福这一信条传播下去。在当前的经济发展模式下，永远渴求更多的消费者是经济发展的源泉，广告业的存在也是维护某个产品能让你幸福的错觉。&lt;br /&gt; &lt;br /&gt;知道了这些又该怎么样了，要做的不是陷入虚无主义，而是如王安石所写的”知世如梦无所求，无所求心普空寂。 还似梦中随梦境， 成就河沙梦功德。“。你知道职位的提升不会另你更加快乐，可是你任然努力工作，这时你就不再是晋升这一目标的人质，因为你已经提前把自己的妄想“杀死了”。你努力工作，不是为了外在的奖励，而只是因为内在的原因，比如这份工作能让你学到新的知识，能让你帮助你关心的陌生人。这就是我开篇所说的，这是一本解毒的书，当前这个时代，有太多人抱怨是个浮躁的社会，这病症的根源，就在于人们没有弄清楚幸福的来源。 &lt;br /&gt; &lt;br /&gt;不过如果你能有幸看到这篇文章，说明你心里大概想明白这个道理。这里再引用书中的一句话”Our brain accepts what the eyes see and our eye looks for whatever our brain wants.”。对于那些一心只想着赢取白富美的人，他们不会明白，心理学家称为习惯化（habituation），经济学家称为边际收益递减的，我们大多数人称其为婚姻。娜拉出走了，可是她并不会因此脱碳换骨。&lt;br /&gt; &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkFXicQZGmCZPX6O6iaqSWxvqIDV89NllHQWoXTGAVdzbABzuSZrAU4MIA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;350&quot; /&gt;&lt;/p&gt;
&lt;p&gt;我们回忆时按照当下的得失重构事实，以避免认知失调，这使我们即使有丰富的经验无法转化为洞见；我们总认为自己不是一般人，以维持自己的自尊，这使得我们听过的道理和故事无法让我们过好一生；我们总是听我们预先选择的观点，以免让自己的世界观频繁的被重建，这使得我们无法从咨询别人的意见中受益。所以这本书给出的建议是，当我们预估未来时，把自己想象成一个普通人，多去查询统计数据；当我们评估过往时，不要借助自己的记忆，而要倚靠数据；当我们倾听时，不要心怀成见。&lt;br /&gt; &lt;br /&gt;寄蜉蝣于天地，渺沧海之一粟。这是前赤壁赋中的句子。前赤壁的苏子，还在感慨时间的流逝带来的虚无之感，这暗示着他还没有完全的领悟到是自己对未来的预估导致了这份感伤，若把自己看成一个平凡的人，把不属于自己的东西不放在未来的预期里，自其不变者而观之，那么又有什么可以羡慕的了，物我皆无尽。从骨子里明白了这些道理，才能用白描的笔触写下后赤壁赋里“曾日月之几何，而江山不可复识也。”。文为心声，诚非虚言。&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Sat, 02 Feb 2019 20:02:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/CYpoQXnPoY</dc:identifier>
</item>
<item>
<title>当深度学习握手脑科学-圣城会议归来</title>
<link>http://www.jintiankansha.me/t/c4o5askVfm</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/c4o5askVfm</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;6760a-0-0&quot;&gt;耶路撒冷号称三教圣地， 而它的牛逼之处绝不仅在于宗教， 如果你深入了解， 你会发现它的科学，尤其是理论创新也同样牛逼， 尤其是在脑科学和人工智能方向。 当然神族人不是特别关心最接地气的问题， 而是更关注形而上的理论框架。  耶路撒冷的脑与深度学习会就是这样一个杰出的体现。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b298c-0-0&quot;&gt;深度学习有关的核心会议， 从NIPS到ICLR 我们都不会陌生， 这些会议对深度学习在人工智能的应用极为相关。 耶路撒冷的这个会议与之不同的是， 它非常关注深度学习与脑的交叉领域， 关注它们背后共同的指导理论， 在这点上也算是独树一帜。&lt;/span&gt; &lt;span data-offset-key=&quot;b298c-0-1&quot;&gt;因为在大家忙于做应用主题的时候， 其实更需要有一些人其思考背后的理论，即使这样的思考在一个时间里不会马上促进应用， 但是在更长远的时间里， 却可能把应用推向一个远高于现在的平衡点。&lt;/span&gt; &lt;span data-offset-key=&quot;b298c-0-2&quot;&gt;就像人类在了解牛顿定律以前就能够建造各种各样的桥梁。有人可能会说我们不需要牛顿定律， 而实际上他们没有看到我们有了牛顿定律后所造的桥根本不是一种桥， 不是石拱桥，或者独木桥，而是跨海大桥。 好了，我们直接来说正事， 来总结下会议里一些有趣的内容。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c18r0-0-0&quot;&gt;脑与深度学习的关系本来就是一个高度双向的主题， 这个会议围绕以下几个核心问题：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;5kv4m-0-0&quot;&gt;1 深度学习的基础理论， 深度学习为何work又为何不work？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;acvn6-0-0&quot;&gt;2 如何从心理学和认知科学的角度归纳当下深度学习的不足？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;einic-0-0&quot;&gt;3 如何用深度学习促进对人脑的理解，包含感知（视觉为主）， 认知与记忆。 反过来如何促进AI？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e7ev5-0-0&quot;&gt;会议最大的一个板块， 在于对深度学习理论的剖析， 这个板块可谓大牛云集， 从信息瓶颈理论的创始人Tshiby 到 MIT的 Tomaso Poggio，   从牛津的Andrew Saxe到MIT的Daniel Lee， 都表达了自己的核心观点， 问题围绕的一个主线就是深度学习的泛化能力 。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;36nnk-0-0&quot;&gt;我们把这个问题分成两个子问题：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;cr9s2-0-0&quot;&gt;深度学习的泛化能力为什么那么好？&lt;/span&gt; &lt;span data-offset-key=&quot;cr9s2-0-1&quot;&gt;大家知道深度学习理论的第一个谜团就是一个大的网络动辄百万参数， 而能够泛化的如此之好, 这是非常不符合贝卡母剃刀原理的（解决同样的问题简单的模型更好），更加作妖的是， 这种泛化能力往往随着参数的继续增加而增强。 这到底是为什么?    几个不同的流派从不同的角度回答了这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vbcr-0-0&quot;&gt;1， 信息流派：&lt;/span&gt; &lt;span data-offset-key=&quot;vbcr-0-1&quot;&gt; 从信息论的角度分析深度学习， Tshiby是该流派的集大成者，也是此次的发言者。 他的核心观点是从把深度网络理解为一个信息管道， 数据， 就是入口的原油 ，里面富集了我们可以预测未来的信息， 那么这个深度网络， 就是首先要把输入数据里那些相关性最高的成分给把握住， 然后再一步步的把我们与预测信息无关的东西给剔除， 最后得到一个与预测对象而非输入数据极为相关的表征。 深度学习的泛化能力， 在于层数越深， 这种对无关信息的抽离的效率就越高， 因为随机梯度下降的训练过程， 每层的网络权重都在做一个随机游走， 越高的层 ，就越容易忘记那些与预测无关的特征， 层数越多， 这个过程其实就越快，我们能够在控制梯度消失的同时拥有更多的层， 会使我们越快的发现那个与预测相关的不变的特征本质。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHibvVQ6ufSAgBdap1gJJYuZibv8u59MexXCib8PKqoapJAtpHG6LqHwrKQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;信息瓶颈理论， 深度网络作为信息抽取的管道。&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;4bd1p-0-0&quot;&gt;2， 几何流派：&lt;/span&gt; &lt;span data-offset-key=&quot;4bd1p-0-1&quot;&gt; 这是Daniel D Lee 的talk 。从Manifold learning的角度理解 ， 深度学习的“类&quot; 对应一个在高维空间里得到一个低维流形，。这一个高， 一个低， 就是深度能力泛化能力的源泉。 这个观点的核心起源可以追溯到SVM的max margin solution。 在SVM的世界， 首先我们可以用增加维度的方法把两堆在低维世界混合分不开的点投影到高维空间， 它们就清楚的分割开来。 然后我们用最大间隔来做限制，让这两堆点分的尽可能开， 就可以避免过拟合。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4bd1p-0-1&quot;&gt;这个做法的本质首先用维度增加增强模型的容量， 然后在模型有了更高容量后我们当然也更容易过拟合。但是我们可以用最大间隔尽可能把数据”打“到一起， 事实上让每个类数据分布的维度尽可能低，这就可以避免过拟合。在深度学习的世界里， 我们每层网络都把之前的数据映射到一个新的流型里， 最简单的假设就是一个球体。比如猫和狗的分类， 就是两个球体， 一个猫星， 一个狗星。 在一个同样的高维空间里， 这两个球的维度越小， 半径越小， 就越容易把它们分开，而且可以分的类越多。  随着深度网络的层数变深， 这个趋势恰恰是每个球的维度越低，半径越小。 如果不同类型的图像对应不同的球，层数越深， 就越容易给它们分开。这个观点的内在事实上和Tshiby的信息瓶颈有异曲同工处， 大家体会下， 那个小球的维度越低是不是在抓取数据里的不变性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHlotz92qJfLtTVBgXq2iah0yn0ww38w5SwUNnaiajQqicOkbhIYyMpyqrw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;几何学派， 猫星和狗星的分离&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHJ1m2vvrv6wz2YUZg0ojdV1EM8wic1XVicKiacEJ4icNUrAw8y7X2ib74auQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHiaPE0uG8SviaXTEyI5oJBck5QdW3qQtkiabWqYiayTX7S1c3ia9SXIopoCA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;几何流派， 高维空间的低维流型随着层数变深的变化&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6topo-0-0&quot;&gt;3， 动力学流派 ：&lt;/span&gt; &lt;span data-offset-key=&quot;6topo-0-1&quot;&gt;高维空间非线性优化的本质是这种优化随着维度增长效率增加。 这是牛津那位仁兄Andrew  Saxe的talk 。 牛津例来是深度学习的阵地， 理论当然当仁不让。 这个talk从非线性优化的角度揭示了深度学习泛化的本质。  网络训练的过程， 事实上是高维空间上一个寻找动力学定点（全局最优）的过程， 每时每刻，梯度下降的方向是由当下x和y的相关性和x和x的自相关性决定的。 当优化进行到定点（最优点）附近时候， 这个相关性信息开始减少， 网络开始对数据里的噪声敏感，  因此我们需要早停法来减少过拟合。 但是， 如果我们的网络足够大，甚至这个早停都不必要我们无需提防这种拟合噪声带来的过拟合。 取得这个结论需要非常复杂的线性代数， 同学们可以参考论文&lt;/span&gt;&lt;span data-offset-key=&quot;6topo-0-2&quot;&gt;High-dimensional dynamics of generalization error in neural network&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f7tf0-0-0&quot;&gt;会议的另一个部分talk，围绕深度学习的泛化能力为何如此之差， 这不是互相矛盾吗？此泛化非彼泛化也。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b8vb-0-0&quot;&gt;1，  先天的偏见与推理的无知&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7af77-0-0&quot;&gt;先验误差导致的失灵:  &lt;/span&gt;&lt;span data-offset-key=&quot;7af77-0-1&quot;&gt;希伯来大学的Shai Shalev  深度网络可以战胜围棋这样牛逼的游戏， 然而你想不想的到， 它可能在学习乘法表的时候都会出错？  这个talk讲解了让深度网络学习并泛化一个乘法表， 然后看在测试集上它是怎么表现得。 非常有趣的是 ，虽然深度网络在训练集上表现完美， 在测试集上出现了让人耻笑的系统误差， 说明它还真的不如一个小孩子的学习能力。  这突出了反应了深度统计学习依然无法绕过统计学习固有的缺陷， 就是缺少真正的推理能力。 而这种系统误差背后的原因， 是网络内在的inductive bias， 这就好像网络自己就带着某种先天的偏见， 我们却对它茫然无知。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHyT9nICG23D5H9JLZGbaHCFWqSv0lhWmIUpIFqUnvrtVAeBDZdnhaCA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;深度学习学乘法出现的难以忍受的系统误差&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;buboe-0-0&quot;&gt;另一个惊人的talk来自于Montreal University的Anron Courville。 他围绕一个深度学习的当红应用领域&lt;/span&gt;&lt;span data-offset-key=&quot;buboe-0-1&quot;&gt;VAQ -视觉看图回答问题&lt;/span&gt;&lt;span data-offset-key=&quot;buboe-0-2&quot;&gt;展开。 这个框架的核心在于让深度网络看图， 回答一个有关图像的问题， 比如图像里有几把桌子几把椅子这种。 我们关键考察那些需要一点推理能力才能回答的问题，  比如回答完了图像里有几个桌子，有什么颜色的椅子后， 问它图像里有什么颜色的桌子。 如果这个网络真的有泛化能力， 它就会回答这个问题。 事实上是我们所设计的超复杂的由CNN和LSTM组成的巨型网络在这个问题面前举步维艰。 它可以找到3张桌子或5张桌子， 但是很难把什么颜色的椅子里学到的东西迁移到桌子里正确回答出灰色的桌子。之后我们从工程学的原理设计了一个全新的结构把这种推理能力人为的迁移进去， 会使问题稍稍好转。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggH9K9c1gAS4dzQJ3TSyhibUXrG7LLZTRHInpT0uXLk8M1BOBibjNJct6Gw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;视觉看图回答问题&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a1a8m-0-0&quot;&gt;2， 你不知道的CNN那些缺陷：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egj4s-0-0&quot;&gt;1 CNN真是平移不变的吗？&lt;/span&gt; &lt;span data-offset-key=&quot;egj4s-0-1&quot;&gt; Yair Weiss  希伯来大学计算机系的Dean给大家讲解了CNN网络最大的根据-平移不变性是错误的。 我们知道CNN网络建立的基础是它模仿生物感受野的原理，建立了一个共享权值的网络系统 ，这样不同位置的图像部分， 会共享同一种特征偏好， 你的鼻子出现在图像的顶端或下面都是鼻子。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;84nbd-0-0&quot;&gt;而Yair Weiss却想了一个方法， 证实了CNN， 哪怕你把图像向上移动了一个像素， 都可能造成它整个看法（分类）的变化。 这和那个在动物脸上加噪声看成其它动物的实验类似， 证明了CNN的脆弱性，同时动摇了平移不变的基础。 一开始我也觉得是天方夜谭， 但是看了他的整个试验后开始稍稍信服。 事实上它证实了对于最早期的CNN-neocognitron ， 平移不变的确是成立的。 但是对于”现代“CNN， Alexnet， VGG， ResNet， 这个性质却不再成立。 因为现代CNN在整个网络结构里，加入了大量的降采样，比如池化， 这些在空间上离散的降采样操作， 导致了一种惊人的脆弱性，就是平移不变的丧失。 当然， 在实际应用中， 它不够成那么大的问题， 因为你永远可以通过数据增强的方法， 来强化网络里的这些不变性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHGdibGlwIra1qJxRmsLWMMbKQVK2l5QpAATYdcMe3YonklibDhGdBIVzg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN居然不是平移不变的&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0dss-0-0&quot;&gt;2，CNN对细节的敏感与对轮廓的忽视。&lt;/span&gt; &lt;span data-offset-key=&quot;a0dss-0-1&quot;&gt; 我们本来相信CNN对不同尺度的图像特征，从细节纹理到图像轮廓， 都会同样器重并做出判断。 而事实上， 来自德国Tubingen的Matthias Bethe, 给我们展示了CNN事实很可能把自己90%的判断依据，放在了细节和纹理上。 也就是说， 它也许可能精确的识别狗和猫，但是它或许真正基于的是狗毛和猫毛的区别做出的判断。 如果你联想一下那么在图像里加入噪声， CNN就可以把熊猫看成长颈鹿的实验， 就觉得这个想法还挺合理的。  它通过它的实验验证了它的这个理论。也就是用那套图像特征迁移的网络， 把一个个图片的纹理抽取， 或者更换掉， 虽然还是猫或者狗， 里面的纹理变了， 那个CNN就彻底傻掉了。 同时它还对比了人的认知测试，看到了CNN的巨大差距。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHFLk2jgwm7orxXGCyKo3QmXHw7Yuibia8lnudYYyUibJibb6TSGn7MJl8SQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CNN难道只对细节感兴趣？&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bqh4d-0-0&quot;&gt;以上这些研究都暴露了CNN和人脑的区别。 即使是图像识别这个目前AI做的最好的领域， 这个”人工智能“ 也显得太”人工“ 了， 而与”智能“差距甚远。&lt;/span&gt;&lt;span data-offset-key=&quot;bqh4d-0-1&quot;&gt; 当然Matthias通过强化对轮廓的训练识别， 可以让它变得更像人一点， 可以识别一定的整体特征， 然而这个时候对总体数据集的识别度会变得更差。&lt;/span&gt;到这里，可以说是从深度学习多么好，到了深度学习多么差， 我们毕竟还没有掌握智能最核心的东西，包括符号推理这些， 也没有具备真正的”泛化能力“ ，  此处之后的几个talk，就是围绕这个智能的真正核心，探讨人脑有多牛逼了。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;914ei-0-0&quot;&gt;脑科学与心理学角度的智能：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b9ge6-0-0&quot;&gt;1， 有关表征学习:  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;t196-0-0&quot;&gt;来自Princeton的Yael Niv讲解了智能科学的核心-表征学习的几个关键问题：首先什么是表征学习， 表征学习的本质概念是学习一个真实世界的神经表示。它可能是从真实世界抽离出来的一些核心特征， 或者我们说的对真实世界的抽象， 而这里面，却可以帮助我们大大增强我们举一反三的学习能力。 比如说你被蛇咬了， 下一次出现运动的细长生物你知道避开。 另一方面， 我们可以把任务根据当下情景在大脑中重构出来， 比如都是讨价还价， 你碰到辣妹可能就没有那么用力了，而是开始谈笑风声起来。我们可以把从相似的任务里学到的经验整合， 或者同一个经验里学到的东西和不同的新的任务结合。   &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;t196-0-0&quot;&gt;这些都依赖于我们大脑中一套灵活的对不同任务和事物的表征系统。 这个系统我们可以管它叫任务表征系统。Yael 讲了这个任务表征系统的一些基本特性， 比如说贝叶斯证据整合，证据如何互相关联和启发（召唤）， 并把这些研究和大量心理学测试联系在一起。  这种对任务的极强的迁移学习能力， 可以从一个任务中的经验，关联到一大堆任务的能力， 是得到更好的泛化能力， 甚至走向通用人工智能的一个关键步骤。 如何能够通过学习得到&lt;/span&gt;&lt;span data-offset-key=&quot;t196-0-1&quot;&gt;这种可以迁移的任务表征&lt;/span&gt;&lt;span data-offset-key=&quot;t196-0-2&quot;&gt;也将成为重中之重。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHPpOXgnXAxicz97Y86ziaMXaSgzFrmvEIWr6W7KibBbFKBImjSGCn6FDHw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;表征学习-智能的核心&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2oc25-0-0&quot;&gt;2，  有关人类记忆的研究：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9vlfk-0-0&quot;&gt;来自哈佛医学院的Anna Schapiro  讲解了海马记忆的两个根本机制。 我们知道， 海马是人和小鼠短期记忆， 情景记忆的载体。 在海马体内有两种不同的记忆模式。 一个事短期的快速的记忆， 每个记忆由相互独立的神经元基团表达， 另一种是长期的稳固的， 某几个记忆根据它们的共性共享大量的神经元基团。 在夜晚睡眠的时候， 我们白天记住的东西一部分会从短期转向长期，另一部分则会被遗忘。 有意思的是 ， 谁会被遗忘， 谁会被增强呢？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9vlfk-0-0&quot;&gt;事实上Anna的研究表明人脑有一种非常灵活的机制， 可以把那些重要的记忆筛选出来，从短期区域走向长期区域， 而一些不重要的就像被水冲过一样遗忘掉。 这个机理可以由海马体的一个网络动力模型理解。   同时这个研究还一定程度解开人类神经编码的方式。 那些长期记住的事物为什么要共享神经元基团？ 这是为了更有效的泛化， 一些类似的事物，或任务，通过共享神经元， 可以更好的提取共性， 预测和它们类似的东西。 反过来这也表明我们大脑内的记忆很多可能是错误的， 类似的东西之间会”相互污染“  ，这就是我们为什么经常会记混或记串。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHxibOQKOELGGicNwp9icTKzINUNibn7xfUT6Op0iaxz1tia2AUHIVYl6VuibzA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;两种记忆承载的模式， 一种很独立， 一种有重合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHH0DdfziaUiaKrm0wJy2nq1gXhsj5eCvso5cwclskNjx8XZR3sfVV8agg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;海马模型&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8i75-0-0&quot;&gt;最后一个模块，就是围绕人脑和深度学习的关系， 虽然我们的最终梦想是把让人脑牛逼的算法迁移到AI系统， 但是第一步最容易实现的恰好是反过来， 如何借助深度学习这个崛起的工具更好的挖掘人脑的原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bmt4v-0-0&quot;&gt;对于这块，来自斯坦福的Daniel L K Yamins 提出了一个非常酷的研究框架， 就是用reverse eigeerneering（逆向工程， 正是我导师的领域） 研究人脑的感知系统（视觉或听觉皮层）。 对人脑视觉或听觉回路进行建模是我们一直的梦想 ，整个计算神经科学， 围绕如何用数学建模来理解这些现象 ，建立实验数据之间的联系。然而建立这样的模型异常复杂， 需要考虑的生物细节极为繁琐。 现在， 深度学习的网络给我们提供了极佳的工具去理解这些现象。我们的一个想法是用这些深度学习模型去学习具体任务，等到它学会了我们再想法来理解它。 那我说你不还是搞一些toy model 给我吗？ 谁信？ 没关系， 不是有实验数据吗， 我们先让它能做任务， 再用它来拟合我们的实验数据， 比如你先训练一个CNN来做图像识别， 同时训练好后， 你想法让这个CNN里的神经元活动能够匹配从大脑视觉皮层得到的实验数据， 这样你就得到“生物版” CNN。为了确定它是一个真正的科学， 而不是一种“形似”的骗术， 我们会用这个生物版本的CNN提出一些新的现象预测， 可以拿回到实验检验， 如果真的成立了， 这个用深度学习“构建出来”的模型， 就可以得到一个我们目前阶段最接近真实生物系统的模型。 你可以理解我们做了一个机器猫，它不仅能够捉老鼠，而且各项生理指标也和真猫差不多。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHIWgL4Pm9FEemgcJlAx4zGcNqLo1Qibz39OlTHmp8PJX0ruPbOOibICgQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;让深度网络和动物看同样的图像，并把它们的内部活动联系起来！&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3mrkj-0-0&quot;&gt;具体可以见Nature论文Using goal-driven deep learning models to understand sensory cortex。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c8gdu-0-0&quot;&gt;这一类的工作还有一个talk是如何构建一个CNN网络理解人类的视网膜系统，同样的，这个网络既有视觉信息的处理能力， 同时还能够描述生物的神经活动， 甚至可以预测一些生物视网膜特有的现象（如对未来运动信息的预测）。这一类工作可以说打通了生物与工程， 虽然人工神经网络无论在单个神经元还是在功能层面和神经元活动层面都获取了类似于真实生物系统的特性， 我们又有多大可能认为我们用这个方法理解大脑的真正工作机理， 这依然是一个仁者见仁 ，智能见智的过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4gvnv-0-0&quot;&gt;&lt;strong&gt;最后， 关于所有人的梦想， 把大脑的牛逼算法迁移到AI， 有一个talk颇有启发。 它来自于斯坦福的Surya Ganguli&lt;/strong&gt;，&lt;/span&gt;&lt;span data-offset-key=&quot;dgb9o-0-0&quot;&gt;如何让深度网络生成语义结构：&lt;/span&gt; &lt;span data-offset-key=&quot;dgb9o-0-1&quot;&gt;一个AI最根本的问题是如何沟通统计主义， 连接主义和符号主义的人工智能， 统计机器学习与深度学习代表了前两者的巅峰， 而早期活跃的符号主义目前只保留了知识图谱这样的果实残留。 事实上， 如果不能让符号主义的思维重新以某个方式进入到深度学习， 真正的AI将很难到来。 而这个方向的第一个步骤就是如何得到语义结构的神经表示。 人类的语言，可以用几千个单词表达十万百万的事物， 由于组合规则和树结构。那些共用特征的概念会被放在一个树枝之上，  而另一些则会放在其它树枝上。 这种特征层级结构， 使得人类的概念学习极为有效率， 只要直接把一个新概念放到它应该在的枝桠上， 有些该有的就都有了。 那么， 基于统计和连接主义的神经网络可不可以再现这种树结构呢？Ganguli 的研究给这个方向提示了可能， 它把学习和非线性系统在高维空间的运动联系起来，训练，就是不同的概念根据其间相似度互相分离的过程。  通过分叉等结构， 把概念的树结构和动力学空间联系在了一起。 详情请见论文： A mathematical theory of semantic development.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.41833333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfo5E3TOlbv4Atw5ym9sggHYWAp5zVM8kUDibZeiaz6Md0f8bQBibKL98OZyvS1DTlcTyg9vric4WyQpw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A mathematical theory of semantic development deep neural networks。 学习过程里的概念分离&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;这个会议， 可以说对于深度学习和脑科学未来的发展， 意义都非常深刻。 我看到的是， 尽管人们都怀揣着统一两个领域的梦想， 但现实的差距还非常遥远， 双方的沟通依然艰难。而这也更突出了这类会议的难能可贵。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6p97q-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 02 Feb 2019 07:35:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/c4o5askVfm</dc:identifier>
</item>
</channel>
</rss>