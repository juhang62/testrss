<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>Alpha Zero登上Science封面- 听铁哥浅析阿尔法元</title>
<link>http://www.jintiankansha.me/t/NNQW9Ngv1Z</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/NNQW9Ngv1Z</guid>
<description>&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; solid=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;section readability=&quot;2.5&quot;&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;从1997年“深蓝”击败国际象棋冠军卡斯帕罗夫，到2017年AlphaGo击败围棋冠军柯洁，AI 在与人类对抗训练中不断提高，而脱胎于 AlphaGo 的 AlphaZero 则完全脱离了人类棋谱的束缚，通过自我博弈，成为多种棋类游戏的王者。在最新一期 Science 中，首次全方位揭示了 AlphaZero 背后的原理。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtCy68f4biajmzchV6stRScXfF9pGthBXRAX51xOibqwd9XYlre3OqVF7SibDeicR71zoFUeJGKhpXSMibQ/640?&quot; class=&quot;&quot; data-ratio=&quot;1.2716763005780347&quot; data-w=&quot;346&quot;/&gt;&lt;/p&gt;

&lt;p&gt;阿尔法元超越自己的大哥-阿尔法狗。 这一代算法被deepmind命名为Alphago Zero， 中文阿尔法元，“元” 含有起点，创世之意。 总之，就是从零开始 ，其实这个元字用意很深， 一方面说， 这个算法是不需要人类数据指导，也不需要它哥哥（阿法狗）指导，就自己演化出来。 另一方面也可以理解为它可以开启新纪元。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dt5il-0-0&quot;&gt;当然， 同时谷歌也宣传了它的TPU， 只需要4台TPU运行几天的功夫就可以了。 那么， 这次的大新闻是不是一个谷歌精心策划的商业广告，还是真的隐藏天机。铁哥就来给大家解读一下阿法元和其背后的深度强化学习，看看这次的大新闻算不算得从零到一。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8ibcs-0-0&quot;&gt;如果大家了解计算机学下棋的事情，就会了解到几十年前，我们就已经用穷举法来解决棋类问题了，在国际象棋这类游戏里， 计算机会以比人脑快的多的速度推演两军对峙的未来，在运用零和游戏里固有的减少风险策略， 在1996年就可以让人类棋手甘拜下风。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;acaae-0-0&quot;&gt;穷举法不适用围棋，因为跟其灿若宇宙星辰的可能性搜索空间（每一步19*19可能，若干步骤后就是天文数字，这种由于可能性爆炸导致的悲剧也称为维度灾难），被称为人工智能界的mission impossible。 而在2015年， 梦幻被粉碎，原因在于深度卷积网络的幽灵终于潜入到了棋类游戏领域。 深度学习最擅长把高维度的问题自动的降维，从而解决了刚说过的维度灾难，如宇宙星辰般的搜索空间瞬间被压榨到很小，在此时的机器算法面前， 围棋无非是一个当年的国际象棋。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcps0Xjicz0kQJbhFWNb3Dev590WibnD2QZA8JbS69KEBdNIGTlzLDicZu2fQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;300&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;7ufh3-0-0&quot;&gt;然而当时立下首要功勋的深度卷积网络，却需要学习三千万组人类数据进行训练， 而整个训练过程需要的能量据说要耗费几吨煤炭。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;19ekg-0-0&quot;&gt;人们说，你秒杀人类智商的阿法狗无非是比人类看棋谱的速度快，难道还真的懂围棋吗？ 你所作的顶多是模仿，里面的强化学习到底有多少作用， 真的不知道。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;389o-0-0&quot;&gt;然而今天，阿法元却能够在不用那3000万数据的时候来个完胜阿法狗。从人工智能的技术角度看， 这是强化学习的胜利， 在不进行监督学习的情况下， 就可以达到一个高于人类的境地。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cf1o3-0-0&quot;&gt;为什么强化学习如此重要？ 让我们先比较一下监督学习和强化学习的基本思想。 监督学习， 强化学习和无监督学习是机器学习的三大框架。 某一个意义说，监督学习是给定输入和输出，机器来学习输入和输出的关系，一个好的监督学习算法犹如一个预言家， 它能够根据自己之前见过的输入输出关系来预测未知的输入。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2psih-0-0&quot;&gt;强化学习呢？ 强化学习的三元素是状态，行为和环境奖励。 强化学习条件下， 学习者每一步看到的是它决策的行为结果， 然后导致下一步行动，为了最终游戏的胜利。 一句话说：强化学习强在决策。 监督学习是预言家，强化学习是决策家。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.48833333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsV4Mu5XbPHGfej0xjDpj72EibTct0ibav9n1Zzn4icv4IWzBMoaWTpialRA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;293&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;cj562-0-0&quot;&gt;我们一比就明白， 强化学习更像是一个日常决策中的人。我们看到一个老虎，监督学习帮你识别出来它是老虎，那么你可能刚说出来就被它吃了。 而强化学习告诉你赶紧跑，你可能活下来。 &lt;strong&gt;监督学习让你成为复读机，而强化学习让你称之为生物。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.29333333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsRxyf7PuFXI3NXVG0IB5N90pmc0QIdZBnEibf4yWCUwhichupUZgjtQkQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;176&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;bfpem-0-0&quot;&gt;再深一点想，其实学习是为了生存，是赢得game of life（想想那些不太读书就能过得很好生活的真是深谙强化学习的道理）。 强化学习赋予机器以灵魂。监督学习的那些任务反而是在这个宗旨之下产生的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;80nhn-0-0&quot;&gt;回到围棋， 我们看看强化学习如何决策： 我们在好好理解一些一下“强化” 二字， 强化的意味是： 强化优势经历，反过来，就是弱化劣势经历。当你走了一部棋导致不好结果，之后被选入这一步棋的概率就降低， 而导致胜利的选择被不停的强化，直到你每次都延着最佳路径前进。这听起来很像进化， 而与进化的区别是，进化是严酷的客观环境对随机变化的生物的选择，而强化学习里的单元可以通过梯度下降主动调整策略。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a1bc6-0-0&quot;&gt;既然强化学习那么牛， 为什么阿法狗还用监督学习这个拐棍呢？一句话说，强化学习太难了！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8s35-0-0&quot;&gt;&lt;span data-text=&quot;true&quot;&gt;强化学习有两大难题：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ev461-0-0&quot;&gt;1， 奖励时间的不确定性： 今天的努力，可能明天回报， 可能十年后才有回报, 今天带来奖励的事情，明天可能就导致悲剧（比如吸毒很爽未来地狱） 对于游戏里的每一次决策，　你都无法获得立即的反馈，相比监督学习时时可以得到对和错的答案，这个信息实在太弱了， 用来指导学习，那是慢慢的（如何利用这个或有或无的信息，强化学习的一系列方法围绕而来，比如Q-learn）。 　&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span data-offset-key=&quot;2l540-0-0&quot;&gt;2， 探索与收益的平衡难以掌握： 有的人一辈子抱残守缺，７岁玩泥巴未来就永远玩泥巴。 有的人一辈子都在探索不同的方向，但是换来换去最终庸庸碌碌。而只有恰当把握探索收益平衡的，比如说27岁前读书去不同国家，27岁开始认准一个方向成为大佬，30岁前各种风流倜傥，30岁选个知书达理另一半从一而终。 强化学习始终面临是探索更多空间，还是开始用现在经验收益的矛盾。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3cja1-0-0&quot;&gt;&lt;span data-text=&quot;true&quot;&gt;这两点放到围棋这个搜索空间犹如宇宙星辰的游戏里，估计学习时间也要用生物进化的尺度算， 然而阿尔法元所用的强化学习算法，号称解决了这个问题。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ajql2-0-0&quot;&gt;仔细看它和它哥哥阿尔法狗的差别没那么大， 只不过这一次的神经网络完全由强化学习训练， 和蒙特卡罗树得融合可以算是完美。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6716o-0-0&quot;&gt;之前的阿尔法狗有策略和估值网络（都是深度卷积网络），策略负责把棋盘现在的状态转化为可能的行为概率， 这个东西被称为策略（policy，是由每个可能的行为概率构成的向量，简称策略向量） ，估值则是输入目前的棋盘状态得到最终结果的概率。 这两个网络在这一次被合成一个巨大的深度残差网络（卷积网络的一种）。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9314079422382672&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsoH0hA9BP2pujxMyw7ZHia69xRjmMAibl7JhVWWsiaCaE9FebZrEpP0NKg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;554&quot; height=&quot;516&quot; width=&quot;554&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;7nudg-0-0&quot;&gt;Nature图： 深度卷积网络计算概率&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8uljh-0-0&quot;&gt;深度卷积网络擅长整体对图像信息进行编码， 我们可以把这个巨大的残差网络所作的事情看成白日梦者对未来的总体规划。 多层卷积本身的天性决定它擅长从这种19*19的格子图像总结出意思来，强化学习的信息一旦可以训练网络，就会产生意想不到的效果。而之后MCTS蒙特卡罗树则对这种初步的结论进行实践修正。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d3a31-0-0&quot;&gt;在这里回顾一下蒙特卡洛树是怎么工作的，说到蒙特卡洛， 这是大名鼎鼎的随机抽样方法。所谓树，大家一定可以想到决策树，树的节点是某一刻的状态，而枝杈代表一个决策（行为），而这里的蒙特卡洛树即生成整个决策树的过程，通过大量的实验（犹如蒙特卡洛抽样的过程）得到每个决策行为取胜的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d3a31-0-0&quot;&gt;决策树从一个状态s出发，每个分支代表一个可能行为（a），而且有一个代表最终赢率的分数与之对应，我们选择分数最高的那个行为继续展开（下一次行动），得到新的状态，用相同的规则行动，直到游戏结束， 最终赢的走法加一分， 输的走法减一分，依次往复模拟无数次后，就会得到从s出发不同决策赢得比赛的概率。 这个过程酷似进化选择算法， 就是让那些有优势的选择有更高的繁殖子代概率， 最终胜出。虽说这仅仅是阿尔法元的一小步，却包含了著名的Q-learning和马尔科夫决策树的思想。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;qrgk-0-0&quot;&gt;我们来看每一步决策神经网络和蒙特卡洛树是怎么结合的： &lt;/span&gt;&lt;span data-offset-key=&quot;qrgk-0-1&quot;&gt;&lt;span data-text=&quot;true&quot;&gt;决策分为搜索阶段和行为阶段&lt;/span&gt;&lt;/span&gt;&lt;span data-offset-key=&quot;qrgk-0-2&quot;&gt;。假定现在我处在状态s，在搜索阶段神经网络对我所能做的所有行为（a）进行根据对未来的猜测进行预判&lt;/span&gt;，生成赢棋的概率v和策略向量p（s，a）。 当然这个预判开始很不靠谱， 蒙特卡洛树在此基础通过无数次模拟实践展开来（注意均是在状态s上），来实践出靠谱的策略向量pi（s，a）。&lt;/p&gt;

&lt;p&gt;有了神经网络的帮助，蒙特卡罗树展开不是瞎展开， 也不是从零开始，每一个树的新分支上，我们都通过神经网络给它一个是正确步骤的先验概率（P）和初始的赢率（V），代表走它通向胜利的概率。在神经网络助攻下，蒙特卡洛树可以更快的更新策略向量（每个行为选择的概率）。此时搜索阶段结束， 我们从这个策略向量里通过抽样得到我们最终进行的行为，是为行为阶段。 这下一步棋还真不容易啊！&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26666666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsiajKyaOtibGwOv1hLBtLtjgNtSAAYibPBwNaiapFvJPyWb8FFcsTOWCkibg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;160&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;77j4h-0-0&quot;&gt;Nature图： 策略更新的方法&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a1pa3-0-0&quot;&gt;最终当游戏结束的时候，神经网络的权重开始更新，这个更新的过程里，我们把整个游戏的过程分成很多小段， 比较神经网络预测的概率和蒙特卡洛树算出来的（策略向量之间的差异），以及预测结果与最终结果的差距进行梯度下降（梯度由如下公式得到，此处混合之前的策略和估值网络）。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.14333333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsWbnhR49iaFicbgX6lQ8jibSQyN8WvXlZ5cYhTkh1u7EibTbDcbDMWal7Dg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;86&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;9jrnd-0-0&quot;&gt;这样周而复始，我们可以推断，最终神经网络的预测将越来越靠谱，和蒙特卡洛树给出的分析越来越一致。 而围棋的套路也会被一一发明出来，所谓无师自通。&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8633333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsQTc3L6c3EdUyfKoVVa0CpgQciacvMYiaHYcdDGYFQaps8Q0NOrXqoJJQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;518&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;5ibte-0-0&quot;&gt;Nature图： 看看右下的图，是不是很像人类选手常用的招！  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9c2ea-0-0&quot;&gt;为什么说阿尔法元敢叫元？ 如果从技术角度看，这一次的阿尔法元没有那么多新的东西，而是在之前基础上让强化学习进行的更彻底了，然而它所展示的深度强化学习的应用未来，却是十分诱人的。&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.35&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsRGYzWDxqCfib19LOQ0gfBSD7qFIIaSQ3bAfbA6ibr02JT5uPI4Oic2wiaw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;210&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;fmjkn-0-0&quot;&gt;图： 强化学习的胜利（蓝）对比监督学习（紫）和监督+强化学习（虚线）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;987es-0-0&quot;&gt;首先，我们看到， 并不是每一件机器学习的事情， 都需要和数据，尤其是需要大量人力的标注数据死磕， 而是可以通过恰当的设立模拟器（比如此处用到的蒙卡树） 来弥补。阿尔法元不是不需要数据，而是数据都是自己模拟产生的。 模拟+深度强化学习， &lt;strong&gt;在简单的游戏规则下，一些复杂的行为范式可以进化出来，而且可以比人类设计的还好&lt;/strong&gt;， 这， 你就可以大开脑洞了。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span data-offset-key=&quot;ckimk-0-0&quot;&gt;这件事在很多设计性的工作里实在是太诱人了。 无论是设计新材料，建筑，还是衣服，&lt;strong&gt;这些可变维度很高的事物，你都可以想象设立一个模拟仿真环境，再设立一个相应的神经网络去做各种尝试，最终设计出的结果有一个奖惩函数反馈，来让这个网络来学习。&lt;/strong&gt;这就打破了深度学习创业只和手里有大量数据的垄断者相关的梦魇。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8k7da-0-0&quot;&gt;这里的深度强化技术， 也才只展示了冰山一角， 在一类被称为SLAM的技术上， 深度强化学习被证明了强大的控制能力， 它能够驱动机器人在非常复杂的空间里进行探索无需GPS，对于这一类深度学习任务， 有别于alphago的任务，因为围棋属于完全信息的博弈， 而真正的空间探索，是通过感知系统探测到的不完全信息， 通过记忆在时间尺度上的综合，这一点，只有搬出大名鼎鼎的LSTM来对付了。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6333333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpskhySEVQXKxWy56LKHsAJ0XXnA0hdiaAua0iaZrdWHaTzGDjdAO0xMQibQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;380&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;4nqss-0-0&quot;&gt;能够控制运动的深度强化学习，迟早会改变工业界，它不仅是无人车里的核心技术， 更是对话，推荐系统， 金融交易， 甚至是图像识别的利器，几乎各类需要监督学习的事情，说到底强化学习都有实力。 你如果制造一个聊天机器人， 你当然希望它能够揣测你的意图和你谈情说爱而不是背书。 你要一个推荐系统， 你当然不需要它天天给你推你刚看过的小黄片，而是带着你探索一段BBC-性的秘密。  所以， 强化学习， 是人工智能的大势所趋啊。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.66&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcpsjUU1ewb6V3XTQvWIy5IuR5tHXrIBWdnPzhAQcE4h8zY3CB0KJXVAOQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; height=&quot;396&quot; width=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;图：强化学习下的装配空间&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5631970260223048&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccPns0lxHhsOFyp82BuMcps8ic1kUm8dgUofdWt4AT4wib66t7NzYhTzVm1ribBxPmeFjgFB0jJhgjPA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;538&quot; height=&quot;303&quot; width=&quot;538&quot;/&gt;&lt;/p&gt;

&lt;p&gt;图： 强化学习下的物流车间&lt;/p&gt;





&lt;p&gt;&lt;span data-offset-key=&quot;evrgp-0-0&quot;&gt;更有甚者，我们可以设立一个具有类似地球的物理环境的地方，让配备了深度强化学习系统的虚拟生物进行各种活动，看它们能否利用这个环境发现和利用其中的物理定律。&lt;/span&gt;&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;&lt;br/&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot;/&gt;&lt;/pre&gt;



&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span data-offset-key=&quot;4csh-0-0&quot;&gt;铁哥本人的研究目前涉及深度强化学习与RNN的结合， 因此参与课程也是与铁哥结盟， 共同进军未来的深度强化学习世界的机会。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;10.284953395472703&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;751&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzq4CdJ7Pj2yE9q9ZGjIpLAQSMrWe8ricgP18icaBWjc39YTLsPLCvWsJNQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;










</description>
<pubDate>Sun, 09 Dec 2018 02:05:53 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/NNQW9Ngv1Z</dc:identifier>
</item>
<item>
<title>[原创]用算法在图像中画重点-浅谈物体识别</title>
<link>http://www.jintiankansha.me/t/rUFL39iTvc</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/rUFL39iTvc</guid>
<description>&lt;p&gt;物体检测是计算机视觉中的最常见应用之一，有极为广泛的用途。例如识别体育影像，确定哪个是运动员那个是篮球。或者你在找在桌上丢失钥匙时，就是在用人眼做物体识别，这时如果能通过机器来帮你完成，该多好啊。又比如交警通过物体识别，能够判定视频中的哪些车辆违纪。&lt;/p&gt;

&lt;p&gt;物体识别这个问题经过近三十年的发展，从深度学习之前的人工特征提取，到之后逐步使用卷积神经网络进行特征提取和分类以及候选框的确定，直到端对端的模型使用一个网络完成所有任务，从而做到了更快的速度，更低的资源消耗，最终达到了实时的物体检测。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6767241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWSVZnsefdmrnX2AQmNq1AcHFLcdqbPwRW8ZQIWKd7w0w7xnLAkylrwg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;232&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不同与图像分类任务，物体识别要逐层的在图像中画出一个个框框，比如先识别出这是一只鸡，再识别出鸡的脑袋，翅膀等。之后对框中的图像进行分类，框框中的图像要尽可能的完整的包含待识别的物体。（如上图所示）&lt;/p&gt;

&lt;p&gt;物体识别最简单粗暴的做法是在图片上滑动切割大大小小的框框，然后对每个框中的结果进行分类，从中选择那些分类可信度高的，但这样实在太慢了。在深度学习席卷计算机图像学之后，出现了一个系列的RCNN，包括基础板，加速板，升级加速版，其中的R代表regional。这里对其逐个进行详细介绍，从其进化中试图总结深度学习算法改进的一般道理。RCNN家族虽然已不是物体识别领域最新最好的方法，但其思路仍值得借鉴。&lt;/p&gt;

&lt;p&gt;ps. 对卷积神经网络不熟悉的，可以看看&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt; 复习下。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5930555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd7XTEuknZ8Iamwwa4E4a4N4tc1kjibEibfyKyDVVFpXM7xTboH4tPa3F9icp13wV9TBjVE6sk8NYsibg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;先说RCNN，这个最简单，就算将手动的特征提取换成了由卷积神经网络去做，其他的和传统模型类似，也是先生成所有可能的框，之后对每个大小形状不同的子图缩放到同样的尺寸，最后对每个框由SVM来判定是不是待识别的物体。但就是这个创新，在不改变分类器的情况下，就能够比之前最好的模型提升任务的准确度50%，下图是13年RCNN文章中给出的结果，其中红色的31.4%的准确率相比之前最好的22.6%进步很大。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/6iajibNtiaBKUzFOP3waCd0ic5WA2BRC8uCd3dage4vwic2zbqjcyMDBpS01nHEozfnj7GvW9qKp0SwJOY0EyD4ur4A/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.7912087912087912&quot; data-w=&quot;637&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从这里看出深度学习的最大优点，就是其能够比之前的方法更好的提取图像的特征，这是卷积神经网络的结构决定的，局部的信息提取以及通过pooling来实现信息的汇总。然而RCNN的分类器依旧是线性的二分类SVM，因此分类的效果不好，而且RCNN要暴力的从一幅图中生成2000张大小不同的被裁剪的图片，再根据要识别的物体种类对每一幅子图片通过SVM进行01分类，判定这幅图片是每一个待识别物体的概念，因此运行时速度很慢。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;RCNN之所以慢，是由于对于2000个候选框中每一个，都要通过卷积层来提取特征，这注定会有很多重复的计算，如果能避免这其中的重复，就可以对算法显著的加速。具体来说，利用了CNN中的池化操作，先对整张图片过卷积层进行信息提取，之后将每一个候选框内的特征进行池化操作。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5915317559153176&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd7XTEuknZ8Iamwwa4E4a4NEk51KPKdSR9MlPx4Kxic30HZicl3HqiabIWLiaATBbrickHBHibAssrgjM6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;803&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里的池化操作，为了考虑到不同颗粒图像带有的不同信息，也是层次化进行的，具体是想将一幅图中的全部特征中选出一个，再将这幅图等分成4个小正方形，从每一个中得出一个特征，最后再将每个子图再切分，最后从16个小图中每个得到一个特征。如此在池化层对不同的候选框中的信息进行汇总，达到提高效率的目的，使RCNN时一张图40-50秒的判别时间变到了2秒&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/6iajibNtiaBKUzFOP3waCd0ic5WA2BRC8uCd96hn6aOWVsqqNV4HTFwtLoYzoE1DmQ1P3eObXgcRibIJwFWpppIMMSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.37703016241299303&quot; data-w=&quot;862&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;但是fast RCNN仍然有候选框的选择这一步，而这种机械化的操作将花费很多时间，faster RCNN对此进行了改进，通过引入神经网络替代了候选框筛选（selective search）。将一整张图片通过卷积神经网络提取特征，之后通过Region proposal network对整张图中确定哪一部分对应着相对应的区域。之后通过卷积层将不同大小形状的子图的特征变为相同长度。最后再通过softmax函数进行分类，使用线性回归去微调筛选框的具体位置。&lt;/p&gt;

&lt;p&gt;Region proposal network通过在一张图片中按照给定的形状，设置k个锚点（anchor box），从而针对每个区域的子图，先预判待识别的物体在这个框中的可能性，在通过训练更新每个框的可能性的估计，从而之关注哪些最有可能性的位置。每个框一开始并没有标注待识别的是什么物体，这里优化的只是物体本身是不是在这个框中的概率，之后的网络将负责分类这个框里究竟是什么。相比于Fast RCNN的2秒，Faster RCNN的识别速度达到了0.2秒。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5398230088495575&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd9HzPsnZh98MmnamCQFayC1wDxbsHW1Nmjz0jXB9Zicbs7DOX7N3qCaAgichLE3opZIibplhf2rhlZg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;339&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Faster RCNN也有自身的问题，比如一幅图像仍然要经过多个筛选框的反复处理。另外Faster RCNN在特征提取的时候采取了训练好的成熟模型，例如Inception或Residual net等，如果待识别的物体和这些网络本身训练的东西类似还好，如果不是，那模型的特征提取就会效果不佳。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结一下。由于物体识别在商业上的广泛应用，其发展很快。RCNN家族已经早已不是最新的模型，YOLO和RetinaNet等端对端的模型通过使用一个网络完成全部任务，实现了实时的物体识别，能够在视频中使用。但RCNN的发展过程，就是神经网络一步步替代掉传统模型的过程。从最初的只替代特征提取，到用softmax多元分类替代多个二元分类，再到使用Region proposal network替代机械化的筛选框穷举。模型运行时间上取得了数量级上的进步。&lt;/p&gt;

&lt;p&gt;要想做到数量级层面的性能优化，一定要来源于一个突破性的创新。从RCNN的发展过程中，我们可以学到将具体任务拆解的方法，以及如何使用学习而不是穷举的方式来减少计算量。智能的标志就是能用最少的计算量达到相近的信息提取能力，而端对端的网络通过将一切步骤整合在一起，从而进一步提升了性能。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383465&amp;amp;idx=1&amp;amp;sn=e579b06baa00207e66f8668a0e161a23&amp;amp;chksm=84f3c8e8b38441fe71ff5765963224016e511aff653e4e5e04b61ce1c679cec5a520030764e7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;放养的深度学习-浅谈自编码器&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Sun, 02 Dec 2018 19:43:31 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/rUFL39iTvc</dc:identifier>
</item>
</channel>
</rss>