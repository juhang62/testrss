<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>让神经网络看懂图像</title>
<link>http://www.jintiankansha.me/t/C8XQvUfjg6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/C8XQvUfjg6</guid>
<description>&lt;p&gt;视觉的重要性毋庸置疑， 你可以想象，我们平时的生活， 从识图辨物， 到读书看电脑， 哪一个离不开视觉。 所谓的互联网信息大爆炸， 你看看我们手机空间里的大部分图片是什么， 一定是照片。 所以， 我们说视觉占领了我们信息的主体。这背后深层的原因是视觉相对听觉或触觉对真实世界的信息效率大的多， 一个图片可能包含很长一段文字的信息， 这点是其它渠道所不能比拟的。生物进化出视觉而有了寒武纪大爆发， 那么让机器拥有视觉能力， 一定是让它变得更聪明的第一步。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.37558062375580625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koFQJ0KyYo2twh0dBzVDic9bb8quJZRv5c5pBP6oEFiafTobNPhstZugw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1507&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;人脑对图像的认知：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;电脑记录下来的图像是由一个个像素构成的，每个像素又分为r，g，b三个通道（可以理解为垂直排列的三个像素），这三个通道起到复现整个光场的作用。而事实上物理里的真实的图象， 是一个由无数光子组成的电磁场， 这个电磁场在我们的视网膜上振动， 从而形成了我们对图象的感知。 因此， 归根到底 ，我们是用大脑， 而不是用双眼来感知图象的， 也许我们永远无法知道真实世界是什么样， 但是是我们的大脑赋予了它形象， 一个很好的例子就是你分不清猪的美丑， 但我想猪是可以的， 这正说明了所谓的相由心生， 你不关心， 就看不到。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;那么什么是我们大脑处理图象的神经基础呢？  多少代的科学家研究这个问题， 最终有了一个比较完备的答案。   一个眼睛正常的人不一定能够产生对视觉的知觉， 有一种叫视觉认知症的人： visual agnosia，   它们虽然可以看见物体， 却无法区分一个物体时什么， 课件， 看到， 不等于知道。 事实上， 大脑对视觉的感知主要时通过视觉回路实现的， 这个视觉回路的概念 ， 主要是通过视皮层V1-V4完成的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5879120879120879&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kb3jh8KiboexvaqpcbeXAAWHYPVGfrYdGsW44hGIrAbGegiadpFme1RqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;910&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 这个v1到v4的视觉回路，  本质起到的作用是一级一级的筛选视觉特征。  我们之前讲过， 每个细胞都相当于一个小的特征检测器， 而我们事实上发现， 这些小的特征选择器所检测的目标是不同的， 有的对简单的特征敏感， 比如桌子的轮廓边角， 有些对复杂的特征敏感， 比如桌子的腿或边角， 一个重要的假设是复杂细胞形成的基础正是简单细胞的组合， 很多简单细胞的输入构成了复杂细胞。  而最“复杂”的一些细胞， 居然会对那些抽象的人名，物体概念敏感。  为了表达这种极端的特性， 我们把这类极为复杂的细胞称为“祖母细胞”就好像每个人的脑子里都有那么一个细胞对自己的祖母是反应的， 它就是祖母的代言。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0314465408805031&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kOlUDudCyB33jMJ5wnvGPibNKORj7lBVRqKJp2zNgicp8RBjwEIPibGia0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;636&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;这种表面的“简单”， “复杂”其实可以被一个称为为层级编码假设的理论解释， 说的是比较底层的细胞先得到从视网膜传来的视觉信号（类似数码相机的图象）进行处理， 然后所谓的“复杂”细胞， 无非是把最底层的特征拼接组合起来， 得到比较了比较复杂的特征。而最终当我们得到的祖母的头， 鼻子， 或眼睛这些特征的时候， 在最后进行一次综合就得到了“祖母细胞”这种复杂概念的对应物。当然， 这只是粗浅版本的视觉编码机制。 很多人认为除了层级特征， 视觉编码还需要具有集群编码的特性， 也不一定存在那么一个特定的祖母细胞， 而是概念被一个细胞发放的集体模式所表达。 这些我们就不一一详述了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;模拟人脑的CNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如何把图象让机算计处理呢？ 我们可以在深度学习兴起以前， 这是一个超级超级难的问题。 我们就拿机器视觉最简单的例子： 图象分类来说。我们前两节课讲过应鸢尾花的识别， 在这个例子里， 我们看到的实际状况是花的照片来了， 然后我们的植物学家告诉我们花瓣的长度和宽度是重要的特征， 它可与把鸢尾花分为三类， 这样，我们的计算机就可以用前面讲过的KNN把花分成三类。这个方法里， 计算机事实上接受的一个表格数据， 也就是花的特征总结， 而得到一个分类的结论。 非常可惜的是， 这和真正的图象识别相差甚远。 因为真正的图象识别，意味着我们直接把图象，也就是我们看到的原始数据给计算机处理。 或者说， 计算机需要自己找出像花的长度和宽度这样的特征。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7663197729422895&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKiadibV0TFibYrLIgxtiapoYGNARzgOMy6SiaiaTUa01jNaakulibgMCDo9bA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1057&quot;/&gt;&lt;/p&gt;

&lt;p&gt;刚刚说了， 计算机眼里的图象是一个巨大的矩阵， 首先图象由像素组成， 每个像素就是一个数字， 它代表我们对信息的采样。 像素组成的图象是黑白的， 然后我们需要对不同波段的光波分别形成这样一个黑白图， 然后把它们拼接在一起，得到我们最后的彩色照片，比如我们拿一个日常的3x256x256的图像看， 那个像素就是256x256个，然后有三个色彩通道。 如此组成了一张图片。最终这个图象这样的信息维度是巨大的。 远非机器学习的常见问题可以比拟。&lt;/p&gt;

&lt;p&gt;让机器来直接看图，这个在过去看似不可能的技术，被一个叫卷积网络的东西给解决掉了，在2012 ，它超过了所有的视觉算法， 并在随后几年在很大的数据集上赶超人类。这个卷积网络正是对刚说的生物神经网络的直接模拟。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是卷积&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;你要理解卷积， 只需要理解一个东西叫滤镜， 人类在处理图像问题的时候， 最有名的发明莫属photoshop了， 在ps里你可以把图片调整各种各样的色调，模糊，锐化， 这些东西统统是一个叫做滤镜的东西做出的。&lt;/p&gt;
&lt;p&gt; 滤镜这个玩应， 你可能想到镜头前的镜片，事实上，它所做的事情是把图像转化为 一个另一个图片。 它是怎么做到的呢？ 数学上的操作，正是今天讲的卷积。数学上， 这些操作对应的运算都有一个特点， 就是对局部的信息进行综合 ，得到一个新的信息。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5936352509179926&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNMkicsrDnx2xgaJqavCTW5TiaicO92Sj5mIUn7SZ58zAFTs18pHkyicdPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;817&quot;/&gt;&lt;/p&gt;

&lt;p&gt;看看卷积的数学操作，卷积，顾名思义， “卷”有席卷的意思，“积“ 有乘积的意思。 卷积实质上是用一个叫kernel的矩阵，从图像的小块上一一贴过去，一次和图像块的每一个像素乘积得到一个output值， 扫过之后就得到了一个新的图像。我们用一个3*3的卷积卷过一个4*4的图像。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6256627783669141&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kiafx1W2wMDMpcpjib6VfT1xRjogT5ptswDhOPM6X16H3TTcteENjmpcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;943&quot;/&gt;&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积网络的基础正是这样的卷积， 我们说通过一个滤镜我们可以提取一个图象的特征， 那么为什么我们要采取看起来这么笨拙的一个方法呢？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图像识别与降维&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;其实要让神经网络告诉某两个照片是香蕉还是苹果， 并不是那么难， 但是图像识别的根本目标是你要识别整个世界的香蕉或苹果， 这个问题背后的核心是我们之前讲的泛化。  也就是让它理解苹果这个概念。当然你可能会想到苹果是红色， 圆形这种具体的特征，这些特征变化了，它就不是苹果了。 但是我今天要说的是， 你要让计算机来学到这个东西， 你要想的是反过来， 那就是， 什么特征变化了， 它还是一个苹果？&lt;/p&gt;

&lt;p&gt;首先，我们想到的是， 一张图象是跟苹果还是香蕉， 首先一定不取决于它所处在图象中的位置。 这个东西叫位置不变性， 或者叫&lt;strong&gt;平移不变性&lt;/strong&gt;。我们把这个特性直接写到神经网络里， 就是卷积。 什么意思， 卷积就是拿着一个恒定不变的小型矩阵， 一行行的扫过整个图像， 这样得到一个特征图。 你的苹果无论出现在什么位置， 对应我的卷积扫描这个行为， 事实上得到的结果都是一样的， 数学上说， 就是你的图像如果移动了5个格， 它在特征图上也做同样的一个移动， 别的什么都不变。 能够满足这种条件的运算-就是卷积。&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicEJJke9nugZB031A0zcCJglstLuNPuGjxZahlHk4qFzGDJ4aqSj2vw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;362&quot;/&gt;&lt;/p&gt;
&lt;p&gt;卷积被当成先验信息写入，每次卷积都对应一个神经元对图像的一个小块进行信息提取， 而每个神经元与输入的连接系数均是一致的，这个特性叫做&lt;strong&gt;权值共享&lt;/strong&gt;。不要小瞧这样一个简化，有了这样一个简化，我们的神经网络得到正确的解就好了很多。用一个术语就是， 我们把问题的维度减少了。 抓住一个不变性， 你就可以把需要解决问题的维度指数级别的减少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;激活函数：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;事实上完成局部特征检测这一步，我们还需要一个东西，就是激活函数， 这个我摸嗯上节课已经讲过了，一般这里用的激活函数是relu，它的作用是把一个信号里为负的部分变成0，你可以把这看成特征提取的实现，更本质的说，如果没有激活函数，我们的神经网络将是一个巨大的线性回归而已。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7770034843205574&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k3LIxRYO5HB5yXEjicsicJ68fOBCvKOMSqnqQBZFvuMW6LOyZV1a1mibJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;287&quot;/&gt;&lt;/p&gt;

&lt;p&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;ReLU函数是小于0是为0，大于0时为自身&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;什么是通道：&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;我们以一个手写数字的历程为例，讲讲我们还需要什么，首先我们说， 一层卷积对应一个特征， 但是，显然一个特征是不够识别的。 就以识别数字为例子降价给你这个问题， 比如你要识别10个数字，仅以1和7为例子。显然识别它们的核心方法就是条纹走向。横线是一个特征，竖线是一个特征。如果一个卷积对应一个特征，那么我们其实需要两个卷积，让一个卷积核可以识别横向条纹， 另一个卷积核识别纵向条纹， 这个操作就可以。&lt;/p&gt;

&lt;p&gt;这样的操作，使用如下的3x3卷积就可以了： -1，1， -1  ，  这样的算子具有和之前提取梯度的运算差不多的样子。只要两种卷积核可以做到这点， 然后，如果我们把这两个卷积核组成一个小组扫描一个特征， 那么我们就会知道每个图像小块上的横竖情况。&lt;/p&gt;

&lt;p&gt;比如这时候我们得到每个图像小块的一个特征编码，一共有四种情况 （0，0），（0，1），（1，0）（1，1），横线对应（1，0）竖线对应（0，1）， 你是不是可以把整个数表看成一个新的图像 ？ 而这个新的图像里的变化从（1，0）到（0，1）是否相当于一个角度呢？ 这就是比条纹走向更高级的一个特征。 怎么提取它？如果你的答案是再放入一层卷积， 恭喜你答对了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4789272030651341&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kxd2XRKOic2My0D0qappe2qgI8L3KYvLk6khicEElAict1x1SKCCQWzXxg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;回顾整个过程，我们要做的无非是在第一卷积层的位置上， 放两个并行的卷积核， 一个核处理横向条纹，一个处理纵条纹， 得到两组不同的特征， 最终我们在前面的两个特征之上读取这组新生成的特征图之上的特征。 下一层卷积寻找的上一组卷积的特征组合。这个操作对应的是在两张并列的图层之上，在它们的同一位置识别信息， 如果两个警报器均响了，说明夹角存在，  我们依然可以用一个3x3卷积网络来完成这个操作，这个新的卷积建立在之前的纵横两组卷积之上，对原先的横纹和纵纹组成的特征空间进行操作（因为这里的维度是2x3x3，最单纯的情况我们也可以用一个2x1x1的一个矩阵综合两个特征）。  因为这个时候， 对之前平行卷积的结果做一个综合， 以及形成一个特征之特征， 即横向和竖线交叉的特征。&lt;/p&gt;

&lt;p&gt;这样的方法无论手写数字出现在什么位置，  我都给你找出来。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;从两层到多层：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚说 kernel就是通过计算小区域内像素的关系来提取局部特征，而最常用的卷积核大小是3x3， 那么这里的一个问题是， 为什么要这么小， 为什么要提取一个局部信息？我们说因为图像这个东西里包含的信息具有以下特点： 最底层的信息，比如边角轮廓， 都存在于局部之中， 只有更上层的信息，比如物体的概念， 才会用到更多部分的信息， 而这种跨度又是逐步发生的。那么，如果实现这种跨度呢？  答案： 多层。局部特征，在更高层上被组合， 会变成整体特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42369186046511625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kKqsX2xB7m1ZnmrTCNuF12RThK0PURyTC5mgzPrDDw2PZpEcS3SF0vw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1376&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  首先，我们把每个神经特征所提取的特征区域， 叫感受野，如果我们始终只能用的其实都是3x3这样的小卷积核， 我们能不能让感受野扩大呢？ 答案是， 可以。 这里的关键是一个叫池化的造作。&lt;/p&gt;

&lt;p&gt;最大池化所做的是事情，是把每四个相邻神经元得到的数值取一个最大的， 其它全部扔掉。每次卷积后如果经过这样一个操作，那么图像就会缩小到原先的四分之一，而再次之上的相邻四个像素， 对应了原始的16个像素， 从而使得感受野迅速扩大。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5582706766917294&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kA57IKwRcHrqUjq6SVb77Sd3ibgSvvcDxNOzRHgaBrmiazv7e1Yrb209g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1064&quot;/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;Pooling的本质依然是降维，或者过滤冗余信息，这个就是pooling。背后能够这样做的理由是，局域特征特征是大量冗余的 ，经过条纹提取的数字一定在大量临近区域里的数值都一样。 冗余踢去后， 经过pooling， 上层细胞得到更大的感受野，也就抽取了更高层次的特征。&lt;/p&gt;
&lt;p&gt; &lt;br/&gt;&lt;/p&gt;
&lt;p&gt;卷积层 ，激活函数，pooling帮我完整的特征提取到剔除冗余的过程 ， 这可以称为卷积网络的三明治， 把这个结构不停迭代，我们可以构建一个很深的网络。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.363479758828596&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMARlAiaha7Tp0OAyibiclNr6c5R7PS7uCicOZjRjCkZP35apJUxgq2rJnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1161&quot;/&gt;&lt;/p&gt;

&lt;p&gt;深度意味着什么？  我们想一下， 要正确的识别一个图像，你不可能只看边，也不可能只看角， 你要对图像的整体有认识才知道张三李四。 也就是说我们要从局部关联进化到全局关联， 真实的图像一定是有一个全局的，比如手我的脸， 只有我的眼镜，鼻子耳朵都被一起观察时候才称得上我的脸，一个只要局部，就什么都不是了。如何提取全局特征？ 从一个层次到另一个层次的递进， 通常是对上一层次做横向以及纵向的整合（图层间的组合或图层之内的组合或两者）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4612005856515373&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kicPBaImgyWqzuJNtkEKMbrIrkzPSdp6UWZyq9NgShHPJb3P6MtCmibjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot;/&gt;&lt;/p&gt;

&lt;p&gt;我们刚刚讲了CNN如何找到边角的过程， 但是它的下一层会是什么？再下一层会是什么？ 我们头脑中的想象力已经不够了。我们只能做让学习得到结构，然后去观测。我们可以把每组卷积网络看做一组基，我们在这组基上重构我们的信息， 就和线性代数里坐标变换相似，只不过非线性更复杂。 每一级别的网络都是一组新的基底，我们把刚刚的全局换一个词叫抽象。深度卷积赋予了神经网络以抽象能力。 这样的一级级向上卷积做基变换的过程，有人说叫搞基（深度学习就是搞基），深一点想叫表征， 和人的思维做个比喻就是抽象。 抽象是我在很深的层次把不同的东西联系起来，CNN教会了我们实现抽象的一种物理方法，  他也体现了在一个空间尺度上我们所能够达到的特征工程。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最终分类：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里我们还差最后一步没讲， 整个CNN网络如同一个等级社会里，最上层的，就是君王。 而这个君王，与直接其下的一层（议会）的关系，事实上往往是全连接网络。为什么，因为这时候君王要做的是最终决策， 它不在“搞基”提取特征了。一个非常复杂的问题，已经在此时变成了线性可分的简单问题。 决策 – 就是做一个线性分类， 得到我最想要的结果。  我们要做的是返回一组最终可能结果的概率。如果得到可能结果的一组概率？ 我们搬出基于最大熵模型的softmax  gate ，这也是正是CNN网络做分类的最后一层。 至此，我们可以得到众多识别物体的抽象信息。 那个概率最大的，即使我想要的结果。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5389435989256938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kF6KG6kGBVeH4Zwib3bCIeFwOrMQygTJ8ckUDyfZHkLTCEKfHmgbfSlQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1117&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7400318979266348&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kByPbeQrjLRAFZMt2UBqXLDompmohkqQN7fjYMqSlaeWfxI0jZaQ2og/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;627&quot;/&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;你所不能不知道的CNN&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作者简介：微信号：ironcruiser 法国巴黎高师物理硕士 ，以色列理工大学计算神经科学博士，巡洋舰科技有限公司创始人, 《机器学习与复杂系统》纸质书作者。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 02 Mar 2019 11:40:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/C8XQvUfjg6</dc:identifier>
</item>
<item>
<title>基于一张“规则表”的人工智能</title>
<link>http://www.jintiankansha.me/t/WW3EirRLSj</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/WW3EirRLSj</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;4ds8l-0-0&quot;&gt;我在过去对人工智能简史的描述中，把人工智能的整个历史描述成围绕一张规则表， 本文是基于这一想法的总结和扩展。我们说，早期的AI发展史围绕如何人为构建这样一个规则表解决复杂问题， 而当下的AI则围绕如何让它在复杂的现象中自己归纳出这个规则表。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bkglj-0-0&quot;&gt;我们说，上帝通过制定规则从简单演绎出复杂。最初的原始人类在黑暗中摸索， 在众多的现象不知所措， 只能通过设立各色大神小神来缓解自己对不确定性的恐惧。 从多神宗教到一神宗教的跨度体现了一个从复杂中寻找简单的跨度， 这可能是基于一种隐隐的直觉，就是现象虽然多样， 但是背后的法则不应该如此复杂。 到了科学的时代， 这种思维在物理学里淋漓尽致起来，四大力学， 把分子原子间的作用力统一到电磁力， 把宏观物体的作用统一到引力和经典的动力方程， 已经是极致。 而后面的对这两者的统一构成了从广义相对论后的现代物理主线。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibembXL81evOfRdy0cTz2qqulnss6iaFGmXLmdjuRu9GiayK86XIM1TVSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图: 简单规则生成复杂的极好例子， 元胞自动机， 每一步细胞的繁殖和扩散方法一定（左图黑格表述的， 从上一行到下一行的变化法则）， 它最后形成的图案就定了， 规则可以很简单， 图案可以很复杂。 &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;早期的智能：  制定规则表 - 迭代&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1j7g-0-0&quot;&gt;另一方面，这样的思想从智能科学诞生之出，也贯穿出来。它在早期的可计算性中， 通过图灵机的构建。它认为存在这样的通用机器，能够和人类一样解决问题， 即使过程非常复杂， 你无非需要四个要素： &lt;strong&gt;1， 输入 2， 中间状态 3， 规则表 4， 输出。&lt;/strong&gt; 并在时间上进行大量迭代， 就可以实现这个过程。 通过这个过程， 我们可以把一个输入转化为一个想要的输出。 如果我们能够在在有限的步骤里将一个输入转化为一个输出，据此解决一个实际问题， 那么这个问题就是可计算的。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6114081996434938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBnC8zkibScsiccAOwoSH4Ld9vdiamljakNVOX5TKF69ywm3Qq2ic3IdhUw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;561&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图： 图灵纸袋， 一个在纸袋上根据一定规则表行走的机械昆虫， 机械昆虫具有一个内部状态， 并接受外部的输入，最终通过规则表找到对应的下一步输出。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c00nl-0-0&quot;&gt;而冯诺依架构让它变成一个技术现实。 它通过可以存储程序的机器， 让人们通过把这些图灵规则表的指令变为计算机二极管的开合代码， 而让图灵纸袋的思想成为了一个每个工程师可以设计的现实，从此有了程序和程序员。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;一个计算机程序， 最基本的部分包括一些简单的形式逻辑， 包括逻辑与或非， if else， for 循环这些。 其实本质上， if else 所描述的就是规则表， 规则里面通常涉及简单的逻辑， 最终通过for循环， 我们就可以得到我们要的东西。 比如一个中学生都会的排序算法， 我们无非需要做的是前面和后面的数比较大小， 然后一个if else进行换位， 最后一个for循环， 多长的序列都可以瞬间搞定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.40556900726392253&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibibEBCIwsAHiaGwDkFcYUeGLqHsuPP0ibBA7ibPX7OxPPq87Jr0NAtFjSibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;826&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b1ct2-0-0&quot;&gt;这就是用程序解决问题的核心思维， 给你一个再复杂手忙脚乱的问题， 只要这个问题可计算， 那么我们只需要设定好我们需要的规则表， 在有限的步骤里迭代， 最终机器总会给你解决。&lt;/span&gt;比如魔方问题， 一般的聪明小孩都很难在短时间解决问题， 但是， 事实上解决魔方问题有一套非常整齐的规则表（你想象打乱一个魔方其实比较容易的， 把它弄整齐是打乱的逆运算，但是破镜重圆总是难的）。 如果按照这个规则表执行若干步， 再困难的魔方也给你整出来。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibz3zliaRoh4oEWf9QQOX3VMpbMFylumcHY7novvRnic1j7aN7R7pohRHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;676&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44333333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBqmPPsMKMLBUUyKWYnVsiaibg6hmZP9cADL9iak5yxuibvmosIG5ZUWbuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e13hv-0-0&quot;&gt;我们说规则表， 加上迭代等操作的思路可以解决大量的工程问题。我们曾经认为按照这样的思路我们可以解决整个智能的问题。 只是填入一张越来越大的表格。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;但是它在通向智能的关键位置， 却停住了， 这个元凶 -就是- &lt;strong&gt;不确定性&lt;/strong&gt;。日常生活中很多东西无法轻易的总结出规则表来， 因为细小的规则实在太多了。 你可以想象我们有无数尺寸和规格各不相同的螺钉螺母。 每一种规格我们都要想一条if else，可悲的是这些螺钉和螺母几乎没有哪两对完全相同， 穷尽一个程序员一生也写不完这些程序。现实生活中的大部分问题属于这一类问题， 比如你无法轻易的写出一段程序来判断A男和B女是否合适结婚。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中期的智能： 让机器学会归纳规则表&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;9mrot-0-0&quot;&gt;统计机器学习 - 机器判断规则&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6pfmt-0-0&quot;&gt;这个问题的解决方法十分自然又十分了不起：  能不能让机器自己学会这个表格， 而不是认为设定它呢？  这就是整个智能问题的第二步 - 学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eeh10-0-0&quot;&gt;整个学习问题的基石其实是古希腊人提出的归纳法和演绎法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-0&quot;&gt;伟大的希腊哲学家早就对学习的本质展开过探讨，它们把学习分类为&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-1&quot;&gt;归纳法和演绎法&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;。所谓演绎法， 就是从用一定规则进行推理的过程。 苏格拉底是人，人都是会死的， 因此苏格拉底会死。这就是三段论， 或者称为演绎法的根基。 而真正学习的过程，是这个演绎法的逆过程。我们先知道一个特例，然后通过特例，得到这个“人都是会死的” 知识，再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibmSX7rgCyJJfTKEhApOyARbIowfAnPvZVr0gFicQTQYD7WgfkLMcPbZw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;让机器实现归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6nhn2-0-0&quot;&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。 我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？ 组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibeB6eia8YghicHCagGO02XX9WcmpmicrD1ya9UNmpAlIqbbygWzLKiarAVQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;1bdeq-0-0&quot;&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！比如机器收集到一个苏格拉底死了， 那么苏格拉底是什么？ 男性，白种人， 哲学家， 于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！ &lt;strong&gt;我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibCChMBjNz3U3sMtU9kp7SpWibY4ocjf7OhopFnViaibcnR1aYIDpYKLaVg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;整个有关统计的机器学习， 都可以看成让机器学习有效归纳的方法， 从数据里得到规则表， 再用规则表进行判断。前面的过程叫训练， 后面的过程叫测试。 &lt;/strong&gt;&lt;span data-offset-key=&quot;9nnub-0-0&quot;&gt; 如果这些规则是有关一个是非的命题， 它就是一个分类器， 如果它是一个连续数值的预测， 就是回归。 但是规则表的本质是不变的， 它就是让你填表，表格的横排和竖排已经有了， 一个叫特征， 一个叫实例。 特征是人为归纳好的， 而实例是我们人为收集的， 表格中有些地方是空的，  就是我们想要判断的东西， 需要机器来填的部分。 比如给你一百幸福和不幸的人的案例， 让你判断第101个人的情况。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7663197729422895&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibGeQKfN4oT43Gf7qPQGibnceOkVZRMtllFgcAHBwVIocWdibkMkFB9V1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1057&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;图：机器从实例中学到分类的方法： 机器学习&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;81s85-0-0&quot;&gt;刚刚的那个例子你应该已经体会到， 这个命题验证过程其实是一个组合爆炸的问题。我们把关于这个世界的互相矛盾的假设都丢尽机器。即使最简单的问题也会有无穷多的情况要判断 （特征的n次方）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib6XNRA36VEFMtlPlZQ2RpPXtla5u66LIcDmhkLUQuPecqg0IVicuibIwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ajopm-0-0&quot;&gt;而机器学习的各个算法， 让我们通过加入更多的假设， 来偷懒解决这个问题， 此处没有比决策树更典型的， 它的高阶版本xgboost成为机器学习竞赛的杀手锏。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;而决策树得核心智慧就是优先级算法简化命题数量。 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心。事实上， 我们永远在抓轻重缓急，在抓主要矛盾， 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。 每次pass， 就减少掉了一半需要判定的命题。 通过这种预设的二叉树逻辑， 一个本来需要2的n次方的步骤解决的事情， 可能只要n步了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7453703703703703&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibq5oBakMtQKgHb2Akq4jNYicdNNniarAhbtJNpWaJ7qOkbJDDKpTTB9MQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;具体如何来学，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;如此，就会形成一个树的结构。构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？我的要求就是， 每一次分化，我们都希望取得最多的信息，如分叉后一个树杈全是yes，一个全是no就是最好的效果， 如果达不到， 也让它尽可能接近这个效果。  这样一个一个问题问下去， 最终达到稳定后过程停止。  这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk0ce-0-0&quot;&gt;另一个得到这样的一个规则表的方法是线性假设。 线性分类器通过假定特征之间的相互独立， 使得命题的成立与否可以通过一个加权求和的关系表达， 即f=wx+b 。最后f如果大于0就是是， 小于0就是否。 线性分类器也是一种特别符合人认知习惯的模型：一般人在决策时候做的事情就是加权平均，比如你平时做分类（决策）， 你最想的一种状态是什么？你要把几个核心的要素放到一起， 按照他们的重要性加和，比如你今天要不要去看电影，可能取决于你的女朋友free否， 下不下雨和电影好不好看， 这个时候，我们可以把这些因素加权在一起， 在和一个我们给定的阈值做比较，大就去， 不大就不去， 这正权衡得失的做法， 就是线性分类器。线性分类器看上去是一个数学公式， 本质还是一个规则表， 只不过这里要学习的规则无非是每个特征给多少权重。最后在表格最后一列得到yes or no。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9387096774193548&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibm99R31Z3g5uTib3k3mrK8kME4XDCicJ6HgXW6a31dQaFa9RABR3z7VHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图： 线性分类器， 一条直线代表一组权重， 把两组数据分的越开越好。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;具体学习的过程， 我们从实例里归纳出每个特征对应的权重参数，然后进行判断。 只要参数都确定了， 也就是一次解决了所有的问题。 线性分类器的高级版本SVM已经超越了线性假设。 也是小数据下生成有效规则的大杀器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;近期的智能： 让机器生成有效的规则 &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8te9f-0-0&quot;&gt;连接主义机器学习， 产生规则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;fk6j-0-0&quot;&gt;刚刚说的那一套， 有一个问题你有没有注意到？ 我们最先提出的问题是让机器产生一个规则表， 而刚刚说的统计机器学习里， 更多的是让机器根据定好的特征收集数据进行命题判断。 这其实离我们说的让机器自己得到规则表只进行了0.5步。&lt;strong&gt;大家想象下， 在真正的实践活动里， 你无法一开始就设定出一堆特征让它进行逻辑判断，在这个情况下如何得到我们所说的“规则”呢？ &lt;/strong&gt; 如何让机器自己生成战胜“复杂”的程序呢？ 连接主义机器学习在一定程度解决了这个问题。  因为， 人类认识事物，生成规则， 其实是通过“&lt;strong&gt;概念&lt;/strong&gt;”来的， “概念”是一个浓缩的信息载体， 通过它我们能够进行任何更复杂的推理。 那么“概念”是如何生成的呢？ 它的载体正是下面说的联结主义的代言人神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5gsn-0-0&quot;&gt;神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dijog-0-0&quot;&gt;首先，神经网络是由神经细胞组成的。  一个神经细胞就是一个最小的认知单元， 何为认知单元，就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习，它就是刚刚说的线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cr2lq-0-0&quot;&gt;正确的分类，是认知的基础，我们对事物的感知比如色彩，物体的形状等，其实都是离散的，而物理信号是连续的，比如光波，声波。这里面的中间步骤就是模数转化，把连续的信号转化成离散的样子，这正是一个分类器干的事情。一个单个神经元可以执行一个简单的基于感知信号的if else语句。 先收集一下特征做个加和，if大于一个值我就放电，小于我就不放电，就这么简单。晶体管当然也在干这个事情。 神经细胞与晶体管和计算机的根本区别在于可塑性。或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib2NvE4HCXevrTz3VAnXzfIxaOZibibe1YnYGN84NF1XWqyKpNdsISR2aA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3f6fb-0-0&quot;&gt;我们这个简化出来的模型，正是所有人工神经网络的祖母－感知机。从名字可以看出，&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-1&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-2&quot;&gt; 它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型， 它的创始人 R 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2b1gb-0-0&quot;&gt;单个的感知机并不能比传统的机器学习多做一丁点的事情， 还要差一些。 但是把很多个感知机比较聪明的联系起来，就发生了一个质变。&lt;/span&gt;首先， 每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示，左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。所以， 多个神经之后，我们还要在头顶放一个神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总，&lt;strong&gt; 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，把哪些被直线分开的四分五裂的块，就可以得到一个非常复杂的判决结果。 &lt;/strong&gt;你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。 其实这里面做的事情， 正是基础的逻辑运算， 一个简单的一层神经网络可以执行与或非这些基本的逻辑操作。事实上它的本质就是把简单的特征组合在一起形成一些原始的概念。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5758323057953144&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibuABrESypc6M08wlHD6dia0vL7GRLUKEQiby0Bicc47icreRupKWw5IHVVQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;811&quot; /&gt;&lt;/p&gt;
&lt;p&gt;它是怎么做到的呢？ 学习。 生物神经网络的学习， 是通过一种叫做hebbian可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是， 我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了一个简单的“概念”。 上述过程被总结hebian学习的一个过程。  我们可想象，一个两层以上的神经网络， 就可以表述香蕉， 苹果， 菠萝这些水果了， 它们无非是底层特征颜色，形状的不同组合而已。 而这些不同水果的概念， 就可以帮助我们形成更加复杂的规则表 ，比如让它根据客户的信息帮它推荐一个水果拼盘。 由此可见， 神经网络通过与或非进行简单特征的组合 ，再通过if esle进行判断选择合适的特征得到概念， 再通过下一层迭代得到概念有关的命题。 就可以生成比之前的传统机器学习复杂的多的规则表。而且我们可以想象出来， 迭代的层数越多，它生成的“概念”和“规则”就越复杂。  &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;当然真实训练中我们用到的不是模仿生物版本的hebbian学习， 而是强大的多的反向传播算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.6111111111111112&quot; data-w=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span data-offset-key=&quot;dmb6s-0-0&quot;&gt;事实上为了让这种生成“概念”得到“规则”的方法更加有效， 我们会加入一些无比强大的先验假设。 其中最有名的一组，  就叫CNN，它所做的，其实是针对于图像这类巨大无比而局部特征不断重复的数据形式， 你可以写一个循环， 来让你的程序更有效。 循环里的每一步都对图像的局部特征进行提取， 由一个可以共用的卷积核实现。 卷积核一点点的卷过图像上的每个小块， 也就是循环的总体。 卷积核在每个图像局部做的， 事实上都是一个小的if esle 语句。 if像素之间符合某个关系，就是yes，否则No。这个结果， 最后被综合出来， 给下一层合成更复杂的图像特征。我们事实上通过学习的过程，让机器自动补全了循环每一步的这个if else语句。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7298850574712644&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibJ7GaCXDzw2dMW8QiaAPh96svf1lqjLico2MoDlVUCfFZgIHkysrSaRibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4vk9r-0-0&quot;&gt;好了，到目前为止， 说的都是和时间无关的规则。 而一开始讲到的真实的图灵机， 是和时间有关的规则。 那么如何得到一个和时间有关的规则表呢？ 如果要处理和时间相关的信息， 你必须要引入记忆，引入内部状态， 而和刚刚说的一样， 这些含时间的规则要是可以学习的，用数学的语言说， 就是要有一个连续可微的载体， 这个东西就是RNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;72qm9-0-0&quot;&gt;def step&lt;/span&gt;&lt;span data-offset-key=&quot;72qm9-0-1&quot;&gt;(self, x):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;di032-0-0&quot;&gt;# update the hidden state&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4f218-0-0&quot;&gt;self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;243tl-0-0&quot;&gt;# compute the output vector&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;591p4-0-0&quot;&gt;y = np.dot(self.W_hy, self.h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a1b57-0-0&quot;&gt;return&lt;/span&gt;&lt;span data-offset-key=&quot;a1b57-0-1&quot;&gt; y&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;g9on-0-0&quot;&gt;以上是RNN的python程序定义。 它说的无非是你有一个刚刚说的线性分类器组成的单隐层神经网络，但是这一回，神经网络的输出，要作为输入，重新回到神经网络的隐层里， 这个关键的增加， 就使得它具有了处理复杂时间信息的能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5qh71-0-0&quot;&gt;这个结构，非但优雅，而且有效。一个非常重要的点是， 你知道信息的传播是有损耗的， 如果把RNN展开， 它事实上相当于一个和历史长度一样长的深度网络， 信息随着每个时间步骤往深度传播， 这个传播的信息是有损耗的， 到一定程度我就记不住之前的信息了， 当然如果你的学习学的足够好， Wij还是可以学到应该学的记忆长度。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26161790017211706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibPf1BYBmsfuHXyspOv0uwulVbuy0UibqOib2grXtp4XpvWU4b6Pj3402Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;事实上叫做“循环神经网络”  循环的本质是什么呢？   它其实正是你的程序里的for循环啊！ RNN的本质是， 在每个时间步里进行同样的操作， 这个操作无非是， 当下的输入， 和神经网络的状态两部分特征的逻辑组合（与或非）然后， 这个组合的结构进行一个if else的逻辑判断， yes or no， 根据这个，生成一个输出的结果， 这个结果， 要回传给神经网络隐层， 生成下一个隐层状态。 大家看这其实就是图灵机的定义啊， 而RNN的本质， 就是一个可以通过微分方法学习的图灵机啊。 虽然每个步骤的规则和执行足够简单， 但是只要步数足够多， 却可以产生非常复杂的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;RNN学习的本质， 就是给你那个足够复杂的结果， 让你反演出那个足够简单的规则， 然后让它在新的环境下再去做预测与决策。 我们可以看到， 这已经非常接近智能的本质了。 那么RNN有没有可能学到真正类似人类的抽象思考能力， 具备人类类似的生成规则的能力呢？ 这可能才是后面真正的问题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;让机器生成有关未来的规则-强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt; &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;强化学习&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;果说监督学习的基本框架已经是在生成用于判断（分类）的规则表， 那么强化学习， 就是生成一套直接用于行动的规则表， 这套语言的元素包括&lt;span&gt;状态， 行为， 观测， 奖励&lt;/span&gt;。 事实上， 强化学习所做的事情是从成功或失败的经历里去归纳行为的准则。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;我开头讲的解魔方的问题， 如过让机器自己找到最短时间完成它的方法， 这就是强化学习所做的事情。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;首先看&lt;span&gt;状态&lt;/span&gt;s， 状态是什么呢？ 它指的是智能体（agent）所在的环境里所有和游戏有关的信息， &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;再来看行为，所谓行为，是指智能体的&lt;/span&gt;&lt;span&gt;决策&lt;/span&gt;&lt;span&gt;，某种情况下我们可以认为它就是监督学习要求的那个y， 或者预测， 但一个决策与预测不同的是，我们并不能马上取得一个信号告诉我们这个决策对不对， 只有在游戏的最后 ，我们才能从整个游戏的收益反观当时的决策好坏。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;寻找到从根据当下的状态s行动的一张规则表， 让我有最好的机会拿到奖励， 就是强化学习在做的事情。 &lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;而深度强化学习呢？  它就是把刚说的连接主义通过概念生成规则的方法，和此处的决策联系起来得到的框架。 &lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;我们还差很远&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;说到所有上述的东西， 你会觉得我们已经无所不能， 既然机器能够自己填写程序， 自己生成规则表来适应多变的世界， 那我们还差什么呢？  我们说都是生成规则， 不同的规则效力差距万千，而目前的AI在此处也就是个小学生。 亚里士多德和牛顿都观测力学现象， 亚里士多德看到的是轻的气体向上飘，重的东西向下沉。 牛顿看到的是受力与加速度的关系。 这两种规则的归纳即使都能解释现象， 但是它们的泛化能力确是千差万别， 一个可以解释全宇宙， 另一个也就适应一些物体吧。 如同下图所示， 坏的规则总结只能解释数据实例周边一丁点的地方， 而好的规则呢， 它可以把点连城片！  &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7445945945945946&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4XExqvasWticuD9ehaBQuFeiazL5LEfoMicN72hz0eQJEZpp4IqichpMVAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;我们说人类总结的规则解释力最强的地方是物理 ，因为物理里描述了客观实体间作用的因果联系，而非简单的相关性。 这恰恰是目前AI所不足的。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5j932-0-0&quot;&gt;有关物理的世界和智能的世界&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;di5qc-0-0&quot;&gt;上面的这些思考无疑开始让人们想象我们所说的包含了逻辑推理， 情感，甚至意识的问题与物理世界的关系到底是什么。 我们说物理的世界里， 主宰一切的是微分方程。 一切因果关系， 都由微分方程所承载。 你有了不同不同微观粒子电磁力的描述，把它们放入薛定谔和狄拉克方程， 你就可以推出原子的不同性质。  这其实可以说是因果推理的极致了。 它甚至导致了机械的宿命论思想。当一切初始的原因输入系统， 那么它就回归于一个必然的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dl784-0-0&quot;&gt;到了非线性动力学的时代看似这点被混沌打破了，亚马逊的蝴蝶引起北美的飓风， 让通俗科学爱好折重新燃起了不可知论的希望，事实上并没有。  所谓的混沌， 无非是一种确定性下的不确定， 或者已知中的未知。 混沌的系统依然在一个被方程高度确定的洛伦兹吸引子里。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span data-offset-key=&quot;af64u-0-0&quot;&gt;到此处， 我认为微分方程依然是描述因果关系最精密的所在， 它可以在输入很少信息的时候， 得到最多的预测产出。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;在看刚刚的智能问题， 我们说， 整个智能问题， 到目前为止其实还是在围绕那张规则表， 只是我们的思路由制定规则表， 到了学习规则表。和物理比较， 目前的机器， 需要输入进去大量的数据， 才能生成一点十分简单的规则。 当然你可以举阿法狗下围棋的例子说明所生成的规则并没有那么简单， 可惜的是， 那些规则只适用于一些非常封闭而特定的领域。 而不像牛顿定律放之四海而皆准。  那么神经网络可不可以观测大量物体坠落的过程把万有引力定律给推出来呢？ 目前看是不能的。其实牛顿引力定律的得出是含有了大量的人类推理。 我们需要先知道物体运动改变和受力的关系， 然后通过观测物体的轨迹得到大量物体的受力情况，再在这些手里情况下得出某一种共同的作用力形式， 这是一个多么复杂的思维链条。 这对于目前统计的巨人， 而只懂得浅显的形式逻辑的神经网络，还是比较困难的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;如果我们可以用神经网络加上强化学习，诱导它掌握特别抽象而复杂的如受力，运动这样的概念，那将是不可想象的， 目前我们也并不清楚有没有那一天。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;另一方面， 人类是通过符号组成的语言思考的， 目前AI总结和归纳符号的能力， 同人类的语言相比依然天差地别。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dsh1-0-0&quot;&gt;有关语言代表的符号世界&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;讨论智能的问题离不开语言。从乔姆斯基开始，人们就开始研究不同语言背后的共同语法基础。 其实如果深究语言问题， 我们会看到它和刚刚说的程序的联系。 语言无非是对世界的符号化，类似于给每个刚刚说的概念赋予一个符号。而语言其实很像程序， 它就是对概念之间关系的表述。 当然数学也是一套伟大的符号系统。  语言和数学以及程序的区别可能在于语言更加模糊， 但是它对付不确定性的能力远远大于程序。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;事实上， 人类语言很少有那种特征精确的关于几何关系和量值的描绘， 而似乎更多定性成分， 比如美丑，近， 远， 大，小。 我们可以想象在一个充满变化的世界那种特别精确的描述不一定十分有用。 恰恰因为这种模糊性，让它具备了更好的适应能力， 和泛化能力。 可以表述那些用程序难以描述的事情。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;既然本质上语言无非是一个现有概念的符号体系，以往的深度学习NLP其实是走了一条南辕北辙的路，我们把不同的词汇和句子压缩成词向量， 句向量喂给神经网络学习， &lt;strong&gt;而事实上神经网络对这些符号背后的实体概念却一无所知。&lt;/strong&gt;虽然词向量也能稍微的带有一点不同词语之间的语义距离， 但是这和真实世界所含有的信息量，也依然是差异巨大。 目前用图卷积网络解决NLP的思路，算是一个进步， 因为它更好的涵盖了整个符号世界的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;语言， 好比一个巨大的人类经验和逻辑的宝库， 这个符号世界几乎就是真实世界的最好压缩体， 如果一天神经网络真正被赋予了语言的power，也就是能够真正理解这个符号世界，或许离通用人工智能也就不远了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;有关物理世界和语言世界的打通&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;刚刚说的精确的物理方程的世界， 和能够应付更多不确定性的模糊的语言， 之间又有哪些联系呢？  我的想法是， 物理的杀手锏微分方程， 当构成了一个非线性的动力学系统， 却可以通过它内在的定点， 极限环，吸引子等概念， 去接近那个模糊性的语言， 就好比在非线性动力学的世界里， 我们往往不再那么关于一个系统如何发展的暂态，很多不同的系统都归一于一个吸引子， 那么它们背后的逻辑可能就是类似的。 这或许会架起一座物理世界和语义世界的桥梁？  &lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;如何让机器学习符号&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;假定语言这样的高效符号系统是大脑产生的，那么我们是否可以根据脑科学的启发把这种能力赋予神经网络，来加强它的推理能力呢？ 如此可以延申的思考还很广很广。&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 26 Feb 2019 00:50:52 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/WW3EirRLSj</dc:identifier>
</item>
<item>
<title>基于一张规则表的人工智能</title>
<link>http://www.jintiankansha.me/t/q56XOgLetH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/q56XOgLetH</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;4ds8l-0-0&quot;&gt;我在过去对人工智能简史的描述中，把人工智能的整个历史描述成围绕一张规则表， 本文是基于这一想法的总结和扩展。我们说， 早期的AI发展史围绕如何人为构建这样一个规则表解决复杂问题， 而当下的AI则围绕如何让它在复杂的现象中自己归纳出这个规则表&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bkglj-0-0&quot;&gt;我们说， 上帝通过制定规则从简单演绎出复杂。最初的原始人类在黑暗中摸索， 在众多的现象不知所措， 只能通过设立各色大神小神来缓解自己对不确定性的恐惧。 从多神宗教到一神宗教的跨度体现了一个从复杂中寻找简单的跨度， 这可能是基于一种隐隐的直觉，就是现象虽然多样， 但是背后的法则不应该如此复杂。 到了科学的时代， 这种思维在物理学里淋漓尽致起来，四大力学， 把分子原子间的作用力统一到电磁力， 把宏观物体的作用统一到引力和经典的动力方程， 已经是极致， 而后面的对这两者的统一构成了从相对论到杨米尔场的现代物理主线。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibembXL81evOfRdy0cTz2qqulnss6iaFGmXLmdjuRu9GiayK86XIM1TVSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图: 简单规则生成复杂的极好例子， 元胞自动机， 每一步细胞的繁殖和阔算方法一定， 它最后形成的图案就定了， 规则可以很简单， 图案可以很复杂。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1j7g-0-0&quot;&gt;另一方面， 这样的思想从智能科学诞生之出，也贯穿出来。 它在早期的可计算性中， 通过图灵机的构建。它认为存在这样的通用机器，能够和人类一样解决问题， 即使过程非常复杂， 你无非需要四个要素： 1， 输入 2， 中间状态 3， 规则表 4， 输出 并在时间上进行大量迭代， 就可以实现这个过程。 通过这个过程， 我们可以把一个输入转化为一个想要的输出。 如果我们能够在在有限的步骤里将一个输入转化为一个输出，据此解决一个实际问题， 那么这个问题就是可计算的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6114081996434938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBnC8zkibScsiccAOwoSH4Ld9vdiamljakNVOX5TKF69ywm3Qq2ic3IdhUw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;561&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c00nl-0-0&quot;&gt;而冯诺依架构让它变成一个技术现实。 它通过可以存储程序的机器， 让人们通过把这些图灵规则表的指令变为计算机二极管的开合代码， 而让图灵纸袋的思想成为了一个每个工程师可以设计的现实。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;一个计算机程序， 最基本的部分包括一些简单的形式逻辑， 包括逻辑与或非， if else， for 循环这些。 其实本质上， if else 所描述的就是规则表， 规则里面通常涉及简单的逻辑， 最终通过for循环， 我们就可以得到我们要的东西。 比如一个中学生都会的排序算法， 我们无法需要做的是前面和后面的数比较大小， 然后一个if else进行换位， 最后一个for循环， 多长的序列都可以瞬间搞定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.40556900726392253&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibibEBCIwsAHiaGwDkFcYUeGLqHsuPP0ibBA7ibPX7OxPPq87Jr0NAtFjSibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;826&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b1ct2-0-0&quot;&gt;这就是用程序解决问题的核心思维， 给你一个再复杂手忙脚乱的问题， 只要这个问题可计算， 那么我们只需要设定好我们需要的规则表， 在有限的步骤里迭代， 最终机器总会给你解决。&lt;/span&gt;比如魔方问题， 一般的聪明小孩都很难在短时间解决问题， 但是， 事实上解决魔方问题有一套非常整齐的规则表（你想象打乱一个魔方其实比较容易的， 把它弄整齐是打乱的逆运算，但是破镜重圆总是难的）。 如果按照这个规则表执行若干步， 再困难的魔方也给你整出来。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibz3zliaRoh4oEWf9QQOX3VMpbMFylumcHY7novvRnic1j7aN7R7pohRHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;676&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44333333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBqmPPsMKMLBUUyKWYnVsiaibg6hmZP9cADL9iak5yxuibvmosIG5ZUWbuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e13hv-0-0&quot;&gt;我们说规则表， 加上迭代等操作的思路可以解决大量的工程问题。我们曾经认为按照这样的思路我们可以解决整个智能的问题。 只是填入一张越来越大的表格。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;但是它在通向智能的关键位置， 却停住了， 这个元凶 -就是- &lt;strong&gt;不确定性&lt;/strong&gt;。日常生活中很多东西无法轻易的总结出规则表来， 因为细小的规则实在太多了。 你可以想象我们有无数尺寸和规格各不相同的螺钉螺母。 每一种规格我们都要想一条if else，可悲的是这些螺钉和螺母几乎没有哪两对完全相同， 穷尽一个程序员一生也写不完这些程序。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9mrot-0-0&quot;&gt;统计机器学习 - 机器判断规则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6pfmt-0-0&quot;&gt;这个问题的解决方法十分自然又十分了不起：  能不能让机器自己学会这个表格， 而不是认为设定它呢？  这就是整个智能问题的第二步 - 学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eeh10-0-0&quot;&gt;整个学习问题的基石其实是古希腊人提出的归纳法和演绎法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-0&quot;&gt;伟大的希腊哲学家早就对学习的本质展开过探讨，它们把学习分类为&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-1&quot;&gt;归纳法和演绎法&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;。所谓演绎法， 就是从用一定规则进行推理的过程。 苏格拉底是人，人都是会死的， 因此苏格拉底会死。 这就是三段论， 或者称为演绎法的根基。 而真正学习的过程，是这个演绎法的逆过程。 我们先知道一个特例， 然后通过特例，得到这个“人都是会死的” 知识， 再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6nhn2-0-0&quot;&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。 我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？ 组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1bdeq-0-0&quot;&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！ 比如机器收集到一个苏格拉底死了， 那么苏格拉蒂是什么？ 男性，白种人， 哲学家， 于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！ 这样的思维范式，就是归纳法，由于我们列举的假设依然用到了人类已有的知识， 因此我们得到的这个机器，事实上是最接近规则机器的一台学习机， 我们可以称之为规则为主体的归纳法。我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nnub-0-0&quot;&gt;整个有关统计的机器学习， 都可以看成让机器学习有效归纳的方法， 从数据里得到规则表， 再用规则表进行判断。前面的过程叫训练， 后面的过程叫测试。  如果这些规则是有关一个是非的命题， 它就是一个分类器， 如果它是一个连续数值的预测， 就是回归。 但是规则表的本质是不变的， 它就是让你填表，表格的横排和竖排已经有了， 一个叫特征， 一个叫实例。  特征是人为归纳好的， 而实例是我们人为收集的， 表格中有些地方是空的，  就是我们想要判断的东西， 需要机器来填的部分。 比如给你一百幸福和不幸的人的案例， 让你判断第101个人的情况。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;81s85-0-0&quot;&gt;刚刚的那个例子你应该已经体会到， 这个命题验证过程其实是一个组合爆炸的问题。我们把关于这个世界的互相矛盾的假设都丢尽机器。即使最简单的问题也会有无穷多的情况要判断 （特征的n次方）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib6XNRA36VEFMtlPlZQ2RpPXtla5u66LIcDmhkLUQuPecqg0IVicuibIwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ajopm-0-0&quot;&gt;而机器学习的各个算法， 让我们通过加入更多的假设， 来偷懒解决这个问题， 此处没有比决策树更典型的， 它的高阶版本xgboost成为机器学习竞赛的杀手锏。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;而决策树得核心智慧就是优先级算法简化命题数量。 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心，事实上， &lt;strong&gt;我们永远在抓轻重缓急，在抓主要矛盾，&lt;/strong&gt; 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。 每次pass， 就减少掉了一半需要判定的命题。 通过这种预设的二叉树逻辑， 一个本来需要2的n次方的步骤解决的事情， 可能只要n步了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7453703703703703&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibq5oBakMtQKgHb2Akq4jNYicdNNniarAhbtJNpWaJ7qOkbJDDKpTTB9MQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;具体如何来学，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 如此，就会形成一个树的结构。构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？我的要求就是， 每一次分化，我们都希望取得最多的信息，如分叉后一个树杈全是yes，一个全是no就是最好的效果， 如果达不到， 也让它尽可能接近这个效果。  这样一个一个问题问下去， 最终达到稳定后过程停止。  这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk0ce-0-0&quot;&gt;另一个得到这样的一个规则表的方法是线性假设。 线性分类器通过假定特征之间的相互独立， 使得命题的成立与否可以通过一个加权求和的关系表达， f=wx+b 。最后f如果大于0就是是， 小于0就是否。 线性分类器也是一种特别符合人认知习惯的模型：一般人在决策时候做的事情就是加权平均，比如你平时做分类（决策）， 你最想的一种状态是什么？你要把几个核心的要素放到一起， 按照他们的重要性加和，比如你今天要不要去看电影，可能取决于你的女朋友free否， 下不下雨和电影好不好看， 这个时候，我们可以把这些因素加权在一起， 在和一个我们给定的阈值做比较，大就去， 不大就不去， 这正权衡得失的做法， 就是线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;具体学习的过程， 我们从实例里归纳出每个特征对应的权重参数，然后进行判断。 只要参数都确定了， 也就是一次解决了所有的问题。 线性分类器的高级版本SVM已经超越了线性假设。 也是小数据下生成有效规则的大杀器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8te9f-0-0&quot;&gt;连接主义机器学习， 产生规则&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk6j-0-0&quot;&gt;刚刚说的那一套， 有一个问题你有没有注意到？ 我们最先提出的问题是让机器产生一个规则表， 而刚刚说的统计机器学习里， 更多的是让机器根据特征进行命题判断。 这其实是只进行了0.5步。 大家想象以下， 在真正的实践活动里， 你无法一开始就设定出一堆特征让它进行逻辑判断，在这个情况下如何得到我们所说的“规则”呢？ 如何让机器自己生成战胜“复杂”的程序呢？ 连接主义机器学习在一定程度解决了这个问题。  因为， 人类认识事物，生成规则， 其实是通过“概念”来的， &lt;strong&gt;“概念”是一个浓缩的信息载体， 通过它我们能够进行任何更复杂的推理。&lt;/strong&gt; 那么“概念”是如何生成的呢？ 它的载体正是下面说的联结主义的代言人神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5gsn-0-0&quot;&gt;神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dijog-0-0&quot;&gt;首先，神经网络是由神经细胞组成的。  一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习，它就是刚刚说的线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cr2lq-0-0&quot;&gt;正确的分类，是认知的基础，我们对事物的感知比如色彩， 物体的形状等，其实都是离散的， 而物理信号是连续的， 比如光波， 声波。这里面的中间步骤就是模数转化， 把连续的信号转化成离散的样子， 这正是一个分类器干的事情。  一个单个神经元可以执行一个简单的基于感知信号的if else语句。 先收集一下特征做个加和， if大于一个值我就放电， 小于我就不放电，就这么简单。 晶体管当然也在干这个事情。 &lt;strong&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。&lt;/strong&gt;或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib2NvE4HCXevrTz3VAnXzfIxaOZibibe1YnYGN84NF1XWqyKpNdsISR2aA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3f6fb-0-0&quot;&gt;我们这个简化出来的模型，　正是所有人工神经网络的祖母　－　感知机。　从名字可以看出，&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-1&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;3f6fb-0-2&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 R 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;单个的感知机并不能比传统的机器学习多做一丁点的事情， 还要差一些。 但是把很多个感知机比较聪明的联系起来，就发生了一个质变。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ui0b-0-0&quot;&gt;首先， 每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有那个头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  其实这里面做的事情， 正是基础的逻辑运算， 一个简单的一层神经网络可以执行与或非这些基本的逻辑操作。事实上它的本质就是把简单的特征组合在一起形成一些原始的概念。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;它是怎么做到的呢？ 学习。  生物神经网络的学习， 是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是， 我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了一个简单的“概念”。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;上述过程被总结H&lt;/span&gt;ebian学习的一个过程。  我们可想象，一个两层以上的神经网络， 就可以表述香蕉， 苹果， 菠萝这些水果了， 它们无非是底层特征颜色，形状的不同组合而已。 而这些不同水果的概念， 就可以帮助我们形成更加复杂的规则表 ，比如让它根据客户的信息帮它推荐一个水果拼盘。 由此可见， 神经网络通过与或非进行简单特征的组合 ，再通过if esle进行判断选择合适的特征得到概念， 再通过下一层迭代得到概念有关的命题。 就可以生成比之前的传统机器学习复杂的多的规则表。而且我们可以想象出来， 迭代的层数越多，它生成的“概念”和“规则”就越复杂。  当然真实训练中我们用到的不是模仿生物版本的Hebian学习， 而是强大的多的反向传播算法。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.6111111111111112&quot; data-w=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dmb6s-0-0&quot;&gt;事实上为了让这种生成“概念”得到“规则”的方法更加有效， 我们会加入一些无比强大的先验假设。 其中最有名的一组，  就叫CNN，它所做的，其实是对于图像这类巨大无比， 而局部特征不断重复的信息形式， 其实你可以写一个循环， 来让你的程序更有效。 循环里的模块每一步是可以共用的， 也就是卷积核。 卷积核一点点的卷个图像上的每个小块， 也就是循环的总体。 卷积核在每个图像局部做的， 事实上都是一个小的if esle 语句。 if像素之间符合某个关系，就是yes，否则No。这个结果， 最后被综合出来， 给下一层合成更复杂的图像特征。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7298850574712644&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibJ7GaCXDzw2dMW8QiaAPh96svf1lqjLico2MoDlVUCfFZgIHkysrSaRibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4vk9r-0-0&quot;&gt;好了， 到目前为止， 说的都是和时间无关的规则。 而一开始讲到的真实的图灵机， 是和时间有关的规则。 那么如何得到一个和时间有关的规则表呢？ 如果要处理和时间相关的信息， 你必须要引入记忆， 引入内部状态， 而和刚刚说的一样， 这些含时间的规则要是可以学习的， 用数学的语言说， 就是要有一个连续可微的载体， 这个东西就是RNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;72qm9-0-0&quot;&gt;def step&lt;/span&gt;&lt;span data-offset-key=&quot;72qm9-0-1&quot;&gt;(self, x):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;di032-0-0&quot;&gt;# update the hidden state&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4f218-0-0&quot;&gt;self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;243tl-0-0&quot;&gt;# compute the output vector&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;591p4-0-0&quot;&gt;y = np.dot(self.W_hy, self.h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a1b57-0-0&quot;&gt;return&lt;/span&gt; &lt;span data-offset-key=&quot;a1b57-0-1&quot;&gt;y&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;g9on-0-0&quot;&gt;以上是RNN的python程序定义。 它说的无非是你有一个刚刚说的线性分类器组成的单隐层神经网络， 但是这一回，神经网络的输出， 要作为输入，重新回到神经网络的隐层里， 这个关键的增加， 就使得它具有了处理复杂时间信息的能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5qh71-0-0&quot;&gt;这个结构，非但优雅，而且有效。一个非常重要的点是， 你知道信息的传播是有损耗的， 如果把RNN展开， 它事实上相当于一个和历史长度一样长的深度网络， 信息随着每个时间步骤往深度传播， 这个传播的信息是有损耗的， 到一定程度我就记不住之前的信息了， 当然如果你的学习学的足够好， Wij还是可以学到应该学的记忆长度。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26161790017211706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibPf1BYBmsfuHXyspOv0uwulVbuy0UibqOib2grXtp4XpvWU4b6Pj3402Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  事实上叫做“循环神经网络”  循环的本质是什么呢？    它其实正是你的程序里的for循环啊！ RNN的本质是， 在每个时间步里进行同样的操作， 这个操作无非是， 当下的输入， 和神经网络的状态两部分特征的逻辑组合（与或非）然后， 这个组合的结构进行一个if else的逻辑判断， yes or no， 根据这个，生成一个输出的结果， 这个结果， 要回传给神经网络隐层， 生成下一个隐层状态。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;大家看这其实就是图灵机的定义啊， 而RNN的本质， 就是一个可以通过微分方法学习的图灵机啊。 虽然每个步骤的规则和执行足够简单， 但是只要步数足够多， 却可以产生非常复杂的结果。  &lt;strong&gt;RNN学习的本质， 就是给你那个足够复杂的结果， 让你反演出那个足够简单的规则， 然后让它在新的环境下再去做预测与决策。&lt;/strong&gt; 我们可以看到， 这已经非常接近智能的本质了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5j932-0-0&quot;&gt;有关物理的世界和智能的世界&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;di5qc-0-0&quot;&gt;上面的这些思考无疑开始让人们想象我们所说的包含了逻辑推理， 情感，甚至意识的问题与物理世界的关系到底是什么。 我们说物理的世界里， 主宰一切的是微分方程。 一切因果关系， 都由微分方程所承载。 你有了不同不同微观粒子电磁力的描述，把它们放入薛定谔和狄拉克方程， 你就可以推出原子的不同性质。  这其实可以说是因果推理的极致了。 它甚至导致了机械的宿命论思想。当一切初始的原因输入系统， 那么它就回归于一个必然的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dl784-0-0&quot;&gt;到了非线性动力学的时代看似这点被混沌打破了，亚马逊的蝴蝶引起北美的飓风， 让通俗科学爱好折重新燃起了不可知论的希望，事实上并没有。  所谓的混沌， 无非是一种确定性下的不确定， 或者已知中的未知。 混沌的系统依然在一个被方程高度确定的洛伦兹吸引子里。&lt;/span&gt;到此处， 我认为微分方程依然是描述因果关系最精密的所在， 它可以在输入很少信息的时候， 得到最多的预测产出。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;在看刚刚的智能问题， 我们说， 整个智能问题， 到目前为止其实还是在围绕那张规则表， 只是我们的思路由制定规则表， 到了学习规则表。和物理比较， 目前的机器， 需要输入进去大量的数据， 才能生成一点十分简单的规则。 当然你可以举阿法狗下围棋的例子说明所生成的规则并没有那么简单， 可惜的是， 那些规则只适用于一些非常封闭而特定的领域。 而不像牛顿定律放之四海而皆准。  那么神经网络可不可以观测大量物体坠落的过程把万有引力定律给推出来呢？ 目前看是不能的。其实牛顿引力定律的得出是含有了大量的人类推理。 我们需要先知道物体运动改变和受力的关系， 然后通过观测物体的轨迹得到大量物体的受力情况，再在这些手里情况下得出某一种共同的作用力形式， 这是一个多么复杂的思维链条。 这对于目前统计的巨人， 而只懂得浅显的形式逻辑的神经网络，还是比较困难的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dsh1-0-0&quot;&gt;有关语言&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;讨论智能的问题离不开语言。从乔姆斯基开始， 人们就开始研究不同语言背后的共同语法基础。 其实如果深究语言问题， 我们会看到它和刚刚说的程序的联系。 &lt;strong&gt;语言无非是对世界的符号化，类似于给每个刚刚说的概念赋予一个符号。&lt;/strong&gt;而语言其实很像程序， 它就是对概念之间关系的表述。  我想语言和程序的区别可能在于语言更加模糊， 但是它对付不确定性的能力远远大于程序，因为这种模糊性， 让它具备了更好的适应能力， 可以表述那些用程序难以描述的事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt; 但是本质上， 语言无非是一个现有概念的符号体系， 描绘概念和概念间的关系。这样看以往的深度学习NLP其实是走了一条南辕北辙的路， 我们把不同的词汇和句子压缩成词向量， 句向量喂给神经网络学习， 而事实上神经网络对这些符号背后的实体概念却一无所知。虽然词向量也能稍微的带有一点不同词语之间的语义距离， 但是这和真实世界所含有的信息量，也依然是差异巨大。 目前用图卷积网络解决NLP的思路，算是一个进步， 因为它更好的涵盖了整个符号世界的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;语言， 好比一个巨大的人类经验和逻辑的宝库， 这个符号世界几乎就是真实世界的极好压缩体， 如果一天神经网络真正被赋予了语言的power，也就是能够真正理解这个符号世界， 或许离通用人工智能也就不远了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;有关物理世界和语言世界的打通&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;刚刚说的精确的物理方程的世界， 和能够应付更多不确定性的模糊的语言， 之间又有哪些联系呢？  我的想法是， 物理的杀手锏微分方程， 当构成了一个非线性的动力学系统， 却可以通过它内在的定点， 极限环，吸引子等概念， 去接近那个模糊性的语言， 就好比在非线性动力学的世界里， 我们往往不再那么关于一个系统如何发展的暂态，很多不同的系统都归一于一个吸引子， 那么它们背后的逻辑可能就是类似的。 这或许会架起一座物理世界和语义世界的桥梁？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 18:31:05 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/q56XOgLetH</dc:identifier>
</item>
<item>
<title>AI最小入门指南（二）-- 人工智能简史</title>
<link>http://www.jintiankansha.me/t/faWzusH9s2</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/faWzusH9s2</guid>
<description>&lt;p&gt;&lt;strong&gt;人类探索人工智能的初级阶段：基于规则运行的机器&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们这次的讲座从一个图片开始， 这张图片，记载了一个历史上非常有名的会议，叫做达特茅斯会议(Dartmouth Conference)&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.7363636363636363&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4F91ibRrKq5Uk6koNGbd9rib9NiapY6LibqcX9ia1kQon9KajfMQLdC6MJ6Q/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;550&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2006年，会议五十年后，当事人重聚达特茅斯。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;这个会议上面坐的几个人物，堪称人工智能早期的奠基者。有shannon， minsky，司马贺等人，像这样的场景，人类的历史也没有几个，也许你会联想起物理学史的索尔维会议。但是肯定举不出三四个。为什么，这样的人物会在这个时间，齐聚一堂？他们的中心议题只有一个，人能否制造出像人类一样思考的机器， 这个人类的终极梦想。&lt;/p&gt;

&lt;p&gt;我们来从头开始， 还原这个故事。首先，智能， 智能是什么， 智能有哪些形式？也许你还会继续问， 如果人有智能？ 动物有没有？ 如果我们理解了智能，是否能够造出一个会思考的机器？ 这些问题， 我们都要回到智能的定义开始。 智能是什么？ 笼统的说智能是解决复杂问题的能力。无论是逻辑，还是语言， 还是运动， 本质上我们都在解决和生存息息相关的问题， 虽然我们从中发展出的技能有时远超那个原有的任务。    &lt;/p&gt;

&lt;p&gt;和智能有关的词语， 逻辑推理， 计划决策，学习应变，形式在多样，离不开的是， 解决问题， 简单到去寻找下一顿猎物， 还是众多异性里寻找配偶， 复杂到设计一个计划成为群体的领袖， 有了一个目标， 我们需要在环境条件进行行动，随机应变， 直到达到目标。  &lt;/p&gt;

&lt;p&gt;当然， 我们身上的这种能力有时候已经到了接近本能的地步， 我们已经并不能说出我们为什么会说话，或者看出那个人是我三天前会上见过的教授。如果回溯历史， 我们会发现，人的智能，主要可以归纳为语言和逻辑计算，通过这两个东西把一个复杂问题变成可以求解。我们通过理解事物（认知）来进行有效决策， 然后使得事物向着对我们有利的方向发展。&lt;/p&gt;

&lt;p&gt; 想象你在手中转动一个魔方， 你通过一系列的转动， 把它向着接近你目标的方向变化，直到得到最后的结果。  那么， 机器是否可以做类似的事情呢？   一个有效的假设是机器需要具备和人类似的东西， 也就是把一般问题求解的过程抽象出来就可以模拟智能。&lt;/p&gt;

&lt;p&gt; 在这方面做出伟大的贡献是阿兰图灵， 它认为存在这样的通用机器，能够和人类一样解决问题， 这就是图灵机的概念， 如果你需要实现这个， 无非需要四个要素： 1， 输入  2， 中间状态 3， 规则表  4， 输出  &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.752&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4ZO9f7rZjvnAXiavt30SVHH5W0M4kaEIMVTZS8yLCgerszU6NDNZNDVQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p&gt; 机器的原型可以看做一个足够大的纸带， 纸带上有很多格子，格子上或者涂成黑色或者涂成白色，上面有一个机械昆虫可以在格子上跑来跑去， 我们就简单的假设成左右吧。  至于它是想往左跑还是往右跑， 在最简单的情况下只取决于纸带它所在位子的格子是黑还是白（输入），我们就假设白右左黑， 那么这条虫子就会无休止的左左右右循环下去， 外人看上去很像一个无休止的运动的小虫。&lt;/p&gt;

&lt;p&gt;显然这是一个毫无用处的数学游戏， 但是， 当我们给这样一个简单的模型加入两个东西，一个是中间状态， 一个是规则表，一个是虫子也可以改变外界环境（纸带）， 那么整个故事就大不一样。 比如我们规定虫子有个内在的状态， 就是饿与不饿， 然后根据他的这个内部状态，它可以对纸带施加不同的作用，见到白色的格子， 如果是饿了，就给他涂黑， 如果是饱了，就什么都不做， 一旦涂黑之后他的状态就由俄变成饱，而他走到黑子又会变成俄。 那么我们会看到一个什么图景？这样，游戏就会表现出一些真正复杂的模样， 纸带自身也开始变化， 再某个时刻， 纸带可能变成全黑， 而虫子也最终停下。&lt;/p&gt;

&lt;p&gt; 如果我们也可以给虫子指定不同的规则，比如饿的时候白左黑右， 饱的时候白右黑左， 他就可能会表现出任意复杂的运动来， 甚至表现的真的像一个在思考的虫子。为什么这样的虫子可以解决问题？  还记得我讲过得算法得概念吗？  我们可以想象一个最终要达到的状态，然后我们需要做的无需是设定这样一个过程， 使得通过若干步骤，得到最后这样一个结果。想下排序算法！    &lt;/p&gt;

&lt;p&gt;如何设计这样一个过程呢？ 这里面蕴含的真正思想，是可编程理论。&lt;strong&gt;问题的中心是那张规则表。输入和内部状态， 经过规则表得到一个行为， 行为改变了环境， 得到下一个状态，&lt;/strong&gt; 如果规则表设计的好， 我们几乎表达任何问题解答过程。&lt;/p&gt;

&lt;p&gt;为什么说很复杂的问题， 通常可以设定为一个比较简单的流程， 然后流程可以简化为一个规则表呢？简单的说， 因为复杂是简单中产生的，  一个简单的规则， 通过很多步骤， 就得到复杂。 一个非常有趣的例子是元胞自动机， 它可以极好的阐述一个简单的规则如何产生极为复杂的图案。  这个机器的原理是， 你有一个由无限多方格组成的纸带，在这个纸袋上，有一个细胞（黑色方格）开始生长繁殖， 扩散， 它的繁殖扩散原理非常简单， 因为假定它的行为只取决于周围两个方格的情况， 具体怎么决定的， 由一个规则表表示， 按照这个规则表，经过一定时间，这个细胞就会变成一个群， 这个群的形状可以很简单， 也可以任意复杂。 这个游戏用来阐述复杂是由简单产生的， 再复杂的现象， 都是简单的规则随时间推衍产生出来的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7072368421052632&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4e5afZccgRz1y4eyMwQa76Qg3tyJX4Y6RCr7qzzZ0IpwricibuUocIwOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;304&quot; /&gt;&lt;/p&gt;

&lt;p&gt; 如果你理解了这个原理， 你就会理解智能到底是什么， 我们为什么能够解决复杂问题，因为我们事实上用到了这个过程的逆过程， 我们有一个最后想要的结果比如娶得某国公主，然后由个初始状态比如你是一个贫困大学生，  然后你需要能够把它分解为一系列中间步骤，然后通过设立一套简单的规则达到那个最终结果， 如果真的实现了， 一个复杂的问题就解决了。 小到玩魔方，大到治理国家， 都可以用类似的方法解决。 &lt;/p&gt;

&lt;p&gt;比如魔方，如果你去随便的转动， 立刻就会疯掉。 但是， 这个问题存在一套非常固定的规则表（tetris）,  它对应有限个状况下的有限种操作。  就如同计算机程序的if， else语句， 在遇到什么色块的时候你要怎样转动， 只要按照这个简单的规则执行， 最终一定可以走出来。 而治国这样复杂的大问题，也无非是遵循有限的几条规则（不同的国家， 会把这个东西记载在不同的经典里， 从论语， 到自由大宪章）。&lt;/p&gt;

&lt;p&gt;上面的例子让我们可以感觉到规则机器的力量? 如果我们能够有效的获取人类总结的规则， 让机器来使用， 那这个机器不就实现了智能？ 这样的想法将导致人工智能的第一个霸主- 专家系统。 那么这套规则的存储形式是什么呢？  知识！ 各个学科的知识库！  人类几千年的解决问题的智慧是以知识的形式存储起来的， 有的知识， 以语言的形式保存， 有的，被一些逻辑或数学符号连接。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7724137931034483&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4EMnweBTefx25MDmy95pUS8MogFMiagxXweyGd3WQzry0JNsHWXhGLUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;290&quot; /&gt;&lt;/p&gt;

&lt;p&gt;人是智能的， 首先在于人能够按这套规则， 在不同的情况下做出反应，解决掉问题！ 比如己所不欲勿施于人， 或者以牙还牙以眼还眼， 或者门当户对， 或者春捂秋冻， 都是这个规则表的形式。 放到那个刚刚说的图灵纸袋上， 说不定那个小机器人就能像我们一样在社会里拼杀，解决从小到大的任务！  所以， 直接模拟这套知识组成的规则系统， 就成为人类所认为的模拟智慧机器的第一步。我们把这些人类的知识和规则直接输送给所谓的智能体， 让它直接利用这人类几千年的知识来运行， 应该就可以解决各行各业，甚至所有的复杂系统。    这个想法导致专家系统的产生， 做这个系统的人， 通常称之为知识工程师。&lt;/p&gt;

&lt;p&gt;6，70年代的知识工程师试图把人类所有领域的专业知识一条一条的输入给计算机，从而解决这个世界所有只有人类才可以解决的问题。 如果你去了解早期的人工智能系统， 你会了解到Elisa这种语言机器人，还可以了解到xcon的公司， 给美国工厂制造知识系统。 而知识工程最狂野的梦想， 以一个称为cyn的机器，它试图把人类所有的知识输入到这个机器里， 然后这个机器就可以达到人一样的智能状态。&lt;/p&gt;

&lt;p&gt; 应该说， 在有限的情况下， 可以说它们的表现真的非常接近人类。知识机器参与到医学诊疗这样的复杂过程，并在某些特定领域表现超过人类。 可惜这个流派在长期的努力里，能够作用的事情非常有限。这样的企图最终失败了。&lt;/p&gt;

&lt;p&gt;你能猜到这个失败的原因吗？ 真实世界的情况太复杂了！而且能够被知识和规则所表现的，只是冰山浮出水面的一角。 还不要说那无处不在的随机性。 人类语言和数学符号表达的那部分知识， 在真实世界就是碰壁。 不要说想象约会这样复杂的情况。 就是让一个机器人走到房子外500米处给你倒垃圾， 你觉得你需要写多少人类的知识法则作为基础？ 比如遇到行人让一下， 遇到车辆让一下， 垃圾满了换一个这些，都是极为局限的情况了（再比如如何判定秃顶）。   再复杂的专家经验，也无法穷举无限的可能性，再细致的规则， 也无法表达那些连人都难以表达的规律，一旦真实情况超出了专家系统已经写入的可能，机器就抓瞎了。不要说应对变化的规则。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;二 会学习的机器&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ok ， 怎么办， 我们要让机器学习到人类应对这种情况的本质-学习！  像人的头脑一样学习，能够从大量的经验里学习总结，从经验出发解决未出现的问题！ &lt;/p&gt;

&lt;p&gt;首先， 什么是学习， 我们理解学习的本质概念吗？ 你可能想说， 学习，就是变化， 对呀， 学习前后的你肯定是不一样的， 经过学习后，你的能力更强大， 你的知识更丰富。你是怎么学习的呢？ 读书？  根据经验学习？&lt;/p&gt;

&lt;p&gt;我们来展开上帝视觉， 从三个不同的角度看学习。&lt;/p&gt;

&lt;p&gt;伟大的希腊哲学家早就对学习的本质展开过探讨， 它们把学习分类为&lt;strong&gt;归纳法和演绎法&lt;/strong&gt;。苏格拉底是人， 人都是会死的， 因此苏格拉底会死。 这就是三段论， 或者称为演绎法的根基。 那么什么是知识呢？  人都是会死的就是知识。 如果我们把这个规则输入进去 ， 让机器给出一个答案， 那么这个过程就是刚刚讲到的专家系统。这显然不是学习，  那么反过来呢？ 反过来， 就是学习。我们先知道一个特例， 然后通过特例，得到这个“人都是会死的” 知识， 再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/p&gt;

&lt;p&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。  我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？  组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/p&gt;

&lt;p&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！   比如机器收集到一个苏格拉底死了， 那么苏格拉蒂是什么？ 男性，白种人， 哲学家，  于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！  这样的思维范式，就是归纳法，由于我们列举的假设依然用到了人类已有的知识， 因此我们得到的这个机器，事实上是最接近规则机器的一台学习机， 我们可以称之为规则为主体的归纳法。我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5903614457831325&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4ChMuwb9PMic4SIfHWzLXbZRAZILCAEFjllCeiaRmaeGXIgqefLmjSpzA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;332&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们把关于这个世界的互相矛盾的假设都丢尽机器。当然，事实上这个问题没有那么简单， 因为组成一个问题的假设可能成千上万。比如刚刚那个什么人会死的问题， 构成人的维度太多了， 远非三个，  比如年龄， 身高， 体重， 学历， 然后你要把所有的组成， 也就是这些特征所有不同的组合都做出会死或不会死的假设，再用刚才的统计机器的思路收集正负样本进行测试， 你看即使每个特征只有两个值， 你要验证的假设有多少个？（你立刻会感到指数爆炸的力量！ ）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。一种极端的情况是，你要把所有的地球存在过的人都输入到电脑， 它才可以学会判断什么人会死， 这样构建的学习器显然失去了任何作用。  &lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  所以， 整个后面的机器学习工作， 都在围绕解决这个灾难。 显然， 一个好的学习器需要可以从比较少的样例里， 得到能够判断比这些样例多的多的结果，这个通常称为&lt;strong&gt;泛化能力&lt;/strong&gt;， 就好像一种推而广之的能力， 一个好的学习者， 还是一个好的学习器， 都是要有这种能力。  &lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.7445945945945946&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4XExqvasWticuD9ehaBQuFeiazL5LEfoMicN72hz0eQJEZpp4IqichpMVAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;740&quot; /&gt;&lt;/p&gt;
&lt;p&gt;机器学习学家为了这个做足了功课， 让我们有一些方法， 比如我们后面会细讲的决策树，这个方法非常接近刚刚说的那个把很多特征放在一起，构成不同假设后连线的问题， 但是它使用了一个关键性的知识， 那就是，不同的特征处并非平等，比如男性和女性， 很可能比哲学家有更重要的影响， 如果我们能够按照特征的不同等级做分组， 就可以极为轻松的解决这个问题，比如我们能够判断出男女是判断生死最终要的特征，男人都会死， 女人需要做进一步判断， 那么一瞬间，我们就解决掉了一半的假设， 只要是男人， 我们就和会死连在一起就好了。   &lt;/p&gt;

&lt;p&gt;我们用于归纳的数据永远是部分的真相， 最终在算法的使用阶段所遇到的数据却永远是新鲜的，这个问题， 归根到底， 就是机器学习的过拟合问题， 而它的根源，确是归纳法本身的问题， 就像尼古拉斯塔勒布的黑天鹅一样，你永远不会知道明天你的池塘是否会飞起一个黑天鹅，从而把你刚刚学到的天鹅都是白的假设给推翻。&lt;/p&gt;

&lt;p&gt;大家注意，上述这套思维本身是有局限性的。因为很多假设并非非黑即白。 可能我们继续收集数据， 发现又有一个叫xx的哲学家白人男性没有死， 这个时候机器不就傻眼了？ 这就是刚说的规则连线法的致命弱点， 落下了一个可能的解决方法就是概率。&lt;/p&gt;

&lt;p&gt;既然有限的数据无法得到一些肯定的答案，某个事实对或者不对，那为什么不给那些模棱两可的假设留下一些空间呢？ 我们保留所有可能的假设， 不要扔掉它们， 最初，我们给每个假设设定一个成立的可能性， 这就是概率（由于是学习前的概率，叫做先验概率）， 然后， 一旦数据到来， 我们不像之前一样直接给出连线得到是否， 而是调整这个概率。 你脑子里把这个概率想象成一个小红线， 小红线越长代表概率越高，如果这个答案支持这个假设，我们就把这个小红线拉长一条，代表我们更肯定这个假设是正确的， 这就是&lt;strong&gt;贝叶斯方法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6507936507936508&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4g0uc17cX0MZuBBLCyDyAgq6GUyqRPJm6BH6HZ4vibFR5DEVKWpZDtLw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;315&quot; /&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯神父早已在两百多年前想到的这个方法， 可以说构成了机器学习的另一大基础流派，就是贝叶斯派。贝叶斯派试图把特征条件，到他们引发的结果， 用概率的箭头连接。 然后我们就得到了一个无比巨大的条件之间互相连接的关系网络，又称贝叶斯网络， 用这个方法，我们可以世界万物的联系浓缩进去， 比如刚说的白人， 男性，哲学家现在变成了被连接起来的三个方块，最后一个会死也一样，  这三个特征加上结论互相影响。 白人男性，可能比白人女性更容易是哲学家， 而这三个条件又在影响是否会死，我们通过不停的收集数据来修正每个小红箭头对应的概率，直到这个网络变得稳定和完美，它就可以源源不断的告诉我们事实。  &lt;/p&gt;

&lt;p&gt;人是极其的不擅长概率性思维的生物， 贝叶学派的人工智能，把学习的过程看作一个由结果推测原因出现概率的过程， 这样就可以得到一个规则的集合。 这一类学习方法， 事实和开始的符号推理某种程度是殊途同归的， 只是在此处， 我们更看重统计的概率。&lt;/p&gt;

&lt;p&gt;我们再次回到学习的本质。 刚刚说的归纳法和演绎法， 是古希腊哲人对学习的理解。而后来人对学习的理解则是完全不同的。 尤其是在生物学起步之后，达尔文的进化论， 到脑科学的出现， 人们开始从生物本质来研究学习是什么。 既然学习是人脑的专长， 那么我们是不是可以模拟人脑的物质基础，来实现学习， 或者说， 做一个机器大脑！&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.6661538461538462&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4VyICsZuoYToMtHG3ZdpOwUZ5DyibupFQtakRswXvUs5Em2xEibHrFBdQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;650&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这是有可能的， 而且导致了机器学习的第二大分支， 连接主义。  连接主义认为， &lt;strong&gt;信息和概念存储在大脑的突触连接之间， 特定的连接形式对应特定的知识。&lt;/strong&gt; 如果我们要让机器能够学习， 就是要让它能够通过学习大脑的连接，来掌握特定的知识。  &lt;/p&gt;

&lt;p&gt;神经元是如何组织的这件事子啊很长时间对人类过于复杂，直到50年代的一天，一个叫hebb的老头提出了一个怪异的想法：人脑是一大堆神经元的网络，而网络权重可以随着自身活动变化，一起放电的细胞会加强彼此的联系，更加容易一起放电：  Hebb学习规则的结果是使神经网络能够提取训练集的统计特性，从而把输入信息按照它们的相似性程度划分为若干类。这一点与人类观察和认识世界的过程非常吻合，人类观察和认识世界在相当程度上就是在根据事物的统计特征进行分类。&lt;/p&gt;

&lt;p&gt;举个简单的例子， 说明， 学习， 就是改变连接。第一个是巴甫洛夫的条件反射实验：每次给狗喂食前都先响铃，时间一长，狗就会将铃声和食物联系起来。以后如果响铃但是不给食物，狗也会流口水。你怎么用hebb法则解决这个问题？ 假定铃声检验对应一个神经元， 食物检验对应一个神经元，  分泌口水对应一个神经元， 一开始食物检验可以引起口水， 但是我们每次给食都有一个铃声，记得一起活跃的神经元连接加强吗，铃声和口水经常一起活跃， 于是它们的连接就加强了。   下一次， 只有铃声， 没有食物， 狗也开始分泌唾液了。  &lt;/p&gt;

&lt;p&gt;我们来看看最早的把连接主义引入机器学习的尝试。 最早的连接主义尝试是模拟大脑的单个神经元， Warren McCulloch 和 Walter  Pitts  在1943 提出而来神经元的模型， 这个模型类似于某种二极管或逻辑门电路。 一定的输入进来，被神经元汇集加和， 如何这个和的总量大于一个阈值，神经元就放电， 小于一个阈值，神经元就不放电。   这个东西就好像某个微小的决定装置， 把很多因素加载在一起， 做一个最终的决策。 我们想象无数的二极管可以构成一个计算机，那么无数这这样的神经元不就可以构成一个具有计算功能的大脑吗？ 这就是感知器的概念。  好了， 这里哪来的学习功能呢？&lt;/p&gt;

&lt;p&gt;单个感知器的学习功能确实很弱， 原因在于，我们没有真正的多个神经元之间的连接。当然， 这里也不是没有可以学习的东西，比如对不同输入的权值是可以调节的。&lt;/p&gt;

&lt;p&gt;还记得我刚刚说的学习就是改变连接（权重）吗？ 假定我们要学习辨析两个概念，一个是苹果， 一个是香蕉，还是刚刚的方法， 我们通过一定的特征， 组成一些假设， 比如颜色和形状， 我们颜色只取黄色和红色， 形状只取圆型和长形， 然后结论是苹果或者香蕉。由此我们会得到8个假设。 然后我们要构建一个感知机对它进行判断。 假定感知机被连接到这四种输入特征上，  然后我们需要输出一个数， 来做判定， 一开始4个连接权重都是1，  并且我们给每个特征的值都设为1. 假定我们给它很多红苹果和黄香蕉的数据样例，  让它来判断， 一开始当然机器给出一样的数字完全无法判断。经过学习呢？ 那个对应黄色的权重会逐步调整为-1， 对应长条形的权重也会逐步调整成为-1， 这样经过一段时间， 香蕉呈现给这个感知机，它会给出-2， 苹果， 它会给出+2， 只要我们设定为大于0输出苹果， 小于0输出香蕉， 那么这个机器就可以判定苹果和香蕉了。&lt;/p&gt;
&lt;p&gt; &lt;img data-ratio=&quot;1.6111111111111112&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;360&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不过你很快会问万一出现几个黄苹果呢？ &lt;/p&gt;

&lt;p&gt;假定我的输入是三个要素， 今天的天气， 我的心情，  外面活动的人数来决定我去不去公园， 那么这三个要素对我决策的重要性就是我学习的目标，没有其他的了。那么此处学习的本质， 就是学习权重， 学习的方法， 依然是之前讲到的， 从特例里学习， 我们可以给定一个初始化的权重， 和一个惩罚函数。 我输入给这个网络一个不同天气情况， 心情， 活动人数， 我去没有去公园的数据， 这个时候感知器对每个情况下我最后去和没有去公园做预测，如果预测错误， 惩罚函数就会发生作用， 指导我向正确的方向调整权重， 就是学习的过程。&lt;/p&gt;
&lt;p&gt; &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;事实上， 人们很快发现感知机的学习有巨大的局限性， 我们很快发现它连抑或这样基本的逻辑运算都无法执行，也就开始对他心灰意冷。  对感知机的失望导致连接主义机器学习的研究陷入低谷达15年， 指导一股新的力量的注入。&lt;/p&gt;

&lt;p&gt;这个新的力量， 来自一群好奇心极强的物理学家，在20世纪80年代， hopefiled提出了它的hopefield网络模型，这个模型受到了物理里的ising模型和自旋玻璃模型的启发，Hopefield发现，自旋玻璃和神经网络具有极大的相似性。 这些听起来是鬼话， 你可以这样理解， 这个模型里又很多的神经元，每个神经元可以看作一个个微小的磁极，它可以一种极为简单的方法影响周围的神经元，一个是兴奋（使得其他人和自己状态相同）， 一个是抑制（相反）。  如果我们用这个模型来表示神经网络， 那么整个问题变得极为简单。&lt;/p&gt;

&lt;p&gt;因为物理学家已经求解过自旋玻璃模型， 所以很多结论都可以直接套用到神经网络里面来。比如说自旋玻璃有个能量的概念， 大家不要慌张， 这个能量的概念无非说的是我们可以把磁极之间的相互作用总量表示成为一数学量。  然后物理学家直接剖出，系统要呆在能量最小的状态才稳定， 这样， 我么就直接得到那些最稳定的神经元活动态， 这是一种非常特定的状态。 就好像操场上训练的哨兵， 每个人都整齐划一的迈着正步，我们用一个词“模式”来形容。  这个整体的模式有什么作用呢？ 我们发现它可以表示和记忆信息！ &lt;/p&gt;

&lt;p&gt;比如说吧， 我要识别某些图片是否属于一个人的脸。你把这个图片用某个方式输入到这个网络里，刚不是说了网络会到达一个特定的状态吗，刚刚好，对应于同一个人脸的照片会导致神经网络到达一个同样的集体状态， 你想象你的照片引起那些神经元用一个姿态迈着正步走， 那么， 你的信息就算是被网络表征和记忆了， 这个网络具有了学习能力！&lt;/p&gt;

&lt;p&gt;这套想法的威力在于， 我们发现了问题的本质， 可能在于神经元的数量，即使每个神经元的能力已经愚蠢至极了， 只要我们能够有足够多的神经元，它也可以干很复杂的事情。这个想法， 引起了神经网络研究的一股旋风， 人们从不同领域开始涌入这个研究。有的人想用这个模型研究人脑， 有的人想用这个模型制造机器大脑， 前者派生出了计算神经科学，后者则导致了联结主义机器学习的复兴（研究猫的和研究机器猫的）。 这批人物里， 有个心理学进来的小伙子叫辛顿， 在漫长的时间里， 它将会把连接主义推向一个新的高潮。&lt;/p&gt;

&lt;p&gt;在漫长的联结主义低谷期， Hinton坚信神经网络既然作为生物智能的载体， 它一定会称为人工智能的救星， 在它的努力下， Hopefield网络很快演化称为新的更强大的模型如玻尔兹曼机， 玻尔兹曼机演化为受限玻尔兹曼机， 自编码器， 堆叠自编码器。算法的进步更多体现在学习方法的改进。 信息存储在无数神经元构成的网络连接里， 如何让它学进去， 也就是最难的问题。 一种叫反向传播的方法60年代就开始出现， 在hinton等人的持续改进下， 终于开始发挥作用，并逐步统治。 它的意思其实是把学习理解成为一个巨大的根据数据来优化的过程， 数据犹如一颗颗子弹打进来， 如果神经网络的预测错误 ，它就会在网络的连接之间一点点的引导网络权重的改变，虽然每次只改一点点， 最终当数据的量特别巨大，却发生一场质变。&lt;/p&gt;

&lt;p&gt;但这还不是全部， 人类很快从模拟人类大脑里汲取更多的养分，来辅助我的人工智能，比如说视觉识别，越是简单的事情， 我们越说不清自己是怎么想的， 我们唯一能做的是打开视神经， 把视神经的细节一一的融汇到神经网络里， 由此诞生了CNN和整个深度学习模型。&lt;/p&gt;

&lt;p&gt;当然，这只是这个故事的一部分，这个故事的另一部分， 是计算机硬件的进步，从原始的计算机， 286， 386 到奔腾， 到GPU计算，一次次的硬件突破，使得大规模的使用BP算法进行优化成为可能。 另一方面， 几个和hinton一样执着的人，一点点的在那里收集数据， 它们建立了一个叫Imagenet的数据库， 这个数据库收集了整个互联网的图像， 期待机器有一天能够理解它们。它们，与算法革命一起催生了深度学习革命。&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;1.0166666666666666&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHuFYrqltaPicLd3e23jyQPBrLHqMDU3s3zvC2Aaq6hicssOlvOfwSshyKQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;240&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这场革命的催生下， 机器不仅能够学习推理， 而且开始接管人类最重要的一种能力-直觉。 机器能够在图像中识别出猫狗， 你和我， 甚至也可以看出一个人的情绪。 能够掌握直觉， 正式深度学习最反直觉的地方。 构成我们决策的大量因子， 其实是我们自身都无法描述的隐形知识， 抑或直觉， 这些， 能够被神经网络学习。在此前连人自己都不理解是怎么发生的。&lt;/p&gt;

&lt;p&gt;CNN一旦出现就开始疯狂生长， 自从在Imagenet上对图像识别夺冠并出现人类 ， 网络越变越深， 出现了一个个名字怪异的新网络，如残差网络，谷歌网络这些， 而它们也一步步潜入那些人们起初没有想到的领域，比如语音识别， 甚至下围棋。而深度学习另一条主线， 沿着让机器听懂人类的语言， 一种叫LSTM的神经网络， 模拟了人类最奇妙的记忆能力， 而开始逐步的替代人类承担起类似翻译的作用。&lt;/p&gt;

&lt;p&gt;好了， 连接主义暂时段落， 我们继续沿着学习的本质来看还有哪些机器学习的流派。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  要真正的掌握人类的智能， 我们需要从硬件层面突破，这个突破的方法就是模拟人脑的结构。我们需要先研究人脑的结构， 尤其是智能组成的基础， 人脑神经网络来达到这点。我们刚刚谈到了模拟生物学习来实现人工智能。 事实上除了模拟大脑的算法。 还有一种更为本源的想法就是&lt;strong&gt;进化&lt;/strong&gt;论。 事实上， 整个由达尔文提出， 经过一两百年发展的进化论可以看作一种学习算法，只不过它在绵长的时间里所进行的，而且是被动型的学习。&lt;/p&gt;

&lt;p&gt;我们来看这个算法的细节以及为什么：&lt;/p&gt;

&lt;p&gt;首先生物的行为无论是否是大脑决定的必然都有其基因基础， 还记得我们之前说的图灵机吗？生命可以表达成为一大堆不同情况下的行为规则，这一堆行为规则其实就是DNA。 每一个碱基对如同规则表的字母。  进化算法就是对这套规则系统的学习和优化。学习的实现通过几步来实现：  1， 遗传， 亲代可以把编程传给子代   2， 变异， 这个过程中一些随机性因素导致编码变化  3， 性：  编码进行交叉  4，  环境不停改变  5， 自然选择， 合适的基因被挑选。  这样， 经过极为漫长的时间， 我们总可以得到一张适合的规则表。&lt;/p&gt;

&lt;p&gt;这就是自然里面， 以复杂制服复杂的方法， 再聪明的个体，也无法穷极变换无穷的环境的所有可能， 而自然挑选的进化算法虽然缓慢， 得到的物种却可以天衣无缝的嵌入环境。  我们可否师法自然把它用于学习算法呢？ 当然可以，我们几乎完全照搬上面的方法， 就可以得到一种和自然进化类似的算法，这一套方法可以教计算机得来几乎和人的运动类似的行为模式，也可以帮我们找到最佳的宏观经济调控政策。   &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3GnvDlNCSjsGosHtINzN4mz72ticKkT97uufzQfUKPibhCVGIKl7wF9O1UWwFbtjsr8vasDM7gR2w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;连接主义和进化算法分别代表了师法自然的两个不同流派，两者并行又相交。所谓相交， 两者一旦结合， 会产生更大的威力。 因为&lt;strong&gt;进化算法擅长的是做非常大尺度的变化&lt;/strong&gt;。比如进化， 可以把线虫一步步变成复杂的人类。但是缺点是效率低下， 要知道这个过程自然可是要用数十亿年。 而连接主义的神经网络， 要学习一套规则表示的速度要快很多 ， 因为它所用的BP算法， 好比不停的瞄准远方的靶子射击的过程， 你每次看到你的子弹离靶心的距离， 从而可以不停的调整枪位， 但是连接主义的方法需要一开始规定好网络的结构而不可以做更大规模的改动。如果用进化算法来设计网络框架， 再用BP来得到好的连接，这个过程就好很多了。 这也是自然先通过进化得到人类， 再让人类通过自己的头脑得到更复杂的知识和组织的过程。&lt;/p&gt;

&lt;p&gt;还有一个重要的思路来源于仿生的学习流派， 就是&lt;strong&gt;强化学习&lt;/strong&gt;， 这个学习流派说的：动物的学习多经过行为反馈， 它做出一个行为， 如果行为得到好的结果，这个行为就要被加强， 如果是坏的结果， 就要减弱。 这可比先要传递自己的DNA，在被自然选择的进化算法来的快多了。直接模仿这个思路的学习方法就是强化学习。 这个思路威力巨大，因为它解决了处在智能中心位置的决策问题，  一旦和连接主义碰撞， 就诞生了如今最强大的人工智能作品，阿法狗和阿法元。&lt;/p&gt;

&lt;p&gt;人工智能的这些不同的流派， 既来源不同， 又互相交叉。 那么， 一波三折的人工智能里， 当家花旦是深度学习， 这个来自连接主义学派的极大成之作。 然而即使是那些我们完全称之为深度学习的算法，也不是完全只用了连接主义一家。比如阿法狗和阿法元， 那里面自然用到了深度残差网络， 但是其更根基的部分确实包含了很多别家算法， 比如逻辑， 符号等学派观点的东西， 如果没有这些作为根基， 就谈不上这些成就。&lt;/p&gt;

&lt;p&gt;三 &lt;strong&gt;离终极算法有多远&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;回顾过去， AI极大流派如同做过山车一样起起伏伏，我们曾经崇拜符号主义忽略连接主义，现在正好反过来， 那么， 这样的历史是会不断的重演， 这样的螺线向上的S曲线还会重复， 抑或是我们已经进入了一个完全不同的新纪元？ 我认为， AI发展的真正瓶颈依然在于我们对人脑自身算法理解的透彻程度。&lt;/p&gt;

&lt;p&gt;有人认为， 所谓的终极算法， 正是人脑自身所使用的算法， 这种算法&lt;strong&gt;必然如同所有的物理规律一样， 具有某种大统一的形式， 而不会是声音一块， 图象一块， 逻辑思维又一块&lt;/strong&gt;。 假使人工智能的发展有一个上限，我认为这个上限应该存在于对这个终极算法的认知程度。 有某些证据表明， 我们在一步步的接近这个终极算法， 比如当下的卷积神经网络， 事实上既能够看画面， 又能听声音，具有我们所俗称的“&lt;strong&gt;抽象&lt;/strong&gt;”能力。 &lt;/p&gt;

&lt;p&gt;然而， 一旦深入到更深层的问题， 卷积网络， 加上LSTM这类的具有记忆的时序神经网络，能否解释我们的逻辑思维， 更深层的我们的目的和动机， 我们的自我意识， 我们就一问三不知了。 有可能， 这些东西本来就是一种进化的副产品， 也就是说， 我们虽然有自我意识，但它并不是解决一些复杂问题的必要条件，简单的说就是和智商无关，也有可能， 本来这个东西就是解决一些最复杂问题的基础， 可悲的是， 目前的任何心理， 或生物， 或数学理论， 对这个问题几乎一无所知。&lt;/p&gt;

&lt;p&gt;当然还有一些问题， 提示我们可能离真正的终极算法还有距离，比如大脑对数据的应用&lt;strong&gt;效率&lt;/strong&gt;和AI算法并非一个等级， 你看到一个数据， 就可以充分的提取里面的信息，比如看到一个陌生人的脸，你就记住他了， 但是对于目前的AI算法， 这是不可能的， 因为我们需要大量的照片输入让他掌握这件事。 我们可以轻松的在学完蛙泳的时候学习自由泳，这对于AI，就是一个困难的问题， 也就是说，同样的效率， 人类脑子能够从中很快提取到信息， 形成新的技能， AI算法却差的远。  这是为什呢？ 可能这里的挂件体现在一种被称为迁移学习的能力。虽然当下的深度学习算法也具备这一类举一反三的迁移学习能力，但是往往集中在一些真正非常相近的任务里， 人的表现却灵活的多。这是为什么呢？ 也许， 目前的AI算法缺少一种元学习的能力。何为&lt;strong&gt;元学习&lt;/strong&gt;， 就是提取一大类问题里类似的本质， 我们人类非常容易干的一个事情。 到底什么造成了人工神经网络和人的神经网路的差距， 还是未知的， 而这个问题也构成一个非常主流的研究方向。&lt;/p&gt;

&lt;p&gt;另外一个重要的蛛丝马迹是&lt;strong&gt;能耗比&lt;/strong&gt;。如果和人类相比， 人工智能系统完成同等任务的功耗是人的极多倍数（比如阿法狗是人脑消耗的三百倍， 3000MJ vs 10MJ 5小时比赛）。 如果耗能如此剧烈， 我们无法想象在能源紧张的地球可以很容易大量普及这样的智能。那么这个问题有没有解呢？  当然有， 一种， 是我们本身对能量提取的能力大大增强，比如小型可控核聚变实用化。 另一种， 依然要依靠算法的进步， 既然人脑可以做到的， 我们相信通过不断仿生机器也可以接近。 这一点上我们更多看到的信息是， 人工智能的能耗比和人相比，还是有很大差距的。&lt;/p&gt;

&lt;p&gt;我们离终极算法相差甚远的另一个重要原因可能是现实人类在解决的AI问题犹如一个个分离的孤岛， 比如说视觉是视觉， 自然语言是自然语言，这些孤岛并没有被打通。 相反， 人类的智慧里， 从来就没有分离的视觉， 运动或自然语言， 这点上看， 我们还处在AI的初级阶段。我们可以预想， 人类的智慧是不可能建立在一个个分离的认知孤岛上的， 我们的世界模型一定建立在把这些孤立的信息领域打通的基础上， 才可以做到真正对某个事物的认知，无论是一个苹果， 还是一只狗。另外， 人类的智慧是建立在沟通之上的， 人与人相互沟通结成社会， 社会基础上才有文明， 目前的人工智能体还没有沟通， 但不代表以后是不能的， 这点，也是一个目前的AI水平与强AI（超级算法）的距离所在。  &lt;/p&gt;

&lt;p&gt;有的人认为， 我们可以直接通过模拟大脑的神经元，组成一个和大脑类似复杂度的复杂系统， 让它自我学习和进化，从而实现强AI。 从我这个复杂系统专业的角度看， 这还是一个不太现实的事情。因为复杂系统里面最重要的是涌现，也就是说当组成一个集合的元素越来越多，相互作用越来越复杂，这个集合在某个特殊条件下会出现一些特殊的总体属性，比如强AI，自我意识。  但是我们几乎不可能指望只要我们堆积了那么多元素， 这个现象（相变）就一定会发生。&lt;/p&gt;

&lt;p&gt;至于回到那个未来人工智能曲线发展展望的话题， 我们可以看到， 这些不确定的因素都会使得这条发展曲线变得不可确定。 然而有一点是肯定的， 就是正在有越来越多非常聪明的人， 开始迅速的进入到这个领域， 越来越多的投资也在进来。 这说明， AI已经是势不可挡的称为人类历史的增长极， 即使有一些不确定性， 它却不可能再进入到一个停滞不前的低谷了， 我们也许不会一天两天就接近终极算法，但却一定会在细分领域取得一个又一个突破。无论是视觉， 自然语言， 还是运动控制。&lt;/p&gt;

&lt;p&gt;我觉的人工智能未来发展最大的变数， 在于人们是否能克服虚化浮躁的心态， 去真正的沉下心来做理论研究。 因为本质上，我们在人工智能的研究上所作的， 依然是在模拟人类大脑的奥秘。 我们越接近人类智慧的终极算法，就越能得到更好的人工智能算法。 &lt;/p&gt;

&lt;p&gt; 更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384053&amp;amp;idx=1&amp;amp;sn=a1292fa38d2b3da000555b4b5ba92849&amp;amp;chksm=84f3c6b4b3844fa27ee93534098a20d3dd745089558629a502bb3b3a264ab3c1a523dceaf01b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;给小白看的AI最小入门指南（一）&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;












</description>
<pubDate>Sat, 23 Feb 2019 06:44:47 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/faWzusH9s2</dc:identifier>
</item>
</channel>
</rss>