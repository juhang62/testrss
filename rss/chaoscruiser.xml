<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>预测神经网络预测准确性的普遍理论</title>
<link>http://www.jintiankansha.me/t/ctLLjVRfg2</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ctLLjVRfg2</guid>
<description>&lt;p&gt;Heavy-Tailed Universality Predicts Trends in Test Accuracies for Very Large Pre-Trained Deep Neural Networks 是今年1月24日在Arxiv上post的一篇论文。作者还有针对这个话题的一系列偏理论的文章，本篇是其中最实用的一篇&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cdXGY1uicfP1GvdpRUIsTzVeCW0jQBn23FMOTGtQu1BCFzKExl0uAKrA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;845&quot; data-cropy1=&quot;10.638489208633095&quot; data-cropy2=&quot;411.8615107913669&quot; data-ratio=&quot;0.4757396449704142&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc8FWgochyYCx565Py8uo3cwDQ2Mb5ROibQ3rN9plKq3Sc88iafn561z58fbL7icweEuEmbPlLdYQZcg/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;845&quot; /&gt;&lt;/p&gt;


&lt;p&gt;这篇文章对神经网络的泛化能力建立了一个大一统性质的理论，不仅能够解释为何神经网络中的各种正则化手段有效，还能够用一个指标预测一个训练好的网络的泛化能力，这篇文章中有很多在我看来高深的数学，其中很多我觉得较难理解，因此这里只概述我理解的部分，写下这篇论文笔记，更多的是像行家请教。&lt;/p&gt;

&lt;p&gt;初学深度学习的时候，我被各种各样的正则化方法搞的很迷茫。传统的机器学习中就是增加L1或者L2正则项，到了深度学习，减少batch size，dropout，early stopping等很多看似完全不同的方法，都被称为正则化，似乎所有能够增加模型泛华能力的都是正则化方法。 有了这么多的正则化方法，对于有监督学习，使用同一数据训练的两个DNN，训练时用到了不同的超参数，不同的优化方法与正则方法，我们除了让模型在真实数据上跑一下，没有方法提前估计模型的泛化能力。如果能够通过对模型本身的分析，就能评估出模型相对的泛化能力，那在对模型进行fine-tune(微调)的时候，就能够起到指导。&lt;/p&gt;

&lt;p&gt;验证该理论的是数据来自真实世界中的神经网络，通过比较针对ImageNet数据集上的50个预训练过的DNN，在不改变模型的损失函数与网络结构，不需要重新训练模型，甚至不需要导入测试数据时，通过计算网络中每一层权重矩阵的Frobenius norm的指数平均值，发现该平均值和模型的预测准确性具有相关性，从而能通过上述指数，预测模型的泛化能力。用于验证的模型涵盖了15个不同的网络结构，例如VGG16，ResNet等，这说明了该方法对各类模型都适用。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.4737394957983193&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cLZ2JXlDD2oOoiaeQrbgLYspwyNHwOBlgib2w2WZbF5Le1dzy1iaqica4QQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;952&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里展示了预训练的不同VGG和带批量正则化的VGG网络的表现，这里的横轴是该网络的预测准确性，纵轴是作者文中定义的评估模型范化能力的指标，左边的图是之前的方法，右边的是本文新提出的指标。可以看出这里拟合的很好，而且是接近线性的拟合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.5208333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cIDzvBEpuFumLXV3hMDZXofEIRVwjJcJNVIsgq9rfhicER0guqamqibOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;912&quot; /&gt;&lt;/p&gt;
&lt;p&gt;上面反映的是不同的ResNet上的情况，除了部分离群点，文中提出指标也反映了模型相对的好坏。更多网络结构下的对比情况如下所示；&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;1.6070726915520628&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cSKGRj6zgA6HkgSGOYibORbAvR9eWDXZceSTxTrJkjHKozSS39Uiaz9rQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;509&quot; /&gt;&lt;/p&gt;
&lt;p&gt;该作者还提供了一个pytorch及Keras下的package，名为WeightWatcher，可以用一行代码，将训练好的模型当作参数传入，就可以计算出上述指标。笔者试用了，挺方便的。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/CalculatedContent/WeightWatcher&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.3822843822843823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cxxK7f8OnePohbahEEibCBy5iacT6xBvAEkYJ8a0aHUPZFZBHOI2WzweQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;429&quot; /&gt;&lt;/p&gt;


&lt;p&gt;接下来要解释这背后的数学原理了，首先对每层网络的权重矩阵W，构建相关矩阵X&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.3722943722943723&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cJnDzhnOawRIiagfv2ibPHqvzy50PPID5En5xvo2tVGuY7sO6egd1gvBg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;231&quot; /&gt;&lt;/p&gt;
&lt;p&gt;对X计算矩阵的秩，将该矩阵的秩写成一系列秩的加权，将其称为Empirical Spectral Density (ESD)&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.24634146341463414&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cxM1RQcicDC73435khfQPCg8NzCf4ibOa9rMb8GmhNxP5BOIyETw965Tg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;410&quot; /&gt;&lt;/p&gt;
&lt;p&gt;之后作者指出可以用一个由随机数生成的矩阵的秩进行幂律运算，来拟合这个分布，这里引入了random matrix theory&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.2925764192139738&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cI6maPKuD2wiaUrTLC99T5ERNTjaHJLFbeic4K68FSEdYLApWnUWW3geg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;229&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而这里的alpha则代表了幂律分布的尾巴有多长，对于ImageNet中的7500个权重矩阵，下图代表了不同矩阵最佳拟合的alpha落在不同区间的次数，图中70-80%的案例，图中的alpha都落在了2-4之间，也有少数极大的情况。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.8426150121065376&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cDnwBxgKicJWXr26T9IJ10y9SdSusoutAm9dSbEH19gKtVI8CICJZmWw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;413&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而上文中出现的用来预测模型预测精度的纵轴&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.8823529411764706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3ck9XN8klS5ic6EpFueJibs4MQS1I256uQz8mjSTrHgsOJ8g71eEwM6Bng/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;51&quot; /&gt;，就是所有层的加群平均， 不同深度的层数对应的权重由式中的beta控制，&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.2826086956521739&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3c2gAwJB9ibQZX8qsOWHY5DPSu1KscbTJmibMBbQdGSYhdbfzeelk1mohQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;322&quot; /&gt;&lt;/p&gt;

&lt;p&gt;之后文章指出，正则化不管怎么去做，都是要避免权重矩阵的秩被过度长尾（heavy tailed)的随机矩阵拟合，也就是说上文中每一层都需要alpha值越小，网络在该层过拟合的风险越低。权重矩阵的秩呈现过度长尾的幂律分布，可以直观的想象成对于某些像素点赋予高的不合比例的权重，而这意味着该层网络的观察野受到了局限，对高权重位点的变化及其敏感，而这会使得模型更容易过拟合。因此将网络中每层的权重矩阵的alpha做权重衰减的加和，就可以用来评估模型是否过拟合。&lt;/p&gt;

&lt;p&gt;总结一下，本文提出的衡量模型相对泛化能力的普适性方法，利用了已有的成熟模型，发现规律，并将其用在指导新模型的训练，该方法可以用在迁移学习和模型微调中。这篇文章的数学我对其只看懂了皮毛，有一些问题，文中也没有给出解答，第一是对于非CNN系列的网络结构，例如capsule network，图卷积网络GCN等，该文的方法是否适用，第二是对于autoencoder系列的模型，如果以重构误差作为横轴，能否也通过文中的指标评价模型的泛化能力，第三点是该方法用在图像切割上，是不是也会有较好的效果，第四个问题是该方法在NLP任务中是否适用。&lt;/p&gt;

</description>
<pubDate>Thu, 28 Mar 2019 06:59:59 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ctLLjVRfg2</dc:identifier>
</item>
<item>
<title>贝叶斯推理实用入门</title>
<link>http://www.jintiankansha.me/t/EY2XU24xDA</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/EY2XU24xDA</guid>
<description>&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;什么是贝叶斯推理， 我早在过去的文章里分析过有关贝叶斯概率的知识， 例如&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381931&amp;amp;idx=1&amp;amp;sn=2eb7ab8b5dadda70d74ae6289ef361f8&amp;amp;chksm=84f3ceeab38447fc736fd8a9e6494b663c12dfa3c347b37054d926751141bbe0ed61f0890d11&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;朴素贝叶斯之实践篇&lt;/a&gt;，这次融入纽约大学weijima的方法论教程（http://www.cns.nyu.edu/malab/index.html），给大家一个更实用的版本。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;一，什么是贝叶斯概率， 它于经典概率由什么关系.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e3m1u-0-0&quot;&gt;谈贝叶斯首先是用概率量化问题。 概率这件事大家都觉得自己很熟悉， 叫你说概率的定义 ， 你却不一定说的出。经典的概率， 说的是事件发生的可能性。 我们中学课本里说概率这个东西表述是一件事发生的频率， 这个频率就代表某件事发生的可能大小。 或者说这叫做客观概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;948n2-0-0&quot;&gt;而贝叶斯框架下的概率理论确从另一个角度给我们展开了答案，&lt;/span&gt; &lt;span data-offset-key=&quot;948n2-0-1&quot;&gt;他说概率是我们个人的一个主观概念， 表明我们对某个事物是否发生的相信程度&lt;/span&gt;&lt;span data-offset-key=&quot;948n2-0-2&quot;&gt;。 如同Pierre Lapalace说的: Probability theory is nothing but common sense reduced to calculation. 这正是贝叶斯流派的核心，换句话说，它解决的是来自外部的信息与我们大脑内信念的交互关系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6jnh-0-0&quot;&gt;两种对于概率的解读区别了频率流派和贝叶斯流派。同时我们不难看出两者之间的联系， 你对一件事情发生的可能性估计正是基于某种频率的统计。 但是它们的区别在哪里呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bc3bn-0-0&quot;&gt;首先，给你下面的事件， 假定你带着孩子去看月亮， 然后孩子说月亮的属性是块奶酪， 你会跟它怎么说呢？  首先， 你一定知道月亮是一个石头的星球而非奶酪， 那么这件事你要如何去跟孩子说呢？ 首先我们生活在概率的世界， 你要和它说的是你可以认为月亮是石头或者奶酪， 但是你不要相信任何一个， 既然不相信， 你把它们称为假设1 和假设2，  然后你给它们各自一个数字来代表可能性的大小， 这就是概率。 然后我们看频率观和贝叶斯的区别&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4h102-0-0&quot;&gt;1， 频率观的家长： 到天空做一些测量， 看看奶酪和石头的比例， 然后算出假设1和假设2的概率。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bj860-0-0&quot;&gt;2，  贝叶斯的家长：  孩子我们去不了天空， 但是我们可以想象下我们生活中的经验， 然后查看一下教科书。 首先， 教科书里说， 到目前为止，天空中发光的99.99%是石头。   然后，  再联想下生活经验， 如果是奶酪， 那么它确实是黄灿灿的发光， 因此生活证据显示， 月亮是奶酪的假设并不违和。 那么把两个综合一下， 通过一系列后面会说的公式， 你给出孩子它的观测结合书里的知识的合理性概率： 月亮是石头的概率99.9%。 贝叶斯通过承认我们自身的无知，给不同的假设以调整空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.05&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Er0r4hHxCTIVzGB4kdGibZPHZDLhufbpAYHZGMGIAKO3cGxYZ1ddJkg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f45d1-0-0&quot;&gt;哪个过程更合理？ 哪个方法更正确？ 你自行去分析。这里要说的是，&lt;/span&gt; &lt;span data-offset-key=&quot;f45d1-0-1&quot;&gt;在真实世界里， 我们所做的往往是把现象的经验推理， 和某种先验结合， 去估计事物的可能性，这正是贝叶斯的思路 . 没有人会对每件重要的事做无限的测量， 也不是所有事件都可以重复（分手不可以， 股灾不可以），这是我们唯一可以做的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7kf2h-0-0&quot;&gt;贝叶斯的数学公式十分简单， 一， 你要有先验概率P（A），二， 似然性  P（B|A）， 最终得到后验概率P（A|B）。这三者构成贝叶斯统计的三要素。&lt;/span&gt;似然性实用条件概率表达， 后验也用条件概率来表达， 基于此的贝叶斯定律数学方程极为简单：&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.30434782608695654&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6KfJ3pWuOyaw1gf77HOwiaPIQef4gxPOfw3UcicvlTZB44OlFxQalW4lw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;184&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-0-0&quot;&gt;套用月亮的例子，P(A)代表月亮是奶酪的假设， P(B|A)代表现象黄色发光。 &lt;/span&gt;&lt;span data-offset-key=&quot;dc4dh-2-0&quot;&gt;即月亮是奶酪的先验概率，&lt;/span&gt; &lt;span data-offset-key=&quot;dc4dh-4-0&quot;&gt;是如果月亮是奶酪， 那么它是黄色发光的概率， 你得到前两者，就可以根据公式算出结合了证据之后的月亮是奶酪的后验概率。 这里比较难计算的是 P（B）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-6-0&quot;&gt;事实上对它的计算你要把所有可以给出这个结果的假设都包含进来， 用概率的marginal law 展开每个假设之下观测到现象的概率， 比如这个问题里， 你就要把月亮是奶酪和石头两个假设都包含进来， 分别计算各自假设下发光现象的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9l6h-0-0&quot;&gt;有一个非常有趣的现象是如果我们的先验概率审定为1或0（即肯定或否定某件事发生）， 那么无论我们如何增加证据你也依然得到同样的条件概率（此时P（A）=0 或 1 ， P（A|B）= 0或1） 这告诉我们的第一个经验就是不要过早的下论断， 下了论断你的预测也就无法进化了， 或者可以称之为信仰。&lt;/span&gt; &lt;span data-offset-key=&quot;a9l6h-0-1&quot;&gt;你如果想让你的认知进步，就要给各种假设留一点空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8vg85-0-0&quot;&gt;贝叶斯分析的思路对于由证据的积累来推测一个事物发生的概率具有重大作用， 它告诉我们当我们要预测一个事物， 我们需要的是首先根据已有的经验和知识推断一个先验概率， 然后在新证据不断积累的情况下调整这个概率。整个通过积累证据来得到一个事件发生概率的过程我们称为贝叶斯分析。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9df1h-0-0&quot;&gt;贝叶斯的数学计算主要考察对条件概率的实用。 但是有时候我们也不理解条件概率， 比如著名的辛普森案， 为了证明辛普森有杀妻之罪，检方说辛普森之前家暴的历史，而辩护律师说，美国有400万女性被丈夫或男友打过，而其中只有1432人被杀，概率是2800分之一。 这其实就是误用了条件概率， 因为辩护律师用的条件是家暴，用来推测的事件是男友杀人， 而事实上这里的条件是被杀而且有家暴，而要推测的事件是凶手是男友（事实上概率高达90%），这才是贝叶斯分析的正当用法， 而辩护律师却把完全在混淆条件与要验证的假设。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7256637168141593&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Mz8k7jElRSnzK5icqYic43ZDLRlJ104tBLOmufISqAIKOn4AcSK26t6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二， 把贝叶斯概率工具用来建模 &lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;4mrm0-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blf3r-0-0&quot;&gt;贝叶斯概率是非常基础的统计知识， 有的人只把它当成统计， 而它在心理学，经济学， 神经科学等领域具有巨大潜力。&lt;/span&gt;为什么？  因为这类问题的研究对象往往具有极高的不确定性， 是由大量较低一级单元组成的复杂系统。 这就造成直接用物理学搞定分子结构解薛定谔方程的思路是不行的， 你不能把人的行为像氢原子光谱一样求解出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;68f6s-0-0&quot;&gt;那么怎么办？ 纯统计？ 你可以做量表， 去统计所有可能的人的属性和和它们的行为之间的联系， 然后求一个皮尔森系数。 但是这样的方法虽然可以用， 但在量化和机器学习发展急速的今天还是naive了一点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;83o4i-0-0&quot;&gt;一个折衷的方法？ 贝叶斯建模。 贝叶斯建模非常善于处理“黑箱” 问题，对付这种不好精确预测但有些用到一点建模的东西很关用。&lt;/span&gt;贝叶斯建模可以很快的把实验数据和理论做一个结合。 而且据说我们的大脑处理信息也确实符合贝叶斯框架。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;34nj4-0-0&quot;&gt;你只要有假设先验， 有观测， 有似然性， 就可以用一个贝叶斯。这里的似然性，经常是由我们的理论模型提供的，而贝叶斯的框架可以把这个模型的参数迅速的推到出来。&lt;/span&gt;这同时也意味着，果你手里有几个不同的模型假设，有一些数据，贝叶斯会迅速告诉你哪个比较合理。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59t31-0-0&quot;&gt;比如你有两个截然不同的假设解释一种心理现象，贝叶斯方法迅速告诉你哪个更合理。&lt;/span&gt;我们通过下面的几个例子说明， 刚刚说了， 我们哟啊建立一个模型， 然后用贝叶斯把它转化为一个预测机器， 模型可以到多简单？ 请看下面的例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1， 多个运动物体例子&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7189655172413794&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6G8nBbLmOjpRibvdBTUicIziaTCyph1aqj5rmRibhOepcyjSMRbqwImFUoQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;580&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.27166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6PIWDfviat9RUnpEKZWGTeyFickUCWI3Y4a87gWWEybbH0iaicDoWOSxqew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6cvit-0-0&quot;&gt;如果看到一组一起移动的物体， 比如上图， 人往往会倾向于认为它们是一个整体。这个现象被格式塔心理学解释为一宗天然的心理倾向。 而解释同样的现象， 你只需要搭建一个简单的贝叶斯概率模型：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7s7nn-0-0&quot;&gt;1， 找到两种可能的假设和现象， A 上面的五个物体是独立的， 刚好一起向上运动  B  上面的5个物体是一个整体  。 现象：  五个物体一起向上运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3frem-0-0&quot;&gt;2， 找到A和B的先验概率：  先验可以。 基于知识或者大量过去的观测， 那么平时生活经验或者书本都会告诉你， 两种情况可能差距不大&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7d852-0-0&quot;&gt;3，A和B得到现象的概率， 事实上它测量假设到现象的关联， 在这个情况下， A几乎一定得到现象， B， 如果每个物体向上或向下的概率是0.5， 那么你应该已经求出来了： 1/32&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e7tph-0-0&quot;&gt;4， 合成后验概率 ：  A压倒B。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4hcon-0-0&quot;&gt;所以， 我么倾向于认为A是对的，即使A和B都有成立的可能。 由此得到的推论是人有把一起移动的物体看成一个整体的趋势， 这符合格式塔原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;49ij9-0-0&quot;&gt;2， 运动眩晕&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;du63j-0-0&quot;&gt;类似的方法可以解释很多我们日常生活中的一些现象，比如我们一个非常常见的现象， 晕船。&lt;/span&gt;关于晕船的一个重要的进化心理学理论说， 这是祖先的一个毒物排出反应， 因为祖先在尝到毒物之后会引起眩晕， 而这个时候呕吐可以排出毒物。 进化心理学用这个例子说明我们事实上生活在祖先的记忆感觉里。&lt;/p&gt;

&lt;p&gt;那么， 这个非常简单的模型假设成立的可能是多大呢？ 如果用贝叶斯方法来分析这个问题会有个很清楚的框架。在此处我们先预设眩晕确实是我们的大脑根据现象对世界做出了预测产生的反应， 我们在船舱里产生了眩晕， 我们有三个可能的模型：&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8v2n-0-0&quot;&gt;A， 我们的大脑检测到我们自己的运动， 是我们自己的运动导致我们的眼睛和前庭（vestibular） 的感觉&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dfcs9-0-0&quot;&gt;B， 我们的大脑检测到了地面的运动， 我们自己的运动（摇摆）导致了我们的视觉感知， 而前庭（vestibular）则感觉到了船舱和地面的相对运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2hlva-0-0&quot;&gt;C   我们的大脑检测到了我们吸入毒物。 毒物的作用导致了你自己的运动， 以及你所感知到的地面的剧烈晃动（幻觉）。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;el956-0-0&quot;&gt;贝叶斯的分析框架告诉我们， A， 似然性为0， 因为因果关系是错的， 我们自己的运动只能解释我们的视觉感知。   B,  先验为0， 除非世界末日，我们的祖先几乎不会在车船这类快速运动的物体上活动  C， 这种情况确实会出现在祖先的生活里，先验不为0， 而一旦吸入毒物， 那么确实有可能产生幻觉， 因此似然性不为0 .  所以相对前两者， C最有可能。  当然细心的你会发现这里还是做了太多的假设，尤其对先验， 但是这无疑是一个相对合理的框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;1g3p9-0-0&quot;&gt;3， 颜色误差&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6B65coY3d19voMDpZe4orr4LVMC10w50ehMFn4T9nvEBCyoyyic0w5eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6487455197132617&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6JYRicIOshSnHSQbdibGYCkYMBZNvyGNAcpLkibDKhBZaiaCCgs13sB9W8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;279&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;88rvr-0-0&quot;&gt;你有没有印象这篇刷爆朋友圈的文章，  这个裙子的颜色是黑色还是金色？ 有的人猜是黑色， 有的人猜是金色，而它到底是什么颜色的？ 没有人知道。 这是不是说明客观世界是不存在的？ 还是说我们发现了一个检测乐观主义和悲观主义者的方法？  如果你在思考前面两个，那么你不懂贝叶斯。  事实上， 这个问题的实质是， 这条裙子的颜色确实是不确定的。 而我们在现实世界中对颜色的判断， 本来就是一个贝叶斯推断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;我们来看为什么，颜色事实上光谱决定的， 也是不同频率光的成分大小。 这里我们做个简化，我们只有黑白灰。 大家知道， 其实真实世界的物体本身谈不上颜色， 它只是在反射， 而入射光乘以反射率决定了我们看到的样子。 黑色的物体代表反射率为0，   白色的物体是1， 而中间就是灰色。 但是， 你记住， 你的研究只能检测反射光强，这个反射光强等于反射率乘以入射光强。 如果你的眼睛检测到一个反射光强， 而我们的物体识别问题实际上正是想找到反射率这个特征（它才与颜色相关）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;也就是说，我们的研究遇到一个两难处境。 它所收集的资料反射光强， 既包含反射率， 又包含入射光的信息。 我们得到的是一组反射率和入射光的组合， 那么我们究竟为什么会看到黑白灰的色彩呢？  原因是， 我们的大脑根据先验和似然性， 做了一个贝叶斯推断。  首先， 这里的先验是什么？ 我们在自然界中， 往往会根据时间现场的光线强度等对于入射光强做一个估计， 这个经验数值就是我们的先验（在这里最好把这个经验数值想成一个以最可能的值为中心的高斯分布）。 因为日常生活吗， 总归是在那几种光线下。然后根据刚刚的乘法法则（这个相当于似然性， 你有了反射度和入射光强， 可以完全确定你眼睛的检测光强，一个狄拉克函数）， 你可以推出反射度的后验分布， 这个分布的峰值， 正是你最可能看到的颜色。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.3416666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6dF1FsW8e2icYA47C4OPNm2uWRYH9XnhVrE14vHuticpl98qE5mBL6hLw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;41g06-0-0&quot;&gt;这个实验解释了那个裙子的颜色问题， 你是看到黑色还是金色， 和你日常经验里对现场光强的先验有关， 看来酒吧里的DG和阳光下的建筑工程师的想法应该不太一样。  而这也在告诉我们， 我们看到的东西永远并非真实，由于我们接受的信息总是有限，我们在不自觉的做大量的脑补， 这些脑补， 组成了我们最终看到的世界。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cor57-0-0&quot;&gt;你估计还记得旋转舞女的实验吧， 如果你理解了刚刚的颜色问题， 那么这个问题很容易解释。  你看到她是向右还是向左旋转？  不是有人引用来解释左脑还是右脑型人？  你还相信吗？ 旋转舞女是一个典型的信息不全， 而可以容纳不同的解释的问题。虽然说一些八卦的说法并不可靠， 如果人和人之间在对这类问题的回答上真有差异， 说不定会告诉我们一定所从未想到的东西（比如是什么导致了我们的先验？）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bntb2-0-0&quot;&gt;你可以举出无数这类脑补的例子，比如为什么一篇英语文章每个单词都只保留首尾字母你还能猜到一些？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3318181818181818&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6NHqeGib34vQPm3ACQCqyCr9YpBwTyg4lr4xd5FnNlQ7KLSZxN2sQXog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;220&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span&gt;4， 和时间有关的因子预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;贝叶斯方法很擅长解决的一个问题就是和时间有关的因子预设。 假设你经常去以加喜爱的餐厅吃饭， 某一天你突然发现这家餐厅的菜突然就好吃了， 这可能是怎么回事呢？  是不是厨师换了呢？ 然而你没法进到餐厅里去看， 这个时候你会开始回想前几天吃的是不是味道也变化了你没有留意。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;其实， 这里你已经开始进行一个贝叶斯推断过程了。 假定你每天来吃一次饭， 你想推测某个点开始厨师坏了的概率， 这就是一个经典的点推测问题。  这个问题之所以有难度， 是因为如果你把每次吃饭看作一次测量， 那么测量本身是有噪声的，这使得你比较难做出决断，到底是那天厨师心情不好做坏了饭，还是换了厨师。  这个问题的实用性不用多说， 无论是在医疗健康诊断问题里， 还是某段人际关系的变化（好好一段感情突然就变了？ No， 所有的突然变化都是潜在的蓄谋已久）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt;那么具体如何做呢?  事实上这已经开始涉及到很复杂的数学，回到贝叶斯本质， 先验在哪里， 似然性在哪里？ 在很多贝叶斯问题， 先验充满主观性， 这里也不例外， 在所有主观里最客观的就是假定它是一个常数， 也就是厨师变化的概率随时间是均匀的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt; 然后， 似然性呢 ?  似然性就是一个生成模型。 也就是给你一个内在的过程，比如厨师的变化， 然后推导出菜的味道的变化。 一个最简单可以放进去的模型， 就是转化概率随时间独立的马尔科夫过程。 有了这个生成模型， 你还会得到系列不同时间厨师变化假定下吃菜味道的分布。 这就完成了生成模型部分， 后面的工作很简单， 只要按照贝叶斯把它反过来，你就得到了给定观测下， 潜在因子（厨师）在不同时间段发生变化的概率分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;&lt;span&gt;三 &lt;span&gt;贝叶斯推理究竟告诉我们什么&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;udk1-0-0&quot;&gt;传统的深度学习的特点是大量标注数据驱动的黑箱， 不太考虑概率分布。 而到了深度生成模型的时代，我们必须考虑概率分布， 因此深度生成模型和贝叶斯有着深刻的内在联系。 同时，贝叶斯框架通过结合有效的先验，可以做到用更少的数据达到更好的泛化效果， 也极为的符合深度学习的需求。 两者在网络训练的结合请参考深度贝叶斯&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;贝叶斯分析里， 你会发现， &lt;strong&gt;你始终要有一个先验， 一个似然性， 而似然性事实上是某种简单的模型&lt;/strong&gt;（也可以很复杂！）。 事实上我们在我们的思维过程， 主动或被动， 正确或不正确的运用着贝叶斯，你所认可的事实里， 很多是你的推断。同样的客观数据面前，先验或似然性不同的人， 可以得出完全相反的结论。  教条的人可能给予了某个假设一个无限强的先验。 而容易被忽悠的大多数可能用到了过于简单的似然性模型， 比如用好人和恶棍解释很复杂的社会现象。 而自做聪明的人呢？ 可能用了一个对自己有利的解释模型， 而忽略了其它可能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383354&amp;amp;idx=1&amp;amp;sn=133519d77356bdae60ad9388c1f53cb5&amp;amp;chksm=84f3c87bb384416d488770175c676b9061c8168f7c84e10822f186fa8934009c67502279ba72&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;贝叶斯大脑&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382202&amp;amp;idx=1&amp;amp;sn=75d481a667221f328ed44ab76f241451&amp;amp;chksm=84f3cffbb38446edadae25d079a91e5f30592cf67a24e6bf60cf7314f3f60a5860e87cb91240&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;趣味贝叶斯推理&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 26 Mar 2019 16:02:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/EY2XU24xDA</dc:identifier>
</item>
</channel>
</rss>