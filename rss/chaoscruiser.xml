<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>随机网络中的智慧</title>
<link>http://www.jintiankansha.me/t/VG3u69CoGB</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/VG3u69CoGB</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;网络是从人类智能到深度学习的基础，可能所有人都认为只有训练好的具有特定结构的网络才能具有功能，如同生物的功能是由结构决定的， 精巧设计的结构可以产生特定的功能， 大概高中生物老师就给我们灌输了这个观念。 而在网络的世界， 这就意味着你要某个功能，就要先产生那样的结构，比如一个具有特定结构的CNN。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而， 可能你不知道的是， 一个随机连接的网络也具有功能。 什么叫随机啊？ 就是任意单元和单元之间连接与否是随机的， 看起来很混乱，它们居然能做事？ 不仅如此， 它能做的事情还很酷炫：比如， 预测火焰的形状演化。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;注：火焰的跳动，是一个我们常说的混沌系统， 所谓难以用常规方法预测， 确可以一定程度被随机网络征服，这是一个以复杂对抗复杂，用无常应对无常的经典例子。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5615384615384615&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCwjxNiat12Ber6FWgHTw70o1ib9sGxPaUWa4OqqzOeiax2iacJ4u5gENmyg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;520&quot; width=&quot;520&quot;/&gt;&lt;p&gt;Machine Learning&amp;amp;amp;amp;#39;s &amp;amp;amp;amp;#39;Amazing&amp;amp;amp;amp;#39; Ability to Predict Chaos&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;任何一个网络的连接都可以由一个矩阵来刻画， 刻画随机网络单元和单元之间的连接就是随机矩阵。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;1）什么是随机矩阵 ？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;可能每个人都很清楚矩阵， 但是提到随机矩阵，就不是每个人都清楚了。 事实上， 随机矩阵是研究所有和网络有关的科学技术， 从机器学习到复杂系统， 极为重要的工具。 那么我们就一层层拨开随机矩阵的神秘面纱。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;首先， 如果矩阵里每行每列的元素都从独立分布的高斯里抽样，那个， 这样的一个方阵称为随机矩阵。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.26666666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxC8McjJVpeuZbYZ99fRzG8iaQ3wPFrKRgYPOtboFZMrO8q8PUUFhN9eKQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;904&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;看起来没什么乱用对吧？ 我们还是直接进入随机矩阵的数学物理本质： 事实上， 随机矩阵用于描述一个动力系统内不同元素间的相互作用， 具体的例子比如：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 描述一个马尔可夫过程的概率迁移矩阵： 矩阵可以用来描述一个马尔可夫过程的迁移矩阵， 那么该矩阵就定义了一个随机连接的图网络， 从i点到j点的迁移概率由对应的矩阵元素表达（因此每一行的和需要为1）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 描述一个动力系统里任意的n个元素和n个元素的相互作用关系， 这n个元素， 既可以是人工或生物神经网络里的神经元， 也可以是生态系统里的各个物种， 或金融市场相互作用的交易者， 我们刚刚说的预测混沌的网络就符合这个类型。 此处随机矩阵就是随机网络的数学表示&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;2）随机矩阵是怎样刻画一个动力系统的&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里， 我们从最简单的系统-二维的线性动力学系统开始， 二维的动力系统定义为：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.17567567567567569&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnF1ia9ib42miafRnAJLIbibIic2m4yt0P3oSjm9DZCB43s85xgsyY8RPsEMtDHM7avvK32/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;148&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1780821917808219&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnowJGOKPTyrK4ic68XPZBGq7z9RBj4YldHQByhk95mZgoFGHzVDRrzffibib3F2d9UHd/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;146&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;可以由一个a，b，c，d组成的二维矩阵（雅可比矩阵）刻画。 这个两两作用的系统在自然界比比皆是， 比如著名的猎手-猎物方程。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;对于任何动力学系统， 我们都要先抓住它的定点， 而整个系统的性质， 由定点向外周扩散迎刃而解。 那么这个简单的线性系统有一个显而易见的定点就是x=0， y=0.&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;定点的作用就像一个巨大的吸引中心， 系统的演化无论多么复杂， 都是以某种形式围绕它展开。 这些展开形式可以被概括到一个叫相图的二维平面里， 这个平面是由二维系统的两个变量为坐标轴， 概括了系统从任何初始状态（x，y）开始演化， 它的未来发展轨迹。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6458036984352774&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCLpIPMCcO48JalHajRLhT6yJs5FFAvKXryicGpqkIlNV8t4TgoHFK9Lw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot; width=&quot;703&quot;/&gt;&lt;p&gt;相图的做法非常简单， 只需要对任意状态求解其变化趋势(dx/dt, dy/dt)并在平面上用箭头表达， 我们就可以看到整个系统从任意点开始的未来走势。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了定点后， 系统具体的演化方法则由它的雅可比矩阵决定。 在这里雅可比矩阵也就是abcd所确定的连接矩阵&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.8115942028985508&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcewMUIibIuV7JdontKCkatxCKAUmF6GAyNmomkLGX8SVq0y4UFoChXoRZDC4hlxAV59copOqn0uvIA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;69&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;/&gt;这个矩阵里的各个元素，它将确定，随着时间，系统将去向何方。 我们可以按照特征矩阵的行列式A = ad -bc 以及迹（trace） a +d 作为坐标轴对系统分类。为什么是这两个东西， 你想一下， 矩阵本身由特征值决定， 在特征分解后，A代表特征值的乘积，trace是特征值的和， 这两个量体现了特征值的性质。 矩阵的特征值是一个复数， 对应复平面上的两个点。 这两个点的几何性质由刚刚说的行列式和迹决定。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;数学的好处在于一次得到所有的可能性。 一切可能皆由定点展开， 这些情况按照定点稳定与否（演化是趋紧还是远离它）， 以及趋于（或远离）定点的方式展开。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们通常用poincare diagram 来表达所有情况。 从左向右， 定点从稳定到不稳定（特征值由负到正），从下到上， 趋于或离开定点的方式由线性变换到旋转（特征值由实入虚， 此处以delta&lt;img class=&quot;&quot; data-ratio=&quot;0.22807017543859648&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPn0lfXOYcMnxjePpHrslD4IBgDFCypck09y8BmxcFPISX9GYICMLdSWQxTrgkgGCHI/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;114&quot;/&gt; 为界）。方程你可以把整个解析解写出来&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.11764705882352941&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnpHeZicycd3gUJqV0lQUgbRFkj75w6wsms7IPciavzOPeTuV5hCGCA7Y9u5Sibx8lJQ3/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;272&quot;/&gt; X=（x，y）&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6486111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxClGz8yibVYIEWOQ8sYH3ibHGO37cI3U3sJvKlMdjl7OxBsMKcnGGYRd8w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;720&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;由如上的两个坐标轴和一个抛物线，我们把平面分割成了6个区域。 你只需要记住临界态的性质， 中间区域及其过渡。 抓住这个平面，就抓住了所有的二维线性动力系统。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;y轴上半 - 此处特征值为纯虚数， 实部为0， 我们既不趋近也不远离， 这也就是周期运动， 或被称为极限环。 系统围绕定点做圆周运动，是稳定和不稳定的过度状态。 典型例子如二维谐振子 - 理解各种复杂的物理系统的毕达哥拉斯之剑。 加入一定非线性形式我们得到猎手-猎物方程， 狼和羊， 资本和劳动力矛盾下的振动平衡。 自然界还是人类现象中振动如此普遍， 背后正是这类动力关系的体现。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7513888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCE1mPUVMIS7t8P4tdgNPiak0C0qkfJRosgxqAuFW7W0cGPMa5uOfPexA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;720&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;X轴下半和det中间区域： 稳定定点， 代表趋势所致， 稳定定点可以预测事物的一般走势（稳定不变的平衡态，任何远离它的阴谋都将破灭），因为在它的管辖区域里， 无论如何折腾， 都回回到它， 因此稳定定点也可以用来存储信息。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.5222222222222223&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCJ44S6z9NENNpS3SzWW0gUjULgTXxDHZ1lxSAwypENapLL0JAO1IW9A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;360&quot; width=&quot;360&quot;/&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Y轴下半及整个下半平面：鞍点（Saddle）与上半的区别在于运动方式， 刚刚说的转动变成线性， 然而依然是从稳定到不稳定的临界，此处的效果是从一个特征向量方向你趋于定点（稳定），而另一个方向则远离（不稳定）。 这样的系统可以表示神经网络的决策或分类： 从一个方向得到的结果是A， 从另一些方向得到的结果是B，这就是天然的分类器。鞍点也用来介导一个动力系统的相变， 从一个方向你达到定点， 再在另外一个方向分离， 例如所有的热恋到失恋的过程。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCol66Ox1rBtdqhxjVbNYpRQnZ9kcm07I49ZvmhWNbsoTxWrgWxrqMpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1200&quot;/&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;X轴： 刚刚那个图一个更加特殊的情况是X轴上（det 为0）的那些解，这条线的数学含义是我们某个特征值为0的情况（此时矩阵的迹为0）， 啥叫特征值为0？ 它意味着只要我们在这个为0的特征值所对应的特征向量，我们就有 &lt;img class=&quot;&quot; data-ratio=&quot;0.16883116883116883&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnS2iaPDxw1nk7d9THKKpjIAry6HnZeicV7bkjqfQCF8Iib56aOjBOWjJdK5HXhmDACzC/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;154&quot;/&gt; ，也就是所有解都是定点！ 而它导致的结果是所谓的线性吸引子， 定点不在是一点而是一条线（line attractor）。 我们会看到这个解在众多的问题里意义重大，比如神经编码 。这是因为线性吸引子是路径依赖的代言人， 你从不同的起点出发， 会停留在不同的位置上， 这就好像把初刻的历史凝固了下来，因为可以比单个稳定定点编码更复杂的信息-甚至是某种抽象关系。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;下图表示一个更一般的非线性系统里的line attractor 。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8024193548387096&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCdiboiaV3ic2pqcPDtHw5KoPsLV9KiaTyt1b9WHpaU8G7m7s6eFeiaerjFIQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;496&quot; width=&quot;496&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;更多信息请见&lt;span class=&quot;visible&quot;&gt;def.fe.up.pt/dynamics/l&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;3）随机矩阵和动力系统的联系&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;理解了二维系统， 你可以抓住它内在本质的东西， 然后一级级向高维延申。 我们回到我们的主题-随机矩阵， 随机矩阵刻画一个高维动力系统， 其不同单元间的连接是随机的。 如果我们假定高维系统依然是线性的， 那么它一般写成：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.15028901734104047&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnanMX88UT29ByXN3lcaycF9B8T9oC6HLC224Uv4hmegjGicLh08fqxfobB3JeEAob9/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;173&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;A是一个nxn的方阵， 由刚刚说的随机数确定。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;首先， 寻找定点， &lt;img class=&quot;&quot; data-ratio=&quot;0.1590909090909091&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnIUvq9flCPzXEXNH9pVebkwZIZ086vQuegLN74k3wPQgEWSkgCibZszD0cBdzB5w0l/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;132&quot;/&gt; 这样的解有一个是肯定的，就是 &lt;img class=&quot;&quot; data-ratio=&quot;0.35714285714285715&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnbiaeJw22g2oWBCovBJryiaYgfnrIo0icqq0uO2hNEW7wIUDX9jFY1Dxg3AtLHGaPsdR/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;56&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;其余性质将由矩阵的特征值和特征向量决定。 我们对连接矩阵A进行特征分解， 得到一系列的特征值和特征向量， 我首先让你猜一下如果你把它的特征值和特征向量在复平面上展开， 它们会长成什么样呢？&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;你依然从二维线性系统， 一个2x2的方阵入手。 这个矩阵的特征值如果你画在复平面上长什么样呢？ 这一类矩阵有两个最特别的情形， 一个对角线为0实对称 &lt;img class=&quot;&quot; data-ratio=&quot;0.3333333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPn5qFcSoict0B4Lgvxdicndsf6mIgozyLtNXsMYHz7qPDHSaucBQHscN4f0zrssdUy9r/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;72&quot;/&gt; ， 一个对角线为0的反对称 &lt;img class=&quot;&quot; data-ratio=&quot;0.29545454545454547&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnqoOjdvicdmlLqWWR0gc3FRZwHh9DGGPUjwhE73XhFJZb9ibBoowWq0BAiafxKiaECblL/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;88&quot;/&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;   , 对于情况一我们得到的两个特征值是-a和a（假定非对角元素为a）， 对于情况2我们有-ai和ai也就是把它们换到虚轴上（这正是刚刚说的谐振子解）。 由此你进行一个类推， 如果我的矩阵的元素不在是这两个特殊情况而是随机的， 我只保证这些矩阵元素每个的期望均是0， 然后你要求出特征值的分布会是什么样的？ 刚刚的解一个是沿着实轴相对原点对称， 一个是沿着虚轴相对原点对称。 如果综合起来呢？ 在实轴和虚轴组成的复平面上， 我们会得到任意方向沿着原点对称的一组点， 从而组成最完美的一个图形- 也就是， 一个圆！ 具体求解请见论文（Introduction to Random Matrices-Theory and Practice）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;好了， 那么维度增加呢？ 当你的矩阵元素越来越多， 这个时候我的高维矩阵的特征值个数将等于我们的矩阵维数， 当这个数字达到一定程度， 我们任意一个矩阵的特征值都将逼近刚刚说的那个所有可能二维矩阵的特征分布， 也就是一个圆，至少非常接近！&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;1.0338345864661653&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxC4j1S0MTDia6yp4ibPqT8UsxOG04bkzt2XTlrtRkoHcwDb8eVKHKWN0Rg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;266&quot; width=&quot;266&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;好了， 从这里我们可以立刻领悟到的是什么？ 特征向量和特征值携带所有矩阵的信息， 那么所有的随机矩阵的性质是类似的。这里只有一个东西是变化的， 就是圆的半径。 这个量有什么意义呢？还回到二维情况进行对比， 在我们刚刚的情况里 ， 矩阵的trace从小于0到大于0引起整个系统从稳定定点过度到一个不稳定定点， 而此处， 这个圆的半径， 正是起到类似的作用。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们把整个动力方程写成：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1477832512315271&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnriapf3Nw8S5f6Dtiat6gejb886Bpno7WU8Hia4gzLgHrEDJrbt8SvibQfvW2QKdpcM8n/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;203&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么矩阵A-I的特征值正负将决定整个系统的稳定性， 这里的情况是如何呢？ 记得刚刚说的我的矩阵A元素都符合是一个平均值为0的高斯分布吗？ A-I这个矩阵就是一个以-1为中心，以A的特征圆为半径的圆形区域， 如果这个圆的半径小于1， 那个特征值整体在负半平面， 系统会趋于稳定的解0。 而一旦半径大于1，这种稳定性就被打破，在高维的系统里， 当你无法回到定点（或闭合轨迹）， 那么登场的正是我们众所周知的混沌， 高维系统的演化进行永不停止的无序运动。 那么1呢？ 我们说， 这就是混沌和稳定的边缘， 记得在二维的系统里， 这个地方会催生非常多的有趣现象， 比如二维谐振子， 比如线性吸引子， 而这些在高维系统里依然正确， 我们会得到各种各样的复杂多解型， 比如-振动解。 而这正是和网络有关的众多有趣现象， 甚至生命本身， 产生的地点。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;注：所谓混沌， 事实上是一大类不同动力学现象的统称， 它们的共同特点是从某个无限接近的初始点出发， 未来的轨迹是发散的。 混沌的最简单形式是三维非线性方程的洛伦兹吸引子， 在此情况下事实上我们的轨迹围绕这两个定点做某种“周期”运动， 只是这个周期无限复杂， 因此混沌并非等于失控， 而可以是非常复杂的信息载体。 而混沌也可以普遍的存在于高维的线性系统里。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.749034749034749&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCJFWtc8XaPkhftyiaq6ES61SvNy9iaHexmaVia6hLxUm79Uib71oGDf6Vpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;259&quot; width=&quot;259&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们说， 这个特征谱一来决定稳定性， 而来决定趋于定点的方式。 实数代表线性的推进， 虚数代表振动，具体是哪种方式推进， 则决定于你是否在某个特征值对应的特征向量方向上。 我们知道， 我们是一个高维的线性系统，在这个高维王国里， 光坐标轴就可以建立维数N个， 那么对应的就是N个特征向量方向。 由此决定了我们以不同的初始状态趋近定点， 可能的结果会非常复杂多变，运动模式趋于无限。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; center=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;4）基于这种理解我们可以做什么呢？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 预测高维网络的一些基本性质： 虽然我们比较难完全预测高维系统的未来， 但是我们预测其稳定性， 我们看到， 当改变一个网络的一个基本属性， 比如连接强度， 就会让网络从稳定到不稳定，从稳定平衡趋于混沌， 那么对于生态系统和社会这意味着什么呢？ 有人说当系统的元素增加连接增强会使得系统更脆弱 ，更容易失衡， 但这仅是理解之一。 一个趋于稳定平衡的系统也通常没有什么功能。 而混沌本身， 确可以是秩序的载体。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 在混沌和稳定边缘的高维随机动力系统具有某种全能可塑性， 如果加上一定的非线性， 则可以包含极为丰富的动力学模式， 稳定定点，周期解， 不稳定定点， 混沌， 各类复杂的吸引子， 都可以在这个区域周围出现。这个区域动力学形式已经开始丰富， 又不像完全混沌那样难以控制， 因此是各类学习的最佳区域。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3， 制作机器学习工具。 我们说大型的随机网络本身就具备一定的学习能力， 而且在很多学习任务里可以匹配其它特定设计的机器学习模型。 这里一个比较著名的例子就是蓄水池网络。刚刚开始说的预测火焰的例子正是来自这里。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;蓄水池网络的根基，正是2提到的混沌和稳定的边缘， 再加上非线性的激活函数, 以及外界环境的输入I。这个微小的非线性将把系统的复杂性再推一个高度， 事实上， 一个非线性的二维系统就可以表达多于一个的定点， 而非线性的三维系统就已经可以产生混沌。 一个非线性的高维混沌系统， 其数学复杂度已经接近解析的极限。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.10358565737051793&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPnN1nUW53HeoYr2Ry5xUYibXEr3RZKSLlkwcVDyZyt7SLxiciaHNdpZapfjLoBT66Zb4T/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;251&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;-这也就是循环神经网络RNN。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;制作一个蓄水池网络最重要的就是控制刚刚说的特征值的谱半径， 我们要让它处于稳定到混沌的临界状态，也就似乎那个谱半径接近1的状态。 在这个时候， 系统的动力学属性最为复杂，最为丰富。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;蓄水池网络具有的一种能力是， 如果你给它一个复杂的时间序列输入（I）， 比如股市的变化， 它可以自动的抽取出这种变化背后的独立性因子，并在一定程度模拟出真实过程的动力关系（因为其自身存在足够丰富的动力关系， 以至于非常容易和真实的系统进行匹配）。 听着有点像PCA，但是PCA是线性的不包含时间， 而这里是一个非线性时间依赖的系统， 复杂性不可同日而语。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4861111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCcGDiclEtwkrQFBuvibmaOkJ3Lad493AOc4iaWYTBMRPycFgeYDFtgpCAQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1507&quot;/&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;比如上面这个图， 我们的输入是真实世界一个很复杂的波动曲线（周期解和混沌间的过度）， 事实上多么复杂的波动背后催生它的因素不一定很复杂， 比如洛伦茨吸引子背后就仅仅是一个三维系统。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;当这个波动输入到蓄水池网络里以后，蓄水池网络可以找寻到这种复杂背后的根基，并对这个信号的发展走势进行预测。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;蓄水池运算的好处是不需要改变内在连接矩阵A， 我们唯一需要求解的是一个读出, 也就是&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.19117647058823528&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78CTicqXAeOgPngcdiav5ZPr116cgYOiaES2vzAAnC4oE31kKF3dWNGp89vO48GengsvicnalpG3cs12ia/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;136&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;就可以对时间序列进行预测, 比如文中开头提到的预测火焰形状的网络， 如下图， 这个过程包含两个阶段，一个是训练，一个是预测， 在训练阶段， RNN的作用事实上相当于一个auto-encoder-自编码器， 它得到一个火焰变化的输入， 通过网络重现这个输入。 而在预测阶段，我们不再有火焰变化的数据，我们直接把RNN输出的结果输入回网络，假设这个网络已经学好了， 那么这个输出就是正确的预测（下图为实际信号（上）于预测信号的比对（下））。 这种预测能力的背后， 正是在训练阶段，RNN高维网络里的某些成分， 抓住了真实系统变化背后的那些核心动因（自编码器的本质即压缩寻找主成分）。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.24861111111111112&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCx2r1rNtGlqOuBVzZjpKgTxzR5pT9hfp3GeF7QTgBzJ5Uk6PbibHY7zg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1146&quot;/&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.3472222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCIjicXAwkE1trAicP38ozM0I53IMCcax5mIfGgMmJUTkWWxhSNDZ3mQIg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1433&quot;/&gt;&lt;p&gt;Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;更深刻的学习（对A进行改变）： 我们还可以做什么呢？ 在刚刚讲到的各类复杂的动力学形式里， 我们看到，无论是稳定定点， 极限环，鞍点，还是线性吸引子事实上都是对世界普遍存在的信息流动形式的通用表达。 我们可以用它表达信息的提取和加工， 甚至某种程度的逻辑推理（决策），那么只要我们能够掌握一种学习形式有效的改变这个随机网络的连接，我们就有可能得到我们所需要的任何一种信息加工过程， 用几何语言说就是，在随机网络的周围， 存在着从毫无意义的运动到通用智能的几乎所有可能性， 打开这些可能的过程如同对随机网络进行一个微扰， 而这个微扰通常代表了某种网络和外在环境的耦合过程（学习）， 当网络的动力学在低维映射里包含了真实世界的动力学本身， 通常学习就成功了。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;无论当下红极一时的鏖战星际争霸的网络，还是从脑电波中解码语言的网络， 无非是一种特殊的RNN（LSTM）加上一定的这种学习的结果。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;真实世界的各种复杂网络， 从生物基因网络到神经网络，到生态网络或经济关系网络， 或许都是如此从随机网络逐步过渡出来的。 无论是通过学习，还是进化。 这或许可以揭示为什么人和猩猩基因差异没有大的情况下智力确是天壤之别， 以及类似人组成的社会受到地理条件影响的微小差异后引起的社会演化巨大差异。 或许， 这个规律可能揭示所有复杂网络到深度学习背后的本质。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;参考文献：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1 Sompolinsky H, Crisanti A, Sommers H J. Chaos in random neural networks[J]. Physical review letters, 1988, 61(3): 259&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2 Pathak J, Hunt B, Gir&lt;span&gt;van M, et al. Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach[J]. Physical review letters, 2018, 120(2): 024102.&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;3 Maass W, Natschläger T, Markram H. Real-time computing without stable states: A new framework for neural computation based on perturbations[J]. Neural computation, 2002, 14(11): 2531-2560.&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;4 Schrauwen B, Verstraeten D, Van Campenhout J. An overview of reservoir computing: theory, applications and implementations[C]//Proceedings of the 15th european symposium on artificial neural networks. p. 471-482 2007. 2007: 471-482.&lt;/span&gt;&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;1.413888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcewMUIibIuV7JdontKCkatxCicN7RgeDric5XARttic2HBGErVTqBVutDiak5jzibNYgrsZRgQPrLwNGGmg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1080&quot;/&gt;
</description>
<pubDate>Wed, 01 May 2019 23:40:00 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/VG3u69CoGB</dc:identifier>
</item>
<item>
<title>AI中的幂率法则-通过Scaling来看AI的未来</title>
<link>http://www.jintiankansha.me/t/9eQ8xd2rMJ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/9eQ8xd2rMJ</guid>
<description>&lt;p&gt;这篇小文是源于《Possible mind》这本书的最后一篇随笔，上面是这个系列的前作，对于这本书感兴趣的小伙伴，这本书在今年7月将会由湛卢出版中文版的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384297&amp;amp;idx=1&amp;amp;sn=27d3e3d3f0bbe0557c784f443014f9dd&amp;amp;chksm=84f3c7a8b3844ebe62cce0a60474c5d6dcb49af2d03598b93fd073a4095de500630aeb847ca6&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;《Possible Mind》读书笔记-强人工智能的公理化思考&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384252&amp;amp;idx=1&amp;amp;sn=bdc733e516f25b44a1b7bdfb6104d2b5&amp;amp;chksm=84f3c7fdb3844eeb9f82b2f7e0590f3514f5b9887c279ccdd5919004b59a8686353a85670742&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;信息的俩种定义&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;读《Possible Mind》，看25位大咖谈AI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;当要解决的问题的规模指数化变化时，总会有哪些特征是线性变化的。幂率法则要告诉你的就是这些特征是存在的，幂率法则的道理由于其简单，因此变得极其具有普适性。（关于幂率法则，参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382599&amp;amp;idx=1&amp;amp;sn=6685868146b23306992836c3abd77ab1&amp;amp;chksm=84f3cd06b384441063d08c6efacbdde5e8c3cd177b37f3a30fc70a037eb1ac31ecb704f21dee&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;“Scale” 读书笔记-天下之大作于细&lt;/a&gt;)  当我们将AI的发展放到幂率法则的视角下，会发现历史上的AI低谷对应问题规模增加后带来的复杂度，而高潮则对应通过找到指数级增长中的线性特征来应对复杂性。&lt;/p&gt;

&lt;p&gt;例数历史中的AI低谷，首先是专家系统的预先设定的知识无法跟上现实问题的复杂性而衰落；之后是单个神经元的感知机无法应对非线性，之后是多层神经网络无法应对现实中非结构化的数据，而当前的深度学习热，也由于缺少解释性，模型的稳定性以及对常识与背景的理解，而有可能跌入低谷。&lt;/p&gt;

&lt;p&gt;说起深度学习的成功，算力的提升和数据量的增加是其俩大成功要素，而这俩点都依赖于香龙在信息论提出的数字化编码的通信系统具有的纠错能力。香龙发现，在数字化的通讯方式（例如用二进制），只要传输的数据中的噪音所占的比例低于一个阀值，那通过线性的增加传输数据的通量，可以指数级的减少整体信息出错的概率，直到信息传递的错误变得在实际中不可能出现。假设你要传递一个信息，你为了避免信息在传递中出错，你可以叫多个人来传递，人越多，总体信息出错的概率是指数级而不是线性下降的。这意味着传输的通道增加，带来的可以传递的信息数量是指数化增长的。如果随着越来越多的微电子元件，计算中传递的中间变量也等比例的出错，那就不可能会有摩尔定律。而随着通信的价格指数化的降低，更多的人将生活中越来越多的部分数字化，从而带来了可供处理的数据的指数化增加。&lt;/p&gt;

&lt;p&gt;除了以上的俩个维度，幂率法则在AI中的应用还体现在模型复杂度的线性增长带来了模型容量的指数级增长。当神经网络变得更深，在应对数据时可以使用的规则是之前规则和新增规则的俩俩组合。深度学习中常见的维度灾难，说的是待处理的数据有太多的维度，导致数据分布的很稀疏。而解决方法是将对问题的最优解的全局搜索变成局部受限下的搜索。剪纸的想法不新鲜，但深度学习通过逐步迭代的方式，将问题用一组可能不是最写实，但却最有利于解决问题的特征表示了出来。当OpenAI战神人类的dota选手时，机器眼中的地图是一个个的矩阵，虽然要解决的问题包含的可能性要多于宇宙中的原子。但将问题映射到便于问题解决的规律却是线性的。&lt;/p&gt;

&lt;p&gt;而当前机器学习缺少解释性的缺陷，我们的大脑中的种种决策，其背后的逻辑也是在外界看来是个不透明的黑箱。但为了让人类相互协作，相互共情，需要解释的是人类做出的行为，而不是行为背后的机理。这指出了应对机器学习模型指数化增加后带来的复杂性的方法，通过训练新的神经网络，来将元模型的行为归类到线性增长的框架中，从而解释模型做出的决策的目的因，而不是模型如何做出决策。&lt;/p&gt;

&lt;p&gt;AI要挑战的终极指数级增长，是如何用一个线性增长的规则合成出能够指数化扩展的自身。这方面自然界给出的例子的生物发育中，人具有的复杂性是指数级增长的，但如何制造人的基因却就是那么长。自然的解法是将发育的过程分成很多步，然后通过HOX基因来调控所有和发育有关的基因。当机器可以直接操纵物质世界中的基本粒子，来重构自身，涌现与进化的闭环就如同咬住自己尾巴的蛇，从原子到分子，从分子到细胞，再到组织，器官，生物体，互联的智能体，最后回到原子。&lt;/p&gt;

&lt;p&gt;通过幂率法则看待AI的历史，是将复杂系统的理论来解释为何复杂的系统是必然会发生的。当我们自身遇到指数化增长的选项的时候，也要看出其中那些因素是线性变化的。所谓的万变不离其宗，说的就是这个道理。&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 21 Apr 2019 17:07:51 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/9eQ8xd2rMJ</dc:identifier>
</item>
<item>
<title>《大脑的故事》-六个关键词串起对大脑的系统性认识</title>
<link>http://www.jintiankansha.me/t/BJpbXITniE</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/BJpbXITniE</guid>
<description>&lt;p&gt;《大脑的故事》是4月份湛庐新出的一本科普书，该书的作者曾写过《创造的故事》（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383522&amp;amp;idx=1&amp;amp;sn=ab9a5e820a8d4566c51912f206363cbc&amp;amp;chksm=84f3c8a3b38441b5a781dff83e24405fcf95f482d46713eabb83bd35c8266345f14ca5ba5e6c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;点击查看读书笔记&lt;/a&gt;），他的一整套“自我进化”四部曲，还包括“自我的故事”，作为“西部世界”的科学顾问，他在这本承前启后的书中对神经科学带给普通人日常生活的启示这个话题，通过诸多大脑异常的患者的例子，导出了具有普遍性的建议。本文通过六个关键词，记录我阅读这本书的收获。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ictmXdPJ7c5m6TnGiaBCqNNlFzBXWsO51eKzlSPuNicqQcZWKaF0utlXBUtB9Lfw5H4ibAtXV9icSE9f2OhbOMp8DgQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;1.3878504672897196&quot; data-w=&quot;428&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一章的关键词是&lt;strong&gt;可塑&lt;/strong&gt;性，孩子出生后，独立的神经员，在受到关爱的坏境下探索世界，神经员的连接突触从童年时的最高点，逐渐去掉多达50%不那么重要的部分，在青春期变得对自我认同格外看重，更爱冒险，对情绪更敏感。到了成年期之后，大脑仍然具有学习改变的能力，只是这时你需要比青春期额外的努力。在生命的每个时刻，记忆都在相互争抢着大脑中的连接，这导致了记忆具有可塑性，如果一段记忆很久都没有被激活，那这段记忆的细节就会淡忘，如果记忆在特定的环境下被唤醒，那问题的问法能部分决定回答。如果能减少当下的认知失调，记忆还可以被当下植入。记忆不止是记录之前发生了什么，失去了与记忆生成有关的海马体，我们不止无法建立新的记忆，也无法想象未来。而当我们老去，可以通过对大脑的训练例如保有责任心，生活有目标，保持忙碌，来减缓大脑的衰老。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9929859719438878&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdDrONuiagA265oQoFTaJNGYqicCFIUqsJqYm149dNylRq8KIhLLm4qmbueM5Fc9MUc4oibW8CKzhpDQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;998&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二章的关键词是&lt;strong&gt;构建&lt;/strong&gt;，这章讲述大脑怎么认识外界。人要看到这个世界，需要的不只是视觉皮质对光子给予阐释，还涉及到对自身的认知。上图中一只小猫自己走，一只小猫小车载着走，俩者的视觉输入完全相同，但只有自己走的那只小猫，学会了将视觉和自己的运动匹配。不止如此，远在大脑接收信息之前，大脑就生成现实的图景，为了节省能量，大脑不提供完整的画面，我们高层的神经连接只汇报收到的视觉信号在那里和内部模型有误差。内部模型如此强大，以至于在某些情况下，我们只能看到我们预期的东西，例如面具的凹面还是在你看来是凸出来的。我们对时间的感觉，也是大脑构造出来的，当生死相关的时候，我们会以为时间变慢了，但实验却会告诉你这是幻觉，不同的感官的时间差，也会在大脑中被自动对齐。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.155925155925156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdDrONuiagA265oQoFTaJNGYiaPvGJk5ttDDHuCVF6tYOUnf90MnswRZ1cL3Sibic5eCoP3EAXrjLY5pg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;962&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三章的关键词是&lt;strong&gt;权衡&lt;/strong&gt;，这章的题目“谁说了算”预告这章的主题是意识。孰能生巧是大脑对重复问题节约能量的一种应对策略，无意识的操作，还能保证稳定性，就像在心流状态下的攀岩者表现的比有意识干预时更糟。无意识下的决策，除了上述优点，也容易受到环境因素的影响，例如外界或饮品的冷热绝对你对一个人的第一印象，这使得我们可以通过助推的方法，来改变人的决策。我们的偏好，在无意识的决策时，我们不知道为何会做出选择，即使我告诉你上图左边的照片瞳孔被放大过，你也让会觉得左边的面孔更有吸引力。但意识如同一家大公司的CEO，当一家公司有成千上万个分支的是否，就需要意识来从上而下的做长远打算。尽管CEO只接触公司运营的极少数细节，但通过CEO，大企业才成为一个整体。复杂系统通过意识反映出自身整体的模样。意识不过是大脑的一种错觉，通过是否让意识照亮，大脑在效率和灵活性，自上而下和自下而上之间随时进行权衡和切换。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;第四章的关键词是&lt;strong&gt;多元&lt;/strong&gt;，人脑的灵活性体现在面对一个决策时，不同功能的脑区在相互竞争，取决于决策呈现的方式，例如在电车困境中是推一个人还是搬动把手。而面对包含了太多细节的选择，大脑根据身体的状态而不是逻辑来进行选择，这解释了我们为何无法抵御垃圾食品的诱惑。为何会为了此刻的享受而不去对未来进行投资，甚至甘愿上当。当我们的自我控制受到损耗之后，我们的决策会倾向于自动化处理，而当我们吃好睡好，就像加好了油的汽车，又能做出理性的判断了。但大脑的可塑性是我们可以预测未来，从而改变对未来奖励的预期，如果期待与现实失调，多巴胺会促使你在无意识的层面重新评估对不同选项的估值。如果你有预期自己未来会犯错，那意识的层面，逻辑推理让你可以提前限制自己的选项。大脑做出的每一项决策，都不是一个系统在起作用，大脑因其多元，而具有智能，也因为其多元，而容易出错，做出让人后悔的决定。&lt;/p&gt;

&lt;p&gt;第五章的关键词是&lt;strong&gt;共情&lt;/strong&gt;，人的大脑的最强大之处不在于能决策，而在于能够在无意识的图景上投射出人的情感，正如能够在动画片中，哪怕是几何图形的抽象动画片中投射出情感，人类，哪怕是婴儿的大脑都会同情弱者，会自然的渴望公正，这使得人与人之间的合作成为可能，使得宗教和道德成为可能，使得人们轻易的将自己与他人区分开，将某些微不足道的特征当成是将其“去人化”的依据，从而掩盖了人的共情能力，让人成为最危险的物种。人的共情能力是神经科学最需要关注的，因为我们不能回避这一刻在大脑回路里的真相，我们彼此需要，尽管你这一概念仅仅限于你皮肤包裹的范围。&lt;/p&gt;

&lt;p&gt;第六章的关键词是&lt;strong&gt;融合&lt;/strong&gt;，这章讨论大脑的未来，例如脑机接口，人工智能模拟大脑。作者举可变超感官传感器的例子，通过将听觉转换穿在衣服上背心的振动，让失聪的人能识别出口语词汇，或者让盲人通过背部的触觉去看到圆形的物体，通过机械手臂，让残疾的患者重新挥动四肢。这些现实中的迫切需求，不同于科幻，是黑科技可以起飞的基点。脑机融合的另一条路是从机器出发，例如通过模拟神经元的连接，在硅基上一比一的重现大脑，通过AI整合出和人脑运作机理不同，但具有意识的智能体。这一章的洞见不在于具体的例子，而在于指出脑科学第一次变成一门科学和应用相互促进的学科，通过对大脑的认知，我们正在改变我们自身，不是像过去那样间接的，无目的性，而是主动的有计划的。正如脑科学可以指导深度学习的发展，上文中我们大脑的特征，如何能在神经网络中重新，正在成为研究的主要目标。&lt;/p&gt;

&lt;p&gt;总结来看，对于普通人，大脑的可塑性让你不放弃对自我的提升，记忆是构建出来的提醒你别把自己的主观印象看的太坚不可摧，意识的协调与心流的动态调整让你意识到有时别想的太多，而多元的决策方式督促你多积累些思维捷径，而共情能力的获得与丧失能帮你解释为何人与人之间会有这么多样的互动，而人脑与机器的融合，则告诉你应当顺应大势所趋，对神经科学的进展保持关注。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384297&amp;amp;idx=2&amp;amp;sn=2e6205eb1776c0eccce5a3e7af082934&amp;amp;chksm=84f3c7a8b3844ebe70217025f68e868eb6566d15fa22ef32678b1ce0d9ce8c539ca79f0ecbac&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑最小自由能法则与我们对不确定性的态度&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383843&amp;amp;idx=1&amp;amp;sn=41e82163f76edfe5ffe31a8518d5bafa&amp;amp;chksm=84f3c662b3844f7430b27f82522dd9414d6c481f6e63822d99deb8baf281be8681ea5c4413a7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑的自由能假说-兼论认知科学与机器学习&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;














</description>
<pubDate>Sat, 20 Apr 2019 15:53:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/BJpbXITniE</dc:identifier>
</item>
</channel>
</rss>