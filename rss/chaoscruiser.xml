<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>《大脑的故事》-六个关键词串起对大脑的系统性认识</title>
<link>http://www.jintiankansha.me/t/BJpbXITniE</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/BJpbXITniE</guid>
<description>&lt;p&gt;《大脑的故事》是4月份湛庐新出的一本科普书，该书的作者曾写过《创造的故事》（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383522&amp;amp;idx=1&amp;amp;sn=ab9a5e820a8d4566c51912f206363cbc&amp;amp;chksm=84f3c8a3b38441b5a781dff83e24405fcf95f482d46713eabb83bd35c8266345f14ca5ba5e6c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;点击查看读书笔记&lt;/a&gt;），他的一整套“自我进化”四部曲，还包括“自我的故事”，作为“西部世界”的科学顾问，他在这本承前启后的书中对神经科学带给普通人日常生活的启示这个话题，通过诸多大脑异常的患者的例子，导出了具有普遍性的建议。本文通过六个关键词，记录我阅读这本书的收获。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ictmXdPJ7c5m6TnGiaBCqNNlFzBXWsO51eKzlSPuNicqQcZWKaF0utlXBUtB9Lfw5H4ibAtXV9icSE9f2OhbOMp8DgQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;1.3878504672897196&quot; data-w=&quot;428&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一章的关键词是&lt;strong&gt;可塑&lt;/strong&gt;性，孩子出生后，独立的神经员，在受到关爱的坏境下探索世界，神经员的连接突触从童年时的最高点，逐渐去掉多达50%不那么重要的部分，在青春期变得对自我认同格外看重，更爱冒险，对情绪更敏感。到了成年期之后，大脑仍然具有学习改变的能力，只是这时你需要比青春期额外的努力。在生命的每个时刻，记忆都在相互争抢着大脑中的连接，这导致了记忆具有可塑性，如果一段记忆很久都没有被激活，那这段记忆的细节就会淡忘，如果记忆在特定的环境下被唤醒，那问题的问法能部分决定回答。如果能减少当下的认知失调，记忆还可以被当下植入。记忆不止是记录之前发生了什么，失去了与记忆生成有关的海马体，我们不止无法建立新的记忆，也无法想象未来。而当我们老去，可以通过对大脑的训练例如保有责任心，生活有目标，保持忙碌，来减缓大脑的衰老。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9929859719438878&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdDrONuiagA265oQoFTaJNGYqicCFIUqsJqYm149dNylRq8KIhLLm4qmbueM5Fc9MUc4oibW8CKzhpDQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;998&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二章的关键词是&lt;strong&gt;构建&lt;/strong&gt;，这章讲述大脑怎么认识外界。人要看到这个世界，需要的不只是视觉皮质对光子给予阐释，还涉及到对自身的认知。上图中一只小猫自己走，一只小猫小车载着走，俩者的视觉输入完全相同，但只有自己走的那只小猫，学会了将视觉和自己的运动匹配。不止如此，远在大脑接收信息之前，大脑就生成现实的图景，为了节省能量，大脑不提供完整的画面，我们高层的神经连接只汇报收到的视觉信号在那里和内部模型有误差。内部模型如此强大，以至于在某些情况下，我们只能看到我们预期的东西，例如面具的凹面还是在你看来是凸出来的。我们对时间的感觉，也是大脑构造出来的，当生死相关的时候，我们会以为时间变慢了，但实验却会告诉你这是幻觉，不同的感官的时间差，也会在大脑中被自动对齐。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.155925155925156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdDrONuiagA265oQoFTaJNGYiaPvGJk5ttDDHuCVF6tYOUnf90MnswRZ1cL3Sibic5eCoP3EAXrjLY5pg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;962&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第三章的关键词是&lt;strong&gt;权衡&lt;/strong&gt;，这章的题目“谁说了算”预告这章的主题是意识。孰能生巧是大脑对重复问题节约能量的一种应对策略，无意识的操作，还能保证稳定性，就像在心流状态下的攀岩者表现的比有意识干预时更糟。无意识下的决策，除了上述优点，也容易受到环境因素的影响，例如外界或饮品的冷热绝对你对一个人的第一印象，这使得我们可以通过助推的方法，来改变人的决策。我们的偏好，在无意识的决策时，我们不知道为何会做出选择，即使我告诉你上图左边的照片瞳孔被放大过，你也让会觉得左边的面孔更有吸引力。但意识如同一家大公司的CEO，当一家公司有成千上万个分支的是否，就需要意识来从上而下的做长远打算。尽管CEO只接触公司运营的极少数细节，但通过CEO，大企业才成为一个整体。复杂系统通过意识反映出自身整体的模样。意识不过是大脑的一种错觉，通过是否让意识照亮，大脑在效率和灵活性，自上而下和自下而上之间随时进行权衡和切换。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;第四章的关键词是&lt;strong&gt;多元&lt;/strong&gt;，人脑的灵活性体现在面对一个决策时，不同功能的脑区在相互竞争，取决于决策呈现的方式，例如在电车困境中是推一个人还是搬动把手。而面对包含了太多细节的选择，大脑根据身体的状态而不是逻辑来进行选择，这解释了我们为何无法抵御垃圾食品的诱惑。为何会为了此刻的享受而不去对未来进行投资，甚至甘愿上当。当我们的自我控制受到损耗之后，我们的决策会倾向于自动化处理，而当我们吃好睡好，就像加好了油的汽车，又能做出理性的判断了。但大脑的可塑性是我们可以预测未来，从而改变对未来奖励的预期，如果期待与现实失调，多巴胺会促使你在无意识的层面重新评估对不同选项的估值。如果你有预期自己未来会犯错，那意识的层面，逻辑推理让你可以提前限制自己的选项。大脑做出的每一项决策，都不是一个系统在起作用，大脑因其多元，而具有智能，也因为其多元，而容易出错，做出让人后悔的决定。&lt;/p&gt;

&lt;p&gt;第五章的关键词是&lt;strong&gt;共情&lt;/strong&gt;，人的大脑的最强大之处不在于能决策，而在于能够在无意识的图景上投射出人的情感，正如能够在动画片中，哪怕是几何图形的抽象动画片中投射出情感，人类，哪怕是婴儿的大脑都会同情弱者，会自然的渴望公正，这使得人与人之间的合作成为可能，使得宗教和道德成为可能，使得人们轻易的将自己与他人区分开，将某些微不足道的特征当成是将其“去人化”的依据，从而掩盖了人的共情能力，让人成为最危险的物种。人的共情能力是神经科学最需要关注的，因为我们不能回避这一刻在大脑回路里的真相，我们彼此需要，尽管你这一概念仅仅限于你皮肤包裹的范围。&lt;/p&gt;

&lt;p&gt;第六章的关键词是&lt;strong&gt;融合&lt;/strong&gt;，这章讨论大脑的未来，例如脑机接口，人工智能模拟大脑。作者举可变超感官传感器的例子，通过将听觉转换穿在衣服上背心的振动，让失聪的人能识别出口语词汇，或者让盲人通过背部的触觉去看到圆形的物体，通过机械手臂，让残疾的患者重新挥动四肢。这些现实中的迫切需求，不同于科幻，是黑科技可以起飞的基点。脑机融合的另一条路是从机器出发，例如通过模拟神经元的连接，在硅基上一比一的重现大脑，通过AI整合出和人脑运作机理不同，但具有意识的智能体。这一章的洞见不在于具体的例子，而在于指出脑科学第一次变成一门科学和应用相互促进的学科，通过对大脑的认知，我们正在改变我们自身，不是像过去那样间接的，无目的性，而是主动的有计划的。正如脑科学可以指导深度学习的发展，上文中我们大脑的特征，如何能在神经网络中重新，正在成为研究的主要目标。&lt;/p&gt;

&lt;p&gt;总结来看，对于普通人，大脑的可塑性让你不放弃对自我的提升，记忆是构建出来的提醒你别把自己的主观印象看的太坚不可摧，意识的协调与心流的动态调整让你意识到有时别想的太多，而多元的决策方式督促你多积累些思维捷径，而共情能力的获得与丧失能帮你解释为何人与人之间会有这么多样的互动，而人脑与机器的融合，则告诉你应当顺应大势所趋，对神经科学的进展保持关注。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384297&amp;amp;idx=2&amp;amp;sn=2e6205eb1776c0eccce5a3e7af082934&amp;amp;chksm=84f3c7a8b3844ebe70217025f68e868eb6566d15fa22ef32678b1ce0d9ce8c539ca79f0ecbac&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑最小自由能法则与我们对不确定性的态度&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383843&amp;amp;idx=1&amp;amp;sn=41e82163f76edfe5ffe31a8518d5bafa&amp;amp;chksm=84f3c662b3844f7430b27f82522dd9414d6c481f6e63822d99deb8baf281be8681ea5c4413a7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;大脑的自由能假说-兼论认知科学与机器学习&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;














</description>
<pubDate>Sat, 20 Apr 2019 15:53:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/BJpbXITniE</dc:identifier>
</item>
<item>
<title>强化学习理解的三重境界</title>
<link>http://www.jintiankansha.me/t/A5jNLvecGM</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/A5jNLvecGM</guid>
<description>&lt;p&gt;&lt;span&gt;想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。&lt;/span&gt;&lt;span&gt;面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定你的行为。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;强化学习有哪些实打实的应用呢？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;只要在问题里包含了动态的决策与控制，&lt;/span&gt; &lt;span&gt;都可以用到强化学习&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1， 制造业&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;强化学习之于制药业有一种天然的契合 ， 把强化学习翻个牌子换个叫法， 也可以叫做控制论， 学习控制机器手的精确动作， 比如让它自动的做比目前所能及的更复杂的事情， 强化学习在制造业的应用潜力是显然的 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6617283950617284&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8YBT4xfFfeIjiatN1CyyZZVqicS0kicw2faTLPZtvkZrkYV4mZ988CiaA1w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;405&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;2， &lt;/span&gt;&lt;strong&gt;&lt;span&gt;无人驾驶&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这就不用多说了， 开车本质是个控制问题, 　自动驾驶不仅需要模拟人类行为，　还需要对前所未遇的情况进行决策，　这需要强化学习。　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5612009237875288&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8BvCqcZ8G0tw81FlSBbLpUvJQkpvZ1WYK40jX7E00y9OknVic4PFNbYg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;433&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;3, &lt;/span&gt;&lt;strong&gt;&lt;span&gt;智能交通&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;智能交通， 显然这里包含了非常多的决策与控制问题， 就拿目前的共享汽车行业 ，滴滴和uber的派单系统时时都是一个动态的决策， 如何把正确的司机和乘客连接在一起， 如何让车辆调动到需求量最大的地方， 这些都要时时的考虑各种因素调整决策。我们说这里面既包含了效率的问题， 也包含了乘客的安全。比如这一次滴滴的事故如果修正强化学习的效用函数， 是有可能避免的。当然除了派单和调动问题， 在每个十字路口交通灯的控制等， 整个城市里的立体交通网络的协调， 本质都是强化学习问题， 所以强化学习在智能交通大有可为 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5841035120147874&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8aEGBiaEgrS7f5yM8cFRoHfwKfEKKmCZRuo2O6r45scgkGPjwbnnajicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;541&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;4，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;金融&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;金融的核心， 交易， 是一个动态控制问题， 即使你不能完全预测明天股市的涨跌， 你依然需要直到我今天要不要下单，下多少单， 这，就是一个强化学习的决策， 它可以影响明天的股市， 也会在非常长远的时间里让我收益或亏损。机器交易，本质是个强化学习问题。当然，金融里能够应用强化学习的绝不仅仅这一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4258720930232558&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8p7o3Dgj17Ygk1o805cWk8rfcYsGpjVTrtThibodXLq2rIrwy2md0gAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;688&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;5, 智能客服&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;智能客服本质是个强化学习问题， 如果你把它处理成监督学习问题， 那个对话机器人只能照猫画虎， 不能够真正从顾客的好恶的角度出发来发言， 而如果用强化学习， 那么机器人学习的就是如何正确的决策， 每句话都是为了最终讨得顾客欢心。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562254259501966&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8TsUlYnticNY3Cd22ziaa2IEQ3vtQQ3eeibia7P9IMc0wsK6SLvhZ84ATuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;763&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;6， 电商&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;电商的本事是如何吸引人买更多的东西， 因此我们买了一个东西后它总会在下面给我们推荐其它的东西。然后我们看到了一个新的东西， 又会点开下一个连接， 这样一步步的就买了一大堆东西， 这样在每一步给你展示不同东西吸引你上钩的过程， 也可以看作是电商系统的动态决策过程， 是一个强化学习问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5674044265593562&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8jydTibwaO24npwTStA4ftpBib5iaAWfnNib78yGGwsZvmpB98WAtHtS2uA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;497&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;7， 艺术创作&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;艺术创作领域看起来与强化学习无关， 事实上它可以很灵活的把人类的好恶加在强化学习的过程里，通过强化学习， 机器作曲可以自发的得到取悦于人的风格，也就是范式。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.47791798107255523&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8ibb1jxhO04FDBz6DCPfIexGdKRsb3vYIIsRJGf4DibR1ZTTxDibxvJLtQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;634&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让机器来决策，首先体现在如何模仿人类的决策。对于决策这个问题，&lt;/span&gt; &lt;span&gt;我们来看人类决策都要解决哪些难题。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;强化学习最初的体现就是试错学习， 因此理解强化学习的第一个层次就是如何通过一个简单的机制在不确定的环境下进行试错， 掌握有用的信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3946830265848671&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8bDDGfnbVQmh1GEHdJu9H2RXBcjibpF1Lc9qDwvd5QzuUdwxtQiaQR61A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1467&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;在这个框架下，&lt;/span&gt; &lt;span&gt;我们需要掌握的只有两个基本要素，&lt;/span&gt; &lt;span&gt;一个是行为，一个是奖励。&lt;/span&gt; &lt;span&gt;在这个级别的强化学习，&lt;/span&gt; &lt;span&gt;就是通过奖励，强化正确的行为&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt; &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;所谓行为，行为的定义是有限的选项里选以恶搞，&lt;/span&gt; &lt;span&gt;所谓智能体的&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;，走哪一个都有正确的可能，但是我们预先不知道这个东西。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所谓奖励， 就是环境在智能体作出一个行为后， 给它的反馈。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;大家看到，如果这个奖励是已知的，那么也就没有了任何的游戏需要进行的可能了。&lt;/span&gt; &lt;span&gt;你为什么要学？&lt;/span&gt; &lt;span&gt;每个行为得到的后果是不知道的啊！&lt;/span&gt;  &lt;span&gt;奖励具有随机性，&lt;/span&gt; &lt;span&gt;同样的条件性，&lt;/span&gt; &lt;span&gt;有的时候我们可以得到奖励，&lt;/span&gt; &lt;span&gt;有时候没有，&lt;/span&gt; &lt;span&gt;因此，&lt;/span&gt; &lt;span&gt;它也是一个随机变量，&lt;/span&gt; &lt;span&gt;理解这一点非常重要，&lt;/span&gt; &lt;span&gt;因此才可以理解很多的后面的算法。&lt;/span&gt; &lt;span&gt;奖励可以是正向的，也可以是负向的（惩罚）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.69875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8AKZgMqIPKFjricunYfGg1bkaiaYA5g9khFDIwVa27VgyOaEka0GsJ5Cw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们迅速的切入一个强化学习的最小实用例子，&lt;/span&gt; &lt;span&gt;又被称为&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;多臂赌博机的例子&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;：&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;你去赌场玩的，&lt;/span&gt; &lt;span&gt;都知道这个机器的存在，&lt;/span&gt; &lt;span&gt;它的构造就是很多个长得一样的摇臂，每个对应不同的中奖概率，&lt;/span&gt; &lt;span&gt;你要玩&lt;/span&gt;N轮， 每次选择摇臂， 使得最终的收益最大。聪明的你一定可以设计一个方案， 让自己的收益最大， 怎么做呢？ &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;首先， 你能不能一下子找到这里的行为和奖励是什么呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你能否设计一个算法解决这个问题呢？首先， 注意， 我此处的题设是N。假定这个N是一次你会怎么做？10次呢？无穷多次呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6067146282973621&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop8ZoLBicVgE11906kib5ODmUn9512MNlBfk4NpnpAGQ2xerLgSzdTF9fLg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1251&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;每次行动后带来不同的奖励&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;很自然的，我们就来到了这么一个问题，就是探索与收益的平衡，&lt;/span&gt; &lt;span&gt;只要&lt;/span&gt;N不是1或无穷， 你都会有如下的窘境。假如你开始就中了 ， 你会一直选择那个中奖的摇臂不放过吗？显然这可能是陷阱，因为还有收益更高的臂，或者只是这次的运气好。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;反过来， 你会不停的随机试下去吗？显然不会， 因为存在收益比较高的臂。所以， 你就需要设计一个策略， 在有效探索的同时加大在那个最有收益可能的臂的概率， 每次玩， 又都增加你的信息。这就是一个极为典型的应对不确定的策略。&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;而且，&lt;/span&gt; &lt;span&gt;可以直接用到产品设计上，&lt;/span&gt; &lt;span&gt;我们也可以把它看成一个特别有方向性的试错。&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这里你要形成的第一个观点就是：&lt;strong&gt;奖励是随机变量 为了量化奖励， 我们需要引入期望。&lt;/strong&gt;这，才是我们要优化的对象。我们所处的环境下，环境给我们的奖励分布是未知的，因此，你必须在一开始边测量， 边引入收益的机制。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;刚刚的游戏，&lt;/span&gt; &lt;span&gt;和真实世界的大多数问题相差甚远，&lt;/span&gt; &lt;span&gt;因为它每轮只有一步就可以看到奖励，而且这一次抽取，&lt;/span&gt; &lt;span&gt;和下一次抽取一点关联都没有。&lt;/span&gt; &lt;span&gt;而事实上世界上的大部分游戏是一个连续多步骤，步步相连的过程。比如各种棋类，扫地机器人（连续离散化）等，&lt;/span&gt; &lt;span&gt;这样的问题，&lt;/span&gt; &lt;span&gt;很快前面的问题就不管用了。我们需要完善我们的框架来改进前面的东西。&lt;/span&gt; &lt;span&gt;好了，假定你是在设计这个东西，&lt;/span&gt; &lt;span&gt;那么你要加入一个什么要素呢？&lt;/span&gt; &lt;span&gt;时间？&lt;/span&gt; &lt;span&gt; &lt;/span&gt;&lt;span&gt;步骤&lt;/span&gt;&lt;span&gt;?&lt;/span&gt;&lt;span&gt;   &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;No,  &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;我们说， 解决这类问题， 首先引入的是状态。在围棋里， 你需要看到的是每一步其实你需要决策的信息都在当时的棋盘布局里，&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;而在扫地的游戏里， 每一步的信息都在当下的位置里， 也就是说， 我们把这些某个时间步骤出现的所有有用信息或特征叫做一个状态。有了这个概念， 多步游戏就可以看成根据状态来决策的游戏。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么， 现在我所有的元素就是行为-状态-奖励，每一步， 我都要根据过去和当时的状态来决策此刻。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.8585365853658536&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce5vF9ib3b3HiaHGBxGjjGop83Tz5VTWEvGO0sabfn3sVNC6JRnl8gPzTUSrrF3YycibRIBtmavic1zYw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;205&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这样我们就有&lt;/span&gt;state（observation）- action - reward 这样的一个组合。或者说环境给你一个state， 然后智能体得到一个action ， 这个action改变环境， 并且环境返回智能体一个reward，如此循环， 当然在真实的游戏下我们并没有这样机械的一步步的过程， 而是一个连续的整体， 这种机械的方法是为了让问题可以轻松的被一个程序解决。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样的思路和图灵最早提出的图灵机智能模型具有异曲同工之妙， 而图灵机被认为是智能产生的基本模型，因此你也可以理解为什么强化学习和强人工智能有关。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;从状态到行为&lt;/span&gt;action的函数，也就是刚刚提到的那个条件概率， 通常称之为&lt;/span&gt;&lt;strong&gt;&lt;span&gt;策略&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;span&gt;，&lt;/span&gt; &lt;span&gt;犹如通常意义上说的战略，&lt;/span&gt; &lt;span&gt;也就是一个行为的指导方案。&lt;/span&gt; &lt;span&gt;当游戏结束的时候，&lt;/span&gt; &lt;span&gt;我们把所有环境给我们的奖励加在一起算分，&lt;/span&gt; &lt;span&gt;越好的策略得到的分数越高，&lt;/span&gt; &lt;span&gt;这就是强化学习的本质。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;TD学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么如何得到一个好的策略呢？这就是强化学习的中心问题， 大家以看就知道这本质上还是一个优化问题。那么整个后面的篇章都围绕这个展开。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如何得到好的策略？在上个游戏里， 游戏没有很多步，而是一步就可以拿到奖励，这个游戏里， 游戏有很多步， 这里必然引入的一个基本问题就是， 如果我还有好多步才得到奖励， 那么根据奖励来强化学习，将是一个极为困难的事， 因为我今天的决策只能影响明天， 但是明天什么结果都看不到， 这将是一个十分令人绝望的事。因为还学习个什么啊， 没有奖励就没有强化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;你能否结合自己的生活设立一个解决方法呢？&lt;/span&gt;  &lt;span&gt;想想我们高中三年，都是为了高考，&lt;/span&gt; &lt;span&gt;但是我们其实中间有无数小目标啊，&lt;/span&gt; &lt;span&gt;比如各种期末考，期中考。再看金融的一个例子，如果你是一个潜力股，&lt;/span&gt; &lt;span&gt;你是可以在你得到最终的结果前贷到款的对吧？&lt;/span&gt; &lt;span&gt;也就是说，&lt;/span&gt; &lt;span&gt;虽然明天并没有任何实际的奖励，&lt;/span&gt; &lt;span&gt;我们可以引入一个虚构的量，它能够把未来的收益给量化，这就是价值函数。它代表的不是当下的收益，而是未来的收益。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;这里就有一个问题了，&lt;/span&gt; &lt;span&gt;如果这个奖励在一个月以后到达，&lt;/span&gt; &lt;span&gt;或者在一年之后到达，&lt;/span&gt; &lt;span&gt;这两种情况是否应该一视同仁呢？你可以以你的直观感受直接告诉我，&lt;/span&gt; &lt;span&gt;不可能的！&lt;/span&gt; &lt;span&gt;我们可以有一个心理学实验，就是马上给你&lt;/span&gt;50元和， 和一年后给你100元， 你愿意选择哪一个（举手）。好了， 这样的机制事实上有着非常强大的现实意义。它的隐含含义就是， 我们需要一个贴现因子， 来惩罚未来的收益。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;在大多数情况下，&lt;/span&gt; &lt;span&gt;我们都生活在这样一种情况里，&lt;/span&gt; &lt;span&gt;我们的游戏里有小的奖励有大的奖励，&lt;/span&gt; &lt;span&gt;有今天的奖励有明天的奖励，&lt;/span&gt; &lt;span&gt;而游戏是连续的。&lt;/span&gt; &lt;span&gt;比如扫地机器人，&lt;/span&gt; &lt;span&gt;你可以想象成每扫一小块，&lt;/span&gt; &lt;span&gt;它就得到一个奖励，&lt;/span&gt; &lt;span&gt;这个时候价值函数将变成一个不同时间点的奖励的求和。&lt;/span&gt; &lt;span&gt;那么，&lt;/span&gt; &lt;span&gt;你有没有发现，&lt;/span&gt; &lt;span&gt;这个贴现因子本身更深刻的含义&lt;/span&gt;? &lt;span&gt;你能用它解释艰苦忍耐的人生观和即使享乐的人生观吗&lt;/span&gt;?&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;平衡当下和未来， 建立当下和未来的桥梁，正是强化学习的第二个基本矛盾。也因为如此，当下和未来收益的统一者，价值函数就成为了强化学习的核心概念。这个函数的定义方法是首先把当下的奖励和未来的奖励加在一起， 由于奖励本身就是随机变量且未来是不确定的， 我们要是把奖励都加在一起， 依然得到的是一个随机变量， 你要衡量一个随机变量的大小， 只能对它取期望。这样， 这个带着未来收益的期望， 就是我们对价值函数的最终定义。强化学习， 就变成了如何找到那么一个策略， 使得我们这个value函数达到最优。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;然后我们来说，&lt;/span&gt; &lt;span&gt;强化学习的核心，&lt;/span&gt; &lt;span&gt;策略。&lt;/span&gt; &lt;span&gt;所谓策略，&lt;/span&gt; &lt;span&gt;无非是把当下的这个对未来的估值，&lt;/span&gt; &lt;span&gt;我们也可以看作趋势，&lt;/span&gt; &lt;span&gt;和我们要的行动联系起来。&lt;/span&gt; &lt;span&gt;我们干脆把可能的行动也放在这个价值的条件里面去，&lt;/span&gt; &lt;span&gt;也就是，&lt;/span&gt; &lt;span&gt;我们定义目前每个行动下的值函数，&lt;/span&gt; &lt;span&gt;给它起个酷炫的名字，叫&lt;/span&gt;action value， 每个行动的价值。我问你， 如果我有了这个函数， 该如何求得策略？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;如果你告诉我这无非是找那个最高估值对应的行动，&lt;/span&gt; &lt;span&gt;恭喜你答对了。&lt;/span&gt; &lt;span&gt;你已经掌握了强化学习的精髓，&lt;/span&gt; &lt;span&gt;我们需要强化（选择）的行动，&lt;/span&gt; &lt;span&gt;就是那个使得整个走势，&lt;/span&gt; &lt;span&gt;也就是值函数最高的行动啊！&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;但是，同时我要告诉你没有完全答对。&lt;/span&gt; &lt;span&gt;为什么？&lt;/span&gt;  &lt;span&gt;有没有同学能发现我刚刚的逻辑漏洞？&lt;/span&gt;&lt;/span&gt;&lt;span&gt;如果你告诉我老师，&lt;/span&gt; &lt;span&gt;哪里来的值函数，&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;你怎么算出来的，&lt;/span&gt; &lt;span&gt;就是非常聪明。&lt;/span&gt; &lt;span&gt;刚刚讲了这么多，&lt;/span&gt; &lt;span&gt;我实际上是偷懒了啊，&lt;/span&gt; &lt;span&gt;我讲的都是一堆定义，&lt;/span&gt; &lt;span&gt;实际怎么操作却一点没讲。&lt;/span&gt; &lt;span&gt;你记得我说的，&lt;/span&gt; &lt;span&gt;我们求的是期望，就需要概率。&lt;/span&gt; &lt;span&gt;这个概率，&lt;/span&gt; &lt;span&gt;就是我的行为，&lt;/span&gt; &lt;span&gt;引起环境如何的改变的概率，&lt;/span&gt; &lt;span&gt;但是，&lt;/span&gt; &lt;span&gt;正如我们在赌博机里不知道每个臂的筹码，&lt;/span&gt; &lt;span&gt;这里我们也不会知道环境给我们反馈的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;对于这个问题的解决，&lt;/span&gt; &lt;span&gt;一个是笨办法，&lt;/span&gt; &lt;span&gt;一个是聪明办法。&lt;/span&gt;  &lt;span&gt;所谓笨办法，&lt;/span&gt; &lt;span&gt;就是不停的去实验，&lt;/span&gt; &lt;span&gt;从当下的状态和行为出发，&lt;/span&gt; &lt;span&gt;试它一万次测个平均，&lt;/span&gt; &lt;span&gt;所谓蒙特卡洛。&lt;/span&gt; &lt;span&gt;这个方法有着一个致命的弊病&lt;/span&gt; &lt;span&gt;，那就是你得等到游戏的结束才能更新一次&lt;/span&gt;v值，速度太慢了，而且想想有些时候你只能进行一次或几次游戏， 比如人生的游戏你只有一次， 你不能死了再回来， 所以这个方法就不那么给力了。怎么办呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然后我们看聪明方法，这个办法就是不等到游戏结束就更新。这个思维有点逆向。也就是你假定游戏已经结束了， 你从终局往回看整个游戏， 假定游戏结束的时候有一个终极的奖励刺激。这个时候， 你能马上更新的v函数一定是那个你离终局最近的状态。而如果你已经更新了这个状态， 那么它之前的那个状态呢？请你想一下， 我可不可以接着更新这个状态？当然， 我离终点奖励差两步， 所以我无非是把终点奖励乘以两次贴现。哦， 这样看可以， 同样的方法， 你可以不可以更新前三步， 前四步， 前5部， 前n步的状态？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;那么下一次游戏呢？&lt;/span&gt; &lt;span&gt;我是否还需要等到游戏的终局再更新呢？&lt;/span&gt;  No！在新的游戏里， 如果你走走走， 恰好达到了上次游戏里某个经过的状态， 你立刻会看到， 这里已经标记过了一个估值， &lt;span&gt;你可以怎么做？&lt;/span&gt; Ok， 你可以利用这个估值， 并用它来更新你之前的一步！ &lt;span&gt;因为你可以假定当你到大了这里，&lt;/span&gt; &lt;span&gt;一切的情景都是有过往经验支撑了，&lt;/span&gt; &lt;span&gt;如同你在一个城市里走走走，&lt;/span&gt; &lt;span&gt;误打误撞到了昨天走过的一条小路，&lt;/span&gt; &lt;span&gt;那么从这里，&lt;/span&gt; &lt;span&gt;一切都变成已知。&lt;/span&gt; &lt;span&gt;这个方法翻译成数学语言就是&lt;/span&gt;TD时间差分学习。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;数学上的表现就是估值函数v的迭代表达式， 你每次格按照v的迭代式定义来更新v函数，这样多步之后v也会趋于正确的值。好比当你在开车的时候， 你险些撞车， 游戏没有终止， 但是足以让你使用这个惊险来更新值函数。另外一个例子是你打公交车去上班， 每过一个站你看一个时间， 看和你的预计是否有差别， 这每一站的时间， 足以让你不停的调整对最终实现时间的预期。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看看这个故事背后的生物学故事。&lt;/span&gt;  &lt;span&gt;所谓的时间差分，&lt;/span&gt; &lt;span&gt;正是条件反射的基础。&lt;/span&gt; &lt;span&gt;你记得巴甫洛夫的狗吗？&lt;/span&gt; &lt;span&gt;摇铃和食物有什么关系？&lt;/span&gt; &lt;span&gt;食物就是我们说的终极的奖赏，&lt;/span&gt; &lt;span&gt;而终极奖赏之前，&lt;/span&gt; &lt;span&gt;摇铃是一个中间步骤，&lt;/span&gt; &lt;span&gt;由于大量的经历里，&lt;/span&gt; &lt;span&gt;摇铃都导致了食物，&lt;/span&gt; &lt;span&gt;所以，&lt;/span&gt; &lt;span&gt;狗就会学会，&lt;/span&gt; &lt;span&gt;摇铃就是那个终极奖赏前的状态，因此，&lt;/span&gt; &lt;span&gt;摇铃也应该得到一个更高的值函数。&lt;/span&gt; &lt;span&gt;这时候，&lt;/span&gt; &lt;span&gt;摇铃也就具有了某种奖励的性质，可以引起狗的流口水了。&lt;/span&gt; &lt;span&gt;因此你可以继续往前推，&lt;/span&gt; &lt;span&gt;你是否看到，&lt;/span&gt; &lt;span&gt;摇铃前还可以放置一个灯光，灯光之前还可以放一个声音呢？&lt;/span&gt;  &lt;span&gt;你是否看到了，&lt;/span&gt; &lt;span&gt;如果人生赢家是抱得美人归，&lt;/span&gt; &lt;span&gt;那之前的高考长夜，&lt;/span&gt; &lt;span&gt;之前的寒窗苦读，&lt;/span&gt; &lt;span&gt;之前的所有一切，&lt;/span&gt; &lt;span&gt;是不是都成为了我们趋之若鹜的奖励。&lt;/span&gt; TD学习， 是所有生物行为学习的基础。  &lt;span&gt;你可不可以用它的原理设计一个控制拖延症的方法呢？&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;有了&lt;/span&gt;TD学习， 我们可以随时更新每个状态下所有可能行为对应的值函数， 这样， 我们就可以选择那个当下估值最高的选项来行动， 所谓策略的更新（比起之前的估值下的行动）。  &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;由于我们每次更新的时候，&lt;/span&gt; &lt;span&gt;我们仅仅是比之前多了一点信息，我们的这个值函数依然是不完美的，&lt;/span&gt; &lt;span&gt;幸运的是，&lt;/span&gt; &lt;span&gt;每个行为最终都导致我们进入一个新的状态，&lt;/span&gt; &lt;span&gt;使得我们进一步的获得了上一个状态的信息，从而进一步的优化我们的值函数，&lt;/span&gt; &lt;span&gt;若干论之后，&lt;/span&gt; &lt;span&gt;我们将会得到一个最优的值函数和策略。这样的迭代过程，&lt;/span&gt; &lt;span&gt;值函数的更新紧跟着一个行为，&lt;/span&gt; &lt;span&gt;如同一组拉丁舞曲。这个算法就是大名鼎鼎的&lt;/span&gt;salsa和Q学习（此处不做区分&lt;span&gt;）的简化版。&lt;/span&gt;  &lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;这就是强化学习的第二个阶段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然后我们来看强化学习的第三个阶段， 所谓用想象和世界模型来填补的阶段。&lt;/span&gt;&lt;span&gt;首先，&lt;/span&gt; &lt;span&gt;我们刚刚遗漏了一个重要的&lt;/span&gt;&lt;span&gt;point， 这个世界难道真的是如刚描述的只有几个有限的状态吗？肯定不是。我们真实生活的状态是无限的。那么，人脑是如何由有限战胜无限的？我们有联想，有想象。&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt; &lt;/span&gt;&lt;span&gt;这，也是强化学习的第三个阶段。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，我们引入估值函数的近似解法。&lt;/span&gt; &lt;span&gt;刚刚说的值函数，&lt;/span&gt; &lt;span&gt;事实上如果永远都走不回之前的小路的时候，&lt;/span&gt; &lt;span&gt;你就不能用了。&lt;/span&gt;  &lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;但是，&lt;/span&gt; &lt;span&gt;如果你看到一个地方类似之前的走过，&lt;/span&gt; &lt;span&gt;你可不可以根据之前的经验判断呢？&lt;/span&gt; &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;所谓一朝被蛇咬，十年怕井绳。&lt;/span&gt; &lt;span&gt;你要引入一个东西，&lt;/span&gt; &lt;span&gt;具有从以往经验推测的能力&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt; &lt;span&gt;我们有没有学习过类似的东西？&lt;/span&gt; &lt;span&gt;所有的监督学习方法，&lt;/span&gt; &lt;span&gt;都是说这件事啊！&lt;/span&gt; &lt;span&gt;我们根据经验特征，从已知数据推知未知数据。&lt;/span&gt;  &lt;span&gt;由此我们得到值函数的近似算法。&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;有了值函数的近似算法， 虽然看起来我们的体系已经相当完整，但是依然有一个比较严重的弊端， 那就是最终学到的策略只能确定性的， 根据定义 ，这个最优化的策略是每一次选取Q（s,a)里最大的那一个。可是在这个不确定的世界， 我们的最优化策略本身就需要包含随机性，我们真正的策略， 恰好是如何设定这个随机性。我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果此时引入一个随机性的策略， 这个问题影刃而解，我无非子啊左右两个灰色的格子里制定左右各50%的策略， 这时候总是最终客户以达到宝藏，就是时间可能稍微长一点。这样的随机性的策略， 引入策略函数就可以可以很容易的学出来。这时候，我们的你策略及从一个状态下确定的行为函数， 变成了了状态的随机变量， 每个行为以一定的概率来取得。如果最初我们的Q学习里， 我们每一步选择那个动作状态的值函数最高的行为， 那么这里，我们就把这种选最大的行为变成一个称为softmax的函数， 它说的就是，我们依然倾向于选择那个值函数较高的行为， 但是， 其它的选项也是可以选择的， 选择的方法根据这个softmax函数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;有模型学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;刚刚讲的方法，通常称为无模型学习，所谓无模型，　就是当环境的动力学（也就是那两个条件概率，想象下棋的例子）不知道的时候，　我们通过直接抽样的方法来更新Ｑ（ａ，ｓ）和ｐｉ来进行控制住．这样做的一切基础在于环境是未知的，&lt;/span&gt; &lt;span&gt;我不知道环境是如何给我反馈，&lt;/span&gt; &lt;span&gt;假定你知道了环境反馈的方法，你会怎么做呢？&lt;/span&gt;  &lt;span&gt;显然，&lt;/span&gt; &lt;span&gt;你要做的是直接展开计算这个值函数而不是通过抽样更新！&lt;/span&gt; &lt;span&gt;这样，我们几乎每一步都会得到精确的值函数，&lt;/span&gt; &lt;span&gt;而游戏几乎会在瞬间得到最优策略！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;可是我们不知道这个世界给我们反馈的方法啊！&lt;/span&gt;  &lt;span&gt;这里，&lt;/span&gt; &lt;span&gt;我们可不可以在这个地方也引入一个机器学习的近似思想，&lt;/span&gt; &lt;span&gt;让我们通过在环境里取得的数据去近似这个世界模型呢？&lt;/span&gt; &lt;span&gt;当然可以的，&lt;/span&gt; &lt;span&gt;我们可以这么做！&lt;/span&gt; &lt;span&gt;所谓的环境反馈，&lt;/span&gt; &lt;span&gt;无非是下一步环境给我的状态，和给我的奖励，&lt;/span&gt; &lt;span&gt;我可以用监督学习的思想，&lt;/span&gt; &lt;span&gt;每走一步，&lt;/span&gt; &lt;span&gt;都收集环境给我的这两个数据，&lt;/span&gt; &lt;span&gt;然后，&lt;/span&gt; &lt;span&gt;我们就可以非常用我们的那些机器学习工具，&lt;/span&gt; &lt;span&gt;比如神经网络，&lt;/span&gt; &lt;span&gt;来计算它们和上一步环境状态，与我所做行为选择的关系。这样，&lt;/span&gt; &lt;span&gt;我就会得到一个可以学习的世界模型。&lt;/span&gt; &lt;span&gt;虽然它依然不是准确的，&lt;/span&gt; &lt;span&gt;却可以大大加速我得到准确的行动的性质。&lt;/span&gt;  &lt;span&gt;在此处，我们可可不可以学习创造一个世界模型，　来提高我们抽样学习的效率呢？　当然可以，　你不是由监督学习吗？&lt;/span&gt; &lt;span&gt;我们可以用类似监督学习的思路来把这个环境的动力学学出来啊！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;我们在这里就进入了有模型学习的范畴．　这个方法，又称为&lt;/span&gt;planning， 犹如一种做白日梦的能力，  &lt;span&gt;它可以通过自己构造的世界模型，&lt;/span&gt; &lt;span&gt;不停的想象某个步骤之上，&lt;/span&gt; &lt;span&gt;如果我采取某个行为后环境如何给我反馈，&lt;/span&gt; &lt;span&gt;得到如何奖赏，&lt;/span&gt; &lt;span&gt;结果，&lt;/span&gt; &lt;span&gt;我不需要不停试错，通过我脑子里的这个p&lt;/span&gt;lanning &lt;span&gt;，&lt;/span&gt; simulation &lt;span&gt;就可以获取关于某个行为该不该做的信息！&lt;/span&gt;  &lt;span&gt;虽然&lt;/span&gt;agent 并没有真正经历那些行为，　就好像经历过了一样，　这样agent 就如同获得了非常多的虚拟数据，　可以更准确的对未知的状态进行估值，　在数据极为稀缺高维诅咒极为明显的强化学习问题里，　这个效果是巨大的．好比正因为你瞻前顾后， 你才减少了很多错误！ &lt;span&gt;当然，&lt;/span&gt; &lt;span&gt;能够实用这个方法也是有条件的，&lt;/span&gt; &lt;span&gt;你觉得条件是什么呢？&lt;/span&gt; &lt;span&gt;如果你告诉我你必须确实能够掌握环境的套路，&lt;/span&gt; &lt;span&gt;给环境建立模型，&lt;/span&gt; &lt;span&gt;恭喜你答对了！&lt;/span&gt; &lt;span&gt;如果环境完全是不可琢磨的，&lt;/span&gt; &lt;span&gt;那么你最好的方法依然是直接试错的方法而非建模。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;有模型学习的最好应用例子就是阿法狗。&lt;/span&gt;  &lt;span&gt;所有棋类游戏，&lt;/span&gt; &lt;span&gt;都实用一个叫马尔可夫决策的框架，&lt;/span&gt; &lt;span&gt;这个框架里，&lt;/span&gt; &lt;span&gt;下一步都只和这一步的状态相关。&lt;/span&gt; &lt;span&gt;然后，&lt;/span&gt; &lt;span&gt;我们的环境是什么呢？&lt;/span&gt; &lt;span&gt;假定你手里是黑子，那么你的&lt;/span&gt; &lt;span&gt;对手白字就是下一步的状态。&lt;/span&gt; &lt;span&gt;显然，如果你能够知道在你下某个棋的时候白字如何落子，&lt;/span&gt; &lt;span&gt;你就可以建立所谓的世界模型&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;阿法狗所用的方法是，&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  先备棋谱在推理。&lt;/span&gt; &lt;span&gt;所谓的背诵棋谱，就是说直接模拟大师下棋的直觉，&lt;/span&gt; &lt;span&gt;我可以通过看很多大师的下法，&lt;/span&gt; &lt;span&gt;了解无论是黑还是白的套路，&lt;/span&gt; &lt;span&gt;这个东西有一个&lt;/span&gt;CNN来完成。之后， 我来看如何能够加入推理， 这个推理的成分， 有一个叫做蒙特卡洛树搜索的过程完成， 怎么个玩法？ &lt;span&gt;就是在&lt;/span&gt;CNN的直接基础上， 我在大脑里多下几步，形成一个模拟过的走子过程，如此，我们就可以知道真实下去这一步， 白字最有可能如何反应，我会如何应对， 白字又如何反应， 这种直觉加推理的方法， 就是阿尔法狗建立世界模型的方法。有了这个方法， 阿尔法狗就可以战胜围棋最大的挑战， 宇宙星辰般的搜索空间，战胜人类。这样的思维方法， 如果你学会， 也是不可小觑的啊！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqCgYN1dD3XAMuRjXZuW8LY5OKAgKPmsVSQzPpDldvFT9aVgpMWrl06g/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-w=&quot;800&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384317&amp;amp;idx=1&amp;amp;sn=329aeb82919ab6237bdb8f355e202311&amp;amp;chksm=84f3c7bcb3844eaab5197fecaba741c6cc97606ba41309729b18a2465e4357c53e81b523d2b2&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;5分钟读懂强化学习之Q-learning&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;铁哥的强化学习特训课&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 19 Apr 2019 15:02:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/A5jNLvecGM</dc:identifier>
</item>
<item>
<title>GAN的五个神奇应用场景</title>
<link>http://www.jintiankansha.me/t/ut0yzl0oXL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ut0yzl0oXL</guid>
<description>&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5039745627980922&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImptoTntHRWibc2DKXtPvcUrGT46E9nSnTgHE3SE2gotHYibvcJMe3EH97Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;629&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中的图片有什么共同点？回答是这些脸都是虚构的，是由GAN（对抗神经网络）生成的。自从2014年第一次提出之后，做为一种生成式的模型，不到五年的时间，GAN已经衍伸出很多意料之外的应用场景，本文简述其中五个，未来预期GAN会在更多的领域大放异彩。关于GAN的基础知识，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383280&amp;amp;idx=1&amp;amp;sn=a6fd903f2c47339c52dcea9eedf65851&amp;amp;chksm=84f3cbb1b38442a7f4aac491852e06c34794154946a3656bc4ac4805b1ef1b41cb4469ae8419&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;对抗神经网络初探&lt;/a&gt;，GAN用在艺术品生成与风格迁移的具体案例分析，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383188&amp;amp;idx=1&amp;amp;sn=ec8d1090fe46741c14ccf7cc02b57c2c&amp;amp;chksm=84f3cbd5b38442c33ef70b698dd07f5b7aa954623fe3724e9f83b8e10a44b86a3777054395a4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;怎么样用深度学习取悦你的女朋友（有代码）&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1）图像编辑&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3046875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpibxozsleicRf8xpibetUEqojxAlsKLpNvKSD8UHJUUA4UMx2UVtE6c8Hg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;768&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这简直是美图秀秀中的各种滤镜的升级版，给出一张原始的妹子图片，可以生成出金发版，卷发版，微笑版，还能ps出你的双胞胎兄弟长什么样。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2776470588235294&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpicms4hLEk6kaQpg3NhGPicW7Ebd3ObPQZEIL6qiaibibhYoymFFgkEebbFQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;850&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不止是对人物进行定向的修改，还能修改图片中的坏境因素，例如上图中左图中是下雨的场景，右图是GAN生成的同样坏境但是没有下雨的照片，俩者人眼看来，就只能看出是否下雨这一个差异来。实现这样黑科技的，是GAN的一个变种，称为conditional GAN， 有了这项技术，就可以根据一个人小时候的照片，生成其长大后，年老后的样子，或者你上传你家狗狗的照片，然后再虚拟的坏境下给狗狗换衣服，然后你再购买你喜欢的狗狗衣服。这项技术也存在着被滥用的风险，比如生成虚拟的果照。关于技术细节，请阅读参考资料【1】与【2】。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;2）恶意攻击检测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过给深度神经网络一些特异生产的训练数据，深度学习生成的模型是可以被黑客攻击，利用甚至控制的。为了对抗这样的逆向攻击（adversarial attacks），可以训练对抗神经网络去生成更多的虚假训练数据作为假想敌，让模型在演习中去识别出这些虚假数据，就如同人类打疫苗，GAN生成的虚假数据让正在做分类的模型更加稳健。&lt;/p&gt;

&lt;p&gt;另一个在安全领域的应用的加密传输，在图片中，可以使用额外的像素来加密一段文字，从而你以为你接收的是一副普通的图片，但实际上通过解密，却可以发现其中包含了加密的信息。GAN的变种SSGAN，通过在传统的GAN模型中增加一个判别器，可以识别出这样加密后的图像，也可以用于更高效生成上述的加密信息。参考【3】&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.48792270531400966&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpgMvx77uaDrbhdsJxMs7MXoKJibbzIkl9wTZc9GGXgG7kNzU33kD1v7Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;621&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;3）数据生成&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在特定的应用场景下，例如医疗领域，缺少训练数据是应用深度学习的最大障碍。数据增强的传统做法是将原图像拉伸旋转剪切，但这毕竟还是原来的图像，通过使用GAN，能够生成更多类似的数据，如下图所示，右边是真实的图片，而左边是合成的训练数据，中间的是GAN生成后原始图片，之后经过了修饰，使得面部的肌肉和纹理更加真实。这个GAN生成的图片，通过了图灵测试，让眼科医生都无法分辨出。而随着生成出的数据被加入训练集，相同的模型在分类任务上的表现也有所提升，具体参考【4】。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.3567251461988304&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpC1hzfPgQk6DEZ1Hb7c2cCxugLBRAMBoricpr6sAvrES9qvnM3eKtNow/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;342&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;4）注意力预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;人类在看一张图片时，往往只关注特定的部分，而通过GAN模型，可以预测出人类关心的区域在哪里，下图展示的SalGAN预测出的图片中人类观测的热点区域，和真实的区域，以及之前模型的对比。SalGAN的预测不止更准确，而且能够包含很多之前模型没有预测到的区域。对注意力的预测，可以指导广告的投送，例如在电影中做植入广告前，可以先预测一下植入的区域是不是在注意力的热点区。类似的方法还可以反过来用，假设不同类型的人有不同的注意力热点模式，根据一个人关注图片中的那个部分，来对人进行归类。参考【5】&lt;/span&gt;&lt;strong&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0885416666666667&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpWf4BsxOsENqRTTONtWNnEbsWhRk09BrfVJwqDcricksmLhjYH8Oib01w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;768&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;5）三维结构生成&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImpkbEaNCb1SkKe0kQ7FbvVjicovlwpK8NSwjOvWpcOdfur7gvLcFeudOA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;673&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;1231.0089928057555&quot; data-ratio=&quot;1.8276374442793462&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccpZBbGf9HicIlJ9xNX1wImphoLlQnm3Jgwzib5XicY71ZBFibtx88ld99P8WdxwPq5d9ib8NCGIsRYCHw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;673&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;总结&lt;/p&gt;
&lt;p&gt;标注数据总是少数的，未标注的数据才是待开掘的金矿。GAN可以通过标注数据，生成模拟数据，也可以用在自监督学习中，让模型具备从部分标记的数据中学习的能力。本文只是列出了在图片上使用GAN的五种有趣的场景，在视频，有向图及自然语言处理上，GAN也有诸多应用，结合贝叶斯网络，GAN还可以应用在因果推理，参考文献【7】。&lt;/p&gt;


&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【1】https:&lt;/span&gt;&lt;span&gt;//github.com/hezhangsprinter/ID-CGAN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【2】 https:&lt;/span&gt;&lt;span&gt;//arxiv.org/pdf/1611.06355.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【3】 https:&lt;/span&gt;&lt;span&gt;//arxiv.org/ftp/arxiv/papers/1707/1707.01613.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【4】 https:&lt;/span&gt;&lt;span&gt;//arxiv.org/pdf/1612.07828.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【5】 https:&lt;/span&gt;&lt;span&gt;//arxiv.org/pdf/1701.01081.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【6】 https:&lt;/span&gt;&lt;span&gt;//github.com/maxorange/pix2vox&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;【7】 https://arxiv.org/pdf/1810.07406.pdf&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 13 Apr 2019 10:53:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ut0yzl0oXL</dc:identifier>
</item>
<item>
<title>当机器学习遇上进化算法</title>
<link>http://www.jintiankansha.me/t/ctzlXu0ZQQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ctzlXu0ZQQ</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;d5ucb-0-0&quot;&gt;进化，是生物智能演化的原动力。 学习， 是人类文明产生的原动力，也是当下红红火火的AI进步的原动力。 如果这两种神秘的力量结合， 我们会得到一个怎样的物种呢？虽然说这方面的尝试还不多， 不过我们已经可以在一些过去人的研究中见出端倪。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3l2r6-0-0&quot;&gt;首先，我们说两种算法的本质都是在做优化。 在充满随机性的世界里， 大部分的自然过程趋势是熵增，耗散，或者说随机性的增加。而唯有生物的进化和学习，却可以抵抗这种趋势，在随机中产生有序，产生结构。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;mot5-0-0&quot;&gt;虽然都在做优化， 它们的优势和缺点也非常明显。  &lt;/span&gt;让我们来概述一下两种方法的核心。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1nhp4-0-0&quot;&gt;&lt;strong&gt;&lt;span&gt;进化算法vs机器学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6sib-0-0&quot;&gt;进化算法&lt;/span&gt;&lt;span data-offset-key=&quot;d6sib-0-1&quot;&gt;：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ea40g-0-0&quot;&gt;进化算法建立在基因之上，基因 - 可以理解为生命在各种条件下的一组行为策略。比如吃什么， 向什么方向移动，肤色的选择等。 这组策略被一套叫DNA的大分子固定， 也就是我们常常说的遗传编码 ， 它通过一个复杂的化学反应， 制造RNA和特定的蛋白质，而一切生命现象都是由特定蛋白质实现的， 我们简单的说就是生命策略， 比如在外界环境出现如何变化时候如何反应。 你可以把DNA的编码看成一系列的if else语句， 就是在某种条件下， 触发某个蛋白质， 实现某一个功能。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuUm28E0usZSQZpFdSE3NmGF9DUZ3KvPmSVU42OT6ruCf2icCJUaLLQdcQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;4o7te-0-0&quot;&gt;那么进化算法包含以下要素：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d1jn4-0-0&quot;&gt;1，生物通过基因编码生存策略。 基因即一组可以编码蛋白质的生物大分子。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;f6mpe-0-0&quot;&gt;2， 单组策略的存在时间有限， 它会以繁殖的形式得到一个和自己一样的策略， 但是这个过程不是完全精确的， 它会以一定的方式出错或者说变化， 这恰恰使得下一代的策略可以轻微的偏离上一代。 从而在一段时间里， 形成越来越多的策略，我们叫做基因池。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;514bc-0-0&quot;&gt;3， 有的时候不同的基因会发生交叉， 也就是说把两组策略把各自的一部分给对方， 然后形成新的策略组合。 这种重组产生新的基因的速度会比变异快的多， 也就是我们说的交配。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;577ih-0-0&quot;&gt;3， 环境会评估某个策略（DNA） 是不是适合自己， 这个通常由一个叫适应度函数的东西表达。 适应度越高， 基因就是越适应当下环境。这个适应度很像机器学习里的目标函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6ds96-0-0&quot;&gt;3， 经过一段时间， 适应环境的策略会比不适应的环境的策略得到更多的个体，因为它自身存活的概率更高， 这样， 最终环境里数量最多的， 是最适应环境的策略。这样的策略不一定有一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;npng-0-0&quot;&gt;4， 环境会变化。当环境变化， 最适宜的策略发生变化， 这个时候最适合的策略也发生变化， 导致新的物种和生态系统的生成。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ll5m-0-0&quot;&gt;机器学习：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vaek-0-0&quot;&gt;理解机器学习最简单的角度是从一个计算机程序来看： 学习算法是一段特殊的程序。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;et0m5-0-0&quot;&gt;如果说一段程序可以看做一连串从输入到输出的过程，无论是工程师还是程序员，我们都想通过设计来完成某种功能， 比如说你做一个网页， 你要画视觉图， UI图， 前端后端交互图，我们都是在给计算机设计一套解决具体问题的流程， 如做一个淘宝网。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;698n-0-0&quot;&gt;而机器学习呢？ 机器学习是你不去设计， 而让计算机自己去磨，如同用一套很一般的模子里打磨出能够解决特定问题的武器。 这点上，机器学习做的正是” 自发能够产生解决问题的程序的程序” ， 一些机器学习的经典算法如线性回归， SVM， 神经网络， 它们单个都不能解决问题， 但是通过“学习”却可以一会去预测房价， 一会去寻找美女。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;65q35-0-0&quot;&gt;生物世界的学习与机器学习最接近的是强化学习。 强化学习的目标函数是未来的奖励总和， 智能体需要学习到合理的行为来实现奖励最大化。 最简单的强化学习即条件反射。 与进化算法非常类似的， 强化学习在优化行为策略， 但是与之不同的是， 强化学习的优化方法是下面要讲的梯度方法， 一种更为贪婪， 高效的优化方法。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5sdfg-0-0&quot;&gt;整个机器学习依靠的寻求最优的方法就是梯度优化， 这个方法相比进化算法， 更具方向性和目的性， 虽然我依然不知道我要寻找的那个最优是什么， 但是我每往前走一步， 都希望最大程度的接近它， 或者说贪婪的接近它， 这个时候我们就会设置一个目标函数（类似于进化算法里的适应度）， 然后我们让参数顺着最快速减少目标函数的方向去自动调整， 如下图。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;22t20-0-0&quot;&gt;深度学习作为机器学习的发展， 其成功几乎完全依靠了以反向传播为基础的梯度下降方法， 而事实上也是， 梯度下降在很多时候更加精准。但是， 如果你认为因为梯度下降完全优于进化算法的优化方法， 就错了。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.71&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuUJ21wR3plUss5PTvxw9DibPwk4ic2JEDgxtT3501gC8kUrvicBNcYeibdIA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;cj6jk-0-0&quot;&gt;首先， 关于优化问题， 我最喜欢用的例子是一个小姑娘在山上采集蘑菇， 地势越低的地方蘑菇越多。 因而， 她需要找到一个最快的到达山谷的路径，小姑娘视力不好，因此她最好的做法就是用脚感受当下地势下降最快的方向往前走一步， 这就是梯度下降法。 在一个山谷形状比较简单的地方， 犹如上图， 你是很容易达到这个目标的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8h2dd-0-0&quot;&gt;然而真实世界的地形却并非如此简单， 比如下图，你看到无数的波峰和波谷。 每个波谷都代表一个局部最小值。 而事实上哪一个谷是最低谷， 这件事并没有那么容易。 如果你采用机器学习所使用的梯度下降， 则你极大可能会陷入到某个小的山谷里长期停滞。 当然， 在深度学习的问题里， 局部最优往往足够好了。 可是在最真实的情况下， 这下波峰和波谷的高低也可能是动态调整的， 今天的谷底可能是明天的波峰。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7795275590551181&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuU17QfcbcRBgHOH2JbLXibBNmMSXicKic8bDcgpzuDzjJALoCMh5oY1qPtA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;254&quot;/&gt;&lt;/p&gt;

地形比较崎岖

&lt;p&gt;&lt;span data-offset-key=&quot;5sqjj-0-0&quot;&gt;而进化算法呢？ 进化算法就不一样了， 进化算法相当于一次释放出无数个小姑娘（基因池）， 这些小姑娘， 各自在这个崎岖的地形里试错寻找蘑菇丰盛的最低点。 每个人的搜索策略（每个人的基因）有不同。 虽然趋近每个小山谷的速度不如梯度下降。 但是最终找到那个最优解的可能反而还更快。 这里面最核心的是， 用一个群体替代个体， 在优化的同时更多的保持多样性。 就如同自然界的物种， 有些物种比如说熊猫，居然进化成那么可爱但是站动力不强的样子， 但是自然还是没有淘汰它。 因为这种当下看着不太有利的基因， 不一定在自然巨变中就一定是没有用的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;进化算法结合机器学习之最小案例&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5r3u-0-0&quot;&gt;废话少说，我们来看看把学习算法和进化算法合在一起， 会发生什么？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1uo9j-0-0&quot;&gt;我们从一篇1994年的文章开始看起（Learning and evolution in neural networks by Nolfi, Elman, &amp;amp; Parisi)  这篇文章的作者试图阐述一件事，就是如果进化和学习是相辅相成的，不仅进化可以促进学习能力的增加，反过来， 学习也会促进进化的过程 ， 造成类似于拉马克进化的效果。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8locs-0-0&quot;&gt;来看如下的儿童游戏：  在一个grid走方格任务里，模拟生物需要在最短的时间里收集足够多的食物（用F代表），  这个生物由一个菜鸟级的神经网络代表。 神经网络接受的输入数据来自周围的食物的方向和距离远近（一种视觉，足以让你找到临近的食物）。 它的行为呢？ 直线前进， 向左或向右转动。  神经网络的输出决定它的行为。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.09&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuUV9iatorUapbf9rcXntx2JauE6mL9Ba4W2ico2aPL6C5RviaOe6PJq2Libw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.2066666666666668&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuU0FIiarVqqeMibGvC4msB76n6D4g5VJukox91NFolZbsF4d2uzm52D07A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;f7i75-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  进化算法在这里具体管什么呢？ 由于网络只有7个隐层神经元， 和两个动作输出神经元， 它的策略就是由它们的权重表达的。 而一组权重， 就可以看作一组DNA。 我们一开始准备很多这样的权重， 代表不同的策略（基因池）然后放到略有不同的环境里让这些虚拟生物跑， 跑到一定时间，就开始看它们采集到的食物的数量（fitness适应度）， 那些食物采集比较少的生物， 而保留优胜者，经过这样一个经典的进化算法流程， 生物就可以学到上图所示的那种策略。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;48t8l-0-0&quot;&gt;然而， 文章的发出者偏偏不是等闲之辈， 这个网络在干这件事之余还干的一件事是， 进行预测！ 不停的预测下一刻它会看到的东西 ，或者说理解它的动作将给它带来什么样的环境变化。你看， 这不就是当下大名鼎鼎的好奇心或预训练的前身吗， 看来还真的是我们总在重复前辈的思想。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2bp3v-0-0&quot;&gt;预测这个事情会产生什么效果呢？ 通过预测的对错（不停的把自己想到的和最终结果对比），它会开始进行学习，一个重点在于， 学习的过程只在代系之内， 也就是说比如100轮（一轮就是一次游戏）做一次进化算法的迭代， 那么学习可能是在这100轮里每轮都进行的， 但是学习的结果不会传递给下一代， 就好像你死了， 你头脑里的知识也消失了。   那么随之而来的是什么现象呢？ 学习不是就没有意义了吗？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;frnfo-0-0&quot;&gt;不是。  在实验中我们发现， 这个预测性学习， 不仅仅是学会了预测， 而且， 它让虚拟生物在进化算法中得到的食物采集能力， 也就是说任务采集能力大大增加，也就是说，进化算法被监督学习助力了！  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8316666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuU4ULZ0B3C9rBbaqEXMxw8lRIKTxn2iczickaP9ng3SC3f42XTZUtkILzw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;3qkv5-0-0&quot;&gt;这件事表明看上去匪夷所思， 它就好像是我们过去所说的拉马克进化的复兴。拉马克进化是说， 亲代在后来习得的能力是可以通过遗传传递给子代的， 用一句话就是“用进废退” 。如果你看上面的曲线， 仿佛是说监督学习的个体， 由于亲代可以把它监督学习的成果传递给下一代， 引起了具备监督学习的批次进化速度更快。 事实上呢？ 这是不可能的， 因为每一次学习到的权重并不会传递给子代， 虽然说传递是会发生的， 但是学习得到的权重改变却没有被传递。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;am9ao-0-0&quot;&gt;那么学习是如何作用于进化的呢？用一句话概括这个过程的本质是： 学习使得进化的选择效应发生了变化。 那些通过监督学习能够最大程度的改变命运的个体，每一次被选择出来， 而非像在没有学习的版本里只是天生丽质被选择了。 一句话， 学习， 使得进化的选择更准确。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;am9ao-0-0&quot;&gt;如果你这样理解这件事情， 还是会觉得有点民科， 我们可以给出一个比较数学的版本。 你依然想象一个高低不平的山区地图， 然后我们希望在上面寻找最小值。 每一个不同的策略， 代表地图上的一个点。 那么学习的效果是什么呢？学习可以在局部改变你的策略， 这就好比， 我们的策略可以从当下的一个点， 丰富到周围的很多点， 甚至是一个小的局域。 由于我们的地图是极度凹凸不平的， 可能在很小的局域里就包含了很多的地势变化。 一个没有学习的群体， 它的效果是在这个凹凸不平的地图上撒上很多点， 而有学习的呢？就是撒上很多小圆圈， 如果小圆圈所覆盖的地方包含了极小值， 我们就可以迅速的锁定它。 也就是说， 虽然学习本身的成果无法遗传， 通过学习， 我们才能更好判断哪些是值得保留的真正优势策略。 用优化的语言说就是学习通过局部优化增加了进化这个全局搜索器的效率。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;76mfl-0-0&quot;&gt;好了 ， 这个游戏看起来有点简单， 但那时你一定不要小瞧简单的游戏， 做AI， 你就应该从toy model 里理解问题， 然后看你的想法是否salable， 可扩展。 恰恰是， 这篇文章的成果可以映射当下的一系列AI成果， 直到征服星际争霸的alpha-star。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3im45-0-0&quot;&gt;如果你追溯这条线的发展， 你首先会看到。机器学习和进化算法， 在这篇文章后， 都得到了飞速发展， 但是机器学习要快很多。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;f37a3-0-0&quot;&gt;然后你会看到， 进化算法开始桥悄悄进入很多机器学习框架。而且， 正在实现之前机器学习所完全不可能实现的任务。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f37a3-0-0&quot;&gt;&lt;strong&gt;进化算法结合机器学习带来的无限可能&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;91kr2-0-0&quot;&gt;首先， 进化算法的本质是一种集群算法， 通过集群， 遍历式的搜索策略空间。 这点， 就比梯度下降更好克服非线性问题里局部极小的问题。  而且在用梯度方法不好并行的一些问题里， 如RNN的训练， 这种方法却可以产生出并行的威力。 再有，进化算法可以帮助我们做梯度下降所难以实现的改变，梯度下降需要问题可以微分， 而进化算法就自由的多， 只要你能够定义适应度和策略就可以做。 它特别擅长做超参数的调整， 改进网络架构， 甚至可以改变学习算法本身。 一句话就是说， 进化算法迈大步， 梯度下降局部调。 这样的思路结合，可以解决相当困难的问题。&lt;strong&gt;最后， 进化算法更有遐想力的地方， 还在于它所带来的智能体间合作与博弈的可能。&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7gvjq-0-0&quot;&gt;我们先来看合作， 后面的文章来自那个AI界的不为人赏识的教父级人物Schmidhuber： Accelerated Neural Evolution through Cooperatively Co-evolved Synapses  这篇08年左右的文章，将这种思路几乎用到了极致。这篇文章用进化算法直接解决了一个传统强化学习的经典任务pole balancing的较困难版本， 并且证明它在这类问题上比强化学习更有可扩展性。  联想到强化学习运用到现实生活中， 却经常碰壁， 原因就是鲁棒性差， 可扩展性不高，进化算法会是对它的一个极好补充。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7gvjq-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e9fdq-0-0&quot;&gt;那么何为合作？ 这篇文章里， Schmidhuber直接用进化算法来优化一个模块化的网络。 网络中的每个模块可以看作群体中的一个个体， 而它们的组合网络就可以解决更复杂的任务， 合作体现在网络模块之间的配合。 这点让人不仅想到那个群体和个体的界限问题。 人由不同的器官组成， 人脑由不同脑区组成， 社会由人组成， 这些都可以看作一种广义的合作。 如果每个个体都很优秀， 组成的国家很弱小， 它还是会灭亡，基因传不下去。因此， 进化就会鼓励合作。同样的道理， 在这个模块化的网络里， 进化算法会促进模块间更好的配合最终完成复杂的控制任务。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.49666666666666665&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuUGLvwG9xbn0tm7TSwWNTZib3tskeNbqx6srB0X0KMv3hthKnODjVfC1w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

四个模块的网路， 网络权重由一组“染色体”编码Accelerated Neural Evolution through Cooperatively Co-evolved Synapses


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44666666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuU3I4iaACCKML0l7MSUFg64j8dF06cw8GzbKrmK3qkkciat1ficVfWcZD5A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

经典强化学习游戏， 平衡杆， 我们需要以恰当的方法移动小车，让上面的倒立单摆稳定。 这个游戏有很多复杂版本。

&lt;p&gt;&lt;span data-offset-key=&quot;33n2q-0-0&quot;&gt;进化算法和机器学习混合的最新应用案例 - AlphaStart： &lt;/span&gt;&lt;span data-offset-key=&quot;bf129-0-0&quot;&gt;AlphaStart, 在星际争霸这样级别的游戏击败人类， 应该说这是一个了不起的胜利， 因为星际争霸这个游戏比围棋要接近真实世界很多， 信息是局部的，远方的世界笼罩在黑雾里， 当下和未来是连续的， 战略需要跨越很多时间尺度。这些问题，需要比阿法狗更加复杂的接近方法。  AlphaStart以一个具有记忆的神经网络LSTM为基础， 然后用到的学习方法， 正是进化算法加机器学习（强化+监督学习）。应该说， 这才是这套方法的灵&lt;/span&gt;魂（参见AlphaStart: An Evolutionary Computation Perspective）。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;40ln9-0-0&quot;&gt;它的思想简直是对1994文章的升华， 把拉马克进化真正用起来。 也就是说， 我们把一个最外层的代系间的进化过程， 和内层的持续不断的强化学习结合起来。 内层的学习会影响外层的进化。 更重要的是， 在这里， 我们通过进化引入了不同参数的网络（智能体）间的博弈。 那些从一个根基上产生的稍有不同的网络， 会通过学习来改变自己， 并在战斗中一决雌雄， 当一轮结束， 优胜者将改写失败者的基因， 但失败者不会马上消失，正如文章中所说：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4931d-0-0&quot;&gt;“The fittest solutions survive longer, naturally providing a form of elitism/hall of fame, but even ancestors that aren’t elites may be preserved, maintaining diversity.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcebdNQcqmFd7cGXNcVMicbuUjvtRygbMC26iciauue49ngHF297rxjqqME094I8CgPhE1GW1DXhQsQwQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

alphastar的进化历程图， 我们看到在每个时间点我们得到的都不是一个点， 而是一群点，代表我们式以族群为单元进化的， alaphastart不是一个网络

&lt;p&gt;&lt;span data-offset-key=&quot;9vqjg-0-0&quot;&gt;这样， 我们就鼓励了种群的多样性， 而得到一个在纳什均衡状态下多样化的解空间（一组不同策略的组合）。也就说， 最后的最优解不是一个网络， 一个是策略集群。 如此， 我们就可以玩转很复杂的问题。  保持多样性的好处还是在之前的小姑娘采蘑菇的例子说的，任何优化都是在高低不平的空间里寻找最低点， 而在星际这样复杂的游戏中， 整个地形都在缓慢或迅速的变化着， 之前处于劣势的个体，可能在下一个拐点变成处于优势的。 &lt;strong&gt;这也是进化算法的温情之处， 世界是充满不确定性的，因此我不要斩尽杀绝。&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ekui-0-0&quot;&gt;正如schmidhuber在上一篇文章说的， 这一类损失函数对应的地形动态变化的问题里， 那些传统的优化问题甚至会完全失效（稳定性丧失）， 而最后剩下的，会是进化算法。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2q9bl-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  deepmind的宣传稿里有一个非常好的例子， 我不是星际玩家， 但是你应该可以很快理解。 大意说的是在早期具有优势的策略（比如用某种武器迅速偷袭对方的高风险策略）， 会随着时间发生变化， 有的时候， 这种变化完全不是之前基础的改进， 而是从完全不同的分支长出来的（比如早期通过增加工人取得经济优势，这与风险策略几乎相反，而这个策略后来居上）。如果你砍掉所有的分支， 你是否还能找到那个后期的优势策略呢？  那个后来最优的策略， 压根不是你先前的最优渐进发展出来的。     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;fd5mp-0-0&quot;&gt;“As the league progresses and new competitors are created, &lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;new counterstrategies&lt;/span&gt; emerge that are able to defeat the earlier strategies. While some new competitors execute a strategy that is merely a refinement of a previous strategy, others discover drastically new strategies consisting of entirely new build orders, unit compositions, and micro-management plans&lt;/span&gt;&lt;span data-offset-key=&quot;fd5mp-6-0&quot;&gt;.“&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6dcl7-0-0&quot;&gt;在进化算法和学习算法的结合里， 我们看到了一种AI发展的未来路径。 我们也看到了让AI从一个单个学习的网络， 发展到群体， 并通过群体间的合作和博弈促进发展的某种必要性。 在整个过程里， 我们看到人类自己智能产生和发展的影子。 这样的一条道路， 会给我们带来怎样的未来， 让我们拭目以待。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;6dcl7-0-0&quot;&gt;更多内容请关注作者新书&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot;/&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 12 Apr 2019 11:37:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ctzlXu0ZQQ</dc:identifier>
</item>
<item>
<title>5分钟读懂强化学习之Q-learning</title>
<link>http://www.jintiankansha.me/t/SrT1aw8yLw</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/SrT1aw8yLw</guid>
<description>&lt;p&gt;强化学习的难点，在于其引入了时间这个维度，不管是有监督还是无监督学习，都是能获得即使反馈，但到了强化学习中，反馈来的没那么及时。在周志华的《机器学习》中，举过一个种西瓜的例子。种瓜有很多步骤，例如选种，浇水，施肥，除草，杀虫这么多操作之后最终才能收获西瓜。但是，我们只有等到西瓜收获之后，才知道种的瓜好不好，也就是说，我们在种瓜过程中执行的某个操作时，并不能立即获得这个操作能不能获得好瓜，仅能得到一个当前的反馈，比如瓜苗看起来更健壮了。因此我们就需要多次种瓜，不断摸索，才能总结一个好的种瓜策略。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.8597222222222223&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了应对时间带来的不确定性，就需要一个框架来量化时间的流逝对我们关心奖励有怎样的影响。按照最简单的线性模型，我们首先确认要引入那些特征，首先是前一个时间的得分，其次是新发生的事件对奖励的影响，由于我们对未来的奖励看的不如现在的重要，因此可以引入折线率，折现率越高，说明我们越处于游戏的早期，对未来的关注也越多，这道理就如同我们在年轻时更要做长久的规划。同时在更新策略时，也会有快慢之分，将其称为学习率。由此得出了时间差分学习(Temporal Difference)，简称TD方法的更新公式：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.14660831509846828&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatGHyEQuonYibg2174nHUCFhLdAGRQvSVal7TrHezmsKZ01ErVogJuYA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;457&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该公式描述了给定一个策略，该怎么去更新下一个时刻的估值函数，其中的V代表估值函数，下一个时刻的估值乘以折现率，再减去当前的差值，代表了一个策略的间接影响，可以看成是战略决策，再加上下一个时刻能立即获得的奖励，就是智能体（agent）应该关注的策略的影响，最后对此乘以学习率，用来控制随机性的影响，既要避免由于学习率过低导致的智能体学的太慢，也要避免学习率过高导致智能体矫枉过正。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;由于在当下，你并不知道下一时刻的估值函数，所以你要做的是对其有一个尽可能准确的估算，这个估算被称为Q value，对应的算法称为Q-learning。如果你是用神经网络得出对未来value的估算的，那你使用的算法框架就从强化学习变为了深度强化学习。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.7262357414448669&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaiaD8kfEVYfNkfJgLhYEbOzyOo6TqicKnb0NGgYfL5ib6Kkb8jOMt56xTg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;526&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是一个具体的例子，将一个结冰的湖看成是一个4×4的方格，&lt;span&gt;每个格子可以是起始块（S），目标块（G）、冻结块（F）或者危险块（H），目标是通过上下左右的移动，找出能最快从起始块到目标块的最短路径来，同时避免走到危险块上，（走到危险块就意味着游戏结束）为了引入随机性的影响，还可以假设有风吹过，会随机的让你向一个方向漂移。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.466893039049236&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaacrLdcvu8QwtBo9mvVzf3KZ3wZGMospic8OhNsuz9sdlNvfjjicP53hdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;589&quot; /&gt;&lt;/p&gt;
&lt;p&gt;左图是每个位置对应的Q value的表，最初都是0，一开始的策略就是随机生成的，假定第一步是向左，那根据上文公式，假定学习率是0.1，折现率是0.5，而每走一步，会带来-0.4的奖励，那么（1.2）的Q value就是 0 + 0.1 ×[ -0.4 + 0.5× (0)-0] = -0.04，为了简化问题，此处这里没有假设湖面有风。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4200626959247649&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaibSXpgch86ApiaPm5EBrQkr7KvJxmlPOoYxDa4icOJiajK8Oyl55fuhd7w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设之后又接着往右走了一步，用类似的方法更新（1，3）的Q value了，得到（1.3）的Q value还为-0.04&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4054878048780488&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaTtSt6Siceq5eg320L4ibrKfiaVFKloHuhiaAXS3ficPIheOXfH8W0Mskqcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;656&quot; /&gt;&lt;/p&gt;

&lt;p&gt;等到了下个时刻，骰子告诉我们要往左走，此时就需要更新（1，2）的Q-value，计算式为：V(s) = 0 +0.1× [ -0.4 + 0.5× (-0.04)-0) ]&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.46333853354134164&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaabAV71N9QPLA95sNdHmxw735yYwBd1JZ4fVxG4fMuYaHrlhLnDrnBcA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;641&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;从这里，智能体就能学到先向右在向左不是一个好的策略，会浪费时间，依次类推，不断根据之前的状态更新左边的Q table，直到目标达成或游戏结束。这就是TD learning的基本步骤，通过多次的实验，智能体掌握了在不同位置下，相应的策略的估值分，从而解决了将较远的未来映射到当下的对不同策略的激励这个强化学习的核心问题。&lt;/p&gt;

&lt;p&gt;根据是否亲自尝试不同的策略，Q learning可以分为在线和离线俩者，用学下棋来举例，前者是AI通过自己和人类选手下棋或者自我对弈来提升，而后者AI不操作只观察他人下棋的棋谱，下面看看再离线（off-line）的Q learning中，Q value更新的公式又有了怎样的改变。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.12110726643598616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa56EFEGaa5FzYynegbiaG0HHB5VXQericOGjxf1VyCWAGKjQyTyzeCPSQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;578&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和之前的公式对比，最大的不同是未来的Q值是所有行动/策略对应的未来Q值中最大的那一个，这代表着模型根据已有的知识，选择了局部最优的那个行动，通过不断的优化Q table，使得这样一个只考虑一步的最简单型启发规则，也能学到全局相对较优的策略。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.292626728110599&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatd4f6hZKGMOiasVOoUG86yF7sKictnZPX3LCNib7BPjlyMxJYAiabw8htA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;434&quot; /&gt;&lt;/p&gt;


&lt;p&gt;还是冰湖的案例，假设在训练的循环中，当前智能体已经学会了在（3，2）这个点上，向左和向右走对应的估值，此时模型要做的是去判定利用当前的知识，还是去探索未知策略的影响，探索是为了发现环境的更多信息，而当探索进行到了一定的程度，就需要根据已知信息去最大化奖励值，在Q learning中，通过一个0-1的参数来用随机性调控探索和开发的权衡。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.54&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaRggot5LiaL2n2jGVV7aOQAQp5L5WXiaqkvtl1CFlv4FLJJuGW0H3IZDA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设骰子告诉智能体应该选择探索，因此选择了向下走，左图代表的之前智能体的Q-table，现在要做的是根据公式，更新（3，2）这里的Q value，由于向下走的Q-value最低，假定学习率是0.1，折现率是1，那么（3，2）这个点向下走这个策略的更新后的Q value就是：&lt;/p&gt;

&lt;p&gt;Q( (3,2) down) = Q( (3,2) down ) + 0.1× ( 0.4 + 1 × max [Q( (4,2) action) ]- Q( (3,2), down））&lt;/p&gt;

&lt;p&gt;Q( (3,2), down) = 0.6 + 0.1× ( 0.4 + 1 × max [0.2, 0.4, 0.6] – 0.6)=0.64&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4582043343653251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaDiahblY26dmQgclaiac3B1IOojqD73sqjlEfqdpjYvgEZf4j0CEcNekw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;646&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在在线的Q learning算法下（称为State-Action-Reward-State-Action ，简称SARSA），Q table的更新公式变为了&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatj8T5icjPo7sDEOpshRUibnb0ldh2ic59JdNcGibaqwicpcicHjiaM8TyEKmA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;588&quot; data-cropy1=&quot;9.517985611510792&quot; data-cropy2=&quot;93.06474820143886&quot; data-ratio=&quot;0.14285714285714285&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa2dTIjl6icv7frMeMnyqIUPDhYeJ55RWKPxtoicbj5p0gpOB5kubo8DMQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;588&quot; /&gt;&lt;/p&gt;
&lt;p&gt; 此处不同的是没有了max，由于是智能体在亲自参与，这里也就没法像离线时那样，选择一个最优的策略。不管是在线还是离线，在训练的时候需要做经验回放，即存储当前训练的状态到记忆体中，等下一次训练时再调用。&lt;/p&gt;

&lt;p&gt;以上就是强化学习中最基础的Q learning，上诉的例子中不存在随机性，要引入随机性，可以需要通过蒙特卡罗的方法，来进行采样，同时引入对弈树，对其进行翦枝，这就是alpha zero的精髓。了解了Q learning的步骤，可以分析强化学习适用的领域所满足的假设，例如必须有能够清晰定义，事先已知且有限的策略，但现实生活中，真正重要的选择都是无限游戏，有无数种可能的选项，有前人根本不曾想到的选项，因此说强化学习不等价于强AI，只是通向强AI的一条必要选项。&lt;/p&gt;

&lt;p&gt;不同于人类的学习，是首先对坏境建模，之后再根据模型找到合适的启发式规则，Q learning框架是模型无关的，不管是什么样的问题，Q learning做的都是去更新状态对应的估值表，不管问题本身具有什么样的特点。和人类思维的另一个不同是Q learning中没有因果关系，学到的Q table只是反映了奖励和策略之间的相关性，而人类的学习则是受着因果关系指引的。关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;，其中有详细的论述。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384219&amp;amp;idx=1&amp;amp;sn=f396d027ea5a6074e0f0cda0aeb0cded&amp;amp;chksm=84f3c7dab3844ecc80be70e9b9e47cd12686624d71158caf69fb79de9b1067d1b548e31ee132&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;空间简史-人类认识空间的旅程与其对强化学习的启示&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 08 Apr 2019 18:01:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/SrT1aw8yLw</dc:identifier>
</item>
</channel>
</rss>