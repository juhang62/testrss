<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何让让神经网络开口说话</title>
<link>http://www.jintiankansha.me/t/o5DC8CEtwb</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/o5DC8CEtwb</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;人类简史说，人类的进步大大依靠与人的语言，人在灵长类中具有最高级的语言能力，并且能够编故事，从而把越来越多的人组织在一起产生了文明，这才是人类社会进化的根本。&lt;/span&gt;那么，神经网络能否掌握人类的语言，听懂人类的故事呢？ 相信朋友们都知道谷歌翻译，谷歌助手，机器写诗， 它们实现了令人惊艳的性能，并在某些具体任务里让人真假难辨。这里的技术核心，就是&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;-&lt;/span&gt;&lt;span&gt;我们常说的传说中的循环神经网络。&lt;/span&gt;RNN&lt;span&gt;可以称得上是深度学习未来最有前景的工具之一。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.65379113018598&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNw72RI3PwDHibjxNr7t1oNQLSv5FLvmPR6GDkZTg0F5aJ9WutibYapUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;699&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;时间序列和&lt;/span&gt;RNN&lt;span&gt;引入&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让我们从基础学起，&lt;span&gt;首先&lt;/span&gt;什么是时间序列，&lt;span&gt;我们又&lt;/span&gt;为什么需要&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6744730679156908&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k663I0G9glMsglxZ9z9S1YH5lmWibkRWLeaj0kU2MwM4WGibDickz81R0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6227848101265823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kwibz1UiaqqF3Aqtee5PF7wZJAFfRAVgWGoGHicAfb7XibGyhibKztNtay6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;395&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;时间序列，&lt;span&gt;是&lt;/span&gt;一个随时间变化的过程，&lt;span&gt;我们&lt;/span&gt;把它像一个数列一样排列下来，&lt;/span&gt; &lt;span&gt;&lt;span&gt;序列&lt;/span&gt;里的数字往往&lt;span&gt;看起来&lt;/span&gt;在随机波动。&lt;/span&gt; &lt;span&gt;&lt;span&gt;一定&lt;/span&gt;程度，&lt;span&gt;我们&lt;/span&gt;可以把它看做以一个一维的图像或向量，&lt;span&gt;这个&lt;/span&gt;图像不停的向前滚动。&lt;span&gt;比如说文字，就是一个典型的时间序列。&lt;/span&gt;处理和时间有关的信息，我们再次回到我们的大脑，看我们的大脑是怎么处理这一类问题的。&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;实验发现，大脑越靠近感官的区域就越像CNN的结构，它的最本质特征是前馈，也就是每一次神经信息都是从感官向大脑的深层一步步推进的，而每层网络之间是没有联系的。而到了深层，这一切发生了变化，大脑内开始出现一些神经网络，这些网络在层间出现了很多的连接，这意味着什么呢？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8511705685618729&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQek5FIxZefqx6QJemfLiceYcbaLAG6P7V3JV0ibsxH67IUb3yAOLSO2A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;span&gt;&lt;span&gt;我们说，我们需要引入时间这个维度才能完全理解这件事，这些层内的链接意味着当下的神经信息会在下一个时刻被层内的其它神经元接受，而这个接受的结果呢？&lt;/span&gt; &lt;span&gt;当下的信息会传给这些层内的神经元，&lt;/span&gt; &lt;span&gt;从而使得这个信息在网络内回响一段时间，就好像社交网络里人们互相发送消息或分享朋友圈一样一个事件发生，就会引发一系列的社交网络动作使得信息的影响停留很久。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个典型的例子是如果人和人彼此链接形成回路，我发出的信息可能会在若干时间又传递给我，从而让我自己直接看到我的过去历史，这个信息停留的效应是什么？大脑处理时间相关信息的关键正是记忆。 最典型的在一个对话过程里，你要记住此时此刻所说的话， 还要关联前面很久的话，否则就会发生歧义。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;引入了记忆， &lt;span&gt;这些问题就好解决了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;记忆分为不同的种类， &lt;span&gt;你在一个对话里可能回忆起很久以前的事情，&lt;/span&gt; &lt;span&gt;但是这和刚刚说的话的重要性显然不一样，&lt;/span&gt; &lt;span&gt;所以我们大脑就把这些记忆分成了长时的记忆和短时的记忆（工作记忆）。这个短时记忆，&lt;/span&gt; &lt;span&gt;正是通过互相高度连接成回路的网络之间的神经元互相喊话造成的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;一个神经网络里神经元之间彼此互相传递信号，形成循环往复的回路结构，就可以让过去的信息保持下来，把&lt;/span&gt;这个&lt;span&gt;原理转化成人工神经网络就是&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;，&lt;span&gt;翻译出来就是循环神经网络，这个循环，正是彼此链接形成回路的意思。我们看怎么把模糊的想法一步步变成一个数学模型。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5401174168297456&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kI3cicAeAs54DqNJF4toeQm7fEB9FGnQ0MINuiauBAydgoGdhvrnTJ67g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1022&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们把这个网络内部神经元互相链接的方式用一个矩阵Wij表达， 也就是从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;i&lt;/span&gt;到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;j&lt;/span&gt;的连接强度表示。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wij&lt;/span&gt;就是神经元间彼此沟通的方法，&lt;/span&gt; &lt;span&gt;我们看rnn的方程，一方面网络要接受此刻从外部进来的输入，另一方面网络接受自己内部的神经元从上一个时间时刻通过&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wi&lt;/span&gt;进来的输入，这一部分代表过去的历史 ，决定网络此刻的状态。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23720136518771331&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kAiaMKQFH5unWWVxrqymUcsCoMmTxMaVhNnsI8TZic9CdBicyhhib9Z51Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;586&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;深度学习的核心就是特征提取。用一句话说，&lt;/span&gt;RNN&lt;span&gt;具备的素质就是把整个过去的时间信息转化成此刻的一组特征，然后让网络做预测或者决策。 比如，此刻股市的价格是所有之前和公司有关的信息一点一滴积累起来的。公司每个时间点的信息， 就是输入向量，&lt;/span&gt; &lt;span&gt; 那么神经元所干的事情是什么呢？&lt;/span&gt;  &lt;span&gt;它把所有过去的信息转化为当下的神经元活动， 而这些活动，就是一组由过去历史信息组成， 决定当下预测的特征。在股价的情况下，你可以想象成， 这些神经元就在描绘人们的信心指数， 而信心是所有过去点信息汇集的结果，RNN把每个时间步的信息通过神经元之间的互相喊话Wij，压缩成当下的一个状态向量hi，它包含了所有和我此刻决策有关的历史。 数学上你可以推到， 这个hi是所有过去输入的一个函数.&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;学说话&lt;/span&gt;&lt;/span&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;什么叫预测说话，这是一个形式上开起来有点傻的例子， 就是给你一个序列的字母或单词让你猜下一个，我想你一定玩过报纸上的填词游戏。 那个每个时间步骤的输入就是一个字母，然后我要输出最后一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;一个处理这个问题的经典模型叫N-gram&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;它是说我可以把这个猜词的游戏看成一系列条件概率来处理，&lt;span&gt;用这个模拟过去的字母对当下字母的影响。因为我们知道语言本身存在非常清晰的规律，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这个规律就是字母都是成对出现的，&lt;/span&gt; &lt;span&gt;如果全面我给你的字母是&lt;/span&gt;hell&lt;span&gt;， 且还有一个字母，那么你基本就知道是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;o&lt;/span&gt;&lt;span&gt;了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;这个现象我们可以用字母共现概率表达，也就是说衡量一些字母在一起的概率&lt;/span&gt;P&lt;span&gt;（&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;w&lt;/span&gt;&lt;/span&gt;1, w2,w3…&lt;span&gt;&lt;span&gt;）&lt;/span&gt;,&lt;/span&gt; &lt;span&gt;&lt;span&gt;那些经常在一起出现的字母概率会很大，&lt;/span&gt; &lt;span&gt;而其他很小，&lt;/span&gt; &lt;span&gt;我们可以用经典的条件概率公式来表达，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;这个事情。&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.03968253968253968&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQz8rY74Za5wiaIaMLlVu9rZfE2QY8lKzQ2WOQoPnQAzJ1LFGdOY2DUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1260&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个公式的意思非常清楚， 就是字母和字母之间不是互相独立的， 就是刚刚说的后面的字母极强的依赖于前面的字母， 这种关系通过条件概率体现。 这件事我们用一个图画出来，就是当下是所有过去的作用， 而一旦字母多了以后，这件事就特别复杂。我们用物理学家惯用的加入假设来简化降维，就是我当下的字母与过去相关，这种相关却是有限的， 比如只与前面n-1个相关。&lt;/span&gt; &lt;span&gt;这个模型就是n&lt;/span&gt; &lt;span&gt;gram&lt;/span&gt;  &lt;span&gt;。&lt;/span&gt;  &lt;span&gt;比如说 只与前面一个相关，这就是2-gram，&lt;/span&gt; &lt;span&gt;而这个模型就是标准的马尔科夫链。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;那么，这里我们换一个模型来，&lt;span&gt;我们用&lt;/span&gt;rnn&lt;span&gt;来做。具体怎么干？ 每个字母给他编个号码（独热编码），我一个一个的输入给这个网络，网络每一刻的状态取决于此刻的输入和上一刻的网络内部状态，而上一个网络内部状态又取决于过去的，输入，这样当我整个单词输出完毕，每个字母的信息可以看作都包含了在了神经元的状态里。我们要&lt;/span&gt;把整个输入切分成小块，&lt;/span&gt; &lt;span&gt;用一个卷积核把&lt;span&gt;它们&lt;/span&gt;卷入到一个隐层网络里，&lt;span&gt;只不过此处的&lt;/span&gt;&lt;/span&gt;Wij&lt;span&gt;&lt;span&gt;代替了&lt;/span&gt;CNN&lt;span&gt;的卷积核，把历史卷入到一个隐层里。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们用一段小巧的&lt;/span&gt;python&lt;span&gt;代码让你重新理解下上述的原理：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.31364031277150306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kpLMkok673hoAia1JmwT3BCCtSajFRyeRxvM9R2KMeRahYMlNZxULcAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1151&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个结构，&lt;span&gt;非但&lt;/span&gt;优雅，&lt;span&gt;而且&lt;/span&gt;有效。&lt;span&gt;一个非常重要的点是，&lt;/span&gt; &lt;span&gt;我们不必在假定那个&lt;/span&gt;n-gram&lt;span&gt;里的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;n&lt;/span&gt;&lt;span&gt;，这时候，因为原则上&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;是所有历史输入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;I1&lt;/span&gt;&lt;/span&gt;…It &lt;span&gt;&lt;span&gt;的函数，&lt;/span&gt; &lt;span&gt;这个过程，&lt;/span&gt; &lt;span&gt;也就是说我们具有一个&lt;/span&gt;&lt;/span&gt; &lt;span&gt;infinit&lt;/span&gt; – &lt;span&gt;gram&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;&lt;span&gt;你来思考一下这是真的吗？&lt;/span&gt;No&lt;/span&gt; &lt;span&gt;，&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;你知道信息的传播是有损耗的，&lt;/span&gt; &lt;span&gt;如果把&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;展开，它事实上相当于一个和历史长度一样长的深度网络，信息随着每个时间步骤往深度传播，这个传播的信息是有损耗的，到一定程度我就记不住之前的信息了，当然如果你的学习学的足够好，&lt;/span&gt;Wij&lt;span&gt;还是可以学到应该学的记忆长度。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2611683848797251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kFoiaJw1g1Q2BhwvicUsrbnibR4o1JDmfnOIGU1uY1cvSwkongmI6DmOcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;582&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图：循环的本质&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;刚刚的词语预测模型，我们最终的目标是每次预测出现的下一个字母，我们的方法是在隐层之上加入一个读出权重，这个矩阵的作用如同我们之前讲的感知机，是在有效的特征基础上加一个线性分类器， 再加一个softmax函数得到一个向量每个数值代表出现某个字母的概率。然后我们希望我们的向量能够完全预测真实字母出现的概率，因此我们把真实数据作为输入不停的让他预测这个字母，直到这个概率和真实是最匹配的，我们就得到了训练好的模型， 然后我们就可以让他生成一段文本了！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8045325779036827&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kt9lGY722ic5zh0p4qugEHz7MXuAGUH9HJmXaB1ibksd7iadjJSvjLTAHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;706&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上面介绍的过程是语言介绍的最基本的部分，这里所说的对字母进行独热编码这件事，如果进入到文本的世界，你要预测的不是字母而是单词， 这时候，我们通常不再采用独热编码，而你自己思考这件事显然是不合理的，因为词语之间相互关联，如果你把词语本身作为完全独立的东西来编码，你事实上就是丧失了这种本来的语义结构的信息，因而我们用更复杂的word2vec来替换，这个方案的核心依然是编码，只不过这套编码也是从神经网络里学过来的，具体来说，这套编码可以把语言内在的语法或语义信息包含在编码向量的空间结构里， 从而使用这部分语言有关的先验信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9627749576988156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kcnRq0wZJO8FLnRkeIrAmTWoF1W7Idhuz1lS0Lp00fbqySqScyGSx1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;591&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;的物理与工程理解&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们说把对&lt;/span&gt;RNN&lt;span&gt;的理解&lt;span&gt;在&lt;/span&gt;抬升一个层次，RNN&lt;span&gt;在物理学家眼里是一个动力系统，&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;&lt;span&gt;循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖&lt;/span&gt;-&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;路径依赖&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。另一个角度看也符合著名的图灵机原理。&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;即此刻的状态包含上一刻的历史，又是下一刻变化的依据。&lt;span&gt;工程上看，&lt;/span&gt; 这其实&lt;span&gt;就是&lt;/span&gt;可编程神经网络的概念，即当你有一个未知的过程，但你可以测量到输入和输出，&lt;/span&gt; &lt;span&gt;你假设当这个过程通过&lt;/span&gt;RNN&lt;span&gt;的时候，&lt;span&gt;你要设计一个程序来完成&lt;/span&gt;这样的输入输出规律，&lt;span&gt;那么这个程序可以被&lt;/span&gt;RNN&lt;span&gt;学出来&lt;/span&gt;。 &lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.071278825995807&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMUht40XmvRwwC2zjpTEgjibqDiakMScEicDqfDYEz4tnGmlrU44Nf0hQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;477&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;总的来说&lt;/span&gt; &lt;span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;无论作为&lt;/span&gt;一个非线性动力系统&lt;span&gt;还是程序生成器&lt;/span&gt;的&lt;/span&gt;RNN&lt;span&gt;, &lt;span&gt;都需要依然数据背后本身是有规律可循的，也就是它背后真的有某种“&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;program&lt;/span&gt;&lt;span&gt;”而非完全随机&lt;/span&gt;。&lt;/span&gt; &lt;span&gt;如果一旦&lt;/span&gt;RNN&lt;span&gt;学习到了&lt;span&gt;真实&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  数据背后的&lt;span&gt;动力系统&lt;/span&gt;的性质，&lt;span&gt;它也就&lt;/span&gt;掌握了过程中复杂的路径依赖，从而能够对&lt;span&gt;过去&lt;/span&gt;和&lt;span&gt;现在&lt;/span&gt;进行建模。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;生成诗歌&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;诗歌的生成，方法类似于句子，区别在于，诗歌是有主题的，我们都知道主题的重要性，同一个句话在不同主题下含义完全不同，如何把这个诗歌的主题输入给rnn呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;事实上，我们可以把这种主题或者背景看作是一种上下文关系，比如说一首诗歌有四行，在生成第一行的时候，我可以输入开头一个关键词，然后让rnn自动生成一行，虽然这个过程还有一定随机性，但是这一行内容无疑确定了诗歌整体的基调，因此，我们需要把这种信息编码成一个包含上下文含义的内容向量，这个向量作为整个网络的输入。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3133535660091047&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4klibJGiaACy2FrV6BFJ7WN70mlv5gI20U41fV8hQD4v7FKrqEpZicuPhJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1318&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们采取的方法是逐行生成诗歌。已经生成的行就作为后面生成行的基调，每行诗歌的生成都使用和之前一样的RNN，但是，它的输入要加上一个主题向量。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如何提取主题向量呢？首先，它是一种由所有之前生成行构成的宏观信息，那么，我们也可以用一个RNN来提取它，由于宏观， 这个RNN的输入是每行诗歌。而我们要用一个方法， 直接把行编码， 而不像刚刚是把字母或单词编码。 一行诗歌本身每个子都是一个向量，整行诗歌构成一个二维的图像（字数一定， 图像尺寸一定）。记得什么东西最好处理图像吗？我们引入一个CNN网络，把这些同样尺存的“图片”，压缩一个特征向量，最后被这个被称为主题RNN的RCM卷成一个主题向量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4824766355140187&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kCiaqcL6B6pl0WAZxmnxWK42KjPXt6sN79tkHTyicHqrpXt5BpxNBFsog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;856&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个主题向量作为一个先验输入交给RGM， 这个普通的RNN， 作为这行诗词生成的关键。由此生成的诗歌不仅是押韵的，而且可以构成一个完整的意思。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kzn6aYicN4L5a7symMoAvt8vusq6QLdcCzYdSgXJRzks7DgfhDrNibMKQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1001&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;引入长短记忆&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;RNN虽然看起来好用，&lt;/span&gt; &lt;span&gt;&lt;span&gt;而且似乎&lt;/span&gt;能够模拟任何一个&lt;span&gt;动力过程或程序&lt;/span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;实际&lt;/span&gt;中，&lt;/span&gt; &lt;span&gt;&lt;span&gt;却&lt;/span&gt;并没有那么容易。&lt;/span&gt; &lt;span&gt;&lt;span&gt;为什么&lt;/span&gt;？&lt;/span&gt; &lt;span&gt;RNN&lt;span&gt;的&lt;/span&gt;强大&lt;span&gt;功能&lt;/span&gt;，&lt;span&gt;体现在&lt;/span&gt;能够学习&lt;span&gt;过去&lt;/span&gt;时间点对现在的影响这件事，&lt;span&gt;但是，我们刚刚说了&lt;/span&gt;RNN&lt;span&gt;相当于于一个无限深的深度网络， 而传播是有损失的， 假定每次这个损失是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.99&lt;/span&gt;&lt;span&gt;， 经过&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;100&lt;/span&gt;&lt;span&gt;层后也是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.36&lt;/span&gt;&lt;span&gt;， 这种信息传递的递减， 会导致&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;无法学到长时间的信息之间的关联性&lt;/span&gt;。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的一个重要的解决这个问题的技巧是&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;引入一个能够学习多个时间尺度的改进版&lt;/span&gt;RNN&lt;/span&gt; – &lt;span&gt;LSTM（&lt;/span&gt;Long short term memory&lt;span&gt;）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6776556776556777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koTXDZNjRYuG90Osb9NhnCzCCkCCq8FTCC50WSkObHsKBQ1bJEXQf0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;819&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;首先，什么是时间尺度，宇宙间最神秘的概念莫过于时间，但是绝对的时间毫无意义，一个时间的长短，一定是根据你所描述的过程，&lt;/span&gt; &lt;span&gt;&lt;span&gt;比如你是描述一个人一生的变化过程，还是描述一次化学反应，&lt;/span&gt; &lt;span&gt;还是生物进化，各自的时间尺度可以有级数之差。我们对时间尺度最数学的理解来自原子的半衰期。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于大脑神经尺度，处理动态过程最麻烦的东西就是时间尺度， 因为生活中的事情往往是多时间尺度的，比如你的一个决策今天晚上吃不吃饭，可能既受到刚刚是不是饿了的影响，又受到这个月你是不是有减肥计划的影响，还受到你长期养成的饮食习惯的影响，因此， 你的大脑需要有对复杂时间尺度的处理能力。也就是说，同时对各个不同的时间尺度变化的信息保持特异性和敏感度， 这和我们图像识别里需要对图像的局部和整体反应是类似的。一个有意思的电影inception描述要改变一个人的意念，我们需要一步步的走进他思想的最深层 ，逐层的改变它对某个东西的认知，而每个层里的时间尺度又有不同，就是对这件事最好的体现。事实上，类似于&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;inception&lt;/span&gt;的描述，我们的确发现在我们的大脑里，有着不同时间尺度的处理， 越浅层，我们就对越近的东西敏感，而进入到大脑的深层，我们开始对慢过程敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;LSTM&lt;span&gt;，所谓的长短记忆机，这是对这个过程的模拟。我们来看它是怎么对&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;进行改进的&lt;/span&gt;，&lt;span&gt;这个道理非常简单，首先，我们加入一个叫做记忆细胞的概念，进入到记忆细胞的信息，可以永久不被改变，&lt;/span&gt; &lt;span&gt;但也可以根据一定触发条件修改，实现的方法是我们加入一些&lt;/span&gt;控制&lt;span&gt;信息流动&lt;/span&gt;的阀门&lt;span&gt;在这些记忆细胞之间&lt;/span&gt;，&lt;span&gt;这个&lt;/span&gt;阀门随着输入和隐层状态决定，&lt;/span&gt; &lt;span&gt;&lt;span&gt;如果是&lt;/span&gt;1&lt;span&gt;，我们让过去的记忆完全进入到当下，信息丝毫不衰减，如果阀门的值是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;，就彻底的遗忘，如果是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;1&lt;/span&gt;&lt;span&gt;就是在一个时间段里记住这个值就是一个时间尺度。只要&lt;/span&gt;控制好这个&lt;span&gt;阀门&lt;/span&gt;，&lt;span&gt;我们就得到了一个动态的可以学习的时间尺度。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5616161616161616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kibwMIicLVK1VNiclzdj2JchRw3DLa25DdPhEuuVRmEXyfqVSib7N9762qA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;495&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;刚刚讲的阀门被称为&lt;/span&gt;遗忘门，&lt;span&gt;为了配合它，我们还加上输入门和输出们，&lt;/span&gt; &lt;span&gt;控制有多少新的信息流入，有多少输出，&lt;/span&gt; 这三层门&lt;span&gt;整体构成一套&lt;/span&gt;信息的闸门，门的形式都是可微分的&lt;/span&gt;sigmoid&lt;span&gt;函数，确保可以通过训练得到最佳参数。根据这一原理，我们可以抓住本质简化&lt;/span&gt;lstm&lt;span&gt;，如&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;或极小&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;span&gt;。其实我们只需要理解这个模型就够了，而且它们甚至比&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;span&gt;更快更好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看一下最小&lt;/span&gt;GRU&lt;span&gt;的结构：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42168674698795183&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kVcbiaYKKbxIBINvz2ISUyxxTcDHga7tFPdTwAdI3jhg7HVN974FPsEg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2278719397363465&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kkKhNMtqduPrfiaLqFiaHqUncdD0kH3v9iax33BgFxU4Y4eZdDHyfTiaABg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;531&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;第一个方程&lt;/span&gt;f&lt;span&gt;即遗忘门，第二方程如果你对比先前的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;会发现它是一样的结构，只是让遗忘门&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;f&lt;/span&gt;&lt;span&gt;来控制每个神经元放多少之前信息出去（改变其它神经元的状态），第三个方程描述&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;“&lt;/span&gt;&lt;span&gt;惯性&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;”&lt;/span&gt; &lt;span&gt;，即最终每个神经元保持多少之前的值，更新多少。这个结构你理解了就理解了记忆体&lt;/span&gt;RNN&lt;span&gt;的精髓。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Attention&lt;span&gt;版&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;LSTM&lt;/span&gt;&lt;span&gt;与宋词生成&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然，我还可以把这个多时间记忆的东西玩到极致，最终我们可以得到一个升级版本的诗词处理器，&lt;/span&gt; &lt;span&gt;我们要加入几个新的概念：&lt;/span&gt; 1&lt;span&gt;， 双向编码&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;/span&gt; 2&lt;span&gt;，&lt;span&gt;多层&lt;/span&gt;lstm&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;3&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为什么双向，因为语言也可以倒过来念啊，甚至很多时候后面的内容越重要越能决定主题， 比如一句话一旦出现“但是”一定是但是后面的内容更中重要有决定性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.35340314136125656&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kTd77wRVfeOS7WzAsLiaibGGIic5BlVrbUlicVLQ6JaibBC6XdiaS8ibOSacOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;764&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么要多层， 记得我刚刚讲过的大脑结构吗？ 如果每层的lstm学习的事件尺度敏感性不同， 会更好的处理多事件尺度新消息， 比如从词语到句子到段落。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;最后，也是最关键的，看看我们如何引入&lt;/span&gt;a&lt;/span&gt;ttention.&lt;span&gt;。&lt;/span&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Google&lt;/span&gt;&lt;span&gt;这一次&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2016&lt;/span&gt;&lt;span&gt;寄出的大法，是在其中加入了&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;机制 ，&lt;span&gt;首先大家理解人脑的注意力模型，人脑的注意力机制的特点是在认知资源有限的情况下，我们只给那些最重要的任务匹配较多的认知资源。&lt;/span&gt; &lt;span&gt;这个机制实现的方法正是&lt;/span&gt;attention&lt;span&gt;。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，在引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;之前，我们的想法是既然我们最终的决策想利用好所有的历史信息，而每个时间的隐层状态都是对那个时刻时间状态的总结，我们完全可以把所有时间点的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;作为一个特征使用， 这一点， 而不只是最后的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;，这点在文章分类这类任务里特别好理解，但是每个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;动辄上百维度，所以当我们把所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;合成在一起的时候 ，我们就会得到几十万个维度，我们再次抛出降维度找重点的思维， 加入一个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;来吸取和当下预测最重要的信息。&lt;/span&gt;&lt;/span&gt; A&lt;span&gt;ttention&lt;span&gt;又称为&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;content&lt;/span&gt;&lt;/span&gt; &lt;span&gt;base&lt;/span&gt; addressing&lt;span&gt;&lt;span&gt;，因为过去哪个东西比较重要，往往取决于我现在要预测的信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.834070796460177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kr774SyBt7bMVY4qVm4sNr1olrWH3nAz6Nfc4XO4dgPGpBUYva0VR2g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有了这个机制，我们可以抛出我们唐诗生成器的改进版，宋词生成器，宋词的生成，确实是比唐诗更复杂的一个东西，因为宋词更长， 更多变，句子长短不同，也更需要多时间尺度。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8047244094488188&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kUlL5A2z6dcIxib9cCsOFiaGaibVDVrnQepcrmpvHqD2MYiaF43ia7EDDy5Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这一次，我们可以把宋词的结构看作是一种对话体，&lt;/span&gt; &lt;span&gt;我们用一个把问题编码，&lt;/span&gt; &lt;span&gt;然后直接从这个基础上预测对上一句的回答，这里，我们生成第一句词的技巧和之前是类似的，&lt;/span&gt; &lt;span&gt;从这个生成的第一句词开始，我们用编码器&lt;/span&gt;LSTM&lt;span&gt;来把这一行编码作为下一句的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;， 然后解码器把这个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;转化为下一行词。为什么我们要用&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;呢？ 因为上一句的所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;被叠加成一打，刚刚说到的，这时候会造成信息过大，所以我们引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;的机制来注意相应的信息， 这样我们就可以找到上一行和下一行之间精细的相关性。&lt;/span&gt;&lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.9306451612903226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k2zxW29QBMksaJTicu0H9yQibxviapVtlRJSVSHvTctMCyibxLkt3HxeWtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;大家可以比较和唐诗生成的时候， 我们的结构不仅更简洁，而且能够处理更难的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;模拟人类对话&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最终， 我们已经接近了让神经网络听懂故事的境界， 你是否认为类似的结构可以用作和我们对话呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kJ6ZIVojPFnRal0gAGQFFaSicQKYYTQmaJzSTJlYhTWslB3yWOKA6uFA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;476.75359712230215&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;491.23201438848923&quot; data-ratio=&quot;1.0294117647058822&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kqgbTa8IAQ8ofH5QVGuYaKLrsdrItYENGJ2hm9hKBDdxOx6bUicmKXfQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;476&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;因为你我在对话时候有大量的背景知识，&lt;/span&gt; &lt;span&gt;而机器人是没有的，&lt;/span&gt; &lt;span&gt;但是有没有补救的方法？&lt;/span&gt; &lt;span&gt;当然有，&lt;/span&gt; &lt;span&gt;谷歌的问答系统，&lt;/span&gt; &lt;span&gt;已经把很多重要的背景知识放入了神经网络，&lt;/span&gt; &lt;span&gt;这种加入很长的背景知识的结构，&lt;/span&gt; &lt;span&gt;被我们称为记忆网络，&lt;/span&gt; &lt;span&gt;事实上，它是对人类的长时间记忆的模拟。&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这些长时间记忆，&lt;/span&gt; &lt;span&gt;包含知识，或者地图等。用它做的聊天机器人，可以成为你电话里的助手，但是，&lt;/span&gt; &lt;span&gt;你是否认为这样的结构已经具备了真正理解语言的能力呢？&lt;/span&gt;   &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;参考文献&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The unreasonable effective RNN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Minimal Gated Unit for Recurrent Neural Networks&lt;/span&gt;&lt;/p&gt;







</description>
<pubDate>Fri, 08 Mar 2019 14:53:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/o5DC8CEtwb</dc:identifier>
</item>
<item>
<title>基于一张规则表的人工智能</title>
<link>http://www.jintiankansha.me/t/q56XOgLetH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/q56XOgLetH</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;4ds8l-0-0&quot;&gt;我在过去对人工智能简史的描述中，把人工智能的整个历史描述成围绕一张规则表， 本文是基于这一想法的总结和扩展。我们说， 早期的AI发展史围绕如何人为构建这样一个规则表解决复杂问题， 而当下的AI则围绕如何让它在复杂的现象中自己归纳出这个规则表&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bkglj-0-0&quot;&gt;我们说， 上帝通过制定规则从简单演绎出复杂。最初的原始人类在黑暗中摸索， 在众多的现象不知所措， 只能通过设立各色大神小神来缓解自己对不确定性的恐惧。 从多神宗教到一神宗教的跨度体现了一个从复杂中寻找简单的跨度， 这可能是基于一种隐隐的直觉，就是现象虽然多样， 但是背后的法则不应该如此复杂。 到了科学的时代， 这种思维在物理学里淋漓尽致起来，四大力学， 把分子原子间的作用力统一到电磁力， 把宏观物体的作用统一到引力和经典的动力方程， 已经是极致， 而后面的对这两者的统一构成了从相对论到杨米尔场的现代物理主线。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibembXL81evOfRdy0cTz2qqulnss6iaFGmXLmdjuRu9GiayK86XIM1TVSg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图: 简单规则生成复杂的极好例子， 元胞自动机， 每一步细胞的繁殖和阔算方法一定， 它最后形成的图案就定了， 规则可以很简单， 图案可以很复杂。 &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1j7g-0-0&quot;&gt;另一方面， 这样的思想从智能科学诞生之出，也贯穿出来。 它在早期的可计算性中， 通过图灵机的构建。它认为存在这样的通用机器，能够和人类一样解决问题， 即使过程非常复杂， 你无非需要四个要素： 1， 输入 2， 中间状态 3， 规则表 4， 输出 并在时间上进行大量迭代， 就可以实现这个过程。 通过这个过程， 我们可以把一个输入转化为一个想要的输出。 如果我们能够在在有限的步骤里将一个输入转化为一个输出，据此解决一个实际问题， 那么这个问题就是可计算的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6114081996434938&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBnC8zkibScsiccAOwoSH4Ld9vdiamljakNVOX5TKF69ywm3Qq2ic3IdhUw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;561&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c00nl-0-0&quot;&gt;而冯诺依架构让它变成一个技术现实。 它通过可以存储程序的机器， 让人们通过把这些图灵规则表的指令变为计算机二极管的开合代码， 而让图灵纸袋的思想成为了一个每个工程师可以设计的现实。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;一个计算机程序， 最基本的部分包括一些简单的形式逻辑， 包括逻辑与或非， if else， for 循环这些。 其实本质上， if else 所描述的就是规则表， 规则里面通常涉及简单的逻辑， 最终通过for循环， 我们就可以得到我们要的东西。 比如一个中学生都会的排序算法， 我们无法需要做的是前面和后面的数比较大小， 然后一个if else进行换位， 最后一个for循环， 多长的序列都可以瞬间搞定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;176de-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.40556900726392253&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibibEBCIwsAHiaGwDkFcYUeGLqHsuPP0ibBA7ibPX7OxPPq87Jr0NAtFjSibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;826&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b1ct2-0-0&quot;&gt;这就是用程序解决问题的核心思维， 给你一个再复杂手忙脚乱的问题， 只要这个问题可计算， 那么我们只需要设定好我们需要的规则表， 在有限的步骤里迭代， 最终机器总会给你解决。&lt;/span&gt;比如魔方问题， 一般的聪明小孩都很难在短时间解决问题， 但是， 事实上解决魔方问题有一套非常整齐的规则表（你想象打乱一个魔方其实比较容易的， 把它弄整齐是打乱的逆运算，但是破镜重圆总是难的）。 如果按照这个规则表执行若干步， 再困难的魔方也给你整出来。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibz3zliaRoh4oEWf9QQOX3VMpbMFylumcHY7novvRnic1j7aN7R7pohRHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;676&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44333333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibBqmPPsMKMLBUUyKWYnVsiaibg6hmZP9cADL9iak5yxuibvmosIG5ZUWbuw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e13hv-0-0&quot;&gt;我们说规则表， 加上迭代等操作的思路可以解决大量的工程问题。我们曾经认为按照这样的思路我们可以解决整个智能的问题。 只是填入一张越来越大的表格。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5g8h1-0-0&quot;&gt;但是它在通向智能的关键位置， 却停住了， 这个元凶 -就是- &lt;strong&gt;不确定性&lt;/strong&gt;。日常生活中很多东西无法轻易的总结出规则表来， 因为细小的规则实在太多了。 你可以想象我们有无数尺寸和规格各不相同的螺钉螺母。 每一种规格我们都要想一条if else，可悲的是这些螺钉和螺母几乎没有哪两对完全相同， 穷尽一个程序员一生也写不完这些程序。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9mrot-0-0&quot;&gt;统计机器学习 - 机器判断规则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;6pfmt-0-0&quot;&gt;这个问题的解决方法十分自然又十分了不起：  能不能让机器自己学会这个表格， 而不是认为设定它呢？  这就是整个智能问题的第二步 - 学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eeh10-0-0&quot;&gt;整个学习问题的基石其实是古希腊人提出的归纳法和演绎法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;17gbk-0-0&quot;&gt;伟大的希腊哲学家早就对学习的本质展开过探讨，它们把学习分类为&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-1&quot;&gt;归纳法和演绎法&lt;/span&gt;&lt;span data-offset-key=&quot;17gbk-0-2&quot;&gt;。所谓演绎法， 就是从用一定规则进行推理的过程。 苏格拉底是人，人都是会死的， 因此苏格拉底会死。 这就是三段论， 或者称为演绎法的根基。 而真正学习的过程，是这个演绎法的逆过程。 我们先知道一个特例， 然后通过特例，得到这个“人都是会死的” 知识， 再指导自己的行动。 学习是知识在脑子或者机器里面形成的过程， 怎么形成？ 这个过程被称为归纳法，也就是根据搜集到的特例比如苏格拉底死了这个事情，来归纳更一般的知识。归纳法， 我们来看我们需要提供给机器怎样的佐料来解决这个问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6nhn2-0-0&quot;&gt;我们想象这样一台机器， 这个机器和之前说的规则机器类似， 唯一的区别是， 我们把大量的假设放在那里，让机器来连线。 我们要让它学习一个知识， 比如-什么人是否会死的。我们把人按照几个特征进行分类， 一个特征对应一个问题， 比如是否是哲学家， 是男还是女， 是白种人还是黄种人。 这些特征， 都对应会死或不会死这两个结论。 这样，你会得到多少个假设呢？ 组合数学告诉我们16种， 于是学习的任务就是给这16个假设和真或者假连接起来。 一旦一条线连起来， 我们就得到了一个新的知识，可以被用于在真实的世界做判断！ 就和之前说的规则机器一样。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1bdeq-0-0&quot;&gt;我们首先给这个机器灌入所有的可能性， 那16种假设。 然后我们让机器来收集案例！ 比如机器收集到一个苏格拉底死了， 那么苏格拉蒂是什么？ 男性，白种人， 哲学家， 于是机器得到男性， 白种人， 哲学家，会死。 于是机器给机器输入亚里士多德， 柏拉图， 大卫休谟，机器都会告诉你会死。然后我们继续收集样例， 比如居里夫人死了， 然后机器会得到女性，白种人， 非哲学家，死了。 这样它能够做的判断就又多了很多！ 这样的思维范式，就是归纳法，由于我们列举的假设依然用到了人类已有的知识， 因此我们得到的这个机器，事实上是最接近规则机器的一台学习机， 我们可以称之为规则为主体的归纳法。我们直接把规则转化为了可以学习的对象。输入样例，得到一个是非的知识， 这个样例我们换个词叫数据， 这个机器我们换个词– 叫做分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nnub-0-0&quot;&gt;整个有关统计的机器学习， 都可以看成让机器学习有效归纳的方法， 从数据里得到规则表， 再用规则表进行判断。前面的过程叫训练， 后面的过程叫测试。  如果这些规则是有关一个是非的命题， 它就是一个分类器， 如果它是一个连续数值的预测， 就是回归。 但是规则表的本质是不变的， 它就是让你填表，表格的横排和竖排已经有了， 一个叫特征， 一个叫实例。  特征是人为归纳好的， 而实例是我们人为收集的， 表格中有些地方是空的，  就是我们想要判断的东西， 需要机器来填的部分。 比如给你一百幸福和不幸的人的案例， 让你判断第101个人的情况。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;81s85-0-0&quot;&gt;刚刚的那个例子你应该已经体会到， 这个命题验证过程其实是一个组合爆炸的问题。我们把关于这个世界的互相矛盾的假设都丢尽机器。即使最简单的问题也会有无穷多的情况要判断 （特征的n次方）这种假设的数量随着问题的复杂度急速指数上升的过程，我们称之为维度灾难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib6XNRA36VEFMtlPlZQ2RpPXtla5u66LIcDmhkLUQuPecqg0IVicuibIwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ajopm-0-0&quot;&gt;而机器学习的各个算法， 让我们通过加入更多的假设， 来偷懒解决这个问题， 此处没有比决策树更典型的， 它的高阶版本xgboost成为机器学习竞赛的杀手锏。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9isr-0-0&quot;&gt;而决策树得核心智慧就是优先级算法简化命题数量。 虽然特征很多， 但是并不是每个特征都一样重要， 我们如果先按照最重要得特征进行判断， 依此往下， 你可能不需要2得N次方个情况， 而是按照树结构做N次判定即可。 优先级， 也是人类智慧得核心，事实上， &lt;strong&gt;我们永远在抓轻重缓急，在抓主要矛盾，&lt;/strong&gt; 无论是有意的还是无意的，当然大部分人的轻重缓急是按照时间来的，时间比较近的就是比较重要的， 这也是为什么很多人有拖延症。 很多人说到优先级算法很想到相亲， 其实这也是一种人类思维自然使用的决策树， 比如女生找男朋友通常心理都有一个优先级构成的树， 首先， 对方的年龄多大？ 如果对方年龄大于50岁直接pass， 然后看工资，如果工资小于20万直接pass，工资在20和30万间看下学历， 学历小于本科直接pass。 这其实就是一个决策树的结构。 每次pass， 就减少掉了一半需要判定的命题。 通过这种预设的二叉树逻辑， 一个本来需要2的n次方的步骤解决的事情， 可能只要n步了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7453703703703703&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibq5oBakMtQKgHb2Akq4jNYicdNNniarAhbtJNpWaJ7qOkbJDDKpTTB9MQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8dl0e-0-0&quot;&gt;具体如何来学，树的根部是你选择的第一个特征， 更好的角度是把特征看成一个问题，树的根部是你要问的第一个问题， 根据这个问题的回答， 数据会在左边右边分成两组。 然后在每个答案的基础上， 你继续问下一个问题， 所谓的决策树的分叉， 每个枝杈就是一个新的问题。 如此，就会形成一个树的结构。构建这个树的主要难点， 在于要由机器决定哪个问题先问， 哪个问题后问， 如何选择这个优先顺序？我的要求就是， 每一次分化，我们都希望取得最多的信息，如分叉后一个树杈全是yes，一个全是no就是最好的效果， 如果达不到， 也让它尽可能接近这个效果。  这样一个一个问题问下去， 最终达到稳定后过程停止。  这样形成的决策树， 我们会形成任何一个情况下的优先级。 或许长的帅的人工资不重要。 或许学历高的人年龄不重要。 这种不同情况不停调整优先级的思维， 真的是被决策树利用到了极致！ 从原始数据里提炼的决策树， 可以对无限的新情况进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk0ce-0-0&quot;&gt;另一个得到这样的一个规则表的方法是线性假设。 线性分类器通过假定特征之间的相互独立， 使得命题的成立与否可以通过一个加权求和的关系表达， f=wx+b 。最后f如果大于0就是是， 小于0就是否。 线性分类器也是一种特别符合人认知习惯的模型：一般人在决策时候做的事情就是加权平均，比如你平时做分类（决策）， 你最想的一种状态是什么？你要把几个核心的要素放到一起， 按照他们的重要性加和，比如你今天要不要去看电影，可能取决于你的女朋友free否， 下不下雨和电影好不好看， 这个时候，我们可以把这些因素加权在一起， 在和一个我们给定的阈值做比较，大就去， 不大就不去， 这正权衡得失的做法， 就是线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0m7b-0-0&quot;&gt;具体学习的过程， 我们从实例里归纳出每个特征对应的权重参数，然后进行判断。 只要参数都确定了， 也就是一次解决了所有的问题。 线性分类器的高级版本SVM已经超越了线性假设。 也是小数据下生成有效规则的大杀器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8te9f-0-0&quot;&gt;连接主义机器学习， 产生规则&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fk6j-0-0&quot;&gt;刚刚说的那一套， 有一个问题你有没有注意到？ 我们最先提出的问题是让机器产生一个规则表， 而刚刚说的统计机器学习里， 更多的是让机器根据特征进行命题判断。 这其实是只进行了0.5步。 大家想象以下， 在真正的实践活动里， 你无法一开始就设定出一堆特征让它进行逻辑判断，在这个情况下如何得到我们所说的“规则”呢？ 如何让机器自己生成战胜“复杂”的程序呢？ 连接主义机器学习在一定程度解决了这个问题。  因为， 人类认识事物，生成规则， 其实是通过“概念”来的， &lt;strong&gt;“概念”是一个浓缩的信息载体， 通过它我们能够进行任何更复杂的推理。&lt;/strong&gt; 那么“概念”是如何生成的呢？ 它的载体正是下面说的联结主义的代言人神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5gsn-0-0&quot;&gt;神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dijog-0-0&quot;&gt;首先，神经网络是由神经细胞组成的。  一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习，它就是刚刚说的线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cr2lq-0-0&quot;&gt;正确的分类，是认知的基础，我们对事物的感知比如色彩， 物体的形状等，其实都是离散的， 而物理信号是连续的， 比如光波， 声波。这里面的中间步骤就是模数转化， 把连续的信号转化成离散的样子， 这正是一个分类器干的事情。  一个单个神经元可以执行一个简单的基于感知信号的if else语句。 先收集一下特征做个加和， if大于一个值我就放电， 小于我就不放电，就这么简单。 晶体管当然也在干这个事情。 &lt;strong&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。&lt;/strong&gt;或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibib2NvE4HCXevrTz3VAnXzfIxaOZibibe1YnYGN84NF1XWqyKpNdsISR2aA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3f6fb-0-0&quot;&gt;我们这个简化出来的模型，　正是所有人工神经网络的祖母　－　感知机。　从名字可以看出，&lt;/span&gt;&lt;span data-offset-key=&quot;3f6fb-0-1&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;3f6fb-0-2&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 R 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;单个的感知机并不能比传统的机器学习多做一丁点的事情， 还要差一些。 但是把很多个感知机比较聪明的联系起来，就发生了一个质变。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5ui0b-0-0&quot;&gt;首先， 每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有那个头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  其实这里面做的事情， 正是基础的逻辑运算， 一个简单的一层神经网络可以执行与或非这些基本的逻辑操作。事实上它的本质就是把简单的特征组合在一起形成一些原始的概念。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;它是怎么做到的呢？ 学习。  生物神经网络的学习， 是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是， 我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了一个简单的“概念”。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4ud6e-0-0&quot;&gt;上述过程被总结H&lt;/span&gt;ebian学习的一个过程。  我们可想象，一个两层以上的神经网络， 就可以表述香蕉， 苹果， 菠萝这些水果了， 它们无非是底层特征颜色，形状的不同组合而已。 而这些不同水果的概念， 就可以帮助我们形成更加复杂的规则表 ，比如让它根据客户的信息帮它推荐一个水果拼盘。 由此可见， 神经网络通过与或非进行简单特征的组合 ，再通过if esle进行判断选择合适的特征得到概念， 再通过下一层迭代得到概念有关的命题。 就可以生成比之前的传统机器学习复杂的多的规则表。而且我们可以想象出来， 迭代的层数越多，它生成的“概念”和“规则”就越复杂。  当然真实训练中我们用到的不是模仿生物版本的Hebian学习， 而是强大的多的反向传播算法。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;48v3r-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/jrbyyXzrKkJqzpvQ60VcjgiacFu21XHHubic1vJveCSZ6PHEDDyJd1LZhn3z6ibqmBehPbx0icZx0ZXosoBaXWceQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.6111111111111112&quot; data-w=&quot;360&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dmb6s-0-0&quot;&gt;事实上为了让这种生成“概念”得到“规则”的方法更加有效， 我们会加入一些无比强大的先验假设。 其中最有名的一组，  就叫CNN，它所做的，其实是对于图像这类巨大无比， 而局部特征不断重复的信息形式， 其实你可以写一个循环， 来让你的程序更有效。 循环里的模块每一步是可以共用的， 也就是卷积核。 卷积核一点点的卷个图像上的每个小块， 也就是循环的总体。 卷积核在每个图像局部做的， 事实上都是一个小的if esle 语句。 if像素之间符合某个关系，就是yes，否则No。这个结果， 最后被综合出来， 给下一层合成更复杂的图像特征。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7298850574712644&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibJ7GaCXDzw2dMW8QiaAPh96svf1lqjLico2MoDlVUCfFZgIHkysrSaRibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4vk9r-0-0&quot;&gt;好了， 到目前为止， 说的都是和时间无关的规则。 而一开始讲到的真实的图灵机， 是和时间有关的规则。 那么如何得到一个和时间有关的规则表呢？ 如果要处理和时间相关的信息， 你必须要引入记忆， 引入内部状态， 而和刚刚说的一样， 这些含时间的规则要是可以学习的， 用数学的语言说， 就是要有一个连续可微的载体， 这个东西就是RNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;72qm9-0-0&quot;&gt;def step&lt;/span&gt;&lt;span data-offset-key=&quot;72qm9-0-1&quot;&gt;(self, x):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;di032-0-0&quot;&gt;# update the hidden state&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4f218-0-0&quot;&gt;self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;243tl-0-0&quot;&gt;# compute the output vector&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;591p4-0-0&quot;&gt;y = np.dot(self.W_hy, self.h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;a1b57-0-0&quot;&gt;return&lt;/span&gt; &lt;span data-offset-key=&quot;a1b57-0-1&quot;&gt;y&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;g9on-0-0&quot;&gt;以上是RNN的python程序定义。 它说的无非是你有一个刚刚说的线性分类器组成的单隐层神经网络， 但是这一回，神经网络的输出， 要作为输入，重新回到神经网络的隐层里， 这个关键的增加， 就使得它具有了处理复杂时间信息的能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5qh71-0-0&quot;&gt;这个结构，非但优雅，而且有效。一个非常重要的点是， 你知道信息的传播是有损耗的， 如果把RNN展开， 它事实上相当于一个和历史长度一样长的深度网络， 信息随着每个时间步骤往深度传播， 这个传播的信息是有损耗的， 到一定程度我就记不住之前的信息了， 当然如果你的学习学的足够好， Wij还是可以学到应该学的记忆长度。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.26161790017211706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccd3W2SkazpguaDUmf1EWibibPf1BYBmsfuHXyspOv0uwulVbuy0UibqOib2grXtp4XpvWU4b6Pj3402Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  事实上叫做“循环神经网络”  循环的本质是什么呢？    它其实正是你的程序里的for循环啊！ RNN的本质是， 在每个时间步里进行同样的操作， 这个操作无非是， 当下的输入， 和神经网络的状态两部分特征的逻辑组合（与或非）然后， 这个组合的结构进行一个if else的逻辑判断， yes or no， 根据这个，生成一个输出的结果， 这个结果， 要回传给神经网络隐层， 生成下一个隐层状态。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2ot4h-0-0&quot;&gt;大家看这其实就是图灵机的定义啊， 而RNN的本质， 就是一个可以通过微分方法学习的图灵机啊。 虽然每个步骤的规则和执行足够简单， 但是只要步数足够多， 却可以产生非常复杂的结果。  &lt;strong&gt;RNN学习的本质， 就是给你那个足够复杂的结果， 让你反演出那个足够简单的规则， 然后让它在新的环境下再去做预测与决策。&lt;/strong&gt; 我们可以看到， 这已经非常接近智能的本质了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5j932-0-0&quot;&gt;有关物理的世界和智能的世界&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;di5qc-0-0&quot;&gt;上面的这些思考无疑开始让人们想象我们所说的包含了逻辑推理， 情感，甚至意识的问题与物理世界的关系到底是什么。 我们说物理的世界里， 主宰一切的是微分方程。 一切因果关系， 都由微分方程所承载。 你有了不同不同微观粒子电磁力的描述，把它们放入薛定谔和狄拉克方程， 你就可以推出原子的不同性质。  这其实可以说是因果推理的极致了。 它甚至导致了机械的宿命论思想。当一切初始的原因输入系统， 那么它就回归于一个必然的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dl784-0-0&quot;&gt;到了非线性动力学的时代看似这点被混沌打破了，亚马逊的蝴蝶引起北美的飓风， 让通俗科学爱好折重新燃起了不可知论的希望，事实上并没有。  所谓的混沌， 无非是一种确定性下的不确定， 或者已知中的未知。 混沌的系统依然在一个被方程高度确定的洛伦兹吸引子里。&lt;/span&gt;到此处， 我认为微分方程依然是描述因果关系最精密的所在， 它可以在输入很少信息的时候， 得到最多的预测产出。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8i7lg-0-0&quot;&gt;在看刚刚的智能问题， 我们说， 整个智能问题， 到目前为止其实还是在围绕那张规则表， 只是我们的思路由制定规则表， 到了学习规则表。和物理比较， 目前的机器， 需要输入进去大量的数据， 才能生成一点十分简单的规则。 当然你可以举阿法狗下围棋的例子说明所生成的规则并没有那么简单， 可惜的是， 那些规则只适用于一些非常封闭而特定的领域。 而不像牛顿定律放之四海而皆准。  那么神经网络可不可以观测大量物体坠落的过程把万有引力定律给推出来呢？ 目前看是不能的。其实牛顿引力定律的得出是含有了大量的人类推理。 我们需要先知道物体运动改变和受力的关系， 然后通过观测物体的轨迹得到大量物体的受力情况，再在这些手里情况下得出某一种共同的作用力形式， 这是一个多么复杂的思维链条。 这对于目前统计的巨人， 而只懂得浅显的形式逻辑的神经网络，还是比较困难的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dsh1-0-0&quot;&gt;有关语言&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt;讨论智能的问题离不开语言。从乔姆斯基开始， 人们就开始研究不同语言背后的共同语法基础。 其实如果深究语言问题， 我们会看到它和刚刚说的程序的联系。 &lt;strong&gt;语言无非是对世界的符号化，类似于给每个刚刚说的概念赋予一个符号。&lt;/strong&gt;而语言其实很像程序， 它就是对概念之间关系的表述。  我想语言和程序的区别可能在于语言更加模糊， 但是它对付不确定性的能力远远大于程序，因为这种模糊性， 让它具备了更好的适应能力， 可以表述那些用程序难以描述的事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8cbc-0-0&quot;&gt; 但是本质上， 语言无非是一个现有概念的符号体系， 描绘概念和概念间的关系。这样看以往的深度学习NLP其实是走了一条南辕北辙的路， 我们把不同的词汇和句子压缩成词向量， 句向量喂给神经网络学习， 而事实上神经网络对这些符号背后的实体概念却一无所知。虽然词向量也能稍微的带有一点不同词语之间的语义距离， 但是这和真实世界所含有的信息量，也依然是差异巨大。 目前用图卷积网络解决NLP的思路，算是一个进步， 因为它更好的涵盖了整个符号世界的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;语言， 好比一个巨大的人类经验和逻辑的宝库， 这个符号世界几乎就是真实世界的极好压缩体， 如果一天神经网络真正被赋予了语言的power，也就是能够真正理解这个符号世界， 或许离通用人工智能也就不远了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;有关物理世界和语言世界的打通&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;刚刚说的精确的物理方程的世界， 和能够应付更多不确定性的模糊的语言， 之间又有哪些联系呢？  我的想法是， 物理的杀手锏微分方程， 当构成了一个非线性的动力学系统， 却可以通过它内在的定点， 极限环，吸引子等概念， 去接近那个模糊性的语言， 就好比在非线性动力学的世界里， 我们往往不再那么关于一个系统如何发展的暂态，很多不同的系统都归一于一个吸引子， 那么它们背后的逻辑可能就是类似的。 这或许会架起一座物理世界和语义世界的桥梁？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blfe1-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383991&amp;amp;idx=1&amp;amp;sn=26f543505499441e7f31cfb15177ff10&amp;amp;chksm=84f3c6f6b3844fe08f91bfec42c55b42d221f452c68d3820eb1612a6c09f39c06956d69f42ca&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;当神经网络遇到神经科学-铁哥18年长文汇总&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 18:31:05 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/q56XOgLetH</dc:identifier>
</item>
<item>
<title>东边日出西边雨：极端天气网络中的遥相关与超指数分布</title>
<link>http://www.jintiankansha.me/t/QkOJZ4teYL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/QkOJZ4teYL</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6661764705882353&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;680&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMG4rtOm0rxStSHkUcm6CUnLa7flsMwAILX9iaQqSUxSSicQRjEaicIKkPw/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1258535&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;&amp;#x6536;&amp;#x85CF;&quot;&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1172402&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section readability=&quot;2.5&quot;&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;2月份的Nature主刊上的一篇论文，着眼于气象科学与复杂网络结合擦出的火花。这篇小文将带你以看侦探小说的视角，批判性地解读这篇论文，让你明白这篇文章在材料的呈现上有哪些值得借鉴之处。在文末，作者将结合该文，谈谈接下来可能的研究方向。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;论文题目：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Complex networks reveal global pattern of extreme-rainfall teleconnections&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;论文地址：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.nature.com/articles/s41586-018-0872-x&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;复杂网络的应用，这些年间越来越广泛，但总有一些共通的规律，最典型的就是&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247490194&amp;amp;idx=1&amp;amp;sn=a85d1f0312d455ec7ef59c6b26c7a581&amp;amp;chksm=e894401fdfe3c9095db99fb55f9ecbd9962dbe50092d69b9fb944168cd378d930bb2fb1aacd6&amp;amp;scene=21#wechat_redirect&quot; data-linktype=&quot;2&quot;&gt;不同尺度下网络呈现相同性质的幂律法则&lt;/a&gt;。不过，凡事总有例外，&lt;span&gt;这篇Nature论文的研究成果&lt;/span&gt;能够打破幂律法则的现象，必然是颠覆式的发现。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;论文摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;72469&quot;&gt;&lt;section data-width=&quot;100%&quot;&gt;&lt;section readability=&quot;23&quot;&gt;&lt;section data-autoskip=&quot;1&quot; class=&quot;&quot; data-style=&quot;clear: none; margin-top: 0px; margin-bottom: 0px; line-height: 1.5em; text-align: right; color: inherit; border-color: rgb(239, 112, 96);&quot; readability=&quot;46&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;Climatic observables are often correlated across long spatial distances, and extreme events, such as heatwaves or floods, are typically assumed to be related to such&lt;/span&gt;&lt;span&gt;teleconnections&lt;/span&gt;&lt;span&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Revealing atmospheric teleconnection patterns and understanding their underlying mechanisms is of great importance for weather forecasting in general and extreme-event prediction in particular, especially considering that the characteristics of extreme events have been suggested to change under ongoing anthropogenic climate change. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Here we reveal the global coupling pattern of extreme-rainfall events by applying complex-network methodology to high-resolution satellite data and introducing a technique that corrects for multiple-comparison bias in functional networks. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We find that the distance distribution of significant connections (P &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;For longer distances, the probability of significant connections is much higher than expected from the scaling of the power law. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We attribute the shorter, power-law-distributed connections to regional weather systems. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;The longer, super-power-law-distributed connections form a global rainfall teleconnection pattern that is probably controlled by upper-level Rossby waves. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We show that extreme-rainfall events in the monsoon systems of south-central Asia, east Asia and Africa are significantly synchronized. Moreover, we uncover concise links between south-central Asia and the European and North American extratropics, as well as the Southern Hemisphere extratropics. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Analysis of the atmospheric conditions that lead to these teleconnections confirms Rossby waves as the physical mechanism underlying these global teleconnection patterns and emphasizes their crucial role in dynamical tropical–extratropical couplings. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Our results provide insights into the function of Rossby waves in creating stable, global-scale dependencies of extreme-rainfall events, and into the potential predictability of associated natural hazards.&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;可上下滑动查看论文摘要&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;优质的摘要，第一句应该是背景介绍，目的是让读者知道这篇文章讲述的是哪个领域的发现。既然是背景，就要和日常生活有所联系，让读者能联想到&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247495932&amp;amp;idx=1&amp;amp;sn=ee1f07d4a50c6a75d3387d0c70e20e90&amp;amp;chksm=e897aa71dfe02367ffc43be660fe13378e85a14dfcce747daf01cf99c4767e4679af54feebbb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;最近的新闻报道&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;本文的第一句指出气候现象，尤其是极端情况，例如高温，暴雨等在长距离上存在关联。近期美国的寒潮，跨越数个州，这还只算是区域性的气候关联，而欧洲、亚洲与美洲同时出现的极端寒冷，就属于文中提到的“&lt;strong&gt;遥相关&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（teleconnection）&lt;/span&gt;&lt;span&gt;”了。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;说完了背景，要论述该现象的意义。假设弄清楚了气候中的遥相关，能解决什么现实问题？能带来哪些新的问题了？即如何扩展人类的认知边界。这是论述研究意义时要回答的模板问题。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;摘要第二句指出，对于极端天气的预测，如果能结合遥相关，那会更加精确。&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247495932&amp;amp;idx=1&amp;amp;sn=ee1f07d4a50c6a75d3387d0c70e20e90&amp;amp;chksm=e897aa71dfe02367ffc43be660fe13378e85a14dfcce747daf01cf99c4767e4679af54feebbb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;而极端气候现象的频繁发生&lt;span&gt;，很大的原因是人类活动的影响&lt;/span&gt;&lt;/a&gt;&lt;span&gt;，如果对极端气候现象的遥相关没有精准的描述，也就无从谈起人类的影响会不会带来改变。电影”后天“中描述的极端寒冷，在19年初出现在了新闻报道中，这说明极端气候离我们的生活并不遥远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.563&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1000&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMxIrnWOKibHTrHwViamS46mecjd9df3eBmkXuag3FVKpSLa6G9mKUDNIQ/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;电影《后天》剧照&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;核心解读&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来概要总结文章的核心发现。一般是先介绍一个通用的结论，再详细的说一个经过详细研究的案例，最后再次用更具体的例子，指出研究的意义。但在此之前，需要先将研究的问题细化。&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;研究对象选择&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;本文从极端气候中选择了暴雨。我猜想，相比于酷暑或者极端严寒，除了数据收集更容易，更准确客观，暴雨的评判标准在不同的时间更具有一致性，不会受到全球变暖与城市热岛效应的影响。因此不需要根据时间进行校正。&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;暴雨是一个仁者见仁的描述，科学上要研究，需要划一条线，来定义清楚何为暴雨。而在不同的区域，不同的年份，人们对暴雨的认知也有所不同。该研究中的设定是在一个地区所有下雨的日子中，如果这一天的降雨量超过了95%，那么这一天就算做是暴雨。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;你也许会问这里95这个数字是怎么来的？我猜这篇论文的审稿人也问了这个问题，而为了应对审稿人的”刁难“，这篇论文的补充材料中将95%这个数字换成了90%-99.9%之间的一组数字，论证了不管你怎么定义暴雨这个事件，文章中的发现都差不多，这说明了该现象的鲁棒性。&lt;/span&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;研究数据与成果鲁棒性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来要看研究用到的数据，降雨量的评估如果是基于地面气象站的数据，这些手工记录的数据，不同的国家，不同的时段，需要规整，还会带着不可避免的缺失和误差，例如有些不靠谱地方出来的数据，也难以精准的记录降雨量。而该研究用的是高分辨率气象卫星TRMM&lt;span&gt;（ Tropical Rainfall Measurement Mission）&lt;/span&gt;提供的全球&lt;span&gt;（北纬50度到南维50度之间）&lt;/span&gt;1998-2016年间的降雨量数据。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;下图展示了在不同地方6-8月间，从57.6万次降雨数据中统计出的超过95%的降雨天对应的降雨量（图A），以及在这段时间里出现暴雨的天数（图B）。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.589010989010989&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;910&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMj3WRl6JjY8NVFRqIN0wyO6ExSlyG3kpVTxQYKFOu9Nda4kWHKkl3Yg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p mpa-paragraph-type=&quot;body&quot;&gt;可以看出处在雨季的南亚次大陆及东南亚，暴雨的天数高达40-60天，而在非洲中部的草原，这段时间也处在雨季。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;除了使用来自TRMM的数据，该研究还使用了Global Precipitation Climatology Project的低精度&lt;span&gt;卫星数据&lt;/span&gt;&lt;span&gt;（每一个经度和一个纬度算一个格子，而不是0.25个经度和纬度算一个格子），&lt;/span&gt;重现了其发现的规律，从而证明了该发现的鲁棒性。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;验证普适性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;而为了说明该发现具有普适性，需要看看在不同的季节，不同的区域之间，研究发现的规律是否都成立，而本文也做了这方面的研究。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;同时在方法学上，由于存在这么多的暴雨事件，而要确定两次暴雨之间是否有同步，需要逐个两两比较，这其中就可能将一些由于随机巧合错当成相关性同步。存在多重检验时，需要进行统计学上的校正，而不止是降低统计显著的P值，这是一个通用的注意点，所有涉及到P值的研究，都要看是否需要对多重检验进行校正。而在本文中，也针对该问题提出了相应的校正方法。&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;超幂律分布&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来展示这篇文章中最重要的一张图，该图说明了极端气候的遥相关不符合幂律分布。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5429184549356223&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;466&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMTw1chibnS9Aszicf0JX4gpBOE0vdh4YFicibvAOraxCFkqTrReqkro0WqQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  图中将暴雨在不同地区的时间序列的数据两两比较后，按P值小于0.001为显著，统计在不同的距离之间，暴雨发生有关系的的概率，图中的纵轴是概率分布函数。在100公里到2500公里之间，在经过了log处理的坐标轴上，概率分布于距离几乎完美的对应到了一条斜率接近-1的直线上&lt;span&gt;（图中左边的虚线）&lt;/span&gt;，按照这个规律，距离越远，暴雨事件同步发生的概率越低。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;但超过2500公里之后，幂律法则不起作用了，极端气候的相关性的概率随距离增加变高了，直到一万公里，暴雨的同步效应才再次随着时间降低。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;下图是南北半球，冬季和夏季，热带与亚热带分开统计的同步概率与距离的趋势,可以看到超幂律分布都出现了.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMnFwG0MicrTicf7gg1d5sD405Peqib5ZyicyTFOH8J74BamPz7NRdT1lPhQ/640?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;696&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;499.46762589928056&quot; data-ratio=&quot;0.7169540229885057&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;696&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMWdEOYgZP2cSfxgYJ0Pqfe7f3IvKEJQ6cnhurQwY6O06WLibE3GYVXbA/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMqIyHnaRWCo7Jibh2jTObEyL9dsiaT5iaGnt5bLSicMrXWsYOEZpTojicGKg/640?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;664&quot; data-cropy1=&quot;10.748201438848922&quot; data-cropy2=&quot;494.4172661870504&quot; data-ratio=&quot;0.7304216867469879&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;664&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMtM7BSXw5EzFlPME7tnqE1BYI8TcOIZXPiaa53UJkOe8kOBsAyTcqKpg/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;2500公里是什么概念，北京到深圳的直线距离是1950公里，也就意味着深圳的暴雨出现的先后和沈阳的同步程度不如深圳和漠河的暴雨，这个发现是不是像蝴蝶效应一样，有足够的有颠覆性？&lt;/p&gt;
&lt;section data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;
&lt;section data-mid=&quot;t4&quot; readability=&quot;1&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span&gt;气象领域的”Dragon-Kings“&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;在过去的研究中，这类现象被称为”Dragon-Kings“，是黑天鹅现象的一种，用来说明为何预测会失败。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;该词来自于Didier Sornette在09年的论文，文中指出了&lt;strong&gt;&lt;span&gt;小处的规律不适用于大处&lt;/span&gt;&lt;/strong&gt;，例如在城市，金融市场，地震等，而背后的机制和沙堆临界原理及混沌现象中的相变有关，而该研究展示了在气象领域的”Dragon-Kings“。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3680870353581142&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1103&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMgFjlGl6xOacW5j2vMatsic4ibY7tOVK6SUFVaXLHW0jKPs2xjQ6VJicxg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;论文地址：&lt;/span&gt;&lt;/p&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;https://arxiv.org/abs/0907.4290&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;按照Dragon-Kings的理论，可以使用Kernel density预测遥相关出现的概率，而这正是上图右边（超过2500公里后）的实心线，而这与真实数据拟合的也很不错。虽然无法直接用混沌理论解释为何在气候中会出现超幂律分布（需要结合具体场景，例如全球大气的气流循环），但能够和之前的研究有所关联，能够支持并扩展前人的概率，也是好的论文写作必须的。&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section data-mid=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;案例分析：印度中部的暴雨分布&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来要对一个案例进行系统性的分析，文中选取了印度中部（南亚及中亚），下图展示了该点的暴雨和全球其他位置的暴雨同步关系的强弱，线越明显，同步的概率越大（图A），在图B，阴影区的深浅标识了这些区域与印度中部的同步概率超过了几个标准差。可以看出其中有很多条超过了2500公里的关联，而且这些关联很多是显著的（超过3-4个标准差，随机出现的概率只有不到0.1%)&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8107302533532041&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;671&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMLIPHGgIKtPj3fMLJquP7WiadcOeE4r2PmAEnVPOcVUbsp6SqduypOTg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;那遥相关之间有没有因果关系了，如果总是一个地方先出现大暴雨，接着是一下个地方，那可以说是两者之间有因果性。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;下图展示的是印度中部和欧洲降雨的相关性，黑线代表是欧洲的暴雨是否会引领印度出现暴雨，在正的4-5天上，曲线有一个峰，而在负的8-9天上，也有一个峰，这说明相关性在这个案例上是双向的，不存在单边的因果关系。毕竟地球是一个球。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2710382513661202&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;915&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMYsicgElb2jLRcPwQHpdcicJR6sCRntnZAxsgibiaic0HOXXTjaia58l9yRJw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;最后再看看该如何解释这一现象，下图bc展示了第0天和第3天欧洲和南亚及中亚（SCA）的降雨情况，可以看到先是欧洲暴雨，3天后到了SCA地区，而d和e图展示了风向，将两张图结合起来，就可以看出是由于风吹起来了，才导致了先后的降雨。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3617929562433298&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;937&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCM0s1LibCEDHobAuYxB7zgml6eSd9f2ViamicCOyFyXrWZQOo41yCK8icj0Q/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;思考延伸&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;总结来看，该文创新性的将复杂网络引入了气象领域，发现了超远程的关联在极端气候中持续存在。我读完后思考，除了暴雨，酷暑，严寒，以及旱灾等，都可以用该文的范式去研究，我猜测遥相关在气候中不止限于暴雨这一个案例。而如何在气象预报中，应用遥相关，也是值得关注的。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;如果说蝴蝶效应让人们开始关注到了远距离的天气现象之间存在相关性，那么这项研究则量化的指出在预测模型中，对多远的现象，根据两者之间同步的概率，应该给予多少相应的关注。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;而如果在不同的年份，例如在90年代和进十年间，极端气候间同步的规律有显著的不同，那最可能的解释是人类的活动造成的干扰，而在太阳&lt;span&gt;（黑子）&lt;/span&gt;的”11年“周期，厄尔尼诺现象对极端气候的影响，都是值得探索的方向。&lt;/p&gt;

&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;注：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;厄尔尼诺现象主要指太平洋东部和中部的热带海洋的海水温度异常地持续变暖，使整个世界气候模式发生变化，造成一些地区干旱而另一些地区又降雨量过多的现象。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;736&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMRIhtnZU2ySf3jObCxzqibOeVZYStoaWfTKNIwaju1FfeEiaOzy89y61g/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 留言精选&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我不认为这是颠覆性的，遥相关这个词本身就是对幂律关系的否认，在大气科学领域是种普遍存在的现象，很多类似的遥相关已经被发现，比如enso，北极涛动等。分析方法通常是EOF即主成分分析。这篇论文应该说是在分析方法上创新，更定量了。btw，还没看什么是complex network。另外，地理统计学的方法也是研究空间相关的一种传统方法，有个地理学第一定律是其理论基础，简单讲是距离上越近的东西越相似Everything is related to everything else, but near things are more related to each other.显然大气现象不符合，但并不颠覆。我认为遥相关背后的基础是大气长波的存在，当然大气中有不同周期的波。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383791&amp;amp;idx=1&amp;amp;sn=8d64cb59ba4d52cad925858badc4cdb3&amp;amp;chksm=84f3c9aeb38440b8856c467faa2c0786c7b97168f38e9e47b5aa92221a6d2dfa256f1bcfff9d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;Science重磅：“要想成功，快抱大腿！”&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 24 Feb 2019 07:52:43 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/QkOJZ4teYL</dc:identifier>
</item>
</channel>
</rss>