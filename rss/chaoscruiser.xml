<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>AI药物研发之惑：我们应如何提高药物研发的成功率</title>
<link>http://www.jintiankansha.me/t/x7GR8vCPNy</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/x7GR8vCPNy</guid>
<description>&lt;p&gt;如果说化学阶段的目的是Be better的话，那么医学阶段的目标似乎又收缩了，变成了Be usable，然而真实的情况是大部分药物分子跨不过这个坎。在药物研发里有个谚语，叫做“Fail fast, Fail early”，这其实是求之不得的事情，因为如果拖到临床II期甚至III期临床再失败，将会造成摧毁整个公司市值的重大损失。&lt;/p&gt;

&lt;p&gt;这看起来似乎是荒谬的，如果前期的生物学机理和化学优化已经完善，为什么放到真人身上就砸了呢。但这个荒谬背后的原因是非常深刻的：药物分子在复杂的人体系统，尤其是具有基因和组学异质性的人群中的效应是难以捉摸的，在不同的维度上可以呈现出不同的usability。&lt;/p&gt;

&lt;p&gt;大部分的药物如果是在II期及之后失败，最大的问题可能不是因为药不好，而是以错误的方式用在了错误的人群中。很多药物其实在临床试验里并没有死透，如果我们知道自己错在那里，其实是有可能通过给药方案和适用范围的调整，达到新的临床终点。&lt;/p&gt;

&lt;p&gt;如果能够及时止损，及时选择合适的适应症，提高成功率的话，这才是真正值钱的地方。而这其实可以借助于机器学习对患者画像的洞察来实现，在临床试验开始之前就对这个药在大人群中的可用性，或是对哪些细分marker的人群可用，以及最重要的，哪些marker人群和临床终点无效做出判断。这样的洞见，在II期及以后的临床试验中都价值上亿！&lt;/p&gt;

&lt;p&gt;可以看到，目前的药物研发的流程，最大的矛盾集中在生物学阶段和医学阶段，相反，化学阶段反而是最成熟的部分。而如果只是在这个非瓶颈部分做优化，并不会显著提升药物研发的时间效率和回报率。&lt;/p&gt;

&lt;p&gt;因此我认为，如果AI药物研发的项目，仅仅是过去计算化学模拟，组学和药物开发自动化的延续，是用AI的工具去优化和加强已有的研发流程，这当然是一个最具可行性的前期策略，但是这并不是那么值钱的市场。这些针对药物研发中“化学”阶段的AI创业项目，做的普遍是容易做，但不是必须做的事情。如果只是提升当前的药物研发效率，那么AI药物研发公司的估值，显然有点高了。&lt;/p&gt;

&lt;p&gt;大型药企对这些创业项目的关注和支持，与其说是看好技术而去投资，不如说是出于财务KPI的考虑，以投资AI药物研发公司的方式，将非药企核心的研发业务外包给了CRO和这些“virtual biotech”的AI创业公司。&lt;/p&gt;

&lt;p&gt;这可以输出药企的优势：充沛的现金流投入，和临床开发“接盘”能力，而产生的收益又不会立即体现在损益表上，而是通过收购-商誉的调节，让报表变得更好看。当然，从投资的角度去看，我也认同这种商业逻辑。&lt;/p&gt;

&lt;p&gt;但真正具有极大价值的，应该是用AI重构药物研发的整体逻辑，这可以从两个方向进行努力：&lt;/p&gt;

&lt;p&gt;1，&lt;strong&gt;在生物学的阶段，甩开可理解性的限制，以无监督学习的方式去更高效寻找新机理和有效的新靶点，往外扩张成药的空间。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;2，&lt;strong&gt;在医学的阶段，结合患者画像参与到临床实验的决策中，以提高药物定位和过审的成功概率，尽早识别并kill掉无底洞的烂药，以免到了3期失败被坑死。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这两个方向其实都体现出同一个理念，那就是应该用AI去提高药物研发的成功率，而非药物研发的运营效率，这两者是质和量的不同。如果能够直面“生物学”和“医学”阶段的Hard Problem，实现颠覆性创新，我相信，这会比在“化学”阶段做的任何渐进式创新，都更有价值。与诸君共勉！ &lt;/p&gt;
</description>
<pubDate>Tue, 02 Apr 2019 15:35:58 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/x7GR8vCPNy</dc:identifier>
</item>
<item>
<title>深度学习背后的基础-神经网络揭秘</title>
<link>http://www.jintiankansha.me/t/aeBEgAR2Il</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/aeBEgAR2Il</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;j108-0-0&quot;&gt;最近， 深度学习三杰获得了计算机界最重要的图灵奖， 它们的贡献都集中在对深度学习的根据神经网络的理论突破。 今天我们看到的所有和人工智能有关的伟大成就， 从阿法狗到自动驾驶， 从海量人脸识别到对话机器人， 都可以归功于人工神经网络的迅速崛起。那么对于不了解神经网络的同学如何入门？ 神经网络的技术为什么伟大， 又是什么让它们这么多年才姗姗走来?  我们一一拆解开来。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;一  引入&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们说人工智能经历了若干阶段， 从规则主导的计算模型到统计机器学习。传统统计机器学习不乏极为强大的算法， 从各种高级的线性回归，SVM到决策树随机森林，它们的共同特点是把人类的学习过程直接转化为算法。 但是沿着直接模拟人类学习和思维的路线， 我们是否可以走向人工智能的终极大厦呢？&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2bfp0-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6180555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFox5Ffs67icfICgmGB0BORtBlhpWxzG2YkfgTSQlcem8vibJuLkJZ4gTjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;答案是否定的。 基于统计， 模拟人类思维的机器学习模型， 最典型的是决策树， 而即使决策树， 最多能够提取的无非是一种数据特征之间的树形逻辑关系。 但是显然我们人的功能， 很多并不是基于这种非常形式化的逻辑。 比如你一看到一个人， 就记住了他的面孔。 比如你有情感， 在你愤怒和恐惧的时候击退敌人。 比如你一不小心产生了灵感， 下了一手妙棋或者画出一幅名画。 这些显然都与决策树那种非常机械的逻辑相差甚远。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bspib-0-0&quot;&gt;人类的智慧根据， 是从感知， 到直觉， 到创造力的一系列很难转化为程序的过程。  那我们如何才能真正模拟人类的智能？ 我们回到人的组成人的智能本身最重要的材料。 大脑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4s0b-0-0&quot;&gt;如果说统计机器学习的故事是一个模拟人类思维的过程， 那么神经网络的故事就是一个信息加工和处理的故事， 我们的思维将一步步接近造物， 接近 - “信息”， 这个一切认知和智能的本源。 看它该如何流动， 才能产生人的思维。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二 神经元&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;favf8-0-0&quot;&gt;生物神经元&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bb2cs-0-0&quot;&gt;首先， 神经网络的灵感来自生物神经网络。 那么生物神经网络是怎么组成的？ 神经元。 神经元的基本结构是树突， 胞体和轴突（如上图）。&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-1&quot;&gt;这样一个结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-3&quot;&gt;就像因特网上的一台电脑，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-5&quot;&gt;它有一部接受器-树突，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-7&quot;&gt;每一个树突的顶端是一个叫突出的结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-9&quot;&gt;上面布满各种各样的传感器（离子通道），&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-11&quot;&gt;接受外界物理信号，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-13&quot;&gt;或其它“电脑”给它发来的脉冲，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-15&quot;&gt;一部发射器，轴突，则以化学递质的形式放出自身的信号。&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-16&quot;&gt;只不过树突和轴突是有形的生物组织， 他们象树枝，根须一般延伸出去，&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-17&quot;&gt;不停的探知外界的情况并调整自己。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;btp85-0-0&quot;&gt;神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8temh-0-0&quot;&gt;我们把生物神经元进行数学抽象， 就得到人工神经元。如何抽取它的灵魂？ 简单的说， 每一个神经元扮演的角色就是一个收集+传话者。树突不停的收集外部的信号，大部分是其它细胞传递进来的信号，也有物理信号， 比如光。 然后把各种各样的信号转化成胞体上的电位， 如果胞体电位大于一个阈值， 它就开始向其它细胞传递信号。 这个过程非常像爆发。 要不是沉默， 要不就疯狂的对外喊话。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.25833333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFotBFVTVGWWuOicpIJ9FfxDqzbRudXnowYgTM3zUD6KoE6zBVSndhic42w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;sejk-0-0&quot;&gt;我们说， 一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习， 就是分类器，不停的对外界数据进行处理。为什么要做分类？  因为正确的分类， 是认知的基础， 也是决策的基础。 因为认识任何概念， 往往都是离散的分类问题， 比如色彩， 物体的形状等等。因此， 神经细胞做的事情， 就像是模数转化， 把连续的信号转化成离散的样子。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6611111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo7IYjoHvwolKM6LJDhufUmWuRIibXQzcpJBuGib3dkFE4gdxT1nOAVE6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
突触：生物神经元对话的媒介， 信号在这里实现转化

&lt;p&gt;&lt;span data-offset-key=&quot;7ggij-0-0&quot;&gt;如果说把神经元看成这样一个信息转化的函数， 那么生物神经元的这副样子，使它能够极好的调节这个函数。 信号的收集者树突， 可以向外界生长决定探测哪些信号， 然后通过一个叫离子通道的东西收集信号，每个树突上这种通道的数量不一样。 有的多一点， 有的少一点， 而且是可以调控的。 这个调控的东西， 就是对外界信号的敏感度。 这样的结构， 可以说得到了最大的对外界信息的选择性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dbmjj-0-0&quot;&gt;什么是外界信号？  这里我们用上次课将的一个东西替代， 特征， 如果把一切看作树， 神经元在不停观测外界， 每个树突都在收集某个特征。而这个离子通道的数量， 就是线性回归里面的那个权重。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibji-0-0&quot;&gt;它在干什么事情呢？ 收集特征， 作为决策证据！  当某些信息积累到一定地步， 它就可以做决策了， 如果把这个功能进一步简化， 我们就可以把这个决策过程描述成以单纯的阈值函数， 要么就干， 否则就不干。 就这么简单的。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4263888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFobf2KiaJPeiamU74V5JkPq7iaicufS8yaZy5iaE0YnT6ssye8F4MPjibNp5dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

从生物神经元到数学神经元


&lt;p&gt;&lt;span data-offset-key=&quot;as9ns-0-0&quot;&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;79bjr-0-0&quot;&gt;我们这个简化出来的模型，正是所有人工神经网络的祖母－感知机。　从名字可以看出，人们设计这个模型的最初目的就是像让它像真实的生物神经元一样，做出感知和判断。　并且从数据中学习。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59a57-0-0&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;59a57-0-1&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 Rosenblatt 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36527777777777776&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9icsW6iaibhrGtXQyKPGibkNHia4LA7Ooia8vymNVBVMYbVC4dn2DLnRj6zA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

把数学神经元变成机器

&lt;p&gt;&lt;span data-offset-key=&quot;27jc1-0-0&quot;&gt;你要是想理解这个过程。最好的方法是几何法。你仔细观察， 这个感知机的方程， 如果只有两个特征的时候， 描述的就是一个x1和x2坐标的平面， 中间有一条直线w1x1+w2x2=0，直线的左边是一类， 右边是二类 。感知机的学习过程就是调整这条分类直线的位置，我们测量错分点到这条线的距离之和，调整线的位置， 直到两个类的点乖乖分布到直线两边，我们就实现了分类过程。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9387096774193548&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod3fSJrcicN5uibzI90zF1ZH6slmflRuaOmYO7gmv6ibiaGY3K6SFbCYEwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

一个神经元相当于一个分类器

&lt;p&gt;&lt;span data-offset-key=&quot;25nko-0-0&quot;&gt;这其实就是一个神经元的偏好。 比你让一个神经元帮你确定是否去看电影的例子。它根据今天的温度，一个是电影的长度来判断， 当然一开始它不了解你， 但是它可以先帮你做做决策， 然后根据你每次给他的反馈意见它调整自己，如果你注重温度，它就加大温度的权重，总之找到你看重的东西， 这就是感知机训练的方法白话版， 如果你给他肯定意见，他就不做改变， 你不满意， 他就变一变。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8kn9j-0-0&quot;&gt;最初的感知机采用阈值函数作为神经元的决策（激活）函数。后来这个函数逐步被调整为sigmoid函数。表现上看， 这个函数把阈值函数进行了软化， 事实上， 它使得我们不是仅仅能够表达非黑即白的逻辑， 而是一个连续变化的概率。 而这个函数的扩展softmax则可以帮助我们实现多个分类的选择。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.665625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoImiazM4ib27dfAwMs1nux9HcuB7LVzJcmp6szTWqoK7E1OndTQuGnAbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;320&quot; /&gt;&lt;/p&gt;

sigmoid 函数

&lt;p&gt;&lt;span data-offset-key=&quot;2om41-0-0&quot;&gt;我们说， 这一类模型开始被人们寄托重大希望， 不久却落空。原因在于， 它真是太简单了。 在几何上， 无论怎么变它都只是一个线性的模型。 而事实上， 这个世界却是非线性的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;三 神经网络降伏非线性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c5d5g-0-0&quot;&gt;什么是非线性， 我们举一个经典的机器学习的房价的例子，首先这是一个经典的线性问题。 我们通常用线性模型描述房价的问题。 把各个影响因子线性的累加起来。比如我们把问题简化为卧室数量， 面积 ，地段， 最后求成交价。 我们用线性模型求解价格。 然而， 着一定不是真实的情况， 这背后的关键假设是，我们的卧室数量，面积， 地段都会独立的影响房价， 这就好像说， 如果你有几个不同的因素， 某个要素变化， 不影响其它要素最终决定房价的方式，我们具体来说， 还是房屋单价只受地段， 距离地铁远近，卧室数量三个要素影响，假定三者的权重是w1，w2, w3. 如果你去改变地段，那么w2，和w3必须是不变的。 就拿海淀和朝阳来说，对这两个区， w2 和w3 是一致的。 事实上呢？  显然这三个要素互相制约，或许海淀的人都做地铁， 朝阳的有车的比较多， 它们对距离地铁远近的敏感就不同， 或者说海淀的单身汉比较多， 朝阳的成家的比较多， 朝阳的就更喜欢卧室多的大户型。 如何表现这种特征之间的相互依赖关系呢？ 一个天然的解决方法，就是：&lt;/span&gt; &lt;span data-offset-key=&quot;c5d5g-0-1&quot;&gt;网络模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.43333333333333335&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoX3xicajmjMl3ZNViabf3Ew7wgqhoBLqOr4C4nx5MwQ38CAAVwlqm3uag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;crcl9-0-0&quot;&gt;看看人类可能是怎么处理这些信息的，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-2&quot;&gt;人对于这种复杂的信息，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-4&quot;&gt;往往采用的是陪审团模式，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-5&quot;&gt;我们可能有好几个房价专家， 就自己了解的方向去提供意见， 最后我们再根据每个人的情况综合出一个结果。虽然这个时候我们失去了简单的线性， 但是通过对不同意见单元的归纳， 和集中的过程， 就可以模拟更复杂的问题。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c7vnh-0-0&quot;&gt;再看看怎么把它变成数学处理房价问题。 首先把他表示成公式， 他其实是说的是&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5736111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFosfnkfkAqIFjIzNq3CxGpbYNyDtXzl2HXIibTSBKQC2XJU8icjEn9baDg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7aatl-0-0&quot;&gt;系数变成了和x有关的，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-1&quot;&gt;仔细想一下，这不就是增加了一个新的层次吗，&lt;/span&gt;&lt;span data-offset-key=&quot;7aatl-0-3&quot;&gt;我们需要再原始特征x1和x2的基础上，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-5&quot;&gt;增加一个新的处理层次，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-7&quot;&gt;让不同的特征区域，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-9&quot;&gt;享受不同的权重&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-10&quot;&gt;。 就好比我们有好几位房价评估师，每一位都是针对某种情况下房价的专家，最后有一个人， 根据专家的特长给出最终的综合结果。这与我们的灵感是相符的。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vqde-0-0&quot;&gt;我们把他表示成图示方法。  首先， 每个专家都仿佛是一个小的线性模型， 它们具有唯一的一组权重， 显示它们再所熟悉的区域的权威。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8126843657817109&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9ibmk3wbb7DoRNJpAnrg2P3qqUEAd7E3mdXW4JGVpR4kMvAPO3XMmBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;678&quot; /&gt;&lt;/p&gt;

图片改编自网络博文Machine Learning is Fun!  by Adam Geitgey

&lt;p&gt;&lt;span data-offset-key=&quot;afvtt-0-0&quot;&gt;然后，我们把它们汇总起来， 得到下面这个图， 这个图和之前我们讲解线性回归的图是一样的， 但是我们增加了一个中间的层次，这就是刚刚说的陪审团， 哪些中间的位置， 事实上就是表达了特征之间复杂的互相影响。而最终我们把它们的意见综合起来， 就是最终的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo2E1HdEBTguA3uM8kic3riaibTQgYib212lbLemDibKD4q4oet6ID2sQCnMw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4bjac-0-0&quot;&gt;你换个角度思考， 那些中间的绿色圆圈， 也就是说每个专家，不就是神经元吗？ 而那个黄色圆圈， 是最终决策的神经元。 这样的一个组织， 而不是单个神经元， 就是神经网络的雏形。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;as5n6-0-0&quot;&gt;当然， 房价的问题是个回归问题， 而这个世界的大部分机器学习问题是分类， 每个神经元也是一个小小的分类器， 所以我们把每个神经元的角色变成线性分类器，再套用刚刚陪审团的逻辑， 看看我们得到了什么： &lt;/span&gt;每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有一组头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  我们说， 这就是一个一层的神经网络所能干的事情。 它可以做出一个复杂的选择， 每个神经元都代表着对特征的一个组合方法，最后决策神经元把这些重组后的特征已经刻画了不同特征之间的关系， 就可以干掉认识现实世界复杂特性-非线性的能力， 特征之间的关系很复杂， 我也可以学出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;所谓神经网络的近似定理， 是说一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，&lt;/span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;它可以以任意的精度来近似任何&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-1&quot;&gt;从一个有限维空间到另一个有限维空间&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-2&quot;&gt;的&lt;/span&gt;Borel 可测函数。  这是关于神经网络最经典的理论了，简称万能函数逼近器&lt;/strong&gt;。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5416666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod4vIAibhcvVQNWJFiaS8q4IGwG6LORjWzfhcPUcGicRbFDAdtGQdGO0OQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2r5k6-0-0&quot;&gt;另外的一种理解是，神经网络具有生成非常复杂的规则的能力， 如果你可以让神经网络很好的学习， 他就可以自发的去做那些与或非， 和逻辑运算，不用你自己写程序就解决非线性问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;四 关于神经网络的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1nvpl-0-0&quot;&gt;是什么阻止了神经网络从出现之后很快的发展， 事实上是它们非常不好学习。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;28o13-0-0&quot;&gt;我们说神经网络能够成为一种机器学习的工具关键在于能够学习，那么如何学习的呢？ 如果说单个神经元可以学习的它的偏好， 通过调整权重来调整自己的偏好。 那么神经网络所干的事情就是调整每个神经元和神经元之间的联系， 通过调整这个连接， 来学习如何处理复杂的特征。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7h3c6-0-0&quot;&gt;说的简单， 这在数学上是一个非常难的问题。 我们通常要把一个机器学习问题转化为优化问题求解。 这里， 神经网络既然在解决非线性问题， 事实上和它有关的优化又叫做非线性优化。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;em7k3-0-0&quot;&gt;先看生物神经网络的学习， 它是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是，我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了对高级特征的认知。 上述过程被总结hebbain学习的一个过程。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4708333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoodYXesR7lOnwbA0FpvdW1hbl8Zxw6KaDCNicI4F7gDSlD09AzSQqHFg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44722222222222224&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo4ibY5gFmA50Wd4zhXO4TaPibTmGgVhwGKqCR6HyEZzGeYHYnRuLA32Dw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;coje1-0-0&quot;&gt;从本质上说， 刚刚说的过程是一种无监督学习，你接受输入引起神经活动， 它慢慢的调整。 如果用这样的方法训练人工神经网络，恐怕需要跑一个世纪。  人工神经网络的训练依赖的是监督学习，一种有效的结果导向的思维，如果我要它判断香蕉和苹果， 我就是要一边给它看图片， 叫它告诉我是什么 ，然后马上告诉它对错， 如果错了，就是寻找那个引起错误的神经连接。 然而这个过程在数学上特别的难以表达， 因为一个复杂网络的判断， 引起错误的原因可能是任何中间环节 ，这就好像调节一大堆互相关联在一起的参数，更加可怕的是， 神经网络往往有很多层， 使得这些参数的相关性更加复杂， 如果你一个个去试探， 那个感觉就是立即疯。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6ogvv-0-0&quot;&gt;最终， 这个问题被BP算法的提出解决。 BP反向传播算法， 是一种精确的根据最终实现的目标，然后通过比较当下输出和最终目标的差距， 然后一级反推如何微小的改变各级连接权重以减少这种误差的方法。 这其实就是梯度下降结合复合函数求导法则的一个更复杂的形式。 我们通常把一组刚刚到来的数据， 扎成一捆喂给神经网络， 让它计算出一个输出， 这个输出当然会错的很离谱， 然后我们把这个结果和我们真正需求的比较得到一个误差信号， 这个误差信号会一级一级的改变所有的连接权重。 每一捆数据， 被称为一个tensor， 都像一个个子弹一样塑造着整个网络。 对于这个方法的理解， 最好的办法是使用一套由tensorflow提供的可视化工具。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9h3f9-0-0&quot;&gt;当然这仅仅是一个神经网络的训练的简短小节， 当你在神经网络坚守的Hinton这样的大神，事实上提出过对学习算法的无数改进细节， 才有了它今天的成功。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;五 关于多层神经网络，深有什么好处？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5jrv-0-0&quot;&gt;刚刚说的方法你可以理解一个一层的神经网络， 那更多层的神经网络呢？深层的神经网络究竟有什么好处？事实上深度学习的深，就是指神经网络之神，可见这是奥妙的门道。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3fkq2-0-0&quot;&gt;用一句化说，一层的神经网络可以对特征做一次变换，就如同得到了一组新的特征， 这种特征的变换， 用数学家的眼光就是做了一个坐标变换， 兑换到一个新的空间。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;63aiu-0-0&quot;&gt;我举一个最小的例子让你理解。 你记得高中学过的直角坐标系和极坐标系吧， 这两个坐标系之间存在一个经典的坐标变换公式。 这就可以看作一个坐标变换。 我们举一个例子看，如果要做一个分类器， 把一些居住在圆形中心的点和它的外周分离开来， 那么这个坐标变换就有奇效。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图片来源： Medium Support vector machine explained  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ej38h-0-0&quot;&gt;多层网络， 就代表一个多次连续的特征变换，最后会把数据从一种性质变到另一个性质，从一个空间拉到另一个空间。最终总有一个空间， 这些数据呆着最合适。这种行为，简称为表征学习（representative learning）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.475&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoAPpjHdFGtYzf774rsmY2Jr9iaZuWXCiayVr3rF2WKFV6eOnSTY24DTPg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7lhu9-0-0&quot;&gt;表征学习顾名思义，是要一种特征转化为另外的特征，因为简单的特征一旦发生关系就可以构成复杂的特征，刚刚那个处在领头的大法官和司令再次变成士兵，给上层的决策者提供信息,隐层神经网络所作的事情，我们说其实就是对特征进行重组， 然后得到一组新的特征的坐标变换， 只不过这个变换的形式是可以学习的。 我们说，越限制， 就越自由，为了更好的学习， 我们会限制神经元对信息的处理只由两部分组成， 一个是用一定的权重组合上层的特征， 另一个是通过某个样子的激活函数把这个总和变一变。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;agvf3-0-0&quot;&gt;多层的神经网络， 通过多次的基变换， 把特征一次次重组，得到越来越复杂的新的特征， 这就是深度神经网络作用的机制。 某种程度上， 它把这种层级特征强加给了它要识别的事物， 但是假定事物本身也是这种按照一层层的方法搭建起来， 那么神经网络就会取得奇效。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8jijp-0-0&quot;&gt;哪些事物是有这种层级结构的呢？ 我们说，所有的感知信号都是， 从视觉， 到听觉，到语言。  &lt;/span&gt;假定你看着一个屏幕， 你真正看到的是每个像素点， 而最终你要组合成为一个可以认知的图象。 假定你听着的是声音， 你最终需要分辨出一个个单词 ，句式，直到语言。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;85nga-0-0&quot;&gt;认知科学里说，抽象使得不同的事物可以在更深的层次上被联系起来， 的就是说， 不同层次的函数迭代，产生的新的空间（某种程度也是降维，比如图形处理中），使得原先的距离概念被打破，具体怎样被打破。 这正是人脑处理信息的本质。 实验表明，越靠近感官的神经元处理的信息就具体， 比如颜色，亮度， 而经过多级处理，达到皮层之后， 我们的神经元就可以对比较抽象的实体，如人名，物体的种类敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5125&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoHicmAzdVLQ0RZUTnlhTqwNic1WWOAj06A4VYNTXvUuiaTrjpGjmtczuhA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c2jbo-0-0&quot;&gt;当然，这只是最形象的一个描述，关于增加深度相比增加宽度对神经网络功能的好处，是整个深度学习问题的焦点问题之一。因此引出了几个不同的数学观点， 有机会给大家总结。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7k452-0-0&quot;&gt;训练深度神经网络是困呐的，由于神经网络的训练里有大量的复杂求导运算， 我们是不是函数要写死为止？ 其实不是， 我们有一个强大的框架， 就是tensorflow，让你的整个求导过程十分容易。你只需要写出你的目标函数cost function，然后简简单单的调用tensorflow内部的optimisor就可以了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;六 神经网络动物园&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5oi27-0-0&quot;&gt;神经网络经过这些年的进化，已经是一个大家族。  我们来看几个具体的神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e2saf-0-0&quot;&gt;Auto-encoder 自编码器：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;15eeh-0-0&quot;&gt;最早成功的一种深度神经网络，自编码器意如其名， 其实做的就是对数据进行编码， 或者压缩。  这种网络通常具有的特点是输出的长度或者说大小远小于输入，但是神奇的是， 这个输出却保留了数据种的大部分信息。 你是不是想到了上节课介绍的PCA了呢？ 是的， 这就是一种降维的模型。 这个模型的实质， 也就是一个复杂， 或者说是非线性的降维操作，  比如说你把一张图象压成很简单的编码。 由于这种输入多输出少的性质， 自编码器的形状形如一个下夸上窄的漏斗， 数据从夸的一面进来， 出去的就是压缩的精华。 自编码器之所以在神经网络的进化历史里举足轻重， 是因为它在回答一个很核心的问题， 那就是在变化的信息里那些是不变的， 这些不变的东西往往也就是最重要的东西。比如说人脸， 你把1万张不同的人脸输入到网络里， 这些人类分属于十个人， 那么按说， 很多照片无非是一个人的不同状态，它的不变性就是这个人本身。 这个网络能够把数据最终一步步的集中到十个维度就够了， 这样， 最终可能用一个十位数的数字就可以压缩整个数据集。 而这， 正是神经网络， 甚至是人类理解事物的关键， 那就是找出纷繁变化事物里的不变性， 它对应的正是一个概念。 抓住了概念， 就抓住了从具象到抽象的过程， 抓住了知识，抓住了人认知的关键所在。 深度学习大师Hinton早期的核心工作，正是围绕自编码器。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4847222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoGuozHjTbiakzdWwM1x10geKvNhZQ9dpDib3yUSGcC0eLfog4Tk0ruWvA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dtmn8-0-0&quot;&gt;卷积神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bmp0a-0-0&quot;&gt;卷积神经网络就是模拟人类是视觉的一种深度神经网络。这个网络的特点是能够把图象数据，像photoshop 里的滤镜一样， 被过滤成某种特征图，比如纹理图， 这些低级的特征图， 将再次被过滤， 得到一个新的特征图， 这个特征图的特点就是更抽象， 还有更多的刚刚讲过的概念性的特征。  比如一个物体的形状轮廓。 直到最后一层， 得到对某类物体概念的认知。 这就是人类视觉知识形成的过程。  也是我们下次课的重点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5c9i-0-0&quot;&gt;含时间的神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1df5j-0-0&quot;&gt;神经网络不仅能够描述静态的各个特征之间的关系， 而且能够描述特征（这里更好叫因子）之间在时间上的复杂相互作用关系， 一个神经网络的做出的含时间过程， 最好的例子就是含有证据积累的决策过程。 即使是单个神经元， 也可以把不同时间的信息积累起来做个决定， 而动态的神经网络， 就可以更好的把这些证据总体的汇集起来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ceh5e-0-0&quot;&gt;总的来说 ， 几个神经元组成的网络， 可以像一个信息的蓄水池一样， 通过互相喊话， 把过去的信息在自己人之间流传起来， 从而产生类似于人的记忆的效应， 这些通过特定方法连接在一起的神经元， 就可以形成人的工作记忆或内隐记忆， 而同时， 也可以帮我们设定出处理和时间有关信号的神经网络工具， 这就是RNN – LSTM家族， 以及其它的长时间记忆网络。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7430555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoC2jGqlma1D0hyq3XB24GdAQgQLNEvE8CicBysZhbiaP8WohAknicu3GRw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在深度时间序列处理种扮演重要核心角色的LSTM，其创始人schmidhuber却无缘此次图灵奖。&lt;/p&gt;

</description>
<pubDate>Sun, 31 Mar 2019 12:22:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/aeBEgAR2Il</dc:identifier>
</item>
<item>
<title>贝叶斯推理实用入门</title>
<link>http://www.jintiankansha.me/t/EY2XU24xDA</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/EY2XU24xDA</guid>
<description>&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;什么是贝叶斯推理， 我早在过去的文章里分析过有关贝叶斯概率的知识， 例如&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381931&amp;amp;idx=1&amp;amp;sn=2eb7ab8b5dadda70d74ae6289ef361f8&amp;amp;chksm=84f3ceeab38447fc736fd8a9e6494b663c12dfa3c347b37054d926751141bbe0ed61f0890d11&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;朴素贝叶斯之实践篇&lt;/a&gt;，这次融入纽约大学weijima的方法论教程（http://www.cns.nyu.edu/malab/index.html），给大家一个更实用的版本。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;一，什么是贝叶斯概率， 它于经典概率由什么关系.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e3m1u-0-0&quot;&gt;谈贝叶斯首先是用概率量化问题。 概率这件事大家都觉得自己很熟悉， 叫你说概率的定义 ， 你却不一定说的出。经典的概率， 说的是事件发生的可能性。 我们中学课本里说概率这个东西表述是一件事发生的频率， 这个频率就代表某件事发生的可能大小。 或者说这叫做客观概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;948n2-0-0&quot;&gt;而贝叶斯框架下的概率理论确从另一个角度给我们展开了答案，&lt;/span&gt; &lt;span data-offset-key=&quot;948n2-0-1&quot;&gt;他说概率是我们个人的一个主观概念， 表明我们对某个事物是否发生的相信程度&lt;/span&gt;&lt;span data-offset-key=&quot;948n2-0-2&quot;&gt;。 如同Pierre Lapalace说的: Probability theory is nothing but common sense reduced to calculation. 这正是贝叶斯流派的核心，换句话说，它解决的是来自外部的信息与我们大脑内信念的交互关系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6jnh-0-0&quot;&gt;两种对于概率的解读区别了频率流派和贝叶斯流派。同时我们不难看出两者之间的联系， 你对一件事情发生的可能性估计正是基于某种频率的统计。 但是它们的区别在哪里呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bc3bn-0-0&quot;&gt;首先，给你下面的事件， 假定你带着孩子去看月亮， 然后孩子说月亮的属性是块奶酪， 你会跟它怎么说呢？  首先， 你一定知道月亮是一个石头的星球而非奶酪， 那么这件事你要如何去跟孩子说呢？ 首先我们生活在概率的世界， 你要和它说的是你可以认为月亮是石头或者奶酪， 但是你不要相信任何一个， 既然不相信， 你把它们称为假设1 和假设2，  然后你给它们各自一个数字来代表可能性的大小， 这就是概率。 然后我们看频率观和贝叶斯的区别&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4h102-0-0&quot;&gt;1， 频率观的家长： 到天空做一些测量， 看看奶酪和石头的比例， 然后算出假设1和假设2的概率。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bj860-0-0&quot;&gt;2，  贝叶斯的家长：  孩子我们去不了天空， 但是我们可以想象下我们生活中的经验， 然后查看一下教科书。 首先， 教科书里说， 到目前为止，天空中发光的99.99%是石头。   然后，  再联想下生活经验， 如果是奶酪， 那么它确实是黄灿灿的发光， 因此生活证据显示， 月亮是奶酪的假设并不违和。 那么把两个综合一下， 通过一系列后面会说的公式， 你给出孩子它的观测结合书里的知识的合理性概率： 月亮是石头的概率99.9%。 贝叶斯通过承认我们自身的无知，给不同的假设以调整空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.05&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Er0r4hHxCTIVzGB4kdGibZPHZDLhufbpAYHZGMGIAKO3cGxYZ1ddJkg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f45d1-0-0&quot;&gt;哪个过程更合理？ 哪个方法更正确？ 你自行去分析。这里要说的是，&lt;/span&gt; &lt;span data-offset-key=&quot;f45d1-0-1&quot;&gt;在真实世界里， 我们所做的往往是把现象的经验推理， 和某种先验结合， 去估计事物的可能性，这正是贝叶斯的思路 . 没有人会对每件重要的事做无限的测量， 也不是所有事件都可以重复（分手不可以， 股灾不可以），这是我们唯一可以做的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7kf2h-0-0&quot;&gt;贝叶斯的数学公式十分简单， 一， 你要有先验概率P（A），二， 似然性  P（B|A）， 最终得到后验概率P（A|B）。这三者构成贝叶斯统计的三要素。&lt;/span&gt;似然性实用条件概率表达， 后验也用条件概率来表达， 基于此的贝叶斯定律数学方程极为简单：&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.30434782608695654&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6KfJ3pWuOyaw1gf77HOwiaPIQef4gxPOfw3UcicvlTZB44OlFxQalW4lw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;184&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-0-0&quot;&gt;套用月亮的例子，P(A)代表月亮是奶酪的假设， P(B|A)代表现象黄色发光。 &lt;/span&gt;&lt;span data-offset-key=&quot;dc4dh-2-0&quot;&gt;即月亮是奶酪的先验概率，&lt;/span&gt; &lt;span data-offset-key=&quot;dc4dh-4-0&quot;&gt;是如果月亮是奶酪， 那么它是黄色发光的概率， 你得到前两者，就可以根据公式算出结合了证据之后的月亮是奶酪的后验概率。 这里比较难计算的是 P（B）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dc4dh-6-0&quot;&gt;事实上对它的计算你要把所有可以给出这个结果的假设都包含进来， 用概率的marginal law 展开每个假设之下观测到现象的概率， 比如这个问题里， 你就要把月亮是奶酪和石头两个假设都包含进来， 分别计算各自假设下发光现象的概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9l6h-0-0&quot;&gt;有一个非常有趣的现象是如果我们的先验概率审定为1或0（即肯定或否定某件事发生）， 那么无论我们如何增加证据你也依然得到同样的条件概率（此时P（A）=0 或 1 ， P（A|B）= 0或1） 这告诉我们的第一个经验就是不要过早的下论断， 下了论断你的预测也就无法进化了， 或者可以称之为信仰。&lt;/span&gt; &lt;span data-offset-key=&quot;a9l6h-0-1&quot;&gt;你如果想让你的认知进步，就要给各种假设留一点空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8vg85-0-0&quot;&gt;贝叶斯分析的思路对于由证据的积累来推测一个事物发生的概率具有重大作用， 它告诉我们当我们要预测一个事物， 我们需要的是首先根据已有的经验和知识推断一个先验概率， 然后在新证据不断积累的情况下调整这个概率。整个通过积累证据来得到一个事件发生概率的过程我们称为贝叶斯分析。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9df1h-0-0&quot;&gt;贝叶斯的数学计算主要考察对条件概率的实用。 但是有时候我们也不理解条件概率， 比如著名的辛普森案， 为了证明辛普森有杀妻之罪，检方说辛普森之前家暴的历史，而辩护律师说，美国有400万女性被丈夫或男友打过，而其中只有1432人被杀，概率是2800分之一。 这其实就是误用了条件概率， 因为辩护律师用的条件是家暴，用来推测的事件是男友杀人， 而事实上这里的条件是被杀而且有家暴，而要推测的事件是凶手是男友（事实上概率高达90%），这才是贝叶斯分析的正当用法， 而辩护律师却把完全在混淆条件与要验证的假设。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7256637168141593&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6Mz8k7jElRSnzK5icqYic43ZDLRlJ104tBLOmufISqAIKOn4AcSK26t6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二， 把贝叶斯概率工具用来建模 &lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;4mrm0-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;blf3r-0-0&quot;&gt;贝叶斯概率是非常基础的统计知识， 有的人只把它当成统计， 而它在心理学，经济学， 神经科学等领域具有巨大潜力。&lt;/span&gt;为什么？  因为这类问题的研究对象往往具有极高的不确定性， 是由大量较低一级单元组成的复杂系统。 这就造成直接用物理学搞定分子结构解薛定谔方程的思路是不行的， 你不能把人的行为像氢原子光谱一样求解出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;68f6s-0-0&quot;&gt;那么怎么办？ 纯统计？ 你可以做量表， 去统计所有可能的人的属性和和它们的行为之间的联系， 然后求一个皮尔森系数。 但是这样的方法虽然可以用， 但在量化和机器学习发展急速的今天还是naive了一点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;83o4i-0-0&quot;&gt;一个折衷的方法？ 贝叶斯建模。 贝叶斯建模非常善于处理“黑箱” 问题，对付这种不好精确预测但有些用到一点建模的东西很关用。&lt;/span&gt;贝叶斯建模可以很快的把实验数据和理论做一个结合。 而且据说我们的大脑处理信息也确实符合贝叶斯框架。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;34nj4-0-0&quot;&gt;你只要有假设先验， 有观测， 有似然性， 就可以用一个贝叶斯。这里的似然性，经常是由我们的理论模型提供的，而贝叶斯的框架可以把这个模型的参数迅速的推到出来。&lt;/span&gt;这同时也意味着，果你手里有几个不同的模型假设，有一些数据，贝叶斯会迅速告诉你哪个比较合理。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59t31-0-0&quot;&gt;比如你有两个截然不同的假设解释一种心理现象，贝叶斯方法迅速告诉你哪个更合理。&lt;/span&gt;我们通过下面的几个例子说明， 刚刚说了， 我们哟啊建立一个模型， 然后用贝叶斯把它转化为一个预测机器， 模型可以到多简单？ 请看下面的例子：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1， 多个运动物体例子&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7189655172413794&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6G8nBbLmOjpRibvdBTUicIziaTCyph1aqj5rmRibhOepcyjSMRbqwImFUoQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;580&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.27166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6PIWDfviat9RUnpEKZWGTeyFickUCWI3Y4a87gWWEybbH0iaicDoWOSxqew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Bayesian modelling of behaviour (Weiji Ma)&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6cvit-0-0&quot;&gt;如果看到一组一起移动的物体， 比如上图， 人往往会倾向于认为它们是一个整体。这个现象被格式塔心理学解释为一宗天然的心理倾向。 而解释同样的现象， 你只需要搭建一个简单的贝叶斯概率模型：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7s7nn-0-0&quot;&gt;1， 找到两种可能的假设和现象， A 上面的五个物体是独立的， 刚好一起向上运动  B  上面的5个物体是一个整体  。 现象：  五个物体一起向上运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3frem-0-0&quot;&gt;2， 找到A和B的先验概率：  先验可以。 基于知识或者大量过去的观测， 那么平时生活经验或者书本都会告诉你， 两种情况可能差距不大&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7d852-0-0&quot;&gt;3，A和B得到现象的概率， 事实上它测量假设到现象的关联， 在这个情况下， A几乎一定得到现象， B， 如果每个物体向上或向下的概率是0.5， 那么你应该已经求出来了： 1/32&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e7tph-0-0&quot;&gt;4， 合成后验概率 ：  A压倒B。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4hcon-0-0&quot;&gt;所以， 我么倾向于认为A是对的，即使A和B都有成立的可能。 由此得到的推论是人有把一起移动的物体看成一个整体的趋势， 这符合格式塔原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;49ij9-0-0&quot;&gt;2， 运动眩晕&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;du63j-0-0&quot;&gt;类似的方法可以解释很多我们日常生活中的一些现象，比如我们一个非常常见的现象， 晕船。&lt;/span&gt;关于晕船的一个重要的进化心理学理论说， 这是祖先的一个毒物排出反应， 因为祖先在尝到毒物之后会引起眩晕， 而这个时候呕吐可以排出毒物。 进化心理学用这个例子说明我们事实上生活在祖先的记忆感觉里。&lt;/p&gt;

&lt;p&gt;那么， 这个非常简单的模型假设成立的可能是多大呢？ 如果用贝叶斯方法来分析这个问题会有个很清楚的框架。在此处我们先预设眩晕确实是我们的大脑根据现象对世界做出了预测产生的反应， 我们在船舱里产生了眩晕， 我们有三个可能的模型：&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8v2n-0-0&quot;&gt;A， 我们的大脑检测到我们自己的运动， 是我们自己的运动导致我们的眼睛和前庭（vestibular） 的感觉&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dfcs9-0-0&quot;&gt;B， 我们的大脑检测到了地面的运动， 我们自己的运动（摇摆）导致了我们的视觉感知， 而前庭（vestibular）则感觉到了船舱和地面的相对运动&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2hlva-0-0&quot;&gt;C   我们的大脑检测到了我们吸入毒物。 毒物的作用导致了你自己的运动， 以及你所感知到的地面的剧烈晃动（幻觉）。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;el956-0-0&quot;&gt;贝叶斯的分析框架告诉我们， A， 似然性为0， 因为因果关系是错的， 我们自己的运动只能解释我们的视觉感知。   B,  先验为0， 除非世界末日，我们的祖先几乎不会在车船这类快速运动的物体上活动  C， 这种情况确实会出现在祖先的生活里，先验不为0， 而一旦吸入毒物， 那么确实有可能产生幻觉， 因此似然性不为0 .  所以相对前两者， C最有可能。  当然细心的你会发现这里还是做了太多的假设，尤其对先验， 但是这无疑是一个相对合理的框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;1g3p9-0-0&quot;&gt;3， 颜色误差&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6B65coY3d19voMDpZe4orr4LVMC10w50ehMFn4T9nvEBCyoyyic0w5eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6487455197132617&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6JYRicIOshSnHSQbdibGYCkYMBZNvyGNAcpLkibDKhBZaiaCCgs13sB9W8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;279&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;88rvr-0-0&quot;&gt;你有没有印象这篇刷爆朋友圈的文章，  这个裙子的颜色是黑色还是金色？ 有的人猜是黑色， 有的人猜是金色，而它到底是什么颜色的？ 没有人知道。 这是不是说明客观世界是不存在的？ 还是说我们发现了一个检测乐观主义和悲观主义者的方法？  如果你在思考前面两个，那么你不懂贝叶斯。  事实上， 这个问题的实质是， 这条裙子的颜色确实是不确定的。 而我们在现实世界中对颜色的判断， 本来就是一个贝叶斯推断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;我们来看为什么，颜色事实上光谱决定的， 也是不同频率光的成分大小。 这里我们做个简化，我们只有黑白灰。 大家知道， 其实真实世界的物体本身谈不上颜色， 它只是在反射， 而入射光乘以反射率决定了我们看到的样子。 黑色的物体代表反射率为0，   白色的物体是1， 而中间就是灰色。 但是， 你记住， 你的研究只能检测反射光强，这个反射光强等于反射率乘以入射光强。 如果你的眼睛检测到一个反射光强， 而我们的物体识别问题实际上正是想找到反射率这个特征（它才与颜色相关）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;129j9-0-0&quot;&gt;也就是说，我们的研究遇到一个两难处境。 它所收集的资料反射光强， 既包含反射率， 又包含入射光的信息。 我们得到的是一组反射率和入射光的组合， 那么我们究竟为什么会看到黑白灰的色彩呢？  原因是， 我们的大脑根据先验和似然性， 做了一个贝叶斯推断。  首先， 这里的先验是什么？ 我们在自然界中， 往往会根据时间现场的光线强度等对于入射光强做一个估计， 这个经验数值就是我们的先验（在这里最好把这个经验数值想成一个以最可能的值为中心的高斯分布）。 因为日常生活吗， 总归是在那几种光线下。然后根据刚刚的乘法法则（这个相当于似然性， 你有了反射度和入射光强， 可以完全确定你眼睛的检测光强，一个狄拉克函数）， 你可以推出反射度的后验分布， 这个分布的峰值， 正是你最可能看到的颜色。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.3416666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6dF1FsW8e2icYA47C4OPNm2uWRYH9XnhVrE14vHuticpl98qE5mBL6hLw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;41g06-0-0&quot;&gt;这个实验解释了那个裙子的颜色问题， 你是看到黑色还是金色， 和你日常经验里对现场光强的先验有关， 看来酒吧里的DG和阳光下的建筑工程师的想法应该不太一样。  而这也在告诉我们， 我们看到的东西永远并非真实，由于我们接受的信息总是有限，我们在不自觉的做大量的脑补， 这些脑补， 组成了我们最终看到的世界。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cor57-0-0&quot;&gt;你估计还记得旋转舞女的实验吧， 如果你理解了刚刚的颜色问题， 那么这个问题很容易解释。  你看到她是向右还是向左旋转？  不是有人引用来解释左脑还是右脑型人？  你还相信吗？ 旋转舞女是一个典型的信息不全， 而可以容纳不同的解释的问题。虽然说一些八卦的说法并不可靠， 如果人和人之间在对这类问题的回答上真有差异， 说不定会告诉我们一定所从未想到的东西（比如是什么导致了我们的先验？）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bntb2-0-0&quot;&gt;你可以举出无数这类脑补的例子，比如为什么一篇英语文章每个单词都只保留首尾字母你还能猜到一些？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3318181818181818&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfVWt9ibVByEGjUWF8zEKAw6NHqeGib34vQPm3ACQCqyCr9YpBwTyg4lr4xd5FnNlQ7KLSZxN2sQXog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;220&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span&gt;4， 和时间有关的因子预测&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;贝叶斯方法很擅长解决的一个问题就是和时间有关的因子预设。 假设你经常去以加喜爱的餐厅吃饭， 某一天你突然发现这家餐厅的菜突然就好吃了， 这可能是怎么回事呢？  是不是厨师换了呢？ 然而你没法进到餐厅里去看， 这个时候你会开始回想前几天吃的是不是味道也变化了你没有留意。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b7br4-0-0&quot;&gt;其实， 这里你已经开始进行一个贝叶斯推断过程了。 假定你每天来吃一次饭， 你想推测某个点开始厨师坏了的概率， 这就是一个经典的点推测问题。  这个问题之所以有难度， 是因为如果你把每次吃饭看作一次测量， 那么测量本身是有噪声的，这使得你比较难做出决断，到底是那天厨师心情不好做坏了饭，还是换了厨师。  这个问题的实用性不用多说， 无论是在医疗健康诊断问题里， 还是某段人际关系的变化（好好一段感情突然就变了？ No， 所有的突然变化都是潜在的蓄谋已久）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt;那么具体如何做呢?  事实上这已经开始涉及到很复杂的数学，回到贝叶斯本质， 先验在哪里， 似然性在哪里？ 在很多贝叶斯问题， 先验充满主观性， 这里也不例外， 在所有主观里最客观的就是假定它是一个常数， 也就是厨师变化的概率随时间是均匀的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4qf18-0-0&quot;&gt; 然后， 似然性呢 ?  似然性就是一个生成模型。 也就是给你一个内在的过程，比如厨师的变化， 然后推导出菜的味道的变化。 一个最简单可以放进去的模型， 就是转化概率随时间独立的马尔科夫过程。 有了这个生成模型， 你还会得到系列不同时间厨师变化假定下吃菜味道的分布。 这就完成了生成模型部分， 后面的工作很简单， 只要按照贝叶斯把它反过来，你就得到了给定观测下， 潜在因子（厨师）在不同时间段发生变化的概率分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;&lt;span&gt;三 &lt;span&gt;贝叶斯推理究竟告诉我们什么&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;udk1-0-0&quot;&gt;传统的深度学习的特点是大量标注数据驱动的黑箱， 不太考虑概率分布。 而到了深度生成模型的时代，我们必须考虑概率分布， 因此深度生成模型和贝叶斯有着深刻的内在联系。 同时，贝叶斯框架通过结合有效的先验，可以做到用更少的数据达到更好的泛化效果， 也极为的符合深度学习的需求。 两者在网络训练的结合请参考深度贝叶斯&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;贝叶斯分析里， 你会发现， &lt;strong&gt;你始终要有一个先验， 一个似然性， 而似然性事实上是某种简单的模型&lt;/strong&gt;（也可以很复杂！）。 事实上我们在我们的思维过程， 主动或被动， 正确或不正确的运用着贝叶斯，你所认可的事实里， 很多是你的推断。同样的客观数据面前，先验或似然性不同的人， 可以得出完全相反的结论。  教条的人可能给予了某个假设一个无限强的先验。 而容易被忽悠的大多数可能用到了过于简单的似然性模型， 比如用好人和恶棍解释很复杂的社会现象。 而自做聪明的人呢？ 可能用了一个对自己有利的解释模型， 而忽略了其它可能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ba0s4-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383354&amp;amp;idx=1&amp;amp;sn=133519d77356bdae60ad9388c1f53cb5&amp;amp;chksm=84f3c87bb384416d488770175c676b9061c8168f7c84e10822f186fa8934009c67502279ba72&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;贝叶斯大脑&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382202&amp;amp;idx=1&amp;amp;sn=75d481a667221f328ed44ab76f241451&amp;amp;chksm=84f3cffbb38446edadae25d079a91e5f30592cf67a24e6bf60cf7314f3f60a5860e87cb91240&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;趣味贝叶斯推理&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 26 Mar 2019 16:02:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/EY2XU24xDA</dc:identifier>
</item>
</channel>
</rss>