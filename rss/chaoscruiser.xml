<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何让有监督学习变得有解释性</title>
<link>http://www.jintiankansha.me/t/7rtMhp85bX</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/7rtMhp85bX</guid>
<description>&lt;p&gt;机器学习目前面临的一个大问题是模型缺少解释性，当别人问你，为何我要相信你的模型的时候，大多模型无法给我一个可靠的解释，仅仅提升准确率，并不能解决问题，这限制了在诸如金融，医疗等模型必须解释性，应用机器学习。今天介绍的模型LIME来自17年的NIPS会议，对应的python/R包能够对目前包括图像，自然语言，以及对数据表的传统分类及回归模型给出模型为何为何做出这样分类的解释。这篇小文先讲原理，再展示效果，最后介绍代码。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4110787172011662&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJzib8RRXTXTGe3Nr9p7UHjGp7XBCCAZONQu3UI7wmiavWNia6CicoZF3tCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;686&quot;/&gt;&lt;/p&gt;

&lt;p&gt;LIME的全称是“&lt;span&gt; Local Interpretable Model-Agnostic Explanations.”，局部性假设一个数据点被分类模型做了标记，LIME只会针对这个数据点，利用该点本身的特征对其进行解释，而Model-Agnostic指的是给出的解释不会与你用的是什么样的模型有关，不会涉及模型如何做出分类，这使得LIME具有通用性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;LIME的原理很简单，下如代表的一个二分类器，模型拟合的函数很复杂，而要待分类的数据点X是图中的粗线条红色十字，LIME在X周围随机生成一些数据点，让分类器进行分类，之后按照随机生成的位点和原位点X的距离，来最优化出图中的虚线，这里虚线的斜率代表了该模型针对数据X分类时，各个特征的重要性，以及当数据点发生变化时分类结果会怎么变化，从而解释了模型为何对X进行如下的分类。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6220614828209765&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ9JIlOiaWDfXz3TTWZo8vm1bDhbDXHd2KCGOictxY139IQmtqAd3I4b2Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;553&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;接下来看分类的代码&lt;/span&gt;&lt;/p&gt;
&lt;pre liberation=&quot;&quot; mono=&quot;&quot; menlo=&quot;&quot; courier=&quot;&quot; monospace=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; auto=&quot;&quot; start=&quot;&quot; readability=&quot;26&quot;&gt;
library(&lt;span class=&quot;pl-smi&quot;&gt;caret&lt;/span&gt;)&lt;br/&gt;library(&lt;span class=&quot;pl-smi&quot;&gt;lime&lt;/span&gt;)&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Split up the data set&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_test&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;]&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[&lt;span class=&quot;pl-k&quot;&gt;-&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;), &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;]&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;iris_lab&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; &lt;span class=&quot;pl-smi&quot;&gt;iris&lt;/span&gt;[[&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;]][&lt;span class=&quot;pl-k&quot;&gt;-&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;)]&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Create Random Forest model on iris data&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; train(&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;iris_lab&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;rf&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Create an explainer object&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;pl-smi&quot;&gt;explainer&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; lime(&lt;span class=&quot;pl-smi&quot;&gt;iris_train&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;model&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Explain new observation&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-smi&quot;&gt;explanation&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt; explain(&lt;span class=&quot;pl-smi&quot;&gt;iris_test&lt;/span&gt;, &lt;span class=&quot;pl-smi&quot;&gt;explainer&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;n_labels&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-v&quot;&gt;n_features&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;explanation&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;&lt;span&gt; And can be visualised directly&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;plot_features(&lt;/span&gt;&lt;span class=&quot;pl-smi&quot;&gt;explanation&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/p&gt;&lt;/pre&gt;

&lt;p&gt;这里的数据集是鸢尾花数据集，用随机森林做了分类，测试时选择了5个数据，需要解释的是最后三句，explain函数将分类器，待解释的特征作为参数，而最后对解释器进行可视化，得出下面的图：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.8895265423242468&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJt1iacBPvNPicW4oK6FYGc2ddxrujia6FFhibqEI4SMoF9jdxiadUCkBKIiaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;697&quot;/&gt;&lt;/p&gt;
&lt;p&gt;针对每个测试案例，给出了一个柱状图，图中给出了原特征中的四个的俩个，柱状图中横轴的那句话代表该特征需满足的条件，柱状图的长度代表该特征在满足该条件对模型做出该分类的重要性，柱状图的上方给出该数据点的分类结果。可以看出，对于分类为setosa，最重要的是花瓣的宽度小于0.4，由于4个数据点中都指出该条件最重要。&lt;/p&gt;

&lt;p&gt;对于回归模型，也能给出类似的结果，下图是波士顿房价数据集，用sklearn的中的随机森林，来做回归后，对模型解释性给出的可视化，这里每个柱状图上的条件按是否对房价提升和降低进行了打分，这可以看成对模型预测结果的解释。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJesToTib9l8eZb1olDUbMsibYzAxMhZr52eE6VHI0fw3OxNicDpMLWQx6Q/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;811&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;192.53956834532374&quot; data-ratio=&quot;0.2379778051787916&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJUoDrOZfeO0sxViaEDFCibRX12hzKImjicxx3yJLcVL2U9b5F2GdPhYlVg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;811&quot;/&gt;&lt;/p&gt;


&lt;p&gt;而对图像数据，LIME可以可以标出图像中那个部分导致了模型导致了分类结果，例如下如被分类为草莓，LIME标出的分类原因正好对应是图片中俩个草莓，&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;1.3266129032258065&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJBZx3JsRaOwcEYVFcibqicqkOEldRG2nH7ZPJFz8WOX4lpm0f4ACHiaib6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;248&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而下图被谷歌的inception从高到低按可能的概率给出不同的分类，LIME可以解释不同的标签为何将图片看成是电吉他，木吉他或者拉布拉多狗。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.38896746817538896&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ1icqKia7fTMonN0v64IicguwXowW0ibic8KWOvX7Cobf8gMbibKuz9fHsNeA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;707&quot;/&gt;&lt;/p&gt;

&lt;p&gt;这里展示的是文本主题分类中，对新闻主题分类进行解释的可视化，这里柱状图的横轴是待分类文本的关键词，这段文本的主题被分类为无神论，而不是基督教，究竟是哪些关键词导致这样的分类了，这里不同的关键词，他们支持的分类结果及对应权重。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.31432360742705573&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ665C8B2UxJGckDXGcgrL4YzuLF59V6PPasOP7qDXuc6tpUHADl6ejw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;754&quot;/&gt;&lt;/p&gt;

&lt;p&gt;LIME还针对文本分类，做了可视化的交互工具Skiny，来互动的展示文本中对分类影响大的关键词。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.7311715481171548&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJSYzorojegCELibibNLTgUc7qVNm0ulTwyp9m1YTq5YfOxOaO9lWLlBbA/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;956&quot;/&gt;&lt;/p&gt;

&lt;p&gt;总结一下，之前提升模型解释性的尝试，只能依靠树模型给出的特征重要性，或者看去掉了那些特征，模型的准确性变差较多，或者看那些特征的相对梯度较大，LIME提供的通用框架，对促成可解释的机器学习，有所帮助，找到了关键特征，对建模时的缺失值补全，数据清洗，以及数据增强，都会有所助益。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384403&amp;amp;idx=1&amp;amp;sn=ab94d9446b1bec4d6ecb3954aeaee7c2&amp;amp;chksm=84f3c412b3844d048b4eff968fa92d2791bd6870b14fc1fa5c9cde9d765e9ef25008c9d92021&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;图像分类中的隐式标签正则化&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384338&amp;amp;idx=1&amp;amp;sn=ef349fadd766aca33d0499ea72425258&amp;amp;chksm=84f3c453b3844d457c19473e8299647b320a292c3544505380345f9f530d6fd83d1c1d5b14fa&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;GAN的五个神奇应用场景&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Jun 2019 03:18:04 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/7rtMhp85bX</dc:identifier>
</item>
<item>
<title>剧变进行时-《剧变-国家危机中的关键时刻》读书笔记</title>
<link>http://www.jintiankansha.me/t/luHmchI3B9</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/luHmchI3B9</guid>
<description>&lt;p&gt;&lt;span&gt;戴蒙德2003年的《崩溃-枪炮，钢铁和病菌》已经成为经典，&lt;span&gt;19&lt;/span&gt;年新出的这本《&lt;span&gt;Upheaval&lt;/span&gt;》，中信出版社翻译为“剧变”，也即将出版。该书盖茨在他的博客大篇幅推荐，书中讲述国家在面临危机时该如何应对.对于当下正在经历几千年前所未有之大变局的我们，读史书，能带我们看清如何才能让一个人，一个国家，甚至整个人类文明浴火重生。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9e09PXB2LxWxWmWMoQWcydlXnWCdhBmuCdonxKPH8BNk8R73yEI7u6A/0?wx_fmt=png&quot; data-cropx1=&quot;16&quot; data-cropx2=&quot;291&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;439&quot; data-ratio=&quot;1.5963636363636364&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9cxDSA3Ls6ukE2YVcaK7Z3soWvicUuOTMuUdkX9Xe8V09MftTekACCSQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;275&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;该书的思路是治大国如烹小鲜，先用个人生活中如何度过危机，来类比国家面临挑战时所需要的应对之策，总结了12个关键因素。之后举了作者熟悉的&lt;span&gt;6&lt;/span&gt;个国家，芬兰，明治维新时的日本，智利，印尼，德国，澳大利亚，分别举例上述几个因素在面对外部压力，内在冲突，以及缓慢积累的危机是如何起作用，从容使得该国从危机中变得更强的。之后介绍了当下正在经历剧变的俩个国家，日本和作者的祖国米国，作者对米国自身的分析，很是靠谱，先是历史上的优势，之后当下的四个问题，并从中指出政治上缺少妥协，民间的社会资本（social capital）减少作为美国遇到的最严重的问题，这些分析是很深刻与全面的。最后一章叙述的是全人类正在面临的危机，其中列出的核战争，全球变暖，资源匮乏也是老生常谈。全书中举的&lt;span&gt;7&lt;/span&gt;个国家的例子，我读来印象最深的是芬兰，建议读者多找些芬兰的历史读读，这段历史，真该当成史诗，拍成电影，太了不起的一个国家了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;北欧小国，紧贴战斗民族，有独特的语言和民族自豪感，芬兰在上世纪30年代还是个落后的农业国，&lt;span&gt;39&lt;/span&gt;年被苏联入侵，孤立无援，全国死了&lt;span&gt;20%&lt;/span&gt;的人，去打游击战，苏德战争后加入德国阵营，但只求收回国土，拒绝德国要求的进一步进攻苏联，等到德军战败，再次被苏联入侵，芬兰总统&lt;span&gt;45&lt;/span&gt;年去莫斯科，去签停战协定，自然有战争赔款，在芬兰驻军，但芬兰人的不屈为他们保住了国土的完全。冷战时期，明白苏联要的是地缘政治上的安全感，在意识形态上，做了诸多妥协，在经济上，成为苏联与西方交流的窗口，对内投资教育，为了还得起战争赔款，启动工业化，几十年后，成了人均收入最高的几个国家，诺基亚这样的国际巨头就来自芬兰，一个国家能够获从危机中重生，芬兰是反脆弱最佳的例子。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;接下来看这12条应对剧变时的&lt;span&gt;12&lt;/span&gt;个经验教训，书中最大的缺点就是没有对其进行排序，给出那些是最重要的，那些是必不可少的，这里我会给出&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1，&lt;span&gt;承认问题&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2，&lt;span&gt;承担起改变的责任，拒绝把自己看成是受害者，抱怨别人&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3，&lt;span&gt;选择性的进行改变&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4，&lt;span&gt;他国的帮助&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5，&lt;span&gt;将其他国家的经验当作借鉴的模范&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6，&lt;span&gt;对国家身份的认同&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7，&lt;span&gt;诚实全面的认清当前情况&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8，&lt;span&gt;历史上曾有的面对危机时的经验&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9，&lt;span&gt;面对暂时失败时的耐心&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10，&lt;span&gt;具体情况具体分析的灵活&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11，&lt;span&gt;国家核心价值观&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12，&lt;span&gt;摆脱地缘政治的束缚&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上述的这些，前俩条和第七条说的是同一件事，就是不讳疾忌医，每日三省吾身，向内寻找改变而不是向外寻求将国内的危机转移到别处，这在我看来，是最重要的，必不可少的。除此之外，最重要的是第九条，也就是把危机的解决当成是持久战，不由于一城一地的得失而怂了。而作者列出的第三条是果，第10条是因，统一可以看成是鲁迅从拿来主义一文中说的道理“人不能自成为新人，没有拿来的，文艺不能自成为新文艺。”，根据具体情况，选择性的改变国家的目标和侧重，做历史经验和他国模范的主人，而不是生搬硬套。至于其他的几点，我不认为有多重要，国家的核心价值观，对国家的认同，与其说是国家战胜危机所需的优势，不如说是人民愿意相信国家能战胜危机的自信，自己不必背井离乡，毕竟国是由家组成的，而每个人都是从都是从自身出发的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.6798029556650246&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYPRyROm6kqibADLsZ2icWWJ5AJAAibY1lrJ8yfPIkibbn9Sg8hsSNWz6n2UvibxJZ3HFkMP61Apb6ribg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;812&quot; /&gt;&lt;/p&gt;
&lt;p&gt;全书总结-标橙色的四条，我认为是危机解决的充分必要条件&lt;/p&gt;

&lt;p&gt;&lt;span&gt;该书的作者虽然已经82岁了，却没有倚老卖老，一开始就承认这本书来自作者的个人体验，书中大部分篇幅，用来描述当作例子的七个国家，选取这些也只是由于他曾经在这里生活过，对其有第一手的体验，又在书后指出了这本书缺少系统性，可量化的分析，还为未来的学者指出了方向。这种对自我的清楚认识，是很难得。不过我觉得这本书在讨论当下正在展开的危机，尤其是人类面临的危机时，完全漏掉了AI带来的挑战，对于国家来说，需要面对的是大规模的失业和产业调整，对于整个人类来说，无人自动化武器将使世界变得更加不安全，更加智能的推荐算法加剧了信息茧房，让人与人之间的信任与全人类的对话变得越发困难，而稀少的社会资本将不利于全球共同面临的问题，例如气候变化的解决。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;整体评价：4星 值得精读，不必读多遍&lt;/p&gt;

&lt;p&gt;相关书籍：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383412&amp;amp;idx=1&amp;amp;sn=b3d8537651bb02abd3ebfa759e5e1db3&amp;amp;chksm=84f3c835b38441239a0cc7412f1e0873a8ff7dfdff3f7044c4a998c9706f444a43cbaf55fbcd&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;《Skin In The Game》塔勒布由风险管理引出的一碗毒鸡汤&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;书中的核心观点都和反脆弱这个主题有关，关于如何转危为安的药方，俩位作者虽然学术背景和生活经历不同，却给出了相近的解法，不同的只是表述，强调的都是自己的主观能动性，要沉着，勇猛，有辨别，不自私。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383631&amp;amp;idx=1&amp;amp;sn=a9c154f62ff847d35278fd8c93b6ee39&amp;amp;chksm=84f3c90eb384401837ec1b5b00840d64c6ad836a6d9eb6eddc692d4a21c40f281a479acf070f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;读《无穷的开始》重新了解进步与启蒙&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;讨论危机的解决，其前提是进步史观，相信未来会比当下要好，相信当下问题的解决不在古书而在于当下的创新。《剧变》这本书作者把进步史观当成了默认选项，但当危机到来时，人们会习惯性的抛弃进步史观。检查进步史观，是剧变这本书忽略的危机解决的必要充分条件，《无穷的开始》全面的讲述了人类为何应该对未来保持乐观。&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 09 Jun 2019 06:37:09 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/luHmchI3B9</dc:identifier>
</item>
<item>
<title>对神经网络进行翦枝-让深度学习变的可以训练</title>
<link>http://www.jintiankansha.me/t/91d8x3ee9U</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/91d8x3ee9U</guid>
<description>&lt;p&gt;今天这篇文章算是ICRL（International Conference on Learning Representations）中标题最有趣的一篇了，也是对实际训练很有指导意义的一篇文章。这篇文章的主要观点一点都不难懂，也没有多少复杂的公式推导。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.2976694915254237&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccI2O8uL63WlDfTDccwGsGVZANnBeamNt6ud90yCwP8JPOr7sKcQeXBmKYsJVzNYnpaFA36OvUmjw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;944&quot; /&gt;&lt;/p&gt;


&lt;p&gt;对训练好的神经网络进行翦枝，去除其中不必须的权重项，可以去除网络中90%的权重项，同时不会影响网络的在分类等问题上性能。通过网络翦枝，可以减少网络的大小，减少计算所需的能耗及计算时间，从而提高使用网络做推理时的效率。&lt;/p&gt;

&lt;p&gt;但问题在于，为何要在训练完成后再去对网络进行翦枝，去除那些&lt;span&gt;当网络收敛到一定程度的时候对网络做分类任务时贡献度较小的权重，而不是直接用一个更小的网络架构来训练。传统上对这个问题的回答是，如果一开始训练时的网络架构就相对较小，模型的容量有限，会导致最终训练好的模型效果相比翦枝后的模型相对较差，因此要先去训练一个大的网络，在通过翦枝对网络进行加速。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而今天介绍的研究则是问，如果在训练的时候就开始对网络随机去除一些权重，而且在训练过程中一次次的去做翦枝，那么网络在分类精度，以及训练所需的时间上，有什么区别了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.54627539503386&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9x53m91FCBnjup6t8ic8W2qW3QsmEkMLpP8gTvibjjbNkxIckTWLq0aFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;443&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上图展示的是针对MINST数据全连接网络（红色），以及用于分类的卷积网络的CIFAR（绿色与黄色），在训练过程中，在什么那个epoth需要停止训练，以避免过拟合，用该值来代表训练所需的计算时间，横轴是随机去掉多少比例的权重，不同的线型代表训练完成后和训练进行中进行翦枝。相比翦枝前的稠密网络，翦枝后的网络训练所需的轮数（epoth）普遍更小。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.44719101123595506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9WCEP9gHRd5Vbt2xk3T1RuHUpNufldWib2ERrHCpClWatgKYo9qKGGhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;445&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上图展示了针对上述俩个网络，比较了分类精度的变化，注意横轴最极端情况下，进行翦枝去除了99%以上的权重。但模型在分类准确度上，以及分类所需的epoth上，都没有出现特别明显的下降，但如果训练中迭代式的对随机选择权重进行翦枝，和训练完一次性的去除对分类帮助不大的权重，在分类的精度上，就存在着10%-20%的差距。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.31385281385281383&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccaib8FFGzpgbdeE995NiaGia9HaPQGR7Z84o7cuyicIEibWqyXiav87Lj1aAC5uc4c9ddibianIy6289cH1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;924&quot; /&gt;&lt;/p&gt;
&lt;p&gt;本文用到的网络的具体参数与大小&lt;/p&gt;

&lt;p&gt;&lt;span&gt;由此作者给出了神经网络中的”lottery tickect“彩票猜想，即对于一个稠密的网络，其中一定包含一个子网络，当该子网络单独训练时，在分类准确性和训练时间上和原网络近似。这样的子网络作者拟人化的称为”彩票中奖者“。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;那图中另一种线条代表的训练过程是怎样的了？首先是随机生成一个稠密网络结构，然后做反向传播来对权重进行优化，再去除优化后网络中权重较小的部分，之后对剩下的网络结构中的权重随机，不断重复上面的四个步骤，迭代式的找出”中了彩票“的最优子网络。&lt;/p&gt;

&lt;p&gt;假设要去除的网络权重为p%，去除过程包括n次迭代，那每次去除 网络中p^(1/n) %的权重项，而不是像传统中在训练后一次性全部去除。由于迭代式的翦枝，在每一次的训练过程中，都需要重新对网络的权重进行随机初始化，正是这个操作，导致了模型的准确性不如训练后一次性翦枝，这反过来说明了在深度学习中，对网络权重的初始化，对提升网络的精度影响很大，不应该简单的用随机值来初始化网络权重。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;该实验引申出的另一个推论是翦枝剩下的网络结构本身和训练数据强相关，从“中了彩票”的网络结构中，甚至可以推出到底是训练数据本身的规律，至少这个网络结构展示了训练数据中存在的偏向性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;综合训练精度和训练时间，如果一开始训练的网络结构本身是一个相对稠密的，包含更多权重的，那在模型的准确性上要好，也更容易训练，因为从一个密集连接的网络结构中，找出最优子网络的几率更大。如果先通过翦枝，找到原网络结构里“中彩票”的子网络，那么模型的训练所需时间会相对较短。&lt;/p&gt;

&lt;p&gt;该研究的猜想如果在更多的数据中重现，那可以用其来降低神经网络训练所需的计算资源（在训练过程中进行权重翦枝），也可以用来提升模型的精度（通过对最优子网络的研究，指导设计新的网络结构及初始化策略，或者将一类数据对应的最优子网络结构用在其他类型的数据上），还可以对加深对深度学习背后的理论基础的研究，例如确认随机梯度下降的优化策略是不是等价于迭代式的选择子网络对其进行优化。&lt;/p&gt;

&lt;p&gt;人类大脑的发育，在幼儿时期，先让神经元之间自由的构建连接，到了童年，再去除多达80%以上的多余连接。神经网络和大脑发育呈现出了相同的趋势，这在AI中并不是特例，更多这方面的进展，参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;该研究并不是百分百完成了的，其中考虑的神经网络，和当前研究用到的网络相比还是比较小的，不确定该文的猜想是不是适合更大规模的神经网络。同时该文也只考虑了计算机视觉，对于时间序列数据以及NLP相关的任务，该文的猜想也没有相应的验证。另外该文中对网络进行权重翦枝的方法，只是基于当个权重的大小一刀切，对于新的翦枝方法，例如对网络中的一个局部整体进行翦枝，不基于权重大小的翦枝选择策略，该文都没有涉及。随机生成的网络要想有效，需要足够大，本文考虑的网络大小，对比两者网络翦枝策略，其实不是一个公平的起跑线。对如何从翦枝最优网络结构中如何提取与训练数据有关的特征，本文也没有进行讨论。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384403&amp;amp;idx=1&amp;amp;sn=ab94d9446b1bec4d6ecb3954aeaee7c2&amp;amp;chksm=84f3c412b3844d048b4eff968fa92d2791bd6870b14fc1fa5c9cde9d765e9ef25008c9d92021&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;图像分类中的隐式标签正则化&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384360&amp;amp;idx=1&amp;amp;sn=9e7a1c88f8dc373730bc5933abd1c616&amp;amp;chksm=84f3c469b3844d7ff3e6cd1d3f9c728f583063c0a95c6ad21596d2d66a2dfd8a2bdc205eadad&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;随机网络中的智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 08 Jun 2019 17:22:20 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/91d8x3ee9U</dc:identifier>
</item>
</channel>
</rss>