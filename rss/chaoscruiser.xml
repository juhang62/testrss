<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]机器学习的本质： 理解泛化的新观点</title>
<link>http://www.jintiankansha.me/t/dkkqYSYgi3</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/dkkqYSYgi3</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;人工智能的主题是学习， 从简单的机器学习到深度学习， 我们始终在头疼的一个事情就是过拟合。 对于过拟合， 我们有很多说法， 过拟合对应的是机器死记硬背， 没有能够举一反三的情况。 关于什么是泛化能力， 我们管它叫机器在新数据面前的预测水平。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.25277777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWbrfH9SBticH79gXibnay0dEzkMoMbau2Ct1Mq01mZycycg5ebVGwhLJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1280&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWFVb6UE9vk27UYjUINHKicY2sQ4kHhdVhXnzgkwHqer5ktuGzIICcsfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;923&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用一个简单的方法理解过拟合，如果你手中的数据有限，比如说星空里观测到的三个星星，　你可以想象出任何一个复杂的图形穿过那三个点，也是你的想象力丰富多彩，你就可以做出越多这样的图形。事实上，我们知道这样的想象不具备任何预测能力，如果天空中出现第四颗星，我们一定就不能确定它是否在该在的位置上。　&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWwbsAEtP3LYFQCFAlPyA5EyzgokiaNiaxnUEgPrgZHricKtWlJb8hmDIfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;400&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;过拟合的反面， 就是泛化， 应该说，它就是学习的本质。 否则， 整个机器学习就是一门拟合而已， 深度学习就是比较复杂的拟合。学习的最高境界，是在纷繁的现象里总结出简单的定理，比如看到大量物体运动的轨迹，总结出牛顿定律: F=ma . 但是它的预测能力几乎是无限的。学习，　本来就是在总结规律，　而不是复制数据。　　　　　　　　&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而翻开机器学习的典籍，关于泛化和过拟合的理解，却非常零碎，表面化。首先，我们回顾，我们看来自不同学派的不同观点和做法：&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 贝叶斯学派的最简单模型： 学习是从先验到后验概率的转化， 一个好的学习算法， 最重要的事情是用最少的假设，得到对数据出现最大概率的解释 ，每多一个假设 ，整个观测成立的概率就乘以一个小于1的因子， 假设解释理论的概率迅速减少。 这个理念翻译为白话就是“如无必须， 勿添实体”。 来自贝叶斯的一个直接方法论是引入最大后验概率和正则化的概念， 贝叶斯派的核心观点是我们需要简单的模型。 模型的参数直接体现复杂度， 因此贝叶斯观点下参数多而数据少， 就是过拟合的直接原因。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.549718574108818&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWVS7ibj6QA3wtUnWdNzo5Mvyb23iahGtIgPkb7I70eoPkG0cshqUx3vQA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;533&quot; width=&quot;533&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.43194444444444446&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWYL2bcflcOecu6ZAQ0uPoVCrwFrgqdLCOOWEiblNH6xeWj59QPenIQcA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1045&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 几何派机器学习的风险最小化： 泛化误差的来源是数据的量不够大， 新的没有见过的数据代表着看不见的额风险， 因此我们要泛化风险最小化， 如果是一个分类问题， 意味着你要找一个分类界限，叫已知的数据点离分类边界足够远， 这个想法， 就是maximum margin solution 支持向量机的源泉。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.7335243553008596&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWrZ1lIuR1OA3eicZcEibrWgdfDzyeg7SCnuIzd8LNKnAk4fQfWGia1uW4g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;349&quot; width=&quot;349&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3， 统计机器学习的模型平均： 过拟合的源泉是数据量不够大而模型的参数过多，引起对于数据集的轻微变化， 模型参数剧烈变化， 从而在测试集的表现就是高方差， 训练集稍微一变，结果就变了。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何抵抗过拟合的一个方法就是做平均，每个模型假设可能会犯错误， 把它们平均起来减少了模型在新数据上的方差。 比如说我们最喜欢的决策树， 我们如果把一棵树变成一片森林， 过拟合的风险大大减少， 因此我们得到随机森林。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWqEUSAdpKWadZjb5TITmib67kMYpBrSorqicfZYN4aBxMwBiaTBCWexVmQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;592&quot; width=&quot;592&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3， 连接主义学派， 连接主义学派认为， 过拟合的风险来自于神经网络具有无与伦比的拟合能力， 那么如果要削弱过拟合的风险， 我们就加入一个东西， 叫dropout， 我们在训练的时候随机的筛选掉一些连接， 然后如果网络还能做正确分类他就是具有鲁棒性， 这就是连接主义的做法。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.4968553459119497&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWVvU45gfib2fwr4IMc2lG6BDWf2bicsJyuiawNzARvcARyL0edQbTibnQJQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;318&quot; width=&quot;318&quot; /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;应该说，这些说法不仅有些杂乱，甚至某些时候是矛盾的。　如果仔细思考，我们会立刻发现下面几个问题：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;１，在机器学习时代，我们希望在同等数据下参数越少越好，而在深度学习时代　，我们发现模型的范化能力随着参数的增加而增加，层数多了，&lt;span&gt;反而泛化误差的能力更好&lt;/span&gt;。更有甚者，　深度学习动辄使用参数的数量大于数据的量，但是模型却不会过拟合。　&lt;br /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;２，即使机器学习传统的正则方法，也看起来矛盾重重，比如很多模型都没有使得模型更简单而是更复杂，却同样能够减少范化误差。　　　　　　　　　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;一种更好的理解方法是，机器学习里说的泛化误差和物理里的熵最大有一种深刻的联系。如果有一个实际观测，有一个模型空间， 这个模型空间的每个模型都代表着我们对真实数据生成过程的一种模拟。 那么我们希望符合实际观测的模型在这个模型空间里越多越好，而不仅仅只有一个， 而且从符合观测到不符合观测的过度要比较均匀，这样我们就可以从容的容纳最多的随机性，这样找到的模型，也就是我们要的泛化能力最高的解。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;简单的来说，机器学习的过拟合，　根源于它是一种反向工程。　一组输入，　经过真实世界的变化得到一组输出，比如说人脑看到一个图片就知道是个苹果。机器只是得到了这些输入和输出，就要产生一个模型，跟那个真实世界的产生过程一样。这其实是不可能的，顶多是类似。你所拥有的数据也仅仅是一些特例而已。我们的机器学习黑箱，无论如何学，只是提供可能的关系中的一个版本。你的训练集上的准确度再高，这仅仅是一个巧合而已。　一个经常被用到的比喻就是，如果一个机器学习模型死记硬背的能力超强，它几乎可以把所有的训练集的标签都记下来，然后一个不差的背出（想象一个神经元数量和数据量一样多的神经网络），但是只要数据哪怕改变一点点，它一定出错。　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何抵抗这样的问题呢？　其实一个很好的方法就是让这个求得的解不那么凑巧。假如你用不同的方法改变你的数据集，　最终得到的结果是一致的。当年的机器学习经典教材里管这个叫低方差，就是说数据变一变，　结果不太变。 如果你一味追求低方差，　你会发现最后模型就傻了，所以，你还要权衡一下模型的准确度。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们再看下物理里熵的概念： 对于一个复杂的系统，比如一群分子，一个人群，你要测量其中的某个性质， 比如分子的动能， 人的身高， 这个东西首先具有的性质就是不确定性，由一个概率分布表达。熵用来衡量这个分布的不确定性大小。一个分布具有的可能性越多， 概率分布越均匀，熵就越大。 物理上一个系统往往趋于熵最大的状态， 熵越高系统越稳定。但是真实情况下， 我们只关心在一定限制条件下的熵， 比如体积温度压强，这个时候熵不能随意的增大而是受到限制条件的最大化， 我们通过在熵最大化公式里引入拉格朗日乘子法解决这个问题，也就是受限条件下的最大化问题。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;在同样能拟合数据的情况下， 我们需要找到模型空间里那个熵最大的解。 学习器永远不是只有一组参数w，而是一个很多参数组成的空间。 每组参数代表一个假设。什么是好的模型？　　一方面你要能给我生成特别多不一样的假设，　也就是模型的容量要足够大，　足够有层次（比如加入深度），这样你具有足够强的拟合能力。　另一方面，　要让符合数据观测的假设足够多， 可能的分布足够均匀， 这也正是熵最大的含义所在。 你找到的解不是那么凑巧得到的， 也就是熵很大，最终， 你在这组解里取一个平均，也就取得了泛化误差最小的解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;一方面模型尽量复杂，一方面有大量行的通的解， 这样就是最好的。 这就把那些特别复杂的模型从潘多拉的魔盒解放了出来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用这样的观点来看，机器学习和深度学习不同视角下的正则化就不会那么多矛盾：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;１，　机器学习，　这个观点在支持向量机算法上体现的淋漓尽致。所谓的max margin solution, 　也就是最大间隔，也就是说这个时候两组点之间的隔离带最胖，两组点被分的最开，这个时候，所有出于隔离带里的直线本质都可以把两组点分开，如果一个直线代表模型的一种可能，这个时候就有最多的模型可以解释数据，而我们选择中央的那一条，是因为这个解是所有那些可能的解的平均，因而也就是最安全的。　我们最终把这个解释为结构风险最小。　&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.697265625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWP0k6ajRBXycoUK2fO6yFGt3t0HH3icRANXsYMLh3rB1U1Szbu91jZmg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;512&quot; width=&quot;512&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看逻辑斯蒂回归，整个逻辑斯蒂回归都可以从熵最大里面推出。事实上，通过交叉熵找到的最后的逻辑斯蒂回归的解，　和支持向量机的解具有一致性。　我们的逻辑斯蒂回归就是加了概率分布的最大间隔解。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;２，关于一范数和二范数：　如果有一组数据ｘ，ｙ同时可以被模型ｗ解释，那么我们偏好w较小的（这基于你假定预测数据的分布符合高斯）。 事实上如果你做一个统计实验你会发现，ｗ的模长越小，你在你选择的解周围就有越多差不多也符合观测的解。　相反，如果你的ｗ模长很大，你会发现，你稍微改变ｗ以后，　模型的解释力就很差了。　一范数的道理是类似的，　只不过你假定数据的分布符合拉普拉斯而非高斯分布，同时引入稀疏性。 　　　　　　　　　&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;３　进入到深度学习的世界， 我们看到之前说过的那个矛盾不见了，　因为，增加模型参数不再和过拟合有必然联系，增加模型参数只是增大了可能的模型空间。&lt;span&gt;但是，如果这里面有很多解是符合观测的，那增大的模型空间不但不是诅咒，还是一种福祉。&lt;/span&gt;　比如如果你增加了很多层，　但是通过dropout这样的随机减枝操作，你的解不受影响，也就说明有大量的解都符合观测，　也就是说这一组解其实都是泛化能力比较好的。　　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们的网络不是宇宙里最特别的那个， 它只是无数个这样的网络里的一元，而最后我们就把这些网络做个平均就可以了， 这正是dropout的道理所在！ 一个网络越接近一个随机的网络， 越是在毫不刻意的情况下发现数据里的规律， 就越有可能是泛化能力最好的模型。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看各类隐式的防止过拟合的手段，比如随机梯度下降，SGD, 　随机梯度下降是说每次取一个数据（或一个小批量数据），　由于取数据的这个过程是随机的，带来的参数改变的信息也就有限，也就是带来一个噪声。我们说，这个过程就像物理里的含有一个漂移项的布朗运动，一方面它在往更正确的方向运动，一方面它也在做一个随机游走。Tshiby信息瓶颈的论文指出到了训练后期，我们几乎完全做的是一个随机游走（我们看到训练误差变化不大）。这件事和泛化的联系就很显然了，一个随机游走的过程又不允许训练误差增大，那么它只能寻找那个周围有很大的自由度，或者说附近的解都比较正确的那个参数区域，　这就是泛化的根本。　&lt;br /&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;４，　回看Tshiby的信息瓶颈理论，其实，它里面有新意的东西已然不多了，它所说的，深度网络实现一个信息压缩，也就是一个降维的过程，它符合公式：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.29765013054830286&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpW2mHKczutwcsQQTQYtm1ZWOFGEBta0zjI1zrTj9YRZWzKqV0uG3hrUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;383&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;而在这其中， 我们也看到深度学习和传统机器学习的潜在不同， 它出现了一个介导T， 也就是我们说的representation。 输入信息X， 先被翻译成表征T， 再达到Y ， 从T到Y的过程。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里说的是，　表征就是被转化出来的特征Ｔ， 需要具有两重性， 一方面， 它需要尽量少的含有Ｘ的信息， 另一方面， 它们需要尽量多的含有标签Y的信息。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;尽量少的含有Ｘ的信息，　也就是你要尽可能的不care X里的变化。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果对这个公式进行处理：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.16195372750642673&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccuj9MK3wbsOzf3xkScicZpWggyWb3NsKoHysZvmdyo8Zcznctfocc0LLecIN71TT1rXfw6Z1ClCjA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;389&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里的 H(T|H) 就是条件熵，数据本身分布的熵 H(X) 是先天确定的，那么最小化互信息，就是最大化这个条件熵。也就回到了我们刚刚说的，要有尽可能多的模型符合观测数据的同时保存对Ｙ的预测力，也正是泛化的本质，也是深度学习模型能够在变得越来越复杂的同时保有抵抗过拟合能力的关键。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;作者简介&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;
&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 28 Nov 2018 04:43:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/dkkqYSYgi3</dc:identifier>
</item>
<item>
<title>[原创]蹭热点旧文重发：一本关于基因编辑伦理问题的新书速读</title>
<link>http://www.jintiankansha.me/t/6IUfCzDvIM</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/6IUfCzDvIM</guid>
<description>&lt;p&gt;今天给大家介绍一本2017年6月13号新出的英文书，书名叫《A crack in creation》，直译成中文，就是进化中的一道裂缝。书的作者就是发明了CRISPR 的女科学家Jennifer A. Doudna.这本书在第一部分讲述了基因编辑这项“黑科技”的发展历史，比&lt;span&gt;王立铭的&lt;/span&gt;《&lt;span&gt;上帝的手术刀:基因编辑&lt;/span&gt;&lt;span&gt;简史&lt;/span&gt;&lt;span&gt;》写的更生动有趣，&lt;/span&gt;而在作者看重的第二部分，讲述了基因编辑带来的伦理问题。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.4984984984984986&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQVEVEZ9J3o0Sx9YuQYxgcYdlWa0p88jWgu1zH0yH5reTXnHNzxY0XIw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;333&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在介绍这本书之前，我先说一个我很喜欢的BBC科幻剧，叫黑色孤儿，五季的剧集，刚刚完结。关于这部以克隆人为主题的剧，我最初是震撼于女主神一般一人非饰多角的演技，但越看到后来，越发现这部剧其实说的是基因编辑和脑机接口的伦理问题，关于这部剧，我可以写长长的一篇剧评。不过这里主要谈的是《A crack in creation》这本书，所以关于剧集的内容就穿插着讲讲。&lt;/p&gt;

&lt;p&gt;看图说话，先简单的说说基因编辑是什么吧。生物的各种特性。都存储在基因中，可以把基因看成一个长长的菜单。CRISPR/Cas9 这组RNA和蛋白质的组合则是本来出自细菌中免疫系统的一个工具，它们可以精确找到不管那个生物中菜单中特定的位置，然后要么删除其中的一个步骤，要么改变步骤中的一个值，或者将步骤之间的顺序打乱。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8601036269430051&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQnfTx4HiabiaH0L8t8lTxv2ARpxa9sF8mKP0u7Q2tBvib1rFqTXIf54fwQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1351&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基因编辑会让人想起转基因，然而这俩个概念之间是有很大的区别的。转基因是引入外来的基因，相当于在一个中餐馆的烹饪手册中加上了西餐的一道菜，而基因编辑则是将蒸和炒的顺序调整了，或者将煮的时间增加了一倍，并没有引入任何外来的菜。下面的图中，左边是转基因（基因重组），我们看到被改造后的细胞中多了一组基因，而右边是基因编辑，变化的只是原来序列上特点位置的某一个碱基。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7825443786982249&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQJ06tXGYPRWvia9Tp097o4hXcWYKkAZ9jib8unu51Gz2admFtOhYlDH6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1352&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那基因编辑有什么用了，要列举起来，那可多了，还是看图说话。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7743362831858407&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQ1sA3ljdTjVKcyWu51PRMnCRjH3gsRicYv0R2FlQIQXzGuDKB2PZaw7g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1356&quot; /&gt;&lt;/p&gt;
&lt;p&gt;比如改变农作物的基因，让农作物能够抵抗虫害。关于我们常用的食物由于单一化的育种造成的病虫害威胁，可以参考：&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382547&amp;amp;idx=1&amp;amp;sn=bd1e11bcdd0d95694b54fb31fe5f5b23&amp;amp;chksm=84f3cd52b384444406a280029a3d177729c90f0d791d0fe6a7b44b74b7b336df855a0afbf0fa&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;以史为鉴，关注我们的食品安全 读《Never out of season》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.896875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQZrN5LvuJ4zDeMCoae5JTUHp8Kibb0dxDhnpIibCF6p3icC7UGBL41l8uw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;
&lt;p&gt;我们还希望能吃到的瘦肉更多，基因编辑也能做到，比如让食物更加美味，更加健康，更加高产。图中将名为MYOSTATIN的基因删除，不管是猪，牛还是兔子，都变得有更多的肌肉了。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7157660991857883&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQ61Q8NOldbZKXjDkEGG3Lm5vElfiaCCYic9n3upNfnf4oovK2vRsRiaxvQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1351&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里说的更加科幻了，异地器官移植，也就是通过改变猪的基因，让猪成为需要器官移植的病人的器官定点供应商，从而解决器官移植供给不足的问题。这方面已经有相关的新闻报道了。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5773501110288675&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQHlYpdRY0BPtjZ31vicq9YTvCOo7Q1g5a2yMSnpZicyDsRGOmia5F8wQNQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1351&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这张图讲的是通过基因编辑，让蚊子绝育，从而彻底的将蚊子从蓝星上抹去。这你也许听新闻中谷歌公司正计划做这样的事。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.706794682422452&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQJ9Hv7SdOUelKZ1Q2U3eQPa6TYzoyCuhQ9dHqwmr7Qu3WDk1WuTmGC1CXdd2EdyLlgw4598MAcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1354&quot; /&gt;&lt;/p&gt;
&lt;p&gt;基因编辑还可以治疗由基因缺陷引起的疾病，图中描述的就是将病人血液中的红细胞抽取出来，在体外进行基因编辑，再输回到病人的体内来治疗先天贫血。未来，基因编辑一定能治疗更多的由单个基因缺陷引起的疾病。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;除了上述的例子，基因编辑还可以辅助癌症治疗，帮助研发新的药品。然而既然身体中的所有细胞都可以被基因编辑所修改，那我们的生殖细胞技术上也可以修改，然而这就是作者想要讨论的伦理问题，也是《黑色孤儿》这部剧要讨论的主题。书名的英文对应的正是这个由技术爆炸引发的一道在生命的创造中的小小裂缝。&lt;/p&gt;

&lt;p&gt;史铁生在我与地坛中写道 &lt;em&gt;“&lt;span&gt;要是没有了残疾，健全会否因其司空见惯而变得腻烦和乏味呢?我常梦想着在人间彻底消灭残疾，但可以相信，那时将由患病者代替残疾人去承担同样的苦难。如果能够把疾病也全数消灭，那么这份苦难又将由(比如说)像貌丑陋的人去承担了。就算我们连丑陋，连愚昧和卑鄙和一切我们所不喜欢的事物和行为，也都可以统统消灭掉，所有的人都一样健康、漂亮、聪慧、高尚，结果会怎样呢?怕是人间的剧目就全要收场了，一个失去差别的世界将是一条死水，是一块没有感觉没有肥力的沙漠。”&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在89年这篇散文发表的时候，这里提的还只是文学奖的想象，而现在基因编辑将这一切变成了可以实现的一种可能的现实。如果我们愿意，我们可以通过基因编辑和产前基因检测，让所有的孩子都没有残疾，可我们真的愿意这样吗？这是这本书作者想讨论和引起读者思考的，作者在这本书中没有给出一个明确的回答，但希望公众能够在接触到了相关的科学知识后再去思考这样的问题。所以这里将讲三条和改变人类基因池有关的知识。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1，基因编辑不是计算机的编辑，它偶尔会犯错，也就是原本要编辑的内容是红烧肉要放一勺糖，结果编辑后发现成了粉蒸肉要放一勺糖。当想要修改的地方没有变，不想改的地方却变了，我们就说这是基因编辑中出现了脱靶。当要编辑的序列越相似，就越容易发生脱靶。这就像相对来说，我们更加不会将菜谱编辑成炒青菜放一勺糖。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2，人类的基因中很多让我们容易得病的基因，其实在特定的情况下对我们有保护作用。没有哪一个基因是完全有害的。我们擅自修改基因，会不会是在未知的情况下，改变了进化留给人类整体的保护网了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3，CRISPR基因编辑的操作很简单，高中毕业生按照说明书都可以学会，材料也没有什么特殊的。这使得监管变得困难，使得那些别有用心的人可以很方便的得到滥用这项技术。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们看到转基因在社会上引起的风波，而基因编辑则是比转基因更加具有革命性的技术跃迁。如果没有广泛的讨论，大众的误解将阻碍这项技术本来能带给社会的诸多好处，而如果大众对技术本身就不了解，那么再多的讨论也无法达到共识。我们讨论科学问题，就要回到科学本身，而不是基于科幻剧集。下面我说说黑色孤儿这部剧中的基因编辑，这部剧之所以好看，就在于作者尽可能的让剧情符合现有科学的框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;这部剧讲述的一群克隆人发现了自己不过是是一群小白鼠后找回自己的灵魂和生命的故事。之所以说是找回生命，是因为这些克隆人都病了，而病的原因正是由于合成的基因序列出现了脱靶，从而使得他们都患上了不治之症。这些克隆人要找到治疗自己的身体的方法。不过脱靶带来的也不全是坏的，本来这些克隆人是无法生育的，但有一对双胞胎却因为脱靶而可以生育。&lt;/span&gt;&lt;/span&gt;&lt;span&gt;而找回灵魂，则是她们通过自己的方式，意识到自己不是特殊的那一个克隆人，从而产生优越感而想除掉其他克隆人。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么在说《A crack in creation》这本书中要说起一个科幻电视剧，是因为我们每个人需要基于科学本身的，不带情感偏好的讨论。我可以去写长长的影评，去推荐我超级喜欢的“黑色孤儿”这部剧，但看完这部剧，我其实更想去看一本一线科学家写的严肃作品，只有拿到第一手的信息，才能在这个和AI一样。会改变我们未来的重要的不能在重要的话题上被人误导。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;扩展阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381572&amp;amp;idx=1&amp;amp;sn=246efbfd4be8445db648e6ce762c8b2d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;关于基因 你需要知道的13句话-中英双语&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382424&amp;amp;idx=1&amp;amp;sn=7c21a08d694b098e08822f38fb2b5ce0&amp;amp;chksm=84f3ccd9b38445cf18fd9c0f449396f9bc446444df4b89f6ae20e2744c3b4e1678f9073acc9c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;远离基因决定论，正视基因检测-读《DNA is not your destiny》&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;


</description>
<pubDate>Mon, 26 Nov 2018 18:04:39 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/6IUfCzDvIM</dc:identifier>
</item>
</channel>
</rss>