<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>机器学习中的8种常见疏忽以及对应的认知偏差</title>
<link>http://www.jintiankansha.me/t/R4lCgiTS99</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/R4lCgiTS99</guid>
<description>&lt;p&gt;随着机器学习工具的“傻瓜化”，越来越多的人在标准数据集上训练出了效果还不错的分类器，但这并不意味着到了真实的应用场景下，可以从头到尾做完一个项目，真实的环境中会出现数据太小，数据的特征不明晰，以及大量的缺失值等很多在标准数据集下预先不到的问题。针对具体的项目，需要了解行业的背景知识，和本领域的专家当面聊，从而有针对性的做出调整。但总有一些共通的要避免的坑，而这些坑则和我们个人的认知偏差也是有对应关系的。（本文主要针对监督学习）&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1 采样偏差&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;要做训练，先要有训练数据。而数据集的选择中，如果取样有偏，那就会导致你训练出的模型到了真实的环境中无法泛化。比如你的观测仪器会不会本身没有校准，你判定是否为合适样本的规则是否会带来某类数据无法被采用。你可以通过检查你数据的分布，是否符合预期即实际情况，来判定是否采样有偏。但做项目之前，首先&lt;strong&gt;要对什么是理想的数据集要有一个清晰准确全面的定义&lt;/strong&gt;，不然即使你以为你是随机的选取数据，你也无法避免采样偏差。&lt;/p&gt;

&lt;p&gt;采样偏差来自于我们大脑与生俱来的选择性偏差（Selection Bias），比如你会表现的比实际情况展示的更加乐观，你会忽略那些和你观点不一致的证据等。在日常生活中，你无法预先设定什么是理想的数据集，因此要维护Ta发声的权利。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2引入无关的特征&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;特征并非是越多越好，在训练数据不足的时候，引入过多无关的特征，只会导致模型过拟合。你可以根据实际问题，选择你认为相关的特征；也可以使用特征选择，选取那些对模型预测贡献度大的特征；还可以使用PCA进行数据降维，使用降维后的数据做为特征。在日常生活中，当你过分关注细节，而缺少大局观时，你就被无关的特征干扰了；自闭症可以看成是一种极端情况，大脑选取了过多的细节，最终导致其无法做出有用的预测。而面对决策时的拖延和犹豫不决，也是大脑中了收集更多无关特征的毒。而特征选择中的方法，也可以借用到生活中，比如你选择是否接受一个工作时，可以列出这项工作的优缺点，再进行比较，但在此之前，你需要先筛选一下你列出的优缺点是否对你的成长与幸福有关系，然后去掉那些关系不大的项。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 数据泄漏&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果你在预测是时候，选取的特征包含了待预测标签的变种，例如你的目的是预测一个人是否感冒，然后你选取的特征中包含Ta是否正在吃感冒药，那你的预测模型结果会出乎意料的好，但这样的模型在实际中是没用的。在特征选择时，要避免那些能够直接导出待预测标签的特征。现实生活中的数据泄露对应的是因果倒置。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4 错误的补全缺少的数据项&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当数据项存在缺失时，常见的方式是用平均值去补齐，或者填入某一个和已有数据相近的随机值。但如果出现数据项缺失的原因并非是随机出现的，这样做就会带来问题。例如如果在数据的持续记录过程中，有些参与者会退出实验（比如医疗数据中患者死亡），从而造成数据项的缺失。这时就不应该使用缺失值补齐或数据插值。而应该选取那些能够包容缺失值的算法。或者将对包含缺失的及不包含缺失的数据分别训练一个分类器。日常生活中，当我们面对缺失的数据时，想当然的补上一个我们最常看见的值，这样大多数还不如补上平均值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 忽略对数据进行正则化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有些模型，例如KNN或支持向量机，对于极端的数据点十分敏感，如果使用原始数据来处理，那么模型关注的就只是那些包含极端大值的特征，但这些特征并不是对预测最有效的，在真实情况下，极端的值不一定会出现，从而影响模型的泛化能力。为此要将数据进行正则化，使其具有相同的方差和均值。日常生活里，我们也容易被极端值迷惑，比如飞机明明是最安全的交通方式，但由于飞机失事后的影响很大，导致人们高估的飞行的危险，因此你要做的是将全部数据拿出来，统一来看，而不是只关注一个数据点。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6 忽略离群点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在清洗数据时，你会发现有极少数数据点很反常，这时你会想将这些离群点拿掉，应该不会有多少影响。这时你要注意的是分析数据点离群的原因，是由于测量仪器的bug，导致了出现了明显不可能出现的情况，还是这些数据就可能会有一些离群点。如果是前者，那离群点应该去掉，但如果是后者，那在训练数据集中去掉离群点，训练出的模型到了包含离群点的真实数据中，还是会难以泛化的。不同的模型对于离群点的容忍程度也是不同的，adaBoost会对离群点给予超高的权重，而决策树对是否包含离群点则不那么在乎，因此当你无法确定为何会出现离群点时，要选择那些对离群点更容忍的分类器。日常生活中，我们总会遇到那些特立独群的人，听到那些离经叛道的观点，这时我们该不该选择性的忽略，需要你来判断这个离群点到底是因何而生，完全的包容或排斥非主流的观点，都不是应选择的道路，你要做的除了甄别异想天开的背后是否有证据支持，还要让自己的价值观不那么容易被极端观点说左右。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7 忽略特征间的多重共线性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果训练数据中的特征存在多重共线性，例如特征A和特征B与C相关，特征B又与特征C,D和E相关，那么一个值的细微变化，就会导致众多特征的变化，从而使得训练数据中的由测量不准确带来误差被指数级的放大，从而使得模型变得对误差敏感，从而影响泛化能力。为此要检查训练数据的特征间是否俩俩存在着相关性，如果存在严重的多重共线性，要去除那些冗余的特征，或者将相关的特征组合起来，作为一个特征来分类。日常生活中的多重共线性是人群中的人云亦云，你以为你是听从多数的意见，但如果你听到的信息追诉起来是来自一个人，那这个信息就不应该被当成是群体智慧。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8 选择了错误的评价指标&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;对于分类器，评价的指标有很多，常见的例如查全（recall），查准（precision）已经ROC，F1等。但对于具体的应用场景，应该选择不同的评价指标，有时假阳性是不可容忍的，有时则是一定要保证假阴性足够的低。如果你没有选择适合应用场景的评价指标，训练好的模型也会由于缺少优化目标，而无法实用化。而日常生活中，在不同的阶段，就该有不同的奋斗目标，不然你将无法确定自己是否成功。&lt;/p&gt;

&lt;p&gt;总结一下，本文总结了8种监督学习者的常见疏忽及解决方法，针对每种，引申了其在日常生活中对应的认知偏差，供读者自查自勉。这8种疏忽分别是：&lt;/p&gt;
&lt;p&gt;1 采样偏差&lt;/p&gt;
&lt;p&gt;2引入无关的特征&lt;/p&gt;
&lt;p&gt;3 数据泄漏&lt;/p&gt;
&lt;p&gt;4 错误的补全缺少的数据项&lt;/p&gt;
&lt;p&gt;5 忽略对数据进行正则化&lt;/p&gt;
&lt;p&gt;6 忽略离群点&lt;/p&gt;
&lt;p&gt;7 忽略特征间的多重共线性&lt;/p&gt;
&lt;p&gt;8 选择了错误的评价指标&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383835&amp;amp;idx=1&amp;amp;sn=c937dbe49d0f6320fed53d727bf63071&amp;amp;chksm=84f3c65ab3844f4cbcc834b0d881069254a193a0f3799282f583c256920bb3582c173b29d863&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习高维数据分析中那些一定可以避开的坑！&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 02 Feb 2019 00:03:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/R4lCgiTS99</dc:identifier>
</item>
<item>
<title>谷歌大脑的“世界模型”简述与启发</title>
<link>http://www.jintiankansha.me/t/ppyxZ1iWM6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ppyxZ1iWM6</guid>
<description>&lt;p&gt;我们的视觉看到什么，部分取决于大脑预测未来会看到什么，例如下图中，如果你预计要看到突出的球体，那也许你就会看到，如果让机器也具有了这样的能力，会带来什么了？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkCLP1ia37xna8aoibHPOOBCLFQTOzxuLjqpcN9xVEbSUSRn1VPiaXqOTHg/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;205&quot; data-cropy1=&quot;13&quot; data-cropy2=&quot;218&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmO13hGkUN4v5e3Eol7Rh0t5ccR205fP1HDW3YDsTMzeXsalwAfjgicg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;205&quot;/&gt;&lt;/p&gt;

&lt;p&gt;18年谷歌大脑提出“世界模型”(World Models)可以在复杂的环境中通过自我学习产生相应的策略，例如玩赛车游戏。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7259615384615384&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmuuDIugmtDicLnq0dNH7YmfVBpw6FUnghDlVMd1PlwYYqibu2f7v7mBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;416&quot;/&gt;&lt;/p&gt;

&lt;p&gt;下面是世界模型的整体架构:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7060185185185185&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkPwWNeqEkezB3PJqGoXtBmibptFBfYIoE0aw4bXxmDySfZcz7DCG8xyA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;432&quot;/&gt;&lt;/p&gt;

&lt;p&gt;整个模型分为3个组件：视觉组件（V），记忆组件（M），控制组件（C）。视觉组件V用来压缩图片信息到一个隐变量z上（其实只是一个VAE编码解码器）：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.6&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkrbBKiaPqBkmPQf3bAYrmU37icPH8icpIrskk3GDib2YUNUnv6iaIz7PibAUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;记忆组件M的输入是一帧帧的游戏图片（论文中的一帧图像似乎叫一个rollout），输出是预测下一帧图像的可能分布，其实就是比一般LSTM更高级一些的MDN-RNN：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4832&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkT0zvQibp5s1TH0RPNUfNfaM6x7602Lfw2kibx0aXn9N2mqh5rR0na1kA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;625&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后控制组件C的目标，就是把前面视觉组件V和记忆组件M的输出一起作为输入，并输出这个时刻智能体agent应该做出的动作（action）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;在所谓的“世界模型”，其中的组件模型几乎没有是谷歌大脑自己创新研制的。但世界模型会很大提高强化学习训练稳定性和成绩 从而使其与其他强化学习相比有一些明显优势，如下表所示;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4444444444444444&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkicKDOBicjWeKZEElnsCzm1qzbbOuBfl5shc6gTfyWAbFOCe5ZQRjsjHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;558&quot;/&gt;&lt;/p&gt;

&lt;p&gt;世界模型有如下的3个特点&lt;/p&gt;
&lt;p&gt;1. 模型拼接得足够巧妙，这个巧妙的拼接模型做到所谓的世界想象能力，就是模型在学习时，自身对环境假想一个模拟的环境，甚至可以在没有环境训练的情况下，自己想象一个环境去训练。其实就是我们人类镜像神经元的功能。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8233532934131736&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkpsQ5KpvR3byIp6ibS2QHKg40F6oRiaMXtLpglgFbz009kBiby8qibdHFug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;

&lt;p&gt;2. 抓住了一些“强视觉”游戏的“痛点”。记忆组件M中的RNN是生成序列的能手，所以根据之前游戏图像再“想象”一些图像帧应该不成问题（RNN生成一些隐变量z，再根据隐变量z，由视觉组件VAE的decode生成的图像帧即可）。所以对于“强视觉”的游戏，把RNN的记忆能力用在视觉预测和控制上是个好主意 。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkMOk09Pib2ia2DDq3HBhuZWqX4NEpsuzbvOx3wDDs0P9kCkVWZBxhEqZw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;334&quot; data-cropy1=&quot;9&quot; data-cropy2=&quot;251&quot; data-ratio=&quot;0.7245508982035929&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkfDoibG4whRIFVy2NAKAibSORrc71CBLG15xdf9WTv9BjOUWbvaxuugPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;


&lt;p&gt;3 不同于我们常见的“不可生”智能算法，例如遗传算法和进化策略只是强调了基因的“变异”与在解空间中进行搜索，神经网络只是固定网络结构；而生物界的基因却可以指导蛋白质构成并且“生长”。如果基因可以构造自身个体，外部环境和个体情况也可以反过来影响基因，而我们的模型都太固定呆板了，模型结构不能随内部隐变量改进，当然最佳的设计形式也许谁也不知道。而世界模型做到了让在内部”幻想“的环境中产生的策略转移到外部世界中。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8567073170731707&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrksicaWK7Ls5EJMsF6hU4iatAd6MG7orOxwHCIZAumfzWvyhjRwWlJU1Dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;328&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;最后简单看一下&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;世界模型&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;的训练过程：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5024752475247525&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkLmQXpD9K5Ic89FCTQT7LA9RlljB2QmHj2E10dv56IWNjqszIhA6MpQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;404&quot;/&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;world models代码&lt;/span&gt;&lt;span&gt;基于&lt;/span&gt;&lt;span&gt;chainer&lt;/span&gt;&lt;span&gt;计算框架，步骤如下:&lt;/span&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1. 准备数据集，随机玩游戏生成训练帧（rollouts意思应该就是多少帧）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python random_rollouts.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --num_rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;10000&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;2. 训练视觉组件V，即前面提到的VAE：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python vision.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;3. 训练记忆组件M，即前面提到的RNN：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python model.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;4. 训练控制组件C，即前面提到的CMA-ES算法（其实就是支持更复杂输入和更新的ES）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python controller.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --lambda_&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mu&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--trials&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--target_cumulative_reward&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;900&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--cluster_mode&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;5. 测试训练结果：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python test.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--record&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://arxiv.org/pdf/1803.10122.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://github.com/AdeelMufti/WorldModels&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文经作者授权，转载自David9的个人博客，著作权属于“David 9的博客”原创，&lt;strong&gt;如需转载，请联系微信: david9ml&lt;/strong&gt;。原文地址： http://nooverfit.com/wp/谷歌大脑的世界模型world-models与基因学的一些思考/#comment-3444&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383931&amp;amp;idx=1&amp;amp;sn=5349aed7549713d893e08f07d0d44859&amp;amp;chksm=84f3c63ab3844f2cb0fbc01efe877031bbcbe25de23d6637f4e35e8e775b39806940e7084b4a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;多任务学习的未来之路&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383923&amp;amp;idx=1&amp;amp;sn=d717ecdc31f055bdcf7d8381c9e8b5c9&amp;amp;chksm=84f3c632b3844f243769de14d893e09d7db3fd4c43367a3eb61ed54ab0720aa2134750b4922a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Nature子刊机器智能综述-通过神经进化（neuroevolution）设计神经网络&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 28 Jan 2019 03:38:08 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ppyxZ1iWM6</dc:identifier>
</item>
<item>
<title>三步理解Alpha Star背后的黑科技</title>
<link>http://www.jintiankansha.me/t/JnQ8WzRZsZ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/JnQ8WzRZsZ</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;efnag-0-0&quot;&gt;在Alpha Zero取得围棋领域的胜利之后，最近的大新闻是： AI又下一城，拿下了重磅即时战略游戏星际争霸。&lt;/span&gt;为什么我们说这个胜利的重要性不亚于当年的阿法狗？ 这背后的黑科技对我们又有怎样的影响。 我虽不懂星际，但我懂得强化学习， 此番为大家讲述。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiazKy0ib9CCr73OryQ3PFfptA4Xv89eCvxibKTicRicb6goZ6cDWI6UYqibJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7katb-0-0&quot;&gt;首先， 星际争霸， 是一个与围棋有着本质不同的游戏-那就是星际争霸是非完全信息下的博弈游戏。 这里的难点一个是&lt;strong&gt;不完全信息&lt;/strong&gt;， 其次是需要&lt;strong&gt;远期计划&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;另外就是&lt;strong&gt;实时性与多主体博弈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;172pp-0-0&quot;&gt;何为非完全信息， 你看所有的即时战略游戏， 从红警到星际，你没有刚刚到达过的区域都被一团战争迷雾所笼罩， 那里可能有对方的军队在采矿， 或者大批小兵集结， 这可能与我此刻的决策关系很大， 但是却不为我所知。 这与围棋这样的游戏有着本质的区别， 因为围棋这样的游戏， 即使策略在复杂， 你方和我方的情况都是一目了然的， 围棋游戏的复杂体现在策略空间的巨大导致的维度灾难， 然而始终都是你知我知的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a6iej-0-0&quot;&gt;而星际这样的游戏， 却更加符合我们的真实生活， 如果你把真实的生活看成一场即时战略游戏， 从选择大学专业， 找工作， 到炒股， 你所了解到的信息， 永远是你需要知道的决策信息的一小部分， 你的对手，永远在黑色迷雾里行动。  这种黑雾， 对你的第一个挑战就是， 你要应对永恒的不确定性， 即使你有一个比目前水平强大10倍的大脑，  你依然生活在不确定性的迷雾里， 因为它是由于客观信息缺失造成的。 如果过度的渴望精确预测， 还可能由于在explore VS exploit 的权衡中偏向一端而走火入魔。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b6tfh-0-0&quot;&gt;而多体博弈， 体现了策略建模的复杂性， 我方的最佳策略取决于有方策略，对方策略，以及盟友的策略，还有兵种的相克，组合带来的1+1大于2的过等等，游戏尽可能的模拟了真实的战争差。&lt;/span&gt;星际争霸的挑战还有超大的动作空间（可能性超过围棋全部组合数百个数量级），游戏长度很长， 而且游戏初期做出的决定会影响最终的成败。但最关键的，星际相对围棋最大的技术挑战在于&lt;strong&gt;非完全信息博弈&lt;/strong&gt;， 也是它和真实世界最接近和最有价值的地方。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3196405648267009&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawia1PlzceSHia3eNAxEB5ic2tttcmGryLTlL46PAJDG6xdumic9pkibesEgHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;779&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是alpha Star 如何处理从输入的图像到神经网络提取的特征，最终到做出的决策（右下）以及预测的实时预测的胜率（右上），而我们最关注的是中间的部分。为了让大家了解alpha star 背后的底层技术， 我们就从最基本的ABC开始， 给大家引入：&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;58bo6-0-0&quot;&gt;A， 强化学习 + 深度监督学习框架&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;deuq9-0-0&quot;&gt;首先，这几年的深度学习进步主要用到的是监督学习技术， 监督学习容易取得突破时因为它们最好训练，监督数据自带正确答案， 机器每分钟都在学到有用信息。 而如果我们要首先更普世的人工智能， 监督学习时不够用的， 因为智能体行为的本质时根据未来的目标制定当下的行为策略，这点监督学习无能为例， 而是需要强化学习的框架。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawia7baISvMSb0eiblm6MWBIwFkjKnEC1NuEGxNCT0HNgnia9SqEicTwu4ibjA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eg361-0-0&quot;&gt;想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，之后由程序员来告诉AI该做什么，但如果有强化学习就是由AI自己决定逃跑还是战斗，哪一个更容易通往通用智能，是显而易见的，对于老虎在你面前，能够提前预测出老虎即将到来，对智能体的生存是有意义的，但对于更加复杂的，无法由程序员事先编码的情景，就需要靠强化学习来决定了你的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e4ej5-0-0&quot;&gt;然而无论是强化学习还是监督学习， 其背后都是神经网络， 这个模拟人脑智能的抽象模型作为载体实现， 神经网络把一种信号，转化为另一种信号， 实现信息的抽取和转化。在Alpha Star里我们用到LSTM这种基于时间反馈的神经网络作为内核。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4b5ei-0-0&quot;&gt;当然， 强化学习是一个对未来的优化问题， 未来有多长， 几步， 几百部， 还是几万部？  如果你走完了一万步才知道你最后的结果对不对， 那么这个信号根本不足以支撑机器的学习， 因为学的太慢。 就好比一个人在信息的荒漠里行走， 看上去一直在劳苦， 实则劳而无功。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2j32-0-0&quot;&gt;DeepMind  强化学习教父David Silver 喊出了 AI = DL + RL ， 意思就是为了解决这个问题， 我们可以先用监督学习让神经网络学习，&lt;/span&gt; &lt;span data-offset-key=&quot;2j32-0-1&quot;&gt;监督学习机器每时每刻都知道正确或者错误， 而后再上强化学习， 这样让机器学习到从当下到未来的战略思考。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2feut-0-0&quot;&gt;这就是实现alpha star最基本的框架。  监督学习为首，强化学习跟上。 那个LSTM网络先要不停的看以往人类选手的比赛记录， 并预测它们的行动和比赛结果， 直到得到一个baseline水平。  这点， 也同时让我们想到Karl Friston所提出的自由能原理和predictive coding法则。 智能体必须把预测外部世界的变化作为核心目标来建立自己内在的representation（世界表征）。&lt;/span&gt; &lt;span data-offset-key=&quot;2feut-0-1&quot;&gt;这里的监督学习过程， 可以看作不停的预测外部世界， 减少预测误差，建立世界表征的过程。&lt;/span&gt; &lt;span data-offset-key=&quot;2feut-0-2&quot;&gt;下图我们可以看到alphastar 在监督学习监督就已经达到一定的水平&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiapJhTxng8Lib0QtTkckibeKMrZ0kWMjWdZYzMaibkeL85WbRX5uWGc1eAQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图片描述训练成绩和训练时间的关系， 我们看到在0之前的训练代表机器单纯靠监督学习达到的基准线， 而后是强化学习阶段，我们看到机器在第8天开始超越TLO的水平（世界排名44）， 在第14天开始达到MaNa水平（职业排名13）&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;akkah-0-0&quot;&gt;B  马尔可夫 vs 局部马尔可夫决策框架&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8m40d-0-0&quot;&gt;强化学习本质上还是一个数学优化优化算法。 它的起初的成功得自一个称为Markov决策的框架。&lt;/span&gt;&lt;strong&gt;所谓的马尔可夫， 就像物理把原子看成一个行星轨道模型一样， 是一种对真实问题的霸王硬上弓的强行简化， &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先你把真实过程分解为无数离散的时刻，这样， 无论问题多复杂，都可以分解为状态（state）- 行为（action）- 奖励（reward）- 策略（policy）四要素。状态是每个时刻你拥有的和决策相关的信息总和，行为是你的决定， 奖励是你的此刻收益。 而策略就是从状态到行为的对应关系。后所谓的马尔可夫，就是说， 当下的行为决策只和当下的状态相关。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaTDKskFuTIMawmdESH1vOSyXj9W8icAyIGA77EH7whe670OXWialwPEtg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eetub-0-0&quot;&gt;其实做马尔可夫决策模型的人脑子里的世界就是一个经典的方格迷宫。  你的状态就是你所在的方格, 你的行为就是上下左右走，你的奖励是中间的加号。 行走的过程每一步都只与上一步相关，此刻拥有绝对信息（位置），因此可以简化为下图b的马尔科夫决策图框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7899159663865546&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaojHBBWwDOccHDnveS2ZEVlhbQZjr8EDJERgIwTsUMcrO9AngW8ku0Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;238&quot; /&gt;&lt;/p&gt;

&lt;p&gt;a&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cpmor-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  看到这个你是不是觉得和星际争霸相去甚远我在骗你呢？ No! .   事实上， 如果你进行比较凶狠的简化，还真的差距不大。  首先， 星际游戏给它拆解开无非包含这样的几个步骤：  1，收集资源   2， 建立生产单元   3， 建立军事单元和武装  4，  部队集结并摧毁敌方生产和军事单元 。 所谓的战略，首先是在这些不同步骤（状态）之间的顺序和时间调配。 当然你要说地图实在太大太复杂了， 你不要忘了深度学习的核心思维， 特征抽取， 请看下图， 机器眼里的星际争霸地图。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaJD9jtd1SCcQUG6rqsIfj2fe1yOjLbTwGYNtjTibfUkEgMsmoeySeaqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8230h-0-0&quot;&gt;机器眼里的星际争霸地图， 就是这样一个离散的单元特征图。上图左边的大图每个格子里有不同的单位，显示整体的布局， 然后对于不同的核心信息， 我们可以又有右边分开的格子图表示（比如某种敌方单元的分布）。  大家是不是想到了卷积网络眼中的特征图？  如果你把这样的特征图看成机器真正“行动”的地图， 你看是不是它就和刚刚那个机器走方格游戏的距离大大拉近？  我们有离散的状态， 可以通过一个个的格子空间表示，然后可以决定采用的动作，此刻的动作决定下一刻的状态， 这些都和走方格游戏类似， 只不过特征很多很复杂，一次要控制多个单元而已。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ekd1e-0-0&quot;&gt;稍等， 此处还有一个特点是刚刚说的马尔可夫框架所不能包含的，那就是特征图里面有那么多黑色区域啊！ 这些黑色区域代表了未知，未知里面敌方的部队可能在集结， 说明当下的状态不包含决定我的决策的所有信息了 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dabb9-0-0&quot;&gt;那怎么办呢？ 此处马尔可夫框架很自然的扩展到局部马尔可夫框架。不要忘记神经网路是有想象力的，它可以脑补黑色区域里的东西。 如果你通过想象填充了那些黑色区域， 你就再一次回到了马尔可夫的世界。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1fc1l-0-0&quot;&gt;怎么脑补？ 你想象一下你自己如果要预测一下明天发生什么， 你第一步要做的是不是会议昨天做过什么呢？  是的， 我们的记忆， 不是帮助你追忆似水年华，而是更好的预测未来（此处自行联想predictive coding 预测编码理论）！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8r5je-0-0&quot;&gt;我们说神经网络是一个巨大的函数模拟器， 如果你要用一个神经网络把过去和当下的信息综合起来， 然后拟合未来的画面，这个神经网络就是循环神经网络RNN，&lt;/span&gt;然后用神经网络来学习这个函数，这就自然的引入了深度强化学习。通过数据可以模拟函数，有了函数就可以把值函数的问题解决。它是类似模仿生物工作记忆的原理，信息放在记忆里面，将其作为决策的依据。RNN的网络结构和前向神经网络差距并不大，只是多了一个时间转移矩阵，即当下隐藏的状态再到下一步隐藏状态的连接。&lt;/p&gt;

&lt;p&gt;这个连接实际上是传递当下的信息到下一刻的过程，这个传递过程中包含了记忆，我们可以把过去的信息往将来不停的迭代，于是神经网络细胞当中就含有过去很多时刻的记忆，其实就是生物的Working Memory。LSTM只是在RNN的基础上多了一记忆细胞层，可以把过去的信息和当下的信息隔离开来，过去的信息直接在Cell运行，当下的决策在Hidden State里面运行。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5se5a-0-0&quot;&gt;加入RNN（LSTM）的神经网络，可以极好的根据对过去信息的整合来预测那些黑色区域里可能是有敌军部队还是有矿藏， 就可以用来驾驭星际争霸。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiageYJiauSx1oe3OulicC1a9IecKMp83HTe1nbNHeXPuCMfOHhcBDE58ibw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN随时间展开的反向传播， U是隐层连接&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaNMAhLLkE8QWmb3Ax5CQr57q0GOJIjrTsABYstkk8uKZ08Q2vQvFAEA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LSTM： 加入Cell&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;5se5a-0-0&quot;&gt;lstm网络中motif（重复单元），通过增加memory cell 引入动态的可训练的记忆单元&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;6gtru-0-0&quot;&gt;C  技术细节创新&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6gtru-0-1&quot;&gt;刚刚说的大框架， 可以帮助我们解决很多动作类游戏（doom）或真实世界的空间导航问题（deepmind： navigation in complex world）。 但是没有一些关键细节的创新， 依然无法在星际这样的游戏里取胜。 我们看看星际具体使用了哪几个新技术呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d4ugo-0-0&quot;&gt;1，&lt;/span&gt; &lt;span data-offset-key=&quot;d4ugo-0-1&quot;&gt;多任务联合训练：&lt;/span&gt; &lt;span data-offset-key=&quot;d4ugo-0-2&quot;&gt;星际争霸最大的任务“赢得战争”里其实包含无数小的任务单元，比如对不同种类矿物的采集， 对不同敌军单位的攻击等， 这些可以分开训练，分别设定奖励函数， 这些小的任务会让整个游戏的学习加速很多。 这种把最难的任务分解为很多基本任务分开训练的思路， 应该可以被需要应用强化学习的工业应用大量采纳。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1vifo-0-0&quot;&gt;2.  &lt;/span&gt; &lt;span data-offset-key=&quot;1vifo-0-1&quot;&gt;多体博弈问题，&lt;/span&gt; &lt;span data-offset-key=&quot;1vifo-0-2&quot;&gt;保留所有训练中产生的历史版本， 并在其之间进行对战， 这有点类似于生物“基因池”的概念。 这个过程里，我们可以做到不会把某些个体学到的有效信息轻易遗忘，而是在多体博弈里有效的运用所有已经学会的东西， 同时选择出那些最强的策略， 最终的训练结果是一个达到纳什均衡状态的最强策略池， 这不得不让人联想到进化算法， 让人想到不同AI算法的融合趋势。 这个不同策略的博弈过程， 也是alphastart对未来的智能算法最有启发的部分之一。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cbi8h-0-0&quot;&gt;3，&lt;/span&gt; &lt;span data-offset-key=&quot;cbi8h-0-1&quot;&gt;RNN联合注意力attention框架改进&lt;/span&gt;&lt;span data-offset-key=&quot;cbi8h-0-2&quot;&gt;： Pointer Networks。我们直到所有RNN（lstm） 在当下序列建模里的成就都和attention机制的结合有关。 而pointer network又对之前的attention进行了改进， 用输出的结果直接反向影响注意力，决定需要注意的对象， 并且这个输出的尺寸是可变的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5083333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaBUzneQuGkicYqYHc7AzA44wXBR2XUGEhmicMtBhFbTPg7yXyMUiaapMag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;pointer network， 根据网络输出反向决定attention焦点(所要关注的输入信息)的框架， 左边是传统seq2seq网络， 右边是新引入的框架。 Pointer Networks 2017&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1ue6-0-0&quot;&gt;4  &lt;/span&gt; &lt;span data-offset-key=&quot;1ue6-0-1&quot;&gt;自回归模型。&lt;/span&gt; &lt;span data-offset-key=&quot;1ue6-0-2&quot;&gt;这里就牵涉到策略的细节表示， 星际游戏的策略是典型的动作序列，而且在不完全的马尔可夫决策框架下所有决策取决于历史， 因此对策略的表达方法用到了自回归的思想， 首先把当下的动作决策表述为所有过去动作为条件的一个条件概率，然后假定这些过去的动作对当下的影响是独立的， 所以可以写成连乘的形式：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.19166666666666668&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaPsu2D9JVSP38a8pFlusQp2C810MzfVFGFPsFrHKCF46rXEict7utFJQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8km2i-0-0&quot;&gt;5， &lt;strong&gt;关于和人类的成绩比较&lt;/strong&gt;： 有的人在阿法start的结果刚出来的时候说这应该是机器可以在单位时间能够操控更多的动作导致，事实上deepmind对机器的行为做了诸多限制， 让它不是单纯因为“速度快” 这种愚蠢的原因赢得人类。&lt;/span&gt;&lt;/p&gt;


&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span data-offset-key=&quot;6m1sj-0-0&quot;&gt;阿法star对我们的启示：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d5m4d-0-0&quot;&gt;这一次的alpha star，依然是有人说这不过是谷歌的广告， 强化学习目前是&lt;span&gt;处在上升期的人工智能技术， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d5m4d-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.67&quot; data-w=&quot;600&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fqjdo-0-0&quot;&gt;然而我们毫无疑问看到了AI在非完全信息博弈， 这个与真实世界的大多数任务非常接近的框架所取得的巨大进步。  AI是否会很快开始指导一场真正的人类战争？ 我想依然没有那么容易， 一个最核心的问题依然是电脑游戏是可以在大脑上无限取得数据的， alphastar在短短时间里经历了人类选手200年的训练才取得了如此的成就。 想象一下， 如果到了真实世界， AI和人类的数据取得速度差不太多， 那么谁强谁弱呢？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4igq-0-0&quot;&gt;因此， alphastar虽然诠释了AI的一个新的篇章， 却无需让我们太恐慌， 我们想的更多的应该是， 如何用alphastar里分解出的新的技术， 真正提高RL在工业里的使用效率， 比如&lt;strong&gt;如果做出能够进行更好多体协作，甚至直接和人进行配合的工业机器人，如何让它帮助我们得到更好的能源调控策略一类的具体任务。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4pked-0-0&quot;&gt;而真正的AI下一个瓶颈的突破， 一定要提高&lt;strong&gt;对数据的使用效率， 和在不同任务之间的泛化能力&lt;/strong&gt;， 如同我在上一篇文章，&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383931&amp;amp;idx=1&amp;amp;sn=5349aed7549713d893e08f07d0d44859&amp;amp;chksm=84f3c63ab3844f2cb0fbc01efe877031bbcbe25de23d6637f4e35e8e775b39806940e7084b4a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span data-offset-key=&quot;4pked-1-0&quot;&gt;&lt;span data-text=&quot;true&quot;&gt;多任务学习的未来之路&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span data-offset-key=&quot;4pked-2-0&quot;&gt;里所说的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;铁哥的强化学习特训课&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383605&amp;amp;idx=1&amp;amp;sn=f734603530cbeda03bd5c7ce3d5b83ba&amp;amp;chksm=84f3c974b3844062b819e778faa5fa4050539874ede0a1586eb971732828bee4dea3ab91071c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;当RNN碰上强化学习-为空间建立通用模型&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383842&amp;amp;idx=1&amp;amp;sn=b4196485b009ebd21e7e0d8db1e2cd61&amp;amp;chksm=84f3c663b3844f756cb7f0547b8f1acf03ac6aeff743d8a37c4d3024b0c30dec19afb8f2298e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Alpha Zero登上Science封面-听铁哥浅析阿尔法元&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;</description>
<pubDate>Sun, 27 Jan 2019 00:02:10 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/JnQ8WzRZsZ</dc:identifier>
</item>
</channel>
</rss>