<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>5分钟读懂强化学习之Q-learning</title>
<link>http://www.jintiankansha.me/t/SrT1aw8yLw</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/SrT1aw8yLw</guid>
<description>&lt;p&gt;强化学习的难点，在于其引入了时间这个维度，不管是有监督还是无监督学习，都是能获得即使反馈，但到了强化学习中，反馈来的没那么及时。在周志华的《机器学习》中，举过一个种西瓜的例子。种瓜有很多步骤，例如选种，浇水，施肥，除草，杀虫这么多操作之后最终才能收获西瓜。但是，我们只有等到西瓜收获之后，才知道种的瓜好不好，也就是说，我们在种瓜过程中执行的某个操作时，并不能立即获得这个操作能不能获得好瓜，仅能得到一个当前的反馈，比如瓜苗看起来更健壮了。因此我们就需要多次种瓜，不断摸索，才能总结一个好的种瓜策略。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.8597222222222223&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了应对时间带来的不确定性，就需要一个框架来量化时间的流逝对我们关心奖励有怎样的影响。按照最简单的线性模型，我们首先确认要引入那些特征，首先是前一个时间的得分，其次是新发生的事件对奖励的影响，由于我们对未来的奖励看的不如现在的重要，因此可以引入折线率，折现率越高，说明我们越处于游戏的早期，对未来的关注也越多，这道理就如同我们在年轻时更要做长久的规划。同时在更新策略时，也会有快慢之分，将其称为学习率。由此得出了时间差分学习(Temporal Difference)，简称TD方法的更新公式：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.14660831509846828&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatGHyEQuonYibg2174nHUCFhLdAGRQvSVal7TrHezmsKZ01ErVogJuYA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;457&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该公式描述了给定一个策略，该怎么去更新下一个时刻的估值函数，其中的V代表估值函数，下一个时刻的估值乘以折现率，再减去当前的差值，代表了一个策略的间接影响，可以看成是战略决策，再加上下一个时刻能立即获得的奖励，就是智能体（agent）应该关注的策略的影响，最后对此乘以学习率，用来控制随机性的影响，既要避免由于学习率过低导致的智能体学的太慢，也要避免学习率过高导致智能体矫枉过正。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;由于在当下，你并不知道下一时刻的估值函数，所以你要做的是对其有一个尽可能准确的估算，这个估算被称为Q value，对应的算法称为Q-learning。如果你是用神经网络得出对未来value的估算的，那你使用的算法框架就从强化学习变为了深度强化学习。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.7262357414448669&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaiaD8kfEVYfNkfJgLhYEbOzyOo6TqicKnb0NGgYfL5ib6Kkb8jOMt56xTg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;526&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是一个具体的例子，将一个结冰的湖看成是一个4×4的方格，&lt;span&gt;每个格子可以是起始块（S），目标块（G）、冻结块（F）或者危险块（H），目标是通过上下左右的移动，找出能最快从起始块到目标块的最短路径来，同时避免走到危险块上，（走到危险块就意味着游戏结束）为了引入随机性的影响，还可以假设有风吹过，会随机的让你向一个方向漂移。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.466893039049236&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaacrLdcvu8QwtBo9mvVzf3KZ3wZGMospic8OhNsuz9sdlNvfjjicP53hdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;589&quot; /&gt;&lt;/p&gt;
&lt;p&gt;左图是每个位置对应的Q value的表，最初都是0，一开始的策略就是随机生成的，假定第一步是向左，那根据上文公式，假定学习率是0.1，折现率是0.5，而每走一步，会带来-0.4的奖励，那么（1.2）的Q value就是 0 + 0.1 ×[ -0.4 + 0.5× (0)-0] = -0.04，为了简化问题，此处这里没有假设湖面有风。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4200626959247649&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaibSXpgch86ApiaPm5EBrQkr7KvJxmlPOoYxDa4icOJiajK8Oyl55fuhd7w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设之后又接着往右走了一步，用类似的方法更新（1，3）的Q value了，得到（1.3）的Q value还为-0.04&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4054878048780488&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaTtSt6Siceq5eg320L4ibrKfiaVFKloHuhiaAXS3ficPIheOXfH8W0Mskqcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;656&quot; /&gt;&lt;/p&gt;

&lt;p&gt;等到了下个时刻，骰子告诉我们要往左走，此时就需要更新（1，2）的Q-value，计算式为：V(s) = 0 +0.1× [ -0.4 + 0.5× (-0.04)-0) ]&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.46333853354134164&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaabAV71N9QPLA95sNdHmxw735yYwBd1JZ4fVxG4fMuYaHrlhLnDrnBcA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;641&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;从这里，智能体就能学到先向右在向左不是一个好的策略，会浪费时间，依次类推，不断根据之前的状态更新左边的Q table，直到目标达成或游戏结束。这就是TD learning的基本步骤，通过多次的实验，智能体掌握了在不同位置下，相应的策略的估值分，从而解决了将较远的未来映射到当下的对不同策略的激励这个强化学习的核心问题。&lt;/p&gt;

&lt;p&gt;根据是否亲自尝试不同的策略，Q learning可以分为在线和离线俩者，用学下棋来举例，前者是AI通过自己和人类选手下棋或者自我对弈来提升，而后者AI不操作只观察他人下棋的棋谱，下面看看再离线（off-line）的Q learning中，Q value更新的公式又有了怎样的改变。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.12110726643598616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa56EFEGaa5FzYynegbiaG0HHB5VXQericOGjxf1VyCWAGKjQyTyzeCPSQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;578&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和之前的公式对比，最大的不同是未来的Q值是所有行动/策略对应的未来Q值中最大的那一个，这代表着模型根据已有的知识，选择了局部最优的那个行动，通过不断的优化Q table，使得这样一个只考虑一步的最简单型启发规则，也能学到全局相对较优的策略。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.292626728110599&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatd4f6hZKGMOiasVOoUG86yF7sKictnZPX3LCNib7BPjlyMxJYAiabw8htA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;434&quot; /&gt;&lt;/p&gt;


&lt;p&gt;还是冰湖的案例，假设在训练的循环中，当前智能体已经学会了在（3，2）这个点上，向左和向右走对应的估值，此时模型要做的是去判定利用当前的知识，还是去探索未知策略的影响，探索是为了发现环境的更多信息，而当探索进行到了一定的程度，就需要根据已知信息去最大化奖励值，在Q learning中，通过一个0-1的参数来用随机性调控探索和开发的权衡。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.54&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaRggot5LiaL2n2jGVV7aOQAQp5L5WXiaqkvtl1CFlv4FLJJuGW0H3IZDA/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;假设骰子告诉智能体应该选择探索，因此选择了向下走，左图代表的之前智能体的Q-table，现在要做的是根据公式，更新（3，2）这里的Q value，由于向下走的Q-value最低，假定学习率是0.1，折现率是1，那么（3，2）这个点向下走这个策略的更新后的Q value就是：&lt;/p&gt;

&lt;p&gt;Q( (3,2) down) = Q( (3,2) down ) + 0.1× ( 0.4 + 1 × max [Q( (4,2) action) ]- Q( (3,2), down））&lt;/p&gt;

&lt;p&gt;Q( (3,2), down) = 0.6 + 0.1× ( 0.4 + 1 × max [0.2, 0.4, 0.6] – 0.6)=0.64&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4582043343653251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaaDiahblY26dmQgclaiac3B1IOojqD73sqjlEfqdpjYvgEZf4j0CEcNekw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;646&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而在在线的Q learning算法下（称为State-Action-Reward-State-Action ，简称SARSA），Q table的更新公式变为了&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatj8T5icjPo7sDEOpshRUibnb0ldh2ic59JdNcGibaqwicpcicHjiaM8TyEKmA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;588&quot; data-cropy1=&quot;9.517985611510792&quot; data-cropy2=&quot;93.06474820143886&quot; data-ratio=&quot;0.14285714285714285&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaa2dTIjl6icv7frMeMnyqIUPDhYeJ55RWKPxtoicbj5p0gpOB5kubo8DMQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;588&quot; /&gt;&lt;/p&gt;
&lt;p&gt; 此处不同的是没有了max，由于是智能体在亲自参与，这里也就没法像离线时那样，选择一个最优的策略。不管是在线还是离线，在训练的时候需要做经验回放，即存储当前训练的状态到记忆体中，等下一次训练时再调用。&lt;/p&gt;

&lt;p&gt;以上就是强化学习中最基础的Q learning，上诉的例子中不存在随机性，要引入随机性，可以需要通过蒙特卡罗的方法，来进行采样，同时引入对弈树，对其进行翦枝，这就是alpha zero的精髓。了解了Q learning的步骤，可以分析强化学习适用的领域所满足的假设，例如必须有能够清晰定义，事先已知且有限的策略，但现实生活中，真正重要的选择都是无限游戏，有无数种可能的选项，有前人根本不曾想到的选项，因此说强化学习不等价于强AI，只是通向强AI的一条必要选项。&lt;/p&gt;

&lt;p&gt;不同于人类的学习，是首先对坏境建模，之后再根据模型找到合适的启发式规则，Q learning框架是模型无关的，不管是什么样的问题，Q learning做的都是去更新状态对应的估值表，不管问题本身具有什么样的特点。和人类思维的另一个不同是Q learning中没有因果关系，学到的Q table只是反映了奖励和策略之间的相关性，而人类的学习则是受着因果关系指引的。关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;，其中有详细的论述。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384219&amp;amp;idx=1&amp;amp;sn=f396d027ea5a6074e0f0cda0aeb0cded&amp;amp;chksm=84f3c7dab3844ecc80be70e9b9e47cd12686624d71158caf69fb79de9b1067d1b548e31ee132&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;空间简史-人类认识空间的旅程与其对强化学习的启示&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 08 Apr 2019 18:01:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/SrT1aw8yLw</dc:identifier>
</item>
<item>
<title>《Possible Mind》读书笔记-强人工智能的公理化思考</title>
<link>http://www.jintiankansha.me/t/1TZgiT0aR4</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/1TZgiT0aR4</guid>
<description>&lt;p&gt;这篇小文还是来源于《Possible mind》这本书，上面是这个系列的前作，对于这本书感兴趣的小伙伴，刚刚接到出版社朋友的通知，这本书在今年7月将会由湛卢出版中文版的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384252&amp;amp;idx=1&amp;amp;sn=bdc733e516f25b44a1b7bdfb6104d2b5&amp;amp;chksm=84f3c7fdb3844eeb9f82b2f7e0590f3514f5b9887c279ccdd5919004b59a8686353a85670742&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;信息的俩种定义&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;读《Possible Mind》，看25位大咖谈AI&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.51875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceaCia3ppia3xp8PybpqCOpJCpvq0M7fIyD4YJafYz3MqV6kIyyx6NVrUb0QxRlFibfRfAxC7XlCBENA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于强AI的话题，在《Possible mind》中被多次提及，值得关注并模仿的不只是作者的观点，而是相近的论述模式，我将其概述为公理化的思考。欧几里得从5个不言自明的公理出发，推导出了整个几何学的大厦，而后世的黎曼几何，则改变了其中一个公理，从而推出了全然不同的结论，当我们讨论强AI是否会出现，强AI的出现是福还是祸的时候，也需要先设定一些假设，再看看由此能推出什么。&lt;/p&gt;

&lt;p&gt;首先介绍诺贝尔物理学得主Frank Wilczek的文，他的推理基于人的意识是一种自然而然的涌现现象，最终是可以通过科学的方式去解释的，就像人们现在解释了之前以为是超自然现象的物种起源，而解释了一个现象，就能用人工的手段重现出来，因此推出人类的智能只不过是更广义的智能的一个特例。这个视角将人类的智能和AI看成了一组各有所长各有所短的选手，由此引出了该文的标题，智能的汇合（The unitity of intelligence）。&lt;/p&gt;

&lt;p&gt;接下来作者对比了俩者智能各自的长处，对于AI，速度更快，更容易的扩展自己所占有的资源，判断决策的稳定性，频率更好，模块化组合更容易，也更有可能利用量子效应。而人脑的优势首先在于其是三维的，可以自我修复，通过神经元连接的方式组成，因此可以重现组织起来改变，而经过自然选择，人脑的感知和行动功能已经汇成了一个整体。&lt;/p&gt;

&lt;p&gt;对比出各自的优势，是要逐个看看AI能否全面的超越人脑，三维的芯片，虽然目前不能做到，但未来不会难，将感知与行动整合，电脑也只是暂时落后，而人脑真正难以超越的优势是其由神经元连接带来的自组织和自我修复，在这方面，已知的AI从原理上就做不到，由此导出AI即使能够超越人类的智能，但人类并非在强AI面前没有一丝一毫的比较优势，既然还有用，人类就可以通过和AI的合作，达到智能上的1+1&amp;gt;2.&lt;/p&gt;

&lt;p&gt;在该书的另一篇文章中，George Dyson在讨论强AI这个问题时，用到的是反证法。他在文中指出了三条定理（law），第一是说一个有效的控制系统，必须像这个被控制的系统一样复杂，第二条来自于冯诺依曼，他说复杂系统的充分必要条件是其包含了关于自身的最简单的描述，因此无法对复杂的系统用比起自身行为更简单的模型来描述，第三条说的是任何一个简单到足够被理解的系统，都不能表现出智能，而表现出智能的系统，都会由于过于复杂而无法被理解。&lt;/p&gt;

&lt;p&gt;这三条分开来看，并没有什么问题，但若是从这里推出，在我们理解大脑之前，强AI就是永远不会出现的，而怎么才算理解了大脑，我们就是重现出了大脑的所有表现，但这离我们确认自己认识到大脑，是否还有不同，这个问题的答案与科学关系不大，更类似于信仰。从这三条可以推出，不管对智能的认识进展到了那种程度，你若是不相信你了解了大脑，你永远能说你没有达到强AI。&lt;/p&gt;

&lt;p&gt;休谟的名言，理性是激情的奴隶，这句话用到当下的背景，就是是激励而不是逻辑，决定了一个智能体的行动。在丹尼尔丹内特的文中，他指出我们不应该把强AI看成是有意识的伙伴，而应该是工具。AI的激励始终来自人提供的数据，但随着AI变得越来越显得有智慧，人们将越来越多本该由人类做的决策，交给了AI，为了避免人类智能的退化，丹尼尔提出了反向的图灵测试，之前测试的是电脑能否模拟人类以假乱真，而现在受试者变成了人类的裁判，人类去判定机器做出的决策和判断是否有缺陷和某些情况下的不足，如果人类不能定位出机器的不足，机器就不应该去作为人类的工具，就如同武士学会一套剑法的标志，是知道这套剑法的破绽，人类只有不断提升对机器智能可能出错地方了解，才能避免被走火入魔的AI带到沟里去。&lt;/p&gt;

&lt;p&gt;而Stuart Russell关心的是我们如何为AI确定一个明确对自己无害的目标这个问题，他给出的解决方法是cooperative inverse-reinforcement learning（ICRL），可以翻译为合作中的反向强化学习，在这个框架下，智能体和人类一起合作达到一个目标，智能体要优化的不是自己的策略，而是通过人类的行为，搞清楚人类最想要的是什么。举一个大家都熟悉的例子，传统的强化学习，是训练一个能在dota比赛中战胜人类的AI，而在ICRL框架下，AI拿到了一组和王校长一起打dota的战队的比赛录像，学到了和王校长一队的人的真正的目标是要思聪玩的开心成为场上的VIP，最终当和王校长组队的由真人换成AI后，思聪不会抱怨有人和他抢人头。小结ICRL，其目地是让强AI即学会赢，也学会何时不赢。强AI能学会在游戏本身中找到乐趣，并通过这份乐趣去实现自我提升。（类似于好奇心的指导）&lt;/p&gt;

&lt;p&gt;这个例子虽然是玩笑，但随着AI和人类合作决策的事情变得越来越和我们的切身利益与生存密不可分，如何解决给AI设定目标这个问题，变成了我们这个时代最亟待解决的难题。为何解决给AI赋予目标这个问题很重要，是由于被赋予了错误目标的AI，可能会改变人类生存的坏境，正如同人类在不了解温室效应的时候，就开始了改变人们生存的坏境，如果行动力指数级超过人类的强AI为了达到目标，用尽了全地球的水，那整个生物圈都会灭绝。机器不同与人类，可以在各种坏境下生存，但人类的生存坏境却需要紧密的微调。&lt;/p&gt;

&lt;p&gt;前文写道，强AI应该既能做到赢得比赛，也能做到不去赢，这就是一个矛盾，所有人学下棋，都是学如何赢，而不是学如何输，但人类是可以做到不服从老师设定的目标的，如果强AI也要像人一样进化，那强AI也需要具有不服从的能力，但如果让强AI通过一个测试来判断是否具有不服从的能力，那强AI正确的做法不应该是不参加这样的测试，从而表现出不服从吗？这个问题也是一个悖论。上面的论述，来自David Deutsch，他先问了该如何惩罚一个犯了错误的AI，是将其全部软件删除吗？接着通过不服从的悖论，指出我们是否应该限制强AI的自我成长，他的观点是站在强AI带给人的利大于弊这一方，指出就如同我们不应该对犯了错误孩子判处死刑，对于正在变强的AI，即使其犯了错，人们也有道德义务，让其有改正和进化的空间，从而让智能的旅程走的更远。&lt;/p&gt;

&lt;p&gt;这一篇就说这么多，关于强AI这个问题，总结了书中几位大牛的观点，多半有些曲解和遗漏，但我个人是认同David Deutsch的，如果我们拒绝了强AI，也压抑了未来可能有的创造力和探索的可能性，而不进则退，不能保持动态的均衡，就只能陷入热寂的平均，我们需要强AI来帮助我们解决全球变暖，极端气候等诸多威胁我们生存的问题，需要强AI来帮我们走向星辰大海。最后给出一段一个多小时的音频地址，是书中13位作者简述自己的观点，由于文件太长，无法上传音频，感兴趣的可以去 https://www.edge.org/conversation/john_brockman-possible-minds-25-ways-of-looking-at-ai-0 这个网站下载mp3音频。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383531&amp;amp;idx=1&amp;amp;sn=4e4b7c6a901810b72427392dad370f69&amp;amp;chksm=84f3c8aab38441bc45707f038e04ecf99cbe83572a16400715cd919d1e006b6575747deb3cb1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;AI会怎样改变战争的未来-读《Army of None》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383580&amp;amp;idx=1&amp;amp;sn=da0870102ecdc4a1802d28bb5db92574&amp;amp;chksm=84f3c95db384404bd330565babd688d50f11fac8a9cda914520f61f538c54eebf2f9466dd5c7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;生命3.0-在亿年的尺度下审视生命的演进&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383706&amp;amp;idx=1&amp;amp;sn=a1640b00320b0e43e769bff619cb78fc&amp;amp;chksm=84f3c9dbb38440cd0a807d9a8d72b9a4701191dd24af206c0703f2fa104ed1cae6b0758964f3&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;机器与人-寻找人机之间的中间地带&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;














</description>
<pubDate>Sat, 06 Apr 2019 08:47:43 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/1TZgiT0aR4</dc:identifier>
</item>
</channel>
</rss>