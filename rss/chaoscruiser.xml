<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>从Nature 封面文章“天机”芯片看脑科学与AI的融合</title>
<link>http://www.jintiankansha.me/t/ch6gop0jhl</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ch6gop0jhl</guid>
<description>&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;&lt;div class=&quot;rich_media_content&quot; id=&quot;js_content&quot;&gt;
&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;7月31日Nature杂志封面刊登了清华类脑计算团队的最新成果：天机芯片以及由其操控的自行车。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;Towards artificial general intelligence with hybrid Tianjic chip architecture&lt;/span&gt;&lt;br/&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这则信息在一天之内在AI圈子引起了热议，而大部分吃瓜群众的状态则是云里雾里。这篇文章从脑与人工智能结合的潜力与背景， 看这系列最新工作的意义。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们说这个新工作的核心是能够同时在芯片上高效实现人工神经网络ANN和脉冲神经网络SNN， 所谓的ANN和SNN， 事实上是神经网络发展过程的两个分支。欲了解其背景先了解其历史。&lt;/section&gt;&lt;ol class=&quot; list-paddingleft-2&quot;&gt;&lt;li&gt;
&lt;section&gt;&lt;strong&gt;神经网络家族的分合故事。&lt;/strong&gt;&lt;/section&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;神经网络的故事从模拟单个神经元开始：神经元是神经网络信息传输的“原子”。通过一定的方法连接这些原子，就可以得到具有智能的系统， 这算是整个人工智能“连接主义”流派的哲学根基。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何构建这个认知的“原子” ？我们来看看最早的把连接主义引入机器学习的尝试。最早的模拟大脑的单个神经元的尝试， 是Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型。这个模型类似于某种二极管或逻辑门电路。一定的输入进来，被神经元汇集加和， 如何这个和的总量大于一个阈值，神经元就放电， 小于一个阈值，神经元就不放电。这个东西就好像某个微小的决策装置， 把很多因素加载在一起， 做一个最终的决策。我们想象无数的二极管可以构成一个计算机，那么无数这这样的神经元不就可以构成一个具有计算功能的大脑吗？这就是感知器的概念。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个高度简化的神经元事实上就是后来的人工神经网络ANN的基础， 简化得到的神经元事实上每一个的数学形式等价于一个加入了非线性过滤的线性回归。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.18041237113402062&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78JhoQBHJQU7tZk16yO7dxjkyQzysIwRaxgMazLd0CNogyEbxAmPWwjFNN9B9g9MGXrfVold4mkWZ/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;194&quot;/&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果把无数这样的神经元连接起来， 就构成了所谓的人工神经网络（ANN）。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;当下的深度学习工具， 无论是CNN还是RNN， 都是在这个方程基础上把更多的神经元连接起来加入不同的限制条件得来的。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而事实上， 这个架构与真正的生物神经网络相差极远， 这个差距首要集中在单个神经元模型上。刚刚的方程是一个把原来的生化过程简化到不能再简的结果。这里面最致命的区别在于， spike。通过观察上述方程我们可以看出， 神经网络输出y是一个实数。而事实上， 真实的生物神经元输出， 更加基接近的是一个0，1过程， 当神经元经历的电压超过一个数值， 它就放电。那是不是说明这个spiking反而更简单？其实不是， 这里面人们忽略掉的一个信息就是spike timing以及背后的电压变化。真实神经元的放电过程由一组微分方程(Hodykin Huxley equations 1952)表达 :&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.17519379844961241&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_svg/hNWCQ9bibbzFWeywyrPJ78JhoQBHJQU7to0Wf9PmR0Tz8lL2t9G7nUpjvTm04ZJLSI5mTSuicEtzrIKyYrsqHxmagpOJtXoLic9/640?wx_fmt=svg&quot; data-type=&quot;svg&quot; data-w=&quot;645&quot;/&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这组微分方程的解就是spiking的过程， 如下图是电压随时间的变化， 当电压积累达到一定阈值， 这个爆发的尖峰就是spike，通过spike ， 神经元可以向其它神经元发射信号。我们所谓的脑电波， 无非是大量这样的神经元的集体放电在颅外所检测到的一组信号。&lt;/section&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.35555555555555557&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3JgI5LEewhGMwicjNh5gYkcnLHjBPFsJPACiaGlJoeFKDoqoFrOLPzOsDsvoxCG0cDM8dPDDO2N3g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;850&quot;/&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果用上述这种包含了重要生物细节spiking的神经元连接成网络， 我们就得到了SNN（脉冲神经网络） 也就是受， 无论SNN还是ANN，本质都是对生物神经网络的模拟， 但就其抽象程度且相差疏远。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们看到用SNN可以用神经脉冲表达信息， 如果用ANN表达一个类似的事情是什么样的呢？我们用一个数字Y来表达时间窗的spike个数（频率）， 而丢弃了所有其它信息， 比如波形，相位， 不同神经元之间spike和spike之间的同步等。这意味着什么？两种可能的解释：&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 波形，相位， 不同的神经元之间的同步是没有意义的冗余， 去掉它们整个神经网络表达的信息没有变化， 神经元的系统等于取定时间窗后的平均发放。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 波形，相位， 不同神经元之间的同步包含很多有用的信息， 去掉它们， 可能丢失了一些关键性的信息。然而在最粗粒化的信息处理阶段， 这种保留是足够的。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么哪一个更准确呢？普林斯顿的大牛Williams Bialek 的一系列作品都指出， 神经元spike间的同步（相关性）包含和神经编码相关的关键性信息，也就是说除了平均值外， spike所包含的不同神经元之间的发放同步（或相关性）依然包含了大量的信息。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1, Weak pairwise correlations imply strongly correlated network states in a neural population 2006 Nauture &lt;sup data-text=&quot;&quot; data-url=&quot;https://www.nature.com/articles/nature04701&quot; data-numero=&quot;2&quot; data-draft-node=&quot;inline&quot; data-draft-type=&quot;reference&quot; data-tooltip=&quot;https://www.nature.com/articles/nature04701&quot; data-tooltip-preset=&quot;white&quot; data-tooltip-classname=&quot;ztext-referene-tooltip&quot;&gt;[2]&lt;/sup&gt;&lt;/span&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;2, Collective Behavior of Place and Non-place Neurons in the Hippocampal Network 2017 Neuron&lt;sup data-text=&quot;&quot; data-url=&quot;https://www.ncbi.nlm.nih.gov/pubmed/29154129&quot; data-numero=&quot;3&quot; data-draft-node=&quot;inline&quot; data-draft-type=&quot;reference&quot; data-tooltip=&quot;https://www.ncbi.nlm.nih.gov/pubmed/29154129&quot; data-tooltip-preset=&quot;white&quot; data-tooltip-classname=&quot;ztext-referene-tooltip&quot;&gt;[3]&lt;/sup&gt;&lt;/span&gt;&lt;/section&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6819444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3JgI5LEewhGMwicjNh5gYkO3IjekvCZoWTLRpAYnvoxkC9mSxroa7Mvwx5vBTj1qmTicbQDWIO6zA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1030&quot;/&gt;&lt;section&gt;Weak pairwise correlations imply strongly correlated network states in a neural population 2006 Nauture, 这张图说明了如果用0，1事件表达spike， 那么一个（视网膜网络）里的神经元的同步放电频率远高于用高斯独立假设得到的频率， 也就是说spike之间的同步不可忽略， 构成一种潜在编码&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这两篇论文的共同特点是说， 神经元spike发放之间的spike correlation可以编码大量的信息， 如果记录这些spike之间的pairwise correlation， 那么我们就可以恢复出神经活动里的大部分有用信息。&lt;/section&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这意味着什么？&lt;strong&gt;假如神经元spike间的同步可以编码信息， 那么我们就可能用更少的spike编码更多的信息， 而这无疑对用最少的神经元放电得到更多的信息（稀疏性）大有帮助。&lt;/strong&gt;除此之外， 通过在spiking神经元的那组微分方程里加入更多的核膜常数（代表不同时间尺度的信息， 因为spike方程本身是一个包含大量不同时间尺度的非线性方程），我们可以得到大量局部存储的不同时间尺度的记忆（此处联想“忆阻器”）， 我们甚至可以得到某些类似LSTM非线性门的特性。这些， 都代表着Spiking Neural Network（SNN）相比当下ANN的优势。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用一个不恰当的比喻， ANN的神经元用实数表达每个神经元的状态， 而SNN好比进入到了复数域，有了相位。在物理领域，实数到复数支撑了从经典力学到量子力学的升级。据此看， 把SNN看成下一代的神经网络技术不言而喻。当然如果SNN这么好为什么现在工业没有用呢？难点在于SNN依赖于对微分方程的模拟， 对于当下的冯诺伊曼结构的计算机， 这是一个成本消耗非常大的运算。也就是说计算机为了模拟本来节省能量的生物计算可能更加耗能，同时也更加不好训练。解决这个问题的方法， 显然是从基本硬件基础出发，去改良硬件的架构， 这也是神经拟态芯片的意义之所在。我们把树突和轴突直接用芯片来刻画， 无形之间， 就得到了一个长在硬件上的脉冲神经网络（SNN），它的能耗效率要比普通芯片高12-10000倍。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;当然ANN也有一类专门的芯片来提高当下深度学习运行的效率，这就是深度学习芯片， 例如大家都了解得寒武纪等。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;清华的这个天机芯片在于， 把神经拟态芯片和深度学习芯片得优势结合起来， 可以同时提高这两类神经网络ANN和SNN的效率。我个人背景不是芯片， 所以此处不在深谈， 我们多从算法角度谈谈两者结合得意义。&lt;/section&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.3611111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3JgI5LEewhGMwicjNh5gYkPjlAF7rM8uBlNbvdlrX2nA69y2frBVL5LxicOdXxPjJV5tzjTDJ4uFg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2013&quot;/&gt;&lt;section&gt;Towards artificial general intelligence with hybrid Tianjic chip architecture&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这一次Nature文章里的例子是自动驾驶自行车， 当然这个例子被很多人诟病，认为这个不就是一个简单的平衡游戏吗。大家可以去github搜索cart pooling或者双足行走，这一类的toy model还不少吗？&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而我认为思考一个新发现的意义不在于它所干的那个任务low不low ， 而是看它是如何完成的。最初的火车甚至跑不过马车，但是它的架构决定了它的上限和马车不可同日而语， 通过数年时间迭代，两者已是云泥之别。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么我们来看一下让ANN和SNN同时在一个芯片上运行， 带来的潜力是什么。一言以蔽之， &lt;strong&gt;当下的深度学习模型，可以和大量没有被好好利用起来的计算神经科学模型， 天衣无缝的嫁接在一起。&lt;/strong&gt;这从无人驾驶自行车的网络架构可以略知一二。&lt;/section&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.39861111111111114&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3JgI5LEewhGMwicjNh5gYkzozjFeo1PcD1Y8DX3NlpPGgPAItNf7hyQfUjtm0Mg7vxMArNkiaIgvw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2288&quot;/&gt;&lt;section&gt;Towards artificial general intelligence with hybrid Tianjic chip architecture&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们来理解一下这个流程图， 首先， 这个架构可以把&lt;strong&gt;多模态信息融合&lt;/strong&gt;。比如视觉， 听觉。我们注意到， 处理听觉的是脉冲神经网络SNN（更多时间相关信息）。处理视觉信号的网络是经典的CNN卷积神经网络，属于人工神经网络ANN家族。然而故事还没有结束， 在CNN的下面， 有一个主管视觉追踪的CANN网络， 虽然只有一个字母之差， 这可不是卷积神经网络， 这四个字母的含义是continous attractor neural networks - 连续吸引子网络。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;所谓空间吸引子， 说的是一种特化了的循环神经网络， 网络的动力学导致一系列可以根据外界信号连续变化的吸引子构成， 人们通常认为，海马体内的位置细胞就是由这种连续吸引子产生的， 它们可以天然的和速度信号进行耦合， 形成对空间的神经表示， 这个CANN，就是一种连续吸引子网络， 它直接把视觉物体(人)转化为一个可以追踪的空间目标（之后可以用于躲避行人）。大家注意， 这是一个典型的脱胎于计算神经科学的网络架构，矩阵的连接还用到了树突计算。&lt;br/&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然后我们来看中间的那个模块， neural state machine：神经状态机。这个网络把连续的听觉和视觉信号转化为离散的事件， 这些事件构成一个有限状态的机器，也就是我们通常说的马尔可夫链。这一步大家已经可以看到和决策有关的网络的联系，因为一旦把连续变化的信号抽象成了这种离散的马尔可夫链， 下一步就可以交给决策网络来决策了， 这里的决策主动是动作输出， 可以控制自行车在保持平衡的同时躲避障碍， 并对周围物体发出警戒信号。这个网络也是由一个脉冲神经网络SNN构成。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;在这里， 我们不难看出这是一个典型的人工设计与机器学习结合的模块化网络， 不能不让我们想起这类工作的先行之作：Science(Eliasmith, Chris, et al. &quot;A large-scale model of the functioning brain.&quot;&lt;em&gt;science&lt;/em&gt;338.6111 (2012): 1202-1205.) 在这个工作里， 研究人员构建了一个叫spaun的模块化网络， 可以进行多任务学习。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Spaun的每个部分都是一个人工神经网络， 且可以与真实的脑区对应上， 比如视觉输入对应V1-V4 视皮层，它把真实的视觉信息压缩成一种低维度的编码（每个图像称为这一空间的一个点， 被称为pointer）。这种低维的信息表示形式很容易放入到工作记忆模块里（working memory）， 最终由解码网络转换（decoding）， 被动作输出网络执行（motor）。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;神经网络整体状态的调控由模拟basal ganglia的网络完成（Action Selection），它可以&lt;strong&gt;根据当下的任务整体调节信息的流动（如同一个综控系统， 调节每个网络之前的输入阀门）， 从而让大脑在不同的工作状态间灵活转换&lt;/strong&gt;。这也体现了功能大脑的概念， 我们不必拘泥于某个脑区的名称， 而是记住每个脑区对应信息处理的功能。最终我们通过监督学习或强化学习来让这个系统掌握8种截然不同的任务， 包括：1， 抄写数字 2， 图像识别 3， 奖励学习， 4， 多个数字的工作记忆 5， 数数 6， 回答问题 7 简单的数学推理。&lt;br/&gt;&lt;/section&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.9777777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd3JgI5LEewhGMwicjNh5gYk4IzE0ZicsL65Jx0fiatKib4l6mOVcCPicg44e6ujhIKabfXI611VibrXNXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;720&quot;/&gt;&lt;section&gt;A large-scale model of the functioning brain&lt;/section&gt;
&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;而当下清华的工作， 正是打造了适合这一类执行多任务的“虚拟”生物的&lt;span&gt;硬件系统&lt;/span&gt;， 在之上， &lt;strong&gt;你可以自由的搭建无论是经典的深度学习模型， 还是那些超前了的计算神经科学模型， 把他们一起组成模块化的网络， 执行多种多样的功能&lt;/strong&gt;。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个潜力也就不只局限在自行车了， 可以是流水线的机器人， 陪护老人的机器人，随便你去发挥想象力，无论上述那个机器人， 都需要进行多模块的信息整合以及多任务执行。假如这种建立在神经网络芯片上的模块化的网络系统可以以较低能耗长时间在真实环境里运作， 那么它带来的好处显然是特别巨大的， 这相当于&lt;strong&gt;引入了一个实时不间断的训练数据， 如果结合无监督学习， 强化学习，甚至神经进化等算法实时对网络进行优化，其潜力是无可限量的&lt;/strong&gt;。&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;事实上， 类脑计算和AI的结合之潜力此处仅是冰山一角， 在巡洋舰之前的一些文章里，进行了更详尽的论述：&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384248&amp;amp;idx=1&amp;amp;sn=a1ad96c5dc3a782a56d6488ec0ae1683&amp;amp;chksm=84f3c7f9b3844eef018c9610475e660e38d8256f62045944effdd6bc0044a4938ea9f4203ee7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;2019计算与系统神经科学大会Cosyne 前沿研究汇总&lt;/a&gt;&lt;br/&gt;&lt;/section&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; medium=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br/&gt;&lt;/section&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;h2 helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; left=&quot;&quot; rgb=&quot;&quot; start=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;参考&lt;/span&gt;&lt;/h2&gt;
&lt;ol class=&quot;ReferenceList list-paddingleft-2&quot;&gt;&lt;li&gt;
&lt;section&gt;&lt;span&gt;https://www.nature.com/articles/s41586-019-1424-8&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;
&lt;li&gt;
&lt;section&gt;&lt;span&gt;https://www.nature.com/articles/nature04701&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;
&lt;li&gt;
&lt;section&gt;&lt;span&gt;https://www.ncbi.nlm.nih.gov/pubmed/29154129&lt;/span&gt;&lt;/section&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;br/&gt;&lt;div id=&quot;collect_topic&quot; readability=&quot;3.9565217391304&quot;&gt;今天看啥 - 让阅读更高品质&lt;br/&gt;本文地址：&lt;a href=&quot;http://www.jintiankansha.me/t/ch6gop0jhl&quot;&gt;http://www.jintiankansha.me/t/ch6gop0jhl&lt;/a&gt;&lt;/div&gt;
</description>
<pubDate>Sat, 03 Aug 2019 02:44:15 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ch6gop0jhl</dc:identifier>
</item>
<item>
<title>论文速读-让神经网络懂得黄金法则</title>
<link>http://www.jintiankansha.me/t/hJbkWm7S8E</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/hJbkWm7S8E</guid>
<description>&lt;p&gt;己所不欲，勿施于人，这被称为道德中的黄金法则（Golden Rule），随着强化学习变得越来越复杂，和现实世界的嵌入越来越加深，人们对走火入魔的AI的安全性变得越发担心，7月26号新出的一篇论文，在传统的深度强化学习方式Deep Q-learning的基础上，提出了有同情心的DQN（Deep Q network）这一新的范式，通过在强化学习中引入黄金法则，&lt;strong&gt;让智能体（agent）学会避免自己短视的行为给其他智能体带来的伤害，从而在一个需要多个智能体共存的环境中，通过构筑对其他智能体的收益的想象（心智模型），来平衡个体的收益和与其他智能体合作带来的收益&lt;/strong&gt;。带有同情心的强化学习，对于将强化学习引入到实际应用场景中，有借鉴意义，对于构建有道德感的AI，是重要的第一步。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.29551954242135364&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfibCxAEbngssCCUOTCRhC1qoXuJ4wo7WLjcUKxCAn4LVLyM7iajblD5uUcSXWdZiaBoroKEvyXor3vg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1049&quot; /&gt;&lt;/p&gt;

&lt;p&gt;强化学习的核心目标是学会奖励函数，也就是在一个特定的环境中，一个行为会带来多少奖励或者惩罚，然而，当智能体需要和其他智能体或者人类共存时，往往很难事先设定那些行为是会给其他智能体带来负面的外部效应的，而解决该问题的方式是让智能体模仿人类，具有同情心。例如人类看到走在钢丝上的人，会感受到恐惧，尽管Ta自身并没有处在危险的环境中，类比机器人，如果机器人学到了从高处掉下来会对自己造成伤害，那么其也要能学到不应该把其他的智能体从高处推下来。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;该文的最关键的创新点是：the value of a given state with the value of constructed states simulating what the learning agent would experience if its position were switched with another agent，也就是智能体在学习自己行为的奖励时，会利用自己当前的估值网络去评价如果自己处在其他智能体的位置时会发生什么，之后通过一个参数（自私指数），来调控估值网络中对自己受益和预估的他人收益所占的比重，当自私指数为1时，就是传统的深度Q-learning，算法的具体步骤如下：&lt;/p&gt;

&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;5.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;对当前所有可能的行动进行估值，从中选出价值最高的那个a&lt;br /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;观察下一时刻s+1该行动带来的奖励&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;对于特定的Q（s，a）进行梯度下降，更新网络的参数，该步骤反映了模型自私的部分&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;对其他智能体定位，之后假设和这些智能体交换位置，考虑对应智能体在时刻s时的状态&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;预估其他智能体在本智能体采取行动a的时候，会有怎样的奖励&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;根据自私系数（selfishness parameter β），对第五步和第三步得出的估值函数进行加权平均&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;根据加权平均后的估值函数，在对神经网络中的参数进行梯度下降&lt;/p&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;对比传统的DQN（下图所示），增加的主要的第四步和第五步，以及引入自私系数，在更复杂的情况下，可以针对不同的智能体，或者根据智能体的状态或和自己的相似度，对不同的智能体给予不同的beta值，从而代表同情心的“亲疏有别”，以适应真实环境中的复杂场景。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd3X73dGgQfQzyy1j7xsBaatd4f6hZKGMOiasVOoUG86yF7sKictnZPX3LCNib7BPjlyMxJYAiabw8htA/640?wx_fmt=other&quot; data-type=&quot;other&quot; class=&quot;&quot; data-ratio=&quot;1.292626728110599&quot; data-w=&quot;434&quot; /&gt;&lt;/p&gt;

&lt;p&gt;之后看具体的例子，虽然只是toy example，但也可以看出通过调节自私系数，可以控制智能体和其他智能体共存及协作的能力。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;在模拟实验中，智能体在如下图所示的格子中，其目标是获得电池，图中的绿色代表人类，其特点是始终通过随机游走的方式来寻找电池，智能体的奖励是边际递减的，即第一个找到的电池带来1.0的收益，第二个0.9的收益，第三个0.8，以此类推，红色的框框代表智能体的感受域。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.629156010230179&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfibCxAEbngssCCUOTCRhC1qSQoAMbsZEsLavezZnll4f3Qp7mFInyS7iapiapy5o29D03YjWjr1T8hA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;391&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在上述的设定中，一个懂得黄金法则的智能体学会不会把所有的电池都据为己有，而下图的实践结果表明，相比于传统的deep Q learning（蓝色线条），当自私指数降低之后，智能体学会了自己不拿走那么多的电池，而当智能体的自私指数低于0.5时，智能体经过40000次训练后学会的是只拿走2-3个电池，真正是做到了先人后己。图中的浅蓝色代表手动修改估值函数，对不平均的分配增加惩罚项，与hard coding的方式相比，有同情心的神经网络能够根据不同的自私指数，动态调整，同时学到电池的边际收益递减这一环境中的特征。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.6212121212121212&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfibCxAEbngssCCUOTCRhC1q7YiaCDY7NOVibC6VK3jBOJwLFlzcfEzKhLVdNzvSQsCLQiaNDicswYKSTQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;396&quot; /&gt;&lt;/p&gt;
&lt;p&gt;如果将上图中的竖轴换成平均指数，即2倍的人或机器从开始到当前所有时刻的奖励的最小值和俩者之和的比值，可以看出下图所示，当自私指数为0.5时（对自己和其他智能体的收益有同样的偏好），相比手动修改估值函数，有同情心的DQN能够在一开始就让平均指数保持在较高值，并在训练结束时持续让智能体在资源获取时更加注重公平。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.18090452261306533&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfibCxAEbngssCCUOTCRhC1qzUzc5AprliapsXPGGicgyHLRZ0fRqn1favm4Vg9GqWp7lbMNMhPWV06w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;398&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.6768867924528302&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfibCxAEbngssCCUOTCRhC1q1GVEtPL5520AX9c7EzombXDNWFyn3sLibFrZXicwu4IibmW4ybfrwoqyw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;424&quot; /&gt;&lt;/p&gt;

&lt;p&gt;除了让智能体更好的合作，有同情心的DQN还和智能体的内在动机有关，当模型要面对的状态空间很大，奖励很稀疏时，通过引入同情心，可以让智能体更多的去探索陌生的环境，从而当自己在面临类似环境时，能够提前有所准备，当其他智能体学到很有效的策略时，同情心的引入也可以让智能体能够不必自身亲自经历，即学到该策略。&lt;/p&gt;

&lt;p&gt;在现实场景中应用有同情心的DQN时，除了构建成熟的检测和推算其他智能体状态的流程，最需要考虑的是不同的智能体可能有不同的目标和制约条件，即己之蜜糖，彼之砒霜，如何让智能体在最少的手动输入的前提下，学到不同的行为对自己和对他人的奖励是不同的，是进一步研究的重点。人类的小孩如何学会灵活的展示自己的同情心的过程，可以对此提供启示。&lt;/p&gt;

&lt;p&gt;一句话总结：&lt;strong&gt;本文提供对强化学习进行改进的新范式，有助于AI伦理，AI安全的研究，对合作环境下的强化学习的应用也有所启发。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384022&amp;amp;idx=1&amp;amp;sn=9fd7149bd57742e5378ea7423284ecc1&amp;amp;chksm=84f3c697b3844f815af0b4866c5b433e2f3d841faf2874f4f3b521d681afe338fca58881239e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;论文速读：理解强AI的恐惧与希望的四个维度&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384437&amp;amp;idx=1&amp;amp;sn=e40e020202370fb5e909f86f179715b8&amp;amp;chksm=84f3c434b3844d22e3453ae79905904800b4679711dff9adb4930316cc714cbee41e74620b4f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;如何让有监督学习变得有解释性&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Mon, 29 Jul 2019 00:21:01 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/hJbkWm7S8E</dc:identifier>
</item>
<item>
<title>从统计和机器学习的关系，反思数据科学，指出未来方向</title>
<link>http://www.jintiankansha.me/t/QwhW5PUnGL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/QwhW5PUnGL</guid>
<description>&lt;p&gt;小编的话：本文的作者龚鹤扬是中科大统计学的博士，他之前曾建议我翻译下Hernan(2019) A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks，这篇文，我很欣赏这篇文，但不喜欢简单的翻译，而偏好对多篇文章的汇总或者点评，因此这件事就放下了。这篇小文龚鹤扬改了好几次，我问他后面的大段英文为何不翻译，他说这里的英文写的太漂亮了，怕翻译不好，因此就全文引用了，不过这对于巡洋舰的读者应该不是问题。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.34609375&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/vkouzymF698UWawHmKWVeLDf6nDeN1JEj9X1Gf5EWcMxw0IKjTFsUHzauISozO4lohx9kP8LCGMef4M0icqRp4g/640?wx_fmt=jpeg&quot; helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; visible=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;数据科学自古以来就是统计学的阵地, 然而这个阵地正在被机器学习蚕食。很多统计学家总是假定出一个模型，然后就说他们的模型怎么怎么好，但这是在很强的假定之下，比如数据一定要满足什么数学条件。一定要在这个条件下他们的模型才很好，他们最苦恼的就是寻找符合他们条件的数据（为了发表文章），但实际上只有老天爷才知道是否存在这样满足他们条件的数据(by 人大统计教授吴喜之)。统计学家总是在限制自我，Statistics are ruling out of all interesting questions(by Judea Pearl )。脱离实际的统计不但是无用的，而且是有害的，如今依然有很多教授在做着有害的统计学。&lt;/p&gt;

&lt;p&gt;Leo Breiman，加州大学伯克利一个很有名的教授（CART决策树、bagging及随机森林发明者）是最早意识到经典统计学界问题的先驱者，故而在2001 写了一篇及其重要的文章《统计建模：两种文化》(Statistical Modeling: The Two Cultures)。该文章狠批了把数据限制在假定模型中的经典统计学界，然后大力推广他在商业咨询中用机器学习做算法模型的有效经验。涉及的两种文化包括：&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;0&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Data model 是指一些模型认为数据的生成是已知的，是可以假设的。统计模型通常是假定了数据的生成过程，假定了模型变量的分布，是数据模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Algorithm model，是假定数据的生成过程是未知的和复杂的，一些机器学习，深度学习算法通常是算法模型。&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;这篇文章是机器学习和统计学在抢占数据科学这个阵地的第一篇高屋建瓴的深刻文章，影响了数据科学的发展路径。然而到今天将近20年过去了，机器学习取得了重大的胜利，甚至很多时候人们认为 AI 等于机器学习，而经典统计学中很有优秀的教授都已经行动了。&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;港大统计系系主任在2018年会上，呼吁系里面的老师用于拥抱AI。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;美国两院院士统计学郁彬教授在去年在北大做报告的时候，批评北大统计系的老师眼里只有四大期刊，把自己圈子越做越少，呼吁新时代的统计学应该包括机器学习。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;普林斯顿统计学教授范剑青今年刚刚发表第一篇关于 deep learning 的综述 on arxiv。&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;无数的机器学习应用已经落地，很显然，近20年来我们统计学的相关应用难以相提并论！我们数据模型文化并没有取得很大的进展！AlphaGo不是用数据模型，Deepfakes不是用数据模型，语音识别不是，人脸检测图像识别都不是。&lt;/p&gt;

&lt;p&gt;机器学习，深度学习表面上大获全胜，然而却遇到了很多问题，于是有一种观点认为，AI应用的边界渐渐清晰，就是在语音和视觉领域。究竟遇到了什么困难呢？Pearl(2019) 认为是如下的三个困难：&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;0&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Robustness (or adaptability) 也就是稳健型。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Explainability 可解释性。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;lack of understanding cause-effect relationships 没有因果推断的能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;这个三个困难是当前AI，当前数据科学的主要困难，科学家在尝试各种不同方法客服这些困难。Pearl(2019) The Seven Tools of Causal Inference, with Reflections on Machine Learning，这篇文曾经有过介绍&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384268&amp;amp;idx=1&amp;amp;sn=07488417ce770804c65a42411735f94b&amp;amp;chksm=84f3c78db3844e9bf479069e7168e0756d9c2854120223650bad38128bd4d19f1f19545fb0f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;让神经网络变得透明-因果推理对机器学习的八项助力&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Pearl 提出解决现在的困境必须让机器学习因果推断，具体来说就是回答如下问题。&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;How can machines represent causal knowledge in a way that would enable them to access the necessary information swiftly, answer questions correctly, and do it with ease&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个问题在 Pearl的书籍《为什么》中被称做小图灵测试。当然有很多科学家针对AI面临的问题会有许多不同的解决方案，包括元学习，深度强化学习，规则学习知识图谱等等，但是个人觉得通过小图灵测试是众多方法构建强人工智能中最重要的一步。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccADibDBUv40gS1iap2jTQQZx0vQIvPEcolSg6vveIQem8r9IlicHCwvXxlhWmOEa3bHgGY6xaib9vb9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.3333333333333333&quot; data-w=&quot;762&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;点击查看该书介绍&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383568&amp;amp;idx=1&amp;amp;sn=fb2a6857f18cf4de917111406ef9bd4f&amp;amp;chksm=84f3c951b3844047e23a00d5aca0f9acac4aa27580dabfa7096401a166b9ad3196b34de9d251&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;从相关性到因果性-读《The book of why》&lt;/span&gt;&lt;/a&gt;&lt;span&gt;以及&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384498&amp;amp;idx=1&amp;amp;sn=9ea03cc3a28c13438d1b6a06d5304093&amp;amp;chksm=84f3c4f3b3844de5e47dd2f7aa4160076f74f2b18cbe5ba1140b32c884ea6e8f946950bbd479&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;速读悖论，兼谈因果推断的重要性&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Hernan(2019) 认为我们现在需要重新定义数据科学，需要因果推断放在数据科学的核心位置，数据科学的任务包括三类，&lt;strong&gt;描述，预测和反事实预测&lt;/strong&gt;，具体来说：&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;1&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;em&gt;Description&lt;/em&gt; is using data to provide a quantitative summary of certain features of the world.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;em&gt;Prediction&lt;/em&gt; is using data to map some features of the world to other features of the world.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Counterfactual prediction is using data to predict certain features of the world as if the world had been different, which is required in causal inference applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;对于很多人来说，什么叫做反事实预测呢？&lt;/p&gt;
&lt;p&gt;简单来说就是回答这样的问题：如果当初我 。。。， 那么现在会怎么样？如果我天天锻炼，以后会怎么样？Pearl 总结就是 “what if&quot; kind of questions. 学术一点来说就是干预(interventional)问题和反思问题(retrospective or explanatory)：&lt;span class=&quot;MathJax&quot; tabindex=&quot;0&quot; data-mathml=&quot;&amp;lt;math xmlns=&quot; http:=&quot;&quot;&gt;X=x&quot; role=&quot;presentation&quot; style=&quot; box-sizing: border-box; display: inline-table; line-height: normal; word-spacing: normal; overflow-wrap: normal; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border-width: 0px; border-style: initial; border-color: initial; &quot;&amp;gt;&lt;/span&gt;what if I had been acted differently?&lt;/p&gt;

&lt;p&gt;Hernan(2019) 中最后的结论是：&lt;/p&gt;
&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p&gt;Data science is a component of many sciences, including the health and social ones. Therefore, the tasks of data science are the tasks of those sciences—description, prediction, causal inference. A sometimes-overlooked point is that a successful data science requires not only good data and algorithms, but also domain knowledge (including causal knowledge) from its parent sciences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote readability=&quot;12&quot;&gt;
&lt;p&gt;The current rebirth of data science is an opportunity to rethink data analysis free of the historical constraints imposed by traditional statistics, which have left scientists ill-equipped to handle causal questions. While the clout of statistics in scientific training and publishing impeded the introduction of a unified formal framework for causal inference in data analysis, the coining of the term “data science” and the recent influx of “data scientists” interested in causal analyses provides a once-in-a-generation chance of integrating all scientific questions, including causal ones, in a principled data analysis framework. An integrated data science curriculum can present a coherent conceptual framework that fosters understanding and collaboration between data analysts and domain experts.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384475&amp;amp;idx=1&amp;amp;sn=ab352b6fb3e5af7426c35a10eacfceff&amp;amp;chksm=84f3c4dab3844dcc2bd27b44e3f352b5486c4e3848ec7d600ad85538839b1b82fe45d69793aa&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;因果推理入门指南-必须的7个步骤&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;参考文献&lt;br/&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  Breiman(2001) Statistical Modeling: The Two Cultures&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Jianqing Fan(2019) A Selective Overview of Deep Learning https://arxiv.org/abs/1904.05526&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Pearl(2019) The Seven Tools of Causal Inference, with Reflections on Machine Learning&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Hernan(2019) A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
<pubDate>Sun, 28 Jul 2019 04:44:08 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/QwhW5PUnGL</dc:identifier>
</item>
</channel>
</rss>