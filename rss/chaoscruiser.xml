<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>三步理解Alpha Star背后的黑科技</title>
<link>http://www.jintiankansha.me/t/JnQ8WzRZsZ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/JnQ8WzRZsZ</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;efnag-0-0&quot;&gt;在Alpha Zero取得围棋领域的胜利之后，最近的大新闻是： AI又下一城，拿下了重磅即时战略游戏星际争霸。&lt;/span&gt;为什么我们说这个胜利的重要性不亚于当年的阿法狗？ 这背后的黑科技对我们又有怎样的影响。 我虽不懂星际，但我懂得强化学习， 此番为大家讲述。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiazKy0ib9CCr73OryQ3PFfptA4Xv89eCvxibKTicRicb6goZ6cDWI6UYqibJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7katb-0-0&quot;&gt;首先， 星际争霸， 是一个与围棋有着本质不同的游戏-那就是星际争霸是非完全信息下的博弈游戏。 这里的难点一个是&lt;strong&gt;不完全信息&lt;/strong&gt;， 其次是需要&lt;strong&gt;远期计划&lt;/strong&gt;&lt;strong&gt;，&lt;/strong&gt;另外就是&lt;strong&gt;实时性与多主体博弈。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;172pp-0-0&quot;&gt;何为非完全信息， 你看所有的即时战略游戏， 从红警到星际，你没有刚刚到达过的区域都被一团战争迷雾所笼罩， 那里可能有对方的军队在采矿， 或者大批小兵集结， 这可能与我此刻的决策关系很大， 但是却不为我所知。 这与围棋这样的游戏有着本质的区别， 因为围棋这样的游戏， 即使策略在复杂， 你方和我方的情况都是一目了然的， 围棋游戏的复杂体现在策略空间的巨大导致的维度灾难， 然而始终都是你知我知的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a6iej-0-0&quot;&gt;而星际这样的游戏， 却更加符合我们的真实生活， 如果你把真实的生活看成一场即时战略游戏， 从选择大学专业， 找工作， 到炒股， 你所了解到的信息， 永远是你需要知道的决策信息的一小部分， 你的对手，永远在黑色迷雾里行动。  这种黑雾， 对你的第一个挑战就是， 你要应对永恒的不确定性， 即使你有一个比目前水平强大10倍的大脑，  你依然生活在不确定性的迷雾里， 因为它是由于客观信息缺失造成的。 如果过度的渴望精确预测， 还可能由于在explore VS exploit 的权衡中偏向一端而走火入魔。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;b6tfh-0-0&quot;&gt;而多体博弈， 体现了策略建模的复杂性， 我方的最佳策略取决于有方策略，对方策略，以及盟友的策略，还有兵种的相克，组合带来的1+1大于2的过等等，游戏尽可能的模拟了真实的战争差。&lt;/span&gt;星际争霸的挑战还有超大的动作空间（可能性超过围棋全部组合数百个数量级），游戏长度很长， 而且游戏初期做出的决定会影响最终的成败。但最关键的，星际相对围棋最大的技术挑战在于&lt;strong&gt;非完全信息博弈&lt;/strong&gt;， 也是它和真实世界最接近和最有价值的地方。  &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3196405648267009&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawia1PlzceSHia3eNAxEB5ic2tttcmGryLTlL46PAJDG6xdumic9pkibesEgHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;779&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是alpha Star 如何处理从输入的图像到神经网络提取的特征，最终到做出的决策（右下）以及预测的实时预测的胜率（右上），而我们最关注的是中间的部分。为了让大家了解alpha star 背后的底层技术， 我们就从最基本的ABC开始， 给大家引入：&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;58bo6-0-0&quot;&gt;A， 强化学习 + 深度监督学习框架&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;deuq9-0-0&quot;&gt;首先，这几年的深度学习进步主要用到的是监督学习技术， 监督学习容易取得突破时因为它们最好训练，监督数据自带正确答案， 机器每分钟都在学到有用信息。 而如果我们要首先更普世的人工智能， 监督学习时不够用的， 因为智能体行为的本质时根据未来的目标制定当下的行为策略，这点监督学习无能为例， 而是需要强化学习的框架。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawia7baISvMSb0eiblm6MWBIwFkjKnEC1NuEGxNCT0HNgnia9SqEicTwu4ibjA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;eg361-0-0&quot;&gt;想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，之后由程序员来告诉AI该做什么，但如果有强化学习就是由AI自己决定逃跑还是战斗，哪一个更容易通往通用智能，是显而易见的，对于老虎在你面前，能够提前预测出老虎即将到来，对智能体的生存是有意义的，但对于更加复杂的，无法由程序员事先编码的情景，就需要靠强化学习来决定了你的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e4ej5-0-0&quot;&gt;然而无论是强化学习还是监督学习， 其背后都是神经网络， 这个模拟人脑智能的抽象模型作为载体实现， 神经网络把一种信号，转化为另一种信号， 实现信息的抽取和转化。在Alpha Star里我们用到LSTM这种基于时间反馈的神经网络作为内核。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4b5ei-0-0&quot;&gt;当然， 强化学习是一个对未来的优化问题， 未来有多长， 几步， 几百部， 还是几万部？  如果你走完了一万步才知道你最后的结果对不对， 那么这个信号根本不足以支撑机器的学习， 因为学的太慢。 就好比一个人在信息的荒漠里行走， 看上去一直在劳苦， 实则劳而无功。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2j32-0-0&quot;&gt;DeepMind  强化学习教父David Silver 喊出了 AI = DL + RL ， 意思就是为了解决这个问题， 我们可以先用监督学习让神经网络学习，&lt;/span&gt; &lt;span data-offset-key=&quot;2j32-0-1&quot;&gt;监督学习机器每时每刻都知道正确或者错误， 而后再上强化学习， 这样让机器学习到从当下到未来的战略思考。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2feut-0-0&quot;&gt;这就是实现alpha star最基本的框架。  监督学习为首，强化学习跟上。 那个LSTM网络先要不停的看以往人类选手的比赛记录， 并预测它们的行动和比赛结果， 直到得到一个baseline水平。  这点， 也同时让我们想到Karl Friston所提出的自由能原理和predictive coding法则。 智能体必须把预测外部世界的变化作为核心目标来建立自己内在的representation（世界表征）。&lt;/span&gt; &lt;span data-offset-key=&quot;2feut-0-1&quot;&gt;这里的监督学习过程， 可以看作不停的预测外部世界， 减少预测误差，建立世界表征的过程。&lt;/span&gt; &lt;span data-offset-key=&quot;2feut-0-2&quot;&gt;下图我们可以看到alphastar 在监督学习监督就已经达到一定的水平&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiapJhTxng8Lib0QtTkckibeKMrZ0kWMjWdZYzMaibkeL85WbRX5uWGc1eAQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图片描述训练成绩和训练时间的关系， 我们看到在0之前的训练代表机器单纯靠监督学习达到的基准线， 而后是强化学习阶段，我们看到机器在第8天开始超越TLO的水平（世界排名44）， 在第14天开始达到MaNa水平（职业排名13）&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;akkah-0-0&quot;&gt;B  马尔可夫 vs 局部马尔可夫决策框架&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8m40d-0-0&quot;&gt;强化学习本质上还是一个数学优化优化算法。 它的起初的成功得自一个称为Markov决策的框架。&lt;/span&gt;&lt;strong&gt;所谓的马尔可夫， 就像物理把原子看成一个行星轨道模型一样， 是一种对真实问题的霸王硬上弓的强行简化， &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先你把真实过程分解为无数离散的时刻，这样， 无论问题多复杂，都可以分解为状态（state）- 行为（action）- 奖励（reward）- 策略（policy）四要素。状态是每个时刻你拥有的和决策相关的信息总和，行为是你的决定， 奖励是你的此刻收益。 而策略就是从状态到行为的对应关系。后所谓的马尔可夫，就是说， 当下的行为决策只和当下的状态相关。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaTDKskFuTIMawmdESH1vOSyXj9W8icAyIGA77EH7whe670OXWialwPEtg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;eetub-0-0&quot;&gt;其实做马尔可夫决策模型的人脑子里的世界就是一个经典的方格迷宫。  你的状态就是你所在的方格, 你的行为就是上下左右走，你的奖励是中间的加号。 行走的过程每一步都只与上一步相关，此刻拥有绝对信息（位置），因此可以简化为下图b的马尔科夫决策图框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7899159663865546&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaojHBBWwDOccHDnveS2ZEVlhbQZjr8EDJERgIwTsUMcrO9AngW8ku0Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;238&quot; /&gt;&lt;/p&gt;

&lt;p&gt;a&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cpmor-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  看到这个你是不是觉得和星际争霸相去甚远我在骗你呢？ No! .   事实上， 如果你进行比较凶狠的简化，还真的差距不大。  首先， 星际游戏给它拆解开无非包含这样的几个步骤：  1，收集资源   2， 建立生产单元   3， 建立军事单元和武装  4，  部队集结并摧毁敌方生产和军事单元 。 所谓的战略，首先是在这些不同步骤（状态）之间的顺序和时间调配。 当然你要说地图实在太大太复杂了， 你不要忘了深度学习的核心思维， 特征抽取， 请看下图， 机器眼里的星际争霸地图。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaJD9jtd1SCcQUG6rqsIfj2fe1yOjLbTwGYNtjTibfUkEgMsmoeySeaqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8230h-0-0&quot;&gt;机器眼里的星际争霸地图， 就是这样一个离散的单元特征图。上图左边的大图每个格子里有不同的单位，显示整体的布局， 然后对于不同的核心信息， 我们可以又有右边分开的格子图表示（比如某种敌方单元的分布）。  大家是不是想到了卷积网络眼中的特征图？  如果你把这样的特征图看成机器真正“行动”的地图， 你看是不是它就和刚刚那个机器走方格游戏的距离大大拉近？  我们有离散的状态， 可以通过一个个的格子空间表示，然后可以决定采用的动作，此刻的动作决定下一刻的状态， 这些都和走方格游戏类似， 只不过特征很多很复杂，一次要控制多个单元而已。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ekd1e-0-0&quot;&gt;稍等， 此处还有一个特点是刚刚说的马尔可夫框架所不能包含的，那就是特征图里面有那么多黑色区域啊！ 这些黑色区域代表了未知，未知里面敌方的部队可能在集结， 说明当下的状态不包含决定我的决策的所有信息了 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dabb9-0-0&quot;&gt;那怎么办呢？ 此处马尔可夫框架很自然的扩展到局部马尔可夫框架。不要忘记神经网路是有想象力的，它可以脑补黑色区域里的东西。 如果你通过想象填充了那些黑色区域， 你就再一次回到了马尔可夫的世界。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1fc1l-0-0&quot;&gt;怎么脑补？ 你想象一下你自己如果要预测一下明天发生什么， 你第一步要做的是不是会议昨天做过什么呢？  是的， 我们的记忆， 不是帮助你追忆似水年华，而是更好的预测未来（此处自行联想predictive coding 预测编码理论）！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8r5je-0-0&quot;&gt;我们说神经网络是一个巨大的函数模拟器， 如果你要用一个神经网络把过去和当下的信息综合起来， 然后拟合未来的画面，这个神经网络就是循环神经网络RNN，&lt;/span&gt;然后用神经网络来学习这个函数，这就自然的引入了深度强化学习。通过数据可以模拟函数，有了函数就可以把值函数的问题解决。它是类似模仿生物工作记忆的原理，信息放在记忆里面，将其作为决策的依据。RNN的网络结构和前向神经网络差距并不大，只是多了一个时间转移矩阵，即当下隐藏的状态再到下一步隐藏状态的连接。&lt;/p&gt;

&lt;p&gt;这个连接实际上是传递当下的信息到下一刻的过程，这个传递过程中包含了记忆，我们可以把过去的信息往将来不停的迭代，于是神经网络细胞当中就含有过去很多时刻的记忆，其实就是生物的Working Memory。LSTM只是在RNN的基础上多了一记忆细胞层，可以把过去的信息和当下的信息隔离开来，过去的信息直接在Cell运行，当下的决策在Hidden State里面运行。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5se5a-0-0&quot;&gt;加入RNN（LSTM）的神经网络，可以极好的根据对过去信息的整合来预测那些黑色区域里可能是有敌军部队还是有矿藏， 就可以用来驾驭星际争霸。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiageYJiauSx1oe3OulicC1a9IecKMp83HTe1nbNHeXPuCMfOHhcBDE58ibw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RNN随时间展开的反向传播， U是隐层连接&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaNMAhLLkE8QWmb3Ax5CQr57q0GOJIjrTsABYstkk8uKZ08Q2vQvFAEA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LSTM： 加入Cell&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;5se5a-0-0&quot;&gt;lstm网络中motif（重复单元），通过增加memory cell 引入动态的可训练的记忆单元&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;6gtru-0-0&quot;&gt;C  技术细节创新&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6gtru-0-1&quot;&gt;刚刚说的大框架， 可以帮助我们解决很多动作类游戏（doom）或真实世界的空间导航问题（deepmind： navigation in complex world）。 但是没有一些关键细节的创新， 依然无法在星际这样的游戏里取胜。 我们看看星际具体使用了哪几个新技术呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d4ugo-0-0&quot;&gt;1，&lt;/span&gt; &lt;span data-offset-key=&quot;d4ugo-0-1&quot;&gt;多任务联合训练：&lt;/span&gt; &lt;span data-offset-key=&quot;d4ugo-0-2&quot;&gt;星际争霸最大的任务“赢得战争”里其实包含无数小的任务单元，比如对不同种类矿物的采集， 对不同敌军单位的攻击等， 这些可以分开训练，分别设定奖励函数， 这些小的任务会让整个游戏的学习加速很多。 这种把最难的任务分解为很多基本任务分开训练的思路， 应该可以被需要应用强化学习的工业应用大量采纳。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1vifo-0-0&quot;&gt;2.  &lt;/span&gt; &lt;span data-offset-key=&quot;1vifo-0-1&quot;&gt;多体博弈问题，&lt;/span&gt; &lt;span data-offset-key=&quot;1vifo-0-2&quot;&gt;保留所有训练中产生的历史版本， 并在其之间进行对战， 这有点类似于生物“基因池”的概念。 这个过程里，我们可以做到不会把某些个体学到的有效信息轻易遗忘，而是在多体博弈里有效的运用所有已经学会的东西， 同时选择出那些最强的策略， 最终的训练结果是一个达到纳什均衡状态的最强策略池， 这不得不让人联想到进化算法， 让人想到不同AI算法的融合趋势。 这个不同策略的博弈过程， 也是alphastart对未来的智能算法最有启发的部分之一。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;cbi8h-0-0&quot;&gt;3，&lt;/span&gt; &lt;span data-offset-key=&quot;cbi8h-0-1&quot;&gt;RNN联合注意力attention框架改进&lt;/span&gt;&lt;span data-offset-key=&quot;cbi8h-0-2&quot;&gt;： Pointer Networks。我们直到所有RNN（lstm） 在当下序列建模里的成就都和attention机制的结合有关。 而pointer network又对之前的attention进行了改进， 用输出的结果直接反向影响注意力，决定需要注意的对象， 并且这个输出的尺寸是可变的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5083333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaBUzneQuGkicYqYHc7AzA44wXBR2XUGEhmicMtBhFbTPg7yXyMUiaapMag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;pointer network， 根据网络输出反向决定attention焦点(所要关注的输入信息)的框架， 左边是传统seq2seq网络， 右边是新引入的框架。 Pointer Networks 2017&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1ue6-0-0&quot;&gt;4  &lt;/span&gt; &lt;span data-offset-key=&quot;1ue6-0-1&quot;&gt;自回归模型。&lt;/span&gt; &lt;span data-offset-key=&quot;1ue6-0-2&quot;&gt;这里就牵涉到策略的细节表示， 星际游戏的策略是典型的动作序列，而且在不完全的马尔可夫决策框架下所有决策取决于历史， 因此对策略的表达方法用到了自回归的思想， 首先把当下的动作决策表述为所有过去动作为条件的一个条件概率，然后假定这些过去的动作对当下的影响是独立的， 所以可以写成连乘的形式：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.19166666666666668&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceQ9kpfbgAD2J2RsRkqgiawiaPsu2D9JVSP38a8pFlusQp2C810MzfVFGFPsFrHKCF46rXEict7utFJQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8km2i-0-0&quot;&gt;5， &lt;strong&gt;关于和人类的成绩比较&lt;/strong&gt;： 有的人在阿法start的结果刚出来的时候说这应该是机器可以在单位时间能够操控更多的动作导致，事实上deepmind对机器的行为做了诸多限制， 让它不是单纯因为“速度快” 这种愚蠢的原因赢得人类。&lt;/span&gt;&lt;/p&gt;


&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span data-offset-key=&quot;6m1sj-0-0&quot;&gt;阿法star对我们的启示：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d5m4d-0-0&quot;&gt;这一次的alpha star，依然是有人说这不过是谷歌的广告， 强化学习目前是&lt;span&gt;处在上升期的人工智能技术， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d5m4d-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.67&quot; data-w=&quot;600&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fqjdo-0-0&quot;&gt;然而我们毫无疑问看到了AI在非完全信息博弈， 这个与真实世界的大多数任务非常接近的框架所取得的巨大进步。  AI是否会很快开始指导一场真正的人类战争？ 我想依然没有那么容易， 一个最核心的问题依然是电脑游戏是可以在大脑上无限取得数据的， alphastar在短短时间里经历了人类选手200年的训练才取得了如此的成就。 想象一下， 如果到了真实世界， AI和人类的数据取得速度差不太多， 那么谁强谁弱呢？  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4igq-0-0&quot;&gt;因此， alphastar虽然诠释了AI的一个新的篇章， 却无需让我们太恐慌， 我们想的更多的应该是， 如何用alphastar里分解出的新的技术， 真正提高RL在工业里的使用效率， 比如&lt;strong&gt;如果做出能够进行更好多体协作，甚至直接和人进行配合的工业机器人，如何让它帮助我们得到更好的能源调控策略一类的具体任务。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4pked-0-0&quot;&gt;而真正的AI下一个瓶颈的突破， 一定要提高&lt;strong&gt;对数据的使用效率， 和在不同任务之间的泛化能力&lt;/strong&gt;， 如同我在上一篇文章，&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383931&amp;amp;idx=1&amp;amp;sn=5349aed7549713d893e08f07d0d44859&amp;amp;chksm=84f3c63ab3844f2cb0fbc01efe877031bbcbe25de23d6637f4e35e8e775b39806940e7084b4a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span data-offset-key=&quot;4pked-1-0&quot;&gt;&lt;span data-text=&quot;true&quot;&gt;多任务学习的未来之路&lt;/span&gt;&lt;/span&gt;&lt;/a&gt;&lt;span data-offset-key=&quot;4pked-2-0&quot;&gt;里所说的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383664&amp;amp;idx=1&amp;amp;sn=89f11f166582925c041b960035f10c37&amp;amp;chksm=84f3c931b3844027a5c484c7af41f73dada1cb15a87fe4aa776fe293e45b66c0ea96e2e20c77&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;强化学习最小手册&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;铁哥的强化学习特训课&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383605&amp;amp;idx=1&amp;amp;sn=f734603530cbeda03bd5c7ce3d5b83ba&amp;amp;chksm=84f3c974b3844062b819e778faa5fa4050539874ede0a1586eb971732828bee4dea3ab91071c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;当RNN碰上强化学习-为空间建立通用模型&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383842&amp;amp;idx=1&amp;amp;sn=b4196485b009ebd21e7e0d8db1e2cd61&amp;amp;chksm=84f3c663b3844f756cb7f0547b8f1acf03ac6aeff743d8a37c4d3024b0c30dec19afb8f2298e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Alpha Zero登上Science封面-听铁哥浅析阿尔法元&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;</description>
<pubDate>Sun, 27 Jan 2019 00:02:10 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/JnQ8WzRZsZ</dc:identifier>
</item>
<item>
<title>多任务学习的未来之路</title>
<link>http://www.jintiankansha.me/t/ykT3e9e8Ga</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ykT3e9e8Ga</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;eapio-0-0&quot;&gt;我们所熟知的各类机器学习和深度学习任务， 大部分围绕单个任务的学习，  比如人脸图像识别， 语音识别， 或者图像生成。 每到一个新的任务， 我们就换一个网络。 但是自然界中， 我们的大脑不停的处理多个任务而非一个任务， 而这千差万别的任务全靠一套神经系统。与深度学习系统另外一个深刻的不同是， 我们的大脑可以急速的学习一个新的任务， 而不需要去从海量数据中重新学习。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9fbi1-0-0&quot;&gt;大脑神奇的多任务学习能力的硬件基础是什么？  这一点又和它惊人的泛化学习能力有什么联系？  我们可否制造一个类似大脑的系统？  这些问题可以说是人工智能和计算神经科学的最前沿问题，也是未来走向通用人工智能的最重要问题。 目前来看， 有一些工作可以给我们一些启发。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;afq4h-0-0&quot;&gt;一个非常有意思的问题来自2012的一篇Science(Eliasmith, Chris, et al. &quot;A large-scale model of the functioning brain.&quot;&lt;/span&gt;&lt;span data-offset-key=&quot;afq4h-0-1&quot;&gt;science&lt;/span&gt;&lt;span data-offset-key=&quot;afq4h-0-2&quot;&gt;338.6111 (2012): 1202-1205.) ， 这个工作试图构建一个全脑网络， 来完成多任务学习。 这台机器被称为semantic pointer unified network ， 先不管这一堆很难理解的词汇， 我们就把它看成一个比较原始的人造大脑。 应该说用计算机模拟全脑的工作这绝对不是第一个， 在此前有欧洲的蓝脑计划等。 而这个框架与之前的区别在于， 它是学习导向的， 我们不去试图模仿生物大脑的细节， 而是把几个和任务学习有关的认知功能模块组合起来， 包含感官信息输入， 信息编码，信息整合， 奖励系统，记忆，信息解码， 运动信息输出这一系列模块。 我们简单的看， 它就是一个从感官到动作以得到奖励的机器。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;atune-0-0&quot;&gt;最终我们通过监督学习或强化学习来让这个系统掌握8种截然不同的任务， 包括： 1， 抄写数字  2， 图像识别  3， 奖励学习， 4， 多个数字的工作记忆  5，  数数  6， 回答问题 7 简单的数学推理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9783333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LKrLibZdnz2SvS0H31HDh9Snz5kJSbEmpliaqymY06KqyOZ3vW0MibTmjg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图A为模拟的大脑区域， 图B为spuan网络的组成部分， 包含信息编码（把具体的图像转化成抽象的神经表示），计算转化（对不同输入之间的关系进行抽取）， 奖励衡量（衡量不同输入对应的奖励）， 信息解码（从记忆里抽取神经表示为动作单元使用），以及动作控制Eliasmith, Chris, et al&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;3qd7r-0-0&quot;&gt;这台机器的每个部分都是一个人工神经网络， 且可以与真实的脑区对应上， 比如视觉输入对应V1-V4 视皮层，它把真实的视觉信息压缩成一种低维度的编码（每个图像称为这一空间的一个点， 被称为pointer）。  这种低维的信息表示形式很容易放入到工作记忆模块里（working memory）， 最终由解码网络转换（decoding）， 被动作输出网络执行（motor）。 神经网络整体状态的调控由模拟basal ganglia的网络完成（Action Selection），它可以根据当下的任务整体调节信息的流动（如同一个综控系统， 调节每个网络之前的输入阀门），  从而让大脑在不同的工作状态间灵活转换。 这也体现了功能大脑的概念， 我们不必拘泥于某个脑区的名称， 而是记住每个脑区对应信息处理的功能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4t2cg-0-0&quot;&gt;我们可以把spaun理解为一个真实大脑的最小功能模型， 这个模型与目前的深度学习网络最大的区别就在于， 它是用来学习多个任务， 而非单一任务的， 你可以理解为它是一个“万金油”型的网络， 而非专精于某个领域的书呆子。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.865&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6Lm6z6AkcxPLC3aSHIF8rGBMbzMuic0weKqSQn7IAcniaiaSheROr3tnyPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;spaun 完成数字抄写任务 Eliasmith, Chris, et al.&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2bdoo-0-0&quot;&gt;上图描述了spaun这个网络执行图像抄写任务的例子。  你给它看一个手写数字， 然后让你根据输入的数字（风格）再写一个， 类似于临摹。 这个任务的完成事实上需要涵盖人类认知的基本模块。首先， 你需要看到一个数字2， 把它压缩成神经编码， 放到大脑的工作记忆里面， 然后过一会， 根据一个召唤信号， 工作记忆里的神经编码要被提取出来， 解压缩， 然后动作执行模块开始响应， 做出一个类似的数字2来。 这个过程， 包含了基本的感知， 认知， 记忆， 和动作执行 ， 而每个部分，都由相应的模块来完成。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dvt1h-0-0&quot;&gt;应该说这个网络本身在图像识别或者生成上一定比不过当下主流的深度学习模型， 但是它的有点在于具备了一种生物多任务学习的灵活性， 而可以不拘泥于特别特定的任务结构。这种模块化的结构与多任务学习的内在联系是什么？我们可以把复杂的任务拆解成基本的认知模块， 就像刚刚说的感知压缩， 认知， 工作记忆， 动作决策， 如此，一个全新的任务， 就可以在之前的分解模块训练完好的情况下， 很快的学习得到。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dvt1h-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dqlh0-0-0&quot;&gt;当然， 这个系统还是有一些比较大的缺点， 在我看来， 比较主要的一点在于， 它太像搭建一台计算机的过程了， 需要把每个主要的功能模块一一设计出来。 我们直到大脑是一个复杂系统，它是演化的而非设计的， 可是你指望一个演化的系统来完成这个复杂的任务模块设定可能在我们能够忍耐的时间里是有点难度的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfh8b-0-0&quot;&gt;好在，一些新的工作指出这也不是完全不可能， 就在今年的Nature neuroscience上Wang XiaoJing的组做了一个用单个RNN网络进行多任务学习的尝试(Yang, Guangyu Robert, et al. &quot;Task representations in neural networks trained to perform many cognitive tasks.&quot;&lt;/span&gt;&lt;span data-offset-key=&quot;bfh8b-0-1&quot;&gt;Nature neuroscience&lt;/span&gt;&lt;span data-offset-key=&quot;bfh8b-0-2&quot;&gt;(2019))。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fc52j-0-0&quot;&gt;一个单独RNN， 被用来学习包括20个常见的认知任务（非常基础的动物学实验任务）， 惊人的现象是， 这个本来没有任何模块化的RNN， 产生了类似于刚刚说的模块化现象， 某种程度让人想到spaun里面的分解模块。这本质上正契合了演化论， 由于我们的任务之间有着共性和差异性， 在完成这样的多任务学习时候， 本来一个单个的RNN自然需要进化出多个不同的功能模块， 那些基本的功能模块正是很多任务的共同基础， 比如工作记忆这样的功能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.51&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LHRV1u61gzYyMogglbMgarEI7rrLaBrR5osaiaWgpnldD81NrO08DpuA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;“任务”的神经表示， 如果两种任务可以分解为相关的子任务， 那么它们的表示也是有类似的组合特性（出现相互重合部分）， 如果两个任务非常独立， 则可能它们的表示是完全分开的。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;djfjb-0-0&quot;&gt;当然， 这里训练的20个任务都是一些非常基本的认知任务， 而不是机器学习的同学所熟知的那类， 比如让一个猴子看到一个方向移动的点， 然后让它根据这个视觉信号在一段时间后做出一个判断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9aoh-0-0&quot;&gt;由于我们一次训练网络完成多个任务， 根据这些任务之间的关系， 网络会分化出来完全不同的结构。 比如当这些任务之间存在较强的联系的时候， 我们就会得到所谓的不同任务的神经表示， 它们依然类似于高维空间种的点， 只是点和点的距离表示了任务之间的差异， 你可以联想一下Word2vec的编码， 不同单词的语义关联被抽象成了空间里不同点的距离关联， 如此得到的结果， 我们就可以取得不同任务之间的学习迁移能力， 类似于踢足球的技能可以由于和打篮球的相似性而别转化为打篮球的技能。 你也可以把这个学习到的不同任务之间的联系看成一种先验， 由了正确的先验， 就可以大大的简化后面的任务的学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LwicaicYp5JK3D3Bl1k45rIFGHsRljMDicfHkMibyfB8iaDOPfrXa8x4m4Ag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;“任务”和“任务”之间的代数运算， 当任意两对任务之间的差距是一样的， 比如都是需要加入工作记忆，那么这两对任务之间的向量就是一样的。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dfgr2-0-0&quot;&gt;这篇文章的惊人之处在于它通过学习沟通了几个神经科学的核心问题。 一个普通的循环神经网络RNN， 在多任务学习的基础上， 涌现出模块化的结构， 和模块之间的关联又恰恰体现任务之间的联系， 那些共同使用的模块，代表了任务之间所共享的部分， 比如工作记忆。 联系之前那篇spaun文章所说的多脑区模型， 我们恍然大悟为什么大脑会演化出这样的多区结构， 它体现了我们所执行的多姿多彩的认知任务， 是互相紧密联系的， 而共同依赖于一些基础模块， 无论是工作记忆， 还是对感知信息的编码和解码。 如果我们能够构建类似的多模块结构， 或者把它通过大量的基础认知任务给学习出来， 就可以做到， 在新任务的学习里， 迅速通过之前任务学到的结构加速学习。  这无疑指出一个救治机器学习的泛化能力缺失问题的可行之道。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383879&amp;amp;idx=1&amp;amp;sn=be6682d117cebc143f439796a11bbe4b&amp;amp;chksm=84f3c606b3844f10217b6886660d1f5ca18ed626f20c0cb0547f6714a6f5fd22493189737797&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习是怎么巧妙揭开大脑工作原理的&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 24 Jan 2019 18:41:49 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ykT3e9e8Ga</dc:identifier>
</item>
<item>
<title>Nature子刊机器智能综述-通过神经进化（ neuroevolution）设计神经网络</title>
<link>http://www.jintiankansha.me/t/4dP9dO54Yz</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/4dP9dO54Yz</guid>
<description>&lt;p&gt;Nature新子刊Machine intelligence中有一篇12页的综述，汇总介绍了神经进化这一前沿的研究方向在神经网络中的众多应用。本文用5分钟概述该文的主要观点，对于不了解神经进化的读者，本文将先带你认识神经进化说的是什么？你不需要太多的背景知识，也能读懂本文。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;人工的神经网络是依靠梯度的反向传播来进行优化的，而在生物界中，神经网络中并没有指出优化方向的梯度，感知从下而上正向传播，之后相近的刺激带来共同激活的神经元，再用这些连接来对新事物编码及预测，而一切都依赖于进化机制。所谓神经进化，就是用遗传算法来进行神经网络的结构生成，参数更新及整体的效率优化。其基本的循环是突变-&amp;gt;选择-&amp;gt;繁衍-&amp;gt;再突变。&lt;/p&gt;

&lt;p&gt;下图来自莫烦Python的视频，其中对比了两种神经进化的策略，一种是不固定网络的结构，通过神经网络间的交叉配对形成下一代的网络，另一组是固定结构，每一代网络中通过引入突变改变连接的强度，最终俩者都通过进化的优胜劣汰来实现神经网络的最优化。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5467625899280576&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECnib3mUNJpjyOPa0EyM8zMzdBwe1aTBvia0aSyf0iajHrlU4o8MdyXPXWA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;695&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不同于传统的随机梯度下降，是基于对现在错误来源的外推决定下一步进化的方向，即使引入了随机性，也只是在原有方向上引入高斯误差，是一种事后的弥补，而神经进化是通过在下一代中引入在算法空间中性质完全不同的点，之后根据适应度在这些点之间进行内推，虽然速度慢，但是可以更大规模的并行处理，且能够更好的避免陷入局部最优。&lt;/p&gt;

&lt;p&gt;神经进化不止在监督学习中应用广泛，在深度强化学习中也有广泛的应用。Uber开发的开源工具&lt;span pingfang=&quot;&quot; sc=&quot;&quot; arial=&quot;&quot; simsun=&quot;&quot; sans-serif=&quot;&quot; left=&quot;&quot; rgb=&quot;&quot;&gt;Visual Inspector for Neuroevolution（VINE），可以&lt;/span&gt; &lt;span pingfang=&quot;&quot; sc=&quot;&quot; arial=&quot;&quot; simsun=&quot;&quot; sans-serif=&quot;&quot; left=&quot;&quot; rgb=&quot;&quot;&gt;用于神经演化的交互式数据可视化工具。而下文的作者之一也来自Uber的AI实验室。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.276006711409396&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbEC8RRxKWIXTgGJYOL57jgkROnvEqfNAT48VUsHsmAryPWOOdoGtLnKsw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1192&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在结束背景介绍之后，进入这篇论文本身的介绍。作者首先指出了神经进化相比神经网络的几个独特的能力，包括通过学习找到合适的网络组成部分（例如激活函数），以及网络的超参数（有几层，每层有多少神经元）以及用于的学习策略本身。不同于AutoML的自动化调参，神经进化始终在搜索答案中保持着一个多样的解法“种群”，而且由于神经进化的研究和传统的神经网络并没有多少交集，因此俩者之间的汇总更容易擦出火花。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;最初的神经进化关注小规模网络的拓扑结构的演化，最初的进化算法仅仅是通过（神经元）连接矩阵间的权重加上随机突变来展开，之后受到基因间调控网络的启发，对网络结构展开了间接的编码。随着引入在俩个网络结构中的杂交（crossover），神经进化可以探索更为复杂的网络结构，但需要面对如何避免让新生成的网络结构由于缺少足够的时间进行局部优化而无法发挥出其最优的性能，该方向上最显著的成果是&lt;span&gt;NeuroEvolution of Augmenting Topologies (NEAT)算法，该算法的成果包括模拟机器人行走的控制程序，下图分别是使用遗传算法和进化策略&lt;/span&gt;训练模拟机器人走路（来自UberAI实验室&lt;span pingfang=&quot;&quot; sc=&quot;&quot; arial=&quot;&quot; simsun=&quot;&quot; sans-serif=&quot;&quot; left=&quot;&quot; rgb=&quot;&quot;&gt; &lt;/span&gt;Mujoco 人&lt;span pingfang=&quot;&quot; sc=&quot;&quot; arial=&quot;&quot; simsun=&quot;&quot; sans-serif=&quot;&quot; left=&quot;&quot; rgb=&quot;&quot;&gt;）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECicKFYFiaibxTJy3iatmroShXEOaVqe6kPTcFIb9ia1xBiayVJ9YBOp8JSoow/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;300&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibcfsQagibNv42s7OeLDzAxbEC7eUfuM8WoYicKd4ckOD0poOqYJO6BqGCFicrbXSyq2b9ibefuVhqPJucA/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;300&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在强化学习领域，natural evolutionary strategy可以在 Atari 游戏机上和Deep Q learning有相近的表现，而且这些算法的并行潜力使得这些算法在有足够计算资源时，可以用更快的时间完成训练，尽管神经进化需要的总的计算资源要多一些。神经进化在强化学习中的成功说明了神经进化方法可以用在现实中的复杂问题上。&lt;/p&gt;

&lt;p&gt;Lehman将神经进化和梯度结合了起来。该方法的灵感来自是通过梯度去选择出那些不那么危险的突变。由于强化学习中评估一个策略的适应度需要花费的比评估网络本身要花费更多的资源，前者需要运行游戏或者模拟环境数回合，才能看到收益，而后者只需要去将网络中的错误项前向传播几步即可。神经进化中对策略（policy）加以随机的突变，部分突变不会影响策略的性能，但少部分会让该策略彻底失效。通过对状态和行为归档记录，可以通过梯度信息对变异的大小进行缩放，从而避免突变后的策略对于当前的状态过于激进或保守，从而使得在深度超过100层的网络上可以使用神经进化的策略。&lt;/p&gt;

&lt;p&gt;神经进化可以模拟真实进化中对多样性和新奇策略的偏好，在要优化的目标中对全新的策略给予奖励，从而避免陷入局部最优，或者以策略种群的多样性为优化主要目标。在强化学习中，一个策略要想和其他策略不同，需要具有不同的基础能力，从而使策略种群多样性为优化目标好于人为设定的损失函数。&lt;/p&gt;

&lt;p&gt;总结：神经进化在meta learning，多任务学习中都可以和现有方法结合。正如卷积操作就是一种编码信息的方式，神经进化还可以找到更好的对信息进行间接编码（Indirect coding）的方法以及通过进化策略重现出类似LSTM的网络结构。强化学习中的自我对弈可以看成是神经进化的一种，而对策略多样性的偏好也鼓励了模型对新策略的探索。最后，在通向通用人工智能的路上，神经进化通过构建开放目地的（open-endedness）的系统，让策略不带有先验目地的探索，模拟自然界的进化，最终得到一个足够普适的智能系统。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383879&amp;amp;idx=1&amp;amp;sn=be6682d117cebc143f439796a11bbe4b&amp;amp;chksm=84f3c606b3844f10217b6886660d1f5ca18ed626f20c0cb0547f6714a6f5fd22493189737797&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习是怎么巧妙揭开大脑工作原理的&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;








</description>
<pubDate>Sun, 20 Jan 2019 07:40:15 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/4dP9dO54Yz</dc:identifier>
</item>
<item>
<title>《Deep learning revolution》摘录与自由能假说详解</title>
<link>http://www.jintiankansha.me/t/1ybmc9NUic</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/1ybmc9NUic</guid>
<description>&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdYlYibF3kjoPmT8MwLlSg7xVbss3bl8cFI2tSEKBo4TuoETagYEo8cX1woKYngVVeJJj89hMMqmhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;之前的年度好书（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383907&amp;amp;idx=1&amp;amp;sn=11ed3a851627305ffaf6488ee31e83d0&amp;amp;chksm=84f3c622b3844f3491f354c2d1983f9baf33dcf84c9a234e3b8d662ecafa1212752fcbba3706&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;2018人工智能十本好书汇总&lt;/a&gt;）中提到过这本书，今天就重点来说说这本书。本书的作者是深度学习的创始人之一，这本书可以说是作者的回忆录，讲述了他几十年学术生涯的经历，包括他和很多著名科学家，例如和发明Mathematica的Walform及发明DNA双螺旋的克拉克共事的经历。书中回溯了人工智能近50年来的发展史，列出了神经网络和人脑的关系，对未来AI广泛应用带来的社会影响进行了讨论，并在书的结尾探讨了生命，意识与智能的关系这个偏哲学的问题。本文将按顺序摘录书中一些有趣的新知与观点，更多内容，可以去原书查看。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;1）弗林效应，深度学习让人类更加智能。&lt;/p&gt;
&lt;p&gt;智力分为晶体智力和流体智力，不同智力随年龄变化的趋势不同，流体智力遵循抛物线式的发展，晶体智力则随着年龄缓慢增高。AlphaGo在一个狭窄的领域同时展现出晶体和流体智力，同时表现出令人惊异的创造力。弗林效应指全人类平均智商每十年上升3个点，该趋势为什么会发生，能否在未来持续，本书认为深度学习的普及在未来可以通过改善学习的体验来提升特定行业人的智能，例如之前要成为国际象棋大师需要在大城市，这样才有好老师，但国际象棋的AI让来自挪威小镇的13岁少年也能成为国际象棋大师。&lt;/p&gt;

&lt;p&gt;2）独立分量分析ICA&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3611650485436893&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECWlRjvickiaA65ICGS0IUOicU6O7cRullGIibXvRWpV4IxC4NvKDCtZwuzQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;515&quot; /&gt;&lt;/p&gt;
&lt;p&gt;上图展示了PCA和ICA的不同，PCA找到的坐标轴总是垂直的，而ICA找到沿点方向伸展的轴，来表示分离的信号，这些信号可能是不垂直的。ICA作为无监督学习的一种，被用于数千个场景，包括音频信号中去掉噪声，去除脑电及frmi中的噪声。而该方法来自大脑中的稀疏性编码，视网膜中的视觉信号的紧凑编码，在皮层中被扩展为高度分布且高度稀疏的新编码，从而使得只需要其中一小部分就能重建原图像中的任意给定区域。通过稀疏编码，可以减少自然场景中的冗余，更有效的传递图像中的信息，从而最大限度的提升网络传递的信息量。&lt;/p&gt;

&lt;p&gt;3）dropout 正则化与大脑中皮层突触失活的相似。&lt;/p&gt;
&lt;p&gt;作为常用的正则化技术，dropout会随机的删除神经网络在的部分人工神经元。而大脑中典型的兴奋性突触有90%的失败率，但当每个神经元存在数千个突触时，大树定理决定了每个神经元活动总和的偏差较低，这意味着每个神经元的性能不会有大规模的下降，而突触需要消耗大量的能量，大脑中神经元的失活可以节省能量。&lt;/p&gt;

&lt;p&gt;4）视觉皮层和卷积网络&lt;/p&gt;
&lt;p&gt;深度学习的一个优点是我们可以从网络中的每个单元提取记录，并追踪信息流从一层到另一层的转变，然后可以将分析这种网络的策略用于分析大脑中的神经元。深度学习中每层神经元的统计特性，与皮层层次结构中的每层神经元的非常接近。卷积网络的每一个性能改进，其背后都有工程师可以理解的计算理由，但有了这些变化，网络越来越接近神经科学发现的视皮层体系结构。例如对视觉层次结构的上层分类表面的分解，决策表面比下面层次的表面更平坦，这类似于卷积神经网络中高层的神经的差异较小。&lt;/p&gt;

&lt;p&gt;5）神经元的稳态的缩放&lt;/p&gt;
&lt;p&gt;突触可塑性的一个特别重要的形式上稳态（homeostasis），以确保神经元将活动水平保持在其最佳动态范围内。当突触强度降低到零或达到极限时，神经元可能永远无法获得足够的输入来达到阀值，或者接收太多的输入并始终保持高水平的活动。而对所有神经元的突触整体进行归一化，可以维持神经元活动的平衡。如平均活动速率过高，所有兴奋性突触的强度都减少，抑制性突触的强度增加。而在深度学习中，梯度爆炸/消失与上述的大脑中的突触强度过大/小类似，且由于随机梯度下降也会带来不同维度变化率不一致的问题，从而需要通过批量正则化来解决。稳态缩放和神经网络中训练技巧的对应，再一次展现了神经科学和深度学习之间存在相得益彰的共生关系。&lt;/p&gt;

&lt;p&gt;6）算法空间与Wolfram‘s law&lt;/p&gt;
&lt;p&gt;想象一个充满了所有算法的空间，这个空间中的每个点是一个可以完成对应任务的算法，其中有些是有用且高效的。之前这些算法是手工创作的。而Wolfram根据对元细胞自动机的统计，发现在算法空间中不必手术很长一段路径，就能找到解决一类有趣问题的算法。这意味着对于一个问题，算法空间中存在大量有效的解法。在神经网络的空间中，也发现其符合Wolfram定律，而之前读过的《&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383691&amp;amp;idx=1&amp;amp;sn=c3eec0a6581f4d995b2b720deefa3f05&amp;amp;chksm=84f3c9cab38440dc4322b5a9c9c1b4b608b1c1ee4437f6098e8b83a908a66940500a2d61ff5b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;适者降临&lt;/a&gt;》则推测在进化中也存在类似的结构。算法空间中存在众多效果相同的解，这带来一个问题，是否存在一种比梯度下降更快的方式需要数据量更小的方式去找到这种解。一种可以借鉴的是生物进化中的间断性平衡，通过偶然的跳跃性的剧变加上每个物种基于随机误差的局部搜索，从而在不同类型间的个体中探寻未知的边界。&lt;/p&gt;

&lt;p&gt;7）神经形态芯片&lt;/p&gt;
&lt;p&gt;不同于谷歌的TPU或者麒麟的NPU，神经形态芯片可以用来模拟人脑中神经元突触连接与放电脉冲，其中处理的是连续的而不是离散的信号。这使得神经形态芯片的能耗显著降低。在未来摩尔定律注定会遇到物理定律的限制而失效时，神经形态芯片的发展值得关注。&lt;/p&gt;

&lt;p&gt;8）自然界中的损失函数是什么？&lt;/p&gt;
&lt;p&gt;深度学习取决于对一个损失函数的优化，而在自然界中，不同的网络中，例如基因网络，代谢网络，免疫网络，神经网络，社交网络，食物链网络中各自有着怎样的损失函数了，这些不同的损失函数的背后是否存在一个通用的模式，从而导致了不同约束条件下的多样性了？这是这本书最后几张提出的问题。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;针对这个问题，简单说说我最近看的一篇综述，其中对这个问题作出了自己的回答，虽然作者的解释有一些宽泛，但还是有一些道理的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5793397231096912&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbEC6rAIY6I23miaVnU1f7Wn5MLEZOOHHSwPKPeEXfncicQhCVpRjjqAxGOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;939&quot; /&gt;&lt;/p&gt;


&lt;p&gt;自由能在之前的文章讲述过（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383879&amp;amp;idx=1&amp;amp;sn=be6682d117cebc143f439796a11bbe4b&amp;amp;chksm=84f3c606b3844f10217b6886660d1f5ca18ed626f20c0cb0547f6714a6f5fd22493189737797&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习是怎么巧妙揭开大脑工作原理的&lt;/a&gt;），说的是内外之间的差别。任何一个动态的有生计的存在，都要和外界环境打交道，外界环境不会是固定的，而有生命的个体，在其内部要保持相对的稳态，对于一个细胞是这样，对于一个器官，一个生物，一个生态系统也是这样。为了保持内在的稳态，可以未来进行预测，也可以选择性的采样，就是选择待在那些对维持自己稳态友好的坏境在。对于大脑来说，选择的是第一条路，即预测误差的最小化，而对于癌细胞来说，则是通过改变周围的环境来让自己更容易保持稳态。所谓自由能是热力学中的一个概念，说的是系统减少的内能中可以转化为对外做功的部分。自由能的最小化之所以可能是所有有生命的网络都需要优化的损失函数，正是由于有生物的东西积聚能量不易，因此要避免将其消耗在对外做功上。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5236559139784946&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECN1NltiaibUfzNG7ePt3hFkq8DaVEV0iaGzUOOVPcmkj14m8yNW49uwZTA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;930&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从自由能的角度来看，生物体为何能维持负熵，在于在每一个尺度上，通过进化，以降低自由能为目标，都找到了维持相对稳态的方法，由于算法空间中有众多可行的解法，从而导致了生物的多样性。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.20630749014454666&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECkTtZ8TBoU8y3MicBu6btj6fWDlYfC2kXHqdNyfkI2xbBGStY81ne3Xw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;761&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于个人来说，自由能理论也不止是智识上的游戏，文中指出了俩种减少自由能的方法，一种是通过行动带降低意外的最大值，另一种是扩大你认知的范围，让你能够看到更多。比如你在一家企业工作，你和企业组成的整体之间就存在着自由能，这包括企业解雇给你带来的冲击，长期在一家企业工作带来技能退化，企业中流行的习惯给你带来的伤害（例如长时间的加班或者推卸责任），所有这些都是你要避免的。类似的，选择和谁构建家庭，也可以从自由能的角度去思考，要使组成的家庭中的自由能最小，首先要确定家庭中会出现那些意外，是个人三观，还是成长背景，经济地位的，在想想该如何通过改变认知及行动的模式（习惯）来降低自由能，从而使家庭稳定。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1109375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结一下，本文摘录了《深度学习》这本书中的一些精彩内容，展示了这本书的内容的深度与广度，整本书围绕作者（&lt;span&gt;美国四大国家学院（国家科学院、国家医学院、国家工程院、国家艺术与科学学院四院院士&lt;/span&gt;）的个人经历展开。文末针对书中提出的问题，结合最近读到的文献，给出了回答，并结合实际生活，指出我们该如何理解并应用自由能最小化的假说。我们的大脑有分层推理的功能，并由此模拟世界的因果顺序从而减少预测误差。大脑通过模拟因果顺序，试图揭露和解释自己输入的感觉信息，努力维持在预测误差最小的状态，而人工智能模拟因果顺序和最小化预测误差是有条件的，这也指出了当前的人工智能和通用智能之间的差距。通用智能来自于简单媒介之间的相互作用，通过对具体问题的深入了解，可以为更综合的理论铺平道理，从而对自然界中的损失函数是什么给出一个比自由能更普世的解答。&lt;/p&gt;




</description>
<pubDate>Sat, 19 Jan 2019 03:16:37 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/1ybmc9NUic</dc:identifier>
</item>
</channel>
</rss>