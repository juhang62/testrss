<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>谷歌大脑的“世界模型”简述与启发</title>
<link>http://www.jintiankansha.me/t/ppyxZ1iWM6</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ppyxZ1iWM6</guid>
<description>&lt;p&gt;我们的视觉看到什么，部分取决于大脑预测未来会看到什么，例如下图中，如果你预计要看到突出的球体，那也许你就会看到，如果让机器也具有了这样的能力，会带来什么了？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkCLP1ia37xna8aoibHPOOBCLFQTOzxuLjqpcN9xVEbSUSRn1VPiaXqOTHg/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;205&quot; data-cropy1=&quot;13&quot; data-cropy2=&quot;218&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmO13hGkUN4v5e3Eol7Rh0t5ccR205fP1HDW3YDsTMzeXsalwAfjgicg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;205&quot;/&gt;&lt;/p&gt;

&lt;p&gt;18年谷歌大脑提出“世界模型”(World Models)可以在复杂的环境中通过自我学习产生相应的策略，例如玩赛车游戏。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7259615384615384&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkmuuDIugmtDicLnq0dNH7YmfVBpw6FUnghDlVMd1PlwYYqibu2f7v7mBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;416&quot;/&gt;&lt;/p&gt;

&lt;p&gt;下面是世界模型的整体架构:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7060185185185185&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkPwWNeqEkezB3PJqGoXtBmibptFBfYIoE0aw4bXxmDySfZcz7DCG8xyA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;432&quot;/&gt;&lt;/p&gt;

&lt;p&gt;整个模型分为3个组件：视觉组件（V），记忆组件（M），控制组件（C）。视觉组件V用来压缩图片信息到一个隐变量z上（其实只是一个VAE编码解码器）：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.6&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkrbBKiaPqBkmPQf3bAYrmU37icPH8icpIrskk3GDib2YUNUnv6iaIz7PibAUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;305&quot;/&gt;&lt;/p&gt;
&lt;p&gt;记忆组件M的输入是一帧帧的游戏图片（论文中的一帧图像似乎叫一个rollout），输出是预测下一帧图像的可能分布，其实就是比一般LSTM更高级一些的MDN-RNN：&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4832&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkT0zvQibp5s1TH0RPNUfNfaM6x7602Lfw2kibx0aXn9N2mqh5rR0na1kA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;625&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后控制组件C的目标，就是把前面视觉组件V和记忆组件M的输出一起作为输入，并输出这个时刻智能体agent应该做出的动作（action）。&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;在所谓的“世界模型”，其中的组件模型几乎没有是谷歌大脑自己创新研制的。但世界模型会很大提高强化学习训练稳定性和成绩 从而使其与其他强化学习相比有一些明显优势，如下表所示;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4444444444444444&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkicKDOBicjWeKZEElnsCzm1qzbbOuBfl5shc6gTfyWAbFOCe5ZQRjsjHQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;558&quot;/&gt;&lt;/p&gt;

&lt;p&gt;世界模型有如下的3个特点&lt;/p&gt;
&lt;p&gt;1. 模型拼接得足够巧妙，这个巧妙的拼接模型做到所谓的世界想象能力，就是模型在学习时，自身对环境假想一个模拟的环境，甚至可以在没有环境训练的情况下，自己想象一个环境去训练。其实就是我们人类镜像神经元的功能。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8233532934131736&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkpsQ5KpvR3byIp6ibS2QHKg40F6oRiaMXtLpglgFbz009kBiby8qibdHFug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;

&lt;p&gt;2. 抓住了一些“强视觉”游戏的“痛点”。记忆组件M中的RNN是生成序列的能手，所以根据之前游戏图像再“想象”一些图像帧应该不成问题（RNN生成一些隐变量z，再根据隐变量z，由视觉组件VAE的decode生成的图像帧即可）。所以对于“强视觉”的游戏，把RNN的记忆能力用在视觉预测和控制上是个好主意 。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkMOk09Pib2ia2DDq3HBhuZWqX4NEpsuzbvOx3wDDs0P9kCkVWZBxhEqZw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;334&quot; data-cropy1=&quot;9&quot; data-cropy2=&quot;251&quot; data-ratio=&quot;0.7245508982035929&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkfDoibG4whRIFVy2NAKAibSORrc71CBLG15xdf9WTv9BjOUWbvaxuugPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;334&quot;/&gt;&lt;/p&gt;


&lt;p&gt;3 不同于我们常见的“不可生”智能算法，例如遗传算法和进化策略只是强调了基因的“变异”与在解空间中进行搜索，神经网络只是固定网络结构；而生物界的基因却可以指导蛋白质构成并且“生长”。如果基因可以构造自身个体，外部环境和个体情况也可以反过来影响基因，而我们的模型都太固定呆板了，模型结构不能随内部隐变量改进，当然最佳的设计形式也许谁也不知道。而世界模型做到了让在内部”幻想“的环境中产生的策略转移到外部世界中。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8567073170731707&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrksicaWK7Ls5EJMsF6hU4iatAd6MG7orOxwHCIZAumfzWvyhjRwWlJU1Dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;328&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;最后简单看一下&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;世界模型&lt;/span&gt;&lt;span noto=&quot;&quot; serif=&quot;&quot; start=&quot;&quot; rgb=&quot;&quot;&gt;的训练过程：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5024752475247525&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkLmQXpD9K5Ic89FCTQT7LA9RlljB2QmHj2E10dv56IWNjqszIhA6MpQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;404&quot;/&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;world models代码&lt;/span&gt;&lt;span&gt;基于&lt;/span&gt;&lt;span&gt;chainer&lt;/span&gt;&lt;span&gt;计算框架，步骤如下:&lt;/span&gt;&lt;/p&gt;
&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1. 准备数据集，随机玩游戏生成训练帧（rollouts意思应该就是多少帧）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python random_rollouts.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --num_rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;10000&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;2. 训练视觉组件V，即前面提到的VAE：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python vision.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;3. 训练记忆组件M，即前面提到的RNN：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python model.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--epoch&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;20&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;4. 训练控制组件C，即前面提到的CMA-ES算法（其实就是支持更复杂输入和更新的ES）：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python controller.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --lambda_&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;64&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mu&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;0.25&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--trials&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--target_cumulative_reward&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;900&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--cluster_mode&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p noto=&quot;&quot; serif=&quot;&quot; baseline=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;5. 测试训练结果：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot;hoverEnabled enlighterEnlighterJS EnlighterJS list-paddingleft-2&quot; readability=&quot;-1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;python test.&lt;/span&gt;&lt;span class=&quot;me0&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--game&lt;/span&gt; &lt;span class=&quot;kw3&quot;&gt;CarRacing&lt;/span&gt;&lt;span class=&quot;&quot;&gt;-v0 --z_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--hidden_dim&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--mixtures&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--temperature&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--weights_type&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;&quot;&gt;--rollouts&lt;/span&gt; &lt;span class=&quot;nu0&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;br0&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;&quot;&gt;--record&lt;/span&gt;&lt;span class=&quot;br0&quot;&gt;]&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://arxiv.org/pdf/1803.10122.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://github.com/AdeelMufti/WorldModels&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文经作者授权，转载自David9的个人博客，著作权属于“David 9的博客”原创，&lt;strong&gt;如需转载，请联系微信: david9ml&lt;/strong&gt;。原文地址： http://nooverfit.com/wp/谷歌大脑的世界模型world-models与基因学的一些思考/#comment-3444&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383931&amp;amp;idx=1&amp;amp;sn=5349aed7549713d893e08f07d0d44859&amp;amp;chksm=84f3c63ab3844f2cb0fbc01efe877031bbcbe25de23d6637f4e35e8e775b39806940e7084b4a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;多任务学习的未来之路&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383923&amp;amp;idx=1&amp;amp;sn=d717ecdc31f055bdcf7d8381c9e8b5c9&amp;amp;chksm=84f3c632b3844f243769de14d893e09d7db3fd4c43367a3eb61ed54ab0720aa2134750b4922a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Nature子刊机器智能综述-通过神经进化（neuroevolution）设计神经网络&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 28 Jan 2019 03:38:08 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ppyxZ1iWM6</dc:identifier>
</item>
<item>
<title>自由能最小说的是什么？又该怎么批判性的去看了？</title>
<link>http://www.jintiankansha.me/t/Z82f8XZpeW</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Z82f8XZpeW</guid>
<description>&lt;p&gt;&lt;span&gt;写在前面的话：本文来自与与许铁，Justin的讨论，其中很多文字都来自许铁，作者应该标明为三人的，只是微信不允许这样标注作者。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;卡尔·弗里斯顿（Karl Friston）是世界上最有影响力的神经科学家之一，他最近提出的一个理论，可以看成是生物科学，认知科学与人工智能的大一统理论，该理论试图用一个原理回答薛定谔提出的“生命为何会产生负熵”的问题，还论及了&lt;span&gt;部落、宗教和物种的产生与生命是什么这个问题有共通的答案&lt;/span&gt;。这个理论就是&lt;strong&gt;“自由能量原理”（free energy principle）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkUnr4dF5n7bytAjt8bcqFLjIgKUTBedpEVUavyEfrlrMl9FJ58psdzw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;984&quot;/&gt;&lt;/p&gt;

&lt;p&gt;自由能原理的前身是对大脑运行机制的一种描述方式，即predictive coding（预测性编码），说的&lt;strong&gt;是感知、运动控制、记忆等大脑功能，都依赖于大脑对现有经验和未来期望的比较&lt;/strong&gt;，大脑在每一层次上，都对特定环境下最可能有什么体验的预测和感官传入的实际信息进行对比，如果发现预测出来的结果不对，那就需要神经系统中更高层的介入，（例如潜意识搞不定的才会显式的进入意识，自动处理系统二预测失败的，才需要耗时更多的手动系统一的介入），通过改变信念或者预测基于的假设，让思维”成为一种“受控制的幻想”。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;认知的过程用机器学习的语言说就是用大脑的内部变量来模拟外部世界， 并希望建立内部世界和外部的一个一一映射关系。 这里我们说认知的模型是一个概率模型，并且可以被一系列条件概率所描述。如果用一个形象的比喻来说， 你可以把你的大脑看成一个可以自由打隔断的巨大仓库， 你要把外部世界不同种类的货放进不同的隔断，你的大脑内部运作要有一种对外界真实变化的推测演绎能力， 即随时根据新的证据调整的能力， 你和外界世界的模型匹配的越好， 你的脑子就运转越有效率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;熟悉神经网络的训练方式的可以想到反向传播算法，或者自编码器（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383465&amp;amp;idx=1&amp;amp;sn=e579b06baa00207e66f8668a0e161a23&amp;amp;chksm=84f3c8e8b38441fe71ff5765963224016e511aff653e4e5e04b61ce1c679cec5a520030764e7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;点击查看介绍文章&lt;/a&gt;），但更贴切的例子是生成查询网络，根据包含几个物体的单幅照片，预测从另一个视角看这些物体是什么样子。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4607142857142857&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrknWB3TKNuPwQ7TsvOu3LKgsEp971rYvdgz9gyo3Kx2n6icbmNduSM4cg/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;560&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图片来源 Deepmind&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;而自由能最小原理，就是用统计物理的范式，将预测误差用数学化的方式描述为自由能。&lt;span&gt;什么是物理里的自由能法则？  一句话说就是：  &lt;/span&gt;&lt;strong&gt;任何处于平衡状态的自组织系统均趋于自由能极小的状态。&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;我们的大脑通过几个标准模块是感知， 认知与行为和外界环境进行相互作用。认知系统包括生命本身一个核心的目的，就是预测外界环境的变化， 如果能够准确预测， 就是说预测误差最小，它就满意了 ，这个预测环境误差最小。&lt;/p&gt;

&lt;p&gt;之所以说自由能最小理论是大一统的，在于其&lt;strong&gt;以用一个原理了感知，运动控制和情感的产生。&lt;/strong&gt;在感知过程中，思维模型被调整；在运动控制中，外部世界被调整；而情感反映了大脑对与生存密切相关的指标的预测，例如血糖不足时你会焦虑，这份焦虑促使你找到食物，从而减少了预测的和实际观察的（血糖水平）的差距。对精神类疾病，例如精神分裂，自闭症，也可以解释为实现自由能最小化这个目标时走火入魔，前者是大脑过分关注自己做出的预测，而无视那些与预测背离的感官信息，而后者是感觉器官相连的低级神经活动的预测误差被过分强调了。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6549192364170338&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibce3ibLYiaxiae6cyGudjrvRxrkU4uzmtnUeBn9viagvd93C31cl7bziaThmWqspHurvklPKHlDGavq2icBg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;681&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;该图对自由能原理进行了概述，解释了两种计算自由能的方式，自由能量是期望状态与测量状态之差。当大脑做出的预判不能很快被感受器证实时，大脑可以通过以下两种方式之一来使自由能量最小化：修改预判——接受意外，允许错误，更新世界模型；或者主动让预判成真。当你把自由能量最小化，也就意味着意外最小化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;马尔科夫毯（Markov blanket）是自由能假说的一个关键组成部分，通过将毯内和毯外的交互限定在特定条件下，要么通过改变内部观察到的模型或者观察其他地方的外部环境，使得内在状态不直接改变外界环境，保护毯内状态不受外部影响，马尔科夫毯可以看成是“认知版本的‘细胞膜’。每个人身体内部也存在各式各样的马尔科夫毯，有分隔器官的、分隔细胞的，还有分隔细胞器的。&lt;/p&gt;

&lt;p&gt;将自由能理论进一步展开，就会推广出无论是生命组织本身还是大脑，都需要不停的构建外部世界的模型。  因为你的模型与外部世界越匹配， 就可以越好的预测外部环境的变化， 从而减少感知误差。有生命的个体，在其内部要保持相对的稳态，对于一个细胞是这样，对于一个器官，一个生物，一个生态系统也是这样。例如细胞层面不同基因的表达量，转录后的修饰，蛋白的折叠不同，都可以看成是为了减少对周围环境的预测误差所做出的行动。&lt;/p&gt;

&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfsQagibNv42s7OeLDzAxbECN1NltiaibUfzNG7ePt3hFkq8DaVEV0iaGzUOOVPcmkj14m8yNW49uwZTA/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.5236559139784946&quot; data-w=&quot;930&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;不同尺度下不同的最小化自由能的方式&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;生物体为了保持内在的稳态，可以未来进行预测，也可以选择性的采样，&lt;/strong&gt;即选择待在那些对维持自己稳态友好的坏境里。对于大脑来说，选择的是第一条路，即预测误差的最小化，而对于癌细胞来说，则是通过改变周围的环境来让自己更容易保持稳态。每一个级别的有机体，都是一个不断在计算内部稳态和外部环境相吻合的程序，所有这些程序不断叠加起来就是我们见到的生命的层级结构，由此产生了自组织，涌现与负熵。而进化本身，则被看做是一个虚拟的“物种” 在对环境做一个贝叶斯推断和决策，由最新采纳的证据更新先验概率得到后验概率，最终也是为了最小化预测误差。&lt;/p&gt;

&lt;p&gt;从总体上来说， 这个理论给出了一个内涵非常丰富的思维框架。 按照该理论的暗示， 无论是生命组织本身还是大脑，都需要不停的构建外部世界的模型。  因为你的模型与外部世界越匹配， 就可以越好的预测外部环境的变化， 从而减少感知误差。自由能最小解释了在一个熵增的热力学宇宙， 生物系统如何能够保持秩序， &lt;strong&gt;由于这种预测外部世界的趋势， 生物系统不再遵守熵增， 而是热力学下的自由能最小， 从而保持了秩序。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; pre-wrap=&quot;&quot; rgb=&quot;&quot;&gt;从这个原则出发， 我们立刻可以得到：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c834i-0-0&quot;&gt;1， 大脑神经编码的的bottom up processing ， bottom up 自下而上， 做的是感知信息的压缩， 为什么要压缩？ 因为&lt;strong&gt;虽然感知信息是高维的， 但是驱动真实世界变化的动因却往往是低维的&lt;/strong&gt;， 你把它压缩成低维， 就有更大的可能得到主要的预测因子。 想象一下整个深度学习的成功的一个关键点在自编码器，这个降维工具的训练成功， 也就可见一斑。   感知神经系统采取类似CNN的多层结构， 做到步步压缩。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6joof-0-0&quot;&gt;2， 大脑神经编码的top down processing，这个步骤所做的是， 大脑的高级神经皮层， &lt;strong&gt;通过联想， 寻找外部世界下一步可能出现的状态，&lt;/strong&gt; 这里面包含的根本原理在于， 如果我们需要预测外部世界的变化， 我们一定需要做到的是内在世界的模型某种程度就是外部世界的映射， 这样， 内在世界的变化， 就可以作为外部世界的预测， 这也就是我们说的predictive coding。 这个东西，反过来通过top down 也就是从深层向表层的连接， 改变我们的感知， 使得我们有了相由心生这样的现象。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;akhnk-0-0&quot;&gt;3，  记忆网络。  如果内在世界需要是外部世界的映射。 而外部世界的网络方程是相互连接的动力学系统， 那么内在世界的模型也应该是类似的包含正负反馈，相互连接的动力学网络，这也正是神经网络的数学本质， 我们抽象出来称为一个常见的机器学习模型RNN，为了让记忆变得可变，引入了长程与短程的记忆单元，即让神经网络学会了遗忘的LSTM。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3bgnb-0-0&quot;&gt;能够得到如此内涵丰富的联想，已经说明了这个理论的博大，但是也不得不看到， 这个理论的本身也包含一些自身难以解决的问题：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ejfa2-0-0&quot;&gt;1， 我觉得该理论最难解决的一点在于认知和行为的关系。 如果说认知要做的是正确的预测， 那么大脑就会变成一个越来越完善的分类器或回归模型， 但是事实不是如此，任何生物系统需要做到的是正确的行为， 那么， &lt;strong&gt;正确的行为是不是正确的预测呢？&lt;/strong&gt; 我们想象一下， 那些细菌， 可能没有丰富的认知， 但是它们的行为具有极好的鲁棒性又如何理解呢？  我们说的， 生物系统的存在本身是为了存在和繁衍， 又如何放到这个框架之下？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ejfa2-0-0&quot;&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4e7r3-0-0&quot;&gt;举一个非常小的例子说明， 假定动物认知和行为的目标是对外界环境的预测误差最小， 那么一个小鼠无非呆在一个墙角里不停的撞墙， 这个时候它一定能精准的预测每次撞墙的作用力， 但是这样的小鼠应该早就被自然淘汰了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6t15-0-0&quot;&gt;2， 过于野心的统一之梦。 与爱因斯坦希望统一基本作用力一样， Karl希望统一从生命到认知在内的所有生物相关的复杂系统现象， 然而， 我们直到， 任何理论往往存在适用边界，这个统一的梦想是否一定正确呢？&lt;strong&gt;一个解释了所有事情的理论，其实就是什么都没有解释。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bt469-0-0&quot;&gt;3， 可证伪性， 如果Karl的理论可能是正确的， 那么有没有一系列的理论可以对其进行证否呢？ 这是一个非常关键的问题， 因为如果一个理论体系无法有效的实验证伪， 解释力再强也称不上科学。 目前来看，&lt;strong&gt; 自由能最小原理数学相对严密， 但缺少特别有效的实验系统&lt;/strong&gt;。  因此也没有得到神经科学界的普遍认可，仍然是一个有争议的假说，虽然最近有越来越多的实验暗示自由能最小是有道理的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dij0n-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  4, 强化学习： 从自由能最小的理论看，行为的产生也是为了给世界一个合理稳定的解释，这一点上看， 这一点和目前的强化学习不完全符合。 虽然强化学习里追求的奖励最大化， 可以转化为时间差分学习里的最小TD误差， 但是那里的核心是我们&lt;strong&gt;需要最有效的预测未来的奖励大小， 而不是单纯的预测下一秒会出现什么，&lt;/strong&gt; 你能预测下一秒出现老虎， 不代表就不会被老虎吃掉。  但在复杂的坏境下，&lt;/span&gt;奖励最大化 AI 表现明显“没那么稳定”；主动推理 AI 则会更好地适应环境。这一点在神经网络进化（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383923&amp;amp;idx=1&amp;amp;sn=d717ecdc31f055bdcf7d8381c9e8b5c9&amp;amp;chksm=84f3c632b3844f243769de14d893e09d7db3fd4c43367a3eb61ed54ab0720aa2134750b4922a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;点击查看相关文章&lt;/a&gt;）中也有涉及，一开始也没急于求成，先积极探索环境，降低预测误差，这样通过进化产生的模型会更有可能在一个变动的环境中脱离最优解。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f81pn-0-0&quot;&gt;5， 心理学证实： 作为一个被认为能够统一所有心智现象的模型， 显然要与心理学的所有现象符合。 这一点看， Karl 的理论可以说是解释了一部分的心理学现象。 比如你我都讨厌不确定性， 都希望生存在一个稳定可预测的世界。 但是还是刚刚说的， 谁也不想预测自己明天会死， 而是预测自己明天会发大财， 单纯把预测客观世界看成一种目标， 恐怕很难说是符合心理学现象的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f81pn-0-0&quot;&gt;对于个人来说，&lt;span&gt;自由能理论也不止是智识上的游戏，文中指出了俩种减少自由能的方法，一种是通过行动带降低意外的最大值，另一种是扩大你认知的范围，让你能够看到更多。比如你在一家企业工作，你和企业组成的整体之间就存在着自由能，这包括企业解雇给你带来的冲击，长期在一家企业工作带来技能退化，企业中流行的习惯给你带来的伤害（例如长时间的加班或者推卸责任），所有这些都是你要避免的。类似的，选择和谁构建家庭，也可以从自由能的角度去思考，要使组成的家庭中的自由能最小，首先要确定家庭中会出现那些意外，是个人三观，还是成长背景，经济地位的，在想想该如何通过改变认知及行动的模式（习惯）来降低自由能，从而使家庭稳定。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;神书《有限与无限的游戏》将世界一分为二，一种可称为有限游戏，另一种为无限游戏。有限游戏以取胜为目的，&lt;span&gt;有限游戏有获胜者，那么这个游戏必须有一个明确的终结。有人获胜，有限游戏便终结了。&lt;/span&gt;而无限游戏以延续游戏为唯一的目的。生命玩的是一个无限的游戏，而现在的AI，不管再怎么厉害，都还是在玩有限的游戏。这本书中有一句话，和自由能最小化也有关系，是这样说的，“&lt;strong&gt;训练在未来重复已完成的过去，教育将未完成的过去延续到未来。”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;训练需要的是有标记的数据，会带来不同奖励的数据，训练的过程只是通过迭代去找出最优的马后炮式的解法。而通过&lt;strong&gt;将预测的误差降低，就将过去的信息应用到了未来&lt;/strong&gt;，这也是终身学习者应有的心态。世上只有一种无限游戏，这个无限的游戏将其他所有的有限游戏当作子游戏。就如我与地坛的结尾写道“宇宙以其不息的欲望将一个歌舞炼为永恒，这欲望或游戏有着一个怎样的名字，我们大可忽略不计。“，宇宙的欲望，按照自由能解说，就是最小化其预测误差。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑：人工智能的救赎之路？&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;














</description>
<pubDate>Sun, 27 Jan 2019 07:45:56 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Z82f8XZpeW</dc:identifier>
</item>
<item>
<title>多任务学习的未来之路</title>
<link>http://www.jintiankansha.me/t/ykT3e9e8Ga</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ykT3e9e8Ga</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;eapio-0-0&quot;&gt;我们所熟知的各类机器学习和深度学习任务， 大部分围绕单个任务的学习，  比如人脸图像识别， 语音识别， 或者图像生成。 每到一个新的任务， 我们就换一个网络。 但是自然界中， 我们的大脑不停的处理多个任务而非一个任务， 而这千差万别的任务全靠一套神经系统。与深度学习系统另外一个深刻的不同是， 我们的大脑可以急速的学习一个新的任务， 而不需要去从海量数据中重新学习。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9fbi1-0-0&quot;&gt;大脑神奇的多任务学习能力的硬件基础是什么？  这一点又和它惊人的泛化学习能力有什么联系？  我们可否制造一个类似大脑的系统？  这些问题可以说是人工智能和计算神经科学的最前沿问题，也是未来走向通用人工智能的最重要问题。 目前来看， 有一些工作可以给我们一些启发。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;afq4h-0-0&quot;&gt;一个非常有意思的问题来自2012的一篇Science(Eliasmith, Chris, et al. &quot;A large-scale model of the functioning brain.&quot;&lt;/span&gt;&lt;span data-offset-key=&quot;afq4h-0-1&quot;&gt;science&lt;/span&gt;&lt;span data-offset-key=&quot;afq4h-0-2&quot;&gt;338.6111 (2012): 1202-1205.) ， 这个工作试图构建一个全脑网络， 来完成多任务学习。 这台机器被称为semantic pointer unified network ， 先不管这一堆很难理解的词汇， 我们就把它看成一个比较原始的人造大脑。 应该说用计算机模拟全脑的工作这绝对不是第一个， 在此前有欧洲的蓝脑计划等。 而这个框架与之前的区别在于， 它是学习导向的， 我们不去试图模仿生物大脑的细节， 而是把几个和任务学习有关的认知功能模块组合起来， 包含感官信息输入， 信息编码，信息整合， 奖励系统，记忆，信息解码， 运动信息输出这一系列模块。 我们简单的看， 它就是一个从感官到动作以得到奖励的机器。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;atune-0-0&quot;&gt;最终我们通过监督学习或强化学习来让这个系统掌握8种截然不同的任务， 包括： 1， 抄写数字  2， 图像识别  3， 奖励学习， 4， 多个数字的工作记忆  5，  数数  6， 回答问题 7 简单的数学推理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9783333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LKrLibZdnz2SvS0H31HDh9Snz5kJSbEmpliaqymY06KqyOZ3vW0MibTmjg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图A为模拟的大脑区域， 图B为spuan网络的组成部分， 包含信息编码（把具体的图像转化成抽象的神经表示），计算转化（对不同输入之间的关系进行抽取）， 奖励衡量（衡量不同输入对应的奖励）， 信息解码（从记忆里抽取神经表示为动作单元使用），以及动作控制Eliasmith, Chris, et al&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;3qd7r-0-0&quot;&gt;这台机器的每个部分都是一个人工神经网络， 且可以与真实的脑区对应上， 比如视觉输入对应V1-V4 视皮层，它把真实的视觉信息压缩成一种低维度的编码（每个图像称为这一空间的一个点， 被称为pointer）。  这种低维的信息表示形式很容易放入到工作记忆模块里（working memory）， 最终由解码网络转换（decoding）， 被动作输出网络执行（motor）。 神经网络整体状态的调控由模拟basal ganglia的网络完成（Action Selection），它可以根据当下的任务整体调节信息的流动（如同一个综控系统， 调节每个网络之前的输入阀门），  从而让大脑在不同的工作状态间灵活转换。 这也体现了功能大脑的概念， 我们不必拘泥于某个脑区的名称， 而是记住每个脑区对应信息处理的功能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4t2cg-0-0&quot;&gt;我们可以把spaun理解为一个真实大脑的最小功能模型， 这个模型与目前的深度学习网络最大的区别就在于， 它是用来学习多个任务， 而非单一任务的， 你可以理解为它是一个“万金油”型的网络， 而非专精于某个领域的书呆子。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.865&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6Lm6z6AkcxPLC3aSHIF8rGBMbzMuic0weKqSQn7IAcniaiaSheROr3tnyPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;spaun 完成数字抄写任务 Eliasmith, Chris, et al.&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2bdoo-0-0&quot;&gt;上图描述了spaun这个网络执行图像抄写任务的例子。  你给它看一个手写数字， 然后让你根据输入的数字（风格）再写一个， 类似于临摹。 这个任务的完成事实上需要涵盖人类认知的基本模块。首先， 你需要看到一个数字2， 把它压缩成神经编码， 放到大脑的工作记忆里面， 然后过一会， 根据一个召唤信号， 工作记忆里的神经编码要被提取出来， 解压缩， 然后动作执行模块开始响应， 做出一个类似的数字2来。 这个过程， 包含了基本的感知， 认知， 记忆， 和动作执行 ， 而每个部分，都由相应的模块来完成。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dvt1h-0-0&quot;&gt;应该说这个网络本身在图像识别或者生成上一定比不过当下主流的深度学习模型， 但是它的有点在于具备了一种生物多任务学习的灵活性， 而可以不拘泥于特别特定的任务结构。这种模块化的结构与多任务学习的内在联系是什么？我们可以把复杂的任务拆解成基本的认知模块， 就像刚刚说的感知压缩， 认知， 工作记忆， 动作决策， 如此，一个全新的任务， 就可以在之前的分解模块训练完好的情况下， 很快的学习得到。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dvt1h-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dqlh0-0-0&quot;&gt;当然， 这个系统还是有一些比较大的缺点， 在我看来， 比较主要的一点在于， 它太像搭建一台计算机的过程了， 需要把每个主要的功能模块一一设计出来。 我们直到大脑是一个复杂系统，它是演化的而非设计的， 可是你指望一个演化的系统来完成这个复杂的任务模块设定可能在我们能够忍耐的时间里是有点难度的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfh8b-0-0&quot;&gt;好在，一些新的工作指出这也不是完全不可能， 就在今年的Nature neuroscience上Wang XiaoJing的组做了一个用单个RNN网络进行多任务学习的尝试(Yang, Guangyu Robert, et al. &quot;Task representations in neural networks trained to perform many cognitive tasks.&quot;&lt;/span&gt;&lt;span data-offset-key=&quot;bfh8b-0-1&quot;&gt;Nature neuroscience&lt;/span&gt;&lt;span data-offset-key=&quot;bfh8b-0-2&quot;&gt;(2019))。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fc52j-0-0&quot;&gt;一个单独RNN， 被用来学习包括20个常见的认知任务（非常基础的动物学实验任务）， 惊人的现象是， 这个本来没有任何模块化的RNN， 产生了类似于刚刚说的模块化现象， 某种程度让人想到spaun里面的分解模块。这本质上正契合了演化论， 由于我们的任务之间有着共性和差异性， 在完成这样的多任务学习时候， 本来一个单个的RNN自然需要进化出多个不同的功能模块， 那些基本的功能模块正是很多任务的共同基础， 比如工作记忆这样的功能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.51&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LHRV1u61gzYyMogglbMgarEI7rrLaBrR5osaiaWgpnldD81NrO08DpuA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;“任务”的神经表示， 如果两种任务可以分解为相关的子任务， 那么它们的表示也是有类似的组合特性（出现相互重合部分）， 如果两个任务非常独立， 则可能它们的表示是完全分开的。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;djfjb-0-0&quot;&gt;当然， 这里训练的20个任务都是一些非常基本的认知任务， 而不是机器学习的同学所熟知的那类， 比如让一个猴子看到一个方向移动的点， 然后让它根据这个视觉信号在一段时间后做出一个判断。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a9aoh-0-0&quot;&gt;由于我们一次训练网络完成多个任务， 根据这些任务之间的关系， 网络会分化出来完全不同的结构。 比如当这些任务之间存在较强的联系的时候， 我们就会得到所谓的不同任务的神经表示， 它们依然类似于高维空间种的点， 只是点和点的距离表示了任务之间的差异， 你可以联想一下Word2vec的编码， 不同单词的语义关联被抽象成了空间里不同点的距离关联， 如此得到的结果， 我们就可以取得不同任务之间的学习迁移能力， 类似于踢足球的技能可以由于和打篮球的相似性而别转化为打篮球的技能。 你也可以把这个学习到的不同任务之间的联系看成一种先验， 由了正确的先验， 就可以大大的简化后面的任务的学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.3166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcemnnzC2Qv7ITPZN5IvDI6LwicaicYp5JK3D3Bl1k45rIFGHsRljMDicfHkMibyfB8iaDOPfrXa8x4m4Ag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;“任务”和“任务”之间的代数运算， 当任意两对任务之间的差距是一样的， 比如都是需要加入工作记忆，那么这两对任务之间的向量就是一样的。&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dfgr2-0-0&quot;&gt;这篇文章的惊人之处在于它通过学习沟通了几个神经科学的核心问题。 一个普通的循环神经网络RNN， 在多任务学习的基础上， 涌现出模块化的结构， 和模块之间的关联又恰恰体现任务之间的联系， 那些共同使用的模块，代表了任务之间所共享的部分， 比如工作记忆。 联系之前那篇spaun文章所说的多脑区模型， 我们恍然大悟为什么大脑会演化出这样的多区结构， 它体现了我们所执行的多姿多彩的认知任务， 是互相紧密联系的， 而共同依赖于一些基础模块， 无论是工作记忆， 还是对感知信息的编码和解码。 如果我们能够构建类似的多模块结构， 或者把它通过大量的基础认知任务给学习出来， 就可以做到， 在新任务的学习里， 迅速通过之前任务学到的结构加速学习。  这无疑指出一个救治机器学习的泛化能力缺失问题的可行之道。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383879&amp;amp;idx=1&amp;amp;sn=be6682d117cebc143f439796a11bbe4b&amp;amp;chksm=84f3c606b3844f10217b6886660d1f5ca18ed626f20c0cb0547f6714a6f5fd22493189737797&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;机器学习是怎么巧妙揭开大脑工作原理的&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 24 Jan 2019 18:41:49 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ykT3e9e8Ga</dc:identifier>
</item>
</channel>
</rss>