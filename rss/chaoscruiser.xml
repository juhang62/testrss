<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何让让神经网络开口说话</title>
<link>http://www.jintiankansha.me/t/o5DC8CEtwb</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/o5DC8CEtwb</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;人类简史说，人类的进步大大依靠与人的语言，人在灵长类中具有最高级的语言能力，并且能够编故事，从而把越来越多的人组织在一起产生了文明，这才是人类社会进化的根本。&lt;/span&gt;那么，神经网络能否掌握人类的语言，听懂人类的故事呢？ 相信朋友们都知道谷歌翻译，谷歌助手，机器写诗， 它们实现了令人惊艳的性能，并在某些具体任务里让人真假难辨。这里的技术核心，就是&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;-&lt;/span&gt;&lt;span&gt;我们常说的传说中的循环神经网络。&lt;/span&gt;RNN&lt;span&gt;可以称得上是深度学习未来最有前景的工具之一。它在时间序列（比如语言文字，股票价格）的处理和预测上具有神功。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.65379113018598&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kNw72RI3PwDHibjxNr7t1oNQLSv5FLvmPR6GDkZTg0F5aJ9WutibYapUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;699&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;时间序列和&lt;/span&gt;RNN&lt;span&gt;引入&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;让我们从基础学起，&lt;span&gt;首先&lt;/span&gt;什么是时间序列，&lt;span&gt;我们又&lt;/span&gt;为什么需要&lt;/span&gt;RNN&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6744730679156908&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k663I0G9glMsglxZ9z9S1YH5lmWibkRWLeaj0kU2MwM4WGibDickz81R0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6227848101265823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kwibz1UiaqqF3Aqtee5PF7wZJAFfRAVgWGoGHicAfb7XibGyhibKztNtay6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;395&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;时间序列，&lt;span&gt;是&lt;/span&gt;一个随时间变化的过程，&lt;span&gt;我们&lt;/span&gt;把它像一个数列一样排列下来，&lt;/span&gt; &lt;span&gt;&lt;span&gt;序列&lt;/span&gt;里的数字往往&lt;span&gt;看起来&lt;/span&gt;在随机波动。&lt;/span&gt; &lt;span&gt;&lt;span&gt;一定&lt;/span&gt;程度，&lt;span&gt;我们&lt;/span&gt;可以把它看做以一个一维的图像或向量，&lt;span&gt;这个&lt;/span&gt;图像不停的向前滚动。&lt;span&gt;比如说文字，就是一个典型的时间序列。&lt;/span&gt;处理和时间有关的信息，我们再次回到我们的大脑，看我们的大脑是怎么处理这一类问题的。&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;实验发现，大脑越靠近感官的区域就越像CNN的结构，它的最本质特征是前馈，也就是每一次神经信息都是从感官向大脑的深层一步步推进的，而每层网络之间是没有联系的。而到了深层，这一切发生了变化，大脑内开始出现一些神经网络，这些网络在层间出现了很多的连接，这意味着什么呢？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8511705685618729&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQek5FIxZefqx6QJemfLiceYcbaLAG6P7V3JV0ibsxH67IUb3yAOLSO2A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; &lt;span&gt;&lt;span&gt;我们说，我们需要引入时间这个维度才能完全理解这件事，这些层内的链接意味着当下的神经信息会在下一个时刻被层内的其它神经元接受，而这个接受的结果呢？&lt;/span&gt; &lt;span&gt;当下的信息会传给这些层内的神经元，&lt;/span&gt; &lt;span&gt;从而使得这个信息在网络内回响一段时间，就好像社交网络里人们互相发送消息或分享朋友圈一样一个事件发生，就会引发一系列的社交网络动作使得信息的影响停留很久。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个典型的例子是如果人和人彼此链接形成回路，我发出的信息可能会在若干时间又传递给我，从而让我自己直接看到我的过去历史，这个信息停留的效应是什么？大脑处理时间相关信息的关键正是记忆。 最典型的在一个对话过程里，你要记住此时此刻所说的话， 还要关联前面很久的话，否则就会发生歧义。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;引入了记忆， &lt;span&gt;这些问题就好解决了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;记忆分为不同的种类， &lt;span&gt;你在一个对话里可能回忆起很久以前的事情，&lt;/span&gt; &lt;span&gt;但是这和刚刚说的话的重要性显然不一样，&lt;/span&gt; &lt;span&gt;所以我们大脑就把这些记忆分成了长时的记忆和短时的记忆（工作记忆）。这个短时记忆，&lt;/span&gt; &lt;span&gt;正是通过互相高度连接成回路的网络之间的神经元互相喊话造成的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;一个神经网络里神经元之间彼此互相传递信号，形成循环往复的回路结构，就可以让过去的信息保持下来，把&lt;/span&gt;这个&lt;span&gt;原理转化成人工神经网络就是&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;，&lt;span&gt;翻译出来就是循环神经网络，这个循环，正是彼此链接形成回路的意思。我们看怎么把模糊的想法一步步变成一个数学模型。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5401174168297456&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kI3cicAeAs54DqNJF4toeQm7fEB9FGnQ0MINuiauBAydgoGdhvrnTJ67g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1022&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们把这个网络内部神经元互相链接的方式用一个矩阵Wij表达， 也就是从&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;i&lt;/span&gt;到&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;j&lt;/span&gt;的连接强度表示。&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wij&lt;/span&gt;就是神经元间彼此沟通的方法，&lt;/span&gt; &lt;span&gt;我们看rnn的方程，一方面网络要接受此刻从外部进来的输入，另一方面网络接受自己内部的神经元从上一个时间时刻通过&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Wi&lt;/span&gt;进来的输入，这一部分代表过去的历史 ，决定网络此刻的状态。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23720136518771331&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kAiaMKQFH5unWWVxrqymUcsCoMmTxMaVhNnsI8TZic9CdBicyhhib9Z51Eg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;586&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;深度学习的核心就是特征提取。用一句话说，&lt;/span&gt;RNN&lt;span&gt;具备的素质就是把整个过去的时间信息转化成此刻的一组特征，然后让网络做预测或者决策。 比如，此刻股市的价格是所有之前和公司有关的信息一点一滴积累起来的。公司每个时间点的信息， 就是输入向量，&lt;/span&gt; &lt;span&gt; 那么神经元所干的事情是什么呢？&lt;/span&gt;  &lt;span&gt;它把所有过去的信息转化为当下的神经元活动， 而这些活动，就是一组由过去历史信息组成， 决定当下预测的特征。在股价的情况下，你可以想象成， 这些神经元就在描绘人们的信心指数， 而信心是所有过去点信息汇集的结果，RNN把每个时间步的信息通过神经元之间的互相喊话Wij，压缩成当下的一个状态向量hi，它包含了所有和我此刻决策有关的历史。 数学上你可以推到， 这个hi是所有过去输入的一个函数.&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;学说话&lt;/span&gt;&lt;/span&gt; &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;什么叫预测说话，这是一个形式上开起来有点傻的例子， 就是给你一个序列的字母或单词让你猜下一个，我想你一定玩过报纸上的填词游戏。 那个每个时间步骤的输入就是一个字母，然后我要输出最后一个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;一个处理这个问题的经典模型叫N-gram&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;它是说我可以把这个猜词的游戏看成一系列条件概率来处理，&lt;span&gt;用这个模拟过去的字母对当下字母的影响。因为我们知道语言本身存在非常清晰的规律，&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这个规律就是字母都是成对出现的，&lt;/span&gt; &lt;span&gt;如果全面我给你的字母是&lt;/span&gt;hell&lt;span&gt;， 且还有一个字母，那么你基本就知道是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;o&lt;/span&gt;&lt;span&gt;了，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;这个现象我们可以用字母共现概率表达，也就是说衡量一些字母在一起的概率&lt;/span&gt;P&lt;span&gt;（&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;w&lt;/span&gt;&lt;/span&gt;1, w2,w3…&lt;span&gt;&lt;span&gt;）&lt;/span&gt;,&lt;/span&gt; &lt;span&gt;&lt;span&gt;那些经常在一起出现的字母概率会很大，&lt;/span&gt; &lt;span&gt;而其他很小，&lt;/span&gt; &lt;span&gt;我们可以用经典的条件概率公式来表达，&lt;/span&gt;&lt;/span&gt; &lt;span&gt;&lt;span&gt;这个事情。&lt;/span&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.03968253968253968&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kQz8rY74Za5wiaIaMLlVu9rZfE2QY8lKzQ2WOQoPnQAzJ1LFGdOY2DUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1260&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个公式的意思非常清楚， 就是字母和字母之间不是互相独立的， 就是刚刚说的后面的字母极强的依赖于前面的字母， 这种关系通过条件概率体现。 这件事我们用一个图画出来，就是当下是所有过去的作用， 而一旦字母多了以后，这件事就特别复杂。我们用物理学家惯用的加入假设来简化降维，就是我当下的字母与过去相关，这种相关却是有限的， 比如只与前面n-1个相关。&lt;/span&gt; &lt;span&gt;这个模型就是n&lt;/span&gt; &lt;span&gt;gram&lt;/span&gt;  &lt;span&gt;。&lt;/span&gt;  &lt;span&gt;比如说 只与前面一个相关，这就是2-gram，&lt;/span&gt; &lt;span&gt;而这个模型就是标准的马尔科夫链。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;那么，这里我们换一个模型来，&lt;span&gt;我们用&lt;/span&gt;rnn&lt;span&gt;来做。具体怎么干？ 每个字母给他编个号码（独热编码），我一个一个的输入给这个网络，网络每一刻的状态取决于此刻的输入和上一刻的网络内部状态，而上一个网络内部状态又取决于过去的，输入，这样当我整个单词输出完毕，每个字母的信息可以看作都包含了在了神经元的状态里。我们要&lt;/span&gt;把整个输入切分成小块，&lt;/span&gt; &lt;span&gt;用一个卷积核把&lt;span&gt;它们&lt;/span&gt;卷入到一个隐层网络里，&lt;span&gt;只不过此处的&lt;/span&gt;&lt;/span&gt;Wij&lt;span&gt;&lt;span&gt;代替了&lt;/span&gt;CNN&lt;span&gt;的卷积核，把历史卷入到一个隐层里。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们用一段小巧的&lt;/span&gt;python&lt;span&gt;代码让你重新理解下上述的原理：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.31364031277150306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kpLMkok673hoAia1JmwT3BCCtSajFRyeRxvM9R2KMeRahYMlNZxULcAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1151&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这个结构，&lt;span&gt;非但&lt;/span&gt;优雅，&lt;span&gt;而且&lt;/span&gt;有效。&lt;span&gt;一个非常重要的点是，&lt;/span&gt; &lt;span&gt;我们不必在假定那个&lt;/span&gt;n-gram&lt;span&gt;里的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;n&lt;/span&gt;&lt;span&gt;，这时候，因为原则上&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;是所有历史输入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;I1&lt;/span&gt;&lt;/span&gt;…It &lt;span&gt;&lt;span&gt;的函数，&lt;/span&gt; &lt;span&gt;这个过程，&lt;/span&gt; &lt;span&gt;也就是说我们具有一个&lt;/span&gt;&lt;/span&gt; &lt;span&gt;infinit&lt;/span&gt; – &lt;span&gt;gram&lt;/span&gt; &lt;span&gt;。&lt;/span&gt; &lt;span&gt;&lt;span&gt;你来思考一下这是真的吗？&lt;/span&gt;No&lt;/span&gt; &lt;span&gt;，&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;你知道信息的传播是有损耗的，&lt;/span&gt; &lt;span&gt;如果把&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;展开，它事实上相当于一个和历史长度一样长的深度网络，信息随着每个时间步骤往深度传播，这个传播的信息是有损耗的，到一定程度我就记不住之前的信息了，当然如果你的学习学的足够好，&lt;/span&gt;Wij&lt;span&gt;还是可以学到应该学的记忆长度。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2611683848797251&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kFoiaJw1g1Q2BhwvicUsrbnibR4o1JDmfnOIGU1uY1cvSwkongmI6DmOcw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;582&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图：循环的本质&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;刚刚的词语预测模型，我们最终的目标是每次预测出现的下一个字母，我们的方法是在隐层之上加入一个读出权重，这个矩阵的作用如同我们之前讲的感知机，是在有效的特征基础上加一个线性分类器， 再加一个softmax函数得到一个向量每个数值代表出现某个字母的概率。然后我们希望我们的向量能够完全预测真实字母出现的概率，因此我们把真实数据作为输入不停的让他预测这个字母，直到这个概率和真实是最匹配的，我们就得到了训练好的模型， 然后我们就可以让他生成一段文本了！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8045325779036827&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kt9lGY722ic5zh0p4qugEHz7MXuAGUH9HJmXaB1ibksd7iadjJSvjLTAHA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;706&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上面介绍的过程是语言介绍的最基本的部分，这里所说的对字母进行独热编码这件事，如果进入到文本的世界，你要预测的不是字母而是单词， 这时候，我们通常不再采用独热编码，而你自己思考这件事显然是不合理的，因为词语之间相互关联，如果你把词语本身作为完全独立的东西来编码，你事实上就是丧失了这种本来的语义结构的信息，因而我们用更复杂的word2vec来替换，这个方案的核心依然是编码，只不过这套编码也是从神经网络里学过来的，具体来说，这套编码可以把语言内在的语法或语义信息包含在编码向量的空间结构里， 从而使用这部分语言有关的先验信息。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9627749576988156&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kcnRq0wZJO8FLnRkeIrAmTWoF1W7Idhuz1lS0Lp00fbqySqScyGSx1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;591&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;的物理与工程理解&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们说把对&lt;/span&gt;RNN&lt;span&gt;的理解&lt;span&gt;在&lt;/span&gt;抬升一个层次，RNN&lt;span&gt;在物理学家眼里是一个动力系统，&lt;/span&gt;&lt;/span&gt; &lt;strong&gt;&lt;span&gt;循环正对应动力学系统的反馈概念，可以刻画复杂的历史依赖&lt;/span&gt;-&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;路径依赖&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;。另一个角度看也符合著名的图灵机原理。&lt;/span&gt;&lt;/strong&gt; &lt;span&gt;即此刻的状态包含上一刻的历史，又是下一刻变化的依据。&lt;span&gt;工程上看，&lt;/span&gt; 这其实&lt;span&gt;就是&lt;/span&gt;可编程神经网络的概念，即当你有一个未知的过程，但你可以测量到输入和输出，&lt;/span&gt; &lt;span&gt;你假设当这个过程通过&lt;/span&gt;RNN&lt;span&gt;的时候，&lt;span&gt;你要设计一个程序来完成&lt;/span&gt;这样的输入输出规律，&lt;span&gt;那么这个程序可以被&lt;/span&gt;RNN&lt;span&gt;学出来&lt;/span&gt;。 &lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.071278825995807&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kMUht40XmvRwwC2zjpTEgjibqDiakMScEicDqfDYEz4tnGmlrU44Nf0hQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;477&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;总的来说&lt;/span&gt; &lt;span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;无论作为&lt;/span&gt;一个非线性动力系统&lt;span&gt;还是程序生成器&lt;/span&gt;的&lt;/span&gt;RNN&lt;span&gt;, &lt;span&gt;都需要依然数据背后本身是有规律可循的，也就是它背后真的有某种“&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;program&lt;/span&gt;&lt;span&gt;”而非完全随机&lt;/span&gt;。&lt;/span&gt; &lt;span&gt;如果一旦&lt;/span&gt;RNN&lt;span&gt;学习到了&lt;span&gt;真实&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  数据背后的&lt;span&gt;动力系统&lt;/span&gt;的性质，&lt;span&gt;它也就&lt;/span&gt;掌握了过程中复杂的路径依赖，从而能够对&lt;span&gt;过去&lt;/span&gt;和&lt;span&gt;现在&lt;/span&gt;进行建模。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;RNN&lt;span&gt;生成诗歌&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;诗歌的生成，方法类似于句子，区别在于，诗歌是有主题的，我们都知道主题的重要性，同一个句话在不同主题下含义完全不同，如何把这个诗歌的主题输入给rnn呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;事实上，我们可以把这种主题或者背景看作是一种上下文关系，比如说一首诗歌有四行，在生成第一行的时候，我可以输入开头一个关键词，然后让rnn自动生成一行，虽然这个过程还有一定随机性，但是这一行内容无疑确定了诗歌整体的基调，因此，我们需要把这种信息编码成一个包含上下文含义的内容向量，这个向量作为整个网络的输入。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3133535660091047&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4klibJGiaACy2FrV6BFJ7WN70mlv5gI20U41fV8hQD4v7FKrqEpZicuPhJw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1318&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们采取的方法是逐行生成诗歌。已经生成的行就作为后面生成行的基调，每行诗歌的生成都使用和之前一样的RNN，但是，它的输入要加上一个主题向量。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如何提取主题向量呢？首先，它是一种由所有之前生成行构成的宏观信息，那么，我们也可以用一个RNN来提取它，由于宏观， 这个RNN的输入是每行诗歌。而我们要用一个方法， 直接把行编码， 而不像刚刚是把字母或单词编码。 一行诗歌本身每个子都是一个向量，整行诗歌构成一个二维的图像（字数一定， 图像尺寸一定）。记得什么东西最好处理图像吗？我们引入一个CNN网络，把这些同样尺存的“图片”，压缩一个特征向量，最后被这个被称为主题RNN的RCM卷成一个主题向量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4824766355140187&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kCiaqcL6B6pl0WAZxmnxWK42KjPXt6sN79tkHTyicHqrpXt5BpxNBFsog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;856&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个主题向量作为一个先验输入交给RGM， 这个普通的RNN， 作为这行诗词生成的关键。由此生成的诗歌不仅是押韵的，而且可以构成一个完整的意思。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kzn6aYicN4L5a7symMoAvt8vusq6QLdcCzYdSgXJRzks7DgfhDrNibMKQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1001&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;引入长短记忆&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;RNN虽然看起来好用，&lt;/span&gt; &lt;span&gt;&lt;span&gt;而且似乎&lt;/span&gt;能够模拟任何一个&lt;span&gt;动力过程或程序&lt;/span&gt;，&lt;/span&gt; &lt;span&gt;&lt;span&gt;实际&lt;/span&gt;中，&lt;/span&gt; &lt;span&gt;&lt;span&gt;却&lt;/span&gt;并没有那么容易。&lt;/span&gt; &lt;span&gt;&lt;span&gt;为什么&lt;/span&gt;？&lt;/span&gt; &lt;span&gt;RNN&lt;span&gt;的&lt;/span&gt;强大&lt;span&gt;功能&lt;/span&gt;，&lt;span&gt;体现在&lt;/span&gt;能够学习&lt;span&gt;过去&lt;/span&gt;时间点对现在的影响这件事，&lt;span&gt;但是，我们刚刚说了&lt;/span&gt;RNN&lt;span&gt;相当于于一个无限深的深度网络， 而传播是有损失的， 假定每次这个损失是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.99&lt;/span&gt;&lt;span&gt;， 经过&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;100&lt;/span&gt;&lt;span&gt;层后也是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0.36&lt;/span&gt;&lt;span&gt;， 这种信息传递的递减， 会导致&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;无法学到长时间的信息之间的关联性&lt;/span&gt;。&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;我们的一个重要的解决这个问题的技巧是&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;引入一个能够学习多个时间尺度的改进版&lt;/span&gt;RNN&lt;/span&gt; – &lt;span&gt;LSTM（&lt;/span&gt;Long short term memory&lt;span&gt;）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6776556776556777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4koTXDZNjRYuG90Osb9NhnCzCCkCCq8FTCC50WSkObHsKBQ1bJEXQf0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;819&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;首先，什么是时间尺度，宇宙间最神秘的概念莫过于时间，但是绝对的时间毫无意义，一个时间的长短，一定是根据你所描述的过程，&lt;/span&gt; &lt;span&gt;&lt;span&gt;比如你是描述一个人一生的变化过程，还是描述一次化学反应，&lt;/span&gt; &lt;span&gt;还是生物进化，各自的时间尺度可以有级数之差。我们对时间尺度最数学的理解来自原子的半衰期。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于大脑神经尺度，处理动态过程最麻烦的东西就是时间尺度， 因为生活中的事情往往是多时间尺度的，比如你的一个决策今天晚上吃不吃饭，可能既受到刚刚是不是饿了的影响，又受到这个月你是不是有减肥计划的影响，还受到你长期养成的饮食习惯的影响，因此， 你的大脑需要有对复杂时间尺度的处理能力。也就是说，同时对各个不同的时间尺度变化的信息保持特异性和敏感度， 这和我们图像识别里需要对图像的局部和整体反应是类似的。一个有意思的电影inception描述要改变一个人的意念，我们需要一步步的走进他思想的最深层 ，逐层的改变它对某个东西的认知，而每个层里的时间尺度又有不同，就是对这件事最好的体现。事实上，类似于&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;inception&lt;/span&gt;的描述，我们的确发现在我们的大脑里，有着不同时间尺度的处理， 越浅层，我们就对越近的东西敏感，而进入到大脑的深层，我们开始对慢过程敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;LSTM&lt;span&gt;，所谓的长短记忆机，这是对这个过程的模拟。我们来看它是怎么对&lt;/span&gt;&lt;/span&gt;RNN&lt;span&gt;&lt;span&gt;进行改进的&lt;/span&gt;，&lt;span&gt;这个道理非常简单，首先，我们加入一个叫做记忆细胞的概念，进入到记忆细胞的信息，可以永久不被改变，&lt;/span&gt; &lt;span&gt;但也可以根据一定触发条件修改，实现的方法是我们加入一些&lt;/span&gt;控制&lt;span&gt;信息流动&lt;/span&gt;的阀门&lt;span&gt;在这些记忆细胞之间&lt;/span&gt;，&lt;span&gt;这个&lt;/span&gt;阀门随着输入和隐层状态决定，&lt;/span&gt; &lt;span&gt;&lt;span&gt;如果是&lt;/span&gt;1&lt;span&gt;，我们让过去的记忆完全进入到当下，信息丝毫不衰减，如果阀门的值是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;，就彻底的遗忘，如果是&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;0&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;1&lt;/span&gt;&lt;span&gt;就是在一个时间段里记住这个值就是一个时间尺度。只要&lt;/span&gt;控制好这个&lt;span&gt;阀门&lt;/span&gt;，&lt;span&gt;我们就得到了一个动态的可以学习的时间尺度。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5616161616161616&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kibwMIicLVK1VNiclzdj2JchRw3DLa25DdPhEuuVRmEXyfqVSib7N9762qA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;495&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;刚刚讲的阀门被称为&lt;/span&gt;遗忘门，&lt;span&gt;为了配合它，我们还加上输入门和输出们，&lt;/span&gt; &lt;span&gt;控制有多少新的信息流入，有多少输出，&lt;/span&gt; 这三层门&lt;span&gt;整体构成一套&lt;/span&gt;信息的闸门，门的形式都是可微分的&lt;/span&gt;sigmoid&lt;span&gt;函数，确保可以通过训练得到最佳参数。根据这一原理，我们可以抓住本质简化&lt;/span&gt;lstm&lt;span&gt;，如&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;或极小&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;GRU&lt;/span&gt;&lt;span&gt;。其实我们只需要理解这个模型就够了，而且它们甚至比&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;span&gt;更快更好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;我们看一下最小&lt;/span&gt;GRU&lt;span&gt;的结构：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42168674698795183&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kVcbiaYKKbxIBINvz2ISUyxxTcDHga7tFPdTwAdI3jhg7HVN974FPsEg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2278719397363465&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kkKhNMtqduPrfiaLqFiaHqUncdD0kH3v9iax33BgFxU4Y4eZdDHyfTiaABg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;531&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;第一个方程&lt;/span&gt;f&lt;span&gt;即遗忘门，第二方程如果你对比先前的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;RNN&lt;/span&gt;&lt;span&gt;会发现它是一样的结构，只是让遗忘门&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;f&lt;/span&gt;&lt;span&gt;来控制每个神经元放多少之前信息出去（改变其它神经元的状态），第三个方程描述&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;“&lt;/span&gt;&lt;span&gt;惯性&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;”&lt;/span&gt; &lt;span&gt;，即最终每个神经元保持多少之前的值，更新多少。这个结构你理解了就理解了记忆体&lt;/span&gt;RNN&lt;span&gt;的精髓。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Attention&lt;span&gt;版&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;LSTM&lt;/span&gt;&lt;span&gt;与宋词生成&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然，我还可以把这个多时间记忆的东西玩到极致，最终我们可以得到一个升级版本的诗词处理器，&lt;/span&gt; &lt;span&gt;我们要加入几个新的概念：&lt;/span&gt; 1&lt;span&gt;， 双向编码&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;lstm&lt;/span&gt;&lt;/span&gt; 2&lt;span&gt;，&lt;span&gt;多层&lt;/span&gt;lstm&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;3&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为什么双向，因为语言也可以倒过来念啊，甚至很多时候后面的内容越重要越能决定主题， 比如一句话一旦出现“但是”一定是但是后面的内容更中重要有决定性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.35340314136125656&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kTd77wRVfeOS7WzAsLiaibGGIic5BlVrbUlicVLQ6JaibBC6XdiaS8ibOSacOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;764&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么要多层， 记得我刚刚讲过的大脑结构吗？ 如果每层的lstm学习的事件尺度敏感性不同， 会更好的处理多事件尺度新消息， 比如从词语到句子到段落。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;最后，也是最关键的，看看我们如何引入&lt;/span&gt;a&lt;/span&gt;ttention.&lt;span&gt;。&lt;/span&gt; &lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;Google&lt;/span&gt;&lt;span&gt;这一次&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;2016&lt;/span&gt;&lt;span&gt;寄出的大法，是在其中加入了&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;机制 ，&lt;span&gt;首先大家理解人脑的注意力模型，人脑的注意力机制的特点是在认知资源有限的情况下，我们只给那些最重要的任务匹配较多的认知资源。&lt;/span&gt; &lt;span&gt;这个机制实现的方法正是&lt;/span&gt;attention&lt;span&gt;。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;首先，在引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;之前，我们的想法是既然我们最终的决策想利用好所有的历史信息，而每个时间的隐层状态都是对那个时刻时间状态的总结，我们完全可以把所有时间点的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;作为一个特征使用， 这一点， 而不只是最后的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;，这点在文章分类这类任务里特别好理解，但是每个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;动辄上百维度，所以当我们把所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;合成在一起的时候 ，我们就会得到几十万个维度，我们再次抛出降维度找重点的思维， 加入一个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;来吸取和当下预测最重要的信息。&lt;/span&gt;&lt;/span&gt; A&lt;span&gt;ttention&lt;span&gt;又称为&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;content&lt;/span&gt;&lt;/span&gt; &lt;span&gt;base&lt;/span&gt; addressing&lt;span&gt;&lt;span&gt;，因为过去哪个东西比较重要，往往取决于我现在要预测的信息。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.834070796460177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kr774SyBt7bMVY4qVm4sNr1olrWH3nAz6Nfc4XO4dgPGpBUYva0VR2g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;452&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有了这个机制，我们可以抛出我们唐诗生成器的改进版，宋词生成器，宋词的生成，确实是比唐诗更复杂的一个东西，因为宋词更长， 更多变，句子长短不同，也更需要多时间尺度。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8047244094488188&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kUlL5A2z6dcIxib9cCsOFiaGaibVDVrnQepcrmpvHqD2MYiaF43ia7EDDy5Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;635&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;这一次，我们可以把宋词的结构看作是一种对话体，&lt;/span&gt; &lt;span&gt;我们用一个把问题编码，&lt;/span&gt; &lt;span&gt;然后直接从这个基础上预测对上一句的回答，这里，我们生成第一句词的技巧和之前是类似的，&lt;/span&gt; &lt;span&gt;从这个生成的第一句词开始，我们用编码器&lt;/span&gt;LSTM&lt;span&gt;来把这一行编码作为下一句的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;， 然后解码器把这个&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;cue&lt;/span&gt;&lt;span&gt;转化为下一行词。为什么我们要用&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;呢？ 因为上一句的所有的&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;h&lt;/span&gt;&lt;span&gt;被叠加成一打，刚刚说到的，这时候会造成信息过大，所以我们引入&lt;/span&gt;&lt;span helvetica=&quot;&quot; neue=&quot;&quot;&gt;attention&lt;/span&gt;&lt;span&gt;的机制来注意相应的信息， 这样我们就可以找到上一行和下一行之间精细的相关性。&lt;/span&gt;&lt;/span&gt;   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.9306451612903226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4k2zxW29QBMksaJTicu0H9yQibxviapVtlRJSVSHvTctMCyibxLkt3HxeWtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;大家可以比较和唐诗生成的时候， 我们的结构不仅更简洁，而且能够处理更难的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;模拟人类对话&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最终， 我们已经接近了让神经网络听懂故事的境界， 你是否认为类似的结构可以用作和我们对话呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kJ6ZIVojPFnRal0gAGQFFaSicQKYYTQmaJzSTJlYhTWslB3yWOKA6uFA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;476.75359712230215&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;491.23201438848923&quot; data-ratio=&quot;1.0294117647058822&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf5diaEmI5SjKhUZmLUmaX4kqgbTa8IAQ8ofH5QVGuYaKLrsdrItYENGJ2hm9hKBDdxOx6bUicmKXfQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;476&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;当然不是，&lt;/span&gt; &lt;span&gt;因为你我在对话时候有大量的背景知识，&lt;/span&gt; &lt;span&gt;而机器人是没有的，&lt;/span&gt; &lt;span&gt;但是有没有补救的方法？&lt;/span&gt; &lt;span&gt;当然有，&lt;/span&gt; &lt;span&gt;谷歌的问答系统，&lt;/span&gt; &lt;span&gt;已经把很多重要的背景知识放入了神经网络，&lt;/span&gt; &lt;span&gt;这种加入很长的背景知识的结构，&lt;/span&gt; &lt;span&gt;被我们称为记忆网络，&lt;/span&gt; &lt;span&gt;事实上，它是对人类的长时间记忆的模拟。&lt;/span&gt;&lt;/span&gt;  &lt;span&gt;&lt;span&gt;这些长时间记忆，&lt;/span&gt; &lt;span&gt;包含知识，或者地图等。用它做的聊天机器人，可以成为你电话里的助手，但是，&lt;/span&gt; &lt;span&gt;你是否认为这样的结构已经具备了真正理解语言的能力呢？&lt;/span&gt;   &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;参考文献&lt;/span&gt; &lt;span&gt;：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The unreasonable effective RNN&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Minimal Gated Unit for Recurrent Neural Networks&lt;/span&gt;&lt;/p&gt;







</description>
<pubDate>Fri, 08 Mar 2019 14:53:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/o5DC8CEtwb</dc:identifier>
</item>
<item>
<title>东边日出西边雨：极端天气网络中的遥相关与超指数分布</title>
<link>http://www.jintiankansha.me/t/QkOJZ4teYL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/QkOJZ4teYL</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6661764705882353&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;680&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMG4rtOm0rxStSHkUcm6CUnLa7flsMwAILX9iaQqSUxSSicQRjEaicIKkPw/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1258535&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;&amp;#x6536;&amp;#x85CF;&quot;&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1172402&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;span&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;导语&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section readability=&quot;2.5&quot;&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;2月份的Nature主刊上的一篇论文，着眼于气象科学与复杂网络结合擦出的火花。这篇小文将带你以看侦探小说的视角，批判性地解读这篇论文，让你明白这篇文章在材料的呈现上有哪些值得借鉴之处。在文末，作者将结合该文，谈谈接下来可能的研究方向。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;论文题目：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Complex networks reveal global pattern of extreme-rainfall teleconnections&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;论文地址：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.nature.com/articles/s41586-018-0872-x&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;复杂网络的应用，这些年间越来越广泛，但总有一些共通的规律，最典型的就是&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247490194&amp;amp;idx=1&amp;amp;sn=a85d1f0312d455ec7ef59c6b26c7a581&amp;amp;chksm=e894401fdfe3c9095db99fb55f9ecbd9962dbe50092d69b9fb944168cd378d930bb2fb1aacd6&amp;amp;scene=21#wechat_redirect&quot; data-linktype=&quot;2&quot;&gt;不同尺度下网络呈现相同性质的幂律法则&lt;/a&gt;。不过，凡事总有例外，&lt;span&gt;这篇Nature论文的研究成果&lt;/span&gt;能够打破幂律法则的现象，必然是颠覆式的发现。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;论文摘要&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;72469&quot;&gt;&lt;section data-width=&quot;100%&quot;&gt;&lt;section readability=&quot;23&quot;&gt;&lt;section data-autoskip=&quot;1&quot; class=&quot;&quot; data-style=&quot;clear: none; margin-top: 0px; margin-bottom: 0px; line-height: 1.5em; text-align: right; color: inherit; border-color: rgb(239, 112, 96);&quot; readability=&quot;46&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;Climatic observables are often correlated across long spatial distances, and extreme events, such as heatwaves or floods, are typically assumed to be related to such&lt;/span&gt;&lt;span&gt;teleconnections&lt;/span&gt;&lt;span&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Revealing atmospheric teleconnection patterns and understanding their underlying mechanisms is of great importance for weather forecasting in general and extreme-event prediction in particular, especially considering that the characteristics of extreme events have been suggested to change under ongoing anthropogenic climate change. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Here we reveal the global coupling pattern of extreme-rainfall events by applying complex-network methodology to high-resolution satellite data and introducing a technique that corrects for multiple-comparison bias in functional networks. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We find that the distance distribution of significant connections (P &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;For longer distances, the probability of significant connections is much higher than expected from the scaling of the power law. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We attribute the shorter, power-law-distributed connections to regional weather systems. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;The longer, super-power-law-distributed connections form a global rainfall teleconnection pattern that is probably controlled by upper-level Rossby waves. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;We show that extreme-rainfall events in the monsoon systems of south-central Asia, east Asia and Africa are significantly synchronized. Moreover, we uncover concise links between south-central Asia and the European and North American extratropics, as well as the Southern Hemisphere extratropics. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Analysis of the atmospheric conditions that lead to these teleconnections confirms Rossby waves as the physical mechanism underlying these global teleconnection patterns and emphasizes their crucial role in dynamical tropical–extratropical couplings. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Our results provide insights into the function of Rossby waves in creating stable, global-scale dependencies of extreme-rainfall events, and into the potential predictability of associated natural hazards.&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;可上下滑动查看论文摘要&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;优质的摘要，第一句应该是背景介绍，目的是让读者知道这篇文章讲述的是哪个领域的发现。既然是背景，就要和日常生活有所联系，让读者能联想到&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247495932&amp;amp;idx=1&amp;amp;sn=ee1f07d4a50c6a75d3387d0c70e20e90&amp;amp;chksm=e897aa71dfe02367ffc43be660fe13378e85a14dfcce747daf01cf99c4767e4679af54feebbb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;最近的新闻报道&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;本文的第一句指出气候现象，尤其是极端情况，例如高温，暴雨等在长距离上存在关联。近期美国的寒潮，跨越数个州，这还只算是区域性的气候关联，而欧洲、亚洲与美洲同时出现的极端寒冷，就属于文中提到的“&lt;strong&gt;遥相关&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（teleconnection）&lt;/span&gt;&lt;span&gt;”了。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;说完了背景，要论述该现象的意义。假设弄清楚了气候中的遥相关，能解决什么现实问题？能带来哪些新的问题了？即如何扩展人类的认知边界。这是论述研究意义时要回答的模板问题。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;摘要第二句指出，对于极端天气的预测，如果能结合遥相关，那会更加精确。&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;amp;mid=2247495932&amp;amp;idx=1&amp;amp;sn=ee1f07d4a50c6a75d3387d0c70e20e90&amp;amp;chksm=e897aa71dfe02367ffc43be660fe13378e85a14dfcce747daf01cf99c4767e4679af54feebbb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;而极端气候现象的频繁发生&lt;span&gt;，很大的原因是人类活动的影响&lt;/span&gt;&lt;/a&gt;&lt;span&gt;，如果对极端气候现象的遥相关没有精准的描述，也就无从谈起人类的影响会不会带来改变。电影”后天“中描述的极端寒冷，在19年初出现在了新闻报道中，这说明极端气候离我们的生活并不遥远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.563&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1000&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMxIrnWOKibHTrHwViamS46mecjd9df3eBmkXuag3FVKpSLa6G9mKUDNIQ/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;电影《后天》剧照&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;核心解读&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来概要总结文章的核心发现。一般是先介绍一个通用的结论，再详细的说一个经过详细研究的案例，最后再次用更具体的例子，指出研究的意义。但在此之前，需要先将研究的问题细化。&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;研究对象选择&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;本文从极端气候中选择了暴雨。我猜想，相比于酷暑或者极端严寒，除了数据收集更容易，更准确客观，暴雨的评判标准在不同的时间更具有一致性，不会受到全球变暖与城市热岛效应的影响。因此不需要根据时间进行校正。&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;暴雨是一个仁者见仁的描述，科学上要研究，需要划一条线，来定义清楚何为暴雨。而在不同的区域，不同的年份，人们对暴雨的认知也有所不同。该研究中的设定是在一个地区所有下雨的日子中，如果这一天的降雨量超过了95%，那么这一天就算做是暴雨。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;你也许会问这里95这个数字是怎么来的？我猜这篇论文的审稿人也问了这个问题，而为了应对审稿人的”刁难“，这篇论文的补充材料中将95%这个数字换成了90%-99.9%之间的一组数字，论证了不管你怎么定义暴雨这个事件，文章中的发现都差不多，这说明了该现象的鲁棒性。&lt;/span&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;研究数据与成果鲁棒性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来要看研究用到的数据，降雨量的评估如果是基于地面气象站的数据，这些手工记录的数据，不同的国家，不同的时段，需要规整，还会带着不可避免的缺失和误差，例如有些不靠谱地方出来的数据，也难以精准的记录降雨量。而该研究用的是高分辨率气象卫星TRMM&lt;span&gt;（ Tropical Rainfall Measurement Mission）&lt;/span&gt;提供的全球&lt;span&gt;（北纬50度到南维50度之间）&lt;/span&gt;1998-2016年间的降雨量数据。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;下图展示了在不同地方6-8月间，从57.6万次降雨数据中统计出的超过95%的降雨天对应的降雨量（图A），以及在这段时间里出现暴雨的天数（图B）。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.589010989010989&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;910&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMj3WRl6JjY8NVFRqIN0wyO6ExSlyG3kpVTxQYKFOu9Nda4kWHKkl3Yg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p mpa-paragraph-type=&quot;body&quot;&gt;可以看出处在雨季的南亚次大陆及东南亚，暴雨的天数高达40-60天，而在非洲中部的草原，这段时间也处在雨季。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;除了使用来自TRMM的数据，该研究还使用了Global Precipitation Climatology Project的低精度&lt;span&gt;卫星数据&lt;/span&gt;&lt;span&gt;（每一个经度和一个纬度算一个格子，而不是0.25个经度和纬度算一个格子），&lt;/span&gt;重现了其发现的规律，从而证明了该发现的鲁棒性。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;strong&gt;&lt;span&gt;验证普适性&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;而为了说明该发现具有普适性，需要看看在不同的季节，不同的区域之间，研究发现的规律是否都成立，而本文也做了这方面的研究。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;同时在方法学上，由于存在这么多的暴雨事件，而要确定两次暴雨之间是否有同步，需要逐个两两比较，这其中就可能将一些由于随机巧合错当成相关性同步。存在多重检验时，需要进行统计学上的校正，而不止是降低统计显著的P值，这是一个通用的注意点，所有涉及到P值的研究，都要看是否需要对多重检验进行校正。而在本文中，也针对该问题提出了相应的校正方法。&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;超幂律分布&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来展示这篇文章中最重要的一张图，该图说明了极端气候的遥相关不符合幂律分布。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5429184549356223&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;466&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMTw1chibnS9Aszicf0JX4gpBOE0vdh4YFicibvAOraxCFkqTrReqkro0WqQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  图中将暴雨在不同地区的时间序列的数据两两比较后，按P值小于0.001为显著，统计在不同的距离之间，暴雨发生有关系的的概率，图中的纵轴是概率分布函数。在100公里到2500公里之间，在经过了log处理的坐标轴上，概率分布于距离几乎完美的对应到了一条斜率接近-1的直线上&lt;span&gt;（图中左边的虚线）&lt;/span&gt;，按照这个规律，距离越远，暴雨事件同步发生的概率越低。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;但超过2500公里之后，幂律法则不起作用了，极端气候的相关性的概率随距离增加变高了，直到一万公里，暴雨的同步效应才再次随着时间降低。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;下图是南北半球，冬季和夏季，热带与亚热带分开统计的同步概率与距离的趋势,可以看到超幂律分布都出现了.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMnFwG0MicrTicf7gg1d5sD405Peqib5ZyicyTFOH8J74BamPz7NRdT1lPhQ/640?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;696&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;499.46762589928056&quot; data-ratio=&quot;0.7169540229885057&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;696&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMWdEOYgZP2cSfxgYJ0Pqfe7f3IvKEJQ6cnhurQwY6O06WLibE3GYVXbA/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMqIyHnaRWCo7Jibh2jTObEyL9dsiaT5iaGnt5bLSicMrXWsYOEZpTojicGKg/640?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;664&quot; data-cropy1=&quot;10.748201438848922&quot; data-cropy2=&quot;494.4172661870504&quot; data-ratio=&quot;0.7304216867469879&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;664&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMtM7BSXw5EzFlPME7tnqE1BYI8TcOIZXPiaa53UJkOe8kOBsAyTcqKpg/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;2500公里是什么概念，北京到深圳的直线距离是1950公里，也就意味着深圳的暴雨出现的先后和沈阳的同步程度不如深圳和漠河的暴雨，这个发现是不是像蝴蝶效应一样，有足够的有颠覆性？&lt;/p&gt;
&lt;section data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;
&lt;section data-mid=&quot;t4&quot; readability=&quot;1&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span&gt;气象领域的”Dragon-Kings“&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;在过去的研究中，这类现象被称为”Dragon-Kings“，是黑天鹅现象的一种，用来说明为何预测会失败。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;该词来自于Didier Sornette在09年的论文，文中指出了&lt;strong&gt;&lt;span&gt;小处的规律不适用于大处&lt;/span&gt;&lt;/strong&gt;，例如在城市，金融市场，地震等，而背后的机制和沙堆临界原理及混沌现象中的相变有关，而该研究展示了在气象领域的”Dragon-Kings“。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3680870353581142&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1103&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMgFjlGl6xOacW5j2vMatsic4ibY7tOVK6SUFVaXLHW0jKPs2xjQ6VJicxg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;论文地址：&lt;/span&gt;&lt;/p&gt;
&lt;p mpa-paragraph-type=&quot;body&quot;&gt;&lt;span&gt;https://arxiv.org/abs/0907.4290&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;按照Dragon-Kings的理论，可以使用Kernel density预测遥相关出现的概率，而这正是上图右边（超过2500公里后）的实心线，而这与真实数据拟合的也很不错。虽然无法直接用混沌理论解释为何在气候中会出现超幂律分布（需要结合具体场景，例如全球大气的气流循环），但能够和之前的研究有所关联，能够支持并扩展前人的概率，也是好的论文写作必须的。&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section data-mid=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;案例分析：印度中部的暴雨分布&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;接下来要对一个案例进行系统性的分析，文中选取了印度中部（南亚及中亚），下图展示了该点的暴雨和全球其他位置的暴雨同步关系的强弱，线越明显，同步的概率越大（图A），在图B，阴影区的深浅标识了这些区域与印度中部的同步概率超过了几个标准差。可以看出其中有很多条超过了2500公里的关联，而且这些关联很多是显著的（超过3-4个标准差，随机出现的概率只有不到0.1%)&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8107302533532041&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;671&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMLIPHGgIKtPj3fMLJquP7WiadcOeE4r2PmAEnVPOcVUbsp6SqduypOTg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;那遥相关之间有没有因果关系了，如果总是一个地方先出现大暴雨，接着是一下个地方，那可以说是两者之间有因果性。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;下图展示的是印度中部和欧洲降雨的相关性，黑线代表是欧洲的暴雨是否会引领印度出现暴雨，在正的4-5天上，曲线有一个峰，而在负的8-9天上，也有一个峰，这说明相关性在这个案例上是双向的，不存在单边的因果关系。毕竟地球是一个球。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2710382513661202&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;915&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMYsicgElb2jLRcPwQHpdcicJR6sCRntnZAxsgibiaic0HOXXTjaia58l9yRJw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;最后再看看该如何解释这一现象，下图bc展示了第0天和第3天欧洲和南亚及中亚（SCA）的降雨情况，可以看到先是欧洲暴雨，3天后到了SCA地区，而d和e图展示了风向，将两张图结合起来，就可以看出是由于风吹起来了，才导致了先后的降雨。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3617929562433298&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;937&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCM0s1LibCEDHobAuYxB7zgml6eSd9f2ViamicCOyFyXrWZQOo41yCK8icj0Q/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot; data-mpa-template-id=&quot;1377480&quot; data-mpa-color=&quot;#ffffff&quot; data-mpa-category=&quot;&amp;#x6807;&amp;#x9898;&quot;&gt;

&lt;section data-mid=&quot;t4&quot;&gt;&lt;section data-preserve-color=&quot;t&quot; data-mid=&quot;&quot;&gt;&lt;section data-mid=&quot;&quot;&gt;&lt;p&gt;&lt;strong mpa-from-tpl=&quot;t&quot;&gt;&lt;span mpa-is-content=&quot;t&quot;&gt;&lt;strong mpa-from-tpl=&quot;t&quot; mpa-is-content=&quot;t&quot;&gt;思考延伸&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p mpa-paragraph-type=&quot;body&quot;&gt;总结来看，该文创新性的将复杂网络引入了气象领域，发现了超远程的关联在极端气候中持续存在。我读完后思考，除了暴雨，酷暑，严寒，以及旱灾等，都可以用该文的范式去研究，我猜测遥相关在气候中不止限于暴雨这一个案例。而如何在气象预报中，应用遥相关，也是值得关注的。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;如果说蝴蝶效应让人们开始关注到了远距离的天气现象之间存在相关性，那么这项研究则量化的指出在预测模型中，对多远的现象，根据两者之间同步的概率，应该给予多少相应的关注。&lt;/p&gt;

&lt;p mpa-paragraph-type=&quot;body&quot;&gt;而如果在不同的年份，例如在90年代和进十年间，极端气候间同步的规律有显著的不同，那最可能的解释是人类的活动造成的干扰，而在太阳&lt;span&gt;（黑子）&lt;/span&gt;的”11年“周期，厄尔尼诺现象对极端气候的影响，都是值得探索的方向。&lt;/p&gt;

&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;注：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;厄尔尼诺现象主要指太平洋东部和中部的热带海洋的海水温度异常地持续变暖，使整个世界气候模式发生变化，造成一些地区干旱而另一些地区又降雨量过多的现象。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;736&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/wibWV1DB7tWIDnjf4EtfFejwpQJmErqCMRIhtnZU2ySf3jObCxzqibOeVZYStoaWfTKNIwaju1FfeEiaOzy89y61g/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 留言精选&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我不认为这是颠覆性的，遥相关这个词本身就是对幂律关系的否认，在大气科学领域是种普遍存在的现象，很多类似的遥相关已经被发现，比如enso，北极涛动等。分析方法通常是EOF即主成分分析。这篇论文应该说是在分析方法上创新，更定量了。btw，还没看什么是complex network。另外，地理统计学的方法也是研究空间相关的一种传统方法，有个地理学第一定律是其理论基础，简单讲是距离上越近的东西越相似Everything is related to everything else, but near things are more related to each other.显然大气现象不符合，但并不颠覆。我认为遥相关背后的基础是大气长波的存在，当然大气中有不同周期的波。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383791&amp;amp;idx=1&amp;amp;sn=8d64cb59ba4d52cad925858badc4cdb3&amp;amp;chksm=84f3c9aeb38440b8856c467faa2c0786c7b97168f38e9e47b5aa92221a6d2dfa256f1bcfff9d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;Science重磅：“要想成功，快抱大腿！”&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 24 Feb 2019 07:52:43 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/QkOJZ4teYL</dc:identifier>
</item>
</channel>
</rss>