<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]正确阅读科学文献的九条建议</title>
<link>http://www.jintiankansha.me/t/DLeKi8Ev0E</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/DLeKi8Ev0E</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;77jo8-0-0&quot;&gt;对于正在读博的博士， 还是刚刚进入科研圈的青椒，什么是最重要的问题呢？  如果你的答案是如何写论文， 说明你还没有入道， 事实上， 在能够正确写论文之前，正确读文献更加重要。 所谓熟读唐诗三百首， 不会吟诗也会吟， 这一点上说 ，读论文是写论文的基础。而又经常被忽略。 最近看到plos上一篇非常好的教人如何读论文的文章， 特此给大家分享。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d2k02-0-0&quot;&gt;我们正在经历一个论文爆炸的时代， 面对这个信息过载的危机， 最好的应对方法是养成正确的阅读习惯：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;b647f-0-0&quot;&gt;1，  正确阅读， 要做到每日一读。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9tq7t-0-0&quot;&gt;一旦意识到阅读文献对研究者的必要性， 你会懂得除非你的阅读成为一种日常习惯， 否则你的知识一定会很快过时， 根本无法和先锋研究者进行对话。 有的人有记录癖好， 把没读过的文章累计成一打， 这样的做法让你觉得暗自很爽但不过是一种幻影。 你要真正去读， 而且不能等到写文章时候再读 ， 而是日常阅读。 最好的办法是每天都设定一个固定的时间段来读， 即使是坐地铁这样的碎片化时间也很好！ 关键是， 要把它仪式化!&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;3846f-0-0&quot;&gt;2，  在你的研究生涯早期， 透彻了解你所有要了解的背景。&lt;/span&gt;&lt;/strong&gt; &lt;span data-offset-key=&quot;3846f-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dbsad-0-0&quot;&gt;不要以自己太忙没有时间为理由， 想象一下越是到了人生的后期， 工作， 家庭， 多少事物会让你无法阅读， 所以 ，在你的研究早期， 透彻的了解整个领域是核心重要的， 你后面更难抽出这个时间。  这些知识将如同你的知识地图， 为你后面更精细的课题研究引入航道。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;9ejch-0-0&quot;&gt;3， 不要忘记阅读领域经典论文&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7gcs3-0-0&quot;&gt;如果你要达到前沿，最好的方法不是直接去看前沿， 而是从你的课题的诞生一刻开始追溯它的发展。 你必须找到那颗一点点达到领域前沿的知识树， 把每个核心概念串起来， 这个串联的过程， 就是把核心知识点通过一篇篇关键论文链接的过程。  有了这些关键论文组成的知识树， 你不再会重新造轮子， 或提出别人已经想过而没有显著意义的学术课题了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;cla5r-0-0&quot;&gt;4， 不要忽略学科发展史里的信息&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1u07b-0-0&quot;&gt;如果你已经开始构建这颗知识树， 那么请你重视这些知识发展和形成的历史， 它经历过哪些戏剧性时刻？ 哪些概念的内涵和外延发生了变化？ 为什么出现了这些转变？ 这个变化的背景是什么？ 经过这些思考， 你会对你的整个学科有新的理解，注意这些论文背后的人， 以及它们之间的关系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;5q7k4-0-0&quot;&gt;5， 不要思维狭隘，只关注本领域的知识&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d659d-0-0&quot;&gt;有趣的知识点， 概念， 方法事实上很多来自其它学科的启迪。 虽然你不能成为达芬奇那样的全才 ，但是在“专”和“精”之间达到平衡， 却还是可以做到的。  一个窍门是寻找那些提供综述类文章的期刊，订阅它们，习惯性的阅读，这些综述性文章是高效的拓宽视野的好方法。  另外你还可以通过社交网络增加这类了解。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;4p3r3-0-0&quot;&gt;6， 制定一个最核心的阅读期刊列表&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5fb5d-0-0&quot;&gt;关注核心期刊， 如果你关注的核心期刊少于20个， 说明你可能缺失一些关键信息。 在定位这些期刊的时候，你既要关心它们的影响因子，也要考虑那些影响因子较小但是内容非常新锐的期刊。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;1oasd-0-0&quot;&gt;7， 不要忘记教科书&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;ahadn-0-0&quot;&gt;经典教科书的信噪比永远高于论文，所以， 阅读教科书是高效的。一些最新的教科书可能在谷歌学术上找到。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;8ed1g-0-0&quot;&gt;8， 合理使用工作， 文献管理器来追踪管理你的文献阅读&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;863ap-0-0&quot;&gt;合理使用工具会事半功倍，但不要依赖工具， 认为把文章放入工具就够了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;br3op-0-0&quot;&gt;9， 主动建立知识索引或综述&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;好记性不如烂笔头， 把核心信息记下来， 自己给自己写综述！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;延申阅读：&lt;/strong&gt; &lt;span&gt;How to keep up with the scientific literature  - science  这是一篇science上关于如何阅读文献的资料， 里面有很多知名科学家有意思的观点,  基本思想和此处是类似的。一个特别值得注意的点在于， 它特点提到了跟踪一些大牛的twitter。目前在深度学习这样的领域， 确实不少大牛直接在twitter上发布自己的新作， 甚至进行学术讨论。 另外文中指出很多年轻研究者的缺陷正是在于在研究初期下载几篇项目有关的文献，后面就基本放弃阅读了，这是一个经常陷入的陷阱， 需要警觉。 &lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;常用的工具介绍： &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;1，  google scholar：&lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;  你的搜索装置也是你最好的文献管理器， 如何订阅喜欢的主题， 进行文献归档整理， 你可以搜索网上资料一大把。 学会利用索引按图索隐的把自己领域一网打尽。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;2，  Zoreto：&lt;/span&gt;&lt;/strong&gt; &lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;极为方便灵活的免费工具。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;3，  paper：&lt;/span&gt;&lt;/strong&gt; &lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;mac用户可以用paper， 管理十分方便简洁。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;4，  Mendeley&lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;b6v4o-0-0&quot;&gt;： 可以将文献归档打标签搜索 。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;96l0u-0-0&quot;&gt;编后记：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6fndk-0-0&quot;&gt;巡洋舰希望在新的一年带领大家阅读人工智能， 复杂系统， 和神经科学的一些经典论文。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6fndk-0-0&quot;&gt;参考文章源地址：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006467&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2 How to keep up with the scientific literature&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;https://www.sciencemag.org/careers/2016/11/how-keep-scientific-literature&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;巡洋舰相关文章： &lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6fndk-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=400566313&amp;amp;idx=1&amp;amp;sn=a6486939518fa90af2ab116c13142bec&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;如何挑导师&lt;/a&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=402751373&amp;amp;idx=1&amp;amp;sn=66e33d02d7b034fbcdac43f078750b29&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;科学的教你写论文&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;





</description>
<pubDate>Mon, 07 Jan 2019 23:16:04 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/DLeKi8Ev0E</dc:identifier>
</item>
<item>
<title>[原创]机器学习是怎么巧妙揭开大脑工作原理的</title>
<link>http://www.jintiankansha.me/t/08EQhGA0JT</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/08EQhGA0JT</guid>
<description>&lt;p&gt;机器学习和复杂系统正在诸多领域改变产业界和学术界的传统方法， 其中一个极好的例子正是给机器学习以巨大启发的神经科学本身。 我在此用一个鲜活的例子展示这个方法在该领域的神展开。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;文章的开头我想问大家一个问题， 如果你给一只猫建立一个模型， 那么最好的方法是什么？&lt;/strong&gt;&lt;/span&gt; 这个问题很深刻，大家可以边读下文边寻找答案。&lt;/p&gt;

&lt;p&gt;神经科学，是一个高度依赖数据的生物学分支， 因为你要理解大脑本来就是用来计算的装置，你要研究它就首先研究它的输入和输出， 然后把它和动物的相应行为联系起来。&lt;/p&gt;

&lt;p&gt;这个故事的第一部分有关&lt;strong&gt;复杂系统&lt;/strong&gt;， 因为神经系统本质上属于&lt;strong&gt;复杂网络&lt;/strong&gt;的一种。我们希望通过在电脑上建立一个和大脑神经网络类似的复杂网络，来理解神经细胞数据到行为间的联系。 这种努力在8，90年代十分盛行。 比如最早出现的hopefield 网络解释记忆现象等。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.715&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUGRGNI92rMurH1E6S4tXFLOA7pia83vXUkjsicYwFCSclGJrSGuMibKB1A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;732&quot; /&gt;&lt;/p&gt;
&lt;p&gt;我们给出一个典型的例子：&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.7183333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUry4pCNSesV3ffBEkKCibgY6gHicqEH0ic41Kibj2VvuG9RN3fQmdKhCQWw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;696&quot; /&gt;&lt;br /&gt;此处科学家发现猴子可以经过训练对不同频率的两个声音进行比较（出错应有惩罚）， 这是一个典型的测量短期记忆能力的实验，如果猴子需要比较两个声音， 就要把第一个声音的信号放在脑子里，然后和第二个声音进行比对。&lt;strong&gt;能够综合不同时间的信息进行决策可以说对生物生存至关重要， 而此实验即为其基础。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;以往神经科学的研究方法可能只是描述这个行为，然后在猴子的脑子里想法放入电极，测量相应的神经信号是什么。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.6316666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIU2sCgXGAq9CnB0tduO5RmTicNcvs7K3xQTG0Gssk7ViaKvdRY2dbXWSrA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;629&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.615&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUiarzUX0tKribhekWYxpjHlTPIED5UfmbWy9HWCQm2y7dmpkJ6o5Kbl9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;677&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;例如我们可以看到上图中测量的神经元放电信号，这个神经元放电的频率随着声音的频率上升而上升（红色代表高频的声音，蓝色代表低频），因此我们就可以根据这个臆想一个模型出来。   下图测量的细胞则相反。  你能不能根据这两个图设计个模型解释猴子的行为呢？&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.6383333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUribnEgJByiasEQku5xHmCjBKVm2J99Zh0wFjTQa4MmqGQMkq3SLl7j0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;612&quot; /&gt;&lt;br /&gt;还真的行！ 我们看到最简单的物理模型都可以解释这个现象。 左图中我们用山谷里的小球来描述这个模型（算法），这是把整个问题放在极低的维度上方便理解， 整个外界输入（声音）可以看成在猴子神经系统上的一个外力， 外力可以看做某种推动力让猴子的神经元状态发生定向改变，犹如小球（猴子神经系统的状态）在一个被外力塑造的山谷里趋向谷底（出现概率最大的状态，对应物理里能量最低的点），这个谷底的位置就是对第一次信号的记录（记忆），当外力（f1）消失，山谷的地形发生了变化，但外力并不马上改变。&lt;/p&gt;

&lt;p&gt;  当第二信号（f2）来到的时候，地形再次发生改变，此次的信号引起山谷的隆起，导致小球滚向新的谷底（左右各一个）， 而小球最终达到的位置这次就不仅与这次的外力有关， 还与之前外力引起的位置高度相关（综合历史信息进行决策），这无形中就实现了对两个信号进行比较（此处即最简单的图灵机），而且是用一个简单的物理系统哦。 那么如何利用我们刚说到的两种神经元（和外界信号正比或反比）来实现这个功能回路呢？  请见右图，我们甚至可以画出一个电路图来解释这个原理。 正号代表正比神经元， 负号代表反比神经元，E代表一个随时间变化的控制信号，S2根据E改变电路连接，那么你可以设计一个带有记忆功能的减法器来实现它。&lt;/p&gt;

&lt;p&gt;然而这个方法说到头是一种类比， 很多真实的神经科学家把这个方法戏称为toy model， 而对其不屑一顾。 &lt;strong&gt;因为你无论说你电脑里的程序行为多么像大脑， 其实与真实都差距万里， 而且能够实现某种功能的算法也有很多， 你凭什么说大脑就是按你的臆想工作？&lt;/strong&gt; &lt;span&gt;用这种方法的计算神经科学家经常纠结于自己的模型需要多大程度仿真的问题上，你真的设计的惟妙惟肖就好吗？就好像有些人说的， &lt;strong&gt;你要给一只猫建立模型， 最好就是找一只猫来。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;当然有模型还是比没有模型好很多， 毕竟它给我们点亮了生物世界和数学世界的联系。使得一个我们可能理解的数学体系得以建立在繁琐摸不到体系的生物体系之上， 让我们能够通过改变参数空间的方法与之玩耍。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;然而模型开始发挥威力的真正时刻是机器学习的介入，传统的复杂系统方法从此得到革新。为什么呢？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.5549828178694158&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUGiarDy7BFDRHRu9vxEZ1g0mdO3BWhticgrm9vHzLEuDajC2yZFn70jxw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;582&quot; width=&quot;582&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因为通过机器学习， 我们有了一个强大精确的从数据反推模型的方法 ，而非之前模糊的类比。&lt;/strong&gt; 机器学习的思路和之前的根本不同在于我不在一开始醉心于设计与真实系统相同的模型， 而是先用与真实模型原理大致相似的标准化模型，去学习真实的输入和输出。&lt;span&gt;&lt;strong&gt;此处的思维即你不在追求画出一个活灵活现的猫，而是先做一个四不像的东西， 让他去学习和猫一模一样的行为， 当这种行为真实到不可区分， 那你就认为它就是那只猫。&lt;/strong&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;一旦机器学习开始介入，模型就被赋予了预测性，而被赋予预测性的模型，可以看做在输入输出层面与真实系统不可区分， 从而解决了模型复杂度不好设定， 模型难以通过奥卡姆剃刀的缺陷。 &lt;strong&gt;机器学习的模型比之前的模型更好的点在于多了cross validation的部分&lt;/strong&gt;， 你用真实数据得到的模型，不仅要在已知输入输出的情况下做到像真猫一样， 还要在已知输入未知输入的情况下像一只猫， 也就是说它真的要有学习能力， 能够像真猫一样不仅可以捉到屋里墙角的老鼠，还可以捉到田间地头的老鼠。&lt;/p&gt;

&lt;p&gt;用这个方法， 我们的研究框架发生了变化。 我们先要寻找一组具体的输入输出作为研究起点。比如要研究视觉区域，你就找到一组猫和狗的照片， 然后输出需要是正确的分类。然后再进行测试。&lt;/p&gt;

&lt;p&gt;与复杂系统模型方法不同的是， 机器学习的方法是一个黑箱操作的思路， 我们首先做一个标准化设备， 然后把大量输入送到这个标准化设备里， 然后让标准化系统改变参数得到一组我们想要的输出。标准化黑箱的好处是训练得到参数的方法已知因为这往往是一个巨难无比的任务，而与传统复杂系统模型只追求定性描述的需求不同。&lt;/p&gt;

&lt;p&gt;如果上面的问题用机器学习的方法来解决，就变成：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.5583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUpKn6qWPPIpjznQQjOOsTXOv87OFl6rcec0fQfevKMz2IYky2lWMulg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;734&quot; /&gt;此处的主角是一种标准化的神经网络RNN（循环神经网络）， 我们给定在各种情况下的输入到输出的映射关系，然后在有监督学习的框架下对网络进行训练-即调整网络连接权重达到模拟这个输入和输出的过程。 而此时我们不需要设计，两种与实验相符的神经元（正比，反比如图左）就会自发浮现出来。也就是说， 机器学习出来的猫完全具备了猫的所有功能，甚至不用我们过度描摹，就长得也有些像猫了（功能和形态的对应？）。&lt;/p&gt;

&lt;p&gt;我们现在有了一只会在各种场合抓老鼠的猫， 但我们毕竟不需要一只机器猫，而是要通过机器猫研究猫的行为，懂得猫是如何通过底层的元件实现功能的，毕竟电子猫比真猫要听话的多， 也可以任意让我们解剖，改变参数。 如何做呢？ 再次回到复杂系统里的动力学分析：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.44&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUQrRLIBlQ4TbYSS4HeePqtRYch14WHT4AsRdW5ibjNInC4K9sibaWliaPg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;860&quot; /&gt;&lt;br /&gt;此处新增添的箭头是从高维神经网络到位低维动力学系统的。 还记得我们之前说的那个山坡小球的玩具模型吗？ 它的机理很完美然而可惜的是人家并不信服， 不过我们可以拿到我们训练好的能够与真实系统不可区分的RNN，然后用它进行“逆向工程（reverse engineering）” ，从中取出一个低维度系统， 看它的工作机理是不是符合我们的猜想。此处应有掌声，因为这里我们用到了一个多么抽象的方法，用真实数据回答了一个几乎不可能验证的假设。&lt;/p&gt;

&lt;p&gt;这里其实还有一个隐含的信仰，&lt;strong&gt;被训练过的黑箱被认为学习了生物系统的本质。&lt;/strong&gt;你的RNN做任务做的再好毕竟和真实还是有区别的， 但此处我们已经达到了目前可以做到的极限。这里涉及到一个相当根本的问题， 就是机器学习的核心即在于通过数据学习得到真实系统的数学表征，而这种表征有多大程度接近真实，依然是难以量化和说清的。&lt;/p&gt;

&lt;p&gt;通过这种高维RNN到低维动力学系统的映射，我们就得到了一个解释整个系统运作的更加简单的示意图。通过学习， 一个高维混沌系统开始出现定点（fix point），定点含有系统输入输出的重要信息。此处是一个简单的二元分类器，这个分类器工作的原理正是一个动力学里的鞍点。 对于f1&amp;gt;f2 和 f1&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb&quot; data-ratio=&quot;0.5816666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf7bWk7oBmaZM1Jdr3ZydIUVoV5S7WkianeOxcRpicn7ibqwIibkxTc1uU6XySuQmHR9kia1BLJUq8D5xw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;751&quot; /&gt;&lt;/p&gt;
&lt;p&gt;此种我们看到了机器学习和复杂系统你中有我我中有你的缠绵关系。 &lt;strong&gt;首先机器学习是一种算法，而算法总要有一个物理实现（复杂系统）。 我们先有数据组成的表象世界， 再有算法组成的符号世界， 最后是抽象的真正解释世界机理的物理世界。&lt;/strong&gt;这三种之间表象世界通过机器学习进入符号世界， 而符号世界又才能与物理世界巧妙的相通。 机器学习是桥梁， 复杂系统是灵魂， 而没有桥梁， 灵魂就是空洞的。&lt;/p&gt;

&lt;p&gt;我们可以进一步追问机器学习方法为什么work，人脑为什么work，此处真正相通的地方到底是什么？注意我们反复在说的低维到高维，与高维到低维的问题， 现实生活中的问题往往是高维到低维的映射，比如信号（高维）-决策（往往二维）过程， 能够在高维空间里找到低维嵌入， 往往就代表神经网络建立了真实世界的模型， &lt;strong&gt;正是因为这些模型，我们具有“举一反三” 和“泛化”的能力&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;此文改编自Omri Barak教授的论文&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读 （深度长文）&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383872&amp;amp;idx=1&amp;amp;sn=07e6ad262787f89af6ea00eaeefb9df1&amp;amp;chksm=84f3c601b3844f170021e030a84c70f662c8f03f96db7eece0670a6a3de2d3a16cfc3370b2f5&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;模拟人类大脑 ：人工智能的救赎之路 ？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;



&lt;br /&gt;</description>
<pubDate>Sat, 05 Jan 2019 16:03:21 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/08EQhGA0JT</dc:identifier>
</item>
<item>
<title>模拟人类大脑 ：人工智能的救赎之路 ？</title>
<link>http://www.jintiankansha.me/t/LwMQM6PYck</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/LwMQM6PYck</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;7sdel-0-0&quot;&gt;这两年， 频频有专家警示深度学习即将进入寒冬。 而同时， 一个名叫“类脑智能”的词汇火起来， 这个概念说的是一种比目前深度学习更加接近人脑的智能。 这背后的故事是， 深度学习的大佬，目前已经注意到深度学习的原创性进展面临瓶颈，甚至呼吁全部重来。为了拯救这种趋势， 模拟人脑再次成为一种希望。 然而这一思路是否经得住深度推敲?  我本人做过多年计算神经科学和AI ， 做一个抛砖引玉的小结。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7sdel-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;av90m-0-0&quot;&gt;AI发展的危机&lt;/span&gt;人工智能， 目前多被理解成一个领域领应用的工程学科，从自动安防系统到无人驾驶是它的疆土，而模式识别和计算机专家， 是这片陆地的原住民。 目前的人工智能事实上以工程思维为主， 从当下人工智能的主流深度学习来看， 打开任何一篇论文， 映入眼帘的是几个知名数据集的性能比较，无论是视觉分类的ImageNet，Pascal Vol， 还是强化学习的Atari game。各种各样的bench mark和曲线， 让我们感觉像是一个CPU或者数码相机的导购指南。   &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;422j3-0-0&quot;&gt;那么， 是否这些在这些流行数据库跑分最高的“智能工具&quot;就更智能？ 这可能取决于对”智能“ 本身的定义。  如果你问一个认知专家“智能”是不是ImageNet的错误率， 那么他一定会觉得相当好笑。 一个人可能在识别图片的时候由于各种劳累和马虎， 在这个数据集的错误率高于机器。但是只要你去和它谈任何一个图片它所理解的东西， 比如一个苹果， 你都会震惊于其信息之丰富， 不仅包含了真实苹果的各种感官， 还包含了关于苹果的各种文学影视， 从夏娃的苹果， 到白雪公主的苹果。 &lt;strong&gt;应该说， 人类理解的苹果更加接近概念网络里的一个节点，和整个世界的所有其它概念相关联， 而非机器学习分类器眼里的n个互相分离的“高斯分布”。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2921875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcePZySeD5xPZfjjWazKSPDh20hLgHcyEoib49ibJgElNU7u5P3FONvupkasJaZ4wGb1IklIsggAVvlA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dmh2-0-0&quot;&gt;如果我们认为， ”智能“ 是解决某一类复杂问题的能力，是否我们就可以完全不care上述那种”理解“呢 ？&lt;/span&gt; &lt;span data-offset-key=&quot;dmh2-0-1&quot;&gt;这样的智能工具， 顶多是一些感官的外延， 而”感官“ 是否可以解决复杂问题呢？&lt;/span&gt; &lt;span data-offset-key=&quot;dmh2-0-2&quot;&gt;一个能够准确的识别1000种苹果的机器， 未必能有效的利用这样的信息去思考如何把它在圣诞节分作为礼品分发给公司的员工， 或者取悦你的女友。没有”理解“ 的智能， 将很快到达解决问题复杂度的上限。 缺少真正的理解， 甚至连做感官有时也会捉襟见肘， 你在图像里加入各种噪声， 会明显的干扰分类的准确性， 这点在人类里并不存在。比如下图的小狗和曲奇， 你可以分出来，AI很难。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdDoicV8Epgicfoh4Exr1IKsyibIzCic4k4QPnV11XG8t7H37pxibJK3KzWWayCbpXny7J1Qia5k0H5secg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7iokk-0-0&quot;&gt;”语言“ 在人类的智能里享有独一无二的特殊性，而刚刚的”理解“问题， 背后的本质是目前深度学习对语言的捉襟见肘。  虽然我们可以用强大的LSTM生成诗歌(下图)， 再配上注意力机制和外显记忆与人类对话， 也不代表它能理解人类的这个语言系统。 目前机器对自然语言处理的能力远不及视觉（当下的图卷积网络或可以这个领域做出贡献）。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5277777777777778&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xbAlDYb8GiaoeibGsNUJo93SQhBwjlDNcyzMLyl3MPJllFdZo2jNYhPmw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3502906976744186&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdDoicV8Epgicfoh4Exr1IKsyqljMrfZjdPk0TQX7MfxUKCqF0hZ0ForTA2R2iaRudsUnGZP9YwxvFicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2064&quot; /&gt;&lt;/p&gt;
&lt;p&gt;LSTM加上注意力机制，可以生成极为复杂的宋词， 却不真正理解人类的语言&lt;/p&gt;
&lt;p&gt;Chinese Song Iambics generation with neural attention-based model (Qinxi Wang 2016)&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Zhang,&lt;/span&gt; &lt;span&gt;Xingxing&lt;/span&gt;&lt;span&gt;, and&lt;/span&gt; &lt;span&gt;Mirella&lt;/span&gt;&lt;span&gt;Lapata&lt;/span&gt;&lt;span&gt;. &quot;Chinese poetry generationwith recurrent neural networks.&quot; &lt;/span&gt;&lt;span&gt;Proceedings of the 2014 Conferenceon Empirical Methods in Natural Language Processing (EMNLP)&lt;/span&gt;&lt;span&gt;. 2014.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5fbqt-0-0&quot;&gt;更加糟糕的还有强化学习， 深度强化学习已经战胜了最强大的人类棋手。 但是强化学习却远非一种可靠的实用方法。 这里面最难的在于目前的强化学习还做不到可扩展， 也就是从一个游戏的问题扩展到真实的问题时候会十分糟糕。 一个已经学的很好的强化学习网络，可以在自己已经学到的领域所向披靡， 然而在游戏里稍微增加一点变化， 神经网络就不知所措。 我们可以想象成这是泛化能力的严重缺失， 在真实世界里，这恰恰一击致命。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.3819444444444444&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xMWfRsAe0fBGJrwOWXRRwnlY1ObT57PdQJwiaVBRiaficeHsdCz1EpMySA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;游戏里的王者不代表真实世界能用&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4j1d4-0-0&quot;&gt;事实上在很长时间里，人工智能的过分依赖工科思维恰恰给它的危机埋下了伏笔，在人工数据上破记录， 并不代表我们就会在刚说的“理解”上做进步。 这更像是两个不同的进化方向。 其实， 关于智能的更深刻的理解， 早就是认知科学家，心理学家和神经科学家的核心任务。 如果我们需要让人工智能进步， 向他们取经就看起来很合理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4j1d4-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7qifh-0-0&quot;&gt;脑科学与人工智能合作与分离的历史&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;da66p-0-0&quot;&gt;虽然看起来模拟生物大脑是达到更高层次人工智能的必由之路，但是从当下的人工智能学者的角度，这远非显然。 这里的渊源来自人工智能的早期发展史，应该说深度学习来自于对脑科学的直接取经， 然而它的壮大却是由于对这条道路的背离。 我们可以把这个历史概括为两次合作一次分离。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a7vps-0-0&quot;&gt;第一次合作：&lt;/span&gt; &lt;span data-offset-key=&quot;a7vps-0-1&quot;&gt;深度学习的前身-感知机。模拟人类大脑的人工智能流派又称为连接主义，最早的连接主义尝试就是模拟大脑的单个神经元。 Warren McCulloch 和 WalterPitts在1943 提出而来神经元的模型， 这个模型类似于某种二极管或逻辑门电路。&lt;/span&gt;事实上， 人们很快发现感知机的学习有巨大的局限性，Minksky等一批AI早期大师发现感知机无法执行“抑或”这个非常基本的逻辑运算，从而让人们彻底放弃了用它得到人类智能的希望。  对感知机的失望导致连接主义机器学习的研究陷入低谷达15年， 直到一股新的力量的注入。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7l7o-0-0&quot;&gt;第二次合作：&lt;/span&gt; &lt;span data-offset-key=&quot;7l7o-0-1&quot;&gt; 这次风波， 来自一群好奇心极强的物理学家，在20世纪80年代，hopefiled提出了它的 Hopefield 网络模型，这个模型受到了物理里的Ising模型和自旋玻璃模型的启发， Hopefield发现，自旋玻璃和神经网络具有极大的相似性。每个神经元可以看作一个个微小的磁极， 它可以一种极为简单的方法影响周围的神经元，一个是兴奋（使得其他神经元和自己状态相同）， 一个是抑制（相反）。 如果我们用这个模型来表示神经网络， 那么我们会立刻得到一个心理学特有的现象： 关联记忆。 比如说你看到你奶奶的照片， 立刻想到是奶奶，再联想到和奶奶有关的很多事。 这里的观点是， 某种神经信息（比如奶奶）对应神经元的集体发放状态（好比操场上正步走的士兵）， 当奶奶的照片被输入进去， 它会召唤这个神经元的集体状态， 然后你就想到了奶奶。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3sjvt-0-0&quot;&gt;由于这个模型可以模拟心理学的现象， 人们开始重新对模拟人脑的人工智能报以希望。 人们从不同领域开始涌入这个研究。 在这批人里，发生了一个有趣的分化。 有的人沿着这个路数去研究真实大脑是怎么思考的， 有的人则想直接用这个模型制造机器大脑，&lt;/span&gt; &lt;span data-offset-key=&quot;3sjvt-0-1&quot;&gt;前者派生出了计算神经科学， 后者则导致了联结主义机器学习的复兴，&lt;/span&gt; &lt;span data-offset-key=&quot;3sjvt-0-2&quot;&gt;你可以理解为前者对猫感兴趣，后者只对机器猫感兴趣，虽然两者都在那里写模型。 CNN和RNN分别在80年中后期被发现， 应该说， CNN的结构是直接借鉴了Husel和Wiesel 发现的视觉皮层处理信息的原理， 而RNN则是刚刚说到的Hopefield 网络的一个直接进化。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6521739130434783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xLhU8icbxc6bCFdW6L3UXFQVgribyp7FE6bS66MFy9GcfM7FOLwVFuI4w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1242&quot; /&gt;&lt;/p&gt;
&lt;p&gt;一批人用模型研究真实大脑， 另一批研究机器大脑&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d7fkb-0-0&quot;&gt;AI与脑科学的分离：&lt;/span&gt; &lt;span data-offset-key=&quot;d7fkb-0-1&quot;&gt;90年代后人工智能的主流是以支持向量机为代表的统计机器学习， 而非神经网络。 在漫长的联结主义低谷期， Hinton坚信神经网络既然作为生物智能的载体， 它一定会称为人工智能的救星， 在它的努力下， Hopefield网络很快演化称为新的更强大的模型玻尔兹曼机， 玻尔兹曼机演化为受限玻尔兹曼机， 自编码器， 堆叠自编码器，这已经很接近当下的深度网络。 而深度卷积网络CNN则连续打破视觉处理任务的记录，宣布深度学习时代开始。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8na4n-0-0&quot;&gt;然而， 如果你认为这一股AI兴起的风波的原因是我们对大脑的借鉴， 则一定会被机器学习专家diss，恰恰相反，这波深度学习的崛起来自于深度学习专家对脑科学的背离。  &lt;/span&gt;CNN虽然直接模拟了大脑视皮层结构的模拟， 利用了层级编码， 局部网络连接， 池化这样和生物直接相关的原理。但是， 网络的训练方法，却来自一种完全非生物的方法。  由于信息存储在无数神经元构成的网络连接里， 如何让它学进去， 也是最难的问题。很久以前，人们使用的学习方法是Hebian learning 的生物学习方法， 这种方法实用起来极为困难。 Hinton等人放弃这条道路而使用没有生物支撑但更加高效的反向传播算法， 使得最终训练成功。 从此数据犹如一颗颗子弹打造出神经网络的雏形 ，虽然每次只改一点点， 最终当数据的量特别巨大， 却发生一场质变。    &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bpke-0-0&quot;&gt;CNN能够在2012 年而不是2011或者2010年开始爆发是因为那一年人们提出了Alexnet。  而Alexnet比起之前的Lenet一个关键性的微小调整在于使用Relu，所谓线性整流单元替换了之前的Sigmoid作为激活函数。Simoid 函数才是更加具有生物基础的学习函数， 然而能够抛弃模拟大脑的想法使用Relu， 使得整个网络的信息流通通畅了很多。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xkVSbabRGrVrBOcg5gvbbTtlbU8PzxMicTTicrr0WyohthGPdz8qDRKHg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;f8fr0-0-0&quot;&gt;深度学习另一条主线， 沿着让机器听懂人类的语言， 一种叫LSTM的神经网络， 模拟了人类最奇妙的记忆能力， 并却开始处理和自然语言有关的任务， LSTM框架的提出依然是没有遵循大脑的结构，而是直接在网络里引入类似逻辑门的结构控制信息。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d0b6l-0-0&quot;&gt;由此我们看到， &lt;strong&gt;神经网络虽然在诞生之初多次吸收了生物学的原理本质， 而其最终的成功却在于它大胆的脱离生物细节， 使用更加讲究效率的数理工科思维&lt;/strong&gt;。 生物的细节千千万， 有一些是进化的副产品， 或者由于生物经常挨饿做出的妥协， 却远非智能的必须， 因此对它们的抛弃极大的解放了人工智能的发展。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;d0b6l-0-0&quot;&gt;&lt;br /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;324r-0-0&quot;&gt;脑科学究竟能否开启深度学习时代的下个阶段  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1b0ts-0-0&quot;&gt;那么生物神经网络究竟可不可以启发人工智能呢？&lt;/span&gt; &lt;span data-offset-key=&quot;1b0ts-0-1&quot;&gt;刚刚的分析我们看到生物的细节并不一定对人工智能有帮助， 而生物大脑计算的根本原理却始终在推动深度学习 。&lt;/span&gt; &lt;span data-offset-key=&quot;1b0ts-0-2&quot;&gt;正如CNN的发展直接使用了层级编码的原理， 然后根据自己计算的需求重新设定了细节， 无论如何变化， 生物视觉处理和CNN背后的数学核心却始终保持一致。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1b0ts-0-2&quot;&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;cipoj-0-0&quot;&gt;那么目前的深度学习工具用到了多少生物大脑计算的基本原理呢， 答案是， 冰山一角。 如果说人工智能要继续革命， 那么无疑还要继续深挖这些原理，然后根据这些原则重新设定细节。&lt;/span&gt; &lt;span data-offset-key=&quot;cipoj-0-1&quot;&gt;答案很简单， 宇宙的基本定律不会有很多， 比如相对论量子论这样的根本原理几乎统治物理世界。 如果生物大脑使用一套原理实现了智能， 那么很可能人工智能也不会差很远。即使细节差距很大， 那个根本的东西极有可能是一致的。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;64a9g-0-0&quot;&gt;这样的数学原理应该不会有很多条， 因为人脑的结构一个惊人的特点就是虽然脑区非常不同， 但不同脑区的构造却极为相似， 这种相似性显示了大脑不同脑区使用类似的基本原理。&lt;/span&gt; &lt;span data-offset-key=&quot;64a9g-0-1&quot;&gt;我们目前的深度学习算法， 无论是CNN还是RNN，都只是发现了这个基本原理的某个局部。&lt;/span&gt;&lt;span data-offset-key=&quot;64a9g-0-2&quot;&gt;  &lt;/span&gt;发现这个基本原理， 恰恰是计算神经科学的使命。 对于智能这个上帝最杰出的作品， 我们能做的只有盲人摸象， 目前摸到的东西有一些已经被用到了人工智能里， 有些则没有，我们随便举几个看看。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9bu1f-0-0&quot;&gt;确定已经被应用的原理：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;fgp2t-0-0&quot;&gt;1，  层级编码原理（Hierarchical coding)&lt;/span&gt;&lt;span data-offset-key=&quot;fgp2t-0-1&quot;&gt;： 生物神经网络最基本的结构特点是多层， 无论是视觉， 听觉， 我们说基本的神经回路都有层级结构， 而且经常是六层。这种纵深的层级， 对应的编码原理正是从具体特征到抽象特征的层级编码结构。 最有名的莫过于祖母细胞， 这一思路直接催生了以CNN为代表的深度学习。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8511705685618729&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1x6YszcyBAdiaIwianPMQyV5XRt0qgibwiaiauFia6LTqC4RE1xLLyAwnCFBXg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; /&gt;&lt;/p&gt;

&lt;p&gt;皮层网络的构成往往是6层结构， 在不同的脑区反复出现&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44722222222222224&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xXoL09eTW2ezXic4cCzWBFPjrwEcHRGQTzw4QFrTSbaGsZ8sjMO4tibvQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;层级编码假设&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a0vog-0-0&quot;&gt;2， 集群编码原理 (Distributed coding)：&lt;/span&gt; &lt;span data-offset-key=&quot;a0vog-0-1&quot;&gt; 一个与层级编码相对应的生物神经编码假设是集群编码， 这里说的是一个抽象的概念， 并非对应某个具体的神经元， 而是被一群神经元所表述。 这种编码方法， 相比层级编码， 会更具备鲁棒性， 或更加反脆弱，因为删除一些细胞不会造成整体神经回路的瘫痪。 集群编码在深度学习里的一个直接体现就是词向量编码， word2vect，  词向量编码并没有采取我们最常想到的每个向量独立的独热编码， 而是每个向量里有大量非零的元素，  如此好比用一个神经集群表述一个单词， 带来的好处不仅是更加具有鲁棒性， 而且我们无形中引入了词语之间本来的互相关联，从而使得神经网络更好的吸收语义信息， 从而增加了泛化能力。 在此处， 每个词语概念都有多个神经元表达， 而同一个神经元，可以参与多个概念的描述。 这与之前说的每个概念比如祖母对应一个特定的神经元有比较大的区别。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4236111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xlChl5rY4IAibvkGwicdm01YE4OfDYfBiaGy71PpcnoC1AJvCHdU5LsrvA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;



&lt;p&gt;&lt;span data-offset-key=&quot;7j9t0-0-0&quot;&gt;然而目前的深度学习依然缺乏对集群编码更深刻的应用， 这点上来看，计算神经科学走的更远，我们使用RNN内在的动力学特性， 可以编码很多属性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5m2cn-0-0&quot;&gt;局部被应用或没有被应用的原理：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;f1phk-0-0&quot;&gt;1，cortical minicolumn：&lt;/span&gt;&lt;span data-offset-key=&quot;f1phk-0-1&quot;&gt;皮层内的神经元都采取簇状结构， 细胞之间不是独立的存在， 而是聚集成团簇， 犹如一个微型的柱状体。  这些柱状体成为信息传输的基本单元。  这种惊人一致的皮层内结构， 背后的认知原理是什么呢？  目前还没有定论。 但是Hinton已经把类似的结构用到了Capsule Network ， 在那里， 每个Capsule对应一个簇状体， 而它们有着非常明确的使命， 就是记录一个物体的不同属性， 由于一个Capsule有很多神经元构成，它也可以看作一个神经元向量， 如果它用来记录一组特征， 则可以对付向旋转不变性这种非常抽象的性质。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;1.243781094527363&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xQ71r2Quaupwib86LVuqmYsicR2sDO7Do1PFXj1waUBBVCC6tBmPibS3Rw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;201&quot; /&gt;&lt;/p&gt;

&lt;p&gt;神经簇细胞， 每个神经簇有80-120个神经元， 犹如大脑认知的基本单元， 你可以把某个组成神经簇的细胞集团看成矢量神经元&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.3&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xX5ia2DPZ0OJYcHUkqHphuWEyAwAOa2Of5I0uiaNSYMeFWYticfmDemw5Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dynamic Routing Between CapsulesCapsule Network (Hinton)   每个Capsule取代之前的单个神经元， 能够同时感知物体的多个属性，如长度，宽度，角度，最终通过多个特征确定物体存在的概率， 因此比卷积网络具备表述更多不变性的能力， 比如旋转不变性&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;epot2-0-0&quot;&gt;2，兴奋抑制平衡：&lt;/span&gt; &lt;span data-offset-key=&quot;epot2-0-1&quot;&gt;生物神经系统的各个组成部分， 尤其是靠近深层的脑区， 都具有的一个性质是兴奋性和抑制性神经元的信号互相抵消，犹如两个队伍拔河， 两边势均力敌（最终和为零）。这使得每个神经元接受的信息输入都在零附近， 这带来的一个巨大的好处是神经元对新进入信号更加敏感， 具有更大的动态范围。  这个原理已经被深度学习悄悄的介入了， 它的直接体现就是极为实用的batch normalization， 输入信号被加上或减去一个值从而成为一个零附近的标准高斯分布（这和兴奋抑制平衡效果类似）， 从而大大提升了网络梯度传输的效率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1g07j-0-0&quot;&gt;3，动态网络连接：&lt;/span&gt; &lt;span data-offset-key=&quot;1g07j-0-1&quot;&gt;生物神经系统的神经元和神经元之间的连接-突触本身是随放电活动变化的。 当一个神经元经过放电， 它的活动将会引起细胞突触钙离子的浓度变化，从而引起两个神经元间的连接强度变化。这将导致神经网络的连接权重跟着它的工作状态变化，  计算神经科学认为动态连接的神经网络可以承载工作记忆， 而这点并没有被目前的深度学习系统很好利用 。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5513888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xfiaDFEJJqGQKsia4XlV3DsVpPaiaYQcg4KzR1Nz4owhWVJTkjgC5iaAgeA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Synaptic Theory of Working Memory （Science）&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;f2pmd-0-0&quot;&gt;4   Top down processing:&lt;/span&gt; &lt;span data-offset-key=&quot;f2pmd-0-1&quot;&gt;  目前深度学习使用的网络以前向网络为主（bottom up）， 而事实上， 在生物大脑里， 不同脑区间反馈的连接数量超过前向的连接， 这些连接的作用是什么？ &lt;strong&gt; 一个主流的观点认为它们是从高级脑区向感官的反向调节（top down）， 如同我们所说的相由心生， 而不是相由眼生。&lt;/strong&gt; 同一个图片有美女拿着蛋糕， 可能一个你在饥肠辘辘的时候只看到蛋糕而吃饱了就只看到美女。 我们所看到的，很大程度上取决于我们想要看到什么，以及我们的心情 。这点对我们的生存无疑十分重要， 你显然不是在被动的认知和识别物体， 你的感知和认知显然是统一的。 你在主动的搜索对你的生存有利的物体， 而非被动的感觉外界存在。这一点目前深度学习还完全没有涉及。 一个引入相应的机制的方法是加入从深层神经网络返回输入层的连接，这样深层的神经活动就可以调控输出层的信息处理，  这可能对真正的“ 理解” 有着极为重大的意义。 &lt;/span&gt; &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5986111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xjRcSy95BGPDSPfClJUtricON0YFziah0YIkB8JjaUEeszeVvLTQ242NQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;


&lt;p&gt;给卷积神经网络加入从输出端返回输入端的连接， 是一个深度学习未来的重要方向Deep Convolutional Neural Networks as Models of the Visual System&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;7qff1-0-0&quot;&gt;7，Grid Cells：&lt;/span&gt; &lt;span data-offset-key=&quot;7qff1-0-1&quot;&gt; 海马栅格细胞是一组能够集群表征空间位置的细胞， 它们的原理类似于对物体所在的位置做了一个傅里叶变换， 形成一组表征物体空间位置的坐标基。为什么要对空间里物体的位置做一次傅里叶变换， 这里包含的原理是对任何环境中的物体形成通用的空间表示， 在新的环境里也可以灵活的学习物体的位置，而不是一下子成为路痴。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xVVTuuT6IibaaA7VGtMls5n0EKX1LLiaUbaCauDSJsx9Az7GVtibdLWaLQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Grid Cell被用在强化学习里，使得我们可以得到更加强大的导航能力。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nb9s-0-0&quot;&gt;我们对栅格细胞的认知可能只是更大的神经编码原则的一个局部，正如同傅里叶变换和量子力学之间存在着隐秘的联系。 虽然栅格网络，目前已经被Deepmind用于空间导航任务， 但是目前AI所应用的应该只是这一原理的冰山一角。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dmpok-0-0&quot;&gt;8   Dale Principle&lt;/span&gt;&lt;span data-offset-key=&quot;dmpok-0-1&quot;&gt;： Dale Principle 说的是兴奋型和抑制型神经元 是完全分开的，犹如动物分雌雄。 兴奋性神经元只对周围神经元发放正向反馈（只分泌兴奋性递质， 如Glutamine），让其它神经元一起兴奋， 而抑制型神经元只发放负向反馈（只分泌抑制型递质， 如GABA），取消其它神经元的兴奋。 目前的深度学习网络不会对某个神经元的连接权重做如此限制 ，每个神经元均可向周围神经元发放正或负的信号。 这一原理到底对AI有没有作用目前未知。   &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dmpok-0-1&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2466666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xWwMurf9ic6mRXS39lZUx3IgibEOSj6dgpWkCn5RdmicLHibUTECdibsfVHw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;450g1-0-0&quot;&gt;8  Routing by Inhibitory cells&lt;/span&gt; &lt;span data-offset-key=&quot;450g1-0-1&quot;&gt;：  生物神经系统包含种类丰富的抑制型神经元， 它们往往在生物神经网络起到调控功能，如同控制信息流动的路由器，在合适的时候开启或关闭某个信号。 当下的AI直接用attention的机制， 或者LSTM里的输入门来调控是否让某个输入进入网络， 其它一点类似路由器的作用， 但是种类和形式的多样性远不及生物系统。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;450g1-0-1&quot;&gt;9 临界： &lt;/span&gt;&lt;/strong&gt; &lt;span data-offset-key=&quot;450g1-0-1&quot;&gt;大脑的神经元组成一个巨大的喧闹的动力系统， 根据很多实验数据发现， 这个动力系统处于平衡和混沌的边缘， 被称为临界。 在临界状态， 神经元的活动是一种混乱和秩序的统一体， 看似混乱， 但是隐含着生机勃勃的秩序。 临界是不是也可以用于优化目前的深度学习系统， 是一个很大的课题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;450g1-0-1&quot;&gt;10 ，自由能假说&lt;/span&gt;&lt;/strong&gt;&lt;span data-offset-key=&quot;450g1-0-1&quot;&gt;：  这个假定认为&lt;/span&gt;&lt;span&gt;大脑是一台贝叶斯推断机器。 贝叶斯推断和决策的核心即由最新采纳的证据更新先验概率得到后验概率。 认知科学的核心（&lt;/span&gt;&lt;strong&gt;Perception&lt;/strong&gt;&lt;span&gt;）就是这样一个过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;这里再说两句认知，认知的过程用机器学习的语言说就是用大脑的内部变量来模拟外部世界， 并希望建立内部世界和外部的一个一一映射关系。 这里我们说认知的模型是一个概率模型，并且可以被一系列条件概率所描述。如果用一个形象的比喻来说， 你可以把你的大脑看成一个可以自由打隔断的巨大仓库， 你要把外部世界不同种类的货放进不同的隔断，你的大脑内部运作要有一种对外界真实变化的推测演绎能力， 即随时根据新的证据调整的能力， 你和外界世界的模型匹配的越好， 你的脑子就运转越有效率。 认知是对外部世界运动的一种编码， 你可以立刻联想到机器学习里的表征方法（&lt;strong&gt;representation&lt;/strong&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  ）， 如果你熟悉RNN或CNN的embeding过程， 就会有一种豁然开朗的感觉。  这个假说的理论如果成立， 我们机器学习目前应当使用的只是冰山一角， 可以参考强化学习种的有模型学习。 更多内容见&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383843&amp;amp;idx=1&amp;amp;sn=41e82163f76edfe5ffe31a8518d5bafa&amp;amp;chksm=84f3c662b3844f7430b27f82522dd9414d6c481f6e63822d99deb8baf281be8681ea5c4413a7&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;大脑的自由能假说-兼论认知科学与机器学习&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3do9v-0-0&quot;&gt;11  一些未被量化的心理学和认知科学领地，比如意识。&lt;/span&gt; &lt;span data-offset-key=&quot;3do9v-0-1&quot;&gt; 意识可以理解为自我对自我本身的感知。 关于意识的起源，已经成为一个重要的神经科学探索方向而非玄学， 最近的一些文章指出（The controversial correlates of consiousness - Science 2018），  意识与多个脑区协同的集体放电相关。 但是， 关于意识的一个重大疑团是它对认知和智能到底有什么作用， 还是一个进化的副产物。 如果它对智能有不可替代的作用， 那么毫无疑问， 我们需要让AI最终拥有意识。  一个假说指出意识与我们的社会属性相关， 因为我们需要预测它人的意图和行动， 就需要对它人的大脑建模， 从而派生了对自己本身大脑的感知和认知，从而有了意识。 那么我们究竟需要不需要让AI之间能够互相交流沟通形成组织呢？ 这就是一个更有趣的问题了。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1jp7m-0-0&quot;&gt;深度学习对脑科学的启发：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6577n-0-0&quot;&gt;反过来， 深度学习的某些发现也在反向启发脑科学， &lt;strong&gt;这点正好对应费曼所说的， 如果你要真正理解一个东西， 请你把它做出来。&lt;/strong&gt; 由于深度学习的BP算法太强大了， 它可以让我们在不care任何生物细节的情况下任意的改变网络权重， 这就好比给我们了一个巨大的检测各种理论假设的东西。 由于当下对大脑连接改变的方式我们也只理解了冰山一角， 我们可以先丢下细节， 直接去检验所有可能的选项。 这点上看， 用深度学习理解大脑甚至更加有趣。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bbfml-0-0&quot;&gt;就那刚刚讲的兴奋抑制平衡来看， 最初人们对兴奋抑制平衡作用的理解更多停留在它通过对信号做一个信息增益， 而在深度学习兴起后 ， 我们越来越多的把它的功能和batch normalization 联系起来， 而batch normalization更大的作用在于对梯度消失问题的改进， 而且提高了泛化性能， 这无疑可以提示它的更多功能。 而最近的一篇文章甚至直接将它和LSTM的门调控机制联系起来。 抑制神经元可以通过有条件的发放对信息进行导流， 正如LSTM种的输入门， 输出门的作用， 而互相连接的兴奋神经元则作为信息的载体（对应LSTM中央的循环神经网络）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5819444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfcibblDq1JfWD2ElpcKZN1xczzp6hlib2cpsxgCglibMFw8JFHgTEN4EpW05ak19vfnmictiblJpsjYWQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cortical Microcircuit as gated recurrent networks   DeepMind    LSTM 和 皮层通用回路具有极为密切的相关性&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fm386-0-0&quot;&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-w=&quot;640&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;fm386-0-0&quot;&gt;我们距离通用人工智能可能还有多远？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1g49h-0-0&quot;&gt;其实人工智能的目标就是找寻那个通用人工智能，而类脑计算是实现它的一个重要途径 。 通用智能和当下的智能到底有什么实质性的区别， 作为本文结尾， 我们来看一下：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c78e7-0-0&quot;&gt;对数据的使用效率：&lt;/span&gt; &lt;span data-offset-key=&quot;c78e7-0-1&quot;&gt;比如大脑对数据的应用效率和AI算法并非一个等级， 你看到一个数据， 就可以充分的提取里面的信息，比如看到一个陌生人的脸， 你就记住他了， 但是对于目前的AI算法， 这是不可能的， 因为我们需要大量的照片输入让他掌握这件事。 我们可以轻松的在学完蛙泳的时候学习自由泳， 这对于AI，就是一个困难的问题， 也就是说，同样的效率， 人脑能够从中很快提取到信息， 形成新的技能， AI算法却差的远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c78e7-0-1&quot;&gt; &lt;/span&gt;&lt;span data-offset-key=&quot;c78e7-0-2&quot;&gt;这是为什呢？ 可能这里的挂件体现在一种被称为迁移学习的能力。虽然当下的深度学习算法也具备这一类举一反三的迁移学习能力， 但是往往集中在一些真正非常相近的任务里， 人的表现却灵活的多。&lt;/span&gt;&lt;span data-offset-key=&quot;c78e7-0-3&quot;&gt;这是为什么呢？ 也许， 目前的AI算法缺少一种元学习的能力。 和为元学习， 就是提取一大类问题里类似的本质， 我们人类非常容易干的一个事情。 到底什么造成了人工神经网络和人的神经网路的差距， 还是未知的， 而这个问题也构成一个非常主流的研究方向。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;2fu41-0-0&quot;&gt;能耗比：&lt;/span&gt;&lt;span data-offset-key=&quot;2fu41-0-1&quot;&gt;如果和人类相比， 人工智能系统完成同等任务的功耗是人的极多倍数（比如阿法狗是人脑消耗的三百倍， 3000MJ vs 10MJ 5小时比赛）。 如果耗能如此剧烈， 我们无法想象在能源紧张的地球可以很容易大量普及这样的智能。 那么这个问题有没有解呢？  当然有， 一种， 是我们本身对能量提取的能力大大增强， 比如小型可控核聚变实用化。 另一种， 依然要依靠算法的进步， 既然人脑可以做到的， 我们相信通过不断仿生机器也可以接近。 这一点上我们更多看到的信息是， 人工智能的能耗比和人相比， 还是有很大差距的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;drpng-0-0&quot;&gt;不同数据整合&lt;/span&gt;&lt;span data-offset-key=&quot;drpng-0-1&quot;&gt;： 我们离终极算法相差甚远的另一个重要原因可能是&lt;/span&gt;&lt;span data-offset-key=&quot;drpng-0-2&quot;&gt;现实人类在解决的AI问题犹如一个个分离的孤岛&lt;/span&gt;&lt;span data-offset-key=&quot;drpng-0-3&quot;&gt;， 比如说视觉是视觉， 自然语言是自然语言， 这些孤岛并没有被打通。 相反，人类的智慧里， 从来就没有分离的视觉， 运动或自然语言， 这点上看， 我们还处在AI的初级阶段。 我们可以预想， 人类的智慧是不可能建立在一个个分离的认知孤岛上的， 我们的世界模型一定建立在把这些孤立的信息领域打通的基础上， 才可以做到真正对某个事物的认知， 无论是一个苹果， 还是一只狗。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6pd07-0-0&quot;&gt;沟通与社会性&lt;/span&gt;&lt;span data-offset-key=&quot;6pd07-0-1&quot;&gt;： 另外，&lt;/span&gt; &lt;span data-offset-key=&quot;6pd07-0-2&quot;&gt;人类的智慧是建立在沟通之上的， 人与人相互沟通结成社会&lt;/span&gt;&lt;span data-offset-key=&quot;6pd07-0-3&quot;&gt;， 社会基础上才有文明， 目前的人工智能体还没有沟通， 但不代表以后是不能的， 这点， 也是一个目前的AI水平与强AI（超级算法）的距离所在。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;aqaeq-0-1&quot;&gt;有的人认为， 我们可以直接通过模拟大脑的神经元，组成一个和大脑类似复杂度的复杂系统， 让它自我学习和进化， 从而实现强AI。 从我这个复杂系统专业的角度看， 这还是一个不太现实的事情。因为复杂系统里面最重要的是涌现，也就是说当组成一个集合的元素越来越多，相互作用越来越复杂， 这个集合在某个特殊条件下会出现一些特殊的总体属性，比如强AI，自我意识。 但是我们几乎不可能指望只要我们堆积了那么多元素， 这个现象（相变）就一定会发生。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8uis4-0-0&quot;&gt;至于回到那个未来人工智能曲线发展展望的话题， 我们可以看到， 这些不确定的因素都会使得这条发展曲线变得不可确定&lt;/span&gt;&lt;span data-offset-key=&quot;8uis4-0-1&quot;&gt;。 然而有一点是肯定的， 就是正在有越来越多非常聪明的人， 开始迅速的进入到这个领域， 越来越多的投资也在进来。 这说明， AI已经是势不可挡的称为人类历史的增长极， 即使有一些不确定性， 它却不可能再进入到一个停滞不前的低谷了， 我们也许不会一天两天就接近终极算法， 但却一定会在细分领域取得一个又一个突破。无论是视觉， 自然语言， 还是运动控制。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;q2iu-0-0&quot;&gt;能否走向通用人工智能的确是人工智能未来发展最大的变数&lt;/span&gt;&lt;span data-offset-key=&quot;q2iu-0-1&quot;&gt;， 或许， 我们真正的沉下心来去和大脑取经还是可以或多或少的帮助我们。 因为本质上， 我们在人工智能的研究上所作的， 依然是在模拟人类大脑的奥秘。&lt;/span&gt; &lt;span data-offset-key=&quot;q2iu-0-2&quot;&gt;我们越接近人类智慧的终极算法， 就越能得到更好的人工智能算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383852&amp;amp;idx=1&amp;amp;sn=09d90baeafb224ab6f4309e170dffe14&amp;amp;chksm=84f3c66db3844f7b196609bf0b68c8e42293df8aec12ecd53844a6871e1ab1830f8f722dc516&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;复杂性思维应对不确定性&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383842&amp;amp;idx=1&amp;amp;sn=b4196485b009ebd21e7e0d8db1e2cd61&amp;amp;chksm=84f3c663b3844f756cb7f0547b8f1acf03ac6aeff743d8a37c4d3024b0c30dec19afb8f2298e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;Alpha Zero登上Science封面- 听铁哥浅析阿尔法元&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383841&amp;amp;idx=1&amp;amp;sn=dcaede7797236936bb76650a8434627b&amp;amp;chksm=84f3c660b3844f767a5f38f0093f94ae781eafaed9737b99e78e2da121b34b3b6ca8234a37ff&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;人工智能vs人类智能小传&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383350&amp;amp;idx=1&amp;amp;sn=f396de4fbaffd7dcf7af2af20b55bb2e&amp;amp;chksm=84f3c877b38441617cf9ecaea03d652f18ab2d159eccdc966107df071cca384e55618ce57671&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;胶囊网络结构Capsule初探&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;你所不能不知道的CNN&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;作者简介&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot; readability=&quot;4.5&quot;&gt;
&lt;br /&gt;&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span readability=&quot;3&quot;&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;p&gt;&lt;strong&gt;铁哥更系统性的关于人脑智能和人工智能的比较分析可见目前在万门大学开设的新课： 模拟人类大脑-跟着许铁老师学人工智能&lt;br /&gt;&lt;/strong&gt;&lt;br /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;&lt;/span&gt;
&lt;/pre&gt;

</description>
<pubDate>Tue, 01 Jan 2019 08:03:02 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/LwMQM6PYck</dc:identifier>
</item>
</channel>
</rss>