<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>深度学习背后的基础-神经网络揭秘</title>
<link>http://www.jintiankansha.me/t/aeBEgAR2Il</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/aeBEgAR2Il</guid>
<description>&lt;p&gt;&lt;span data-offset-key=&quot;j108-0-0&quot;&gt;最近， 深度学习三杰获得了计算机界最重要的图灵奖， 它们的贡献都集中在对深度学习的根据神经网络的理论突破。 今天我们看到的所有和人工智能有关的伟大成就， 从阿法狗到自动驾驶， 从海量人脸识别到对话机器人， 都可以归功于人工神经网络的迅速崛起。那么对于不了解神经网络的同学如何入门？ 神经网络的技术为什么伟大， 又是什么让它们这么多年才姗姗走来?  我们一一拆解开来。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;一  引入&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们说人工智能经历了若干阶段， 从规则主导的计算模型到统计机器学习。传统统计机器学习不乏极为强大的算法， 从各种高级的线性回归，SVM到决策树随机森林，它们的共同特点是把人类的学习过程直接转化为算法。 但是沿着直接模拟人类学习和思维的路线， 我们是否可以走向人工智能的终极大厦呢？&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2bfp0-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6180555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFox5Ffs67icfICgmGB0BORtBlhpWxzG2YkfgTSQlcem8vibJuLkJZ4gTjQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;答案是否定的。 基于统计， 模拟人类思维的机器学习模型， 最典型的是决策树， 而即使决策树， 最多能够提取的无非是一种数据特征之间的树形逻辑关系。 但是显然我们人的功能， 很多并不是基于这种非常形式化的逻辑。 比如你一看到一个人， 就记住了他的面孔。 比如你有情感， 在你愤怒和恐惧的时候击退敌人。 比如你一不小心产生了灵感， 下了一手妙棋或者画出一幅名画。 这些显然都与决策树那种非常机械的逻辑相差甚远。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bspib-0-0&quot;&gt;人类的智慧根据， 是从感知， 到直觉， 到创造力的一系列很难转化为程序的过程。  那我们如何才能真正模拟人类的智能？ 我们回到人的组成人的智能本身最重要的材料。 大脑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4s0b-0-0&quot;&gt;如果说统计机器学习的故事是一个模拟人类思维的过程， 那么神经网络的故事就是一个信息加工和处理的故事， 我们的思维将一步步接近造物， 接近 - “信息”， 这个一切认知和智能的本源。 看它该如何流动， 才能产生人的思维。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;二 神经元&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;favf8-0-0&quot;&gt;生物神经元&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bb2cs-0-0&quot;&gt;首先， 神经网络的灵感来自生物神经网络。 那么生物神经网络是怎么组成的？ 神经元。 神经元的基本结构是树突， 胞体和轴突（如上图）。&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-1&quot;&gt;这样一个结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-3&quot;&gt;就像因特网上的一台电脑，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-5&quot;&gt;它有一部接受器-树突，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-7&quot;&gt;每一个树突的顶端是一个叫突出的结构，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-9&quot;&gt;上面布满各种各样的传感器（离子通道），&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-11&quot;&gt;接受外界物理信号，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-13&quot;&gt;或其它“电脑”给它发来的脉冲，&lt;/span&gt; &lt;span data-offset-key=&quot;bb2cs-0-15&quot;&gt;一部发射器，轴突，则以化学递质的形式放出自身的信号。&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-16&quot;&gt;只不过树突和轴突是有形的生物组织， 他们象树枝，根须一般延伸出去，&lt;/span&gt;&lt;span data-offset-key=&quot;bb2cs-0-17&quot;&gt;不停的探知外界的情况并调整自己。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;btp85-0-0&quot;&gt;神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8temh-0-0&quot;&gt;我们把生物神经元进行数学抽象， 就得到人工神经元。如何抽取它的灵魂？ 简单的说， 每一个神经元扮演的角色就是一个收集+传话者。树突不停的收集外部的信号，大部分是其它细胞传递进来的信号，也有物理信号， 比如光。 然后把各种各样的信号转化成胞体上的电位， 如果胞体电位大于一个阈值， 它就开始向其它细胞传递信号。 这个过程非常像爆发。 要不是沉默， 要不就疯狂的对外喊话。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.25833333333333336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFotBFVTVGWWuOicpIJ9FfxDqzbRudXnowYgTM3zUD6KoE6zBVSndhic42w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;sejk-0-0&quot;&gt;我们说， 一个神经细胞就是一个最小的认知单元， 何为认知单元， 就是把一定的数据组成起来，对它做出一个判断， 我们可以给它看成一个具有偏好的探测器。  联系机器学习， 就是分类器，不停的对外界数据进行处理。为什么要做分类？  因为正确的分类， 是认知的基础， 也是决策的基础。 因为认识任何概念， 往往都是离散的分类问题， 比如色彩， 物体的形状等等。因此， 神经细胞做的事情， 就像是模数转化， 把连续的信号转化成离散的样子。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6611111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo7IYjoHvwolKM6LJDhufUmWuRIibXQzcpJBuGib3dkFE4gdxT1nOAVE6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
突触：生物神经元对话的媒介， 信号在这里实现转化

&lt;p&gt;&lt;span data-offset-key=&quot;7ggij-0-0&quot;&gt;如果说把神经元看成这样一个信息转化的函数， 那么生物神经元的这副样子，使它能够极好的调节这个函数。 信号的收集者树突， 可以向外界生长决定探测哪些信号， 然后通过一个叫离子通道的东西收集信号，每个树突上这种通道的数量不一样。 有的多一点， 有的少一点， 而且是可以调控的。 这个调控的东西， 就是对外界信号的敏感度。 这样的结构， 可以说得到了最大的对外界信息的选择性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dbmjj-0-0&quot;&gt;什么是外界信号？  这里我们用上次课将的一个东西替代， 特征， 如果把一切看作树， 神经元在不停观测外界， 每个树突都在收集某个特征。而这个离子通道的数量， 就是线性回归里面的那个权重。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibji-0-0&quot;&gt;它在干什么事情呢？ 收集特征， 作为决策证据！  当某些信息积累到一定地步， 它就可以做决策了， 如果把这个功能进一步简化， 我们就可以把这个决策过程描述成以单纯的阈值函数， 要么就干， 否则就不干。 就这么简单的。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4263888888888889&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFobf2KiaJPeiamU74V5JkPq7iaicufS8yaZy5iaE0YnT6ssye8F4MPjibNp5dg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

从生物神经元到数学神经元


&lt;p&gt;&lt;span data-offset-key=&quot;as9ns-0-0&quot;&gt;神经细胞与晶体管和计算机的根本区别在于可塑性。或者更准确的说具有学习能力。从机器学习的角度看， 它实现的是一个可以学习的分类器，就和我们上次课讲的一样， 具有自己调整权重的能力， 也就是调整这个w1和w2.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;79bjr-0-0&quot;&gt;我们这个简化出来的模型，正是所有人工神经网络的祖母－感知机。　从名字可以看出，人们设计这个模型的最初目的就是像让它像真实的生物神经元一样，做出感知和判断。　并且从数据中学习。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59a57-0-0&quot;&gt;感知机算是最早的把连接主义引入机器学习的尝试。&lt;/span&gt; &lt;span data-offset-key=&quot;59a57-0-1&quot;&gt;它直接模拟Warren McCulloch 和 Walter Pitts 在1943 提出而来神经元的模型，  它的创始人 Rosenblatt 事实上制造了一台硬件装置的跟神经元器件装置。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36527777777777776&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9icsW6iaibhrGtXQyKPGibkNHia4LA7Ooia8vymNVBVMYbVC4dn2DLnRj6zA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

把数学神经元变成机器

&lt;p&gt;&lt;span data-offset-key=&quot;27jc1-0-0&quot;&gt;你要是想理解这个过程。最好的方法是几何法。你仔细观察， 这个感知机的方程， 如果只有两个特征的时候， 描述的就是一个x1和x2坐标的平面， 中间有一条直线w1x1+w2x2=0，直线的左边是一类， 右边是二类 。感知机的学习过程就是调整这条分类直线的位置，我们测量错分点到这条线的距离之和，调整线的位置， 直到两个类的点乖乖分布到直线两边，我们就实现了分类过程。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.9387096774193548&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod3fSJrcicN5uibzI90zF1ZH6slmflRuaOmYO7gmv6ibiaGY3K6SFbCYEwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;

一个神经元相当于一个分类器

&lt;p&gt;&lt;span data-offset-key=&quot;25nko-0-0&quot;&gt;这其实就是一个神经元的偏好。 比你让一个神经元帮你确定是否去看电影的例子。它根据今天的温度，一个是电影的长度来判断， 当然一开始它不了解你， 但是它可以先帮你做做决策， 然后根据你每次给他的反馈意见它调整自己，如果你注重温度，它就加大温度的权重，总之找到你看重的东西， 这就是感知机训练的方法白话版， 如果你给他肯定意见，他就不做改变， 你不满意， 他就变一变。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8kn9j-0-0&quot;&gt;最初的感知机采用阈值函数作为神经元的决策（激活）函数。后来这个函数逐步被调整为sigmoid函数。表现上看， 这个函数把阈值函数进行了软化， 事实上， 它使得我们不是仅仅能够表达非黑即白的逻辑， 而是一个连续变化的概率。 而这个函数的扩展softmax则可以帮助我们实现多个分类的选择。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.665625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoImiazM4ib27dfAwMs1nux9HcuB7LVzJcmp6szTWqoK7E1OndTQuGnAbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;320&quot; /&gt;&lt;/p&gt;

sigmoid 函数

&lt;p&gt;&lt;span data-offset-key=&quot;2om41-0-0&quot;&gt;我们说， 这一类模型开始被人们寄托重大希望， 不久却落空。原因在于， 它真是太简单了。 在几何上， 无论怎么变它都只是一个线性的模型。 而事实上， 这个世界却是非线性的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;三 神经网络降伏非线性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c5d5g-0-0&quot;&gt;什么是非线性， 我们举一个经典的机器学习的房价的例子，首先这是一个经典的线性问题。 我们通常用线性模型描述房价的问题。 把各个影响因子线性的累加起来。比如我们把问题简化为卧室数量， 面积 ，地段， 最后求成交价。 我们用线性模型求解价格。 然而， 着一定不是真实的情况， 这背后的关键假设是，我们的卧室数量，面积， 地段都会独立的影响房价， 这就好像说， 如果你有几个不同的因素， 某个要素变化， 不影响其它要素最终决定房价的方式，我们具体来说， 还是房屋单价只受地段， 距离地铁远近，卧室数量三个要素影响，假定三者的权重是w1，w2, w3. 如果你去改变地段，那么w2，和w3必须是不变的。 就拿海淀和朝阳来说，对这两个区， w2 和w3 是一致的。 事实上呢？  显然这三个要素互相制约，或许海淀的人都做地铁， 朝阳的有车的比较多， 它们对距离地铁远近的敏感就不同， 或者说海淀的单身汉比较多， 朝阳的成家的比较多， 朝阳的就更喜欢卧室多的大户型。 如何表现这种特征之间的相互依赖关系呢？ 一个天然的解决方法，就是：&lt;/span&gt; &lt;span data-offset-key=&quot;c5d5g-0-1&quot;&gt;网络模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.43333333333333335&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoX3xicajmjMl3ZNViabf3Ew7wgqhoBLqOr4C4nx5MwQ38CAAVwlqm3uag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;crcl9-0-0&quot;&gt;看看人类可能是怎么处理这些信息的，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-2&quot;&gt;人对于这种复杂的信息，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-4&quot;&gt;往往采用的是陪审团模式，&lt;/span&gt; &lt;span data-offset-key=&quot;crcl9-0-5&quot;&gt;我们可能有好几个房价专家， 就自己了解的方向去提供意见， 最后我们再根据每个人的情况综合出一个结果。虽然这个时候我们失去了简单的线性， 但是通过对不同意见单元的归纳， 和集中的过程， 就可以模拟更复杂的问题。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c7vnh-0-0&quot;&gt;再看看怎么把它变成数学处理房价问题。 首先把他表示成公式， 他其实是说的是&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5736111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFosfnkfkAqIFjIzNq3CxGpbYNyDtXzl2HXIibTSBKQC2XJU8icjEn9baDg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7aatl-0-0&quot;&gt;系数变成了和x有关的，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-1&quot;&gt;仔细想一下，这不就是增加了一个新的层次吗，&lt;/span&gt;&lt;span data-offset-key=&quot;7aatl-0-3&quot;&gt;我们需要再原始特征x1和x2的基础上，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-5&quot;&gt;增加一个新的处理层次，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-7&quot;&gt;让不同的特征区域，&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-9&quot;&gt;享受不同的权重&lt;/span&gt; &lt;span data-offset-key=&quot;7aatl-0-10&quot;&gt;。 就好比我们有好几位房价评估师，每一位都是针对某种情况下房价的专家，最后有一个人， 根据专家的特长给出最终的综合结果。这与我们的灵感是相符的。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;vqde-0-0&quot;&gt;我们把他表示成图示方法。  首先， 每个专家都仿佛是一个小的线性模型， 它们具有唯一的一组权重， 显示它们再所熟悉的区域的权威。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.8126843657817109&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo9ibmk3wbb7DoRNJpAnrg2P3qqUEAd7E3mdXW4JGVpR4kMvAPO3XMmBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;678&quot; /&gt;&lt;/p&gt;

图片改编自网络博文Machine Learning is Fun!  by Adam Geitgey

&lt;p&gt;&lt;span data-offset-key=&quot;afvtt-0-0&quot;&gt;然后，我们把它们汇总起来， 得到下面这个图， 这个图和之前我们讲解线性回归的图是一样的， 但是我们增加了一个中间的层次，这就是刚刚说的陪审团， 哪些中间的位置， 事实上就是表达了特征之间复杂的互相影响。而最终我们把它们的意见综合起来， 就是最终的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo2E1HdEBTguA3uM8kic3riaibTQgYib212lbLemDibKD4q4oet6ID2sQCnMw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4bjac-0-0&quot;&gt;你换个角度思考， 那些中间的绿色圆圈， 也就是说每个专家，不就是神经元吗？ 而那个黄色圆圈， 是最终决策的神经元。 这样的一个组织， 而不是单个神经元， 就是神经网络的雏形。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;as5n6-0-0&quot;&gt;当然， 房价的问题是个回归问题， 而这个世界的大部分机器学习问题是分类， 每个神经元也是一个小小的分类器， 所以我们把每个神经元的角色变成线性分类器，再套用刚刚陪审团的逻辑， 看看我们得到了什么： &lt;/span&gt;每个线性分类器， 刚刚讲过都是一个小的特征检测器， 具有自己的偏好，这个偏好刚好用一个直线表示， 左边是yes，右边是no， 那么多个神经元表达的是什么呢？ 很多条这样yes or no的直线！  最终的结果是什么呢？ 我们得到一个被一条条直线割的四分五裂的结构， 既混乱又没用！  这就好比每个信息收集者按照自己的偏好得到一个结论。幸好我们有一组头顶的神经元， 它就是最终的大法官， 它把每个人划分的方法， 做一个汇总。 大法官并不需要什么特殊的手段做汇总， 它所做到的，无非是逻辑运算， 所谓的“与”， “或”， “非”， 这个合并方法，可以得到一个非常复杂的判决结果。 你可以把大法官的工作看成是筛选， 我们要再空间里筛选出一个我们最终需要的形状来， 这有点像是小孩子玩的折纸游戏，每一次都这一条直线， 最终会得到一个边界非常复杂的图形。  我们说， 这就是一个一层的神经网络所能干的事情。 它可以做出一个复杂的选择， 每个神经元都代表着对特征的一个组合方法，最后决策神经元把这些重组后的特征已经刻画了不同特征之间的关系， 就可以干掉认识现实世界复杂特性-非线性的能力， 特征之间的关系很复杂， 我也可以学出来。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;所谓神经网络的近似定理， 是说一个前馈神经网络如果具有线性输出层和至少一层具有任何一种‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，&lt;/span&gt;&lt;strong&gt;&lt;span data-offset-key=&quot;dibd9-0-0&quot;&gt;它可以以任意的精度来近似任何&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-1&quot;&gt;从一个有限维空间到另一个有限维空间&lt;/span&gt;&lt;span data-offset-key=&quot;dibd9-0-2&quot;&gt;的&lt;/span&gt;Borel 可测函数。  这是关于神经网络最经典的理论了，简称万能函数逼近器&lt;/strong&gt;。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5416666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFod4vIAibhcvVQNWJFiaS8q4IGwG6LORjWzfhcPUcGicRbFDAdtGQdGO0OQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2r5k6-0-0&quot;&gt;另外的一种理解是，神经网络具有生成非常复杂的规则的能力， 如果你可以让神经网络很好的学习， 他就可以自发的去做那些与或非， 和逻辑运算，不用你自己写程序就解决非线性问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;四 关于神经网络的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1nvpl-0-0&quot;&gt;是什么阻止了神经网络从出现之后很快的发展， 事实上是它们非常不好学习。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;28o13-0-0&quot;&gt;我们说神经网络能够成为一种机器学习的工具关键在于能够学习，那么如何学习的呢？ 如果说单个神经元可以学习的它的偏好， 通过调整权重来调整自己的偏好。 那么神经网络所干的事情就是调整每个神经元和神经元之间的联系， 通过调整这个连接， 来学习如何处理复杂的特征。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7h3c6-0-0&quot;&gt;说的简单， 这在数学上是一个非常难的问题。 我们通常要把一个机器学习问题转化为优化问题求解。 这里， 神经网络既然在解决非线性问题， 事实上和它有关的优化又叫做非线性优化。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;em7k3-0-0&quot;&gt;先看生物神经网络的学习， 它是通过一种叫做可塑性的性质进行调节的。 这种调控的法则十分简单。说的是神经细胞之间的连接随着它们的活动而变化， 这个变化的方法是， 如果有两个上游的神经元同时给一个共同的下游神经元提供输入， 那么这个共同的输入将导致那个弱的神经元连接的增强， 或者说权重的增强。 这个原理导致的结果是，我们会形成对共同出现的特征的一种相关性提取。 比如一个香蕉的特征是黄色和长形， 一个猴子经常看到香蕉， 那么一个连接到黄色和长形这两种底层特征的细胞就会越来越敏感， 形成一个对香蕉敏感的细胞，我们简称香蕉细胞。 也就是说我们通过底层特征的“共现” 形成了对高级特征的认知。 上述过程被总结hebbain学习的一个过程。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4708333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoodYXesR7lOnwbA0FpvdW1hbl8Zxw6KaDCNicI4F7gDSlD09AzSQqHFg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.44722222222222224&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFo4ibY5gFmA50Wd4zhXO4TaPibTmGgVhwGKqCR6HyEZzGeYHYnRuLA32Dw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;coje1-0-0&quot;&gt;从本质上说， 刚刚说的过程是一种无监督学习，你接受输入引起神经活动， 它慢慢的调整。 如果用这样的方法训练人工神经网络，恐怕需要跑一个世纪。  人工神经网络的训练依赖的是监督学习，一种有效的结果导向的思维，如果我要它判断香蕉和苹果， 我就是要一边给它看图片， 叫它告诉我是什么 ，然后马上告诉它对错， 如果错了，就是寻找那个引起错误的神经连接。 然而这个过程在数学上特别的难以表达， 因为一个复杂网络的判断， 引起错误的原因可能是任何中间环节 ，这就好像调节一大堆互相关联在一起的参数，更加可怕的是， 神经网络往往有很多层， 使得这些参数的相关性更加复杂， 如果你一个个去试探， 那个感觉就是立即疯。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6ogvv-0-0&quot;&gt;最终， 这个问题被BP算法的提出解决。 BP反向传播算法， 是一种精确的根据最终实现的目标，然后通过比较当下输出和最终目标的差距， 然后一级反推如何微小的改变各级连接权重以减少这种误差的方法。 这其实就是梯度下降结合复合函数求导法则的一个更复杂的形式。 我们通常把一组刚刚到来的数据， 扎成一捆喂给神经网络， 让它计算出一个输出， 这个输出当然会错的很离谱， 然后我们把这个结果和我们真正需求的比较得到一个误差信号， 这个误差信号会一级一级的改变所有的连接权重。 每一捆数据， 被称为一个tensor， 都像一个个子弹一样塑造着整个网络。 对于这个方法的理解， 最好的办法是使用一套由tensorflow提供的可视化工具。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9h3f9-0-0&quot;&gt;当然这仅仅是一个神经网络的训练的简短小节， 当你在神经网络坚守的Hinton这样的大神，事实上提出过对学习算法的无数改进细节， 才有了它今天的成功。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;五 关于多层神经网络，深有什么好处？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;5jrv-0-0&quot;&gt;刚刚说的方法你可以理解一个一层的神经网络， 那更多层的神经网络呢？深层的神经网络究竟有什么好处？事实上深度学习的深，就是指神经网络之神，可见这是奥妙的门道。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3fkq2-0-0&quot;&gt;用一句化说，一层的神经网络可以对特征做一次变换，就如同得到了一组新的特征， 这种特征的变换， 用数学家的眼光就是做了一个坐标变换， 兑换到一个新的空间。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;63aiu-0-0&quot;&gt;我举一个最小的例子让你理解。 你记得高中学过的直角坐标系和极坐标系吧， 这两个坐标系之间存在一个经典的坐标变换公式。 这就可以看作一个坐标变换。 我们举一个例子看，如果要做一个分类器， 把一些居住在圆形中心的点和它的外周分离开来， 那么这个坐标变换就有奇效。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图片来源： Medium Support vector machine explained  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ej38h-0-0&quot;&gt;多层网络， 就代表一个多次连续的特征变换，最后会把数据从一种性质变到另一个性质，从一个空间拉到另一个空间。最终总有一个空间， 这些数据呆着最合适。这种行为，简称为表征学习（representative learning）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.475&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoAPpjHdFGtYzf774rsmY2Jr9iaZuWXCiayVr3rF2WKFV6eOnSTY24DTPg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;7lhu9-0-0&quot;&gt;表征学习顾名思义，是要一种特征转化为另外的特征，因为简单的特征一旦发生关系就可以构成复杂的特征，刚刚那个处在领头的大法官和司令再次变成士兵，给上层的决策者提供信息,隐层神经网络所作的事情，我们说其实就是对特征进行重组， 然后得到一组新的特征的坐标变换， 只不过这个变换的形式是可以学习的。 我们说，越限制， 就越自由，为了更好的学习， 我们会限制神经元对信息的处理只由两部分组成， 一个是用一定的权重组合上层的特征， 另一个是通过某个样子的激活函数把这个总和变一变。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;agvf3-0-0&quot;&gt;多层的神经网络， 通过多次的基变换， 把特征一次次重组，得到越来越复杂的新的特征， 这就是深度神经网络作用的机制。 某种程度上， 它把这种层级特征强加给了它要识别的事物， 但是假定事物本身也是这种按照一层层的方法搭建起来， 那么神经网络就会取得奇效。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8jijp-0-0&quot;&gt;哪些事物是有这种层级结构的呢？ 我们说，所有的感知信号都是， 从视觉， 到听觉，到语言。  &lt;/span&gt;假定你看着一个屏幕， 你真正看到的是每个像素点， 而最终你要组合成为一个可以认知的图象。 假定你听着的是声音， 你最终需要分辨出一个个单词 ，句式，直到语言。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;85nga-0-0&quot;&gt;认知科学里说，抽象使得不同的事物可以在更深的层次上被联系起来， 的就是说， 不同层次的函数迭代，产生的新的空间（某种程度也是降维，比如图形处理中），使得原先的距离概念被打破，具体怎样被打破。 这正是人脑处理信息的本质。 实验表明，越靠近感官的神经元处理的信息就具体， 比如颜色，亮度， 而经过多级处理，达到皮层之后， 我们的神经元就可以对比较抽象的实体，如人名，物体的种类敏感。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5125&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoHicmAzdVLQ0RZUTnlhTqwNic1WWOAj06A4VYNTXvUuiaTrjpGjmtczuhA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;c2jbo-0-0&quot;&gt;当然，这只是最形象的一个描述，关于增加深度相比增加宽度对神经网络功能的好处，是整个深度学习问题的焦点问题之一。因此引出了几个不同的数学观点， 有机会给大家总结。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7k452-0-0&quot;&gt;训练深度神经网络是困呐的，由于神经网络的训练里有大量的复杂求导运算， 我们是不是函数要写死为止？ 其实不是， 我们有一个强大的框架， 就是tensorflow，让你的整个求导过程十分容易。你只需要写出你的目标函数cost function，然后简简单单的调用tensorflow内部的optimisor就可以了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;六 神经网络动物园&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;5oi27-0-0&quot;&gt;神经网络经过这些年的进化，已经是一个大家族。  我们来看几个具体的神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;e2saf-0-0&quot;&gt;Auto-encoder 自编码器：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;15eeh-0-0&quot;&gt;最早成功的一种深度神经网络，自编码器意如其名， 其实做的就是对数据进行编码， 或者压缩。  这种网络通常具有的特点是输出的长度或者说大小远小于输入，但是神奇的是， 这个输出却保留了数据种的大部分信息。 你是不是想到了上节课介绍的PCA了呢？ 是的， 这就是一种降维的模型。 这个模型的实质， 也就是一个复杂， 或者说是非线性的降维操作，  比如说你把一张图象压成很简单的编码。 由于这种输入多输出少的性质， 自编码器的形状形如一个下夸上窄的漏斗， 数据从夸的一面进来， 出去的就是压缩的精华。 自编码器之所以在神经网络的进化历史里举足轻重， 是因为它在回答一个很核心的问题， 那就是在变化的信息里那些是不变的， 这些不变的东西往往也就是最重要的东西。比如说人脸， 你把1万张不同的人脸输入到网络里， 这些人类分属于十个人， 那么按说， 很多照片无非是一个人的不同状态，它的不变性就是这个人本身。 这个网络能够把数据最终一步步的集中到十个维度就够了， 这样， 最终可能用一个十位数的数字就可以压缩整个数据集。 而这， 正是神经网络， 甚至是人类理解事物的关键， 那就是找出纷繁变化事物里的不变性， 它对应的正是一个概念。 抓住了概念， 就抓住了从具象到抽象的过程， 抓住了知识，抓住了人认知的关键所在。 深度学习大师Hinton早期的核心工作，正是围绕自编码器。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4847222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoGuozHjTbiakzdWwM1x10geKvNhZQ9dpDib3yUSGcC0eLfog4Tk0ruWvA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dtmn8-0-0&quot;&gt;卷积神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bmp0a-0-0&quot;&gt;卷积神经网络就是模拟人类是视觉的一种深度神经网络。这个网络的特点是能够把图象数据，像photoshop 里的滤镜一样， 被过滤成某种特征图，比如纹理图， 这些低级的特征图， 将再次被过滤， 得到一个新的特征图， 这个特征图的特点就是更抽象， 还有更多的刚刚讲过的概念性的特征。  比如一个物体的形状轮廓。 直到最后一层， 得到对某类物体概念的认知。 这就是人类视觉知识形成的过程。  也是我们下次课的重点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a5c9i-0-0&quot;&gt;含时间的神经网络：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;1df5j-0-0&quot;&gt;神经网络不仅能够描述静态的各个特征之间的关系， 而且能够描述特征（这里更好叫因子）之间在时间上的复杂相互作用关系， 一个神经网络的做出的含时间过程， 最好的例子就是含有证据积累的决策过程。 即使是单个神经元， 也可以把不同时间的信息积累起来做个决定， 而动态的神经网络， 就可以更好的把这些证据总体的汇集起来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ceh5e-0-0&quot;&gt;总的来说 ， 几个神经元组成的网络， 可以像一个信息的蓄水池一样， 通过互相喊话， 把过去的信息在自己人之间流传起来， 从而产生类似于人的记忆的效应， 这些通过特定方法连接在一起的神经元， 就可以形成人的工作记忆或内隐记忆， 而同时， 也可以帮我们设定出处理和时间有关信号的神经网络工具， 这就是RNN – LSTM家族， 以及其它的长时间记忆网络。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7430555555555556&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccz2D6uHjtoKgPicp2AjUkFoC2jGqlma1D0hyq3XB24GdAQgQLNEvE8CicBysZhbiaP8WohAknicu3GRw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在深度时间序列处理种扮演重要核心角色的LSTM，其创始人schmidhuber却无缘此次图灵奖。&lt;/p&gt;

</description>
<pubDate>Sun, 31 Mar 2019 12:22:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/aeBEgAR2Il</dc:identifier>
</item>
<item>
<title>预测神经网络预测准确性的普遍理论</title>
<link>http://www.jintiankansha.me/t/ctLLjVRfg2</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ctLLjVRfg2</guid>
<description>&lt;p&gt;Heavy-Tailed Universality Predicts Trends in Test Accuracies for Very Large Pre-Trained Deep Neural Networks 是今年1月24日在Arxiv上post的一篇论文。作者还有针对这个话题的一系列偏理论的文章，本篇是其中最实用的一篇&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cdXGY1uicfP1GvdpRUIsTzVeCW0jQBn23FMOTGtQu1BCFzKExl0uAKrA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;845&quot; data-cropy1=&quot;10.638489208633095&quot; data-cropy2=&quot;411.8615107913669&quot; data-ratio=&quot;0.4757396449704142&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc8FWgochyYCx565Py8uo3cwDQ2Mb5ROibQ3rN9plKq3Sc88iafn561z58fbL7icweEuEmbPlLdYQZcg/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;845&quot; /&gt;&lt;/p&gt;


&lt;p&gt;这篇文章对神经网络的泛化能力建立了一个大一统性质的理论，不仅能够解释为何神经网络中的各种正则化手段有效，还能够用一个指标预测一个训练好的网络的泛化能力，这篇文章中有很多在我看来高深的数学，其中很多我觉得较难理解，因此这里只概述我理解的部分，写下这篇论文笔记，更多的是像行家请教。&lt;/p&gt;

&lt;p&gt;初学深度学习的时候，我被各种各样的正则化方法搞的很迷茫。传统的机器学习中就是增加L1或者L2正则项，到了深度学习，减少batch size，dropout，early stopping等很多看似完全不同的方法，都被称为正则化，似乎所有能够增加模型泛华能力的都是正则化方法。 有了这么多的正则化方法，对于有监督学习，使用同一数据训练的两个DNN，训练时用到了不同的超参数，不同的优化方法与正则方法，我们除了让模型在真实数据上跑一下，没有方法提前估计模型的泛化能力。如果能够通过对模型本身的分析，就能评估出模型相对的泛化能力，那在对模型进行fine-tune(微调)的时候，就能够起到指导。&lt;/p&gt;

&lt;p&gt;验证该理论的是数据来自真实世界中的神经网络，通过比较针对ImageNet数据集上的50个预训练过的DNN，在不改变模型的损失函数与网络结构，不需要重新训练模型，甚至不需要导入测试数据时，通过计算网络中每一层权重矩阵的Frobenius norm的指数平均值，发现该平均值和模型的预测准确性具有相关性，从而能通过上述指数，预测模型的泛化能力。用于验证的模型涵盖了15个不同的网络结构，例如VGG16，ResNet等，这说明了该方法对各类模型都适用。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.4737394957983193&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cLZ2JXlDD2oOoiaeQrbgLYspwyNHwOBlgib2w2WZbF5Le1dzy1iaqica4QQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;952&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这里展示了预训练的不同VGG和带批量正则化的VGG网络的表现，这里的横轴是该网络的预测准确性，纵轴是作者文中定义的评估模型范化能力的指标，左边的图是之前的方法，右边的是本文新提出的指标。可以看出这里拟合的很好，而且是接近线性的拟合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.5208333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cIDzvBEpuFumLXV3hMDZXofEIRVwjJcJNVIsgq9rfhicER0guqamqibOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;912&quot; /&gt;&lt;/p&gt;
&lt;p&gt;上面反映的是不同的ResNet上的情况，除了部分离群点，文中提出指标也反映了模型相对的好坏。更多网络结构下的对比情况如下所示；&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;1.6070726915520628&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cSKGRj6zgA6HkgSGOYibORbAvR9eWDXZceSTxTrJkjHKozSS39Uiaz9rQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;509&quot; /&gt;&lt;/p&gt;
&lt;p&gt;该作者还提供了一个pytorch及Keras下的package，名为WeightWatcher，可以用一行代码，将训练好的模型当作参数传入，就可以计算出上述指标。笔者试用了，挺方便的。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;https://github.com/CalculatedContent/WeightWatcher&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.3822843822843823&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cxxK7f8OnePohbahEEibCBy5iacT6xBvAEkYJ8a0aHUPZFZBHOI2WzweQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;429&quot; /&gt;&lt;/p&gt;


&lt;p&gt;接下来要解释这背后的数学原理了，首先对每层网络的权重矩阵W，构建相关矩阵X&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.3722943722943723&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cJnDzhnOawRIiagfv2ibPHqvzy50PPID5En5xvo2tVGuY7sO6egd1gvBg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;231&quot; /&gt;&lt;/p&gt;
&lt;p&gt;对X计算矩阵的秩，将该矩阵的秩写成一系列秩的加权，将其称为Empirical Spectral Density (ESD)&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.24634146341463414&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cxM1RQcicDC73435khfQPCg8NzCf4ibOa9rMb8GmhNxP5BOIyETw965Tg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;410&quot; /&gt;&lt;/p&gt;
&lt;p&gt;之后作者指出可以用一个由随机数生成的矩阵的秩进行幂律运算，来拟合这个分布，这里引入了random matrix theory&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.2925764192139738&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cI6maPKuD2wiaUrTLC99T5ERNTjaHJLFbeic4K68FSEdYLApWnUWW3geg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;229&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而这里的alpha则代表了幂律分布的尾巴有多长，对于ImageNet中的7500个权重矩阵，下图代表了不同矩阵最佳拟合的alpha落在不同区间的次数，图中70-80%的案例，图中的alpha都落在了2-4之间，也有少数极大的情况。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.8426150121065376&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3cDnwBxgKicJWXr26T9IJ10y9SdSusoutAm9dSbEH19gKtVI8CICJZmWw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;413&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而上文中出现的用来预测模型预测精度的纵轴&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.8823529411764706&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3ck9XN8klS5ic6EpFueJibs4MQS1I256uQz8mjSTrHgsOJ8g71eEwM6Bng/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;51&quot; /&gt;，就是所有层的加群平均， 不同深度的层数对应的权重由式中的beta控制，&lt;img class=&quot;inline-img&quot; data-ratio=&quot;0.2826086956521739&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc8FWgochyYCx565Py8uo3c2gAwJB9ibQZX8qsOWHY5DPSu1KscbTJmibMBbQdGSYhdbfzeelk1mohQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;322&quot; /&gt;&lt;/p&gt;

&lt;p&gt;之后文章指出，正则化不管怎么去做，都是要避免权重矩阵的秩被过度长尾（heavy tailed)的随机矩阵拟合，也就是说上文中每一层都需要alpha值越小，网络在该层过拟合的风险越低。权重矩阵的秩呈现过度长尾的幂律分布，可以直观的想象成对于某些像素点赋予高的不合比例的权重，而这意味着该层网络的观察野受到了局限，对高权重位点的变化及其敏感，而这会使得模型更容易过拟合。因此将网络中每层的权重矩阵的alpha做权重衰减的加和，就可以用来评估模型是否过拟合。&lt;/p&gt;

&lt;p&gt;总结一下，本文提出的衡量模型相对泛化能力的普适性方法，利用了已有的成熟模型，发现规律，并将其用在指导新模型的训练，该方法可以用在迁移学习和模型微调中。这篇文章的数学我对其只看懂了皮毛，有一些问题，文中也没有给出解答，第一是对于非CNN系列的网络结构，例如capsule network，图卷积网络GCN等，该文的方法是否适用，第二是对于autoencoder系列的模型，如果以重构误差作为横轴，能否也通过文中的指标评价模型的泛化能力，第三点是该方法用在图像切割上，是不是也会有较好的效果，第四个问题是该方法在NLP任务中是否适用。&lt;/p&gt;

</description>
<pubDate>Thu, 28 Mar 2019 06:59:59 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ctLLjVRfg2</dc:identifier>
</item>
<item>
<title>让神经网络变得透明-因果推理对机器学习的八项助力</title>
<link>http://www.jintiankansha.me/t/7xSXhTttBM</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/7xSXhTttBM</guid>
<description>&lt;p&gt;如今的AI已经能够在Dota2这样的对战游戏中战胜人类了，但游戏主播和解说却不需要担心自己失业，因为当前的神经网络还无法解释自己为何做出决定。我们可以训练另一个神经网络，对前一个神经网络做出的每一个决定，来预测人类会给予其什么样的解释。还拿Dota2举例子，可以训练一个神经网络，对操作游戏的AI点的每一个技能，买的每一个装备，去猜测人类会给予怎么解释。但当AI进化出人类想不到的策略时，就像围棋中Alpha Go已经能够走出人类棋手想不出的套路，上述策略就不行了。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceE1piajBpf3TqrYxHmeibImWckebOwWJ4fsd31zBMibQVet43icCpicq509m0AAGXoDATR6LAql1KIfEQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;1.4970059880239521&quot; data-w=&quot;334&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本文是《Possible Mind》（&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;点击查看相关的读书笔记&lt;/a&gt;）系列读书笔记的第三篇，围绕因果推理的创始人，&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383568&amp;amp;idx=1&amp;amp;sn=fb2a6857f18cf4de917111406ef9bd4f&amp;amp;chksm=84f3c951b3844047e23a00d5aca0f9acac4aa27580dabfa7096401a166b9ad3196b34de9d251&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;《The book of why》&lt;/a&gt;的作者Judea Pearl的题为“The limitation of opaque machine learning”，延伸而写成。当前神经网络缺少解释性，其成功的原理是如魔法一般的黑盒子。当前提升模型解释性的方法，无论是注意力机制，对模型进行压缩，用线性的浅层的模型来模拟深层的模型，还是对隐藏层的权重可视化展示，都只是隔靴搔痒，无法带来质变。本文基于Judea Pearl的随笔以及他去年12月的一篇论文整合而成，先说明可解释的重要性，再讨论因果推理带来的本质改变，再详述因果推理对深度学习带来的八大助力。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.5215736040609137&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFoSUOFGsoGM4fGOgfiaU7oP7SBm1qzg8Ic3sicYT0ZVll1OKdKK5e584A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;788&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当扫地机器人遇到上图的情况，卡住了，如果其内部的AI能够正确的给出为何会卡住的解释，那下次机器人就会避免不平的环境，不管造成不平的是地毯还是掉在地上的衣物，这就提高了模型的可扩展性。能够给出解释，还可以帮助人更好与AI协作，假设每次扫地机器人能够告诉人自己今天因为那种原因卡住了，从而多花了多少分钟才扫完屋子，那这家的主人就可以下一次避免让家里出现地面不平的情况，从而让机器人能够更高效的工作，节约能源。而机器具有因果推理后，还会学到是自己清扫地毯的角度，使得地毯打折从而使得地面不平的，从而下次用其他的角度去清扫地毯覆盖的地方。&lt;/p&gt;

&lt;p&gt;这些都是实际的好处，而在《Possible mind》这本书中，借用哲学家Stephen Toulmin在1961年的书《Foresight and Understanding》中的对比，引入了巴比伦和雅典科学的区别，同样是预测天体的运行，四季的节律，巴比伦的预测在精确程度和一致性上都好过同时代的雅典人，但雅典的预测背后有神话去提供解释，并且当一种解释比另一种解释更符合时，前者会战胜后者，巴比伦式的精准预测，没有带给这个文明天文学，而古希腊则孕育了现代科学。&lt;/p&gt;

&lt;p&gt;类似的还有李约瑟之问，为何古代中国没有发展出科学，尽管其很长的一段时间中技术是领先全球的。比如勾股定理，明明中国人早在《周髀算经》中就有所记录，但西方人却称之为毕达哥拉斯定理。这里的区别在于前者只是从经验的层面记录了这个现象，而后者是对此给予了普遍化的证明。用机器学习的视角来看，前者无法确定这个规律的可扩展性如何，而后者保证了在所有的宇宙中，该规律都是适用的。&lt;/p&gt;

&lt;p&gt;借用这个对比，当前的深度学习，尽管取得了突飞猛进的进展，但由于其缺少可解释性，不透明，还是属于巴比伦式的。即使其在所有的游戏上，都战胜了人类，都需要花更少的能源即时间去训练，也不能算是能匹敌人脑的强人工智能。雅典的天文学者，可以根据自己的理论，去设计一个实验，来估算地球的半径，还和真实的结果相差不多，而巴比伦式的不透明的“天文知识”，则根本不会问出这样的问题。当今的大数据，不会用虚假的概念来讲故事，而根据《人类简史》中的论述，正是想象出的共同体，使得人类走进了文明时代。&lt;/p&gt;

&lt;p&gt;按照Judea Peral的分类，&lt;span&gt;认识世界分三个层次&lt;/span&gt;，最低的是关联，只需要观察就好，例如那些症状告诉医生这个人患病了，那些行为告诉我这个人容易被促销打动；再上一层是干预，也就是去通过行为去改变世界之后，看会发生什么，这个层次要回答的问题是吃药能不能治病，而最高的层次是反事实的推理，要达到这一层，需要想象力，需要反思，要回答的问题是如果我之前多一些锻炼，现在是不是就不会生病。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.4044526901669759&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFxWUOzYB4LibnMGkdNSe2zn74SnJzHMnRvGCtZ9226cutBZ2S1qeJYGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1078&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当前的有监督学习，是站在了认知阶梯的第一层，强化学习由于和坏境有所互动，是站在了第二层上，通过无监督学习，去预测下一秒会发生什么，根据类比来推测位置的情况，也是介于第一层和第二层之间的。不论是那种学习范式，都是基于统计的，得出的结论是概率性的。正如同不懂得证明勾股定理，永远也无法百分之百打包票说这个规律是普世的，当前基于统计的机器学习，如同三体中被智子锁死的地球科技，看似进步神速，但总会碰到天花板。&lt;/p&gt;

&lt;p&gt;Judea Pearl给出的解药是他发明的公理化式的因果推理图，想要详细了解的推荐下面的免费课程，&lt;span&gt;edx&lt;/span&gt;&lt;span&gt;平台上的，名为&lt;/span&gt;&lt;span&gt;Causal Diagrams: Draw Your Assumptions Before Your Conclusions。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.46193265007320644&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceE1piajBpf3TqrYxHmeibImWH4Zs7x6s6ZVY1ODPsY9vzs4zTme2ibBCM1aqicmicl4jM1p0ERNa3OiaTw/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那如何机器学习在中加入因果推理了？回答是结构化因果模型（Structural Causal Models，下文简称SCM），通过有向图的形式，对常识中的因果作用方向的假设做结构化的建模，例如下图中x代表是否采取实验疗法，y代表从癌症中康复，z代表性别，那么对世界的模型就是性别可以决定一个人是否更可能会选择实验疗法，是否更容易从癌症中康复，但反方向的因果却是不现实的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFkmIdVhnicKHGQYTyub3mlicxY5lTTz2ibIZ5BVOoYXQ6DSEdtjuDv1UZA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;364&quot; data-cropy1=&quot;13&quot; data-cropy2=&quot;124&quot; data-ratio=&quot;0.3076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFel6zNBrX1FuaFPE8xN15j0oer3GKTicFFpmtm7M3J0J0uD4POqaURPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;364&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了因果图，那SCM就可以根据提问，来计算一个和因果推理的问题，究竟有多少数据的支持，不管这个问题位于上述的认知阶梯的第几层，都能够给予回答。这里的&lt;img class=&quot;rich_pages&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFWq1RlL2E9ySJjEkseRibSshqKcKKTjmTmguvnRBPhUtIRNriauX7npCw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;209&quot; data-cropy1=&quot;6&quot; data-cropy2=&quot;35&quot; data-ratio=&quot;0.13875598086124402&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BF1skbnL0pdd07gt5DyDqecNgQE8uGdoFfPgZ2oFaj0e6wcokLm8cnOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;209&quot; /&gt;，也就是考虑不同性别下是否采取实验疗法对癌症康复的概率，而根据深度学习的模型，可以估算出真实的状况下的概率，如果发现俩者的差距很大，那因果推断的模型就可以对实验疗法导致癌症康复这个因果论断给予反驳，说明这是没有事实依据的，而不是只说明实验疗法和癌症治愈缺少统计显著性。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;rich_pages&quot; data-ratio=&quot;0.5781893004115226&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFhdzw631DGKqudv649xN39sTpUam05u3XianZjiaxkAuPdX4KhZetEoAA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;486&quot; /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;那这对于解决当前机器学习缺少解释性的问题，又什么本质的改变了？这里列出Judea Pearl的八条回答：&lt;/p&gt;

&lt;p&gt;首先是让机器做的假设以人类容易理解的方式（因果图）呈现出来，从而&lt;span&gt;让模型更加透明&lt;/span&gt;，也让测试模型的推论的后人能够更精准的去检验模型的鲁棒性。可以说只有能够解释清楚自己每一步的推理的逻辑，模型才算具有了可证伪性，否则即使有一个不符合模型预测的事件，由于不确定其和模型的因果假设有什么关系，又该怎么区分这究竟是应该被去除的噪音还是能推翻整个认知模型的反例。&lt;/p&gt;

&lt;p&gt;第二点是通过因果推断，&lt;span&gt;去除混杂因素的影响&lt;/span&gt;。当前的机器学习，重要先进性数据清洗，特征提取，往往花在特征工程上的时间占到了全流程的大头。有了因果推断，就不必人，来根据常识去掉那些可能影响相关性的混杂因素，从而在更复杂的坏境下，做到端对端的学习。这使得模型能够超越建模者的认知局限，从而模拟真实坏境更复杂的相互作用。&lt;/p&gt;

&lt;p&gt;第三点是算法化的&lt;span&gt;回答反事实的问题&lt;/span&gt;。人类能够区分出充分条件和必要条件，能区分cause of effect与effect of cause，例如小明酒后游泳溺水而死，游戏是小明死亡的必要而不是充分条件，要回答这样的问题，就需要进行反事实的思考，去幻想如果小明没有游泳会怎样，如果游泳时小明没有喝酒会怎样。如果机器能够做这样的思考，那AI思考的模块化程度就会进一步提高，需要的训练数据也会减少，对于跨领域的迁移学习也会有所助力。&lt;/p&gt;

&lt;p&gt;第四个助力是&lt;span&gt;区分直接和间接的诱因&lt;/span&gt;，如果只有关联分析，那在较长的时间尺度上，就会面临如何区分是否之前的决定带来了奖励的问题，但如果能够将因果关系描述出来，并且根据数据来评价每一条因果链条的坚固程度，那就能够去解决强化学习中在较长的时间尺度上，该如何分配奖励的问题。区分了直接的诱因与通过第三方作用间接的影响，就能够判定数据中那些异常点处在间接影响的链条上，受到未知因素的影响，属于噪音，而对于处于直接因果链条上的，则异常不应该被视作是噪音，而是可以证伪模型的“黑天鹅”。&lt;/p&gt;

&lt;p&gt;第五个助力是&lt;span&gt;模型具有跨领域的适用性&lt;/span&gt;，还能够通过其他领域来验证该模型的鲁棒性。如果一个通过强化学习的智能体在一种游戏中表现优异，那预期换一个游戏，该模型也不会表现的太差，这种能力被称为domain adaptation，人类就有这个能力，例如dota玩的好的人，玩英雄联盟也不差。如果智能体是通过因果推理，来决定下一回合的policy，那这个思考过程就更像人类做决定时的所思所想，由此类比推出，智能体也会具有更好的domain adaptation。&lt;/p&gt;

&lt;p&gt;第六个助力&lt;span&gt;避免sampling bias&lt;/span&gt;，正如人类的认知偏见会让人丢掉那些对支持自己结论不适合的数据，人类在对机器建模时，也会展现出类似的认知偏见。如果机器具有了公理化的因果推理，那通过反事实的问题，就可以指出人类可能受到了采样偏见的影响。这指出了人机协作的新的可能性，不是机器只懂得找出相关性，从而指数级的放大人的认知偏见，而是机器根据人对世界的建模，去帮助人做人类不擅长的用数据找出偏见，从而带来一个更公平的模型。&lt;/p&gt;

&lt;p&gt;第七个助力是通过因果模型，来&lt;span&gt;判定数据集中是否存在数据缺失的问题&lt;/span&gt;。例如建模者以为女性不擅长数学，那用来训练该录取那个学生的分类器时女性申请者的样本就会很少，从而使得基于统计相关的模型预测女生不应该录取到数理相关的专业。但对于基于因果判定的模型，那只要对世界的假设中包含性别会影响是否报名数理相关专业这个因果联系，那模型就能够根据数据判定出这里存在着可能的数据缺失，从而提醒建模者注意。&lt;/p&gt;

&lt;p&gt;第八个助力是去&lt;span&gt;发现因果关系&lt;/span&gt;，例如遗传学中的孟德尔随机，就是利用了基因在有性生殖中自然会发生重组，来区别到底基因的差距与人身体表现出来的胖瘦高矮这样的表型到底是因果性还是相关性。现实中存在着诸多类似孟德尔随机的自然形成的与随机双盲实验等价的场景，通过让模型具有因果推断能力，就能够发现未知的因果关系。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.4863498483316481&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcc6X0PFMnnyvydl7OaIk5BFjuZl8uM1icZrPsLAef5gOxzIcwGK0GEkmibArREh39o98G0ibEztsXWVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;989&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;附图是孟德尔随机的示意图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;总结全文，当前的深度学习，这样model free的模式，无法达到人类水平的智力，只有增加了因果推断，才能像科学的精神孕育出持续不断的发现那样，让机器学习走到之前无法企及之地，例如上文列出的八种具体任务，都需要更高层次的思考。当前的深度学习，模型缺少解释性，可迁移性，也不够鲁棒，会由于一个像素的改变而彻底改变分类的结果。&lt;span&gt;所有的指数级增长都会有尽头，当前深度学习取得的成就，大多&lt;/span&gt;只是依赖计算资源和训练数据的指数化增加，&lt;span&gt;只有不断站在更高的认知阶梯，通过&lt;/span&gt;自我指称获得的递归式的思考，才是无限的增长模式。因此将因果模型引入下一代人工智能中，是充分且必要的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;需要注意的是，因果推断的模型，依赖与建模者去根据常识或本领域背景，给定对因果关系运行的方向，如果这个假设有问题，那模型是无法从数据中发现人类建模者犯了因果倒置的问题的，这说明不管多么先进的模型，都需要建模者的智慧参与其中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参考文献：&lt;span&gt;The Seven Tools of Causal Inference with Reflections on Machine Learning&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更多阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384252&amp;amp;idx=1&amp;amp;sn=bdc733e516f25b44a1b7bdfb6104d2b5&amp;amp;chksm=84f3c7fdb3844eeb9f82b2f7e0590f3514f5b9887c279ccdd5919004b59a8686353a85670742&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;信息的俩种定义&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384244&amp;amp;idx=1&amp;amp;sn=b7ea9482daef936f6f433719ad419097&amp;amp;chksm=84f3c7f5b3844ee35fb7a98ae317ed19ea32a5e79f1529a8d0c7da2bfb361545443b03f0f5ba&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;如何让神经网络具有好奇心&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384215&amp;amp;idx=1&amp;amp;sn=bd8e32534f656af0aecc8cba60b1a608&amp;amp;chksm=84f3c7d6b3844ec053cc3b7d853b18f8754135c8e074fd22ae2ca58cb76e82554aef9b13e111&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;读《Possible Mind》，看25位大咖谈AI&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 23 Mar 2019 11:56:59 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/7xSXhTttBM</dc:identifier>
</item>
<item>
<title>与不确定性作战-从物理模型到特征提取</title>
<link>http://www.jintiankansha.me/t/pcrJ6lbBMG</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/pcrJ6lbBMG</guid>
<description>&lt;p&gt; &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;一    无所不在的预测&lt;/strong&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;想象一下，如果你能预测你的人生， 或者某个人能够预测一个国家甚至人类的兴衰， 那将是何等意义的事。 阿西莫夫的科幻小说基地里的，  一个叫哈里谢顿的人发明了一个叫心理史学的学问能够预测整个银河帝国的兴衰， 但现实中了，你该怎么回答当下是进入A股的最佳时机吗？不要认为这个问题和人工智能无关， 事实上， 智能的故事第一步在于， 让机器学习预测。 因为， 预测， 是一切正确行为决策的基础。 那机器不能够学习预测， 何谈智能？&lt;/p&gt;

&lt;p&gt;所有的古代文明， 从埃及到玛雅， 到希腊， 有一个学科是共通的， 这就是天文历法。 天文立法的本质是根据历史记录的天象， 预测未来星辰季节的变化， 这一点，对于刚刚走出蒙昧的人类的重要性，是不言而喻的。 你要在什么时候播种， 插秧， 收割， 都是和季节星辰的预测有关的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpiaxPqIwOowIaicUY75cxa7GqDVlxf7NMgRwVcdIaSkgjpIzLxoQmOx3Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;日月星辰这种有严谨周期性的东西也是最好预测的东西， 对它们的观测， 就会提醒我们发现很多的数据之间存在一些关联， 这种关联，比如月亮的升降与潮汐，太阳的角度与季节，这种规律性，无时无刻不提示着我们，世上的很多东西是可以预知的。&lt;/p&gt;

&lt;p&gt;因此也有各种派别开始提出它们的预测方法，从神话故事， 到占星术， 周易八卦， 都可算作预测， 其中最成功的一门， 是根基于数学的物理模型， 物理模型的历史。人类的历史经历过三个阶段，农业革命，工业革命，信息革命，应该说， 三次革命背后的驱动都与物理模型有关。 物理的基础是数学， 数学是农业革命中产生的， 古代埃及人们需要丈量土地和知晓尼罗河水泛滥的时间， 那个时候人们有了土地丈量和观察星象的需求，我们开始定量的描述事物和收集数据， 但是远远谈不上建模。&lt;/p&gt;

&lt;p&gt;真正的数学模型应该说出现在古希腊。那时候人们不仅关心记录那些数据，而且开始关心从记录数据背后总结共性的东西，比如设计一套理论解释运动的物体运动的原因，或者行星运行的道理。我们开始想要对众多的事物做预测，我们发明了一种东西，它好像能够代表真实的世界， 但是却简单非常多，它由数字构成。 然后我们管他叫真实事物的缩影， 这就是模型 。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;二 物理模型的胜利&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;物理模型里最具经典的莫过于托勒密 (Ptolemy) 的宇宙模型系统， 模型说日月星辰的运动可以分解为一系列互相嵌套的圆周运动，因为圆形是最美丽的图形。行星循着本轮 (epicenter，周转圆) 的小圆运行，而本轮的中心循著著称为均轮的大圆绕地球运行。这种模型可以定性的解释行星为什么会逆行，而且可以相当精确的预测大量的天文现象。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;1.007462686567164&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpZmB1ggvCbqkEOJqETjrHh9S2RyBJu5wZkQEdv2Z6Lia4t6hXGfIiaEIw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;268&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型因为如同巫师的水晶球一样能够预测而成名， 也会因为遇到更好的模型而下架。 精密的托勒密模型最终让位给哥白尼的日心说， 哥白尼的日心说可以说是一大类新模型的开端， 然而一开始这个模型却并不具备比之前模型更好的预测力， 直到开普勒把之前的圆形轨道变成椭圆轨道。 这个模型不仅在一些现象的预测上开始超越托勒密模型，而且还有很关键的一点， 就是更简单。 一个地球绕太阳椭圆运动的假设， 就可以取代很多小圆。  事实证明， 往往简单的模型是更加正确的， 就好像你能够用一个方法能以更小的成成本取得同样的效果，那这个方法就算更好的。&lt;/p&gt;

&lt;p&gt;物理学家对基本原理有着非常强的直觉， 库伦发现电荷间的吸引力， 安培发现了电流间的吸引力， 安培把这种电流间的作用归结于磁场，就是那种磁铁之间或吸引或排斥的作用。 紧接着法拉第发现变化的磁场产生了库伦的琥珀间的那种电力 ，并且天才的用电场来表达这种空间中无所不在的作用。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpCoOlgW2Tj0cIib1icUX9KmMD5ko6pf5Ip2nEQ2N57GmQhMXHzzJG8Svg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然后一个叫麦克斯韦的大佬直接不根据实验预测了变化的电场也同样会产生磁场，并且用一组叫做麦克斯韦方程组的东西统一了整个电磁学。这是一个纯粹依赖场来描述的理论， 好像那些我们之前以为物理实体的电流， 磁铁都不在重要， 真正存在恰是模型描述的那些场线。这个系统里， 我们再也看不到任何平时现象中的物体。 真空中的力线，这种人类的想象物，就是模型的全部。&lt;/p&gt;

&lt;p&gt;更令人不可思议的是，如果变化的磁场产生电场， 变化的电场产生磁场， 那么由此周而复始，会有什么现象呢？没错，电磁波，我们不仅预言了电磁波的存在，而且预言了它的速度是３０万公里每秒，没错，这就是光速。这些交织的力线，通过数学方程的力量，变成转动发电圈里的电流， 变成电脑里晶体管里的电流， 变成手机之间传递的电磁波， 成就了整个电力革命时代。&lt;/p&gt;

&lt;p&gt;如果你以为这就是物理预测的力量，你依然小瞧了它。很快， 这种思维开始进入微观世界。 尽管在很长时间我们并不能真正看到那些比细胞还要微小那么多的原子，但是物理学家使用它们的模型，已经开始对微观世界的各种实体建模。由此产生了量子力学，一个叫量子场论的理论，结合了刚刚说的电磁场的思想和量子力学， 甚至预言了一种叫做正电子的从未有人见过的粒子，而之后被实验证实。 没错，物理预测的力量正在与每一次我们都在实验发生前就知道了将要发生什么。&lt;/p&gt;

&lt;p&gt;差不多同时，我们开始构建微观到宏观的桥梁。 如果原子的世界变得越来越清楚， 而它们组成了所有宏观物质，它们有温度，有软硬，有形状，有化学性质， 那么，这些东西可不可以也被预测呢？  我们又一次的成功了， 而这在常识里， 可以被看成不可能的任务。 因为组成物质，比如一块磁铁的所有原子我们不仅不知道它们的情况， 而且即使有所观测， 也非常随机。 但是， 我们为社么又胜利了呢？ 这不得不归功于统计物理这样一个学科。 &lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpHZj2VrGlicFLs3qictf7NCcuXXiamW6LwB9lZhXQQ2BkxtM7fg169VhTg/0?wx_fmt=jpeg&quot; data-cropx1=&quot;222&quot; data-cropx2=&quot;500&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;350&quot; data-ratio=&quot;1.2589928057553956&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpuggbVjU4tWuUFiaMfKwUkicYzWWNZn7yib5L6OA1gceKoIxjqiarldzFew/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;278&quot; /&gt;&lt;/p&gt;

&lt;p&gt;统计物理的最重要的观点是， 如果对于一个事物的情况我们不知道， 我们就假定它是随机的。 即使微观尺度上的大量粒子运动是随机的，通过一个叫大数定律的方法，我们在宏观尺度上得到的东西却可以如磁铁般稳固， 且可以预测，比如我们可以计算它的磁性。 这种随机到宏观精确的可预测性的保证， 正是大数定律。  为什么大数定律的作用如此之大， 我们可以通过几个简单的例子了解， 最简单的例子， 每年都有无数家庭破产或发财， 但是整个国家的财富却一般都是非常稳定的增长。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;三 社会与人类心理预测的失败&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;在20世纪以后， 在所有之前工业革命基础上出现了计算机， 以至于后面的互联网， 使得人类积累数据， 以及用数学来预测的能力又一次空前提高， 迎来了信息爆炸的时代。我们依然按照之前的思路继续建模，因循着这样的思路，人类不仅懂得了星体的运行，而且开始改变自身社会，我们认为社会本身也存在类似牛顿原则一样的第一性原理，开始出现马克思这样的从社会基本运行规律出发试图改造社会的理论家。&lt;/p&gt;

&lt;p&gt;而在心理学领域，我们也开始认识到人类的心智也如同一台动力学机器， 在客观的受力法则下思考和运行。 对于这一类愿望的典型描述在阿西莫夫的基地里淋漓尽致， 它借哈里谢顿教授的口说出： 人类社会的每个成员如同气体里的分子，他们的运动可能是随机的不可预测， 但是其组成的人类社会确是高度可以预测的。因此它预测了整个“基地“ 的没落， 从而展开了整个它的改变人类历史的计划。 这简直就是对统计物理的观点重演，每个人随机的不可预测， 但是很多人加总在一起， 一个国家，一个民族的兴衰确是高度精确可测的。&lt;/p&gt;

&lt;p&gt;但是这些幻想很快落空， 这条路走不下去了。 我们发现， 凡是跟社会和人类有关的领域，我们几乎输的精光。 你想应用大数定律， 你却总在失败。  一个最典型的例子莫过于股市，大家都想预测股票价格，但是就是搞不定， 我们想象股市的价格是大量的进入交易所买入和卖出的交易者决定的， 股市价格的本质是人们对公司未来总价值的预期。&lt;/p&gt;

&lt;p&gt;如果这些交易者， 就像组成物质实体的粒子一样， 虽然表面的的行为是随机的， 但是加和在一起， 由于交易者众多， 就应该表示出确定的性质， 如果这个逻辑是真的， 股市就应该可以预测的。 然而事实却恰恰相反， 这么多的聪明人进入到股市交易搏杀， 却没有几个人能预测2009年的金融危机。对于社会科学的其它问题， 也是类似的情况。 当年没有多少专家预测苏东的巨变，特朗普的当选， 或者英国的脱欧。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpB8n9QRWn7UTBibT8Mmhrm61qK53alh22PB5KldFlmPmsFUL3wYqltXQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;960&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而另一个角度， 我们对自己的大脑的预测更是知之甚少， 比如目前几乎还没有什么仪器能够预测你老了会不会得老年痴呆，或者你今晚会不会失眠。我们并不能用受力分析的方法把人脑分解为细胞，然后找到一组方程， 或者用统计的方法， 解出心智运行的模型。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;四 预测失败的元凶 – 网络&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;到底是什么， 破坏了我们建造预测性的通天塔的目标？&lt;/p&gt;

&lt;p&gt;我们先前的理论一个根本缺陷是一个最重要的假设是不成立的， 它就是独立性假设。 当组成整体的每个个体之间的相互作用可忽略， 我们称为独立假设成立， 这时候，如果每一个个体的行为是随机的， 但是组成总体的特性确实可以确定的。&lt;/p&gt;

&lt;p&gt;最致命的是， 人的大脑还是人类社会， 独立性假设不成立。 显然你的成长无法忽略周围的某些特定个体对你的影响， 同样的你的大脑里的某个神经元离开周围神经元的信号传递一无是处，几乎就是一个比较大的电阻电容电路而已。&lt;/p&gt;

&lt;p&gt;所有这些问题的背后， 隐藏了一个新的问题范式 – 网络。 如果个体和个体不独立，意味着它们的互相影响， 这个互相影响， 导致之前的预测范式无法很好的起作用， 这个网络， 会引入如下几个效应：&lt;/p&gt;

&lt;p&gt;&lt;span&gt;非线性：&lt;/span&gt;  整体不再是部分的加和我们管它叫非线性，这个整体的特性变得特别难以预测。  这个你可以理解当下的互联网使得个人的作用已经远远不是加法， 这样的例子在你身边也很多， 两个很优秀组成的家庭可能非常不幸福， 而一群乌合之众在某个清晰聪明的规则下， 可以其力断金。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;混沌：&lt;/span&gt;气象预测一直以来是农业生产的重要课题，曾经有一段时间， 物理学家认为只要能够合理的解出气体运动的流体力学方程， 那么一个月后的天气无非是大量模拟可以解决。 然而气象学家试图对大气建模（当然是牛顿力学了）的过程里，发现了一个不可思议的现象，它们算出来的天气一会儿晴天一会儿下雨：完全相同的一份数据，多保留小数点后一位数，计算出来的就是晴天；少保留一位小数，计算出来的就是下雨。这是机器的问题吗，比如说这是程序员都遇见过的计算机浮点误差导致的？&lt;/p&gt;

&lt;p&gt;牛顿力学里三体问题的时候就已经很难预测了， 三个物体之间只要有复杂的相互预测，在一定时间后， 得到的解都已经极不稳定， 假设计算机程序出那么一丁点的误差， 都会使得我们最终的解与之前完全不同。 这里的不确定性来自于我们无法控制所研究系统的所有信息，而每一丁点的不同都导致结果完全不同。 大量练成网络的系统都具备这类性质， 也就特别难以预测。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;核心节点与一般节点：&lt;/span&gt; 巨大的网络分为核心节点和一般节点， 而核心节点的作用会对总体起到巨大的作用。 比如在历史的关键期某个人的决策。 无论是戊戌变法前的袁世凯， 还是改革开放中的总设计师。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;信息缺失：&lt;/span&gt;在这些难以预测的事物面前， 你能够知道的信息永远只是一部分。  比如你要预测一个公司能否发展好， 你可能真正要掌握的信息在CEO的大脑里， 而这样的信息一定在你所能接触涉及的范围挚爱。 我们在生活中往往很难考虑信息缺失对我们的决策的影响。 我们所有的决策都是信息缺失下的决策， 因此很多人之后会陷入后悔， 比如当时为什么没有早一点投资阿里， 事实上在你的每个决策节点， 你都是在信息稀缺的状态下，  此时的最优决策， 根本不意味着结果最好。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;集群涌现：&lt;/span&gt;  无论是社会还是人脑，一旦连城网络， 都会出现不可思议的效应，而这种效应绝不是个体水平能理解的， 这其实是第一个说的非线性作用的一个结果。  比如单个神经细胞可以用一堆电阻和电容来模拟， 而很多神经细胞凑合在一起， 居然会出现思维，甚至自我意识这么复杂的东西。而再把这群大脑连在一起， 居然出现了社会经济，文明， 意识形态这类的东西， 每个级别的现象，都不是组成它的那个级别所能够拥有的。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;观察者带来的反身性：&lt;/span&gt;曾经有人说如果你知道每个原子的轨迹 ， 你就可以预测整个宇宙。  这就是著名的拉普拉斯妖思想实验：我们可以把宇宙现在的状态视为其过去的果以及未来的因。假若一位智者会知道在某一时刻所有促使自然运动的力和所有组构自然的物体的位置，假若他也能够对这些数据进行分析，则在宇宙里，从最大的物体到最小的粒子，它们的运动都包含在一条简单公式里。对于这位智者来说，没有任何事物会是含糊的，并且未来只会像过去般出现在他眼前。&lt;/p&gt;

&lt;p&gt;“知道在某一时刻所有促使自然运动的力和所有组构自然的物体的位置”显然对于处于宇宙中的我们显然是不可能的。假如我们使用仪器对一个原子进行测量，仪器本身则会改变其它原子的状态。对一个系统的测量，不可避免的对系统造成干扰，这就是观察者效应。在股市里， 你在对某个东西进行预测的时候，你的行为决策必然作为新的变量对你所预测的对象产生改变， 你可以看做是一种现实扭曲力场。 这种观测者和观测对象的纠缠， 使得我们在稍微复杂的问题面前无能为力。&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;网络， 但来了世界的丰富多彩， 也带来了不可预测， 而这， 需要我们提出一个不同于以往的预测范式来解决。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;五 无处不在的相关性&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;我们看到， 虽然说这些不同的系统难以预测，但是它们却存在一个共同的属性，就是具备某种内在特征的相关性。 这些相关性经常被一些定性研究的学问所总结。 比如古代王朝的更迭被人们总结出合久必分， 分久必合的规律。 经济的运行通常呈现为周期律。  还有五形人格理论， 总结人的一些心理特征经常搭配的规律。 我们也经过把这些相互一起出现的特征， 称为模式。 &lt;/p&gt;

&lt;p&gt;复杂网络里潜藏的模式往往在网络的结构之中， 而如果抓住这些模式， 则给我们提供了一种预测的可能性。 巴拉巴西的一篇论文， 说的是你要不要预测一个国家的发展？ 我们可以看它在产业链网络中的位置， 因为存在这样一个网络， 较低级的产品和较低级的产品相连， 较高级的和较高级的相连， 如果你是生产香蕉的， 那么顶多生产和香蕉有关的一些食品或香料产业。 但是如果你是生产汽车上的门把手的， 你却可以在产业链中越级上升。 这就是产业网络的秘密。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccmUN8mLOsmGSknEX5C3jIHf3JCTLdowAyVv9FZZiavqWOia9msNicibibxhGnFiaxtmeVoW0WHlcSHfAvw/640?&quot; class=&quot;&quot; data-ratio=&quot;0.909375&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;另一个很好的例子是经济周期， 由于之前所说的所有混沌非线性，经济周期是一种非常复杂的现象，但是它里面的规律却也是比较明显的 ， 比如它的曲线走势有一个较明显的形状， 我们大概定性的知道这个形状背后的原因， 比如经济好的时候人们信心倍增开始举债， 然后到某个时候资本的价格开始上升， 利润率降低， 人们开始信心下降， 一些人破产， 等等。&lt;/p&gt;

&lt;p&gt;这些模式，给我们提示了，至少是在短期预测未来的可能性。 你去提取出模式， 就可能从某些信息， 推到另一个信息。 这个在古代， 经常被结合了迷信思想的理论所总结， 比如周易里面的周期律， 阴阳的理论等。 那么， 有没有可能把这些问题逐步的量化，成为一个可以定量预测的学科呢？&lt;/p&gt;

&lt;p&gt;如果说这个问题去问牛顿， 他一定说不可以， 如果你去问今天的人， 那就是可以。为什么呢？答案是： 大数据。   简单的说， 既然理论和逻辑推理不那么有效了， 我们就相信数字。 你要预测什么， 我就看一些历史数据，  从历史数据里拟合出一个趋势或者什么来预测。 或者直接对结果进行采样也行。&lt;/p&gt;

&lt;p&gt;那么随着人类积累的数据指数上升， 比如来自互联网， 摄像头记录， 以及每一台手机传递的数据，  我们就可以搞清所有复杂事物间的关系并做出任何预测。 我们说， 我们干脆放弃因果，而使用相关性来预测事物。通俗的讲就是数据告我们是什么，就是什么，莫要问为什么。&lt;/p&gt;

&lt;p&gt;这条思路我们可以打个比方叫做根据足印识别式子， 假设你住的地方有狮子， 不定时出现，你既然无法用一个牛顿方程预测狮子现在在哪， 就去寻找狮子的足印， 因为这和它的出现是高度相关的， 根据经验，我们认为足印和狮子是联系在一起的， 所以只要我有摄像头在关键道路记录足印，就可以了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; &lt;span&gt;六 大数据的成败根源&lt;/span&gt;&lt;/strong&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  大数据是每个人都非常熟悉的话题， 有的人说大数据是万能灵药， 有的人说它是骗人的， 其实关于大数据的总总争论， 只在于数据的质量。&lt;/p&gt;

&lt;p&gt;比如对于美国大选这个事，民意调查说明的是大数据的预测完全不靠谱，而谷歌搜索则表示大数据的预测很靠谱， 这里的区别在哪里？数据源。 哪个数据源更真实客观， 更接近决定选举结果的那群人， 就是更好。 事实上，谷歌搜索的数据往往是信息量最大， 也是最客观的， 因为直接的民意调查首选受到采样的不完全的影响， 比如在幼儿园和银行门口你的采样结果一定不一样。 另一方面， 被采样的人会因为这样或那样的原因来掩饰自己， 让自己看起来更cool一些。 而谷歌搜索就不会。&lt;/p&gt;

&lt;p&gt;这是川普和希拉里竞选时人们通过谷歌搜索看到的两人的排名情况。虽然官方的一些调查认为希拉里获胜的概率较大， 但是事实上拥有更大数据的谷歌早已预测了不同的情况。首先，我们相信搜索点击的数量客观反应了民意，  然后，我们认为这种民意会决定最后的选举结果。有了这样的逻辑， 我们用超大量的搜索数量验证了我们的想法是对的。&lt;/p&gt;

&lt;p&gt;当然，谷歌搜索数据也不是万能的。比如，谷歌流感预测。 谷歌流感是一个非常有野心的计划， 希望通过网络对关键词的搜索来判断流感爆发的趋势。 GFT预测H1N1流感的原理非常朴素如果在某一个区域某一个时间段有大量的有关流感的搜索指令那么就可能存在一种潜在的关联在这个地区就有很大可能性存在对应的流感人群相关部门就值得发布流感预警信息。&lt;/p&gt;
&lt;p&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.651685393258427&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpY7X1sCrf2DrALWYP5TObnHzqjQSo3HG0JlpSK1y5Voo8FOvWSAf6icQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;623&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然而这个计划最终失败了。 为什么失败？ 笼统的说叫噪声。大部分时候，我们都看不到数据里的信息。因为进行这一类信息查询的人事实上根本不清楚什么是流感。 很多搜索流感相关特征的人或者完全不知道流感是什么， 另一方面， 和流感有着类似特征的人太多， 这导致谷歌收集的数据里充满噪声。完全不了解更多的流感的知识而只是靠流感病本身的分析， 就完全不能分析出流感病爆发的趋势。  而且这样一个系统本身也在制造噪声，因为google flu流感指数提升反过来引起很多媒体对这个问题的敏感， 从而制造更多这方面的数据。&lt;/p&gt;

&lt;p&gt;还有一些事情， 比如全球变暖，即使我们到今天收集了那么多数据 ，依然是公说公有理， 婆说婆有理。如果你要靠谷歌搜索来确定全球到底变没变暖， 这还不如求助于那个会预测世界杯结果的章鱼。  &lt;/p&gt;

&lt;p&gt;数据一旦包含了和人有关的信息，就要考虑人的心理作用对数据客观性的影响， 比如大部分人认为自己努力工作的程度高于平均水平， 如果这是真的， 又什么是平均水平呢？ 由于人的认知偏差， 使得由人产生的数据总是充满噪声。&lt;/p&gt;

&lt;p&gt;那么我们直接剔除噪声呢？ 想的简单， 做起来不容易， 在数据处理中， 判断哪些是正常和异常点，通常需要巨量的专业知识。也就是说， 大数据的威力是巨大的， 但是单单靠大数据是不行的。 我们需要用我们先前讲过的模型， 来帮我们去伪存真， 剔除噪声。我们来讲讲这一类方法的思维。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;七 计算思维的三板斧&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;首先， 在数据驱动的模型前， 我们往往使用一个唯一认为正确的模型来预测， 比如牛顿定律， 给定了已知条件， 比如自由落体小球的位置和速度， 我们就可以得到一个唯一的结果来。  而现在的模型则是数据驱动的模型， 我们需要的是设定一组可能的原因和结果之间的关系，相当于我们有N个不同版本的牛顿定律（假设）， 当然这N个定律（假设）要满足一定的基本假设， 就好像是一个国家的公民要遵守基本法， 然后我们通过数据来求解哪一个假设为真的可能性大一些。  在这样的一个思维下， 占星术可以和牛顿方法同台竞技，最后谁胜出， 数据说了算。   &lt;/p&gt;

&lt;p&gt;为什么这样的方法可以从数据里把潜在的模式抓出来？ 因为事实上我们所处理的是一种已知的未知， 我们知道数据里一定存在某个模式， 只是我们不知道是什么， 这些数据驱动的模型，可以帮我们完成这个认定的过程。  &lt;/p&gt;
&lt;p&gt;相比物理模型的精确， 单单简单的看相关性来做预测的算法是不太够意思了。但是在巨大的数据矿藏面前， 配之以合适算法， 这类模型却成为战胜不可预测性， 发掘潜在模式的利器。 如果说以前的物理动力模型像是个一般正经背诵经典的书生， 那么机器学习模型就像经济学都不懂却可以在市场上讨价还价的小贩，他们具有一种更加practical的方法，就是经验调整法， 每天的经验告诉他们什么样的价格或时候可以该买进或卖出，以及看人下菜碟。无它，见人够多，从经验中总结教训。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpwZYNDOIibLIcibHVl2HyRYHO1n3bsiaKvnF4F0QEFFLZtV50rz7SuBiagg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于这一类模型背后的数学基础， 本质就是贝叶斯分析的一套东西。 首先， 你要给我一个对假设正确的先验分析， 然后， 我通过加载数据， 来调整数据正确的概率。这个先验知识， 就是之前我们的理论基础。  我们可以通过一两个有意思的例子来理解这个思路。&lt;/p&gt;

&lt;p&gt;比如说你要判断月亮是一块石头还是一个奶酪， 这个东西呢， 书里说是石头， 但是看上去（数据）比较像奶酪。 一个只听数据话的模型， 就会告诉你它是一个奶酪。 但是反过来， 如果这个模型是贝叶斯流的， 它会用先验把月亮是石头记作概率0.99， 奶酪0.01， 你的观测根据贝叶斯公式会增加它是奶酪的概率， 但是最多调整到0.02。当然， 这个模型过于简化了，我们可以给出一个更好点的数据驱动的模型例子， 即线性回归。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;八 模型的构建&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;从数据中预测的关键在于一种有果索因的思维， 我们从一个非常有趣的词 regression 说起。 很多人会用线性回归， 却未必知道这个词的真实含义。  这个词如果你去了解拉丁语的愿意， 意思是act of going back， 是回去，往回走的意思。 那么回的是哪里？ 我们说， 如果一组变量间存在相关性，比如A，B，C到D。 而且我们认为A，B，C可以确定D 。  那么我们说D就应该可以归属于一个A，B，C共同确定的函数上。    &lt;/p&gt;

&lt;p&gt;这个归属，也就是回归的意思了，即使你一开始发现的D的数据在这个曲线周围扰动， 如果你不停的收集数据，总是要回到那里去的。一个非常好的理解线性回归的例子， 是预测房产的价格， 一个房子的价格决定于非常多的相关因素，比如房子的地点，房子的面积，房子中卧室，厕所，客厅的数量和面积，周围环境的交通，安静与否，有无学校等，我们要从这些要素中知道房子的价格。如果你是链接聘请的房产专家，让你找到一个方法预测房价。你的方法一定是到各个地点做调查成交价，随着经验的增加， 你会得到各个区域的基准价格， 然后按照这个基本价格结合房子的特征上下调整，得出你的经验公式。这真是累死小哥哥。&lt;/p&gt;

&lt;p&gt;来，让我们用回归的思路来解一解这个问题，我们剖出我们的第一个是从数据反推假设的机器学习模型-线性回归。之后你会看到我们可以把过程让机器解决。我们现在需要的是设计一份表格，把之前的数据放到网上（或者搞一个爬虫），让人们把有关房子的有用无用的信息和价格都填了， 做一个巨大的excel表格。&lt;/p&gt;

&lt;p&gt;然后我们要设计一个机器（算法），我们用它从这些信息学习价格的pattern（模式），管他有用没用哦。然后看看我们如何教会机器思考， 机器开始不会思考，人要告诉他如何思考，机器思考首先是对人思考的模拟（三大范式-线性，决策树，贝叶斯）。 &lt;/p&gt;

&lt;p&gt;怎么说呢， 你要思考， 或者说决策，首先要考虑决定这个东西的所有可能的要素， 然后把这些按一定权重叠加一起，比如房子的价格，你看看决定房子的东西哪些比较重要， 你把它们按你的经验赋予一个权重（即使你自己不知道）然后你给出你心里的理想价位。&lt;/p&gt;

&lt;p&gt;换到机器上，也是一样的思路， 我们 把之前说的因素在这里换以一个新的词汇-特征。每一个特征， 这次要用一个数表示， 如同线性代数里的坐标基。而这些特征如何决定价格的， 我们就可以不管， 让机器决定去。特征的个数我们通常称之为维度，一个问题的维度往往决定其复杂性， 以及所使用的方法， 这就是人复杂系统所擅长讨论的范畴了在此不详述。 维度本身同时决定我们可能需求的数据量多少， 高维度意味着我们需要求解问题的信息量也成比例的增长。&lt;/p&gt;

&lt;p&gt;机器学习用模型来思考，模型就是寻找一个基本学习框架，比如刚刚我们说的加权重要性，翻译成机器学习的模型叫做线性回归：这就是把人的思维放了过来，我们随便给定一个权重， 机器就可以自己预测了。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;九 模型的求解&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;然后我们说在错误中改进和学习， 卖房小哥的评估也许开始是按照自己的经验来是房子的面积比较重要还是地点比较重要， 但是之后他很快被上级打的一塌糊涂，之后在一步步调整权重。其实比较好的方法， 是小哥把历史的经验好好看一遍，但是需要的数据毕竟太多了， 而小哥是真的做不到啊。&lt;/p&gt;

&lt;p&gt;机器，也是一样，我们要写一个程序，不仅模拟小哥挨揍学习的过程，而且还让它从全数据学。换成比较装逼的语言， 就是让一个非常初级的模型自动的进化，机器学习工程师要做的是把这个模式的毛坯找出来，以及这个寻找自动算法的算法做出来，这个一旦做出来， 后续的数据就像一颗颗子弹把模型打造成型。&lt;/p&gt;

&lt;p&gt;这个把模型打磨成型的过程就是学习，行里叫求解参数。什么叫参数呢， 首先，你会发现，就是刚刚思考说的权重， 每一个数字乘以具体的因子， 你会得到价格， 比如刚刚的卧室三间，面积2000，你会有无数个组合得到最后的结果250000（实验），但是这个组合势必会使得第二个数组很糟糕 （实验），如果你这么一个个的试下去， 你会累死。客观上讲， 你是不可能用那么几个参数，拟合这成千上万的数据！错误要有多少！&lt;/p&gt;

&lt;p&gt;所以，参数好不好，我们需要一次拿出所有数据来看，算出一个数， 这个数是整个数据集合的错误之和。Cost+=（a-b）*2 ，它可以衡量这组参数好不好用，继续看一下cost function的东西衡量模型预测的结果与真实值的差距， 模型的效果越差，这个cost function的值就越高。你可以理解为&lt;/p&gt;
&lt;p&gt;这个函数告诉你的是，Cost说的是代价，每次犯错都是潜在的风险和损失，我们称之为cost。而cost 函数来衡量目前模型离最终正确模型的距离。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;这个cost函数恰恰是由刚刚说的参数决定的。 当你改变某一个特征比如房屋的面积，你的房子价格就会变动，因此你的cost也会变化，但是变动的方式是什么， 你不知道， 你就去随机的扰动这些参数看看得来的价格会如何变化，并且和真实的价格信息比较，得到每次的cost。 如果你想两个参数theta1和theta2构成一个地图， 而地图上的等高线就是cost ， 那么你就是在寻找这座山最低的点，或者叫做谷底。更好的方式则是你每次在可能的方式是里寻找让cost减少最快的方向，俗名梯度， 然后往下走，忽然某个瞬间，你发现你身边的点都比较高， 你已经成功过的站在谷底，称为一名成功人士。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5617977528089888&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcd6xbaj5wPjJ0YjyGYoQ8QpfIxeO22zqfdGlBLlgGvqKku3Tn3OtfvFVs9FCRPBv1PSwzSwhv9zsg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;534&quot; /&gt;&lt;/p&gt;



&lt;p&gt;这组参数被称为房价问题最好的拟合，整个数据集都被这数涵盖。 这就是我们学习的结果， 放佛具有了一种人一样的预测未知房价的能力。因此， 你就可能拥有比全可国最牛看房师还牛的力量！为什么我们说是站在谷底， 而不是cost为0 呢？  因为在真实的情况， 100分本不存在。 你想想一下， 房子价格本身就有很多你根本无法了解的因素。 你买了王府井附近的 房子觉得好便宜， 然后你的邻居告你，上个月， 这里刚有个老妇人上吊死了。&lt;/p&gt;

&lt;p&gt;你永远不可能知道全部真相， 那些未知的因素， 我们统统称之为噪声， 就是对不可控的一种昵称。机器学习能否成功，就在于这个最优位置能否找到， 而在大多数时候，我们连这个最优位置是否存在都不知道。也有一些时候，我们开头的模型框架就选错了。当然刚刚说的随机方法很是不靠谱，实际操作中我们用到的是一个叫梯度下降的方法， 让参数顺着最快速减少预测错误的方向去自动调整。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;用一句话总结全文：物理学的预测遇到联网的人，不灵了，需要从相关性出发，通过寻找合适的数据， 合适的特征，合适的模型，合适的错误度量，合适的学习方法， 来用机器学习的范式在大数据时代做好预测。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; 更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384053&amp;amp;idx=1&amp;amp;sn=a1292fa38d2b3da000555b4b5ba92849&amp;amp;chksm=84f3c6b4b3844fa27ee93534098a20d3dd745089558629a502bb3b3a264ab3c1a523dceaf01b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;给小白看的AI最小入门指南（一）&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651384122&amp;amp;idx=1&amp;amp;sn=bb8cf6395f608a2eb6a59fa954cf481f&amp;amp;chksm=84f3c77bb3844e6da116ab919b8a7bf5ed60d203d8ec05f09c4453e39ccb9ae13b0c1a03a1e6&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-itemshowtype=&quot;0&quot; data-linktype=&quot;2&quot;&gt;AI最小入门指南（二）-- 人工智能简史&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Fri, 22 Mar 2019 16:56:44 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/pcrJ6lbBMG</dc:identifier>
</item>
</channel>
</rss>