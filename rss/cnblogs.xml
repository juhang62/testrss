<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>《SQL CookBook 》笔记-第三章-多表查询 - shanzm</title>
<link>http://www.cnblogs.com/shanzhiming/p/10403520.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/shanzhiming/p/10403520.html</guid>
<description>&lt;p&gt;第三章&lt;/p&gt;
&lt;p&gt;shanzm&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;注：笔记中的SQL语句只在SQL Server2014上测试过，不一定适应其他的DBMS，SQL server默认输出结果是网格格式，在此之后改为文本格式。&lt;/p&gt;
&lt;h2 id=&quot;叠加两个行集&quot;&gt;3.1 叠加两个行集&lt;/h2&gt;
&lt;p&gt;问题：&lt;br/&gt;你想返回保存在多个表中的数据&lt;/p&gt;
&lt;p&gt;解决方案：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;联合查询&lt;/strong&gt;，使用&lt;code&gt;union all&lt;/code&gt;合并多个表中的行&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select ename as ename_and_dname,deptno
from EMP
where deptno=10

union all 

select '----------',null
from T1

union all

select dname,deptno
from DEPT&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;ename_and_dname     deptno
---------------     -----------
CLARK               10
KING                10
MILLER              10
----------          NULL
ACCOUNTING          10
RESEARCH            20
SALES               30
OPERATIONS          40

(8 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【注意】&lt;/p&gt;
&lt;ol readability=&quot;0.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;多个select查询使用&lt;code&gt;union all&lt;/code&gt;连接，查询的列的结果放在一个结果集中，但是有一点要注意的是，每一个select的的列数必须相同，且每一个select查询的每一列的数据类型必须匹配 。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;如果每一个select查询的结果会有重复的行，那么可以使用&lt;code&gt;union&lt;/code&gt;来连接每一个查询。&lt;br/&gt;具体看下例：&lt;br/&gt;使用&lt;code&gt;union all&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from EMP
union all
select deptno
from DEPT&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deptno
-----------
20
30
30
20
30
30
10
20
10
30
20
30
20
10
10
20
30
40

(18 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;使用&lt;code&gt;union&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from emp
union
select deptno
from dept&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deptno
-----------
10
20
30
40

(4 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;大体而言，使用&lt;code&gt;union&lt;/code&gt;就相当于针对&lt;code&gt;union all&lt;/code&gt;的输出结果在执行一次&lt;code&gt;distinct&lt;/code&gt;操作&lt;/p&gt;
&lt;p&gt;如下：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select distinct deptno
from
(
    select deptno
    from emp
    union all
    select deptno
    from dept 
)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;关于优化，除非有必要，否则不要在查询中使用 DISTINCT 操作，同样的规则也适用于 UNION 。除非有&lt;br/&gt;必要，否则不要用 UNION 代替 UNION ALL&lt;/strong&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;h2 id=&quot;合并相关行&quot;&gt;3.2 合并相关行&lt;/h2&gt;
&lt;p&gt;问题：&lt;br/&gt;如果数据存储在多个表中，怎样用一条 SELECT 语句就检索出数据呢？&lt;/p&gt;
&lt;p&gt;例如：&lt;br/&gt;你想显示部门编号为 10 的全部员工的名字及其部门所在地，但这些数据分别存储在两个表里&lt;/p&gt;
&lt;p&gt;解决方案：&lt;br/&gt;使用连接。&lt;br/&gt;联结是一种机制，用来在一条 SELECT 语句中关联表，因此称为联结。使用特殊的语法，可以联结多个表返回一组输出，联结在运行时关联表中正确的行。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename,d.loc
from EMP as e,DEPT as d
where e.deptno=d.deptno and e.deptno=10&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;table&gt;&lt;thead/&gt;&lt;tbody&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;CLARK&lt;/td&gt;
&lt;td&gt;NEW YORK&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;KING&lt;/td&gt;
&lt;td&gt;NEW YORK&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;MILLER&lt;/td&gt;
&lt;td&gt;NEW YORK&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;【说明】&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;首先关于&lt;strong&gt;连接&lt;/strong&gt;，必须要理解什么是&lt;strong&gt;关系数据库&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;关系表的设计就是要把信息分解成多个表，一类数据一个表。各表通过某些共同的值互相关联（所以才叫关系数据库）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;连接查询就是查询多个表中的数据然后一起返回。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;使用表的连接查询，为了防止产生歧义，就要使用完全限定名，即：表名.列名。&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;需要注意，表别名只在查询执行中使用。与列别名不一样，表别名不返回到客户端。&lt;/p&gt;
&lt;p&gt;此处我们使用的是表别名来简写，在from子句中我们在带查询的表后使用&lt;code&gt;as&lt;/code&gt;写别名，其中as可以省略。&lt;/p&gt;
&lt;p&gt;因为Oracle不需要使用as，所以我们以后也就默认省略as。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;如果没有 WHERE子句，第一个表中的每一行将与第二个表中的每一行配对，而不管它们逻辑上是否能配在一起（这就是笛卡尔积）。&lt;/li&gt;
&lt;/ol&gt;&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;由没有联结条件的表关系返回的结果为&lt;strong&gt;笛卡儿积&lt;/strong&gt;。检索出的行的数目将是第一个表中的行数乘以第二个表中的行数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;【说明】&lt;/p&gt;
&lt;p&gt;内连接又称为等值连接(equal join),他是基于两个表之间的某列相等来做连接。&lt;/p&gt;
&lt;p&gt;上面的例子就是内连接，但是那称为&lt;strong&gt;隐式的内连接&lt;/strong&gt;，&lt;/p&gt;
&lt;p&gt;我们也可以使用&lt;strong&gt;显示的内连接&lt;/strong&gt;&lt;br/&gt;显示的内连接使用&lt;code&gt;inner join&lt;/code&gt;连接两个表，使用&lt;code&gt;on&lt;/code&gt;子句做连接条件。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename,d.loc
from EMP as e inner join DEPT as d
on e.deptno=d.deptno and e.deptno=10&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【注意】关于SQL优化：不要联结不必要表。联结的表越多，性能下降越厉害。&lt;/p&gt;

&lt;hr/&gt;&lt;h2 id=&quot;查找两个表中相同的行&quot;&gt;3.3 查找两个表中相同的行&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你想找出两个表中相同的行&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;create view V
as
select ename,job,sal
from emp
where job = 'CLERK'&lt;/code&gt;
&lt;/pre&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select * from V&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;ENAME      JOB        SAL
---------- --------- ----------
SMITH      CLERK      800
ADAMS      CLERK      1100
JAMES      CLERK      950
MILLER     CLERK      1300&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;视图 V 只包含职位是 CLERK 的员工，但并没有显示 EMP 表中所有可能的列。你想从 EMP 表获取与视图 V 相匹配的全部员工的 EMPNO 、 ENAME 、 JOB 、 SAL 和 DEPTNO ，并且希望得到如下所示的结果集:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;EMPNO    ENAME      JOB       SAL        DEPTNO
-------- ---------- --------- ---------- ----------
7369     SMITH      CLERK     800        20
7876     ADAMS      CLERK     1100       20
7900     JAMES      CLERK     950        30
7934     MILLER     CLERK     1300       10
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;解决方案:使用连接查询&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.empno,e.ename,e.job,e.sal,e.deptno
from emp e, V
where e.ename = v.ename
and e.job = v.job
and e.sal = v.sal&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;也可以使用&lt;code&gt;join&lt;/code&gt;写法&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.empno,e.ename,e.job,e.sal,e.deptno
from emp e join V
on 
( e.ename = v.ename
  and e.job = v.job
  and e.sal = v.sal 
)&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;&lt;h2 id=&quot;查找只存在于一个表中的数据&quot;&gt;3.4 查找只存在于一个表中的数据&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你希望从一个表（可以称之为源表）里找出那些在某个目标表里不存在的值。&lt;/p&gt;
&lt;p&gt;例如：&lt;br/&gt;你想找出在 DEPT 表中存在而在 EMP 表里却不存在的部门编号（如果有的话）。&lt;/p&gt;
&lt;p&gt;解决方案：&lt;br/&gt;使用子查询得到 EMP 表中所有的 DEPTNO ，并将该结果传入外层查询，然后外层查询会检索DEPT 表，找出没有出现在子查询结果里的 DEPTNO 值。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno from DEPT where deptno not in (select deptno from EMP)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;p&gt;【注意】如果子查询&lt;code&gt;select deptno from EMP&lt;/code&gt;的返回值的中有NUll值，就会复杂了。&lt;/p&gt;
&lt;p&gt;因为IN 和NOT IN 本质上是 OR 运算，由于 Null 值参与 OR 逻辑运算的方式不同， IN 和 NOT IN 将会产生不同的结果。&lt;/p&gt;
&lt;p&gt;考虑以下分别使用 IN 和 OR 的例子&lt;br/&gt;其实我们知道DEPT表中的deptno列中的值有10，没有50和null&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from dept
where deptno in ( 10,50,null )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from dept
where (deptno=10 or deptno=50 or deptno=null)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;p&gt;再来看看使用 NOT IN 和 NOT OR 的例子:&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from dept
where deptno not in ( 10,50,null )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;br/&gt;无&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno
from dept
where not (deptno=10 or deptno=50 or deptno=null)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;br/&gt;无&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;in ( 10,50,null )等价于(deptno=10 or deptno=50 or deptno=null)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【分析】&lt;/p&gt;
&lt;p&gt;and的真值表&lt;/p&gt;
&lt;table&gt;&lt;thead/&gt;&lt;tbody&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;or的真值表&lt;/p&gt;
&lt;table&gt;&lt;thead/&gt;&lt;tbody&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;not的真值表&lt;/p&gt;
&lt;table&gt;&lt;thead/&gt;&lt;tbody&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;flase&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;切记：在 SQL 中， TRUE or NULL 的运算结果是 TRUE ，但 FALSE or NULL 的运算结果却是 Null&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;必须谨记，当使用 IN 谓词以及当执行 OR 逻辑运算的时候，你要想到是否会涉及 Null 值&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;【注意】&lt;br/&gt;为了避免 NOT IN 和 Null 值带来的问题，需要结合使用 NOT EXISTS 和关联子查询。关联子查询指的是外层查询执行后获得的结果集会被内层子查询引用。下面的例子给出了一个免受 Null 值影响的替代方案&lt;/p&gt;
&lt;p&gt;回到“问题”--你想找出在 DEPT 表中存在而在 EMP 表里却不存在的部门编号（如果有的话）&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.demptno
from DEPT d
where not exists
(
    select null
    from EMP e
    where e.deptno=d.deptno
)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【分析】&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;如果子查询有结果返回给外层查询，那么 EXISTS (...) 的评估结果是 TRUE ，这样 NOT EXISTS (...) 就是 FALSE ，如此一来，外层查询就会舍弃当前行。&lt;/li&gt;
&lt;li&gt;如果子查询没有返回任何结果，那么 NOT EXISTS (...) 的评估结果是 TRUE ，由此外层查询就会返回当前行（因为它是一个不存在于 EMP 表中的部门编号）。&lt;/li&gt;
&lt;li&gt;把 EXISTS/NOT EXISTS 和关联子查询一起使用时， SELECT 列表里的项目并不重要，因此我在这个例子中用了 SELECT NULL&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;&lt;h2 id=&quot;从一个表检索与另一个表不相关的行&quot;&gt;3.5 从一个表检索与另一个表不相关的行&lt;/h2&gt;
&lt;p&gt;问题：&lt;br/&gt;两个表有相同的键，你想在一个表里查找与另一个表不相匹配的行。&lt;/p&gt;
&lt;p&gt;例如：&lt;br/&gt;你想找出哪些部门没有员工。&lt;/p&gt;
&lt;p&gt;分析：&lt;br/&gt;你发现好像和 &lt;em&gt;3.3 查找只存在于一个表中的数据&lt;/em&gt; 是一样的，只要修改为&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.*
from dept d
where not exists 
( 
    select null
    from emp e
    where d.deptno = e.deptno
)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;当然我们还有其他方法：使用&lt;strong&gt;外连接&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.*
from dept d left outer join emp e
on (d.deptno = e.deptno)
where e.deptno is null
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;DEPTNO      DNAME          LOC
----------- -------------- -------------
40          OPERATIONS     BOSTON

(1 行受影响)&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;&lt;h2 id=&quot;新增连接查询而不影响其他连接查询&quot;&gt;3.6 新增连接查询而不影响其他连接查询&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你已经有了一个查询语句，它可以返回你想要的数据。你需要一些额外信息，但当你试图获取这些信息的时候，却丢失了原有的查询结果集中的数据。&lt;/p&gt;
&lt;p&gt;例如:&lt;br/&gt;首先，这个例子我们要补充一个员工奖金表EMP_Bonus&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;EMPNO        RECEIVED       TYPE
----------   -----------    ----------
7369         14-MAR-2005    1
7900         14-MAR-2005    2
7788         14-MAR-2005    3&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;你想查找所有员工的信息，包括他们所在部门的位置，以及他们收到奖金的日期。&lt;/p&gt;
&lt;p&gt;最初，你使用如下所示的查询语句&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename, d.loc
from emp e, dept d
where e.deptno=d.deptno&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;ENAME           LOC
----------      -------------
SMITH           DALLAS
ALLEN           CHICAGO
WARD            CHICAGO
JONES           DALLAS
MARTIN          CHICAGO
BLAKE           CHICAGO
CLARK           NEW YORK
SCOTT           DALLAS
KING            NEW YORK
TURNER          CHICAGO
ADAMS           DALLAS
JAMES           CHICAGO
FORD            DALLAS
MILLER          NEW YORK&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;对于有奖金的员工，你希望把他们收到奖金的日期也添加到结果集里，但连接了 EMP_BONUS 表后得到的行数却比预期的要少，因为并非所有的员工都有奖金。&lt;/p&gt;
&lt;p&gt;如下：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename, d.loc,eb.received
from emp e, dept d, emp_bonus eb
where e.deptno=d.deptno
and e.empno=eb.empno
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;ENAME      LOC           RECEIVED
---------- ------------- -----------
SCOTT      DALLAS        14-MAR-2005
SMITH      DALLAS        14-MAR-2005
JAMES      CHICAGO       14-MAR-2005&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;但是我们想要的结果是这样的：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;ename      loc           received
---------- ------------- -----------------------
SMITH      DALLAS        2005-03-14 00:00:00.000
ALLEN      CHICAGO       NULL
WARD       CHICAGO       NULL
JONES      DALLAS        NULL
MARTIN     CHICAGO       NULL
BLAKE      CHICAGO       NULL
CLARK      NEW YORK      NULL
SCOTT      DALLAS        2005-03-14 00:00:00.000
KING       NEW YORK      NULL
TURNER     CHICAGO       NULL
ADAMS      DALLAS        NULL
JAMES      CHICAGO       2005-03-14 00:00:00.000
FORD       DALLAS        NULL
MILLER     NEW YORK      NULL

(14 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;解决方案1:&lt;br/&gt;使用外连接既能够获得额外信息，又不会丢失原有的信息。&lt;/p&gt;
&lt;p&gt;首先连接 EMP 表和 DEPT 表，得到全部员工和他们所在部门的位置。&lt;br/&gt;然后外连接 EMP_BONUS 表，如果某个员工有奖金，则检索其收到奖金的日期。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename, d.loc, eb.received
from emp e join dept d
on (e.deptno=d.deptno)
left join emp_bonus eb
on (e.empno=eb.empno)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;解决方案2：&lt;br/&gt;使用&lt;strong&gt;标量子查询&lt;/strong&gt;（即把子查询放置在 SELECT 列表里）来模仿外连接操作&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename, d.loc,
    (
        select eb.received from emp_bonus eb 
        where eb.empno=e.empno
    ) as received
from emp e, dept d
where e.deptno=d.deptno&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果同上。&lt;/p&gt;
&lt;p&gt;【分析】&lt;/p&gt;
&lt;ol readability=&quot;0.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;外连接查询会返回一个表中的所有行，以及另一个表中与之匹配的行。外连接之所以能够解决本问题，是因为它不会过滤掉任何应该被返回的行。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;在不破坏当前结果集的情况下，标量子查询是为现有查询语句添加额外数据的好办法。当使用标量子查询时，必须确保它们返回的是标量值（单值）。如果 SELECT 列表里的子查询返回多行，那么查询将会出错&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr/&gt;&lt;h2 id=&quot;确定两个表是否有相同的数据&quot;&gt;3.7 确定两个表是否有相同的数据&lt;/h2&gt;
&lt;p&gt;问题：&lt;br/&gt;你想知道两个表或两个视图里是否有相同的数据（行数和值）&lt;/p&gt;
&lt;p&gt;为了距离我们新建一个视图V：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;create view    
as
select * from EMP
where deptno!=10
union all
select * from EMP 
where ename='WARD'&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;视图的具体数据如下：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select * from V&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;EMPNO       ENAME      JOB       MGR         HIREDATE                SAL         COMM        DEPTNO
----------- ---------- --------- ----------- ----------------------- ----------- ----------- -----------
7369        SMITH      CLERK     7902        1980-12-17 00:00:00.000 800         NULL        20
7499        ALLEN      SALESMAN  7698        1981-02-20 00:00:00.000 1600        300         30
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30
7566        JONES      MANAGER   7839        1981-04-02 00:00:00.000 2975        NULL        20
7654        MARTIN     SALESMAN  7698        1981-09-28 00:00:00.000 1250        1400        30
7698        BLAKE      MANAGER   7839        1981-05-01 00:00:00.000 2850        NULL        30
7788        SCOTT      ANALYST   7566        1982-12-09 00:00:00.000 3000        NULL        20
7844        TURNER     SALESMAN  7698        1981-09-08 00:00:00.000 1500        0           30
7876        ADAMS      CLERK     7788        1983-01-12 00:00:00.000 1100        NULL        20
7900        JAMES      CLERK     7698        1981-12-03 00:00:00.000 950         NULL        30
7902        FORD       ANALYST   7566        1981-12-03 00:00:00.000 3000        NULL        20
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30

(12 行受影响)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个视图V的内容和MEP表的不同就是视图V不包含部门编号为10的员工，同时这个视图有同时包含两个ename=WARD的数据。&lt;/p&gt;
&lt;p&gt;我们的问题就是要找到视图V和EMP表的不同之处：&lt;br/&gt;我们渴望的结果是：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;empno       ename      job       mgr         hiredate                sal         comm        deptno      cnt
----------- ---------- --------- ----------- ----------------------- ----------- ----------- ----------- -----------
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30          1
7782        CLARK      MANAGER   7839        1981-06-09 00:00:00.000 2450        NULL        10          1
7839        KING       PRESIDENT NULL        1981-11-17 00:00:00.000 5000        NULL        10          1
7934        MILLER     CLERK     7782        1982-01-23 00:00:00.000 1300        NULL        10          1
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30          2

(5 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;解决方案：&lt;br/&gt;使用关联子查询和 UNION ALL 找出那些存在于视图 V 而不存在于 EMP 表的数据，以及存在于 EMP 表而不存在于视图 V 的数据，并将它们合并起来。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select *
from 
(
  select e.empno,e.ename,e.job,e.mgr,e.hiredate,e.sal,e.comm,e.deptno, count(*) as cnt
  from emp e
  group by empno,ename,job,mgr,hiredate,
  sal,comm,deptno
)e
where not exists 
(
  select null
  from 
  (
    select v.empno,v.ename,v.job,v.mgr,v.hiredate,
    v.sal,v.comm,v.deptno, count(*) as cnt
    from v
    group by empno,ename,job,mgr,hiredate,
    sal,comm,deptno
  )v
 where v.empno = e.empno
 and v.ename = e.ename
 and v.job = e.job
 and v.mgr = e.mgr
 and v.hiredate = e.hiredate
 and v.sal = e.sal
 and v.deptno = e.deptno
 and v.cnt = e.cnt
 and coalesce(v.comm,0) = coalesce(e.comm,0)
 )

 union all

 select *
 from 
 (
   select v.empno,v.ename,v.job,v.mgr,v.hiredate,v.sal,v.comm,v.deptno, count(*) as cnt
   from v
   group by empno,ename,job,mgr,hiredate,
   sal,comm,deptno
 )v
 where not exists 
 (
   select null
   from 
   (
      select e.empno,e.ename,e.job,e.mgr,e.hiredate,
      e.sal,e.comm,e.deptno, count(*) as cnt
      from emp e
      group by empno,ename,job,mgr,hiredate,
      sal,comm,deptno
   )e
 where v.empno = e.empno
 and v.ename = e.ename
 and v.job = e.job
 and v.mgr = e.mgr
 and v.hiredate = e.hiredate
 and v.sal = e.sal
 and v.deptno = e.deptno
 and v.cnt = e.cnt
 and coalesce(v.comm,0) = coalesce(e.comm,0)
 )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中：首先是查询视图e(由EMP表创建)中存在，而视图V中不存在的数据&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select *
from 
(
  select e.empno,e.ename,e.job,e.mgr,e.hiredate,e.sal,e.comm,e.deptno, count(*) as cnt
  from emp e
  group by empno,ename,job,mgr,hiredate,
  sal,comm,deptno
)e

where not exists 
(
  select null
  from 
  (
    select v.empno,v.ename,v.job,v.mgr,v.hiredate,v.sal,v.comm,v.deptno, count(*) as cnt
    from v
    group by empno,ename,job,mgr,hiredate,sal,comm,deptno
  )v
 where v.empno = e.empno
 and v.ename = e.ename
 and v.job = e.job
 and v.mgr = e.mgr
 and v.hiredate = e.hiredate
 and v.sal = e.sal
 and v.deptno = e.deptno
 and v.cnt = e.cnt
 and coalesce(v.comm,0) = coalesce(e.comm,0)
 )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;
empno       ename      job       mgr         hiredate                sal         comm        deptno      cnt
----------- ---------- --------- ----------- ----------------------- ----------- ----------- ----------- -----------
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30          1
7782        CLARK      MANAGER   7839        1981-06-09 00:00:00.000 2450        NULL        10          1
7839        KING       PRESIDENT NULL        1981-11-17 00:00:00.000 5000        NULL        10          1
7934        MILLER     CLERK     7782        1982-01-23 00:00:00.000 1300        NULL        10          1

(4 行受影响)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【注意】注意，这里比较的不是 EMP 表和视图 V ，而是内嵌视图 E 和内嵌视图 V 。&lt;/p&gt;
&lt;p&gt;同理，UNION ALL 后面的查询语句做了相反的操作，它找出了所有存在于内嵌视图 V 而不存在于内嵌视图 E 的行。&lt;/p&gt;
&lt;p&gt;如下:&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select *
from (
select v.empno,v.ename,v.job,v.mgr,v.hiredate,
v.sal,v.comm,v.deptno, count(*) as cnt
from v
group by empno,ename,job,mgr,hiredate,
sal,comm,deptno
) v
where not exists (
select null
from (
select e.empno,e.ename,e.job,e.mgr,e.hiredate,
e.sal,e.comm,e.deptno, count(*) as cnt
from emp e
group by empno,ename,job,mgr,hiredate,
sal,comm,deptno
) e
where v.empno = e.empno
and v.ename = e.ename
and v.job = e.job
and v.mgr = e.mgr
and v.hiredate = e.hiredate
and v.sal = e.sal
and v.deptno = e.deptno
and v.cnt = e.cnt
and coalesce(v.comm,0) = coalesce(e.comm,0)
)&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;empno       ename      job       mgr         hiredate                sal         comm        deptno      cnt
----------- ---------- --------- ----------- ----------------------- ----------- ----------- ----------- -----------
7521        WARD       SALESMAN  7698        1981-02-22 00:00:00.000 1250        500         30          2

(1 行受影响)&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;&lt;h2 id=&quot;识别并消除笛卡儿积&quot;&gt;3.8 识别并消除笛卡儿积&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你想找出部门编号为 10 的所有员工的名字及其部门所在的城市。&lt;/p&gt;
&lt;p&gt;解决方案：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.ename, d.loc
from emp e, dept d
where e.deptno = 10
and d.deptno = e.deptno&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;分析：你也许会说这个问题好像很简单呀！但是很多时候我们会忘记&lt;code&gt;d.deptno = e.deptno&lt;/code&gt;这个限制条件&lt;/p&gt;
&lt;p&gt;记不记得我们之前说过，没有where语句限制的连接查询就是交叉连接，返回的结果就是笛卡尔积&lt;/p&gt;
&lt;p&gt;【注意】&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;为了消除笛卡儿积，我们通常会用到 &lt;strong&gt;n-1 法则&lt;/strong&gt;，其中 n 代表 FROM 子句里表的个数，n-1 则代表消除笛卡儿积所必需的连接查询的最少次数。&lt;br/&gt;依据表里有什么样的键以及基于哪些列来实现表之间的连接操作，有时候必要的连接查询次数可能会超过 n-1 次，但是当我们编写查询语句的时候，n-1 法则仍然是一个很好的指导原则。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr/&gt;&lt;h2 id=&quot;组合使用连接查询与聚合函数&quot;&gt;3.9 组合使用连接查询与聚合函数&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你想执行一个聚合操作，但查询语句涉及多个表。你希望确保表之间的连接查询不会干扰聚合操作。&lt;/p&gt;
&lt;p&gt;例如；你希望计算部门编号为 10 的员工的工资总额以及奖金总和。&lt;/p&gt;
&lt;p&gt;在这个问题中， EMP_BONUS 表里有如下数据。&lt;/p&gt;
&lt;p&gt;EMP_BONUS 表中的 TYPE 列决定了奖金的数额。若 TYPE 值等于 1，则奖金为工资的 10%；若 TYPE 值等于2，则奖金为工资的 20%；若 TYPE 值等于 3，则奖金为工资的 30%。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select * from emp_bonus&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;EMPNO RECEIVED    TYPE
----- ----------- ----------
7934  17-MAR-2005 1
7934  15-FEB-2005 2
7839  15-FEB-2005 3
7782  15-FEB-2005 1&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;首先，考虑下面的查询语句，它返回了部门编号为 10 的所有员工的工资和奖金。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.empno,
       e.ename,
       e.sal,
       e.deptno,
       e.sal*case when eb.type=1 then .1
                  when eb.type=2 then .2
                  else .3
              end as bonus
from emp e ,emp_bonus eb
where e.empno=eb.empno
and e.deptno=10       
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;empno       ename      sal         deptno      bonus
----------- ---------- ----------- ----------- ------------
7934        MILLER     1300        10          130.0
7934        MILLER     1300        10          260.0
7839        KING       5000        10          1500.0
7782        CLARK      2450        10          245.0

(4 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;现在我们想要计算部门编号为10 的所有员工的工资总额和奖金总额，我们能按照下面SQL语句计算吗？&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno,sum(sal) as total_sal,sum(bonus) as total_bonus
from
(
select e.empno,
       e.ename,
       e.sal,
       e.deptno,
       e.sal*case when eb.type=1 then .1
                  when eb.type=2 then .2
                  else .3
              end as bonus
from emp e ,emp_bonus eb
where e.empno=eb.empno
and e.deptno=10       
)X
group by deptno&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deptno      total_sal   total_bonus
----------- ----------- ----------------
10          10050       2135.0

(1 行受影响)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面结果中total_bonus是对的，但是 total_sal并不对，因为7934号员工得了两次奖金，所以他出现了两次在子查询结果中，最后在计算total_sal时，计算了两次。&lt;br/&gt;简而言之，就是因为有部分员工多次获得奖金，所以在 EMP 表和 EMP_BONUS 表连接之后再执行聚合函数 SUM ，就会得出错误的计算结果。&lt;/p&gt;
&lt;p&gt;解决方案：&lt;/p&gt;
&lt;p&gt;在连接查询里进行聚合运算时，必须十分小心才行。如果连接查询产生了重复行，通常有两种办法来使用聚合函数，而且可以避免得出错误的计算结果。&lt;/p&gt;
&lt;p&gt;法1：调用聚合函数时直接使用关键字 &lt;code&gt;DISTINCT&lt;/code&gt; ，这样每个值都会先去掉重复项再参与计算；&lt;br/&gt;SQL语句如下：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno,
       sum( distinct sal) as total_sal,
       sum( bonus) as total_bonus
from
(
    select e.empno,
           e.ename,
           e.sal,
           e.deptno,
           e.sal*case when eb.type=1 then .1
                      when eb.type=2 then .2
                      else .3
                  end as bonus
    from emp e ,emp_bonus eb
    where e.empno=eb.empno
    and e.deptno=10       
)X
group by deptno
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;法2：在进行连接查询之前先执行聚合运算（以内嵌视图的方式），这样可以避免错误的结果，因为聚合运算发生在连接查询之前。&lt;/p&gt;
&lt;p&gt;SQL语句如下：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,
       d.total_sal,
       sum(e.sal*case when eb.type = 1 then .1
                      when eb.type = 2 then .2
                      else .3 end) as total_bonus
from emp e,
     emp_bonus eb,
     (
       select deptno, sum(sal) as total_sal
       from emp
       where deptno = 10
       group by deptno
     ) d
where e.deptno = d.deptno
and e.empno = eb.empno
group by d.deptno,d.total_sal
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中子查询所得到的临时表d：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deptno      total_sal
----------- -----------
10          8750&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;&lt;h2 id=&quot;组合使用外连接查询与聚合函数&quot;&gt;3.10 组合使用外连接查询与聚合函数&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;本节的问题和 3.9 节的大致相同:计算出了部门编号为 10 的员工的工资总额和奖金总额。&lt;/p&gt;
&lt;p&gt;但是略微修改了 EMP_BONUS 表的数据，使得部门编号为10 的员工中只有部分人获得了奖金。&lt;br/&gt;(上一题的EMP_BONUS 表有四行数据，其中一人得了两次奖金，但是10号部门（总共就3人）的所有人都是有奖金的)&lt;/p&gt;
&lt;p&gt;EMP_BONUS表的数据：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;EMPNO   RECEIVED     TYPE
------- --------     ---------
7934    17-MAR-2005  1
7934    15-FEB-2005  2&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;我们依旧按照上一节的SQL语句查询&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select deptno,
       sum( distinct sal) as total_sal,
       sum( bonus) as total_bonus
from
(
     select e.empno,
            e.ename,
            e.sal,
            e.deptno,
            e.sal*case when eb.type=1 then .1
                       when eb.type=2 then .2
                       else .3
                   end as bonus
     from emp e ,emp_bonus eb
     where e.empno=eb.empno
     and e.deptno=10       
)X
group by deptno
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果是错误的，为什么呢？&lt;br/&gt;我们来看其中的子查询：&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.empno,
       e.ename,
       e.sal,
       e.deptno,
       e.sal*case when eb.type=1 then .1
                  when eb.type=2 then .2
                  else .3
              end as bonus
from emp e ,emp_bonus eb
where e.empno=eb.empno
and e.deptno=10 &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;empno       ename      sal         deptno      bonus
----------- ---------- ----------- ----------- ------------
7934        MILLER     1300        10          130.0
7934        MILLER     1300        10          260.0

(2 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所以按照上一题的SQL语句来查询，则查询没有计算部门编号为 10 的全部员工的工资总额，实际上只有 MILLER 的工资被计入总和，而且被错误地计算了两次。&lt;/p&gt;
&lt;p&gt;解决方案1：&lt;br/&gt;下面的解决方案也和 3.9 节的类似，不同之处在于要外连接 EMP_BONUS 表，确保把部门编号为 10 的全部员工都包括进来。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select  deptno,
        sum(distinct sal) as total_sal,
        sum(bonus) as total_bonus
from 
(
    select e.empno,
           e.ename,
           e.sal,
           e.deptno,
           eb.type,
           e.sal*case when eb.type is null then 0
                      when eb.type = 1 then .1
                      when eb.type = 2 then .2
                      else .3 
                 end as bonus
     from emp e left outer join emp_bonus eb
     on (e.empno = eb.empno)
     where e.deptno = 10
 )X
 group by deptno&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deptno      total_sal   total_bonus
----------- ----------- ------------
10          8750        390.0

(1 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;注意其中的子查询&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select e.empno,
       e.ename,
       e.sal,
       e.deptno,
       eb.type,
       e.sal*case when eb.type is null then 0
                  when eb.type=1 then .1
                  when eb.type=2 then .2
                  else .3
              end as bonus
from emp e left outer join emp_bonus eb
on e.empno=eb.empno
where e.deptno=10 &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;注意看：为什么case语句比上一题中多了一个&lt;code&gt;when eb.type is null then 0&lt;/code&gt;,因为有一部分的10号部门的员工是没有奖金的，所以外连接的时候，eb.type是null&lt;/p&gt;
&lt;p&gt;这个子查询的结果：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;empno       ename      sal         deptno      type        bonus
----------- ---------- ----------- ----------- ----------- -----------
7782        CLARK      2450        10          NULL        735.0
7839        KING       5000        10          NULL        1500.0
7934        MILLER     1300        10          1           130.0
7934        MILLER     1300        10          2           260.0

(4 行受影响)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;解决方案2:&lt;br/&gt;首先计算部门编号为 10 的员工的工资总额，然后再连接 EMP 表和 EMP_BONUS 表（这样就避免了使用外连接）。&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,
       d.total_sal,
       sum(e.sal*case when eb.type = 1 then .1
                      when eb.type = 2 then .2
                      else .3 
                  end) as total_bonus
from emp e,
     emp_bonus eb,
     (
        select deptno, sum(sal) as total_sal
        from emp
        where deptno = 10
        group by deptno
     ) d
where e.deptno = d.deptno
and e.empno = eb.empno
group by d.deptno,d.total_sal
&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;&lt;h2 id=&quot;从多个表中返回缺失值&quot;&gt;3.11 从多个表中返回缺失值&lt;/h2&gt;
&lt;p&gt;问题:&lt;br/&gt;你想从多个表中返回缺失值。(换言之，就是使用某种连接，将两个表相连接，之后找到两个表的所有相匹配的数据，包括空值)&lt;/p&gt;
&lt;p&gt;列出所有的部门，以及该部门的所有员工（如果这个部门有员工的话）&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,d.dname,e.ename
from dept d left outer join emp e
on (d.deptno=e.deptno)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;列出所有的员工，以及该员工所属的部门（如果这个员工有部门的话，当然我们的表中所有的员工都是有部门的，这里仅仅是为了举例子）&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,d.dname,e.ename
from  dept d right outer join emp e
on (d.deptno=e.deptno)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;那么我们想要把所有的员工和部门都列出来，不论该部门是否有员工，也不论某个员工是否有部门&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,d.dname,e.ename
from dept d left outer join emp e
on (d.deptno=e.deptno)

union 

select d.deptno,d.dname,e.ename
from  dept d right outer join emp e
on (d.deptno=e.deptno)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其实我们可以使用全连接&lt;code&gt;full outer join&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select d.deptno,d.dname,e.ename
from  dept d full outer join emp e
on (d.deptno=e.deptno)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;【说明】&lt;br/&gt;全外连接查询其实就是合并两个表的外连接查询的结果集&lt;/p&gt;

&lt;hr/&gt;&lt;h2 id=&quot;在运算和比较中使用-null&quot;&gt;3.12 在运算和比较中使用 Null&lt;/h2&gt;
&lt;p&gt;问题：&lt;br/&gt;Null 不会等于或不等于任何值，甚至不能与其自身进行比较，但是你希望对从 Null 列返回的数据进行评估，就像评估具体的值一样。&lt;/p&gt;
&lt;p&gt;例如：&lt;br/&gt;你想找出 EMP 表里业务提成（ COMM列）比员工 WARD 低的所有员工。检索结果应该包含业务提成为 Null 的员工。、&lt;/p&gt;
&lt;p&gt;解决方案：使用使用 &lt;code&gt;coalesce()&lt;/code&gt; 函数将 Null 值替代为实际值。（&lt;code&gt;coalesce&lt;/code&gt;函数说明见 1.12）&lt;/p&gt;
&lt;p&gt;&lt;code&gt;coalesce&lt;/code&gt; 函数会返回参数列表里的第一个非 Null 值&lt;/p&gt;
&lt;pre class=&quot;sql&quot;&gt;
&lt;code&gt;select ename , comm
from emp
where coalesce(comm, 0)&amp;lt;(select comm from emp where ename='WARD')&lt;/code&gt;
&lt;/pre&gt;

&lt;hr/&gt;</description>
<pubDate>Tue, 05 Mar 2019 18:26:00 +0000</pubDate>
<dc:creator>shanzm</dc:creator>
<og:description>第三章 shanzm [TOC] 注：笔记中的SQL语句只在SQL Server2014上测试过，不一定适应其他的DBMS，SQL server默认输出结果是网格格式，在此之后改为文本格式。 3.1</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/shanzhiming/p/10403520.html</dc:identifier>
</item>
<item>
<title>目标检测之YOLO V2 V3 - Brook_icv</title>
<link>http://www.cnblogs.com/wangguchangqing/p/10480995.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/wangguchangqing/p/10480995.html</guid>
<description>&lt;h2 id=&quot;yolo-v2&quot;&gt;YOLO V2&lt;/h2&gt;
&lt;p&gt;YOLO V2是在YOLO的基础上，融合了其他一些网络结构的特性（比如：Faster R-CNN的Anchor,GooLeNet的&lt;span class=&quot;math inline&quot;&gt;\(1\times1\)&lt;/span&gt;卷积核等），进行的升级。其目的是弥补YOLO的两个缺陷：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;YOLO中的大量的定位错误&lt;/li&gt;
&lt;li&gt;和基于区域推荐的目标检测算法相比，YOLO的召回率（Recall）较低。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;YOLO V2的目标是：在保持YOLO分类精度的同时，提高目标定位的精度以及召回率。其论文地址：&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.08242&quot; title=&quot;YOLO 9000:Better，Faster，Stronger&quot;&gt;YOLO 9000:Better，Faster，Stronger&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;YOLO论文的名称总是如此的直抒胸臆，&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Better 指的是和YOLO相比，YOLO V2有更好的精度&lt;/li&gt;
&lt;li&gt;Faster 指的是修改了网络结构，其检测更快&lt;/li&gt;
&lt;li&gt;Stronger 指的就是YOLO 9000,使用联合训练的方法，同时使用目标检测和图像分类的数据集，训练YOLO V2，训练出来的模型能够实时的识别多达9000种目标，所以也称为YOLO9000。&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;better&quot;&gt;Better&lt;/h3&gt;
&lt;p&gt;这部分主要是改进YOLO的两个缺点：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;定位不精确&lt;/li&gt;
&lt;li&gt;召回率较低（和基于候选区域的方法相比）&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;YOLO V2种并没有加深或者加宽网络结构，反而简化了网络（faster）。&lt;/p&gt;
&lt;ul readability=&quot;1&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Batch Normalization&lt;br/&gt;这个是CNN网络通用的方法了，不但能够改善网络的收敛性，而且能够抑制过拟合，有正则化的作用。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;High Resolution Classifier&lt;br/&gt;相比图像的分类任务，目标检测需要更高的图像分辨率。而为了提取图像的特征，目标检测网络的提取特征部分，通常要在ImageNet数据集上进行预训练。从AlexNet结构开始，大多数分类的网络的输入图像都小于&lt;span class=&quot;math inline&quot;&gt;\(256 \times 256\)&lt;/span&gt;，在YOLO中，使用&lt;span class=&quot;math inline&quot;&gt;\(224 \times 224\)&lt;/span&gt;的图像进行预训练，但是在目标检测的网络中使用&lt;span class=&quot;math inline&quot;&gt;\(448 \times 448\)&lt;/span&gt;的图像进行训练。这样就意味着，从用于分类的特征提取模型切换到目标检测网络，还需要适应这种图像分辨率的改变。&lt;br/&gt;在YOLO V2中对此进行了改变了，使用ImageNet数据集，首先使用&lt;span class=&quot;math inline&quot;&gt;\(224 \times 224\)&lt;/span&gt;的分辨率训练160个epochs，然后调整为&lt;span class=&quot;math inline&quot;&gt;\(448 \times 448\)&lt;/span&gt;在训练10个epochs。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;convolutional-with-anchor-boxes&quot;&gt;Convolutional With Anchor Boxes&lt;/h4&gt;
&lt;p&gt;在YOLO中在最后网络的全连接层直接预测目标边框的坐标，在YOLO V2中借鉴 Fast R-CNN中的Anchor的思想。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;去掉了YOLO网络的全连接层和最后的池化层，使提取特征的网络能够得到更高分辨率的特征。&lt;/li&gt;
&lt;li&gt;使用&lt;span class=&quot;math inline&quot;&gt;\(416 \times 416\)&lt;/span&gt;代替&lt;span class=&quot;math inline&quot;&gt;\(448 \times 448\)&lt;/span&gt;作为网络的输入。这是因为希望得到的特征图的尺寸为奇数。奇数大小的宽和高会使得每个特征图在划分cell的时候就只有一个center cell（比如可以划分成7&lt;em&gt;7或9&lt;/em&gt;9个cell，center cell只有一个，如果划分成8&lt;em&gt;8或10&lt;/em&gt;10的，center cell就有4个）。为什么希望只有一个center cell呢？因为大的object一般会占据图像的中心，所以希望用一个center cell去预测，而不是4个center cell去预测。网络最终将416&lt;em&gt;416的输入变成13&lt;/em&gt;13大小的feature map输出，也就是缩小比例为32。（5个池化层，每个池化层将输入的尺寸缩小1/2）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Anchor Boxes&lt;/strong&gt; 在YOLO中，每个grid cell只预测两个bbox，最终只能预测98个bbox（&lt;span class=&quot;math inline&quot;&gt;\(7\times 7\times 2=98\)&lt;/span&gt;），而在Faster RCNN在输入大小为&lt;span class=&quot;math inline&quot;&gt;\(1000\times 600\)&lt;/span&gt;时的boxes数量大概是6000，在SSD300中boxes数量是8732。显然增加box数量是为了提高object的定位准确率。 过少的bbox显然影响了YOLO的定位的精度，在YOLO V2中引入了Anchor Boxes的思想，其预测的bbox则会超过千个（以输出的feature map为&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13\)&lt;/span&gt;为例，每个grid cell有9个anchor box的话，其预测的bbox数量为&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 9 = 1521\)&lt;/span&gt;个）。&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;dimension-clusters&quot;&gt;Dimension Clusters&lt;/h4&gt;
&lt;p&gt;YOLO V2中引入了Faster R-CNN思想，但是让大佬单纯的使用别人的想法而不加以改进是不可能的。在Faster R-CNN中每个Anchor box的大小以及形状是预先设定好的，然后在网络种通过边框回归来调整每个Anchor Box的边框。但是，如果开始就选择好的边框（Faster R-CNN中的边框是手工设计的，3种大小，3种形状共9种），那么网络肯定能更好的预测。&lt;br/&gt;YOLO 作者使用据类的思想，对训练集能够生成的所有Anchor box做聚类，以此来找到合适的预设的Anchor box.另外作者发现如果采用标准的k-means（即用欧式距离来衡量差异），在box的尺寸比较大的时候其误差也更大，而我们希望的是误差和box的尺寸没有太大关系。所以通过IOU定义了如下的距离函数，使得误差和box的大小无关,故使用如下的距离度量&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ d ( \text { box, centroid } ) = 1 - \text { IOU } ( \text { box, centroid } ) \]&lt;/span&gt;&lt;br/&gt;也就是针对同一个grid cell，其将IOU相近的聚到一起，如下图&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015711769-138072207.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;左边表示选择不同聚类中心的&lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt;和平均IOU的关系，不同曲线表示两种数据集：2007 VOC 和COCO。 YOLO V2选择了&lt;span class=&quot;math inline&quot;&gt;\(K=2\)&lt;/span&gt;，在模型的复杂度和召回率之间做个平衡。 右边5种紫框和黑色的边框表示两种数据集下，最终聚类选择的5中bbox的形状和大小，从图看出两种数据集的形状类似大小相近。图中也可以看出，一个的大的bbox差不多是正方形，另外3个是高瘦的形状，最后一个则是矮胖的形状，这和Faster R-CNN手动选择的9种形状还是有所不同的。&lt;/p&gt;
&lt;h4 id=&quot;direct-location-prediction&quot;&gt;Direct location prediction&lt;/h4&gt;
&lt;p&gt;解决了每个Grid Cell生成的bounding box的个数问题，直接按照Faster R-CNN的方法，又遇到了第二个问题：模型不稳定，特别是在早期的迭代中，而这种不稳定是由于预测box的位置&lt;span class=&quot;math inline&quot;&gt;\((x,y)\)&lt;/span&gt;引起的。在区域推荐的方法中，其网络学习的结果&lt;span class=&quot;math inline&quot;&gt;\((tx,ty)\)&lt;/span&gt;bbox的中心位置相对于ground truth的中尉&lt;span class=&quot;math inline&quot;&gt;\((x,y)\)&lt;/span&gt;的平移量，如候选区域的bbox的中心为&lt;span class=&quot;math inline&quot;&gt;\((x_p,y_p)\)&lt;/span&gt;，宽和高为&lt;span class=&quot;math inline&quot;&gt;\((w_p,h_p)\)&lt;/span&gt;，则有如下的等式&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ x = x_p + w_p * tx \\ y = y_p + h_p * ty \]&lt;/span&gt;&lt;br/&gt;这种位置的平移是没有任何限制的,例如，&lt;span class=&quot;math inline&quot;&gt;\(t_x = 1\)&lt;/span&gt;,则将bbox在&lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;轴向右移动&lt;span class=&quot;math inline&quot;&gt;\(w_p\)&lt;/span&gt;；&lt;span class=&quot;math inline&quot;&gt;\(t_x = -1\)&lt;/span&gt;则将其向左移动&lt;span class=&quot;math inline&quot;&gt;\(w_p\)&lt;/span&gt;。也是说，不管初始的bbox在图像的什么位置，通过预测偏移量可以将bbox移动到图像的任何位置。对于YOLO V2这种随机初始化bbox的位置，需要训练很久的一段时间才能学习到平移量的合适的值。&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;基于候选区域的R-CNN 其初始的bbox并不是随机的，而是通过RPN网络生成的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;YOLO V2中没有使用候选区域的直接预测偏移量，而是沿用YOLO的方法，预测位置相对于当前grid cell的偏移量。YOLO V2网络最后输出的特征层为&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13\)&lt;/span&gt;，然后每个cell生成5个bbox，针对每个bbox得到5个值&lt;span class=&quot;math inline&quot;&gt;\((t_x,t_y,t_w,t_h,t_o)\)&lt;/span&gt;，&lt;span class=&quot;math inline&quot;&gt;\((t_x,t_y)\)&lt;/span&gt;表示bbox中心相对于grid cell左上角的偏移，并且将其平移量限制在一个grid cell内，使用&lt;span class=&quot;math inline&quot;&gt;\(sigmoid\)&lt;/span&gt;函数处理处理偏移值，将其限制在&lt;span class=&quot;math inline&quot;&gt;\((0,1)\)&lt;/span&gt;范围内（每个grid cell的尺度看做1）。所以得到下面的公式&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{aligned} b _ { x } &amp;amp; = \sigma \left( t _ { x } \right) + c _ { x } \\ b _ { y } &amp;amp; = \sigma \left( t _ { y } \right) + c _ { y } \\ b _ { w } &amp;amp; = p _ { w } e ^ { t _ { w } } \\ b _ { h } &amp;amp; = p _ { h } e ^ { t _ { h } } \end{aligned} \]&lt;/span&gt;&lt;br/&gt;其中，&lt;span class=&quot;math inline&quot;&gt;\((C_x,C_y)\)&lt;/span&gt;为当前grid cell相对于图像的左上角的距离，以grid cell的个数为单位。&lt;span class=&quot;math inline&quot;&gt;\(p_w,p_h\)&lt;/span&gt;为为先验框的宽和高。&lt;/p&gt;
&lt;p&gt;如下图，&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015729590-535942363.png&quot;/&gt;&lt;br/&gt;&lt;span class=&quot;math inline&quot;&gt;\((C_x,C_y)\)&lt;/span&gt;为当前grid cell相对于图像的左上角的距离，以grid cell为单位，则当前cell的左上角坐标为&lt;span class=&quot;math inline&quot;&gt;\((1,1)\)&lt;/span&gt;;&lt;span class=&quot;math inline&quot;&gt;\(p_w,p_h\)&lt;/span&gt;为为先验框的宽和高,其值也是相对于特征图的大小，在特征都中每个cell的大小为1。这里记特征图的大小为&lt;span class=&quot;math inline&quot;&gt;\((W,H)\)&lt;/span&gt;（YOLO V2为&lt;span class=&quot;math inline&quot;&gt;\((13,13)\)&lt;/span&gt;），这样就可以将边框相对于图像的大小和位置计算出来&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{aligned} b _ { x } &amp;amp; = (\sigma \left( t _ { x } \right) + c _ { x })/W \\ b _ { y } &amp;amp; = (\sigma \left( t _ { y } \right) + c _ { y })/H \\ b _ { w } &amp;amp; = p _ { w } e ^ { t _ { w } } / W\\ b _ { h } &amp;amp; = p _ { h } e ^ { t _ { h } }/H \end{aligned} \]&lt;/span&gt;&lt;br/&gt;在将上面得到的&lt;span class=&quot;math inline&quot;&gt;\(b_x,b_y,b_w,b_H\)&lt;/span&gt;乘以图像的宽和高（像素为单位）就可以求得边框在图像的位置。&lt;/p&gt;
&lt;p&gt;例如，假如预测输出的值&lt;span class=&quot;math inline&quot;&gt;\((t_x,t_y,t_w,t_h) = (0.2,0.1,0.2,0.32)\)&lt;/span&gt;；当前cell的相对于特征图左上角的坐标为&lt;span class=&quot;math inline&quot;&gt;\((1,1)\)&lt;/span&gt;，Anchor box预设的宽和高为&lt;span class=&quot;math inline&quot;&gt;\(p_w = 3.19275,p_h = 4.00944\)&lt;/span&gt;，则有&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{align*} b_x &amp;amp;= 0.2 + 1 = 1.2 \\ b_y &amp;amp;= 0.1 + 1 = 1.1 \\ b_w &amp;amp;= 3.19275 * e ^ {0.2} = 3.89963 \\ b_h &amp;amp;= 4.00944 * e ^ {0.32} = 5.52151 \end{align*} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;上面的计算的距离都是相对于&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13\)&lt;/span&gt;的特征图而言的，其单位为grid cell的边长。 YOLO V2输入的图像尺寸为&lt;span class=&quot;math inline&quot;&gt;\(416 \times 416\)&lt;/span&gt;，则每个grid cell的边长为&lt;span class=&quot;math inline&quot;&gt;\(416 / 13 = 32\)&lt;/span&gt;，将上述位置换算到以像素为单位&lt;br/&gt;&lt;span class=&quot;math display&quot;&gt;\[ \begin{align*} b_x &amp;amp;= 1.2 * 32 = 38.4 \\ b_y &amp;amp;= 1.1 * 32 = 35.2 \\ b_w &amp;amp;= 3.89963 * 32 = 124.78 \\ b_h &amp;amp;= 5.52151 * 32 = 176.68 \end{align*} \]&lt;/span&gt;&lt;br/&gt;这样就得到了一个在原图上以&lt;span class=&quot;math inline&quot;&gt;\((38.4,35.2)\)&lt;/span&gt;为中心，宽高为&lt;span class=&quot;math inline&quot;&gt;\((124.78,176.68)\)&lt;/span&gt;的边框。&lt;/p&gt;
&lt;h4 id=&quot;fine-grained-features&quot;&gt;Fine-Grained Features&lt;/h4&gt;
&lt;p&gt;YOLO V2是在&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13\)&lt;/span&gt;的特征图上做检测，这对于一些大的目标是足够了，但是对于小目标则需要更写细粒度的特征。 Faser R-CNN和SSD都在不同层次的特征图上产生区域建议（SSD直接就可看得出来这一点），获得了多尺度的适应性，YOLO V2则使用了一种不同的方法，添加要给转移层(passthrough layer)，该层将浅层的特征图(&lt;span class=&quot;math inline&quot;&gt;\(26 \times 26\)&lt;/span&gt;)连接到最终使用的深层特征度(#13 \times 13$)。&lt;/p&gt;
&lt;p&gt;这个转移层有点类似ResNet的dentity mappings结构，将浅层和深层两种不同尺寸的特征连接起来，将&lt;span class=&quot;math inline&quot;&gt;\(26 \times 26 \times 512\)&lt;/span&gt;的特征图和&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 1024\)&lt;/span&gt;的特征图连接起来。passthrough layer，具体来说就是特征重排（不涉及到参数学习），&lt;span class=&quot;math inline&quot;&gt;\(26 \times 26 \times 512\)&lt;/span&gt;的特征使用按行和按列隔行采样的方法，就可以得到4个新的特征图，维度都是&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 512\)&lt;/span&gt;的特征，然后做concat操作，得到&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 2048\)&lt;/span&gt;的特征图，将其拼接到后面&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times1024\)&lt;/span&gt;得到&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 3072\)&lt;/span&gt;的特征图，相当于做了一次特征融合，有利于检测小目标。下图是passthrough layer的一个实例&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015752770-403594012.jpg&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;multi-scale-training&quot;&gt;Multi-Scale Training&lt;/h4&gt;
&lt;p&gt;YOLO中使用&lt;span class=&quot;math inline&quot;&gt;\(448\times448\)&lt;/span&gt;作为输入，而且由于使用了全连接层，无法改变输入的图像的大小；在 YOLO V2中将全连接层替换为了卷积层，也就是说只有卷积层和池化层，这样就可以处理任意尺寸的图像。为了应对不同尺寸的图像，YOLO V2中在训练的时候使用不同的尺寸图像。&lt;/p&gt;
&lt;p&gt;YOLO V2在训练的时候每经过几轮（每经过10epochs）迭代后就会微调网络，随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度&lt;span class=&quot;math inline&quot;&gt;\(\{320,352,\cdots，608\}\)&lt;/span&gt;。最终最小的尺寸为&lt;span class=&quot;math inline&quot;&gt;\(320 \times 320\)&lt;/span&gt;，最大的尺寸为&lt;span class=&quot;math inline&quot;&gt;\(608 \times 608\)&lt;/span&gt;。&lt;/p&gt;
&lt;h4 id=&quot;summary&quot;&gt;Summary&lt;/h4&gt;
&lt;p&gt;YOLO V2针对YOLO定位不准确以及召回率低的问题，进行一些改变。 主要是借鉴Faster R-CNN的思想，引入了Anchor box。并且使用k-means的方法，通过聚类得到每个Anchor应该生成的Anchor box的的大小和形状。为了是提取到的特征有更细的粒度，其网络中借鉴ResNet的思想，将浅层的高分辨率特征和深层的特征进行了融合，这样能够更好的检测小的目标。 最后，由于YOLO V2的网络是全卷积网络，能够处理任意尺寸的图像，在训练的时候使用不同尺度的图像，以应对图像尺寸的变换。&lt;/p&gt;
&lt;h3 id=&quot;faster&quot;&gt;Faster&lt;/h3&gt;
&lt;p&gt;大多数检测网络有赖于VGG-16作为特征提取部分，VGG-16的确是一个强大而准确的分类网络，相应的其计算量也是巨大的。 YOLO V2中使用基于GoogLeNet的网络结构Darknet-19，在损失一些精度的情况下，大大的提高运算速度。&lt;/p&gt;
&lt;p&gt;Darknet-19作为YOLO V2的特征提取网络，参考了一些其他的网络结构的经验&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;VGG，使用了较多的&lt;span class=&quot;math inline&quot;&gt;\(3\times3\)&lt;/span&gt;卷积核，在每一次池化操作后把通道数翻倍。&lt;/li&gt;
&lt;li&gt;GoogLeNet的network in network的思想，网络使用了全局平均池化（global average pooling），把&lt;span class=&quot;math inline&quot;&gt;\(1\times1\)&lt;/span&gt;的卷积核置于&lt;span class=&quot;math inline&quot;&gt;\(3\times3\)&lt;/span&gt;的卷积核之间，用来压缩特征。&lt;/li&gt;
&lt;li&gt;使用batch normalization稳定模型训练，抑制过拟合&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;最终得出的基础模型就是Darknet-19，如下图，其包含19个卷积层、5个最大值池化层（maxpooling layers ），下图展示网络具体结构。Darknet-19在ImageNet图片分类top-1准确率72.9%，top-5准确率91.2%&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306020000001-1900743594.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;上述的网络结构是用于ImageNet的预训练网络，其输入的是&lt;span class=&quot;math inline&quot;&gt;\(224\times224\)&lt;/span&gt;（最后几轮调整为&lt;span class=&quot;math inline&quot;&gt;\(448 \times 448\)&lt;/span&gt;）。在ImageNet预训练完成后，需要调整上面的网络结构：去掉最后的卷积层，添加三个&lt;span class=&quot;math inline&quot;&gt;\(3 \times 3 \times 1024\)&lt;/span&gt;的卷积层，且在这三个卷积层的每个后面添加&lt;span class=&quot;math inline&quot;&gt;\(1 \times 1\)&lt;/span&gt;的卷积层。 在检测的时，输入的是&lt;span class=&quot;math inline&quot;&gt;\(416 \times 416\)&lt;/span&gt;，通过了5个池化层的降维，在最后的卷积层 输出的特征为&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 1024\)&lt;/span&gt;。 前面提到，为了得到更细粒度的特征，添加了passthrough layer,将浅层的&lt;span class=&quot;math inline&quot;&gt;\(26 \times 26 \times 512\)&lt;/span&gt;（是输入到最后一个池化层前的特征）融合到最终输出的&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 1024\)&lt;/span&gt;，作为最终用于检测的特征&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \3072\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;用于的检测的cell有$13 \times 13 &lt;span class=&quot;math inline&quot;&gt;\(，每个cell要生成的5个bbox，每个bbox需要预测其位置和置信度\)&lt;/span&gt;(t_x,t_y,t_w,t_h,t_0)$以及其每个类别的概率20个，所以最终输出为&lt;span class=&quot;math inline&quot;&gt;\(13 \times 13 \times 5 \times (5 + 20) = 13 \times 13 \times 125\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;stroner-yolo9000&quot;&gt;Stroner YOLO9000&lt;/h3&gt;
&lt;p&gt;YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。&lt;/p&gt;
&lt;h3 id=&quot;summary-1&quot;&gt;summary&lt;/h3&gt;
&lt;p&gt;YOLO V2在YOLO主要的改动就是，引入了Anchor box以及修改了其用于特征提取的网络，在检测时去掉了全连接层，整个网络全部使用卷积层。&lt;/p&gt;
&lt;h2 id=&quot;yolo-v3&quot;&gt;YOLO V3&lt;/h2&gt;
&lt;p&gt;YOLO作者对 YOLO V2做了一些小的改动。主要以下两个方面:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;使用残差模型,构建更深的特征提取网络&lt;/li&gt;
&lt;li&gt;使用FPN架构（Feature Pyramid Networks for Object Detection）来实现多尺度检测&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;darkent-53&quot;&gt;Darkent-53&lt;/h3&gt;
&lt;p&gt;YOLO V3特征提取网络使用了残差模型，相比YOLO V2使用的Darknet-19，其包含53个卷积层所以称为Darknet-53.其网络结构如下图&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015909934-359286925.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Darknet-53在ImageNet数据集上的性能&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015920517-1479444016.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;和ResNet相比，其速度快了很多，精度却相差不大。&lt;/p&gt;
&lt;h3 id=&quot;多尺度预测&quot;&gt;多尺度预测&lt;/h3&gt;
&lt;p&gt;采用FPN架构（Feature Pyramid Networks for Object Detection）来实现多尺度检测，如下图&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/439761/201903/439761-20190306015937405-1413676333.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;YOLOv3采用了3个尺度的特征图（当输入为&lt;span class=&quot;math inline&quot;&gt;\(416 \times 416\)&lt;/span&gt;时）:&lt;span class=&quot;math inline&quot;&gt;\((13\times 13)，(26\times 26),(52\times 52)\)&lt;/span&gt;,YOLOv3每个位置使用3个先验框，所以使用k-means得到9个先验框，并将其划分到3个尺度特征图上，尺度更大的特征图使用更小的先验框。&lt;/p&gt;
&lt;h2 id=&quot;summary-2&quot;&gt;summary&lt;/h2&gt;
&lt;p&gt;大体学习了下YOLO系列的目标检测，但是其中的一些细节还不是很明了，需要结合代码实现了。&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 18:00:00 +0000</pubDate>
<dc:creator>Brook_icv</dc:creator>
<og:description>YOLO V2 YOLO V2是在YOLO的基础上，融合了其他一些网络结构的特性（比如：Faster R CNN的Anchor,GooLeNet的$1\times1$卷积核等），进行的升级。其目的是弥</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/wangguchangqing/p/10480995.html</dc:identifier>
</item>
<item>
<title>Laravel 和 Spring Boot 两个框架比较创业篇（一：开发效率） - 曾俊杰的专栏</title>
<link>http://www.cnblogs.com/ymstars/p/10480934.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/ymstars/p/10480934.html</guid>
<description>&lt;p&gt;我个人是比较不喜欢去正儿八经的比较两个框架的，这样没有意义，不过欲善其事先利其器！&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;技术是相通的，但是在某个特定的领域的某个阶段肯定有相对最适合的一个工具！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里比较不是从技术角度比较，而是从公司技术选型考虑的，特别是初创的互联网创业公司。没办法，谁让互联网公司离不开软件呢！哈哈哈。&lt;/p&gt;
&lt;p&gt;首先是双方选手出场介绍：&lt;/p&gt;
&lt;h2 id=&quot;laravel&quot;&gt;Laravel&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-gold-cdn.xitu.io/2019/3/5/1694e5eafba9440d?w=916&amp;amp;h=732&amp;amp;f=png&amp;amp;s=48905&quot;/&gt;&lt;br/&gt;Laravel框架号称是Web艺术家的框架，富有生产力，代表了最优雅最流行的PHP框架，经过一段时间的使用，也上了一个项目，感觉特点如下：&lt;/p&gt;
&lt;p&gt;好啦，介绍完选手，就开始来分析一下该用哪个啦，这里我们设定一个情境：&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;假设 小红 是一位有一个自认为价值 20亿 的Idea，并且打算付诸实践的小BOSS（即将成为），稍懂软件架构和开发技术，没错，是很菜的那种（如果很厉害那随便怎么用框架了，没所谓），且启动资金只有 30万。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我也不想假设的这么惨的，现实中这种情况很多，那我们就以这种情景展开分析。小红要以最低成本、最快速度推出 1.0 版本，投放市场，收集反馈，持续迭代。这是一个系统工程，讲其他因素剔除，只考虑技术问题，可以总结成以下几点：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;成本（开发效率和人工成本）&lt;/li&gt;
&lt;li&gt;响应（迭代和部署效率）&lt;/li&gt;
&lt;li&gt;安全（稳定性和 BUG解决速度）&lt;/li&gt;
&lt;li&gt;协作（团队协作和扩展性）&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;开发效率&quot;&gt;1.开发效率&lt;/h2&gt;
&lt;p&gt;开发这个过程，我们将它定义为需求和原型都已经确定，并且已经简单建模完毕，嗯，就是猿们到岗后拿着需求文档打开电脑（Windows）的时候开始，到 1.0 版本发布这段时间，是谁跑得快！O(∩_∩)O哈哈~&lt;/p&gt;
&lt;p&gt;首先是 Laravel 框架，步骤是这样的：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;配置本地环境：包括PHP-CLI、Vagrant 、VirtualBox、HomeStead Box、Composer、nodejs（Mix要用到）、Python、Virtual Studio、Node-gyp（Node-Sass要用到）、PHPStorm、Git，一切就绪后&lt;code&gt;composer create-project laravel/laravel xxx&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;开发：定义migration、model，然后transformer和repository，再写service和passport啥的，再写controller，view视图，然后完善 Event、Notification、推送啥的，期间伴随着单元测试&lt;/li&gt;
&lt;li&gt;部署：Git push、Git Clone 、Pull，env整一个，上线&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;对 Laravel 的开发流程熟悉的人呢，开发速度是很快的。&lt;/p&gt;
&lt;p&gt;我们再来看看Spring Boot：&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;业务不复杂就不要折腾微服务啦，不要像某人一样明明只有一台机器，硬是要开几十个端口，然后跑几十个Spring Boot的小服务，还用Cloud全家桶串起来了。我竟无言以对&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;单体应用撸起来，步骤如下：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;配置开发环境：IntelliJ IDEA下一个、JDK装一个、其他要用到的Redis啥的装上，分分钟就搞定可以开撸了。&lt;/li&gt;
&lt;li&gt;开发：定义JAP Entity，Repository、Service，配置Spring Security（包括Oauth2），定义Validation，开撸Controller、异常处理，视图层啥的，单元测试也少不了&lt;/li&gt;
&lt;li&gt;部署：打出Jar包，扔到服务器上执行吧，nginx映射一下，搞定&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;我个人觉得Spring Boot的开发效率要比 Laravel 框架高些！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为什么呢? 因为如果对 Spring 的机制熟悉，也了解 Security、JPA、Thymeleaf模板、RabbitMQ 等等功能模块的使用，Spring Boot 的封装是比 Laravel 要好的，但前提是对Spring 那一套熟悉，不然从何入手都弄不清楚。&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;spring-有些组件是非常复杂的例如-spring-security&quot;&gt;Spring 有些组件是非常复杂的，例如 Spring Security&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;Laravel 框架借鉴了很多 Java Spring 的思想，比如容器，依赖注入、切面，这方面明显 Spring Boot 是正宗，注解啥的6得飞起！&lt;/p&gt;
&lt;p&gt;Java 语言非常严谨，在开发过程中的体验比较好，至少像我这样天马行空的猿，还非得要 Java 这个老头来管着，不然分分钟要跑偏。&lt;/p&gt;
&lt;p&gt;回到开发效率这个问题上，如果对两个框架都比较熟悉的情况下，Spring Boot 是开发比较快的，但 Laravel在某些方面是完胜Spring Boot，如下：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Laravel 框架的 ORM 构建需要经历两个步骤，migration 和 model ，而且改动 migration 需要调整 model，无法向 JPA 一样Entity 即数据库结构；&lt;/li&gt;
&lt;li&gt;Laravel 框架需要手动实现一些注入绑定，通常是&lt;code&gt;$app-&amp;gt;bind&lt;/code&gt;，尽管这不消耗多少时间，但是比起Spring强大的注解还是慢不少，而且主流IDE对 Spring 的 Bean 提供了导航查看功能，牛逼哄哄啊；&lt;/li&gt;
&lt;li&gt;如果要做网页渲染，Laravel的动态脚本语言特性加上Blade模板基本是秒杀Spring Boot 的；&lt;/li&gt;
&lt;li&gt;要让层次更分明一些的话，Laravel 需要手动实现Repository 模式，反正我是受不了Model 直接定义业务逻辑的，放在Controller里也受不了，不但难看，还不好扩展；&lt;/li&gt;
&lt;li&gt;在授权这方面，Laravel 自带的和Spring Security 都很强大，可以说是开箱即用，打平；&lt;/li&gt;
&lt;li&gt;Laravel框架开发反馈调试方面是完胜Spring Boot的，这方面可以说所有非编译型的语言都很爽！尽管Spring Boot 也有DevTool，但是架不住 PHP 根本就不需要重新启动呀。&lt;/li&gt;
&lt;li&gt;Laravel框架的代码提示远远比不上Spring Boot，而且还需要第三方包Ide-Helper的加持，不然代码追踪都不行，可是就算用了第三方包还是看不了 容器内长啥样啊；&lt;/li&gt;
&lt;li&gt;像 Laravel 这样靠面向对象体现优雅的框架，却遇到了PHP 这门面向对象不太完全的语言，以致于在 Java 体系内很容易实现的一个功能，到了PHP体系却无能为力；&lt;/li&gt;
&lt;li&gt;Route 路由这方面 Laravel 非常强大，而且直观，比Spring Boot 灵活，所以定义路由的时候效率完爆Spring Boot；&lt;/li&gt;
&lt;li&gt;异常处理两者都非常方便，提供了统一处理的方式，难分伯仲；&lt;/li&gt;
&lt;li&gt;Api Json数据定制这方面，Laravel 比 Spring Boot 要强大，这是因为PHP的数组操作非常灵活，对于 Java 来说需要定义工具类和实体类来专门处理；&lt;/li&gt;
&lt;li&gt;i18n国际化，Laravel 比Spring Boot 方便；&lt;/li&gt;
&lt;li&gt;前端资源处理，就这个功能本身来说，Laravel的Mix配合Blade模板完爆Spring Boot，但是话说回来，只要不是全栈，这不算什么优势。设想一下如果是前端做好页面，拿到后端套模板，那Thymeleaf 完爆 Blade，因为Thymeleaf 可以保留预览数据，渲染实际数据，Blade 做不到这一点。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;总结：在技能掌握充足的情况下，个人感觉 Spring Boot 开发效率要略高于Laravel。个人掌握情况不一样，请勿喷，可以参考文中的几个维度，自己思考一下。&lt;/p&gt;
&lt;p&gt;最后想提一下，顺便求证：&lt;/p&gt;
&lt;blockquote&gt;
&lt;h3 id=&quot;laravel-不念-拉瓦&quot;&gt;Laravel 不念 “拉瓦”&lt;/h3&gt;
&lt;h3 id=&quot;laravel-不念-拉瓦-1&quot;&gt;Laravel 不念 “拉瓦”&lt;/h3&gt;
&lt;h3 id=&quot;laravel-不念-拉瓦-2&quot;&gt;Laravel 不念 “拉瓦”&lt;/h3&gt;
&lt;/blockquote&gt;
&lt;p&gt;时候不早了，有点困。今天就写到这，明天再写人工成本的考量。&lt;/p&gt;
&lt;p&gt;大家晚安！谢谢&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 17:05:00 +0000</pubDate>
<dc:creator>曾俊杰的专栏</dc:creator>
<og:description>我个人是比较不喜欢去正儿八经的比较两个框架的，这样没有意义，不过欲善其事先利其器！ 技术是相通的，但是在某个特定的领域的某个阶段肯定有相对最适合的一个工具！ 这里比较不是从技术角度比较，而是从公司技术</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/ymstars/p/10480934.html</dc:identifier>
</item>
<item>
<title>.Net  Core 自定义配置源从配置中心读取配置 - Agile.Zhou</title>
<link>http://www.cnblogs.com/kklldog/p/configruation_source.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/kklldog/p/configruation_source.html</guid>
<description>&lt;p&gt;配置，几乎所有的应用程序都离不开它。.Net Framework时代我们使用App.config、Web.config，到了.Net Core的时代我们使用appsettings.json，这些我们再熟悉不过了。然而到了容器化、微服务的时代，这些本地文件配置有的时候就不太合适了。当你把本地部署的服务搬到docker上后，你会发现要修改一个配置文件变的非常麻烦。你不得不通过宿主机进入容器内部来修改文件，也许容器内还不带vi等编辑工具，你连看都不能看，改都不能。更别说当你启动多个容器实例来做分布式应用的时候，一个个去修改容器的配置，这简直要命了。&lt;br/&gt;因为这些原因，所以“配置中心”就诞生了。配置中心是微服务的基础设施，它对配置进行集中的管理并对外暴露接口，当应用程序需要的时候通过接口读取。配置通常为Key/Value模式，然后通过http接口暴露。好了，配置中心不多说了，感觉要偏了，这次是介绍怎么自定义一个配置源从配置中心读取配置。废话不多说直接上代码吧。&lt;/p&gt;
&lt;h2 id=&quot;模拟配置中心&quot;&gt;模拟配置中心&lt;/h2&gt;
&lt;p&gt;我们新建一个asp.net core webapi站点来模拟配置中心服务，端口配置到5000，并添加相应的controller来模拟配置中心对外的接口。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;     [Route(&quot;api/[controller]&quot;)]
    [ApiController]
    public class ConfigsController : ControllerBase
    {
        public List&amp;lt;KeyValuePair&amp;lt;string,string&amp;gt;&amp;gt; Get()
        {
            var configs = new List&amp;lt;KeyValuePair&amp;lt;string, string&amp;gt;&amp;gt;();
            configs.Add(new KeyValuePair&amp;lt;string, string&amp;gt;(&quot;SecretKey&quot;,&quot;1238918290381923&quot;));
            configs.Add(new KeyValuePair&amp;lt;string, string&amp;gt;(&quot;ConnectionString&quot;, &quot;user=123;password=123;server=.&quot;));

            return configs;
        }
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;添加一个configscontroller，并修改Get方法，返回2个配置键值对。&lt;/em&gt;&lt;br/&gt;&lt;img src=&quot;https://images.cnblogs.com/cnblogs_com/kklldog/1401672/o_%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190305230201.png&quot;/&gt;&lt;br/&gt;&lt;em&gt;访问下/api/configs看下返回是否正确&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;自定义配置源&quot;&gt;自定义配置源&lt;/h2&gt;
&lt;p&gt;从现在开始我们真正开始来定义一个自定义的配置源然后当程序启动的时候从配置中心读取配置文件信息，并提供给后面的代码使用配置。&lt;br/&gt;新建一个asp.net core mvc站点来模拟客户端程序。&lt;/p&gt;
&lt;h3 id=&quot;myconfigprovider&quot;&gt;MyConfigProvider&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;  public class MyConfigProvider : ConfigurationProvider
    {
        /// &amp;lt;summary&amp;gt;
        /// 尝试从远程配置中心读取配置信息
        /// &amp;lt;/summary&amp;gt;
        public async override void Load()
        {
            var response = &quot;&quot;;
            try
            {
                var serverAddress = &quot;http://localhost:5000&quot;;
                var client = new HttpClient();
                client.BaseAddress = new Uri(serverAddress);
                response = await client.GetStringAsync(&quot;/api/configs&quot;);
            }
            catch (Exception ex)
            {
                //write err log
            }

            if (string.IsNullOrEmpty(response))
            {
                throw new Exception(&quot;Can not request configs from remote config center .&quot;);
            }

            var configs = JsonConvert.DeserializeObject&amp;lt;List&amp;lt;KeyValuePair&amp;lt;string, string&amp;gt;&amp;gt;&amp;gt;(response);

            Data = new ConcurrentDictionary&amp;lt;string, string&amp;gt;();

            configs.ForEach(c =&amp;gt;
            {
                Data.Add(c);
            });
        }
    
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;新建一个MyConfigProvider的类，这个类从ConfigurationProvider继承，并重写其中的Load方法。使用HttpClient从配置中心读取信息后，进行反序列化，并把配置转换为字典。这里注意一下，虽然Data的类型为IDictionary&amp;lt;string,string&amp;gt;，但是这里实例化对象的时候使用了ConcurrentDictionary&amp;lt;string, string&amp;gt;类，因为Dictionary&amp;lt;string,string&amp;gt;是非线程安全的，如果进行多线程读写会出问题。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;myconfigsource&quot;&gt;MyConfigSource&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;   public class MyConfigSource : IConfigurationSource
    {
        public IConfigurationProvider Build(IConfigurationBuilder builder)
        {
            return new MyConfigProvider();
        }
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;新建一个MyConfigSource的类，这个类实现IConfigurationSource接口，IConfigurationSource接口只有一个Build方法，返回值为IConfigurationProvider，我们刚才定义的MyConfigProvider因为继承自ConfigurationProvider所以已经实现了IConfigurationProvider，我们直接new一个MyConfigProvider并返回。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;myconfigbuilderext&quot;&gt;MyConfigBuilderExt&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;   public static class MyConfigBuilderExt
    {
        public static IConfigurationBuilder AddMyConfig(
            this IConfigurationBuilder builder
            )
        {
            return builder.Add(new MyConfigSource());
        }
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;给IConfigurationBuilder定义一个AddMyConfig的扩展方法，跟.Net Core自带的几个配置源使用风格保持一致。当调用AddMyConfig的时候给IConfigurationBuilder实例添加一个MyConfigSource的源。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;使用配置源&quot;&gt;使用配置源&lt;/h2&gt;
&lt;h3 id=&quot;在program中添加myconfigsource&quot;&gt;在Program中添加MyConfigSource&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;     public class Program
    {
        public static void Main(string[] args)
        {
            CreateWebHostBuilder(args).Build().Run();
        }

        public static IWebHostBuilder CreateWebHostBuilder(string[] args) =&amp;gt;
            WebHost.CreateDefaultBuilder(args)
            .ConfigureAppConfiguration((context, configBuiler) =&amp;gt;
            {
                configBuiler.AddMyConfig();
            })
            .UseStartup&amp;lt;Startup&amp;gt;();
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;在ConfigureAppConfiguration的匿名委托方法中调用AddMyConfig扩展方法，这样程序启动的时候会自动使用MyConfigSource源并从配置中心读取配置到本地应用程序。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改homecontroller&quot;&gt;修改HomeController&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;   public class HomeController : Controller
    {
        IConfiguration _configuration;
        public HomeController(IConfiguration configuration)
        {
            _configuration = configuration;
        }

        public IActionResult Index()
        {
            var secretKey = _configuration[&quot;SecretKey&quot;];
            var connectionString = _configuration[&quot;ConnectionString&quot;];

            ViewBag.SecretKey = secretKey;
            ViewBag.ConnectionString = connectionString;

            return View();
        }
      
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;修改homecontroller，把IConfiguration通过构造函数注入进去，在Index Action方法中读取配置，并赋值给ViewBag&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;修改index视图&quot;&gt;修改Index视图&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt; @{
    ViewData[&quot;Title&quot;] = &quot;Test my config&quot;;
}

&amp;lt;h3&amp;gt;
    SecretKey: @ViewBag.SecretKey
&amp;lt;/h3&amp;gt;
&amp;lt;h3&amp;gt;
    ConnectionString: @ViewBag.ConnectionString
&amp;lt;/h3&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;修改Index视图的代码，把配置信息从ViewBag中读取出来并在网页上展示。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;运行一下&quot;&gt;运行一下&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://images.cnblogs.com/cnblogs_com/kklldog/1401672/o_%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20190305234032.png&quot;/&gt;&lt;br/&gt;&lt;em&gt;先运行配置中心站点再运行一下网站，首页出现了我们在配置中心定义的SecretKey跟ConnectionString信息，表示我们的程序成功的从配置中心读取了配置信息。我们的自定义配置源已经能够成功运行了。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;改进&quot;&gt;改进&lt;/h2&gt;
&lt;p&gt;以上配置源虽然能够成功运行，但是仔细看的话显然它有2个比较大的问题。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;配置中心的服务地址是写死在类里的。我们的配置中心很有可能会修改ip或者域名，写死在代码里显然不是高明之举，所以我们还是需要保留本地配置文件，把配置中心的服务地址写到本地配置文件中。&lt;/li&gt;
&lt;li&gt;配置中心作为微服务的基础设施一旦故障会引发非常严重的后果，新启动或者重启的客户端会无法正常启动。如果我们在配置中心正常的时候冗余一份配置在本地，当配置中心故障的时候从本地读取配置，至少可以保证一部分客户端程序能够正常运行。&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;{
  &quot;Logging&quot;: {
    &quot;LogLevel&quot;: {
      &quot;Default&quot;: &quot;Warning&quot;
    }
  },
  &quot;AllowedHosts&quot;: &quot;*&quot;,
  &quot;myconfigServer&quot;: &quot;http://localhost:5000&quot;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;修改本地appsettings.json文件，添加myconfigServer的配置信息。&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; public class MyConfigProvider : ConfigurationProvider
    {
        private string _serverAddress;
        public MyConfigProvider()
        {
            var jsonConfig = new JsonConfigurationSource();
            jsonConfig.FileProvider = new PhysicalFileProvider(Directory.GetCurrentDirectory());
            jsonConfig.Path = &quot;appsettings.json&quot;;
            var jsonProvider = new JsonConfigurationProvider(jsonConfig);
            jsonProvider.Load();

            jsonProvider.TryGet(&quot;myconfigServer&quot;, out string serverAddress);

            if (string.IsNullOrEmpty(serverAddress))
            {
                throw new Exception(&quot;Can not find myconfigServer's address from appsettings.json&quot;);
            }

            _serverAddress = serverAddress;
        }

        /// &amp;lt;summary&amp;gt;
        /// 尝试从远程配置中心读取配置信息，当成功从配置中心读取信息的时候把配置写到本地的myconfig.json文件中，当配置中心无法访问的时候尝试从本地文件恢复配置。
        /// &amp;lt;/summary&amp;gt;
        public async override void Load()
        {
            var response = &quot;&quot;;
            try
            {
                var client = new HttpClient();
                client.BaseAddress = new Uri(_serverAddress);
                response = await client.GetStringAsync(&quot;/api/configs&quot;);

                WriteToLocal(response);
            }
            catch (Exception ex)
            {
                //write err log
                response = ReadFromLocal();
            }

            if (string.IsNullOrEmpty(response))
            {
                throw new Exception(&quot;Can not request configs from remote config center .&quot;);
            }

            var configs = JsonConvert.DeserializeObject&amp;lt;List&amp;lt;KeyValuePair&amp;lt;string, string&amp;gt;&amp;gt;&amp;gt;(response);

            Data = new ConcurrentDictionary&amp;lt;string, string&amp;gt;();

            configs.ForEach(c =&amp;gt;
            {
                Data.Add(c);
            });
        }

        private void WriteToLocal(string resp)
        {
            var file = Directory.GetCurrentDirectory() + &quot;/myconfig.json&quot;;
            File.WriteAllText(file,resp);
        }

        private string ReadFromLocal()
        {
            var file = Directory.GetCurrentDirectory() + &quot;/myconfig.json&quot;;
            return File.ReadAllText(file);
        }
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;修改MyConfigProvider，修改构造函数，通过JsonConfigurationProvider从本地读取appsettings.json中的myconfigServer配置信息。新增WriteToLocal方法把配置中心返回的json数据写到本地文件中。新增ReadFromLocal方法，从本地文件读取json信息。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;再次运行&quot;&gt;再次运行&lt;/h3&gt;
&lt;p&gt;先运行配置中心站点，再运行客户端网站，可以看到配置信息展示到首页界面上。关闭配置中心客跟客户端网站，并且重启客户端网站依然能够展示配置信息，说明自定义配置源当配置中心故障的时候成功从本地文件恢复了配置。图跟上面的图是一致的，就不贴了。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;通过以上我们定义了一个比较简单的自定义配置源，它能够通过http从配置中心读取配置，并且提供了同传统json配置文件一致的使用风格，最大程度的复用旧代码，减少因为引入配置中心而大规模改动代码。我们从上面的代码可以更清楚的知道.Net Core的配置源是如何工作的。ConfigurationSource只是ConfigurationProvider的建造器。真正完成配置加载、查找工作的是ConfigurationProvider。&lt;br/&gt;以上代码还是演示级别的代码，还有很多改进的空间，比如http访问失败的重试，我们可以使用polly重构；比如支持定时从配置中心刷新配置等，有兴趣可以自己去实践一下。&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 16:50:00 +0000</pubDate>
<dc:creator>Agile.Zhou</dc:creator>
<og:description>配置，几乎所有的应用程序都离不开它。.Net Framework时代我们使用App.config、Web.config，到了.Net Core的时代我们使用appsettings.json，这些我们再</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/kklldog/p/configruation_source.html</dc:identifier>
</item>
<item>
<title>Django的rest_framework的认证组件的源码解析 - bainianminguo</title>
<link>http://www.cnblogs.com/bainianminguo/p/10480887.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/bainianminguo/p/10480887.html</guid>
<description>&lt;p&gt;前言：&lt;/p&gt;
&lt;p&gt;　　Django的rest_framework组件的功能很强大，今天来我来给大家剖析一下认证组件&lt;/p&gt;

&lt;p&gt;下面进入正文分析，我们从视图开始，一步一步来剖析认证组件&lt;/p&gt;
&lt;p&gt;1、进入urls文件&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
url(r'^login/', views.LoginCBV.as_view(),name=&quot;login&quot;),
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;2、然后执行LoginCBV这个类的as_view方法&lt;/p&gt;

&lt;p&gt;3、LoginCBV这个类是我们自己的写的，但是LoginCBV类根本没有写as_view这个方法，那么我们该怎么办？ 此时我们应该去找LoginCBV的父类，看父类是否as_view方法&lt;/p&gt;

&lt;p&gt;4、先确认LoginCBV这个类的父类，很清楚，我们看到LoginCBV这个类的父类是APIView这个类&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
class LoginCBV(APIView):
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;5、下面我们去APIView这个类中查找是否有as_view这个方法，我们在APIView这个类中找到了as_view方法，这个被classmethod修饰符修饰，也就是说这个方法是一个类方法，由一个类本身就可以调用这个方法，这个时候大家在会议一下，在urls文件中，是不是一个类在调用as_view方法。&lt;/p&gt;
&lt;p&gt;如果大家都classmethod这个修饰符不清楚，可以看下我的这篇博客：https://www.cnblogs.com/bainianminguo/p/10475204.html&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    @classmethod
    def as_view(cls, **initkwargs):
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;6、下面我们来具体看下as_view这个方法，到底做了什么事情？下面是方法的源码&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;42&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    @classmethod
    def as_view(cls, **initkwargs):
        &quot;&quot;&quot;
        Store the original class on the view function.

        This allows us to discover information about the view when we do URL
        reverse lookups.  Used for breadcrumb generation.
        &quot;&quot;&quot;
        if isinstance(getattr(cls, 'queryset', None), models.query.QuerySet):
            def force_evaluation():
                raise RuntimeError(
                    'Do not evaluate the `.queryset` attribute directly, '
                    'as the result will be cached and reused between requests. '
                    'Use `.all()` or call `.get_queryset()` instead.'
                )
            cls.queryset._fetch_all = force_evaluation

        view = super(APIView, cls).as_view(**initkwargs)
        view.cls = cls
        view.initkwargs = initkwargs

        # Note: session based authentication is explicitly CSRF validated,
        # all other authentication is CSRF exempt.
        return csrf_exempt(view)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;我们来重点看下需要我们知道的，首先这个函数的返回值是一个view方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305230615910-1900494146.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 接着我们看下view这个方法，从这里我们可以看到，view就是执行APIView父类的as_view方法，下面我们接着去找APIView类的父类的as_view方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305230808294-1578345279.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 7、进入APIView父类中，我们看到APIView类的父类是View&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
class APIView(View):
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt; 8、进入View类中，看下as_view这个方法到底了干了什么？&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;49&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    @classonlymethod
    def as_view(cls, **initkwargs):
        &quot;&quot;&quot;
        Main entry point for a request-response process.
        &quot;&quot;&quot;
        for key in initkwargs:
            if key in cls.http_method_names:
                raise TypeError(&quot;You tried to pass in the %s method name as a &quot;
                                &quot;keyword argument to %s(). Don't do that.&quot;
                                % (key, cls.__name__))
            if not hasattr(cls, key):
                raise TypeError(&quot;%s() received an invalid keyword %r. as_view &quot;
                                &quot;only accepts arguments that are already &quot;
                                &quot;attributes of the class.&quot; % (cls.__name__, key))

        def view(request, *args, **kwargs):
            self = cls(**initkwargs)
            if hasattr(self, 'get') and not hasattr(self, 'head'):
                self.head = self.get
            self.request = request
            self.args = args
            self.kwargs = kwargs
            return self.dispatch(request, *args, **kwargs)
        view.view_class = cls
        view.view_initkwargs = initkwargs

        # take name and docstring from class
        update_wrapper(view, cls, updated=())

        # and possible attributes set by decorators
        # like csrf_exempt from dispatch
        update_wrapper(view, cls.dispatch, assigned=())
        return view
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;下面我们来分析这个方法的源码，方法的返回值是view这个函数，而view这个函数的返回值是self.dispatch这个方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305231242619-214561334.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 9、下面我们首先要找到self.dispatch这个方法，然后在看下这个方法到底干了什么？&lt;/p&gt;
&lt;p&gt; 这个self到底是哪个类的实例呢？我们来梳理一下子类和父类的关系&lt;/p&gt;
&lt;p&gt;LoginCBV【类】-------&amp;gt;APIView【类】-------&amp;gt;View【类】------&amp;gt;view【方法】-----》dispatch【方法】&lt;/p&gt;
&lt;p&gt;那么我们就需要先从LoginCBV这个类中找dispatch方法，发现没有找到，然后继续找LoginCBV这个类的父类，也就是APIView这个类，看这个类是否dispatch方法&lt;/p&gt;


&lt;p&gt;10、我们最终在APIView这个类中找到了dispatch方法，所以这里调的dispatch方法一定是APIView这个类的方法&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;52&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def dispatch(self, request, *args, **kwargs):
        &quot;&quot;&quot;
        `.dispatch()` is pretty much the same as Django's regular dispatch,
        but with extra hooks for startup, finalize, and exception handling.
        &quot;&quot;&quot;
        self.args = args
        self.kwargs = kwargs
        request = self.initialize_request(request, *args, **kwargs)
        self.request = request
        self.headers = self.default_response_headers  # deprecate?

        try:
            self.initial(request, *args, **kwargs)

            # Get the appropriate handler method
            if request.method.lower() in self.http_method_names:
                handler = getattr(self, request.method.lower(),
                                  self.http_method_not_allowed)
            else:
                handler = self.http_method_not_allowed

            response = handler(request, *args, **kwargs)

        except Exception as exc:
            response = self.handle_exception(exc)

        self.response = self.finalize_response(request, response, *args, **kwargs)
        return self.response
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这个方法很重要，我们来看下&lt;/p&gt;
&lt;p&gt;首先rest_framework处理后的request&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305232034154-964944260.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 然后看下self.initialize.request方法干了什么，当然找这个方法到底在是哪个类的方法，也是要按照之前我们找dispatch方法的一样，我这里就直接找到这个方法了，self.initialize.request这个方法是APIView这个类的方法&lt;/p&gt;

&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;42&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def initialize_request(self, request, *args, **kwargs):
        &quot;&quot;&quot;
        Returns the initial request object.
        &quot;&quot;&quot;
        parser_context = self.get_parser_context(request)

        return Request(
            request,
            parsers=self.get_parsers(),
            authenticators=self.get_authenticators(),
            negotiator=self.get_content_negotiator(),
            parser_context=parser_context
        )
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;这个函数返回一个Request的实例对象，然后我们在看下Request这个类的，Request类的源码如下&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;50&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
class Request(object):
    &quot;&quot;&quot;
    Wrapper allowing to enhance a standard `HttpRequest` instance.

    Kwargs:
        - request(HttpRequest). The original request instance.
        - parsers_classes(list/tuple). The parsers to use for parsing the
          request content.
        - authentication_classes(list/tuple). The authentications used to try
          authenticating the request's user.
    &quot;&quot;&quot;

    def __init__(self, request, parsers=None, authenticators=None,
                 negotiator=None, parser_context=None):
        assert isinstance(request, HttpRequest), (
            'The `request` argument must be an instance of '
            '`django.http.HttpRequest`, not `{}.{}`.'
            .format(request.__class__.__module__, request.__class__.__name__)
        )

        self._request = request
        self.parsers = parsers or ()
        self.authenticators = authenticators or ()
        self.negotiator = negotiator or self._default_negotiator()
        self.parser_context = parser_context
        self._data = Empty
        self._files = Empty
        self._full_data = Empty
        self._content_type = Empty
        self._stream = Empty

        if self.parser_context is None:
            self.parser_context = {}
        self.parser_context['request'] = self
        self.parser_context['encoding'] = request.encoding or settings.DEFAULT_CHARSET

        force_user = getattr(request, '_force_auth_user', None)
        force_token = getattr(request, '_force_auth_token', None)
        if force_user is not None or force_token is not None:
            forced_auth = ForcedAuthentication(force_user, force_token)
            self.authenticators = (forced_auth,)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305232722086-722697709.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;不知道大家是否明白这段代码的意思，如果authenticators为真，则self.authenticators等于authenticators，如果authenticators为假，则self.authenticators等于一个空的元组&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
       self.authenticators = authenticators or ()
&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt; 我们这里要看下实例化Request这个类的时候，authenticators这个参数传递的是什么？&lt;/p&gt;
&lt;p&gt;我们在回到initlize_request方法的返回值，下面我们要来看下self.get_authenticators()方法是在做什么&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305232908370-328173763.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 下面看下self.get_authenticators()这个方法的源码，从字面的我们就可以理解，self.authentication_classes是一个认证的类的列表。auth（）是每个类的实例对象，这个方法的返回值就是列表，列表中的元素就是每个认证类的实例对象，这里先剧透一下，authentication_class这个属性是由我们自己的配置的&lt;/p&gt;

&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def get_authenticators(self):
        &quot;&quot;&quot;
        Instantiates and returns the list of authenticators that this view can use.
        &quot;&quot;&quot;
        return [auth() for auth in self.authentication_classes]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt; 到这里，APIView类中的dispatch方法的initialize_request条线就做完了，就是给我们返回了一个新的Request类的实例，这个实例的authenticators就包括我们认证组件相关的类的实例对象&lt;/p&gt;

&lt;p&gt;下面我们继续往下走APIView类的dispatch方法，走self.initial方法&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305233656762-216172452.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 11、下面先看下initial方法的源码&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;45&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def initial(self, request, *args, **kwargs):
        &quot;&quot;&quot;
        Runs anything that needs to occur prior to calling the method handler.
        &quot;&quot;&quot;
        self.format_kwarg = self.get_format_suffix(**kwargs)

        # Perform content negotiation and store the accepted info on the request
        neg = self.perform_content_negotiation(request)
        request.accepted_renderer, request.accepted_media_type = neg

        # Determine the API version, if versioning is in use.
        version, scheme = self.determine_version(request, *args, **kwargs)
        request.version, request.versioning_scheme = version, scheme

        # Ensure that the incoming request is permitted
        self.perform_authentication(request)
        self.check_permissions(request)
        self.check_throttles(request)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305233848731-1203784743.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 12、我们这里来看下认证组件干了什么事情？进入认证组件perform_authentication方法。只返回一个request.user&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def perform_authentication(self, request):
        &quot;&quot;&quot;
        Perform authentication on the incoming request.

        Note that if you override this and simply 'pass', then authentication
        will instead be performed lazily, the first time either
        `request.user` or `request.auth` is accessed.
        &quot;&quot;&quot;
        request.user
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;13、莫名其妙，返回一个实例的属性？其实这里大家不要忘记了，如果一个类的方法被property修饰了，调用这个方法的就可以使用属性的方式调用，而不用加括号了，如果大家不清楚，可以看我这篇博客：https://www.cnblogs.com/bainianminguo/p/9950607.html&lt;/p&gt;

&lt;p&gt;14、下面我们看下request.user到底是什么？我们先要知道request是什么？看下面的截图&lt;/p&gt;
&lt;p&gt;在dispatch方法中initial方法的参数有一个request，而这个request就是initialize_request的返回值，而initialize_request的返回值就是Request的实例对象&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305234255144-89474866.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305234531005-1574812785.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 这个request有一个user的属性或者方法，我们下面来找下&lt;/p&gt;

&lt;p&gt;15、下面我们来看下request.user到底是个什么东西？我们在Request类中确实找到了user这个方法，这个方法也被property装饰器装饰了，所以也印证了我们之前的猜测了，request.user是一个被property修饰的方法&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    @property
    def user(self):
        &quot;&quot;&quot;
        Returns the user associated with the current request, as authenticated
        by the authentication classes provided to the request.
        &quot;&quot;&quot;
        if not hasattr(self, '_user'):
            with wrap_attributeerrors():
                self._authenticate()
        return self._user
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;16、然后看下self._authenticate方法&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
    def _authenticate(self):
        &quot;&quot;&quot;
        Attempt to authenticate the request using each authentication instance
        in turn.
        &quot;&quot;&quot;
        for authenticator in self.authenticators:
            try:
                user_auth_tuple = authenticator.authenticate(self)
            except exceptions.APIException:
                self._not_authenticated()
                raise

            if user_auth_tuple is not None:
                self._authenticator = authenticator
                self.user, self.auth = user_auth_tuple
                return

        self._not_authenticated()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305235217496-1058309760.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;如果符合规范，则返回None，如果不符合规范，则raise抛出错误&lt;/p&gt;

&lt;p&gt; 到这里，我们就认证组件的源码梳理完了，下面我们来看下具体怎么写认证组件&lt;/p&gt;

&lt;p&gt; 17、下面进入如何写认证组件&lt;/p&gt;
&lt;p&gt;我们的类中要有这么一个属性。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190305235844512-1883325152.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;然后认证组件的类中要&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306000053877-573942762.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;


&lt;p&gt; 18、做认证，我们是通过一个token来做的，每次用户登陆，我们都会给他重新一个token，然后把这个token告诉客户，下次客户来访问带着token，我们就认为认证通过了&lt;/p&gt;
&lt;p&gt;所以我们先设计表，一个model表，一个Token表，两张表是一对一的关系&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;34&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
class User(models.Model):
    name = models.CharField(max_length=32)
    pwd = models.CharField(max_length=32)

class Token(models.Model):
    user = models.OneToOneField(to=User)
    token = models.CharField(max_length=128)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt; 19、然后我们写用户登陆的处理逻辑&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;42&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
from django.http import JsonResponse

class LoginCBV(APIView):
    def get(self,request):
        pass

    def post(self,request):
        name = request.data.get(&quot;name&quot;)
        pwd = request.data.get(&quot;pwd&quot;)

        obj = models.User.objects.filter(name=name,pwd=pwd).exists()
        res = {&quot;code&quot;:200,&quot;message&quot;:&quot;&quot;,&quot;token&quot;:&quot;&quot;}
        if obj:
            user_obj = models.User.objects.filter(name=name,pwd=pwd).first()
            token = create_token(name)
            models.Token.objects.update_or_create(user_obj,defaults={&quot;token&quot;:token})

            token_obj = models.Token.objects.get(user=user_obj)
            res[&quot;token&quot;] = token_obj.token

        else:
            res[&quot;code&quot;] = 201
            res[&quot;message&quot;] = &quot;用户名或者密码错误&quot;
        import json
        return JsonResponse(json.dumps(res))
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306000801605-1769071779.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;上面的update_or_create的方法写错了，正确的写法是下面的写法&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
  models.Token.objects.update_or_create(user=user_obj,defaults={&quot;token&quot;:token})
&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;20、这里还写了一个生成token的函数，加盐的盐为用户的名称&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306000913486-1309995784.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;


&lt;p&gt; 利用时间和用户的名称计算出来一个md5值，作为这次登陆的token&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
import hashlib
import time
def create_token(user):
    now = time.time()
    test_md5 = hashlib.md5(bytes(str(now),encoding=&quot;utf-8&quot;))
    test_md5.update(bytes(user,encoding=&quot;utf-8&quot;))
    return test_md5.hexdigest()
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt; 21、下面我们开始写的认证组件，如果我们想控制访问这条url：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
   url(r'^book_cbv/', views.Book_cbv.as_view(),name=&quot;test3&quot;),
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;22、那么我们就需要进入Book_cbv这个类中来做操作，这个属性我们之前也看到了，名称必须是authentication，且值要为一个list&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306001157886-1547015470.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;


&lt;p&gt; 23、最后我们下Book_auther这个类&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
class Book_auther(BaseAuthentication):
    def authenticate(self,request):
        token = request.GET.get(&quot;token&quot;)
        token_obj = models.Token.objects.filter(token=token).first()
        if token_obj:
            return token_obj.user.name,token_obj.token
        else:
            raise exceptions.AuthenticationFailed(&quot;验证失败&quot;)
    def authenticate_header(self,request):
        pass
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306001406687-628714924.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 24、最后我们用postman测试一下，首先先用post登陆一下，生成一下token&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306003029258-942238659.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;然后看下Token表中是否有token字段&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306003055845-1266192113.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 25、我们再次用postman登录一下，看下token是否会更新，我们看到token已经更新&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306003148791-895396474.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 26、我们首先先不携带token去访问book表，看下效果,提示验证失败&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306003507129-1315169431.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;27、下面我们携带token去访问，这样就可以返回查询到的结果了&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1101486/201903/1101486-20190306003720932-1216523251.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 大家要慢慢的体会&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 16:38:00 +0000</pubDate>
<dc:creator>bainianminguo</dc:creator>
<og:description>前言： Django的rest_framework组件的功能很强大，今天来我来给大家剖析一下认证组件 下面进入正文分析，我们从视图开始，一步一步来剖析认证组件 1、进入urls文件 2、然后执行Log</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/bainianminguo/p/10480887.html</dc:identifier>
</item>
<item>
<title>大数据技术之_10_Kafka学习_Kafka概述+Kafka集群部署+Kafka工作流程分析+Kafka API实战+Kafka Producer拦截器+Kafka Streams - 黑泽君</title>
<link>http://www.cnblogs.com/chenmingjun/p/10480793.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/chenmingjun/p/10480793.html</guid>
<description>&lt;p id=&quot;tocid_0&quot; class=&quot;toc&quot;&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h1kafka&quot;&gt;第1章 Kafka概述&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h11&quot;&gt;1.1 消息队列&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h12&quot;&gt;1.2 为什么需要消息队列&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h13kafka&quot;&gt;1.3 什么是Kafka&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h14kafka&quot;&gt;1.4 Kafka架构&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h2kafka&quot;&gt;第2章 Kafka集群部署&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h21&quot;&gt;2.1 环境准备&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h211&quot;&gt;2.1.1 集群规划&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h212jar&quot;&gt;2.1.2 jar包下载&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h22kafka&quot;&gt;2.2 Kafka集群部署&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h23kafka&quot;&gt;2.3 Kafka命令行操作&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h3kafka&quot;&gt;第3章 Kafka工作流程分析&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h31kafka&quot;&gt;3.1 Kafka 生产过程分析&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h311&quot;&gt;3.1.1 写入方式&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h312partition&quot;&gt;3.1.2 分区（Partition）&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h313replication&quot;&gt;3.1.3 副本（Replication）&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h314&quot;&gt;3.1.4 写入流程&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h32broker&quot;&gt;3.2 Broker 保存消息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h321&quot;&gt;3.2.1 存储方式&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h322&quot;&gt;3.2.2 存储策略&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h323zookeeper&quot;&gt;3.2.3 Zookeeper存储结构&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h33kafka&quot;&gt;3.3 Kafka 消费过程分析&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h331api&quot;&gt;3.3.1 高级API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h332api&quot;&gt;3.3.2 低级API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h333&quot;&gt;3.3.3 消费者组&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h334&quot;&gt;3.3.4 消费方式&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h335&quot;&gt;3.3.5 消费者组案例&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h4kafkaapi&quot;&gt;第4章 Kafka API实战&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h41&quot;&gt;4.1 环境准备&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h42kafkajavaapi&quot;&gt;4.2 Kafka生产者Java API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h421api&quot;&gt;4.2.1 创建生产者（过时的API）&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h422api&quot;&gt;4.2.2 创建生产者（新的API）&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h423api&quot;&gt;4.2.3 创建生产者带回调函数（新的API）&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h424&quot;&gt;4.2.4 自定义分区生产者&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h43kafkajavaapi&quot;&gt;4.3 Kafka消费者Java API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h431api&quot;&gt;4.3.1 高级API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h432api&quot;&gt;4.3.2 低级API&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h5kafkaproducerinterceptor&quot;&gt;第5章 Kafka Producer拦截器(interceptor)&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h51&quot;&gt;5.1 拦截器原理&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h52&quot;&gt;5.2 拦截器案例&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h6kafkastreams&quot;&gt;第6章 Kafka Streams&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h61&quot;&gt;6.1 概述&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h611kafkastreams&quot;&gt;6.1.1 Kafka Streams&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h612kafkastreams&quot;&gt;6.1.2 Kafka Streams 特点&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h613kafkastream&quot;&gt;6.1.3 为什么要有 Kafka Stream？&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h62kafkastream&quot;&gt;6.2 Kafka Stream 数据清洗案例&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h7&quot;&gt;第7章 扩展知识&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h71kafkaflume&quot;&gt;7.1 Kafka 与 Flume 比较&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h72flumekafka&quot;&gt;7.2 Flume 与 kafka 集成&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h73kafka&quot;&gt;7.3 Kafka配置信息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h731broker&quot;&gt;7.3.1 Broker 配置信息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h732producer&quot;&gt;7.3.2 Producer 配置信息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h733consumer&quot;&gt;7.3.3 Consumer 配置信息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;toc_item&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;span class=&quot;toc_left&quot;&gt;&lt;a href=&quot;http://www.cnblogs.com/chenmingjun/p/10480793.html#h74kafkaoffset&quot;&gt;7.4 如何查看 Kafka 集群维护的 offset 信息&lt;/a&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;h2 id=&quot;h1kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;第1章 Kafka概述&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h11&quot;&gt;&lt;span&gt;&lt;strong&gt;1.1 消息队列&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj06pj.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;&lt;strong&gt;1）点对点模式（一对一，消费者主动&lt;code&gt;拉取&lt;/code&gt;数据，消息收到后消息清除）&lt;/strong&gt;&lt;br/&gt;  点对点模型通常是一个&lt;code&gt;基于拉取或者轮询&lt;/code&gt;的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者）&lt;/strong&gt;&lt;br/&gt;  发布订阅模型则是一个&lt;code&gt;基于推送&lt;/code&gt;的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。&lt;/p&gt;
&lt;h3 id=&quot;h12&quot;&gt;&lt;span&gt;&lt;strong&gt;1.2 为什么需要消息队列&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1）解耦：&lt;/strong&gt;&lt;br/&gt;  允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2）冗余：&lt;/strong&gt;&lt;br/&gt;  消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的&quot;插入-获取-删除&quot;范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3）扩展性：&lt;/strong&gt;&lt;br/&gt;  因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4）灵活性 &amp;amp; 峰值处理能力：&lt;/strong&gt;&lt;br/&gt;  在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5）可恢复性：&lt;/strong&gt;&lt;br/&gt;  系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6）顺序保证：&lt;/strong&gt;&lt;br/&gt;  在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7）缓冲：&lt;/strong&gt;&lt;br/&gt;  有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;8）异步通信：&lt;/strong&gt;&lt;br/&gt;  很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。&lt;/p&gt;
&lt;h3 id=&quot;h13kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;1.3 什么是Kafka&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;  Kafka 是最初由 Linkedin 公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper 协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/Spark流式处理引擎，web/nginx日志、访问日志，消息服务等等，用scala语言编写，Linkedin 于 2010 年贡献给了 Apache 基金会并成为顶级开源项目。&lt;br/&gt;  在流式计算中，Kafka 一般用来&lt;code&gt;缓存数据&lt;/code&gt;，Storm通过消费Kafka的数据进行计算。&lt;br/&gt;  Kafka 是基于点对点模式的消息队列。&lt;/p&gt;
&lt;p&gt;  1）Apache Kafka是一个&lt;code&gt;开源消息系统&lt;/code&gt;，&lt;code&gt;由 Scala 写成&lt;/code&gt;。是由 Apache 软件基金会开发的一个开源消息系统项目。&lt;/p&gt;
&lt;p&gt;  2）Kafka 最初是由&lt;code&gt;LinkedIn&lt;/code&gt;公司开发，并于 2011 年初开源。2012 年 10月从Apache Incubator毕业，并成为顶级开源项目。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。&lt;/p&gt;
&lt;p&gt;  3）&lt;code&gt;Kafka 是一个分布式消息队列&lt;/code&gt;。Kafka 对消息保存时根据 Topic 进行归类，发送消息者称为 Producer，消息接受者称为Consumer，此外 kafka 集群有多个 kafka 实例组成，每个实例(server)称为 broker。真正存储数据的地方叫做 Topic。&lt;/p&gt;
&lt;p&gt;  4）无论是 kafka 集群，还是 Consumer 都依赖于&lt;code&gt;Zookeeper集群&lt;/code&gt;保存一些meta信息，来保证系统可用性。&lt;/p&gt;
&lt;h3 id=&quot;h14kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;1.4 Kafka架构&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Kafka整体架构图&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0Wn0.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;Kafka整体架构图详解&lt;br/&gt;&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0gcn.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;  1）Producer ：消息生产者，就是向 kafka broker 发消息的客户端。&lt;/p&gt;
&lt;p&gt;  2）Consumer ：消息消费者，向 kafka broker 取消息的客户端。&lt;/p&gt;
&lt;p&gt;  3）Topic ：可以理解为一个队列。&lt;/p&gt;
&lt;p&gt;  4） Consumer Group（CG）：这是 kafka 用来实现一个 topic 消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个 topic 可以有多个 CG。topic 的消息会复制（不是真的复制，是概念上的）到所有的 CG，但每个 partion 只会把消息发给该CG中的一个 consumer。如果需要实现广播，只要每 consumer 有一个独立的 CG 就可以了。要实现单播只要所有的 consumer 在同一个CG。&lt;code&gt;用CG还可以将 consumer 进行自由的分组而不需要多次发送消息到不同的 topic&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;  5）Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一 个broker 可以容纳多个 topic。&lt;/p&gt;
&lt;p&gt;  6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。partition 中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个 partition 中的顺序将消息发给consumer，不保证一个 topic 的整体（多个partition间）的顺序。&lt;/p&gt;
&lt;p&gt;  7）Offset：&lt;code&gt;kafka 的存储文件都是按照 offset.kafka 来命名&lt;/code&gt;，用 offset 做名字的好处是&lt;code&gt;方便查找&lt;/code&gt;。例如你想找位于2049的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就是 00000000000.kafka。&lt;/p&gt;
&lt;p&gt;  &lt;strong&gt;8）分区对于Kafka集群的好处是：实现负载均衡。分区对于消费者来说，可以提高并发度，提高效率。在公司中应用的时候，针对于某一个 Topic，它有几个分区(n个)，我们就对应的建一个有几个消费者的消费者组(m个)。即：n大于或者等于m，最好是n=m。当n&amp;gt;m时，就意味着某一个消费者会消费多个分区的数据。不仅如此，一个消费者还可以消费多个 Topic 数据。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;h2kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;第2章 Kafka集群部署&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h21&quot;&gt;&lt;span&gt;&lt;strong&gt;2.1 环境准备&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h211&quot;&gt;&lt;span&gt;&lt;strong&gt;2.1.1 集群规划&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs nginx&quot;&gt;&lt;span class=&quot;hljs-attribute&quot;&gt;hadoop102&lt;/span&gt;                    hadoop103               hadoop104&lt;br/&gt;zk                            zk                      zk&lt;br/&gt;kafka                        kafka                   kafka&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h212jar&quot;&gt;&lt;span&gt;&lt;strong&gt;2.1.2 jar包下载&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  &lt;a href=&quot;http://kafka.apache.org/downloads.html&quot; target=&quot;_blank&quot;&gt;http://kafka.apache.org/downloads.html&lt;/a&gt;&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj05AU.png&quot; alt=&quot;&quot;/&gt;&lt;h3 id=&quot;h22kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;2.2 Kafka集群部署&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;1）解压安装包&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 software]$ tar -zxvf kafka_2.&lt;span class=&quot;hljs-number&quot;&gt;11&lt;/span&gt;-&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;.&lt;span class=&quot;hljs-number&quot;&gt;11.0&lt;/span&gt;.&lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt;.tgz -C /opt/&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;module&lt;/span&gt;/&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2）修改解压后的文件名称&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;module&lt;/span&gt;]$ &lt;span class=&quot;hljs-title&quot;&gt;mv&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;kafka_2&lt;/span&gt;.11-0.11.0.2/ &lt;span class=&quot;hljs-title&quot;&gt;kafka&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;3）在/opt/module/kafka目录下创建logs文件夹&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs perl&quot;&gt;[atguigu@hadoop102 kafka]$ &lt;span class=&quot;hljs-keyword&quot;&gt;mkdir&lt;/span&gt; logs&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;4）修改配置文件&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 kafka]$ cd config/&lt;br/&gt;[atguigu@hadoop102 config]$ vim server.properties&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;输入以下内容：&lt;/p&gt;
&lt;pre readability=&quot;10&quot;&gt;
&lt;code class=&quot;hljs coffeescript&quot; readability=&quot;14&quot;&gt;&lt;br/&gt;broker.id=&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;delete&lt;/span&gt;.topic.enable=&lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;num.network.threads=&lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;num.io.threads=&lt;span class=&quot;hljs-number&quot;&gt;8&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;socket.send.buffer.bytes=&lt;span class=&quot;hljs-number&quot;&gt;102400&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;socket.receive.buffer.bytes=&lt;span class=&quot;hljs-number&quot;&gt;102400&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;socket.request.max.bytes=&lt;span class=&quot;hljs-number&quot;&gt;104857600&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;log.dirs=/opt/&lt;span class=&quot;hljs-built_in&quot;&gt;module&lt;/span&gt;/kafka/logs&lt;/p&gt;&lt;p&gt;&lt;br/&gt;num.partitions=&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;num.recovery.threads.per.data.dir=&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;log.retention.hours=&lt;span class=&quot;hljs-number&quot;&gt;168&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;zookeeper.connect=hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt;,hadoop103:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt;,hadoop104:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;5）配置环境变量&lt;/p&gt;
&lt;pre readability=&quot;5&quot;&gt;
&lt;code class=&quot;hljs coffeescript&quot; readability=&quot;4&quot;&gt;[atguigu@hadoop102 &lt;span class=&quot;hljs-built_in&quot;&gt;module&lt;/span&gt;]$ sudo vim /etc/profile&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; KAFKA_HOME=/opt/&lt;span class=&quot;hljs-built_in&quot;&gt;module&lt;/span&gt;/kafka&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; PATH=$PATH:$KAFKA_HOME/bin&lt;/p&gt;&lt;p&gt;[atguigu@hadoop102 &lt;span class=&quot;hljs-built_in&quot;&gt;module&lt;/span&gt;]$ source /etc/profile&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;6）分发安装包&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;module&lt;/span&gt;]$ &lt;span class=&quot;hljs-title&quot;&gt;xsync&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;kafka&lt;/span&gt;/&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;注意：&lt;/code&gt;分发之后记得配置其他机器的环境变量。&lt;br/&gt;7）分别在hadoop103和hadoop104上修改配置文件/opt/module/kafka/config/server.properties中的broker.id=1、broker.id=2&lt;br/&gt;&lt;code&gt;注：&lt;/code&gt;broker.id不得重复。&lt;br/&gt;8）启动Kafka集群&lt;br/&gt;依次在hadoop102、hadoop103、hadoop104节点上启动kafka&lt;/p&gt;
&lt;pre readability=&quot;8&quot;&gt;
&lt;code class=&quot;hljs ruby&quot; readability=&quot;10&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-server-start.sh config/server.properties &amp;amp;&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-server-start.sh config/server.properties &amp;amp;&lt;br/&gt;[atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh config/server.properties &amp;amp;&lt;p&gt;启动Kafka是一个阻塞进程，会打印我们操作kafka的日志，我们可以把窗口放到后台，在命令后面加一个与&amp;amp;符号，将该阻塞进程放到后台。&lt;/p&gt;&lt;p&gt;写群起Kafka集群脚本的时候，需要使用-daemon命令，具体如下：&lt;br/&gt;[atguigu@hadoop102 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties&lt;br/&gt;[atguigu@hadoop104 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties&lt;/p&gt;&lt;p&gt;-daemon 表示守护进程，会将日志打印在后台。&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;9）关闭Kafka集群&lt;/p&gt;
&lt;pre readability=&quot;5.5&quot;&gt;
&lt;code class=&quot;hljs vbscript&quot; readability=&quot;5&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh &lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh &lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;&lt;br/&gt;[atguigu@hadoop104 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh &lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;&lt;p&gt;写群起Kafka集群脚本的时候，需要使用-daemon命令，具体如下：&lt;br/&gt;[atguigu@hadoop102 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh -daemon config/&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;.properties&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh -daemon config/&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;.properties&lt;br/&gt;[atguigu@hadoop104 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;-&lt;span class=&quot;hljs-keyword&quot;&gt;stop&lt;/span&gt;.sh -daemon config/&lt;span class=&quot;hljs-built_in&quot;&gt;server&lt;/span&gt;.properties&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;h23kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;2.3 Kafka命令行操作&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;0）补充知识&lt;/p&gt;
&lt;pre readability=&quot;5.5&quot;&gt;
&lt;code class=&quot;hljs perl&quot; readability=&quot;5&quot;&gt;jps         查看当前进程&lt;br/&gt;jps -l      查看当前进程所属主类&lt;p&gt;&lt;span class=&quot;hljs-string&quot;&gt;`注意：`&lt;/span&gt;当有很多进程都是同一个名字，我们该如何区分？&lt;br/&gt;&lt;span class=&quot;hljs-string&quot;&gt;`答：`&lt;/span&gt;每一次启动一个进程后，我们将该进程与对应的进程ID写入一个文档中。如果某一个进程出现问题或者某一个框架出现问题，便于我们&lt;span class=&quot;hljs-keyword&quot;&gt;kill&lt;/span&gt;掉相应的进程。不至于关闭整个系统。（生产环境下一般不允许关闭或重启整个系统！）&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;1）查看当前服务器中的所有topic&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs php&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;list&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2）创建topic&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs sql&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh &lt;p&gt;bin/kafka-topics.sh &lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;选项说明：&lt;br/&gt;  --topic 定义topic名&lt;br/&gt;  --replication-factor 定义副本数（&lt;code&gt;注：副本数不能大于节点数，否则会报错！&lt;/code&gt;）&lt;br/&gt;  --partitions 定义分区数&lt;/p&gt;
&lt;p&gt;3）删除topic&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; \&lt;br/&gt;--&lt;span class=&quot;hljs-keyword&quot;&gt;delete&lt;/span&gt; --topic first&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;注意：&lt;/code&gt;需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。&lt;/p&gt;
&lt;p&gt;4）发送消息（生产者连接的是kafka集群默认的端口号是：9092）&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;code class=&quot;hljs ruby&quot; readability=&quot;2&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh \&lt;br/&gt;--broker-list &lt;span class=&quot;hljs-symbol&quot;&gt;hadoop102:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;9092&lt;/span&gt; --topic first&lt;br/&gt;&amp;gt;hello world&lt;br/&gt;&amp;gt;atguigu  atguigu&lt;p&gt;&lt;span class=&quot;hljs-string&quot;&gt;`注意：`&lt;/span&gt;生产者连接的是kafka集群。&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;5）消费消息&lt;/p&gt;
&lt;pre readability=&quot;6&quot;&gt;
&lt;code class=&quot;hljs coffeescript&quot; readability=&quot;6&quot;&gt;新版本&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt;-beginning --topic first&lt;p&gt;或者&lt;/p&gt;&lt;p&gt;老版本&lt;br/&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--bootstrap-server hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;9092&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt;-beginning --topic first&lt;/p&gt;&lt;p&gt;`&lt;span class=&quot;javascript&quot;&gt;注意：&lt;/span&gt;`消费者会将自己的offset文件保存在 zookeeper(低版本的kafka)。所以消费者连接的是 zookeeper。&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;--from-beginning&lt;/code&gt;：会把first主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。如果不加该配置，那么消费者消费的消息将是最新的消息(不包括以往的所有数据)。&lt;/p&gt;
&lt;p&gt;6）查看某个topic的详情&lt;/p&gt;
&lt;pre readability=&quot;4.5&quot;&gt;
&lt;code class=&quot;hljs ruby&quot; readability=&quot;3&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --zookeeper &lt;span class=&quot;hljs-symbol&quot;&gt;hadoop102:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; \&lt;br/&gt;--describe --topic first&lt;br/&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;Topic:&lt;/span&gt;first    &lt;span class=&quot;hljs-symbol&quot;&gt;PartitionCount:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;    &lt;span class=&quot;hljs-symbol&quot;&gt;ReplicationFactor:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;hljs-symbol&quot;&gt;Configs:&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;Topic:&lt;/span&gt; first    &lt;span class=&quot;hljs-symbol&quot;&gt;Partition:&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;    &lt;span class=&quot;hljs-symbol&quot;&gt;Leader:&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;hljs-symbol&quot;&gt;Replicas:&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;hljs-symbol&quot;&gt;Isr:&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;&lt;p&gt;Isr的作用：当 leader 挂掉后，选举新 leader 时使用的。Isr 的排序规则是：与 leader 的相似度，越高越在前，越在前越有可能成为新 leader。&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;7）警告问题解释&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs vbnet&quot;&gt;[atguigu@hadoop103 kafka]$ bin/kafka-console-consumer.sh \&lt;br/&gt;&amp;gt; --zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt;-beginning --topic first&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;Using&lt;/span&gt; the ConsoleConsumer &lt;span class=&quot;hljs-keyword&quot;&gt;with&lt;/span&gt; old consumer &lt;span class=&quot;hljs-keyword&quot;&gt;is&lt;/span&gt; deprecated &lt;span class=&quot;hljs-keyword&quot;&gt;and&lt;/span&gt; will be removed &lt;span class=&quot;hljs-keyword&quot;&gt;in&lt;/span&gt; a future major release. Consider &lt;span class=&quot;hljs-keyword&quot;&gt;using&lt;/span&gt; the &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; consumer &lt;span class=&quot;hljs-keyword&quot;&gt;by&lt;/span&gt; passing [bootstrap-server] instead &lt;span class=&quot;hljs-keyword&quot;&gt;of&lt;/span&gt; [zookeeper].&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在高版本的kafka中，消费者会将自己的 offset文件 保存在 kafka 集群的本地，不交给 zookeeper 维护了！如下图所示：&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0shQ.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;&lt;strong&gt;这样做的好处是：提高了效率，减少了网络传输。&lt;/strong&gt;
&lt;h2 id=&quot;h3kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;第3章 Kafka工作流程分析&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0c1s.png&quot; alt=&quot;&quot;/&gt;&lt;h3 id=&quot;h31kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;3.1 Kafka 生产过程分析&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h311&quot;&gt;&lt;span&gt;&lt;strong&gt;3.1.1 写入方式&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  producer 采用推（push）模式将消息发布到 broker，每条消息都被追加（append）到分区（patition）中，属于&lt;code&gt;顺序写磁盘&lt;/code&gt;（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。&lt;/p&gt;
&lt;h4 id=&quot;h312partition&quot;&gt;&lt;span&gt;&lt;strong&gt;3.1.2 分区（Partition）&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  消息发送时都被发送到一个 topic，其本质就是一个目录，而topic是由一些 Partition Logs(分区日志)组成，其组织结构如下图所示：&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0fBV.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;  我们可以看到，&lt;code&gt;每个 Partition 中的消息都是有序&lt;/code&gt;的，生产的消息被不断追加到 Partition log 上，其中的每一个消息都被赋予了一个唯一的 &lt;code&gt;offset值&lt;/code&gt;。&lt;br/&gt;&lt;strong&gt;1）分区的原因&lt;/strong&gt;&lt;br/&gt;  （1）&lt;code&gt;方便在集群中扩展&lt;/code&gt;，每个 Partition 可以通过调整以适应它所在的机器，而一个topic又可以有多个 Partition 组成，因此整个集群就可以适应任意大小的数据了。&lt;br/&gt;  （2）&lt;code&gt;可以提高并发&lt;/code&gt;，因为可以以 Partition 为单位读写了。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2）分区的原则&lt;/strong&gt;&lt;br/&gt;  （1）指定了 patition，则直接使用。&lt;br/&gt;  （2）未指定 patition 但指定 key，通过对 key 的 value 进行 hash 出一个 patition。&lt;br/&gt;  （3）patition 和 key 都未指定，使用轮询选出一个 patition。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java language-java hljs&quot;&gt;DefaultPartitioner类&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String topic, Object key, &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] keyBytes, Object value, &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] valueBytes, Cluster cluster)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;List&amp;lt;PartitionInfo&amp;gt; partitions = cluster.partitionsForTopic(topic);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; numPartitions = partitions.size();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (keyBytes == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; nextValue = nextValue(topic);&lt;br/&gt;List&amp;lt;PartitionInfo&amp;gt; availablePartitions = cluster.availablePartitionsForTopic(topic);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (availablePartitions.size() &amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; part = Utils.toPositive(nextValue) % availablePartitions.size();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; availablePartitions.get(part).partition();&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; Utils.toPositive(nextValue) % numPartitions;&lt;br/&gt;}&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h313replication&quot;&gt;&lt;span&gt;&lt;strong&gt;3.1.3 副本（Replication）&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  同一个 partition 可能会有多个 replication（对应 server.properties 配置中的 default.replication.factor=N）。没有 replication 的情况下，一旦b roker 宕机，其上所有 patition 的数据都不可被消费，同时 producer 也不能再将数据存于其上的 partition。引入 replication 之后，同一个 partition 可能会有多个 replication，而这时需要在这些 replication 之间选出一个 leader，producer 和 consumer 只与这个 leader 交互，其它 replication 作为 follower 从leader 中复制数据。&lt;/p&gt;
&lt;h4 id=&quot;h314&quot;&gt;&lt;span&gt;&lt;strong&gt;3.1.4 写入流程&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;producer写入消息流程如下：&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj02Xq.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;  1）producer 先从 zookeeper 的 &quot;/brokers/…/state&quot;节点找到该 partition 的 leader&lt;br/&gt;  2）producer 将消息发送给该 leader&lt;br/&gt;  3）leader 将消息写入本地 log&lt;br/&gt;  4）followers 从 leader pull 消息，写入本地 log 后向 leader 发送 ACK&lt;br/&gt;  5）leader 收到所有ISR中的 replication 的 ACK 后，增加 HW（high watermark，最后 commit 的offset）并向 producer 发送 ACK&lt;br/&gt;  &lt;strong&gt;&lt;code&gt;注意&lt;/code&gt;：要特别注意ACK应答模式！&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;h32broker&quot;&gt;&lt;span&gt;&lt;strong&gt;3.2 Broker 保存消息&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h321&quot;&gt;&lt;span&gt;&lt;strong&gt;3.2.1 存储方式&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  物理上把 topic 分成一个或多个 patition（对应 server.properties 中的num.partitions=3配置），每个 patition 物理上对应一个文件夹（该文件夹存储该 patition 的所有消息和索引文件），如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 logs]$ cd first-&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;/&lt;br/&gt;[atguigu@hadoop102 first-&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;]$ ll&lt;br/&gt;总用量 &lt;span class=&quot;hljs-number&quot;&gt;16&lt;/span&gt;&lt;br/&gt;-rw-rw-r--. &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; atguigu atguigu   &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;月   &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;34&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000000&lt;/span&gt;.index&lt;br/&gt;-rw-rw-r--. &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; atguigu atguigu &lt;span class=&quot;hljs-number&quot;&gt;225&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;月   &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000000&lt;/span&gt;.log&lt;br/&gt;-rw-rw-r--. &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; atguigu atguigu  &lt;span class=&quot;hljs-number&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;月   &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;34&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000000&lt;/span&gt;.timeindex&lt;br/&gt;-rw-rw-r--. &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; atguigu atguigu  &lt;span class=&quot;hljs-number&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;月   &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;34&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000003&lt;/span&gt;.snapshot&lt;br/&gt;-rw-rw-r--. &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt; atguigu atguigu   &lt;span class=&quot;hljs-number&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;月   &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;hljs-symbol&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;24&lt;/span&gt; leader-epoch-checkpoint&lt;br/&gt;[atguigu@hadoop102 first-&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;]$ &lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h322&quot;&gt;&lt;span&gt;&lt;strong&gt;3.2.2 存储策略&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：&lt;br/&gt;    1）基于时间：log.retention.hours=168 （单位是小时，168小时即7天）&lt;br/&gt;    2）基于大小：log.retention.bytes=1073741824&lt;br/&gt;  需要注意的是，因为 Kafka 读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。&lt;/p&gt;
&lt;h4 id=&quot;h323zookeeper&quot;&gt;&lt;span&gt;&lt;strong&gt;3.2.3 Zookeeper存储结构&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0INF.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;&lt;code&gt;注意&lt;/code&gt;：producer 不在zk中注册，消费者在zk中注册。
&lt;h3 id=&quot;h33kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3 Kafka 消费过程分析&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;  kafka提供了两套 consumer API：高级 Consumer API 和低级 Consumer API。&lt;/p&gt;
&lt;h4 id=&quot;h331api&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3.1 高级API&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1）高级API优点&lt;/strong&gt;&lt;br/&gt;  高级 API 写起来简单。&lt;br/&gt;  &lt;code&gt;不需要自行去管理 offset，系统通过 zookeeper 自行管理。&lt;/code&gt;&lt;br/&gt;  &lt;code&gt;不需要管理分区、副本等情况，系统自动管理。&lt;/code&gt;&lt;br/&gt;  消费者断线会自动根据上一次记录在 zookeeper 中的 offset 去接着获取数据（默认设置1分钟更新一下 zookeeper 中存的 offset）。&lt;br/&gt;  可以使用 group 来区分对同一个 topic 的不同程序的访问分离开来（不同的 group 记录不同的 offset，这样不同程序读取同一个 topic 才不会因为 offset 互相影响）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2）高级API缺点&lt;/strong&gt;&lt;br/&gt;  &lt;code&gt;不能自行控制offset（对于某些特殊需求来说）。&lt;/code&gt;&lt;br/&gt;  不能细化控制如分区、副本、zk等。&lt;/p&gt;
&lt;h4 id=&quot;h332api&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3.2 低级API&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1）低级 API 优点&lt;/strong&gt;&lt;br/&gt;  &lt;code&gt;能够让开发者自己控制 offset，想从哪里读取就从哪里读取。&lt;/code&gt;&lt;br/&gt;  自行控制连接分区，对分区自定义进行负载均衡。&lt;br/&gt;  对 zookeeper 的依赖性降低（如：offset 不一定非要靠zk存储，自行存储 offset 即可，比如存在文件或者内存中）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2）低级API缺点&lt;/strong&gt;&lt;br/&gt;  太过复杂，需要自行控制 offset，连接哪个分区，找到分区 leader 等。&lt;/p&gt;
&lt;h4 id=&quot;h333&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3.3 消费者组&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0H39.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;  消费者是以 consumer group 消费者组的方式工作，由一个或者多个消费者组成一个组，共同消费一个 topic。每个分区在同一时间只能由 group 中的一个消费者读取，但是多个 group 可以同时消费这个 partition。在图中，有一个由三个消费者组成的 group，有一个消费者读取主题中的两个分区，另外两个分别读取一个分区。某个消费者读取某个分区，也可以叫做某个消费者是某个分区的拥有者。&lt;br/&gt;  在这种情况下，消费者可以通过&lt;code&gt;水平扩展&lt;/code&gt;的方式同时读取大量的消息。另外，如果一个消费者失败了，那么其他的 group 成员会自动负载均衡读取之前失败的消费者读取的分区。
&lt;h4 id=&quot;h334&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3.4 消费方式&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  &lt;code&gt;consumer采用 pull（拉）模式从 broker 中读取数据。&lt;/code&gt;&lt;br/&gt;  push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。&lt;br/&gt;  对于 Kafka 而言，pull 模式更合适，它可简化 broker 的设计，consumer 可自主控制消费消息的速率，同时 consumer 可以自己控制消费方式--&lt;code&gt;即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义&lt;/code&gt;。&lt;br/&gt;  pull 模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞（并且可选地等待到给定的字节数，以确保大的传输大小）。&lt;/p&gt;
&lt;h4 id=&quot;h335&quot;&gt;&lt;span&gt;&lt;strong&gt;3.3.5 消费者组案例&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。&lt;br/&gt;2）案例实操：&lt;br/&gt;（1）在hadoop102、hadoop103上修改/opt/module/kafka/config/consumer.properties配置文件中的group.id属性为任意组名。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs cs&quot;&gt;[&lt;span class=&quot;hljs-meta&quot;&gt;atguigu@hadoop103 config&lt;/span&gt;]$ vim consumer.properties&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;group&lt;/span&gt;.id=atguigu&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）在hadoop102、hadoop103上分别启动消费者&lt;/p&gt;
&lt;pre readability=&quot;4.5&quot;&gt;
&lt;code class=&quot;hljs coffeescript&quot; readability=&quot;3&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --topic first --consumer.config config/consumer.properties&lt;p&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --topic first --consumer.config config/consumer.properties&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（3）在hadoop104上启动生产者&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop104 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-producer.sh \&lt;br/&gt;--broker-list hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;9092&lt;/span&gt; --topic first&lt;br/&gt;&amp;gt;hello world&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（4）查看hadoop102和hadoop103的接收者。&lt;br/&gt;  &lt;code&gt;结论：&lt;/code&gt;同一时刻只有一个消费者接收到消息。&lt;/p&gt;
&lt;h2 id=&quot;h4kafkaapi&quot;&gt;&lt;span&gt;&lt;strong&gt;第4章 Kafka API实战&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h41&quot;&gt;&lt;span&gt;&lt;strong&gt;4.1 环境准备&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;1）启动zk集群和kafka集群，在kafka集群中打开一个消费者&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --topic first&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2）导入pom依赖&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;xml language-xml hljs&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;dependencies&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;org.apache.kafka&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;kafka-clients&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;0.11.0.0&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;org.apache.kafka&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;kafka_2.12&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;0.11.0.2&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;dependencies&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;h42kafkajavaapi&quot;&gt;&lt;span&gt;&lt;strong&gt;4.2 Kafka生产者Java API&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h421api&quot;&gt;&lt;span&gt;&lt;strong&gt;4.2.1 创建生产者（过时的API）&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;pre readability=&quot;16.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;27&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.producer.KeyedMessage;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.producer.Producer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.producer.ProducerConfig;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;OldProducer&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@SuppressWarnings&lt;/span&gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;deprecation&quot;&lt;/span&gt;)&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;metadata.broker.list&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;request.required.acks&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;1&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;serializer.class&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;kafka.serializer.StringEncoder&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;Producer&amp;lt;Integer, String&amp;gt; producer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Producer&amp;lt;Integer, String&amp;gt;(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerConfig(props));&lt;/p&gt;&lt;p&gt;KeyedMessage&amp;lt;Integer, String&amp;gt; message = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KeyedMessage&amp;lt;Integer, String&amp;gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hello world&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;producer.send(message);&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h422api&quot;&gt;&lt;span&gt;&lt;strong&gt;4.2.2 创建生产者（新的API）&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;pre readability=&quot;23&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;40&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.Producer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerRecord;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;NewProducer&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;acks&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;all&quot;&lt;/span&gt;);&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;retries&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;batch.size&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;16384&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;linger.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;buffer.memory&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;33554432&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;Producer&amp;lt;String, String&amp;gt; producer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaProducer&amp;lt;String, String&amp;gt;(props);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;50&lt;/span&gt;; i++) {&lt;br/&gt;producer.send(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;, Integer.toString(i), &lt;span class=&quot;hljs-string&quot;&gt;&quot;hello world-&quot;&lt;/span&gt; + i));&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;producer.close();&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h423api&quot;&gt;&lt;span&gt;&lt;strong&gt;4.2.3 创建生产者带回调函数（新的API）&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;pre readability=&quot;25&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;44&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.*;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.Future;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;CallBackNewProducer&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props;&lt;br/&gt;props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;acks&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;all&quot;&lt;/span&gt;);&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;retries&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;batch.size&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;16384&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;linger.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;buffer.memory&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;33554432&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;Producer&amp;lt;String, String&amp;gt; producer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaProducer&amp;lt;String, String&amp;gt;(props);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;50&lt;/span&gt;; i++) {&lt;/p&gt;&lt;p&gt;producer.send(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hello world-&quot;&lt;/span&gt; + i), &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Callback() {&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;onCompletion&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(RecordMetadata metadata, Exception exception)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata != &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;/p&gt;&lt;p&gt;System.err.println(metadata.partition() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;---&quot;&lt;/span&gt; + metadata.offset());&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;});&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;producer.close();&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h424&quot;&gt;&lt;span&gt;&lt;strong&gt;4.2.4 自定义分区生产者&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;0）需求：将所有数据存储到topic的第0号分区上。&lt;br/&gt;1）定义一个类实现Partitioner接口，重写里面的方法（过时API）&lt;/p&gt;
&lt;pre readability=&quot;7.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;9&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.producer.Partitioner;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;PartitionerOldProducer&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;Partitioner&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;PartitionerOldProducer&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;super&lt;/span&gt;();&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(Object key, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; numPartitions)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2）自定义分区（新API）&lt;/p&gt;
&lt;pre readability=&quot;11&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;16&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.Partitioner;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.common.Cluster;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Map;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;PartitionerNewProducer&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;Partitioner&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(Map&amp;lt;String, ?&amp;gt; configs)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String topic, Object key, &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] keyBytes, Object value, &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] valueBytes, Cluster cluster)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;3）在代码中调用&lt;/p&gt;
&lt;pre readability=&quot;26.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;47&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.producer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.*;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.concurrent.Future;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;CallBackNewProducer&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props;&lt;br/&gt;props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;acks&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;all&quot;&lt;/span&gt;);&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;retries&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;batch.size&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;16384&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;linger.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;buffer.memory&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;33554432&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;partitioner.class&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;com.atguigu.kafka.producer.PartitionerNewProducer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;Producer&amp;lt;String, String&amp;gt; producer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaProducer&amp;lt;String, String&amp;gt;(props);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;50&lt;/span&gt;; i++) {&lt;/p&gt;&lt;p&gt;producer.send(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hello world-&quot;&lt;/span&gt; + i), &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Callback() {&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;onCompletion&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(RecordMetadata metadata, Exception exception)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata != &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;/p&gt;&lt;p&gt;System.err.println(metadata.partition() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;---&quot;&lt;/span&gt; + metadata.offset());&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;});&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;producer.close();&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;4）测试&lt;br/&gt;（1）在hadoop102上监控/opt/module/kafka/logs/目录下second主题2个分区的log日志动态变化情况&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 second-&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;]$ tail -f &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000000&lt;/span&gt;.log&lt;br/&gt;[atguigu@hadoop102 second-&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;]$ tail -f &lt;span class=&quot;hljs-number&quot;&gt;00000000000000000000&lt;/span&gt;.log&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）发现数据都存储到指定的分区了。&lt;/p&gt;
&lt;h3 id=&quot;h43kafkajavaapi&quot;&gt;&lt;span&gt;&lt;strong&gt;4.3 Kafka消费者Java API&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h431api&quot;&gt;&lt;span&gt;&lt;strong&gt;4.3.1 高级API&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;0）在控制台创建发送者&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop104 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-producer.sh \&lt;br/&gt;--broker-list hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;9092&lt;/span&gt; --topic second&lt;br/&gt;&amp;gt;hello world&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;1）创建消费者（过时API）&lt;/p&gt;
&lt;pre readability=&quot;23&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;40&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.consumer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.consumer.Consumer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.consumer.ConsumerConfig;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.consumer.ConsumerIterator;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.consumer.KafkaStream;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.consumer.ConsumerConnector;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.HashMap;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.List;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Map;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;OldConsumer&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@SuppressWarnings&lt;/span&gt;(&lt;span class=&quot;hljs-string&quot;&gt;&quot;deprecation&quot;&lt;/span&gt;)&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;/p&gt;&lt;p&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;zookeeper.connect&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:2181&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;group.id&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;g1&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;zookeeper.session.timeout.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;500&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;zookeeper.sync.time.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;250&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;auto.commit.interval.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;1000&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;ConsumerConnector consumer = Consumer.createJavaConsumerConnector(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ConsumerConfig(props));&lt;/p&gt;&lt;p&gt;HashMap&amp;lt;String, Integer&amp;gt; topicCount = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; HashMap&amp;lt;String, Integer&amp;gt;();&lt;br/&gt;topicCount.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;Map&amp;lt;String, List&amp;lt;KafkaStream&amp;lt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[]&amp;gt;&amp;gt;&amp;gt; consumerMap = consumer.createMessageStreams(topicCount);&lt;/p&gt;&lt;p&gt;KafkaStream&amp;lt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[]&amp;gt; stream = consumerMap.get(&lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;).get(&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;ConsumerIterator&amp;lt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[]&amp;gt; it = stream.iterator();&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;while&lt;/span&gt; (it.hasNext()) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; String(it.next().message()));&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2）官方提供案例（自动维护消费情况）（新API）&lt;/p&gt;
&lt;pre readability=&quot;24.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;43&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.consumer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.consumer.ConsumerRecord;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.common.TopicPartition;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Arrays;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Collections;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;NewConsumer&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;Properties props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;group.id&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;test&quot;&lt;/span&gt;);&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;enable.auto.commit&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;true&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;auto.commit.interval.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;1000&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;key.deserializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;value.deserializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;KafkaConsumer&amp;lt;String, String&amp;gt; consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaConsumer&amp;lt;String, String&amp;gt;(props);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;consumer.subscribe(Arrays.asList(&lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;third&quot;&lt;/span&gt;));&lt;br/&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;while&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;true&lt;/span&gt;) {&lt;br/&gt;ConsumerRecords&amp;lt;String, String&amp;gt; records = consumer.poll(&lt;span class=&quot;hljs-number&quot;&gt;100&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (ConsumerRecord&amp;lt;String, String&amp;gt; record : records)&lt;br/&gt;System.out.println(record.topic() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;---&quot;&lt;/span&gt; + record.partition() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;---&quot;&lt;/span&gt; + record.offset() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;---&quot;&lt;/span&gt; +  record.value());&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;h432api&quot;&gt;&lt;span&gt;&lt;strong&gt;4.3.2 低级API&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;实现使用低级API读取指定topic，指定partition，指定offset的数据。&lt;br/&gt;1）消费者使用低级API 的主要步骤：&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0h7T.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;2）方法描述：&lt;br/&gt;&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0bcR.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;3）完整版代码：
&lt;pre readability=&quot;97.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;189&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.consumer;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.nio.ByteBuffer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.ArrayList;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Collections;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.HashMap;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.List;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Map;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.api.FetchRequest;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.api.FetchRequestBuilder;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.api.PartitionOffsetRequestInfo;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.cluster.BrokerEndPoint;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.common.ErrorMapping;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.common.TopicAndPartition;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.FetchResponse;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.OffsetResponse;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.PartitionMetadata;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.TopicMetadata;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.TopicMetadataRequest;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.javaapi.consumer.SimpleConsumer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; kafka.message.MessageAndOffset;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;SimpleExample&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; List&amp;lt;String&amp;gt; m_replicaBrokers = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;String&amp;gt;();&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;SimpleExample&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;m_replicaBrokers = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;String&amp;gt;();&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String args[])&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;SimpleExample example = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; SimpleExample();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; maxReads = Long.parseLong(&lt;span class=&quot;hljs-string&quot;&gt;&quot;3&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;String topic = &lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; partition = Integer.parseInt(&lt;span class=&quot;hljs-string&quot;&gt;&quot;0&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;List&amp;lt;String&amp;gt; seeds = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;String&amp;gt;();&lt;br/&gt;seeds.add(&lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102&quot;&lt;/span&gt;);&lt;br/&gt;seeds.add(&lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop103&quot;&lt;/span&gt;);&lt;br/&gt;seeds.add(&lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop103&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; port = Integer.parseInt(&lt;span class=&quot;hljs-string&quot;&gt;&quot;9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;try&lt;/span&gt; {&lt;br/&gt;example.run(maxReads, topic, partition, seeds, port);&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Oops:&quot;&lt;/span&gt; + e);&lt;br/&gt;e.printStackTrace();&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; a_maxReads, String a_topic, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_partition, List&amp;lt;String&amp;gt; a_seedBrokers, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_port)&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;throws&lt;/span&gt; Exception &lt;/span&gt;{&lt;br/&gt;PartitionMetadata metadata = findLeader(a_seedBrokers, a_port, a_topic, a_partition);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Can't find metadata for Topic and Partition. Exiting&quot;&lt;/span&gt;);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata.leader() == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Can't find Leader for Topic and Partition. Exiting&quot;&lt;/span&gt;);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;String leadBroker = metadata.leader().host();&lt;br/&gt;String clientName = &lt;span class=&quot;hljs-string&quot;&gt;&quot;Client_&quot;&lt;/span&gt; + a_topic + &lt;span class=&quot;hljs-string&quot;&gt;&quot;_&quot;&lt;/span&gt; + a_partition;&lt;/p&gt;&lt;p&gt;SimpleConsumer consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; SimpleConsumer(leadBroker, a_port, &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;64&lt;/span&gt; * &lt;span class=&quot;hljs-number&quot;&gt;1024&lt;/span&gt;, clientName);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.EarliestTime(), clientName);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; numErrors = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;while&lt;/span&gt; (a_maxReads &amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (consumer == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; SimpleConsumer(leadBroker, a_port, &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;64&lt;/span&gt; * &lt;span class=&quot;hljs-number&quot;&gt;1024&lt;/span&gt;, clientName);&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;FetchRequest req = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; FetchRequestBuilder().clientId(clientName).addFetch(a_topic, a_partition, readOffset, &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;).build();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;FetchResponse fetchResponse = consumer.fetch(req);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (fetchResponse.hasError()) {&lt;br/&gt;numErrors++;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;short&lt;/span&gt; code = fetchResponse.errorCode(a_topic, a_partition);&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Error fetching data from the Broker:&quot;&lt;/span&gt; + leadBroker + &lt;span class=&quot;hljs-string&quot;&gt;&quot; Reason: &quot;&lt;/span&gt; + code);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (numErrors &amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;5&lt;/span&gt;)&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;break&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (code == ErrorMapping.OffsetOutOfRangeCode()) {&lt;br/&gt;readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;continue&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;consumer.close();&lt;br/&gt;consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;;&lt;br/&gt;leadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;continue&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;numErrors = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; numRead = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition)) {&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; currentOffset = messageAndOffset.offset();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (currentOffset &amp;lt; readOffset) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Found an old offset: &quot;&lt;/span&gt; + currentOffset + &lt;span class=&quot;hljs-string&quot;&gt;&quot; Expecting: &quot;&lt;/span&gt; + readOffset);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;continue&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;readOffset = messageAndOffset.nextOffset();&lt;br/&gt;ByteBuffer payload = messageAndOffset.message().payload();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] bytes = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[payload.limit()];&lt;br/&gt;payload.get(bytes);&lt;br/&gt;System.out.println(String.valueOf(messageAndOffset.offset()) + &lt;span class=&quot;hljs-string&quot;&gt;&quot;: &quot;&lt;/span&gt; + &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; String(bytes, &lt;span class=&quot;hljs-string&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;));&lt;/p&gt;&lt;p&gt;numRead++;&lt;br/&gt;a_maxReads--;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (numRead == &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;try&lt;/span&gt; {&lt;br/&gt;Thread.sleep(&lt;span class=&quot;hljs-number&quot;&gt;1000&lt;/span&gt;);&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;catch&lt;/span&gt; (InterruptedException ie) {&lt;/p&gt;&lt;p&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (consumer != &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;)&lt;br/&gt;consumer.close();&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;getLastOffset&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(SimpleConsumer consumer, String topic, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; partition, &lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; whichTime, String clientName)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;TopicAndPartition topicAndPartition = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; TopicAndPartition(topic, partition);&lt;br/&gt;Map&amp;lt;TopicAndPartition, PartitionOffsetRequestInfo&amp;gt; requestInfo = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; HashMap&amp;lt;TopicAndPartition, PartitionOffsetRequestInfo&amp;gt;();&lt;br/&gt;requestInfo.put(topicAndPartition, &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; PartitionOffsetRequestInfo(whichTime, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;));&lt;/p&gt;&lt;p&gt;kafka.javaapi.OffsetRequest request = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; kafka.javaapi.OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientName);&lt;br/&gt;OffsetResponse response = consumer.getOffsetsBefore(request);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (response.hasError()) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Error fetching data Offset Data the Broker. Reason: &quot;&lt;/span&gt; + response.errorCode(topic, partition));&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt;[] offsets = response.offsets(topic, partition);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; offsets[&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;];&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; String &lt;span class=&quot;hljs-title&quot;&gt;findNewLeader&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String a_oldLeader, String a_topic, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_partition, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_port)&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;throws&lt;/span&gt; Exception &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;; i++) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;boolean&lt;/span&gt; goToSleep = &lt;span class=&quot;hljs-keyword&quot;&gt;false&lt;/span&gt;;&lt;br/&gt;PartitionMetadata metadata = findLeader(m_replicaBrokers, a_port, a_topic, a_partition);&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;goToSleep = &lt;span class=&quot;hljs-keyword&quot;&gt;true&lt;/span&gt;;&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (metadata.leader() == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;goToSleep = &lt;span class=&quot;hljs-keyword&quot;&gt;true&lt;/span&gt;;&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (a_oldLeader.equalsIgnoreCase(metadata.leader().host()) &amp;amp;&amp;amp; i == &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;) {&lt;br/&gt;goToSleep = &lt;span class=&quot;hljs-keyword&quot;&gt;true&lt;/span&gt;;&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; metadata.leader().host();&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (goToSleep) {&lt;br/&gt;Thread.sleep(&lt;span class=&quot;hljs-number&quot;&gt;1000&lt;/span&gt;);&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Unable to find new leader after Broker failure. Exiting&quot;&lt;/span&gt;);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Exception(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Unable to find new leader after Broker failure. Exiting&quot;&lt;/span&gt;);&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; PartitionMetadata &lt;span class=&quot;hljs-title&quot;&gt;findLeader&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(List&amp;lt;String&amp;gt; a_seedBrokers, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_port, String a_topic, &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; a_partition)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;PartitionMetadata returnMetaData = &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;loop:&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (String seed : a_seedBrokers) {&lt;br/&gt;SimpleConsumer consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;try&lt;/span&gt; {&lt;br/&gt;consumer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; SimpleConsumer(seed, a_port, &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;64&lt;/span&gt; * &lt;span class=&quot;hljs-number&quot;&gt;1024&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;leaderLookup&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;List&amp;lt;String&amp;gt; topics = Collections.singletonList(a_topic);&lt;br/&gt;TopicMetadataRequest req = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; TopicMetadataRequest(topics);&lt;br/&gt;kafka.javaapi.TopicMetadataResponse resp = consumer.send(req);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;List&amp;lt;TopicMetadata&amp;gt; topicMetadata = resp.topicsMetadata();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (TopicMetadata topic : topicMetadata) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (PartitionMetadata part : topic.partitionsMetadata()) {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (part.partitionId() == a_partition) {&lt;br/&gt;returnMetaData = part;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;break&lt;/span&gt; loop;&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Error communicating with Broker [&quot;&lt;/span&gt; + seed + &lt;span class=&quot;hljs-string&quot;&gt;&quot;] to find Leader for [&quot;&lt;/span&gt; + a_topic + &lt;span class=&quot;hljs-string&quot;&gt;&quot;, &quot;&lt;/span&gt; + a_partition + &lt;span class=&quot;hljs-string&quot;&gt;&quot;] Reason: &quot;&lt;/span&gt; + e);&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;finally&lt;/span&gt; {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (consumer != &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;)&lt;br/&gt;consumer.close();&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (returnMetaData != &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;m_replicaBrokers.clear();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (BrokerEndPoint replica : returnMetaData.replicas()) {&lt;br/&gt;m_replicaBrokers.add(replica.host());&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; returnMetaData;&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;h5kafkaproducerinterceptor&quot;&gt;&lt;span&gt;&lt;strong&gt;第5章 Kafka Producer拦截器(interceptor)&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h51&quot;&gt;&lt;span&gt;&lt;strong&gt;5.1 拦截器原理&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;  Producer 拦截器(interceptor)是在 Kafka 0.10 版本被引入的，主要用于实现 clients 端的定制化控制逻辑。&lt;br/&gt;  对于 producer 而言，interceptor 使得用户在消息发送前以及 producer 回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer 允许用户指定多个 interceptor 按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor 的实现接口是 &lt;code&gt;org.apache.kafka.clients.producer.ProducerInterceptor&lt;/code&gt;，其定义的方法包括：&lt;br/&gt;（1）configure(configs)：&lt;br/&gt;  获取配置信息和初始化数据时调用。&lt;br/&gt;（2）onSend(ProducerRecord)：&lt;br/&gt;  该方法封装进 KafkaProducer.send 方法中，即它运行在用户主线程中。Producer 确保在消息被序列化以及计算分区前调用该方法。&lt;code&gt;用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的 topic 和分区&lt;/code&gt;，否则会影响目标分区的计算。&lt;br/&gt;（3）onAcknowledgement(RecordMetadata, Exception)：&lt;br/&gt;  &lt;code&gt;该方法会在消息被应答或消息发送失败时调用&lt;/code&gt;，并且通常都是在 producer 回调逻辑触发之前。onAcknowledgement 运行在 producer 的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢 producer 的消息发送效率。&lt;br/&gt;（4）close：&lt;br/&gt;  &lt;code&gt;关闭 interceptor，主要用于执行一些资源清理工作。&lt;/code&gt;&lt;br/&gt;  如前所述，interceptor 可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外&lt;code&gt;倘若指定了多个 interceptor，则 producer 将按照指定顺序调用它们&lt;/code&gt;，并仅仅是捕获每个 interceptor 可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。&lt;/p&gt;
&lt;h3 id=&quot;h52&quot;&gt;&lt;span&gt;&lt;strong&gt;5.2 拦截器案例&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;1）需求：&lt;br/&gt;实现一个简单的双 interceptor 组成的拦截链。第一个 interceptor 会在消息发送前将时间戳信息加到消息 value 的最前部；第二个 interceptor 会在消息发送后更新成功发送消息数或失败发送消息数。&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0oh4.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;2）案例实操&lt;br/&gt;（1）增加时间戳拦截器&lt;/p&gt;
&lt;pre readability=&quot;14.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;23&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.interceptor;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerInterceptor;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.RecordMetadata;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Map;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;TimeInterceptor&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;ProducerInterceptor&lt;/span&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;hljs-title&quot;&gt;String&lt;/span&gt;&amp;gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(Map&amp;lt;String, ?&amp;gt; configs)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt; &lt;span class=&quot;hljs-title&quot;&gt;onSend&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(ProducerRecord&amp;lt;String, String&amp;gt; record)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(),&lt;br/&gt;System.currentTimeMillis() + &lt;span class=&quot;hljs-string&quot;&gt;&quot;,&quot;&lt;/span&gt; + record.value().toString());&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;onAcknowledgement&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(RecordMetadata metadata, Exception exception)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器&lt;/p&gt;
&lt;pre readability=&quot;15&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;24&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.interceptor;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerInterceptor;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerRecord;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.RecordMetadata;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Map;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;CounterInterceptor&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;ProducerInterceptor&lt;/span&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;hljs-title&quot;&gt;String&lt;/span&gt;&amp;gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; errorCounter = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; successCounter = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;configure&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(Map&amp;lt;String, ?&amp;gt; configs)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt; &lt;span class=&quot;hljs-title&quot;&gt;onSend&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(ProducerRecord&amp;lt;String, String&amp;gt; record)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; record;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;onAcknowledgement&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(RecordMetadata metadata, Exception exception)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (exception == &lt;span class=&quot;hljs-keyword&quot;&gt;null&lt;/span&gt;) {&lt;br/&gt;successCounter++;&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {&lt;br/&gt;errorCounter++;&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Successful sent: &quot;&lt;/span&gt; + successCounter);&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Failed sent: &quot;&lt;/span&gt; + errorCounter);&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（3）producer主程序&lt;/p&gt;
&lt;pre readability=&quot;23&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;40&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.interceptor;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.KafkaProducer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.Producer;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerConfig;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.clients.producer.ProducerRecord;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.ArrayList;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.List;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;InterceptorProducer&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;throws&lt;/span&gt; Exception &lt;/span&gt;{&lt;br/&gt;Properties props = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;acks&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;all&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;retries&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;batch.size&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;16384&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;linger.ms&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;buffer.memory&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;33554432&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;key.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;br/&gt;props.put(&lt;span class=&quot;hljs-string&quot;&gt;&quot;value.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-string&quot;&gt;&quot;org.apache.kafka.common.serialization.StringSerializer&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;List&amp;lt;String&amp;gt; interceptors = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ArrayList&amp;lt;String&amp;gt;();&lt;br/&gt;interceptors.add(&lt;span class=&quot;hljs-string&quot;&gt;&quot;com.atguigu.kafka.interceptor.TimeInterceptor&quot;&lt;/span&gt;);&lt;br/&gt;interceptors.add(&lt;span class=&quot;hljs-string&quot;&gt;&quot;com.atguigu.kafka.interceptor.CounterInterceptor&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;String topic = &lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;;&lt;br/&gt;Producer&amp;lt;String, String&amp;gt; producer = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaProducer&amp;lt;String, String&amp;gt;(props);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;10&lt;/span&gt;; i++) {&lt;/p&gt;&lt;p&gt;ProducerRecord&amp;lt;String, String&amp;gt; record = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProducerRecord&amp;lt;String, String&amp;gt;(topic, &lt;span class=&quot;hljs-string&quot;&gt;&quot;message&quot;&lt;/span&gt; + i);&lt;br/&gt;producer.send(record);&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;producer.close();&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;3）测试&lt;br/&gt;（1）在kafka上启动消费者，然后运行客户端java程序。&lt;/p&gt;
&lt;pre readability=&quot;10&quot;&gt;
&lt;code class=&quot;hljs coffeescript&quot; readability=&quot;14&quot;&gt;[atguigu@hadoop102 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt;-beginning --topic second&lt;p&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150698&lt;/span&gt;,message2&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150699&lt;/span&gt;,message5&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150701&lt;/span&gt;,message8&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150601&lt;/span&gt;,message0&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150699&lt;/span&gt;,message3&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150699&lt;/span&gt;,message6&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150701&lt;/span&gt;,message9&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150698&lt;/span&gt;,message1&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150699&lt;/span&gt;,message4&lt;br/&gt;&lt;span class=&quot;hljs-number&quot;&gt;1551784150701&lt;/span&gt;,message7&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）观察java平台控制台输出数据如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs nginx&quot;&gt;&lt;span class=&quot;hljs-attribute&quot;&gt;Successful&lt;/span&gt; sent: &lt;span class=&quot;hljs-number&quot;&gt;10&lt;/span&gt;&lt;br/&gt;Failed sent: &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;h6kafkastreams&quot;&gt;&lt;span&gt;&lt;strong&gt;第6章 Kafka Streams&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h61&quot;&gt;&lt;span&gt;&lt;strong&gt;6.1 概述&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h611kafkastreams&quot;&gt;&lt;span&gt;&lt;strong&gt;6.1.1 Kafka Streams&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  Kafka Streams。Apache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。&lt;/p&gt;
&lt;h4 id=&quot;h612kafkastreams&quot;&gt;&lt;span&gt;&lt;strong&gt;6.1.2 Kafka Streams 特点&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;1）功能强大&lt;br/&gt;  高扩展性，弹性，容错&lt;br/&gt;2）轻量级&lt;br/&gt;  无需专门的集群&lt;br/&gt;  一个库，而不是框架&lt;br/&gt;3）完全集成&lt;br/&gt;  100%的与Kafka 0.10.0版本兼容&lt;br/&gt;  易于集成到现有的应用程序&lt;br/&gt;4）实时性&lt;br/&gt;  毫秒级延迟&lt;br/&gt;  &lt;code&gt;并非微批处理&lt;/code&gt;，&lt;strong&gt;而spark是微处理框架&lt;/strong&gt;&lt;br/&gt;  窗口允许乱序数据&lt;br/&gt;  允许迟到数据&lt;/p&gt;
&lt;h4 id=&quot;h613kafkastream&quot;&gt;&lt;span&gt;&lt;strong&gt;6.1.3 为什么要有 Kafka Stream？&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;  当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有&lt;code&gt;Spark Streaming&lt;/code&gt; 和 &lt;code&gt;Apache Storm&lt;/code&gt;。Apache Storm 发展多年，应用广泛，提供记录级别的处理能力，当前也支持 SQL on Stream。而 Spark Streaming 基于 Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它 Spark 应用开发的用户而言使用门槛低。另外，目前主流的 Hadoop 发行版，如 Cloudera 和 Hortonworks，都集成 了Apache Storm 和 Apache Spark，使得部署更容易。&lt;br/&gt;  既然 Apache Spark 与 Apache Storm 拥用如此多的优势，那为何还需要 Kafka Stream 呢？主要有如下原因：&lt;/p&gt;
&lt;p&gt;  第一，Spark 和 Storm 都是流式处理框架，&lt;code&gt;而Kafka Stream提供的是一个基于Kafka的流式处理类库&lt;/code&gt;。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而 Kafka Stream 作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj079J.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;  第二，虽然 Cloudera 与 Hortonworks 方便了 Storm 和 Spark 的部署，但是这些框架的部署仍然相对复杂。而 &lt;code&gt;Kafka Stream 作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;  第三，就流式处理系统而言，基本都支持 Kafka 作为数据源。例如 Storm 具有专门的 kafka-spout，而 Spark 也提供专门的 spark-streaming-kafka 模块。事实上，Kafka 基本上是主流的流式处理系统的标准数据源。换言之，&lt;code&gt;大部分流式系统中都已部署了 Kafka，此时使用 Kafka Stream 的成本非常低&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;  第四，&lt;code&gt;使用 Storm 或 Spark Streaming 时，需要为框架本身的进程预留资源&lt;/code&gt;，如 Storm 的 supervisor 和 Spark on YARN 的 node manager。即使对于应用实例而言，框架本身也会占用部分资源，如 Spark Streaming 需要为 shuffle 和 storage 预留内存。但是 Kafka作为类库不占用系统资源。&lt;/p&gt;
&lt;p&gt;  第五，由于 &lt;code&gt;Kafka 本身提供数据持久化&lt;/code&gt;，因此 Kafka Stream 提供滚动部署和滚动升级以及重新计算的能力。&lt;/p&gt;
&lt;p&gt;  第六，由于 Kafka Consumer Rebalance 机制，&lt;code&gt;Kafka Stream 可以在线动态调整并行度&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&quot;h62kafkastream&quot;&gt;&lt;span&gt;&lt;strong&gt;6.2 Kafka Stream 数据清洗案例&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;0）需求：&lt;br/&gt;  实时处理单词带有”&amp;gt;&amp;gt;&amp;gt;”前缀的内容。例如输入”atguigu&amp;gt;&amp;gt;&amp;gt;ximenqing”，最终处理成“ximenqing”。&lt;br/&gt;1）需求分析：&lt;/p&gt;
&lt;img title=&quot;&quot; src=&quot;https://s2.ax1x.com/2019/03/05/kj0qj1.png&quot; alt=&quot;&quot;/&gt;&lt;br/&gt;2）案例实操&lt;br/&gt;（1）创建一个工程，并添加jar包或在pom文件中添加依赖
&lt;pre&gt;
&lt;code class=&quot;xml language-xml hljs&quot;&gt;    &lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;org.apache.kafka&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;groupId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;kafka-streams&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;artifactId&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;0.11.0.2&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;version&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-name&quot;&gt;dependency&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）创建主类&lt;/p&gt;
&lt;pre readability=&quot;20&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;34&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.stream;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.KafkaStreams;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.StreamsConfig;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.processor.Processor;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.processor.ProcessorSupplier;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.processor.TopologyBuilder;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; java.util.Properties;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;KafkaStream&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;br/&gt;String from = &lt;span class=&quot;hljs-string&quot;&gt;&quot;first&quot;&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;String to = &lt;span class=&quot;hljs-string&quot;&gt;&quot;second&quot;&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;Properties settings = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Properties();&lt;br/&gt;settings.put(StreamsConfig.APPLICATION_ID_CONFIG, &lt;span class=&quot;hljs-string&quot;&gt;&quot;logFilter&quot;&lt;/span&gt;);&lt;br/&gt;settings.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &lt;span class=&quot;hljs-string&quot;&gt;&quot;hadoop102:9092&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;StreamsConfig config = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; StreamsConfig(settings);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;TopologyBuilder builder = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; TopologyBuilder();&lt;/p&gt;&lt;p&gt;builder.addSource(&lt;span class=&quot;hljs-string&quot;&gt;&quot;SOURCE&quot;&lt;/span&gt;, from).addProcessor(&lt;span class=&quot;hljs-string&quot;&gt;&quot;PROCESS&quot;&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; ProcessorSupplier&amp;lt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[]&amp;gt;() {&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; Processor&amp;lt;&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[]&amp;gt; get() {&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; LogProcessor();&lt;br/&gt;}&lt;br/&gt;}, &lt;span class=&quot;hljs-string&quot;&gt;&quot;SOURCE&quot;&lt;/span&gt;)&lt;br/&gt;.addSink(&lt;span class=&quot;hljs-string&quot;&gt;&quot;SINK&quot;&lt;/span&gt;, to, &lt;span class=&quot;hljs-string&quot;&gt;&quot;PROCESS&quot;&lt;/span&gt;);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;KafkaStreams streams = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; KafkaStreams(builder, config);&lt;br/&gt;streams.start();&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（3）具体业务处理&lt;/p&gt;
&lt;pre readability=&quot;16&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;26&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;package&lt;/span&gt; com.atguigu.kafka.stream;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.processor.Processor;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; org.apache.kafka.streams.processor.ProcessorContext;&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;LogProcessor&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;Processor&lt;/span&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;byte&lt;/span&gt;[], &lt;span class=&quot;hljs-title&quot;&gt;byte&lt;/span&gt;[]&amp;gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;private&lt;/span&gt; ProcessorContext context;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(ProcessorContext context)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.context = context;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(&lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] key, &lt;span class=&quot;hljs-keyword&quot;&gt;byte&lt;/span&gt;[] value)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;String input = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; String(value);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (input.contains(&lt;span class=&quot;hljs-string&quot;&gt;&quot;&amp;gt;&amp;gt;&amp;gt;&quot;&lt;/span&gt;)) {&lt;br/&gt;input = input.split(&lt;span class=&quot;hljs-string&quot;&gt;&quot;&amp;gt;&amp;gt;&amp;gt;&quot;&lt;/span&gt;)[&lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;].trim();&lt;/p&gt;&lt;p&gt;&lt;br/&gt;context.forward(&lt;span class=&quot;hljs-string&quot;&gt;&quot;logProcessor&quot;&lt;/span&gt;.getBytes(), input.getBytes());&lt;br/&gt;} &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {&lt;br/&gt;context.forward(&lt;span class=&quot;hljs-string&quot;&gt;&quot;logProcessor&quot;&lt;/span&gt;.getBytes(), input.getBytes());&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;punctuate&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; l)&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-meta&quot;&gt;@Override&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;{&lt;/p&gt;&lt;p&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（4）运行程序&lt;br/&gt;（5）在hadoop104上启动生产者&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;code class=&quot;hljs ruby&quot; readability=&quot;2&quot;&gt;[atguigu@hadoop104 kafka]$ bin/kafka-console-producer.sh \&lt;br/&gt;--broker-list &lt;span class=&quot;hljs-symbol&quot;&gt;hadoop102:&lt;/span&gt;&lt;span class=&quot;hljs-number&quot;&gt;9092&lt;/span&gt; --topic first&lt;p&gt;&amp;gt;hello&lt;span class=&quot;hljs-meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;world&lt;br/&gt;&amp;gt;h&lt;span class=&quot;hljs-meta&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;atguigu&lt;br/&gt;&amp;gt;hahaha&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（6）在hadoop103上启动消费者&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop103 kafka]$ bin/kafka-&lt;span class=&quot;hljs-built_in&quot;&gt;console&lt;/span&gt;-consumer.sh \&lt;br/&gt;--zookeeper hadoop102:&lt;span class=&quot;hljs-number&quot;&gt;2181&lt;/span&gt; --&lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt;-beginning --topic second&lt;p&gt;world&lt;br/&gt;atguigu&lt;br/&gt;hahaha&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;h7&quot;&gt;&lt;span&gt;&lt;strong&gt;第7章 扩展知识&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3 id=&quot;h71kafkaflume&quot;&gt;&lt;span&gt;&lt;strong&gt;7.1 Kafka 与 Flume 比较&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;  在企业中必须要清楚流式数据采集框架 flume 和 kafka 的定位是什么：&lt;/p&gt;
&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;flume：Cloudera 公司研发：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;适合多个生产者；（一个生产者对应一个 Agent 任务）&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;适合下游数据消费者不多的情况；（多 channel 多 sink 会耗费很多内存）&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;适合数据安全性要求不高的操作；（实际中更多使用 Memory Channel）&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;适合与 Hadoop 生态圈对接的操作。（Cloudera 公司的特长）&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;kafka：Linkedin 公司研发：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;适合数据下游消费者众多的情况；（开启更多的消费者任务即可，与 Kafka 集群无关）&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;适合数据安全性要求较高的操作，支持replication。（数据放在磁盘里）&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;因此我们常用的一种模型是：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;线上数据 --&amp;gt; flume(适合采集tomcat日志) --&amp;gt; kafka(离线/实时) --&amp;gt; flume(根据情景增删该流程) --&amp;gt; HDFS&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;h72flumekafka&quot;&gt;&lt;span&gt;&lt;strong&gt;7.2 Flume 与 kafka 集成&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;1）配置flume(flume-kafka.conf)&lt;/p&gt;
&lt;pre readability=&quot;10&quot;&gt;
&lt;code class=&quot;conf language-conf&quot; readability=&quot;14&quot;&gt;# define&lt;br/&gt;a1.sources = r1&lt;br/&gt;a1.sinks = k1&lt;br/&gt;a1.channels = c1&lt;p&gt;# source&lt;br/&gt;a1.sources.r1.type = exec&lt;br/&gt;a1.sources.r1.command = tail -F -c +0 /opt/module/datas/flume.log&lt;br/&gt;a1.sources.r1.shell = /bin/bash -c&lt;/p&gt;&lt;p&gt;# sink&lt;br/&gt;a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink&lt;br/&gt;a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092&lt;br/&gt;a1.sinks.k1.kafka.topic = first&lt;br/&gt;a1.sinks.k1.kafka.flumeBatchSize = 20&lt;br/&gt;a1.sinks.k1.kafka.producer.acks = 1&lt;br/&gt;a1.sinks.k1.kafka.producer.linger.ms = 1&lt;/p&gt;&lt;p&gt;# channel&lt;br/&gt;a1.channels.c1.type = memory&lt;br/&gt;a1.channels.c1.capacity = 1000&lt;br/&gt;a1.channels.c1.transactionCapacity = 100&lt;/p&gt;&lt;p&gt;# bind&lt;br/&gt;a1.sources.r1.channels = c1&lt;br/&gt;a1.sinks.k1.channel = c1&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;2） 启动kafka IDEA消费者&lt;br/&gt;3） 进入flume根目录下，启动flume&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs coffeescript&quot;&gt;[atguigu@hadoop102 flume]$ bin&lt;span class=&quot;hljs-regexp&quot;&gt;/flume-ng agent -n a1 -c conf/&lt;/span&gt; -f job/flume-kafka.conf &lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;4） 向 /opt/module/datas/flume.log里追加数据，查看kafka消费者消费情况&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 datas]$$ echo hello &amp;gt; &lt;span class=&quot;hljs-regexp&quot;&gt;/opt/module&lt;/span&gt;&lt;span class=&quot;hljs-regexp&quot;&gt;/datas/flume&lt;/span&gt;.log&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;h73kafka&quot;&gt;&lt;span&gt;&lt;strong&gt;7.3 Kafka配置信息&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 id=&quot;h731broker&quot;&gt;&lt;span&gt;&lt;strong&gt;7.3.1 Broker 配置信息&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;默认值&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;96&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;code&gt;broker.id&lt;/code&gt;&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt;&lt;code&gt;必填参数，broker的唯一标识&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;&lt;code&gt;log.dirs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;/tmp/kafka-logs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Kafka 数据存放的目录。可以指定多个目录，中间用逗号分隔，当新partition被创建的时会被存放到当前存放partition最少的目录。&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;code&gt;port&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;9092&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;BrokerServer接受客户端连接的端口号&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td&gt;&lt;code&gt;zookeeper.connect&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;null&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Zookeeper的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个zookeeper路径来存放此kafka集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path 。需要注意的是，消费者的参数要和此参数一致。&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;message.max.bytes&lt;/td&gt;
&lt;td&gt;1000000&lt;/td&gt;
&lt;td&gt;服务器可以接收到的最大的消息大小。注意此参数要和consumer的maximum.message.size大小一致，否则会因为生产者生产的消息太大导致消费者无法消费。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;num.io.threads&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;服务器用来执行读写请求的IO线程数，此参数的数量至少要等于服务器上磁盘的数量。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;queued.max.requests&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;I/O线程可以处理请求的队列大小，若实际请求数超过此大小，网络线程将停止接收新的请求。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;socket.send.buffer.bytes&lt;/td&gt;
&lt;td&gt;100 * 1024&lt;/td&gt;
&lt;td&gt;The SO_SNDBUFF buffer the server prefers for socket connections.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;socket.receive.buffer.bytes&lt;/td&gt;
&lt;td&gt;100 * 1024&lt;/td&gt;
&lt;td&gt;The SO_RCVBUFF buffer the server prefers for socket connections.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;socket.request.max.bytes&lt;/td&gt;
&lt;td&gt;100 * 1024 * 1024&lt;/td&gt;
&lt;td&gt;服务器允许请求的最大值， 用来防止内存溢出，其值应该小于 Java heap size.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;num.partitions&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;默认partition数量，如果topic在创建时没有指定partition数量，默认使用此值，建议改为5&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;log.segment.bytes&lt;/td&gt;
&lt;td&gt;1024 * 1024 * 1024&lt;/td&gt;
&lt;td&gt;Segment文件的大小，超过此值将会自动新建一个segment，此值可以被topic级别的参数覆盖。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;log.roll.{ms,hours}&lt;/td&gt;
&lt;td&gt;24 * 7 hours&lt;/td&gt;
&lt;td&gt;新建segment文件的时间，此值可以被topic级别的参数覆盖。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;log.retention.{ms,minutes,hours}&lt;/td&gt;
&lt;td&gt;7 days&lt;/td&gt;
&lt;td&gt;Kafka segment log的保存周期，保存周期超过此时间日志就会被删除。此参数可以被topic级别参数覆盖。数据量大时，建议减小此值。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;log.retention.bytes&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;每个partition的最大容量，若数据量超过此值，partition数据将会被删除。注意这个参数控制的是每个partition而不是topic。此参数可以被log级别参数覆盖。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;log.retention.check.interval.ms&lt;/td&gt;
&lt;td&gt;5 minutes&lt;/td&gt;
&lt;td&gt;删除策略的检查周期&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;auto.create.topics.enable&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;自动创建topic参数，建议此值设置为false，严格控制topic管理，防止生产者错写topic。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;default.replication.factor&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;默认副本数量，建议改为2。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;replica.lag.time.max.ms&lt;/td&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;在此窗口时间内没有收到follower的fetch请求，leader会将其从ISR(in-sync replicas)中移除。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;replica.lag.max.messages&lt;/td&gt;
&lt;td&gt;4000&lt;/td&gt;
&lt;td&gt;如果replica节点落后leader节点此值大小的消息数量，leader节点就会将其从ISR中移除。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;replica.socket.timeout.ms&lt;/td&gt;
&lt;td&gt;30 * 1000&lt;/td&gt;
&lt;td&gt;replica向leader发送请求的超时时间。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;replica.socket.receive.buffer.bytes&lt;/td&gt;
&lt;td&gt;64 * 1024&lt;/td&gt;
&lt;td&gt;The socket receive buffer for network requests to the leader for replicating data.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;replica.fetch.max.bytes&lt;/td&gt;
&lt;td&gt;1024 * 1024&lt;/td&gt;
&lt;td&gt;The number of byes of messages to attempt to fetch for each partition in the fetch requests the replicas send to the leader.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;replica.fetch.wait.max.ms&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;The maximum amount of time to wait time for data to arrive on the leader in the fetch requests sent by the replicas to the leader.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;num.replica.fetchers&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Number of threads used to replicate messages from leaders. Increasing this value can increase the degree of I/O parallelism in the follower broker.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;fetch.purgatory.purge.interval.requests&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;The purge interval (in number of requests) of the fetch request purgatory.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;zookeeper.session.timeout.ms&lt;/td&gt;
&lt;td&gt;6000&lt;/td&gt;
&lt;td&gt;ZooKeeper session 超时时间。如果在此时间内server没有向zookeeper发送心跳，zookeeper就会认为此节点已挂掉。 此值太低导致节点容易被标记死亡；若太高，.会导致太迟发现节点死亡。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;zookeeper.connection.timeout.ms&lt;/td&gt;
&lt;td&gt;6000&lt;/td&gt;
&lt;td&gt;客户端连接zookeeper的超时时间。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;zookeeper.sync.time.ms&lt;/td&gt;
&lt;td&gt;2000&lt;/td&gt;
&lt;td&gt;H ZK follower落后 ZK leader的时间。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;controlled.shutdown.enable&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;允许broker shutdown。如果启用，broker在关闭自己之前会把它上面的所有leaders转移到其它brokers上，建议启用，增加集群稳定性。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;auto.leader.rebalance.enable&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;If this is enabled the controller will automatically try to balance leadership for partitions among the brokers by periodically returning leadership to the “preferred” replica for each partition if it is available.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;leader.imbalance.per.broker.percentage&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;The percentage of leader imbalance allowed per broker. The controller will rebalance leadership if this ratio goes above the configured value per broker.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;leader.imbalance.check.interval.seconds&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;The frequency with which to check for leader imbalance.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;offset.metadata.max.bytes&lt;/td&gt;
&lt;td&gt;4096&lt;/td&gt;
&lt;td&gt;The maximum amount of metadata to allow clients to save with their offsets.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;connections.max.idle.ms&lt;/td&gt;
&lt;td&gt;600000&lt;/td&gt;
&lt;td&gt;Idle connections timeout: the server socket processor threads close the connections that idle more than this.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;num.recovery.threads.per.data.dir&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;unclean.leader.election.enable&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;code&gt;delete.topic.enable&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;启用deletetopic参数，建议设置为true。&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;offsets.topic.num.partitions&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;The number of partitions for the offset commit topic. Since changing this after deployment is currently unsupported, we recommend using a higher setting for production (e.g., 100-200).&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;offsets.topic.retention.minutes&lt;/td&gt;
&lt;td&gt;1440&lt;/td&gt;
&lt;td&gt;Offsets that are older than this age will be marked for deletion. The actual purge will occur when the log cleaner compacts the offsets topic.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;offsets.retention.check.interval.ms&lt;/td&gt;
&lt;td&gt;600000&lt;/td&gt;
&lt;td&gt;The frequency at which the offset manager checks for stale offsets.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;offsets.topic.replication.factor&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;The replication factor for the offset commit topic. A higher setting (e.g., three or four) is recommended in order to ensure higher availability. If the offsets topic is created when fewer brokers than the replication factor then the offsets topic will be created with fewer replicas.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;offsets.topic.segment.bytes&lt;/td&gt;
&lt;td&gt;104857600&lt;/td&gt;
&lt;td&gt;Segment size for the offsets topic. Since it uses a compacted topic, this should be kept relatively low in order to facilitate faster log compaction and loads.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;offsets.load.buffer.size&lt;/td&gt;
&lt;td&gt;5242880&lt;/td&gt;
&lt;td&gt;An offset load occurs when a broker becomes the offset manager for a set of consumer groups (i.e., when it becomes a leader for an offsets topic partition). This setting corresponds to the batch size (in bytes) to use when reading from the offsets segments when loading offsets into the offset manager’s cache.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;offsets.commit.required.acks&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;The number of acknowledgements that are required before the offset commit can be accepted. This is similar to the producer’s acknowledgement setting. In general, the default should not be overridden.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;offsets.commit.timeout.ms&lt;/td&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;The offset commit will be delayed until this timeout or the required number of replicas have received the offset commit. This is similar to the producer request timeout.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h4 id=&quot;h732producer&quot;&gt;&lt;span&gt;&lt;strong&gt;7.3.2 Producer 配置信息&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;默认值&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;33&quot;&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;metadata.broker.list&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt;启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;request.required.acks&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;参见3.2节介绍&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;request.timeout.ms&lt;/td&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;producer.type&lt;/td&gt;
&lt;td&gt;sync&lt;/td&gt;
&lt;td&gt;同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;serializer.class&lt;/td&gt;
&lt;td&gt;kafka.serializer.DefaultEncoder&lt;/td&gt;
&lt;td&gt;序列号类，.默认序列化成 byte[] 。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;key.serializer.class&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt;Key的序列化类，默认同上。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;partitioner.class&lt;/td&gt;
&lt;td&gt;kafka.producer.DefaultPartitioner&lt;/td&gt;
&lt;td&gt;Partition类，默认对key进行hash。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;compression.codec&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;指定producer消息的压缩格式，可选参数为： “none”, “gzip” and “snappy”。关于压缩参见4.1节&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;compressed.topics&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;message.send.max.retries&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Producer发送失败时重试次数。若网络出现问题，可能会导致不断重试。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;retry.backoff.ms&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;11&quot;&gt;&lt;td&gt;topic.metadata.refresh.interval.ms&lt;/td&gt;
&lt;td&gt;600 * 1000&lt;/td&gt;
&lt;td&gt;The producer generally refreshes the topic metadata from brokers when there is a failure (partition missing, leader not available…). It will also poll regularly (default: every 10min so 600000ms). If you set this to a negative value, metadata will only get refreshed on failure. If you set this to zero, the metadata will get refreshed after each message sent (not recommended). Important note: the refresh happen only AFTER the message is sent, so if the producer never sends a message the metadata is never refreshed&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;queue.buffering.max.ms&lt;/td&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;启用异步模式时，producer缓存消息的时间。比如我们设置成1000时，它会缓存1秒的数据再一次发送出去，这样可以极大的增加broker吞吐量，但也会造成时效性的降低。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;queue.buffering.max.messages&lt;/td&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;采用异步模式时producer buffer 队列里最大缓存的消息数量，如果超过这个数值，producer就会阻塞或者丢掉消息。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;queue.enqueue.timeout.ms&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;当达到上面参数值时producer阻塞等待的时间。如果值设置为0，buffer队列满时producer不会阻塞，消息直接被丢掉。若值设置为-1，producer会被阻塞，不会丢消息。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;batch.num.messages&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;采用异步模式时，一个batch缓存的消息数量。达到这个数量值时producer才会发送消息。&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;send.buffer.bytes&lt;/td&gt;
&lt;td&gt;100 * 1024&lt;/td&gt;
&lt;td&gt;Socket write buffer size&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;client.id&lt;/td&gt;
&lt;td&gt;“”&lt;/td&gt;
&lt;td&gt;The client id is a user-specified string sent in each request to help trace calls. It should logically identify the application making the request.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h4 id=&quot;h733consumer&quot;&gt;&lt;span&gt;&lt;strong&gt;7.3.3 Consumer 配置信息&lt;/strong&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;属性&lt;/th&gt;
&lt;th&gt;默认值&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;34.5&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;group.id&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt;Consumer的组ID，相同goup.id的consumer属于同一个组。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;zookeeper.connect&lt;/td&gt;
&lt;td&gt; &lt;/td&gt;
&lt;td&gt;Consumer的zookeeper连接串，要和broker的配置一致。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;consumer.id&lt;/td&gt;
&lt;td&gt;null&lt;/td&gt;
&lt;td&gt;如果不设置会自动生成。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;socket.timeout.ms&lt;/td&gt;
&lt;td&gt;30 * 1000&lt;/td&gt;
&lt;td&gt;网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;socket.receive.buffer.bytes&lt;/td&gt;
&lt;td&gt;64 * 1024&lt;/td&gt;
&lt;td&gt;The socket receive buffer for network requests.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;fetch.message.max.bytes&lt;/td&gt;
&lt;td&gt;1024 * 1024&lt;/td&gt;
&lt;td&gt;查询topic-partition时允许的最大消息大小。consumer会为每个partition缓存此大小的消息到内存，因此，这个参数可以控制consumer的内存使用量。这个值应该至少比server允许的最大消息大小大，以免producer发送的消息大于consumer允许的消息。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;num.consumer.fetchers&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;The number fetcher threads used to fetch data.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;auto.commit.enable&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;auto.commit.interval.ms&lt;/td&gt;
&lt;td&gt;60 * 1000&lt;/td&gt;
&lt;td&gt;Consumer提交offset值到zookeeper的周期。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;queued.max.message.chunks&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;auto.commit.interval.ms&lt;/td&gt;
&lt;td&gt;60 * 1000&lt;/td&gt;
&lt;td&gt;Consumer提交offset值到zookeeper的周期。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;queued.max.message.chunks&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;fetch.min.bytes&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;The minimum amount of data the server should return for a fetch request. If insufficient data is available the request will wait for that much data to accumulate before answering the request.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;fetch.wait.max.ms&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy fetch.min.bytes.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;rebalance.backoff.ms&lt;/td&gt;
&lt;td&gt;2000&lt;/td&gt;
&lt;td&gt;Backoff time between retries during rebalance.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;refresh.leader.backoff.ms&lt;/td&gt;
&lt;td&gt;200&lt;/td&gt;
&lt;td&gt;Backoff time to wait before trying to determine the leader of a partition that has just lost its leader.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;auto.offset.reset&lt;/td&gt;
&lt;td&gt;largest&lt;/td&gt;
&lt;td&gt;What to do when there is no initial offset in ZooKeeper or if an offset is out of range ;smallest : automatically reset the offset to the smallest offset; largest : automatically reset the offset to the largest offset;anything else: throw exception to the consumer&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;consumer.timeout.ms&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;若在指定时间内没有消息消费，consumer将会抛出异常。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;exclude.internal.topics&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;Whether messages from internal topics (such as offsets) should be exposed to the consumer.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;zookeeper.session.timeout.ms&lt;/td&gt;
&lt;td&gt;6000&lt;/td&gt;
&lt;td&gt;ZooKeeper session timeout. If the consumer fails to heartbeat to ZooKeeper for this period of time it is considered dead and a rebalance will occur.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;zookeeper.connection.timeout.ms&lt;/td&gt;
&lt;td&gt;6000&lt;/td&gt;
&lt;td&gt;The max time that the client waits while establishing a connection to zookeeper.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;zookeeper.sync.time.ms&lt;/td&gt;
&lt;td&gt;2000&lt;/td&gt;
&lt;td&gt;How far a ZK follower can be behind a ZK leader&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3 id=&quot;h74kafkaoffset&quot;&gt;&lt;span&gt;&lt;strong&gt;7.4 如何查看 Kafka 集群维护的 offset 信息&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;步骤：&lt;br/&gt;（1）修改配置文件consumer.properties，增加一个属性&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;code class=&quot;hljs ruby&quot; readability=&quot;2&quot;&gt;[atguigu@hadoop102 config]$ pwd&lt;br/&gt;/opt/&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;module&lt;/span&gt;/&lt;span class=&quot;hljs-title&quot;&gt;kafka&lt;/span&gt;/&lt;span class=&quot;hljs-title&quot;&gt;config&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;[atguigu@hadoop102 config]$ vim consumer.properties&lt;p&gt;exclude.internal.topics=&lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（2）分发配置好的额文件&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs ruby&quot;&gt;[atguigu@hadoop102 config]$ xsync consumer.properties&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;（3）执行新的消费者命令&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs diff&quot;&gt;bin/kafka-console-consumer.sh \&lt;br/&gt;&lt;span class=&quot;hljs-deletion&quot;&gt;--zookeeper hadoop102:2181 --topic __consumer_offsets \&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-deletion&quot;&gt;--formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; \&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-deletion&quot;&gt;--consumer.config config/consumer.properties \&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;hljs-deletion&quot;&gt;--from-beginning &lt;/span&gt;&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Tue, 05 Mar 2019 16:00:00 +0000</pubDate>
<dc:creator>黑泽君</dc:creator>
<og:description>第1章 Kafka概述1.1 消息队列1.2 为什么需要消息队列1.3 什么是Kafka1.4 Kafka架构第2章 Kafka集群部署2.1 环境准备2.1.1 集群规划2.1.2 jar包下载2.</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/chenmingjun/p/10480793.html</dc:identifier>
</item>
<item>
<title>Pytorch入门实战二：LeNet、AleNet、VGG、GoogLeNet、ResNet模型详解 - 泽积</title>
<link>http://www.cnblogs.com/shenpings1314/p/10468418.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/shenpings1314/p/10468418.html</guid>
<description>&lt;p&gt;&lt;strong&gt;&lt;span lang=&quot;EN-US&quot;&gt;LeNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;　　1998年，&lt;span lang=&quot;EN-US&quot;&gt;LeCun提出了第一个真正的卷积神经网络，也是整个神经网络的开山之作，称为&lt;span lang=&quot;EN-US&quot;&gt;LeNet，现在主要指的是&lt;span lang=&quot;EN-US&quot;&gt;LeNet5或&lt;span lang=&quot;EN-US&quot;&gt;LeNet-5，如图1.1所示。它的主要特征是将卷积层和下采样层相结合作为网络的基本机构，如果不计输入层，该模型共&lt;span lang=&quot;EN-US&quot;&gt;7层，包括&lt;span lang=&quot;EN-US&quot;&gt;2个卷积层，&lt;span lang=&quot;EN-US&quot;&gt;2个下采样层，&lt;span lang=&quot;EN-US&quot;&gt;3个全连接层。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190303225522945-1713391211.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图1.1&lt;/p&gt;
&lt;p&gt;　　注：由于在接入全连接层时，要将池化层的输出转换成全连接层需要的维度，因此，必须清晰的知道全连接层前feature map的大小。卷积层与池化层输出的图像大小，其计算如图&lt;span lang=&quot;EN-US&quot;&gt;1.2所示。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190303225807748-569818319.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图1.2&lt;/p&gt;
&lt;p&gt; 　　本次利用&lt;span lang=&quot;EN-US&quot;&gt;pytorch实现整个&lt;span lang=&quot;EN-US&quot;&gt;LeNet模型，图中的&lt;span lang=&quot;EN-US&quot;&gt;Subsampling层即可看作如今的池化层，最后一层（输出层）也当作全连接层进行处理。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;57&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch as torch
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn as nn
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; LeNet(nn.Module):
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;(self):
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;         super(LeNet,self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;         layer1 =&lt;span&gt; nn.Sequential()
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;         layer1.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;conv1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.Conv2d(1,6,5&lt;span&gt;))
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;         layer1.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.MaxPool2d(2,2&lt;span&gt;))
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;         self.layer1 =&lt;span&gt; layer1
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; 
&lt;span&gt;11&lt;/span&gt;         layer2 =&lt;span&gt; nn.Sequential()
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;         layer2.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;conv2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.Conv2d(6,16,5&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;         layer2.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.MaxPool2d(2,2&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;         self.layer2 =&lt;span&gt; layer2
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; 
&lt;span&gt;16&lt;/span&gt;         layer3 =&lt;span&gt; nn.Sequential()
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;         layer3.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;fc1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.Linear(16*5*5,120&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         layer3.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;fc2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.Linear(120,84&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;         layer3.add_module(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;fc3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,nn.Linear(84,10&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         self.layer3 =&lt;span&gt; layer3
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; 
&lt;span&gt;22&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;         x =&lt;span&gt; self.layer1(x)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;         x =&lt;span&gt; self.layer2(x)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         x = x.view(x.size(0),-1&lt;span&gt;)#转换（降低）数据维度，进入全连接层
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;         x =&lt;span&gt; self.layer3(x)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt;&lt;span&gt; x
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt;代入数据检验&lt;/span&gt;
&lt;span&gt;29&lt;/span&gt; y = torch.randn(1,1,32,32&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; model =&lt;span&gt; LeNet()
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; model(y)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt; AlexNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　在&lt;span lang=&quot;EN-US&quot;&gt;2010年，斯坦福大学的李飞飞正式组织并启动了大规模视觉图像识别竞赛（&lt;span lang=&quot;EN-US&quot;&gt;ImageNet Large Scale Visual Recognition Challenge，&lt;span lang=&quot;EN-US&quot;&gt;ILSVRC）。在&lt;span lang=&quot;EN-US&quot;&gt;2012年，&lt;span lang=&quot;EN-US&quot;&gt;Alex Krizhevsky、&lt;span lang=&quot;EN-US&quot;&gt;Ilya Sutskever提出了一种非常重要的卷积神经网络模型，它就是&lt;span lang=&quot;EN-US&quot;&gt;AlexNet，如图1.3所　　示，在&lt;span lang=&quot;EN-US&quot;&gt;ImageNet竞赛上大放异彩，领先第二名&lt;span lang=&quot;EN-US&quot;&gt;10%的准确率夺得了冠军，吸引了学术界与工业界的广泛关注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;　　&lt;strong&gt;AlexNet神经网络相比&lt;span lang=&quot;EN-US&quot;&gt;LeNet：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span lang=&quot;EN-US&quot;&gt;1、 使用&lt;span lang=&quot;EN-US&quot;&gt;ReLU激活函数。在&lt;span lang=&quot;EN-US&quot;&gt;AlexNet之前，神经网络一般都使用&lt;span lang=&quot;EN-US&quot;&gt;sigmoid或&lt;span lang=&quot;EN-US&quot;&gt;tanh作为激活函数，这类函数在自变量非常大或者非常小时，函数输出基本不变，称之为饱和函数。为了提高训练速度，&lt;span lang=&quot;EN-US&quot;&gt;AlexNet使用了修正线性函数&lt;span lang=&quot;EN-US&quot;&gt;ReLU，它是一种非饱和函数，与 &lt;span lang=&quot;EN-US&quot;&gt;sigmoid 和&lt;span lang=&quot;EN-US&quot;&gt;tanh 函数相比，&lt;span lang=&quot;EN-US&quot;&gt;ReLU分片的线性结构实现了非线性结构的表达能力，梯度消失现象相对较弱，有助于训练更深层的网络。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span lang=&quot;EN-US&quot;&gt;2、 使用&lt;span lang=&quot;EN-US&quot;&gt;GPU训练。与&lt;span lang=&quot;EN-US&quot;&gt;CPU不同的是，&lt;span lang=&quot;EN-US&quot;&gt;GPU转为执行复杂的数学和几何计算而设计，&lt;span lang=&quot;EN-US&quot;&gt;AlexNet使用了&lt;span lang=&quot;EN-US&quot;&gt;2个&lt;span lang=&quot;EN-US&quot;&gt;GPU来提升速度，分别放置一半卷积核。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span lang=&quot;EN-US&quot;&gt;3、 局部响应归一化。&lt;span lang=&quot;EN-US&quot;&gt;AlexNet使用局部响应归一化技巧，将&lt;span lang=&quot;EN-US&quot;&gt;ImageNet上的&lt;span lang=&quot;EN-US&quot;&gt;top-1与&lt;span lang=&quot;EN-US&quot;&gt;top-5错误率分别减少了&lt;span lang=&quot;EN-US&quot;&gt;1.4%和&lt;span lang=&quot;EN-US&quot;&gt;1.2%。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span lang=&quot;EN-US&quot;&gt;4、 重叠池化层。与不重叠池化层相比，重叠池化层有助于缓解过拟合，使得&lt;span lang=&quot;EN-US&quot;&gt;AlexNet的&lt;span lang=&quot;EN-US&quot;&gt;top-1和&lt;span lang=&quot;EN-US&quot;&gt;top-5错误率分别降低了&lt;span lang=&quot;EN-US&quot;&gt;0.4%和&lt;span lang=&quot;EN-US&quot;&gt;0.3%。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span lang=&quot;EN-US&quot;&gt;5、 减少过拟合。&lt;span lang=&quot;EN-US&quot;&gt;AlexNet使用了数据扩增与丢失输出两种技巧。数据扩增：&lt;span lang=&quot;EN-US&quot;&gt;a、图像的平移、翻转，&lt;span lang=&quot;EN-US&quot;&gt;b、基于&lt;span lang=&quot;EN-US&quot;&gt;PCA的&lt;span lang=&quot;EN-US&quot;&gt;RGB强度调整。丢失输出技巧（&lt;span lang=&quot;EN-US&quot;&gt;DropOut层），&lt;span lang=&quot;EN-US&quot;&gt;AlexNet以&lt;span lang=&quot;EN-US&quot;&gt;0.5的概率将两个全连接层神经元的输出设置为&lt;span lang=&quot;EN-US&quot;&gt;0，有效阻止了过拟合现象的发生。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190304000803901-518573141.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图1.3&lt;/p&gt;
&lt;p&gt;　　利用pytorch实现&lt;span lang=&quot;EN-US&quot;&gt;AlexNet网络，由于当时，&lt;span lang=&quot;EN-US&quot;&gt;GPU的计算能力不强，因此&lt;span lang=&quot;EN-US&quot;&gt;Alex采用了&lt;span lang=&quot;EN-US&quot;&gt;2个&lt;span lang=&quot;EN-US&quot;&gt;GPU并行来计算，如今的&lt;span lang=&quot;EN-US&quot;&gt;GPU计算能力，完全可以替代。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;79&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn as nn
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; 
&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; AlexNet(nn.Module):
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;(self,num_classes):
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;         super(AlexNet,self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;         self.features =&lt;span&gt; nn.Sequential(
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;             nn.Conv2d(3,64,11,4,padding=2&lt;span&gt;),
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;             &lt;span&gt;#&lt;/span&gt;&lt;span&gt; inplace=&lt;span data-mce-=&quot;&quot;&gt;True&lt;/span&gt;，是对于Conv2d这样的上层网络传递下来的tensor直接进行修改，好处就是可以节省运算内存，不用多储存变量&lt;/span&gt;
&lt;span&gt;10&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;             nn.MaxPool2d(kernel_size=3,stride=2&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; 
&lt;span&gt;13&lt;/span&gt;             nn.Conv2d(64,192,kernel_size=5,padding=2&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;             nn.MaxPool2d(kernel_size=3,stride=2&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; 
&lt;span&gt;17&lt;/span&gt;             nn.Conv2d(192,384,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;             nn.Conv2d(384,256,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; 
&lt;span&gt;22&lt;/span&gt;             nn.Conv2d(256,256,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;             nn.MaxPool2d(kernel_size=3,stride=1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt; &lt;span&gt;        )
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;         self.classifier =&lt;span&gt; nn.Sequential(
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;            nn.Dropout(),
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;             nn.Linear(256*6*6,4096&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; &lt;span&gt;            nn.Dropout(),
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;             nn.Linear(4096,4096&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             nn.Linear(4096&lt;span&gt;,num_classes)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; &lt;span&gt;        )
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;         x =&lt;span&gt; self.features(x)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;         x = x.view(x.size(0),-1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;         x =&lt;span&gt; self.classifier(x)
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; x
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;span lang=&quot;EN-US&quot;&gt;VGGNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　在&lt;span lang=&quot;EN-US&quot;&gt;2014年，参加&lt;span lang=&quot;EN-US&quot;&gt;ILSVRC竞赛的“&lt;span lang=&quot;EN-US&quot;&gt;VGG”队在&lt;span lang=&quot;EN-US&quot;&gt;ImageNet上获得了比赛的亚军。&lt;span lang=&quot;EN-US&quot;&gt;VGG的核心思想是利用较小的卷积核来增加网络的深度。常用的有&lt;span lang=&quot;EN-US&quot;&gt;VGG16、&lt;span lang=&quot;EN-US&quot;&gt;VGG19两种类型。&lt;span lang=&quot;EN-US&quot;&gt;VGG16拥有&lt;span lang=&quot;EN-US&quot;&gt;13个卷积层（核大小均为&lt;span lang=&quot;EN-US&quot;&gt;3*3），&lt;span lang=&quot;EN-US&quot;&gt;5个最大池化层，&lt;span lang=&quot;EN-US&quot;&gt;3个全连接层。&lt;span lang=&quot;EN-US&quot;&gt;VGG19拥有&lt;span lang=&quot;EN-US&quot;&gt;16个卷积层（核大小均为&lt;span lang=&quot;EN-US&quot;&gt;3*3），&lt;span lang=&quot;EN-US&quot;&gt;5个最大池化层，&lt;span lang=&quot;EN-US&quot;&gt;3个全连接层，如图1.4所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190304220838188-1483249144.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图1.4&lt;/p&gt;
&lt;p&gt;　　加深结构都使用&lt;span lang=&quot;EN-US&quot;&gt;ReLU激活函数，&lt;span lang=&quot;EN-US&quot;&gt;VGG19比&lt;span lang=&quot;EN-US&quot;&gt;VGG16的区别在于多了&lt;span lang=&quot;EN-US&quot;&gt;3个卷积层，利用&lt;span lang=&quot;EN-US&quot;&gt;pytorch实现整&lt;span lang=&quot;EN-US&quot;&gt;VG16模型，&lt;span lang=&quot;EN-US&quot;&gt;VGG19同理。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;113&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch as torch
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn as nn
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; 
&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; VGG16(nn.Module):
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;(self,num_classes):
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;         super(VGG16,self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;         self.features =&lt;span&gt; nn.Sequential(
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;             nn.Conv2d(3,64,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;             nn.Conv2d(64,64,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; 
&lt;span&gt;13&lt;/span&gt;             nn.Conv2d(64,128,kernel_size=3,padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;             nn.Conv2d(128, 128, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; 
&lt;span&gt;18&lt;/span&gt;             nn.Conv2d(128, 256, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;             nn.Conv2d(256, 256, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;             nn.Conv2d(256, 256, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt; 
&lt;span&gt;25&lt;/span&gt;             nn.Conv2d(256, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;             nn.Conv2d(512, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;             nn.Conv2d(512, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; 
&lt;span&gt;32&lt;/span&gt;             nn.Conv2d(512, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;             nn.Conv2d(512, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;             nn.Conv2d(512, 512, kernel_size=3, padding=1&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True)
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt; &lt;span&gt;        )
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt; 
&lt;span&gt;40&lt;/span&gt;         self.classifier =&lt;span&gt; nn.Sequential(
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;             nn.Linear(512*7*7,4096&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;             nn.ReLU(inplace=&lt;span&gt;True),
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt; &lt;span&gt;            nn.Dropout(),
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt;             nn.Linear(4096,4096&lt;span&gt;),
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; &lt;span&gt;            nn.ReLU(True),
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;            nn.Dropout(),
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; 
&lt;span&gt;49&lt;/span&gt;             nn.Linear(4096&lt;span&gt;,num_classes)
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt; &lt;span&gt;        )
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;         x =&lt;span&gt; self.features(x),
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt;         x = x.view(x.size(0),-1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;         x =&lt;span&gt; self.classifier(x)
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; x
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;　　GoogLeNet专注于加深网络结构，与此同时引入了新的基本结构——&lt;span lang=&quot;EN-US&quot;&gt;Inception模块，从而来增加网络的宽度。&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet一共&lt;span lang=&quot;EN-US&quot;&gt;22层，它没有全连接层，在&lt;span lang=&quot;EN-US&quot;&gt;2014年的比赛中获得了冠军。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　每个原始&lt;span lang=&quot;EN-US&quot;&gt;Inception模块由&lt;span lang=&quot;EN-US&quot;&gt;previous layer、并行处理层及&lt;span lang=&quot;EN-US&quot;&gt;filter concatenation层组成，如图&lt;span lang=&quot;EN-US&quot;&gt;1.5。并行处理层包含&lt;span lang=&quot;EN-US&quot;&gt;4个分支，即&lt;span lang=&quot;EN-US&quot;&gt;1*1卷积分支，&lt;span lang=&quot;EN-US&quot;&gt;3*3卷积分支，&lt;span lang=&quot;EN-US&quot;&gt;5*5卷积分支和&lt;span lang=&quot;EN-US&quot;&gt;3*3最大池化分支。一个关于原始&lt;span lang=&quot;EN-US&quot;&gt;Inception模块的最大问题是，&lt;span lang=&quot;EN-US&quot;&gt;5*5卷积分支即使采用中等规模的卷积核个数，在计算代价上也可能是无法承受的。这个问题在混合池化层之后会更为突出，很快的出现计算量的暴涨。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;     &lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190304233450094-509721844.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;   图&lt;span lang=&quot;EN-US&quot;&gt;1.5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　为了克服原始&lt;span lang=&quot;EN-US&quot;&gt;Inception模块上的困难，&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet推出了一个新款，即采用&lt;span lang=&quot;EN-US&quot;&gt;1*1的卷积层来降低输入层的维度，使网络参数减少，因此减少网络的复杂性，如图&lt;span lang=&quot;EN-US&quot;&gt;1.6。因此得到降维&lt;span lang=&quot;EN-US&quot;&gt;Inception模块，称为&lt;span lang=&quot;EN-US&quot;&gt;inception V1。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;  &lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190304233515838-345936654.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; 图1.6&lt;/p&gt;
&lt;p&gt;　　从&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet中明显看出，共包含&lt;span lang=&quot;EN-US&quot;&gt;9个&lt;span lang=&quot;EN-US&quot;&gt;Inception V1模块，如图1.7所示。所有层均采用了&lt;span lang=&quot;EN-US&quot;&gt;ReLU激活函数。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305220144078-1371624951.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 图1.7&lt;/p&gt;
&lt;p&gt;　　自从&lt;span lang=&quot;EN-US&quot;&gt;2014年过后，&lt;span lang=&quot;EN-US&quot;&gt;Inception模块不断的改进，现在已发展到&lt;span lang=&quot;EN-US&quot;&gt;V4。&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet V2中的&lt;span lang=&quot;EN-US&quot;&gt;Inception参考&lt;span lang=&quot;EN-US&quot;&gt;VGGNet用两个&lt;span lang=&quot;EN-US&quot;&gt;3*3核的卷积层代替了具有&lt;span lang=&quot;EN-US&quot;&gt;5*5核的卷积层，与此同时减少了一个辅助分类器，并引入了&lt;span lang=&quot;EN-US&quot;&gt;Batch Normalization（&lt;span lang=&quot;EN-US&quot;&gt;BN），它是一个非常有用的正则化方法。&lt;span lang=&quot;EN-US&quot;&gt;V3相对于&lt;span lang=&quot;EN-US&quot;&gt;V2的学习效率提升了很多倍，并且训练时间大大缩短了。在&lt;span lang=&quot;EN-US&quot;&gt;ImageNet上的&lt;span lang=&quot;EN-US&quot;&gt;top-5错误率为&lt;span lang=&quot;EN-US&quot;&gt;4.8%。&lt;span lang=&quot;EN-US&quot;&gt;Inception V3通过改进&lt;span lang=&quot;EN-US&quot;&gt;V2得到，其核心思想是将一个较大的&lt;span lang=&quot;EN-US&quot;&gt;n*n的二维卷积拆成两个较小的一维卷积&lt;span lang=&quot;EN-US&quot;&gt;n*1和&lt;span lang=&quot;EN-US&quot;&gt;1*n。&lt;span lang=&quot;EN-US&quot;&gt;Inception V3有三种不同的结构（&lt;span lang=&quot;EN-US&quot;&gt;Base的大小分别为&lt;span lang=&quot;EN-US&quot;&gt;35*35、&lt;span lang=&quot;EN-US&quot;&gt;17*17、&lt;span lang=&quot;EN-US&quot;&gt;8*8），如图1.8所示，其中分支可能嵌套。&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet也只用了一个辅助分类器，在&lt;span lang=&quot;EN-US&quot;&gt;ImageNet上&lt;span lang=&quot;EN-US&quot;&gt;top-5的错误率为&lt;span lang=&quot;EN-US&quot;&gt;3.5%。&lt;span lang=&quot;EN-US&quot;&gt;Inception V4是一种与&lt;span lang=&quot;EN-US&quot;&gt;Inception V3类似或更复杂的网络模块。&lt;span lang=&quot;EN-US&quot;&gt;V4在&lt;span lang=&quot;EN-US&quot;&gt;ImageNet上&lt;span lang=&quot;EN-US&quot;&gt;top-5的错误率为&lt;span lang=&quot;EN-US&quot;&gt;3.08%。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305220244072-1917649206.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图1.8&lt;/p&gt;

&lt;p&gt;　　接下来利用&lt;span lang=&quot;EN-US&quot;&gt;pytorch实现&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet中的&lt;span lang=&quot;EN-US&quot;&gt;Inception V2模块，其实整个&lt;span lang=&quot;EN-US&quot;&gt;GoogLeNet都是由&lt;span lang=&quot;EN-US&quot;&gt;Inception模块构成的。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;73&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn as nn
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch as torch
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn.functional as F
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torchvision.models.inception
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; BasicConv2d(nn.Module):
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self,in_channels,out_channels,**&lt;span&gt;kwargs):
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;         super(BasicConv2d,self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;         self.conv = nn.Conv2d(in_channels,out_channels,bias=False,**&lt;span&gt;kwargs)
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;         self.bn = nn.BatchNorm2d(out_channels,eps=0.001&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;         x =&lt;span&gt; self.conv(x)
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;         x =&lt;span&gt; self.bn(x)
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; F.relu(x,inplace=&lt;span&gt;True)
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt; 
&lt;span&gt;15&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; Inception(nn.Module):
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;(self,in_channels,pool_features):
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;         super(Inception,self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         self.branch1X1 = BasicConv2d(in_channels,64,kernel_size = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; 
&lt;span&gt;20&lt;/span&gt;         self.branch5X5_1 = BasicConv2d(in_channels,48,kernel_size = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         self.branch5X5_2 = BasicConv2d(48,64,kernel_size=5,padding = 2&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt;         self.branch3X3_1 = BasicConv2d(in_channels,64,kernel_size = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;         self.branch3X3_2 = BasicConv2d(64,96,kernel_size = 3,padding = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; self.branch3X3_2 = BasicConv2d(96, 96, kernel_size=1,padding = 1)&lt;/span&gt;
&lt;span&gt;26&lt;/span&gt; 
&lt;span&gt;27&lt;/span&gt;         self.branch_pool = BasicConv2d(in_channels,pool_features,kernel_size = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;         branch1X1 =&lt;span&gt; self.branch1X1(x)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; 
&lt;span&gt;31&lt;/span&gt;         branch5X5 =&lt;span&gt; self.branch5X5_1(x)
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;         branch5X5 =&lt;span&gt; self.branch5X5_2(branch5X5)
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; 
&lt;span&gt;34&lt;/span&gt;         branch3X3 =&lt;span&gt; self.branch3X3_1(x)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;         branch3X3 =&lt;span&gt; self.branch3X3_2(branch3X3)
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; 
&lt;span&gt;37&lt;/span&gt;         branch_pool = F.avg_pool2d(x,kernel_size = 3,stride = 1,padding = 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;         branch_pool =&lt;span&gt; self.branch_pool(branch_pool)
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt; 
&lt;span&gt;40&lt;/span&gt;         outputs =&lt;span&gt; [branch1X1,branch3X3,branch5X5,branch_pool]
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; torch.cat(outputs,1)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;span lang=&quot;EN-US&quot;&gt;ResNet&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　随着神经网络的深度不断的加深，梯度消失、梯度爆炸的问题会越来越严重，这也导致了神经网络的学习与训练变得越来越困难。有些网络在开始收敛时，可能出现退化问题，导致准确率很快达到饱和，出现层次越深、错误率反而越高的现象。让人惊讶的是，这不是过拟合的问题，仅仅是因为加深了网络。这便有了&lt;span lang=&quot;EN-US&quot;&gt;ResNet的设计，&lt;span lang=&quot;EN-US&quot;&gt;ResNet在&lt;span lang=&quot;EN-US&quot;&gt;2015年的&lt;span lang=&quot;EN-US&quot;&gt;ImageNet竞赛获得了冠军，由微软研究院提出，通过残差模块能够成功的训练高达&lt;span lang=&quot;EN-US&quot;&gt;152层深的网络，如图&lt;span lang=&quot;EN-US&quot;&gt;1.10所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;ReNet与普通残差网络不同之处在于，引入了跨层连接（&lt;span lang=&quot;EN-US&quot;&gt;shorcut connection），来构造出了残差模块。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　在一个残差模块中，一般跨层连接只有跨&lt;span lang=&quot;EN-US&quot;&gt;2~3层，如图&lt;span lang=&quot;EN-US&quot;&gt;1.9所示，但是不排除跨更多的层，跨一层的实验效果不理想。&lt;/span&gt;&lt;/span&gt;在去掉跨连接层，用其输出用&lt;span lang=&quot;EN-US&quot;&gt;H(x)，当加入跨连接层时，&lt;span lang=&quot;EN-US&quot;&gt;F(x) 与&lt;span lang=&quot;EN-US&quot;&gt;H(x)存在关系：&lt;span lang=&quot;EN-US&quot;&gt;F(x)：&lt;span lang=&quot;EN-US&quot;&gt;=H(x)-X，称为残差模块。既可以用全连接层构造残差模块，也可以用卷积层构造残差模块。基于残差模块的网络结构非常的深，其深度可达&lt;span lang=&quot;EN-US&quot;&gt;1000层以上。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305225132404-1774391844.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图&lt;span lang=&quot;EN-US&quot;&gt;1.9&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305225239151-1740331971.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图&lt;span lang=&quot;EN-US&quot;&gt;1.10&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　用于&lt;span lang=&quot;EN-US&quot;&gt;ImageNet&lt;/span&gt;的&lt;span lang=&quot;EN-US&quot;&gt;5&lt;/span&gt;种深层残差网络结构，如图&lt;span lang=&quot;EN-US&quot;&gt;1.11&lt;/span&gt;所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305225314618-1356204480.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;图1.11&lt;/p&gt;
&lt;p&gt;　　从何凯明的论文中也读到&lt;span lang=&quot;EN-US&quot;&gt;plain-18、&lt;span lang=&quot;EN-US&quot;&gt;plain-34（即未加&lt;span lang=&quot;EN-US&quot;&gt;shotcut层）错误率比&lt;span lang=&quot;EN-US&quot;&gt;ResNet-18、&lt;span lang=&quot;EN-US&quot;&gt;ResNet-34（加了&lt;span lang=&quot;EN-US&quot;&gt;shotcut层）大了很多，如图&lt;span lang=&quot;EN-US&quot;&gt;1.12所示。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;EN-US&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1456807/201903/1456807-20190305225332236-1733364872.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;图&lt;span lang=&quot;EN-US&quot;&gt;1.12&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　下面利用&lt;span lang=&quot;EN-US&quot;&gt;pytorch实现&lt;span lang=&quot;EN-US&quot;&gt;ReNet的残差学习单元，此处参考了torchvision的model。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;51&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; torch.nn as nn
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; conv3x3(in_planes, out_planes, stride=1&lt;span&gt;):
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;3x3 convolution with padding&lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt; nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=&lt;span&gt;stride,
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;                      padding=1, bias=&lt;span&gt;False)
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; BasicBlock(nn.Module):
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     expansion = 1
&lt;span&gt; 8&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self, inplanes, planes, stride=1, downsample=&lt;span&gt;None):
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;         super(BasicBlock, self).&lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;()
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;         self.conv1 =&lt;span&gt; conv3x3(inplanes, planes, stride)
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;         self.bn1 =&lt;span&gt; nn.BatchNorm2d(planes)
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;         self.relu = nn.ReLU(inplace=&lt;span&gt;True)
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;         self.conv2 =&lt;span&gt; conv3x3(planes, planes)
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;         self.bn2 =&lt;span&gt; nn.BatchNorm2d(planes)
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;         self.downsample =&lt;span&gt; downsample
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;         self.stride =&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; 
&lt;span&gt;18&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt;&lt;span&gt; forward(self, x):
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;         residual =&lt;span&gt; x
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         out =&lt;span&gt; self.conv1(x)
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         out =&lt;span&gt; self.bn1(out)
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;         out =&lt;span&gt; self.relu(out)
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;         out =&lt;span&gt; self.conv2(out)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;         out =&lt;span&gt; self.bn2(out)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; self.downsample &lt;span&gt;is&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; None:
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;             residual =&lt;span&gt; self.downsample(x)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;         out +=&lt;span&gt; residual
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;         out =&lt;span&gt; self.relu(out)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; out
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当然，不管是LeNet，还是VGGNet，亦或是ResNet，这些经典的网络结构，pytorch的torchvision的model中都已经实现，并且还有预训练好的模型，可直接对模型进行微调便可使用。&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 15:36:00 +0000</pubDate>
<dc:creator>泽积</dc:creator>
<og:description>LeNet 1998年，LeCun提出了第一个真正的卷积神经网络，也是整个神经网络的开山之作，称为LeNet，现在主要指的是LeNet5或LeNet-5，如图1.1所示。它的主要特征是将卷积层和下采样</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/shenpings1314/p/10468418.html</dc:identifier>
</item>
<item>
<title>Redis Cluster搭建高可用Redis服务器集群 - java_lover</title>
<link>http://www.cnblogs.com/haha12/p/10480534.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/haha12/p/10480534.html</guid>
<description>&lt;h3 id=&quot;hrediscluster&quot;&gt;一、Redis Cluster集群简介&lt;/h3&gt;
&lt;p&gt;Redis Cluster是Redis官方提供的分布式解决方案，在3.0版本后推出的，有效地解决了Redis分布式的需求，当一个节点挂了可以快速的切换到另一个节点，当遇到单机内存、并发等瓶颈时，可以采用分布式方案要解决问题。&lt;/p&gt;
&lt;h3 id=&quot;h&quot;&gt;二、集群原理&lt;/h3&gt;
&lt;h3&gt;&lt;img title=&quot;Redis Cluster架构图&quot; src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/OXnTwHqwxnbpv8C671GKz7n50nmk8UpRstJUPoSOX3AzYjllV66nlXS4b4icicibCOt9pD9BRqHAoMYLzJxzxkhag/0?wx_fmt=jpeg&quot; alt=&quot;Redis Cluster架构图&quot;/&gt;Redis Cluster架构图&lt;br/&gt;Redis Cluster集群采用了P2P的模式，完全去中心化，Redis把所有的Key分成了16384个slot，每个Redis实例负责其中一部分slot，集群中的所有信息（节点、端口、slot等），都通过节点之间定期的数据交换而更新，Redis客户端可以在任意一个Redis实例发出请求，如果所需数据不在该实例中，通过重定向命令引导客户端访问所需的实例。&lt;/h3&gt;

&lt;p&gt;其结构特点：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽。&lt;/li&gt;
&lt;li&gt;节点的fail是通过集群中超过半数的节点检测失效时才生效。&lt;/li&gt;
&lt;li&gt;客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可。&lt;/li&gt;
&lt;li&gt;redis-cluster把所有的物理节点映射到[0-16383]slot上（不一定是平均分配），cluster负责维护node&amp;lt;-&amp;gt;slot&amp;lt;-&amp;gt;value。&lt;/li&gt;
&lt;li&gt;Redis集群预分好16384个桶，当需要在Redis集群中放置一个key-value 时，根据CRC16(key) mod 16384的值，决定将一个key放到哪个桶中。&lt;/li&gt;
&lt;/ol&gt;&lt;h3 id=&quot;h-1&quot;&gt;三、集群搭建&lt;/h3&gt;
&lt;p&gt;要让集群正常工作至少需要3个主节点，一共就需要6个节点，其中3个为主节点，3个为从节点，为了简单在下面在一台机器上演示，演示使用了linux服务器上7000到7005的6个端口。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;下载redis、解压、安装&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs go&quot;&gt;wget http:&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;将redis-trib.rb复制到/usr/local/bin目录下&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs bash&quot;&gt;&lt;span class=&quot;hljs-built_in&quot;&gt;cd src&lt;br/&gt;cp redis-trib.rb /usr/&lt;span class=&quot;hljs-built_in&quot;&gt;local/bin&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;创建Redis节点并修改配置文件&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs perl&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;mkdir redis_cluster　　&lt;br/&gt;cd redis_cluster/&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;mkdir &lt;span class=&quot;hljs-number&quot;&gt;7000 &lt;span class=&quot;hljs-number&quot;&gt;7001 &lt;span class=&quot;hljs-number&quot;&gt;7002 &lt;span class=&quot;hljs-number&quot;&gt;7003 &lt;span class=&quot;hljs-number&quot;&gt;7004 &lt;span class=&quot;hljs-number&quot;&gt;7005&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7000&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7001&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7002&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7003&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7004&lt;br/&gt;cp redis.conf redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7005&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;按下面提示修改6个配置文件&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs delphi&quot;&gt;port  &lt;span class=&quot;hljs-number&quot;&gt;7000       &lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;启动6个Redis实例，并且要指定配置文件，这些配置文件分别在各自的子目录下面&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs vbscript&quot;&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7000/redis.conf &lt;br/&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7001/redis.conf &lt;br/&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7002/redis.conf &lt;br/&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7003/redis.conf &lt;br/&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7004/redis.conf &lt;br/&gt;./src/redis-&lt;span class=&quot;hljs-built_in&quot;&gt;server redis_cluster/&lt;span class=&quot;hljs-number&quot;&gt;7005/redis.conf &lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;5&quot;&gt;&lt;li&gt;安装ruby&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs sql&quot;&gt;yum &lt;span class=&quot;hljs-keyword&quot;&gt;install ruby&lt;br/&gt;yum &lt;span class=&quot;hljs-keyword&quot;&gt;install -y rubygems&lt;br/&gt;gem &lt;span class=&quot;hljs-keyword&quot;&gt;install redis -v &lt;span class=&quot;hljs-number&quot;&gt;3.2&lt;span class=&quot;hljs-number&quot;&gt;.2&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;6&quot;&gt;&lt;li&gt;使用redis-trib.rb创建集群&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs nginx&quot;&gt;&lt;span class=&quot;hljs-attribute&quot;&gt;cd /usr/local/bin&lt;br/&gt;./redis-trib.rb create --replicas &lt;span class=&quot;hljs-number&quot;&gt;1 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7000 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7001 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7002 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7003 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7004 &lt;span class=&quot;hljs-number&quot;&gt;192.168.0.204:7005  &lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;运行结果如下：&lt;/p&gt;
&lt;h3&gt;&lt;img title=&quot;Redis Cluster启动成功截图&quot; src=&quot;https://mmbiz.qpic.cn/mmbiz_png/OXnTwHqwxnbpv8C671GKz7n50nmk8UpRW0m3tRAlhJ5hzJ2dq6Giaib6nRpicksFcZ12vsicHZ8TgicWXS20cFkOZiaA/0?wx_fmt=png&quot; alt=&quot;Redis Cluster启动成功截图&quot;/&gt;Redis Cluster启动成功截图&lt;/h3&gt;

&lt;p&gt;如果一切顺利，你会看到类似截图上的消息： [OK] All 16384 slots covered， 这说明Redis的Cluster集群环境搭建成功。&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;简单解释一下这个命令：调用 ruby 命令来进行创建集群，--replicas 1 表示主从节点比例为 1:1，即一个主节点对应一个从节点；然后，默认给我们分配好了每个主节点和对应从节点服务，以及solt的大小，因为在Redis集群中有且仅有16383个solt，默认情况会给我们平均分配，当然你可以指定，后续的增减节点也可以重新分配。&lt;br/&gt;M: 5237fa04bd793832b605d92ceb1d2f493da22e43 为主节点Id&lt;br/&gt;S: b6b696c11bbffa8f9d4e6397ef4d27b0b54fea32 192.168.0.204:7003 replicates 5237fa04bd793832b605d92ceb1d2f493da22e43 从节点下对应主节点Id&lt;br/&gt;目前来看，7000-7002 为主节点，7003-7005 为从节点，并向你确认是否同意这么配置，输入yes后，会开始集群创建。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;h-2&quot;&gt;四、验证集群&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;通过Cluster Nodes命令和Cluster Info命令来看看集群效果&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;hljs&quot;&gt;./redis-cli -c  -h 192.168.0.204 -p 7001&lt;br/&gt;cluster info&lt;br/&gt;cluster nodes&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;运行结果如下：&lt;/p&gt;
&lt;h3&gt;&lt;img title=&quot;运行成功截图&quot; src=&quot;https://mmbiz.qpic.cn/mmbiz_png/OXnTwHqwxnbpv8C671GKz7n50nmk8UpR5Z4MYN9nNPmFKcsJpxfYicYmpx75vTCjzWEEnOD5Zz7pBu2anORxrcQ/0?wx_fmt=png&quot; alt=&quot;运行成功截图&quot;/&gt;运行成功截图&lt;/h3&gt;

&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;在集群上通过增加数据来测试集群效果&lt;br/&gt;运行结果如下：&lt;br/&gt;&lt;img title=&quot;测试集群截图&quot; src=&quot;https://mmbiz.qpic.cn/mmbiz_png/OXnTwHqwxnbpv8C671GKz7n50nmk8UpRdnzNlBPrpEnlA5hvEXTSnEK8QnXD1mXpZnaJYE3CVsF31rbbJVVTPw/0?wx_fmt=png&quot; alt=&quot;测试集群截图&quot;/&gt;测试集群截图&lt;/li&gt;
&lt;/ol&gt;&lt;hr/&gt;&lt;p&gt;下面的是我的公众号二维码图片，欢迎关注，欢迎留言，一起学习，一起进步。&lt;/p&gt;
&lt;h3&gt;&lt;img title=&quot;Java碎碎念公众号&quot; src=&quot;https://mmbiz.qpic.cn/mmbiz_jpg/OXnTwHqwxnYc8Qxiappy2M1MnJcc3MLI9R1PorLeibiczcODP9IAYGdrB2HOTWW24NX5rfeB1ZJ8G4jPfKRnah2WQ/0?wx_fmt=jpeg&quot; alt=&quot;Java碎碎念公众号&quot;/&gt;Java碎碎念公众号&lt;/h3&gt;

</description>
<pubDate>Tue, 05 Mar 2019 15:21:00 +0000</pubDate>
<dc:creator>java_lover</dc:creator>
<og:description>一、Redis Cluster集群简介 Redis Cluster是Redis官方提供的分布式解决方案，在3.0版本后推出的，有效地解决了Redis分布式的需求，当一个节点挂了可以快速的切换到另一个节</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/haha12/p/10480534.html</dc:identifier>
</item>
<item>
<title>深入理解es6-Promise对象 - 热爱前端的17号诶</title>
<link>http://www.cnblogs.com/sqh17/p/10480431.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/sqh17/p/10480431.html</guid>
<description>&lt;h4 id=&quot;前言&quot;&gt;前言&lt;/h4&gt;
&lt;p&gt;    在之前翻博客时，看到promise，又重读了一边，突然发现理解很浅，记的笔记也不是很好理解，又重新学习promise，加深理解，学以致用&lt;br/&gt;    在promise出来之前，js常用解决异步方式都是采用回调函数方式，但是如果需求过多，会形成一系列的回调函数，俗称：回调地狱。导致后期阅读和维护代码特别麻烦。所以es6的Promise就是为了解决这个麻烦而出来的新对象，之前早就存在，ES6将其写进了语言标准，统一了用法，原生提供了Promise对象。&lt;/p&gt;
&lt;h4 id=&quot;定义&quot;&gt;定义&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;Promise对象是为了简化异步编程。解决回调地狱情况。&lt;/code&gt;&lt;br/&gt;Promise，简单说就是一个容器，里面保存着某个未来才会结束的事件（通常是一个异步操作）的结果。重点是取决与这个事件之后的一系列动作，then()或catch()的等等。&lt;br/&gt;从语法上说，Promise 是一个对象，从它可以获取异步操作的消息。Promise对象用于延迟(deferred) 计算和异步(asynchronous ) 计算。一个Promise对象代表着一个还未完成，但预期将来会完成的操作。这样表示了&lt;code&gt;一旦用了promise对象，就不能退出，直到出现结果为止（resloved或rejected）&lt;/code&gt;&lt;br/&gt;Promise是一个对象，可以用构造函数来创建一个Promise实例。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;let promise = new Promise((resolve, reject) =&amp;gt;{
    // .... some coding
    if (true){   // 异步操作成功
        resolve(value);
    } else {
        reject(error);
    }
})
promise.then(value=&amp;gt;{
    // 成功的回调函数
}, error=&amp;gt;{
    // 失败后的回调函数
})
console.log(typeof promise) // object&lt;/code&gt;
&lt;/pre&gt;
&lt;h6 id=&quot;参数解释&quot;&gt;参数解释&lt;/h6&gt;
&lt;p&gt;params:传参是一个回调函数。这个回调函数有两个参数resolve和reject。&lt;/p&gt;
&lt;ul readability=&quot;8.5&quot;&gt;&lt;li&gt;resolve: 将Promise对象的状态从“未完成”变为“成功”（即从 pending 变为 resolved），在异步操作成功时调用，并将异步操作的结果，作为参数传递出去。(简单来说就是&lt;code&gt;成功了的执行&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;reject: 将Promise对象的状态从“未完成”变为“失败”（即从 pending 变为 rejected），在异步操作失败时调用，并将异步操作报出的错误，作为参数传递出去。(简单来说就是&lt;code&gt;失败了的执行&lt;/code&gt;)&lt;/li&gt;
&lt;li readability=&quot;20&quot;&gt;promise之后then的参数：
&lt;ul&gt;&lt;li&gt;第一个参数是成功的回调函数，必选&lt;/li&gt;
&lt;li&gt;第二个参数是失败的回调函数,可选&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;// 成功时
let promise = new Promise((resolve, reject) =&amp;gt;{
    console.log('开始')
    if (2 &amp;gt; 1){   // 异步操作成功
        resolve({name:'peter',age:25});
    } else {
        reject(error);
    }
})
promise.then(value=&amp;gt;{
    // 成功的回调函数
    console.log(value)
}, error=&amp;gt;{
    // 失败后的回调函数
    console.log(error)
})
// 开始
// {name: &quot;peter&quot;, age: 25} &lt;/code&gt;
&lt;/pre&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;// 失败时
let promise = new Promise((resolve, reject) =&amp;gt;{
    console.log('开始')
    if (2 &amp;gt; 3){   // 异步操作成功
        resolve(a);
    } else {
        reject('未知错误');
    }
})
promise.then(value=&amp;gt;{
    // 成功的回调函数
    console.log(value)
}, error=&amp;gt;{
    // 失败后的回调函数
    console.log(error)
})
// 开始
// 未知错误&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ps：Promise实例化一个对象后，会立即实行。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;new Promise((resolve, reject)=&amp;gt;console.log('promise'));
console.log('123');
// promise
// 123&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个结果发现，先执行promise后执行123。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h5 id=&quot;promise的特点&quot;&gt;Promise的特点&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;对象的状态不受外界影响。Promise对象代表一个异步操作，有三种状态：pending（进行中）、fulfilled（已成功）和rejected（已失败）。只有异步操作的结果，可以决定当前是哪一种状态，任何其他操作都无法改变这个状态。&lt;/li&gt;
&lt;li&gt;一旦状态改变，就不会再变，任何时候都可以得到这个结果。就是成功了就一直是成功的状态fulfilled，失败一直是失败的状态rejected。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;code&gt;如果改变已经发生了，你再对Promise对象添加回调函数，也会立即得到这个结果。这与事件（Event）完全不同，事件的特点是，如果你错过了它，再去监听，是得不到结果的&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;promise先按顺序实行完promise实例中方法再实行then中的resolve或者reject。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;let promise = new Promise((resolve, reject)=&amp;gt;{
    console.log('promise')
    if (2 &amp;gt; 1){   // 异步操作成功
        resolve({name:'peter',age:25});
    } else {
        reject(error);
    }
    console.log('end')
})
promise.then(
    value=&amp;gt;{
        console.log(value)
    },
    error=&amp;gt;{
        console.log(error)
    }
)
// promise
// end 
// {name: &quot;peter&quot;, age: 25}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ajax是最常见的异步操作方式，那么用promise封装Ajax的例子&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;const getJSON = function (url) {
    const promise = new Promise(function (resolve, reject) {
        const handler = function () {
            if (this.readyState !== 4) {
                return;
            }
            if (this.status === 200) {
                resolve(this.response);
            } else {
                reject(new Error(this.statusText));
            }
        };
        const client = new XMLHttpRequest();
        client.open(&quot;GET&quot;, url);
        client.onreadystatechange = handler;
        client.responseType = &quot;json&quot;;
        client.setRequestHeader(&quot;Accept&quot;, &quot;application/json&quot;);
        client.send();
    });
    return promise;
};

getJSON(&quot;xxxxx&quot;).then(function (value) {
    console.log('Contents: ' + value);
}, function (error) {
    console.error('出错了', error);
});&lt;/code&gt;
&lt;/pre&gt;
&lt;h5 id=&quot;promise方法&quot;&gt;Promise方法&lt;/h5&gt;
&lt;h6 id=&quot;promise.then&quot;&gt;promise.then()&lt;/h6&gt;
&lt;p&gt;then() 为 Promise 实例添加状态改变时的回调函数,上面已经提起过。&lt;br/&gt;params解释:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;第一个参数是resolved状态的回调函数， 必选&lt;/li&gt;
&lt;li&gt;第二个参数是rejected状态的回调函数, 可选&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;通常情况下，then方法作为成功时的回调方法，catch方法作为失败时回调方法。catch()在后面,可以理解为then方法中的reject参数&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;let promise = new Promise((resolve, rejected)=&amp;gt;{
    if(2&amp;lt;3){
        resolve()
    }else{
        rejected()
    }
})
promise.then(resolve=&amp;gt;{
    console.log('right')
}).catch(reject=&amp;gt;{
    console.log('error')
})&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;ps: then方法返回的是一个新的Promise实例（注意，不是原来那个Promise实例）。&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;var aPromise = new Promise(function (resolve) {
    resolve(100);
});
var thenPromise = aPromise.then(function (value) {
    console.log(value);
});
var catchPromise = thenPromise.catch(function (error) {
    console.error(error);
});
console.log(aPromise !== thenPromise); // =&amp;gt; true
console.log(thenPromise !== catchPromise);// =&amp;gt; true&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所以每一个then()方法就是一个新promise对象。因此可以采用链式写法，即then方法后面再调用另一个then方法。这样必须要传一个参数过去。&lt;br/&gt;promise的链式编程，就是第一个的Promise实例的返回的值作为下一个Promise实例的参数。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;function start() {
    return new Promise((resolve, reject) =&amp;gt; {
        resolve('start');
    });
}
start()
    .then(data =&amp;gt; {
        // promise start
        console.log(data);
        return Promise.resolve(1); // 1
    })
    .then(data =&amp;gt; {
        // promise 1
        console.log(data);
    })
// start
// 1&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;从上面例子可知：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;start函数里resolve里传了一个参数‘start’&lt;/li&gt;
&lt;li&gt;第一个then方法接受了start,然后return 一个成功的值 1&lt;/li&gt;
&lt;li&gt;第二个then方法接受上一个then传来的值 1&lt;/li&gt;
&lt;/ul&gt;&lt;h6 id=&quot;promise.catch&quot;&gt;Promise.catch()&lt;/h6&gt;
&lt;p&gt;catch()和then()都是挂载在promise对象的原型上的。&lt;br/&gt;&lt;code&gt;Promise.prototype.catch&lt;/code&gt;方法是&lt;code&gt;promise.then(null, rejection&lt;/code&gt;)或&lt;code&gt;promise.then(undefined, rejection)&lt;/code&gt;的别名，用于指定发生错误时的回调函数。&lt;br/&gt;一般是等价于：（在遇到失败的情况下）&lt;br/&gt;&lt;code&gt;Promise.catch()&lt;/code&gt; &amp;lt;=&amp;gt; &lt;code&gt;promise.then(null,e=&amp;gt;reject())&lt;/code&gt;&lt;br/&gt;如果异步操作抛出错误，状态就会变为rejected，就会调用catch方法指定的回调函数，处理这个错误。另外，then方法指定的回调函数，如果运行中抛出错误，也会被catch方法捕获。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;const promise = new Promise(function(resolve, reject) {
  throw new Error('test');
});
promise.catch(function(error) {
  console.log(error);
});
// Error: test&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Promise 对象的错误具有“冒泡”性质，会一直向后传递，直到被捕获为止。也就是说，错误总是会被下一个catch语句捕获。&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;function throwError(value) {
    // 抛出异常
    throw new Error(value);
}
// &amp;lt;1&amp;gt; onRejected不会被调用
function badMain(onRejected) {
    return Promise.resolve(42).then(throwError, onRejected);
}
// &amp;lt;2&amp;gt; 有异常发生时onRejected会被调用
function goodMain(onRejected) {
    return Promise.resolve(42).then(throwError).catch(onRejected);
}
// 运行示例
badMain(function(){
    console.log(&quot;BAD&quot;);
});
goodMain(function(){
    console.log(&quot;GOOD&quot;);
});
// GOOD&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在上面的代码中， badMain 是一个不太好的实现方式（但也不是说它有多坏）， goodMain 则是一个能非常好的进行错误处理的版本。&lt;br/&gt;为什么说 badMain 不好呢？，因为虽然我们在 .then 的第二个参数中指定了用来错误处理的函数，但实际上它却不能捕获第一个参数 onFulfilled 指定的函数（本例为 throwError ）里面出现的错误。也就是说，这时候即使 throwError 抛出了异常，onRejected 指定的函数也不会被调用（即不会输出&quot;BAD&quot;字样）。&lt;br/&gt;与此相对的是， goodMain 的代码则遵循了 throwError→onRejected 的调用流程。 这时候 throwError 中出现异常的话，在会被方法链中的下一个方法，即 .catch 所捕获，进行相应的错误处理。&lt;br/&gt;.then 方法中的onRejected参数所指定的回调函数，实际上针对的是其promise对象或者之前的promise对象，而不是针对 .then 方法里面指定的第一个参数，即onFulfilled所指向的对象，这也是 then 和 catch 表现不同的原因。（详见Javascript Promise 迷你版）&lt;br/&gt;这个是从别人的博客拿来的代码和解释，了那么多，总结为，&lt;code&gt;catch能够捕获它之前的异常，而在then()方法中第二个参数是没办法捕获到的，因为实行了resolve方法。&lt;/code&gt;&lt;/p&gt;
&lt;h6 id=&quot;promise.resolve&quot;&gt;Promise.resolve()&lt;/h6&gt;
&lt;p&gt;看字面量的意思，是返回一个成功的promise实例。&lt;br/&gt;&lt;code&gt;Promise.resolve()&lt;/code&gt; &amp;lt;=&amp;gt; &lt;code&gt;new Promise((resolve,rejected)=&amp;gt;resolve())&lt;/code&gt;&lt;br/&gt;最常见的就是将不是promise对象的异步操作转化为promise对象。&lt;br/&gt;该方法有四个参数：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;无参数&lt;br/&gt;直接返回一个resolved状态的 Promise 对象，所谓的字面量意思。&lt;/li&gt;
&lt;li&gt;参数是一个 Promise 实例&lt;br/&gt;Promise.resolve将不做任何修改、原封不动地返回这个实例。&lt;/li&gt;
&lt;li&gt;参数是一个thenable对象&lt;br/&gt;所谓的thenable对象指的就是具有then方法的对象，类似于类数组具有数组的length，但不是数组一样。&lt;br/&gt;&lt;code&gt;javascript let thenable = { then: function(resolve, reject) { resolve(42); } }; var promise = Promise.resolve(thenable) .then(value=&amp;gt;console.log(value));// 42&lt;/code&gt;&lt;br/&gt;上面的例子就是将具有then方法的thenable对象转化为promise对象，并且立即执行resolve方法。&lt;/li&gt;
&lt;li&gt;参数不是具有then方法的对象，或根本就不是对象&lt;br/&gt;如果参数是一个原始值，或者是一个不具有then方法的对象，则Promise.resolve方法返回一个新的 Promise 对象，状态为resolved。&lt;br/&gt;&lt;code&gt;javascript var str = '17号' Promise.resolve(str).then(value=&amp;gt;console.log(value)) // 17号&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h6 id=&quot;promise.reject&quot;&gt;Promise.reject()&lt;/h6&gt;
&lt;p&gt;返回一个新的 Promise 实例，该实例的状态为rejected。用法和resolve一样，但是都是以失败返回结果&lt;br/&gt;&lt;code&gt;Promise.reject()&lt;/code&gt; &amp;lt;=&amp;gt; &lt;code&gt;new Promise((resolve,reject) = &amp;gt;reject())&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ps:Promise.reject()方法的参数，会原封不动地作为reject的理由，变成后续方法的参数。这一点与Promise.resolve方法不一致。&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;const thenable = {
  then(resolve, reject) {
    reject('出错了');
  }
};

Promise.reject(thenable)
        .catch( e=&amp;gt; {
            console.log(e)
        })
// 返回的是thenable对象&lt;/code&gt;
&lt;/pre&gt;
&lt;h6 id=&quot;promise.all&quot;&gt;Promise.all()&lt;/h6&gt;
&lt;p&gt;Promise.all 接收一个 promise对象的数组作为参数，当这个数组里的所有promise对象全部变为resolve或reject状态的时候，它才会去调用 .then() 方法。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;该方法的参数是一个数组&lt;/li&gt;
&lt;li&gt;该方法的参数数组是必须含有promise对象的数组&lt;/li&gt;
&lt;li&gt;只有数组中所有的promise对象都变成resolve或者reject才能进行下一步操作。&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;// `delay`毫秒后执行resolve
function timerPromisefy(delay) {
    return new Promise(function (resolve) {
        setTimeout(function () {
            resolve(delay);
        }, delay);
    });
}
var startDate = Date.now();
// 所有promise变为resolve后程序退出
Promise.all([
    timerPromisefy(1),
    timerPromisefy(32),
    timerPromisefy(64),
    timerPromisefy(128)
]).then(function (values) {
    console.log(Date.now() - startDate + 'ms');
    console.log(values); 
});
// 129ms
// 1,32,64,128&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;从上述结果可以看出，传递给 Promise.all 的promise并不是一个个的顺序执行的，而是同时开始、并行执行的。&lt;br/&gt;假设法：如果这些promise全部串行处理的话，那么需要 等待1ms → 等待32ms → 等待64ms → 等待128ms ，全部执行完毕需要225ms的时间。但实际上不是，而是129ms左右。&lt;/p&gt;
&lt;h6 id=&quot;promise.race&quot;&gt;Promise.race()&lt;/h6&gt;
&lt;p&gt;和Promise.all()方法一样，参数是一个数组，但是只要有一个promise对象更改状态时就实行下一步。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;// `delay`毫秒后执行resolve
function timerPromisefy(delay) {
    return new Promise(function (resolve) {
        setTimeout(function () {
            resolve(delay);
        }, delay);
    });
}
// 任何一个promise变为resolve或reject 的话程序就停止运行
Promise.race([
    timerPromisefy(1),
    timerPromisefy(32),
    timerPromisefy(64),
    timerPromisefy(128)
]).then(function (value) {
    console.log(value);    // =&amp;gt; 1
});&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面的例子是1秒后就resolve了，所以直接then()了。&lt;/p&gt;
&lt;h6 id=&quot;promsie.finally&quot;&gt;Promsie.finally()&lt;/h6&gt;
&lt;p&gt;该方法用于指定不管 Promise 对象最后状态如何，都会执行的操作。无论resolve还是reject都会实行的操作，不依赖其他的操作。按照执行顺序。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;function promise(){
    return new Promise((resolve, reject) =&amp;gt; {
        resolve('success');
    })
};
promise().then(data =&amp;gt; {
    console.log(data)
    return Promise.reject('fail')
}).finally(() =&amp;gt; {
    console.log('end')
}).catch(data =&amp;gt;{
    console.log(data)
})
// success
// end
// fail&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;从上面的例子可知，是按照promise的实行顺序执行的，在then()中，要求返回一个失败的状态，但是却没先实行失败的方法，而是按照顺序实行了finally方法。&lt;/p&gt;
&lt;h6 id=&quot;promise.done&quot;&gt;Promise.done()&lt;/h6&gt;
&lt;p&gt;Promise 对象的回调链，不管以then方法或catch方法结尾，要是最后一个方法抛出错误，都有可能无法捕捉到（因为 Promise 内部的错误不会冒泡到全局）。因此，我们可以提供一个done方法，总是处于回调链的尾端，保证抛出任何可能出现的错误。&lt;/p&gt;
&lt;pre class=&quot;javascript&quot;&gt;
&lt;code&gt;Promise.prototype.done = function (resolve, reject) {
    this.then(resolve, reject)
        .catch( function (reason) {
            // 抛出一个全局错误
            setTimeout( () =&amp;gt; { throw reason }, 0);
        });
}

// 使用示例
var p = new Promise( (resolve, reject) =&amp;gt; {
    resolve('p');
});
p.then(ret =&amp;gt; {
    console.log(ret);
    return 'then1';
}).catch( err =&amp;gt; {
    console.log(err.toString());
}).then( ret =&amp;gt; {
    console.log(ret);
    return 'then2';
}).then( ret =&amp;gt; {
    console.log(ret);
    x + 2;
}).done();&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;该例子参考别人的例子。发现到最后直接抛出了 'Uncaught ReferenceError: x is not defined'。说明最后一个then实行时会抛出异常,也可以类似于catch方法吧。&lt;/p&gt;
&lt;h6 id=&quot;总结&quot;&gt;总结&lt;/h6&gt;
&lt;p&gt;总结来说Promise其实就是做了一件事情，那就是对异步操作进行了封装，然后可以将异步操作以同步的流程表达出来，避免了层层嵌套的回调地狱，提供统一的接口方法，使得控制异步操作更加容易，但是也有一定的缺点，promise一旦没确定状态，是没法终止的，同样的，也无法取消promise。&lt;br/&gt;如果本文有什么不对的地方，欢迎指出，谢谢，大家一起进步加油。我把笔记放到&lt;a href=&quot;https://github.com/sqh17/notes/blob/master/ways/promise.md&quot;&gt;github&lt;/a&gt;了，如果满意的话给个star。&lt;/p&gt;
&lt;h6 id=&quot;参考资料&quot;&gt;参考资料&lt;/h6&gt;
&lt;p&gt;&lt;a href=&quot;http://es6.ruanyifeng.com/#docs/promise&quot;&gt;ES6标准入门&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://liubin.org/promises-book/#introduction&quot;&gt;Javascript Promise 迷你版&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000007685095&quot;&gt;学习Promise&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 15:10:00 +0000</pubDate>
<dc:creator>热爱前端的17号诶</dc:creator>
<og:description>前言     在之前翻博客时，看到promise，又重读了一边，突然发现理解很浅，记的笔记也不是很好理解，又重新学习promise，加深理解，学以致用 &amp;nbs</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/sqh17/p/10480431.html</dc:identifier>
</item>
<item>
<title>typescript中抽象类与接口详细对比与应用场景介绍 - 牙疼哥哥</title>
<link>http://www.cnblogs.com/pomelott/p/10480390.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/pomelott/p/10480390.html</guid>
<description>&lt;p&gt;现如今，TS正在逐渐成为前端OO编程的不二之选，以下是我在学习过程中对抽象类和接口做的横向对比。&lt;/p&gt;
&lt;p&gt;1. &lt;strong&gt;抽象类当做父类，被继承。且抽象类的派生类的构造函数中必须调用super()；接口可以当做“子类”继承其他类&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;抽象类派生：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:javascript;gutter:true;&quot;&gt;
abstract class Human {
    constructor (readonly name:string) {}
    
}

class Student extends Human {
    constructor (name:string) {
        super(name)
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;接口继承类：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;34&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
class Man {
    job: string;
    constructor (readonly name: string) {}
    earnMoney () {
        console.log(`earning money`)
    }
}

interface HumanRule extends Man{
    nose: string;
    mouth: string;
    ear: string;
    eye: string;
    eyebrow: string
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;当类被接口继承时，通常是需要为这个类的子类添加约束。例如下面的例子中，Man类的子类就需要去实现特定的五官属性，否则将会报错。&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
class Player extends Man implements HumanRule{
    nose: string;
    mouth: string;
    ear: string;
    eye: string;
    eyebrow: string
    constructor (name) {
        super(name);
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2. &lt;strong&gt;抽象类与接口都无法实例化， 类类型接口实际上是一种 抽象类型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;按个人理解，在使用类类型的接口时，类类型的接口其实就相当于抽象类的子集。抽象类中除了可以像接口那样只定义不实现外，还可以部分实现，而且也可以使用类型修饰符。&lt;/p&gt;
&lt;p&gt;类类型的接口更多的是当做一种抽象的数据类型使用，此处所说的类型通常是某个类的实例类型。&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
let James: Player = new Player('james');  // 类类型使用

class SoccerPlayer extends Player {
    constructor (name) {
        super(name)
    }

    playSoccer () {
        console.log(`${this.name} is playing soccer.`)
    }
}

function createPlayer (pl: SoccerPlayer, name: string) {  // 类类型调用
    return new SoccerPlayer(name);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;3. &lt;strong&gt;接口中不能包含具体实现，抽象类中除抽象函数之外，其他函数可以包含具体实现&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此处我们将Human类增加一些内容：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
abstract class Human {
    constructor (readonly name:string) {}
    protected thinking () {
        console.log(`I am a human, so i can think, ${this.name} is thinking.`)
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;作为人类，可以在人类 这个类中添加具体实现，因为人类都可以思考。所以思考这个类就不必非要放到子类中去具体实现，这也正是抽象类的灵活之处。&lt;/p&gt;
&lt;p&gt;4. &lt;strong&gt;抽象类中的抽象方法在子类中必须实现， 接口中的非可选项在接口被调用时必须实现。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;此处我们继续增加Human类的内容，增加move的具体实现方法为抽象方法，因为不同类型的人，移动的实现不同。（此处实际上也是OO的特性中，多态的一种具体实现）&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
abstract class Human {
    constructor (readonly name:string) {}
    protected thinking () {
        console.log(`I am a human, so i can think, ${this.name} is thinking.`)
    }
    abstract move (): void
}

class Student extends Human {
    constructor (name:string) {
        super(name)
    }

    move () {
        console.log(`I am a student, so i move by bus`)
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;而接口一旦调用，就必须要严格实现。此处以函数类型的接口为例：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
interface createPlayer {
    (pl: SoccerPlayer, name:string): SoccerPlayer
}

let createPlayer:createPlayer = function  (pl: SoccerPlayer, name: string) {  // 修改createPlayer 使用匿名函数方法创建
    return new SoccerPlayer(name);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;5. &lt;strong&gt;抽象方法可当做类的实例方法，添加访问修饰符；但是接口不可以&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;抽象方法的添加访问修饰符和接口的严格实现其实都是各自的特点，我们也往往是根据这些特点去选择究竟是使用抽象类还是使用接口。&lt;/p&gt;
&lt;p&gt;还拿Human类来说：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
abstract class Human {
    constructor (readonly name:string) {}
    protected thinking () {
        console.log(`I am a human, so i can think, ${this.name} is thinking.`)
    }
    protected abstract move (): void
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;我们为move方法添加abstract标识符，是想让开发者非常明白，Human的派生类中必须要实现此方法；而使用protected标识符，是想限制move方法调用或重载的权限。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;综合来说抽象类更多的是实现业务上的严谨性；接口更多的是制定各种规范，而此规范又分为很多类规范，就像官方文档在介绍接口这一节的时候所说的，例如函数型规范、类类型规范、混合规范、索引规范等。&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 05 Mar 2019 14:51:00 +0000</pubDate>
<dc:creator>牙疼哥哥</dc:creator>
<og:description>现如今，TS正在逐渐成为前端OO编程的不二之选，以下是我在学习过程中对抽象类和接口做的横向对比。 1. 抽象类当做父类，被继承。且抽象类的派生类的构造函数中必须调用super()；接口可以当做“子类”</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/pomelott/p/10480390.html</dc:identifier>
</item>
</channel>
</rss>