<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>pythn print格式化输出---------&quot;%s 和 % d&quot; 都是什么意思? - 牧牛人</title>
<link>http://www.cnblogs.com/ooo888ooo/p/10434731.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/ooo888ooo/p/10434731.html</guid>
<description>&lt;p&gt;pythn print格式化输出。&lt;/p&gt;

&lt;p&gt;%r 用来做 debug 比较好，因为它会显示变量的原始数据（raw data），而其它的符&lt;br/&gt;号则是用来向用户显示输出的。&lt;/p&gt;

&lt;p&gt;1. 打印字符串&lt;/p&gt;
&lt;p&gt;print (&quot;His name is %s&quot;%(&quot;Aviad&quot;))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;2.打印整数&lt;/p&gt;
&lt;p&gt;print (&quot;He is %d years old&quot;%(25))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;3.打印浮点数&lt;/p&gt;
&lt;p&gt;print (&quot;His height is %f m&quot;%(1.83))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;4.打印浮点数（指定保留小数点位数）&lt;/p&gt;
&lt;p&gt;print (&quot;His height is %.2f m&quot;%(1.83))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;5.指定占位符宽度&lt;/p&gt;
&lt;p&gt;print (&quot;Name:%10s Age:%8d Height:%8.2f&quot;%(&quot;Aviad&quot;,25,1.83))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;6.指定占位符宽度（左对齐）&lt;/p&gt;
&lt;p&gt;print (&quot;Name:%-10s Age:%-8d Height:%-8.2f&quot;%(&quot;Aviad&quot;,25,1.83))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;7.指定占位符（只能用0当占位符？）&lt;/p&gt;
&lt;p&gt;print (&quot;Name:%-10s Age:%08d Height:%08.2f&quot;%(&quot;Aviad&quot;,25,1.83))&lt;br/&gt;效果：&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;8.科学计数法&lt;/p&gt;
&lt;p&gt;format(0.0015,'.2e')&lt;br/&gt;效果：&lt;/p&gt;


&lt;p&gt;我们还可以用词典来传递真实值。如下：&lt;br/&gt;print (&quot;I'm %(c)s. I have %(l)d yuan.&quot; % {'c':'hungry','l':22})&lt;br/&gt;调试输出：&lt;br/&gt;I'm hungry. I have 22 yuan.&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;格式符&lt;/p&gt;
&lt;p&gt;格式符为真实值预留位置，并控制显示的格式。格式符可以包含有一个类型码，用以控制显示的类型，如下:&lt;/p&gt;
&lt;p&gt;%s    字符串 (采用str()的显示)&lt;/p&gt;
&lt;p&gt;%r    字符串 (采用repr()的显示)&lt;/p&gt;
&lt;p&gt;%c    单个字符&lt;/p&gt;
&lt;p&gt;%b    二进制整数&lt;/p&gt;
&lt;p&gt;%d    十进制整数&lt;/p&gt;
&lt;p&gt;%i    十进制整数&lt;/p&gt;
&lt;p&gt;%o    八进制整数&lt;/p&gt;
&lt;p&gt;%x    十六进制整数&lt;/p&gt;
&lt;p&gt;%e    指数 (基底写为e)&lt;/p&gt;
&lt;p&gt;%E    指数 (基底写为E)&lt;/p&gt;
&lt;p&gt;%f    浮点数&lt;/p&gt;
&lt;p&gt;%F    浮点数，与上相同&lt;/p&gt;
&lt;p&gt;%g    指数(e)或浮点数 (根据显示长度)&lt;/p&gt;
&lt;p&gt;%G    指数(E)或浮点数 (根据显示长度)&lt;/p&gt;

&lt;p&gt;%%    字符&quot;%&quot;&lt;/p&gt;

&lt;p&gt;可以用如下的方式，对格式进行进一步的控制：&lt;/p&gt;
&lt;p&gt;%[(name)][flags][width].[precision]typecode&lt;/p&gt;
&lt;p&gt;(name)为命名&lt;/p&gt;
&lt;p&gt;flags可以有+,-,' '或0。+表示右对齐。-表示左对齐。' '为一个空格，表示在正数的左侧填充一个空格，从而与负数对齐。0表示使用0填充。&lt;/p&gt;
&lt;p&gt;width表示显示宽度&lt;/p&gt;
&lt;p&gt;precision表示小数点后精度&lt;/p&gt;

&lt;p&gt;比如：&lt;/p&gt;
&lt;p&gt;print(&quot;%+10x&quot; % 10)&lt;br/&gt;print(&quot;%04d&quot; % 5)&lt;br/&gt;print(&quot;%6.3f&quot; % 2.3)&lt;br/&gt; &lt;/p&gt;
&lt;p&gt;上面的width, precision为两个整数。我们可以利用*，来动态代入这两个量。比如：&lt;/p&gt;
&lt;p&gt;print(&quot;%.*f&quot; % (4, 1.2))&lt;br/&gt;Python实际上用4来替换*。所以实际的模板为&quot;%.4f&quot;。&lt;/p&gt;

&lt;p&gt;总结&lt;/p&gt;
&lt;p&gt;Python中内置的%操作符可用于格式化字符串操作，控制字符串的呈现格式。Python中还有其他的格式化字符串的方式，但%操作符的使用是最方便的。&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 15:50:00 +0000</pubDate>
<dc:creator>牧牛人</dc:creator>
<og:description>pythn print格式化输出。 %r 用来做 debug 比较好，因为它会显示变量的原始数据（raw data），而其它的符号则是用来向用户显示输出的。 1. 打印字符串 print (</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/ooo888ooo/p/10434731.html</dc:identifier>
</item>
<item>
<title>深度学习之卷积神经网络(CNN)详解 - w_x_w1985</title>
<link>http://www.cnblogs.com/further-further-further/p/10430073.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/further-further-further/p/10430073.html</guid>
<description>&lt;p&gt;&lt;span&gt;                                           本文系作者原创，转载请注明出处:&lt;a id=&quot;Editor_Edit_hlEntryLink&quot; title=&quot;view: 深度学习之卷积神经网络(CNN)详解&quot; href=&quot;https://www.cnblogs.com/further-further-further/p/10430073.html&quot; target=&quot;_blank&quot;&gt;https://www.cnblogs.com/further-further-further/p/10430073.html&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;1.应用场景&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;2.卷积神经网络结构&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt; 2.1 卷积（convelution）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.2 Relu激活函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.3 池化（pool）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.4 全连接（full connection）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.5 损失函数（softmax_loss）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.6 前向传播（forward propagation）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.7 反向传播（backford propagation）&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;3.代码实现流程图以及介绍&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;4.代码实现（python3.6）&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;5.运行结果以及分析&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;6.参考文献&lt;/span&gt;&lt;/h2&gt;

&lt;h2&gt;&lt;span&gt;1.应用场景&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;卷积神经网络的应用不可谓不广泛，主要有两大类，数据预测和图片处理。数据预测自然不需要多说，图片处理主要包含有图像分类，检测，识别，以及分割方面的应用。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像分类：场景分类，目标分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像检测：显著性检测，物体检测，语义检测等等&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像识别：人脸识别，字符识别，车牌识别，行为识别，步态识别等等&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像分割：前景分割，语义分割&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225110007640-734520127.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;2.卷积神经网络结构&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;卷积神经网络主要是由输入层、卷积层、激活函数、池化层、全连接层、损失函数组成，表面看比较复杂，其实质就是&lt;span&gt;特征提取&lt;/span&gt;以及&lt;span&gt;决策推断&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;要使特征提取尽量准确，就需要将这些网络层结构进行组合，比如经典的卷积神经网络模型AlexNet:5个卷积层+3个池化层+3个连接层结构。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.1 卷积（convolution）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;卷积的作用就是提取特征，因为一次卷积可能提取的特征比较粗糙，所以多次卷积，以及层层纵深卷积，层层提取特征（千万要区别于多次卷积，因为每一层里含有多次卷积）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里可能就有小伙伴问：为什么要进行层层纵深卷积，而且还要每层多次？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你可以理解为物质A有自己的多个特征（高、矮、胖、瘦、、、），所以在物质A上需要多次提取，得到不同的特征，然后这些特征组合后发生化学反应生成物质B，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;而物质B又有一些新的专属于自己的特征，所以需要进一步卷积。这是我个人的理解，不对的话或者有更形象的比喻还请不吝赐教啊。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225143917459-1247536480.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在卷积层中，每一层的卷积核是不一样的。比如AlexNet&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一层：96*11*11（96表示卷积核个数，11表示卷积核矩阵宽*高） stride（步长） = 4  pad（边界补零） = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二层：256*5*5 stride（步长） = 1  pad（边界补零） = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三，四层：384*3*3 stride（步长） = 1  pad（边界补零） = 1&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第五层：256*3*3 stride（步长） = 1  pad（边界补零） = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积的篇幅说了这么多，那么到底是如何进行运算的呢，虽说网络上关于卷积运算原理铺天盖地，但是个人总感觉讲得不够透彻，或者说本人智商有待提高，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;希望通过如下这幅图（某位大神的杰作）来使各位看官们能够真正理解。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225154557731-1535384866.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这里举的例子是一个输入图片（5*5*3），卷积核（3*3*3），有两个（Filter W0，W1），偏置b也有两个（Bios b0，b1），卷积结果Output Volumn（3*3*2），步长stride = 2。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;输入：7*7*3 是因为 pad = 1 （在图片边界行和列都补零，补零的行和的数目是1），&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;（对于彩色图片，一般都是RGB3种颜色，号称3通道，7*7指图片高h * 宽w）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;，补零的作用是能够提取图片边界的特征。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核深度为什么要设置成3呢？这是因为输入是3通道，所以卷积核深度必须与输入的深度相同。至于卷积核宽w，高h则是可以变化的，但是宽高必须相等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,0] = 3 （Output Volumn下浅绿色框结果），这个结果是如何得到的呢？ 其实关键就是矩阵&lt;span&gt;对应位置相乘在相加&lt;span&gt;（千万不要跟矩阵乘法搞混淆啦）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;=&amp;gt; w0[:,:,0] * x[:,:,0]蓝色区域矩阵(R通道) +  w0[:,:,1] * x[:,:,1]蓝色区域矩阵（G通道）+  w0[:,:,2] * x[:,:,2]蓝色区域矩阵（B通道） + &lt;span&gt;b0（千万不能丢，因为 y = w * x + b）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一项  =&amp;gt; 0 * 1 + 0 * 1 + 0 * 1 + 0 * (-1) + 1 * (-1) + 1 * 0 + 0 * (-1) + 1 * 1 + 1 * 0  =  0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二项 =&amp;gt; 0 * (-1) + 0 * (-1) + 0 * 1 + 0 * (-1) + 0 * 1 + 1 * 0 + 0 * (-1) + 2 * 1 + 2 * 0 = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三项 =&amp;gt; 0 * 1 + 0 * 0 + 0 * (-1) + 0 * 0 + 2 * 0 + 2 * 0 + 0 * 1 + 0 * (-1) + 0 * (-1) = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,0] = &amp;gt; 第一项 + 第二项 + 第三项 + b0 = 0 + 2 + 0 + 1 = 3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;o[0,0,1] = -5 又是如何得到的呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因为这里的stride = 2 ，所以 输入的窗口就要滑动两个步长，也就是红色框的区域，而运算跟之前是一样的&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一项  =&amp;gt; 0 * 1 + 0 * 1 + 0 * 1 + 1 * (-1) + 2 * (-1) + 2 * 0 + 1 * (-1) + 1 * 1 + 2 * 0 = -3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二项 =&amp;gt; 0 * (-1) + 0 * (-1) + 0 * 1 + 1 * (-1) + 2 * 1 + 0 * 0 + 2 * (-1) + 1 * 1 + 1 * 0 = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三项 =&amp;gt; 0 * 1 + 0 * 0 + 0 * (-1) + 2 * 0 + 0 * 0 + 1 * 0 + 0 * 1 + 2 * (-1) + 1 * (-1)  = - 3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,1] = &amp;gt; 第一项 + 第二项 + 第三项 + b0 = (-3) + 0 + (-3) + 1 = -5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;之后以此卷积核窗口大小在输入图片上滑动，卷积求出结果，因为有两个卷积核，所有就有两个输出结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里小伙伴可能有个疑问，输出窗口是如何得到的呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里有一个公式：输出窗口宽 w = (输入窗口宽 w - 卷积核宽 w + 2 * pad)/stride  + 1 ，输出高 h  = 输出窗口宽 w&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;以上面例子， 输出窗口宽 w = ( 5 - 3 + 2 * 1)/2 + 1 = 3 ，则输出窗口大小为 3 * 3，因为有2个输出，所以是 3*3*2。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.2 Relu激活函数&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225163325379-1450551226.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;相信看过卷积神经网络结构（CNN）的伙伴们都知道，激活函数无处不在，特别是CNN中，在卷积层后，全连接（FC）后都有激活函数Relu的身影，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么这就自然不得不让我们产生疑问：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;问题1、为什么要用激活函数？它的作用是什么？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;问题2、在CNN中为什么要用Relu，相比于sigmoid，tanh，它的优势在什么地方？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于第1个问题：由 y = w * x + b 可知，如果不用激活函数，每个网络层的输出都是一种线性输出，而我们所处的现实场景，其实更多的是各种非线性的分布。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这也说明了激活函数的作用是将线性分布转化为非线性分布，能更逼近我们的真实场景。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于第2个问题： 先看sigmoid，tanh分布&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225164757082-386473751.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他们在 x -&amp;gt;&lt;img title=&quot;&quot; src=&quot;https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D15/sign=d11bc50915d8bc3ec20802cf838b1dd3/f9198618367adab4f7776b7087d4b31c8701e45a.jpg&quot; alt=&quot;&quot; width=&quot;15&quot; height=&quot;7&quot; align=&quot;absmiddle&quot;/&gt; 时，输出就变成了恒定值，因为求梯度时需要对函数求一阶偏导数，而不论是sigmoid，还是tanhx，他们的偏导都为0，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;也就是存在所谓的&lt;span&gt;梯度消失问题&lt;span&gt;，最终也就会导致权重参数w ， b 无法更新。相比之下，Relu就不存在这样的问题，另外在 x &amp;gt; 0 时，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Relu求导 = 1，这对于反向传播计算dw，db，是能够大大的简化运算的。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;使用sigmoid还会存在梯度爆炸的问题，比如在进行前向传播和反向传播迭代次数非常多的情况下，sigmoid因为是指数函数，其结果中&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;某些值会在迭代中累计，并成指数级增长，最终会出现NaN而导致溢出。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;2.3 池化&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;池化层一般在卷积层+ Relu之后，它的作用是：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;1、减小输入矩阵的大小（只是宽和高，而不是深度），提取主要特征。（不可否认的是，在池化后，特征会有一定的损失，所以，有些经典模型就去掉了池化这一层）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;它的目的是显而易见的，就是在后续操作时能降低运算。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;2、一般采用mean_pooling（均值池化）和max_pooling（最大值池化），对于输入矩阵有translation（平移），rotation（旋转），能够保证特征的不变性。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;mean_pooling 就是输入矩阵池化区域求均值，这里要注意的是池化窗口在输入矩阵滑动的步长跟stride有关，一般stride = 2.（图片是直接盗过来，这里感谢原创）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;最右边7/4 =&amp;gt; (1 + 1 + 2 + 3)/4&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225172838359-1167296564.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;max_pooling 最大值池化，就是每个池化区域的最大值放在输出对应位置上。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225172637537-860553733.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.4 全连接（full connection）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：分类器角色，将特征映射到样本标记空间，本质是矩阵变换（affine）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;至于变换的实现见后面的代码流程图，或者最好是跟一下代码，这样理解更透彻。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.5 损失函数（softmax_loss）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：计算损失loss，从而求出梯度grad。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;常用损失函数有:MSE均方误差，SVM（支持向量机）合页损失函数，Cross Entropy交叉熵损失函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这几种损失函数目前还看不出谁优谁劣，估计只有在具体的应用场景中去验证了。至于这几种损失函数的介绍，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大家可以去参考《常用损失函数小结》&lt;a href=&quot;https://blog.csdn.net/zhangjunp3/article/details/80467350&quot; target=&quot;_blank&quot;&gt;https://blog.csdn.net/zhangjunp3/article/details/80467350&lt;/a&gt;，这个哥们写得比较详细。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在后面的代码实例中，用到的是softmax_loss，它属于Cross Entropy交叉熵损失函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;softmax计算公式：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225185252832-1147975865.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;其中&lt;/span&gt;，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bz_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;span&gt; 是要计算的类别 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 的网络输出，分母是网络输出所有类别之和（共有 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20k&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;10&quot; height=&quot;16&quot;/&gt; 个类别），&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bp_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示第 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类的概率。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;交叉熵损失：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225190245199-312658746.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20y_%7Bi%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是类别 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 的真实标签，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bp_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示第 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类的概率，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20N&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是样本总数，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20k&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是类别数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;梯度:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;   &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20grad_%7Bj%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;47&quot; height=&quot;21&quot;/&gt;  =  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;50&quot; height=&quot;19&quot;/&gt;      当  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; != &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;   &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20grad_%7Bj%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;49&quot; height=&quot;22&quot;/&gt;  =  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;45&quot; height=&quot;17&quot;/&gt;  - 1   当  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; = &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示&lt;span&gt;真实标签对应索引&lt;/span&gt;下预测的目标值，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类别索引。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个有点折磨人，原理讲解以及推导请大家可以参考这位大神的博客：&lt;a href=&quot;http://www.cnblogs.com/zongfa/p/8971213.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/zongfa/p/8971213.html&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.6 前向传播（forward propagation）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;前向传播包含之前的卷积，Relu激活函数，池化（pool），全连接(fc)，可以说，在损失函数之前操作都属于前向传播。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;主要是权重参数w , b 初始化，迭代，以及更新w, b,生成分类器模型。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.7 反向传播（back propagation）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;反向传播包含损失函数，通过梯度计算dw，db，Relu激活函数逆变换，反池化，反全连接。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.8 随机梯度下降（sgd_momentum）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：由梯度grad计算新的权重矩阵w&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;sgd公式：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225200258307-64239916.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中，&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-39&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-40&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-41&quot; class=&quot;mi&quot;&gt;η为学习率，g&lt;sub&gt;t&lt;/sub&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;为x在t时刻的梯度。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;一般我们是将整个数据集分成n个epoch，每个epoch再分成m个batch，每次更新都利用一个batch的数据，而非整个训练集。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;优点：batch的方法可以减少机器的压力，并且可以更快地收敛。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;缺点：其更新方向完全依赖于当前的batch，因而其更新十分不稳定。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为了解决这个问题，momentum就横空出世了，具体原理详解见下路派出所（这名字霸气）的博客&lt;a href=&quot;http://www.cnblogs.com/callyblog/p/8299074.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/callyblog/p/8299074.html&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225201006070-1516485048.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中，&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-74&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-75&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-76&quot; class=&quot;mi&quot;&gt;ρ 即momentum，表示要在多大程度上保留原来的更新方向，这个值在0-1之间，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-74&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-75&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-76&quot; class=&quot;mi&quot;&gt;当梯度不那么大时，改为0.9。&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-77&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-78&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-79&quot; class=&quot;mi&quot;&gt;η 是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-80&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-81&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-82&quot; class=&quot;mi&quot;&gt;ρ 与 &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-83&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-84&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-85&quot; class=&quot;mi&quot;&gt;η 之和不一定为1。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;3.代码实现流程图以及介绍&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;代码流程图：费了老大劲，终于弄完了，希望对各位看官们有所帮助，建议对比流程图和跟踪代码，加深对原理的理解。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;特别是前向传播和反向传播维度的变换，需要重点关注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225221658952-232231034.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;4.代码实现&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;当然，代码的整个实现是某位大神实现的，我只是在上面做了些小改动以及重点函数做了些注释，有不妥之处也希望大家不吝指教。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;因为原始图片数据集太大，不好上传，大家可以直接在&lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot; target=&quot;_blank&quot;&gt;http://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;下载CIFAR-10 python version，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;有163M，放在代码文件同路径下即可。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222131668-2140712885.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; start.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('7779ba27-1e37-4387-bbe9-eba910650fb9')&quot; readability=&quot;50&quot;&gt;&lt;img id=&quot;code_img_closed_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('7779ba27-1e37-4387-bbe9-eba910650fb9',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;95&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; matplotlib.pyplot as plt
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;&lt;span&gt;同路径下py模块引用&lt;/span&gt;&lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; solver
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; cnn
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; solver
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; cnn
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 获取样本数据&lt;/span&gt;
&lt;span&gt;16&lt;/span&gt; data =&lt;span&gt; data_utils.get_CIFAR10_data()
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; model初始化（权重因子以及对应偏置 w1,b1 ,w2,b2 ,w3,b3，数量取决于网络层数）&lt;/span&gt;
&lt;span&gt;18&lt;/span&gt; model = cnn.ThreeLayerConvNet(reg=0.9&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; solver =&lt;span&gt; solver.Solver(model, data,
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                 lr_decay=0.95&lt;span&gt;,                
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;                 print_every=10, num_epochs=5, batch_size=2&lt;span&gt;, 
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;                 update_rule=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;sgd_momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,                
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                 optim_config={&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 5e-4, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 0.9&lt;span&gt;})
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练，获取最佳model&lt;/span&gt;
&lt;span&gt;25&lt;/span&gt; &lt;span&gt;solver.train()                 
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; 
&lt;span&gt;27&lt;/span&gt; plt.subplot(2, 1, 1&lt;span&gt;) 
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; plt.title(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Training loss&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt; plt.plot(solver.loss_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; plt.xlabel(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Iteration&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; 
&lt;span&gt;32&lt;/span&gt; plt.subplot(2, 1, 2&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; plt.title(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Accuracy&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; plt.plot(solver.train_acc_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;-o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, label=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; plt.plot(solver.val_acc_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;-o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, label=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; plt.plot([0.5] * len(solver.val_acc_history), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;k--&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt; plt.xlabel(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Epoch&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt; plt.legend(loc=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;lower right&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt; plt.gcf().set_size_inches(15, 12&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; &lt;span&gt;plt.show()
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt; 
&lt;span&gt;42&lt;/span&gt; 
&lt;span&gt;43&lt;/span&gt; best_model =&lt;span&gt; model
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; y_test_pred = np.argmax(best_model.loss(data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt; y_val_pred = np.argmax(best_model.loss(data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Validation set accuracy: &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,(y_val_pred == data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]).mean())
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Test set accuracy: &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, (y_test_pred == data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]).mean())
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Validation set accuracy:  about 52.9%&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Test set accuracy:  about 54.7%&lt;/span&gt;
&lt;span&gt;50&lt;/span&gt; 
&lt;span&gt;51&lt;/span&gt; 
&lt;span&gt;52&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Visualize the weights of the best network&lt;/span&gt;
&lt;span&gt;53&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;54&lt;/span&gt; &lt;span&gt;from vis_utils import visualize_grid
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt; 
&lt;span&gt;56&lt;/span&gt; &lt;span&gt;def show_net_weights(net):    
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; &lt;span&gt;    W1 = net.params['W1']    
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt; &lt;span&gt;    W1 = W1.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)    
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt; &lt;span&gt;    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))   
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt; &lt;span&gt;    plt.gca().axis('off')    
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt; &lt;span&gt;show_net_weights(best_model)
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt; &lt;span&gt;plt.show()
&lt;/span&gt;&lt;span&gt;63&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p&gt; cnn.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('6c8f719a-23b6-458e-b55b-b295adc1ba2a')&quot; readability=&quot;62&quot;&gt;&lt;img id=&quot;code_img_closed_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('6c8f719a-23b6-458e-b55b-b295adc1ba2a',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;119&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layer_utils
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layer_utils
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt;  ThreeLayerConvNet(object):
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;12&lt;/span&gt; &lt;span&gt;    A three-layer convolutional network with the following architecture:       
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;       conv - relu - 2x2 max pool - affine - relu - affine - softmax
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt; 
&lt;span&gt;16&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7&lt;span&gt;,             
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;                  hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                  dtype=&lt;span&gt;np.float32):
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;         self.params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         self.reg =&lt;span&gt; reg
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         self.dtype =&lt;span&gt; dtype
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Initialize weights and biases&lt;/span&gt;
&lt;span&gt;24&lt;/span&gt;         C, H, W =&lt;span&gt; input_dim
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale *&lt;span&gt; np.random.randn(num_filters, C, filter_size, filter_size)
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(num_filters)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale * np.random.randn(num_filters*H*W//4&lt;span&gt;, hidden_dim)
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(hidden_dim)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale *&lt;span&gt; np.random.randn(hidden_dim, num_classes)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(num_classes)
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; 
&lt;span&gt;32&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.params.items():    
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             self.params[k] =&lt;span&gt; v.astype(dtype)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; 
&lt;span&gt;35&lt;/span&gt; 
&lt;span&gt;36&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; loss(self, X, y=&lt;span&gt;None):
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;         W1, b1 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;         W2, b2 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;         W3, b3 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pass conv_param to the forward pass for the convolutional layer&lt;/span&gt;
&lt;span&gt;42&lt;/span&gt;         filter_size = W1.shape[2&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;         conv_param = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: (filter_size - 1) // 2&lt;span&gt;}
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pass pool_param to the forward pass for the max-pooling layer&lt;/span&gt;
&lt;span&gt;46&lt;/span&gt;         pool_param = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2&lt;span&gt;}
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; 
&lt;span&gt;48&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; compute the forward pass&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt;         a1, cache1 =&lt;span&gt; layer_utils.conv_relu_pool_forward(X, W1, b1, conv_param, pool_param)
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;         a2, cache2 =&lt;span&gt; layer_utils.affine_relu_forward(a1, W2, b2)
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt;         scores, cache3 =&lt;span&gt; layers.affine_forward(a2, W3, b3)
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt; 
&lt;span&gt;53&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; y &lt;span&gt;is&lt;/span&gt;&lt;span&gt; None:    
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;             &lt;span&gt;return&lt;/span&gt;&lt;span&gt; scores
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt; 
&lt;span&gt;56&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; compute the backward pass&lt;/span&gt;
&lt;span&gt;57&lt;/span&gt;         data_loss, dscores =&lt;span&gt; layers.softmax_loss(scores, y)
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt;         da2, dW3, db3 =&lt;span&gt; layers.affine_backward(dscores, cache3)
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;         da1, dW2, db2 =&lt;span&gt; layer_utils.affine_relu_backward(da2, cache2)
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt;         dX, dW1, db1 =&lt;span&gt; layer_utils.conv_relu_pool_backward(da1, cache1)
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt; 
&lt;span&gt;62&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Add regularization 引入修正因子，重新计算损失，梯度&lt;/span&gt;
&lt;span&gt;63&lt;/span&gt;         dW1 += self.reg *&lt;span&gt; W1
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;         dW2 += self.reg *&lt;span&gt; W2
&lt;/span&gt;&lt;span&gt;65&lt;/span&gt;         dW3 += self.reg *&lt;span&gt; W3
&lt;/span&gt;&lt;span&gt;66&lt;/span&gt;         reg_loss = 0.5 * self.reg * sum(np.sum(W * W) &lt;span&gt;for&lt;/span&gt; W &lt;span&gt;in&lt;/span&gt;&lt;span&gt; [W1, W2, W3])
&lt;/span&gt;&lt;span&gt;67&lt;/span&gt; 
&lt;span&gt;68&lt;/span&gt;         loss = data_loss +&lt;span&gt; reg_loss
&lt;/span&gt;&lt;span&gt;69&lt;/span&gt;         grads = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: db1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: db2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW3, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: db3}
&lt;/span&gt;&lt;span&gt;70&lt;/span&gt; 
&lt;span&gt;71&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; loss, grads
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;data.utils.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('2c746635-9f3b-4008-9c7e-77cc52d615f0')&quot; readability=&quot;97&quot;&gt;&lt;img id=&quot;code_img_closed_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('2c746635-9f3b-4008-9c7e-77cc52d615f0',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;189&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt;  2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle 
&lt;/span&gt;&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; os
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt; 
&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt;from scipy.misc import imread&lt;/span&gt;
&lt;span&gt;  7&lt;/span&gt; 
&lt;span&gt;  8&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_CIFAR_batch(filename):
&lt;/span&gt;&lt;span&gt;  9&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; load single batch of cifar &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 10&lt;/span&gt;   with open(filename, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;rb&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) as f:
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     datadict = pickle.load(f, encoding=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;bytes&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt;     X = datadict[b&lt;span&gt;'&lt;/span&gt;&lt;span&gt;data&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt; 13&lt;/span&gt;     Y = datadict[b&lt;span&gt;'&lt;/span&gt;&lt;span&gt;labels&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt;     X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;float&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt;     Y =&lt;span&gt; np.array(Y)
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; X, Y
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; 
&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_CIFAR10(ROOT):
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; load all of cifar &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 20&lt;/span&gt;   xs =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt;   ys =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt;   &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt; range(1,2&lt;span&gt;):
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt;     f = os.path.join(ROOT, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;data_batch_%d&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (b, ))
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt;     X, Y =&lt;span&gt; load_CIFAR_batch(f)
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;    xs.append(X)
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;    ys.append(Y)    
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt;   Xtr =&lt;span&gt; np.concatenate(xs)
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;   Ytr =&lt;span&gt; np.concatenate(ys)
&lt;/span&gt;&lt;span&gt; 29&lt;/span&gt;   &lt;span&gt;del&lt;/span&gt;&lt;span&gt; X, Y
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;   Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;test_batch&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;))
&lt;/span&gt;&lt;span&gt; 31&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; Xtr, Ytr, Xte, Yte
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt; 
&lt;span&gt; 33&lt;/span&gt; 
&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; get_CIFAR10_data(num_training=500, num_validation=50, num_test=50&lt;span&gt;):
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; 
&lt;span&gt; 36&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;    it for classifiers. These are the same steps as we used for the SVM, but
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;    condensed to a single function.
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 41&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Load the raw CIFAR-10 data&lt;/span&gt;
&lt;span&gt; 42&lt;/span&gt; 
&lt;span&gt; 43&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;cifar10_dir = 'C://download//cifar-10-python//cifar-10-batches-py//'&lt;/span&gt;
&lt;span&gt; 44&lt;/span&gt;     cifar10_dir = &lt;span&gt;'&lt;/span&gt;&lt;span&gt;.\\cifar-10-batches-py\\&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;span&gt; 45&lt;/span&gt;     X_train, y_train, X_test, y_test =&lt;span&gt; load_CIFAR10(cifar10_dir)
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;&lt;span&gt; (X_train.shape)
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Subsample the data&lt;/span&gt;
&lt;span&gt; 48&lt;/span&gt;     mask = range(num_training, num_training +&lt;span&gt; num_validation)
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt;     X_val =&lt;span&gt; X_train[mask]
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt;     y_val =&lt;span&gt; y_train[mask]
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt;     mask =&lt;span&gt; range(num_training)
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt;     X_train =&lt;span&gt; X_train[mask]
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt;     y_train =&lt;span&gt; y_train[mask]
&lt;/span&gt;&lt;span&gt; 54&lt;/span&gt;     mask =&lt;span&gt; range(num_test)
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt;     X_test =&lt;span&gt; X_test[mask]
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt;     y_test =&lt;span&gt; y_test[mask]
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; 
&lt;span&gt; 58&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 标准化数据，求样本均值，然后 样本 - 样本均值，作用：使样本数据更收敛一些，便于后续处理&lt;/span&gt;
&lt;span&gt; 59&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Normalize the data: subtract the mean image&lt;/span&gt;
&lt;span&gt; 60&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果2维空间 m*n np.mean()后 =&amp;gt; 1*n&lt;/span&gt;
&lt;span&gt; 61&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 对于4维空间 m*n*k*j np.mean()后 =&amp;gt; 1*n*k*j&lt;/span&gt;
&lt;span&gt; 62&lt;/span&gt;     mean_image = np.mean(X_train, axis=&lt;span&gt;0)
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     X_train -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     X_val -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt;     X_test -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt; 
&lt;span&gt; 67&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 把通道channel 提前&lt;/span&gt;
&lt;span&gt; 68&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Transpose so that channels come first&lt;/span&gt;
&lt;span&gt; 69&lt;/span&gt;     X_train = X_train.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt;     X_val = X_val.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 71&lt;/span&gt;     X_test = X_test.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt; 
&lt;span&gt; 73&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Package data into a dictionary&lt;/span&gt;
&lt;span&gt; 74&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 75&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_train, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_train,
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_val, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_val,
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_test, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_test,
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt;     
&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;def load_tiny_imagenet(path, dtype=np.float32):
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt;   
&lt;span&gt; 83&lt;/span&gt; &lt;span&gt;  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and
&lt;/span&gt;&lt;span&gt; 84&lt;/span&gt; &lt;span&gt;  TinyImageNet-200 have the same directory structure, so this can be used
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt; &lt;span&gt;  to load any of them.
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt; 
&lt;span&gt; 87&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt; &lt;span&gt;  - path: String giving path to the directory to load.
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; &lt;span&gt;  - dtype: numpy datatype used to load the data.
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt; 
&lt;span&gt; 91&lt;/span&gt; &lt;span&gt;  Returns: A tuple of
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;  - class_names: A list where class_names[i] is a list of strings giving the
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;    WordNet names for class i in the loaded dataset.
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;  - X_train: (N_tr, 3, 64, 64) array of training images
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;  - y_train: (N_tr,) array of training labels
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;  - X_val: (N_val, 3, 64, 64) array of validation images
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;  - y_val: (N_val,) array of validation labels
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt; &lt;span&gt;  - X_test: (N_test, 3, 64, 64) array of testing images.
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt; &lt;span&gt;  - y_test: (N_test,) array of test labels; if test labels are not available
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt; &lt;span&gt;    (such as in student code) then y_test will be None.
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt;   
&lt;span&gt;102&lt;/span&gt; &lt;span&gt;  # First load wnids
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'wnids.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt; &lt;span&gt;    wnids = [x.strip() for x in f]
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt; 
&lt;span&gt;106&lt;/span&gt; &lt;span&gt;  # Map wnids to integer labels
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt; &lt;span&gt;  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt; 
&lt;span&gt;109&lt;/span&gt; &lt;span&gt;  # Use words.txt to get names for each class
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'words.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;111&lt;/span&gt; &lt;span&gt;    wnid_to_words = dict(line.split('\t') for line in f)
&lt;/span&gt;&lt;span&gt;112&lt;/span&gt; &lt;span&gt;    for wnid, words in wnid_to_words.iteritems():
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt; &lt;span&gt;      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt; &lt;span&gt;  class_names = [wnid_to_words[wnid] for wnid in wnids]
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt; 
&lt;span&gt;116&lt;/span&gt; &lt;span&gt;  # Next load training data.
&lt;/span&gt;&lt;span&gt;117&lt;/span&gt; &lt;span&gt;  X_train = []
&lt;/span&gt;&lt;span&gt;118&lt;/span&gt; &lt;span&gt;  y_train = []
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt; &lt;span&gt;  for i, wnid in enumerate(wnids):
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt; &lt;span&gt;    if (i + 1) % 20 == 0:
&lt;/span&gt;&lt;span&gt;121&lt;/span&gt; &lt;span&gt;      print 'loading training data for synset %d / %d' % (i + 1, len(wnids))
&lt;/span&gt;&lt;span&gt;122&lt;/span&gt; &lt;span&gt;    # To figure out the filenames we need to open the boxes file
&lt;/span&gt;&lt;span&gt;123&lt;/span&gt; &lt;span&gt;    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt; &lt;span&gt;    with open(boxes_file, 'r') as f:
&lt;/span&gt;&lt;span&gt;125&lt;/span&gt; &lt;span&gt;      filenames = [x.split('\t')[0] for x in f]
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt; &lt;span&gt;    num_images = len(filenames)
&lt;/span&gt;&lt;span&gt;127&lt;/span&gt;     
&lt;span&gt;128&lt;/span&gt; &lt;span&gt;    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;129&lt;/span&gt; &lt;span&gt;    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)
&lt;/span&gt;&lt;span&gt;130&lt;/span&gt; &lt;span&gt;    for j, img_file in enumerate(filenames):
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt; &lt;span&gt;      img_file = os.path.join(path, 'train', wnid, 'images', img_file)
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt; &lt;span&gt;      img = imread(img_file)
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; &lt;span&gt;      if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;134&lt;/span&gt; &lt;span&gt;        ## grayscale file
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; &lt;span&gt;        img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;136&lt;/span&gt; &lt;span&gt;      X_train_block[j] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;137&lt;/span&gt; &lt;span&gt;    X_train.append(X_train_block)
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt; &lt;span&gt;    y_train.append(y_train_block)
&lt;/span&gt;&lt;span&gt;139&lt;/span&gt;       
&lt;span&gt;140&lt;/span&gt; &lt;span&gt;  # We need to concatenate all training data
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt; &lt;span&gt;  X_train = np.concatenate(X_train, axis=0)
&lt;/span&gt;&lt;span&gt;142&lt;/span&gt; &lt;span&gt;  y_train = np.concatenate(y_train, axis=0)
&lt;/span&gt;&lt;span&gt;143&lt;/span&gt;   
&lt;span&gt;144&lt;/span&gt; &lt;span&gt;  # Next load validation data
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt; &lt;span&gt;    img_files = []
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt; &lt;span&gt;    val_wnids = []
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt; &lt;span&gt;    for line in f:
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; &lt;span&gt;      img_file, wnid = line.split('\t')[:2]
&lt;/span&gt;&lt;span&gt;150&lt;/span&gt; &lt;span&gt;      img_files.append(img_file)
&lt;/span&gt;&lt;span&gt;151&lt;/span&gt; &lt;span&gt;      val_wnids.append(wnid)
&lt;/span&gt;&lt;span&gt;152&lt;/span&gt; &lt;span&gt;    num_val = len(img_files)
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt; &lt;span&gt;    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt; &lt;span&gt;    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt; &lt;span&gt;    for i, img_file in enumerate(img_files):
&lt;/span&gt;&lt;span&gt;156&lt;/span&gt; &lt;span&gt;      img_file = os.path.join(path, 'val', 'images', img_file)
&lt;/span&gt;&lt;span&gt;157&lt;/span&gt; &lt;span&gt;      img = imread(img_file)
&lt;/span&gt;&lt;span&gt;158&lt;/span&gt; &lt;span&gt;      if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;159&lt;/span&gt; &lt;span&gt;        img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;160&lt;/span&gt; &lt;span&gt;      X_val[i] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt; 
&lt;span&gt;162&lt;/span&gt; &lt;span&gt;  # Next load test images
&lt;/span&gt;&lt;span&gt;163&lt;/span&gt; &lt;span&gt;  # Students won't have test labels, so we need to iterate over files in the
&lt;/span&gt;&lt;span&gt;164&lt;/span&gt; &lt;span&gt;  # images directory.
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt; &lt;span&gt;  img_files = os.listdir(os.path.join(path, 'test', 'images'))
&lt;/span&gt;&lt;span&gt;166&lt;/span&gt; &lt;span&gt;  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;167&lt;/span&gt; &lt;span&gt;  for i, img_file in enumerate(img_files):
&lt;/span&gt;&lt;span&gt;168&lt;/span&gt; &lt;span&gt;    img_file = os.path.join(path, 'test', 'images', img_file)
&lt;/span&gt;&lt;span&gt;169&lt;/span&gt; &lt;span&gt;    img = imread(img_file)
&lt;/span&gt;&lt;span&gt;170&lt;/span&gt; &lt;span&gt;    if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;171&lt;/span&gt; &lt;span&gt;      img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;172&lt;/span&gt; &lt;span&gt;    X_test[i] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;173&lt;/span&gt; 
&lt;span&gt;174&lt;/span&gt; &lt;span&gt;  y_test = None
&lt;/span&gt;&lt;span&gt;175&lt;/span&gt; &lt;span&gt;  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')
&lt;/span&gt;&lt;span&gt;176&lt;/span&gt; &lt;span&gt;  if os.path.isfile(y_test_file):
&lt;/span&gt;&lt;span&gt;177&lt;/span&gt; &lt;span&gt;    with open(y_test_file, 'r') as f:
&lt;/span&gt;&lt;span&gt;178&lt;/span&gt; &lt;span&gt;      img_file_to_wnid = {}
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;      for line in f:
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; &lt;span&gt;        line = line.split('\t')
&lt;/span&gt;&lt;span&gt;181&lt;/span&gt; &lt;span&gt;        img_file_to_wnid[line[0]] = line[1]
&lt;/span&gt;&lt;span&gt;182&lt;/span&gt; &lt;span&gt;    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]
&lt;/span&gt;&lt;span&gt;183&lt;/span&gt; &lt;span&gt;    y_test = np.array(y_test)
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt;   
&lt;span&gt;185&lt;/span&gt; &lt;span&gt;  return class_names, X_train, y_train, X_val, y_val, X_test, y_test
&lt;/span&gt;&lt;span&gt;186&lt;/span&gt; 
&lt;span&gt;187&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;188&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_models(models_dir):
&lt;/span&gt;&lt;span&gt;189&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;190&lt;/span&gt; &lt;span&gt;  Load saved models from disk. This will attempt to unpickle all files in a
&lt;/span&gt;&lt;span&gt;191&lt;/span&gt; &lt;span&gt;  directory; any files that give errors on unpickling (such as README.txt) will
&lt;/span&gt;&lt;span&gt;192&lt;/span&gt; &lt;span&gt;  be skipped.
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt; 
&lt;span&gt;194&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt; &lt;span&gt;  - models_dir: String giving the path to a directory containing model files.
&lt;/span&gt;&lt;span&gt;196&lt;/span&gt; &lt;span&gt;    Each model file is a pickled dictionary with a 'model' field.
&lt;/span&gt;&lt;span&gt;197&lt;/span&gt; 
&lt;span&gt;198&lt;/span&gt; &lt;span&gt;  Returns:
&lt;/span&gt;&lt;span&gt;199&lt;/span&gt; &lt;span&gt;  A dictionary mapping model file names to models.
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;201&lt;/span&gt;   models =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;202&lt;/span&gt;   &lt;span&gt;for&lt;/span&gt; model_file &lt;span&gt;in&lt;/span&gt;&lt;span&gt; os.listdir(models_dir):
&lt;/span&gt;&lt;span&gt;203&lt;/span&gt;     with open(os.path.join(models_dir, model_file), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;rb&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) as f:
&lt;/span&gt;&lt;span&gt;204&lt;/span&gt;       &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;205&lt;/span&gt;         models[model_file] = pickle.load(f)[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;model&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt;       &lt;span&gt;except&lt;/span&gt;&lt;span&gt; pickle.UnpicklingError:
&lt;/span&gt;&lt;span&gt;207&lt;/span&gt;         &lt;span&gt;continue&lt;/span&gt;
&lt;span&gt;208&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt; models
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;layer.utils.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('9472e966-549f-4722-ad84-39f2ef8b3ad3')&quot; readability=&quot;65&quot;&gt;&lt;img id=&quot;code_img_closed_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('9472e966-549f-4722-ad84-39f2ef8b3ad3',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;125&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;   &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;   &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; 
&lt;span&gt; 7&lt;/span&gt; 
&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_relu_forward(x, w, b):
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;12&lt;/span&gt; &lt;span&gt;  Convenience layer that perorms an affine transform followed by a ReLU
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;  - x: Input to the affine layer
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;  - w, b: Weights for the affine layer
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; 
&lt;span&gt;18&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;  - out: Output from the ReLU
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;22&lt;/span&gt;   a, fc_cache =&lt;span&gt; layers.affine_forward(x, w, b)
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;   out, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;   cache =&lt;span&gt; (fc_cache, relu_cache)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; 
&lt;span&gt;27&lt;/span&gt; 
&lt;span&gt;28&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_relu_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;30&lt;/span&gt; &lt;span&gt;  Backward pass for the affine-relu convenience layer
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;32&lt;/span&gt;   fc_cache, relu_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(dout, relu_cache)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.affine_backward(da, fc_cache)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; 
&lt;span&gt;37&lt;/span&gt; 
&lt;span&gt;38&lt;/span&gt; &lt;span&gt;pass&lt;/span&gt;
&lt;span&gt;39&lt;/span&gt; 
&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_forward(x, w, b, conv_param):
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt; &lt;span&gt;  A convenience layer that performs a convolution followed by a ReLU.
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; &lt;span&gt;  - x: Input to the convolutional layer
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;  - w, b, conv_param: Weights and parameters for the convolutional layer
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;   
&lt;span&gt;49&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt; &lt;span&gt;  - out: Output from the ReLU
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;53&lt;/span&gt;   a, conv_cache =&lt;span&gt; layers.conv_forward_fast(x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;   out, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;   cache =&lt;span&gt; (conv_cache, relu_cache)
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; 
&lt;span&gt;58&lt;/span&gt; 
&lt;span&gt;59&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;61&lt;/span&gt; &lt;span&gt;  Backward pass for the conv-relu convenience layer.
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;63&lt;/span&gt;   conv_cache, relu_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(dout, relu_cache)
&lt;/span&gt;&lt;span&gt;65&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.conv_backward_fast(da, conv_cache)
&lt;/span&gt;&lt;span&gt;66&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;67&lt;/span&gt; 
&lt;span&gt;68&lt;/span&gt; 
&lt;span&gt;69&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_pool_forward(x, w, b, conv_param, pool_param):
&lt;/span&gt;&lt;span&gt;70&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;71&lt;/span&gt; &lt;span&gt;  Convenience layer that performs a convolution, a ReLU, and a pool.
&lt;/span&gt;&lt;span&gt;72&lt;/span&gt; 
&lt;span&gt;73&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;74&lt;/span&gt; &lt;span&gt;  - x: Input to the convolutional layer
&lt;/span&gt;&lt;span&gt;75&lt;/span&gt; &lt;span&gt;  - w, b, conv_param: Weights and parameters for the convolutional layer
&lt;/span&gt;&lt;span&gt;76&lt;/span&gt; &lt;span&gt;  - pool_param: Parameters for the pooling layer
&lt;/span&gt;&lt;span&gt;77&lt;/span&gt; 
&lt;span&gt;78&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;79&lt;/span&gt; &lt;span&gt;  - out: Output from the pooling layer
&lt;/span&gt;&lt;span&gt;80&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;81&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;82&lt;/span&gt;   a, conv_cache =&lt;span&gt; layers.conv_forward_naive(x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;83&lt;/span&gt;   s, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;84&lt;/span&gt;   out, pool_cache =&lt;span&gt; layers.max_pool_forward_naive(s, pool_param)
&lt;/span&gt;&lt;span&gt;85&lt;/span&gt;   cache =&lt;span&gt; (conv_cache, relu_cache, pool_cache)
&lt;/span&gt;&lt;span&gt;86&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;87&lt;/span&gt; 
&lt;span&gt;88&lt;/span&gt; 
&lt;span&gt;89&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_pool_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;90&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;91&lt;/span&gt; &lt;span&gt;  Backward pass for the conv-relu-pool convenience layer
&lt;/span&gt;&lt;span&gt;92&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;93&lt;/span&gt;   conv_cache, relu_cache, pool_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;94&lt;/span&gt;   ds =&lt;span&gt; layers.max_pool_backward_naive(dout, pool_cache)
&lt;/span&gt;&lt;span&gt;95&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(ds, relu_cache)
&lt;/span&gt;&lt;span&gt;96&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.conv_backward_naive(da, conv_cache)
&lt;/span&gt;&lt;span&gt;97&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt; dx, dw, db
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;layers.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('3f8ab327-6695-4ef0-943b-74fdc83786ed')&quot; readability=&quot;150&quot;&gt;&lt;img id=&quot;code_img_closed_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('3f8ab327-6695-4ef0-943b-74fdc83786ed',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;295&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; 
&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;全连接层：矩阵变换，获取对应目标相同的行与列
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt; &lt;span&gt;输入x: 2*32*16*16 
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;输入x_row: 2*8192
&lt;/span&gt;&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;超参w：8192*100
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt; &lt;span&gt;输出：矩阵乘法 2*8192 -&amp;gt;8192*100 =&amp;gt;2*100
&lt;/span&gt;&lt;span&gt;  9&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 10&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_forward(x, w, b):   
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 12&lt;/span&gt; &lt;span&gt;    Computes the forward pass for an affine (fully-connected) layer. 
&lt;/span&gt;&lt;span&gt; 13&lt;/span&gt; &lt;span&gt;    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N   
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;    examples, where each example x[i] has shape (d_1, ..., d_k). We will    
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;    reshape each input into a vector of dimension D = d_1 * ... * d_k, and    
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; &lt;span&gt;    then transform it to an output vector of dimension M.    
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)    
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; &lt;span&gt;    - w: A numpy array of weights, of shape (D, M)    
&lt;/span&gt;&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;    - b: A numpy array of biases, of shape (M,)   
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt; &lt;span&gt;    - out: output, of shape (N, M)    
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;    - cache: (x, w, b)   
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 25&lt;/span&gt;     out =&lt;span&gt; None
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Reshape x into rows&lt;/span&gt;
&lt;span&gt; 27&lt;/span&gt;     N =&lt;span&gt; x.shape[0]
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;     x_row = x.reshape(N, -1)         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D) -1表示不知道多少列，指定行，就能算出列 = 2 * 32 * 16 * 16/2 = 8192&lt;/span&gt;
&lt;span&gt; 29&lt;/span&gt;     out = np.dot(x_row, w) + b       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,M) 2*8192 8192*100 =&amp;gt;2 * 100&lt;/span&gt;
&lt;span&gt; 30&lt;/span&gt;     cache =&lt;span&gt; (x, w, b)
&lt;/span&gt;&lt;span&gt; 31&lt;/span&gt; 
&lt;span&gt; 32&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;反向传播之affine矩阵变换
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; &lt;span&gt;根据dout求出dx,dw,db
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt; &lt;span&gt;由 out = w * x =&amp;gt;
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;dx = dout * w
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;dw = dout * x
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;db = dout * 1
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; &lt;span&gt;因为dx 与 x，dw 与 w，db 与 b 大小（维度）必须相同
&lt;/span&gt;&lt;span&gt; 41&lt;/span&gt; &lt;span&gt;dx = dout * wT  矩阵乘法
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; &lt;span&gt;dw = dxT * dout 矩阵乘法
&lt;/span&gt;&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;db = dout 按列求和
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_backward(dout, cache):   
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 47&lt;/span&gt; &lt;span&gt;    Computes the backward pass for an affine layer.    
&lt;/span&gt;&lt;span&gt; 48&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;    - dout: Upstream derivative, of shape (N, M)    
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; &lt;span&gt;    - cache: Tuple of: 
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, d_1, ... d_k)    
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    - w: Weights, of shape (D, M)    
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt; &lt;span&gt;    Returns a tuple of:   
&lt;/span&gt;&lt;span&gt; 54&lt;/span&gt; &lt;span&gt;    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt; &lt;span&gt;      dx = dout * w
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt; &lt;span&gt;    - dw: Gradient with respect to w, of shape (D, M)
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; &lt;span&gt;      dw = dout * x
&lt;/span&gt;&lt;span&gt; 58&lt;/span&gt; &lt;span&gt;    - db: Gradient with respect to b, of shape (M,)
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt; &lt;span&gt;      db = dout * 1
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 61&lt;/span&gt; 
&lt;span&gt; 62&lt;/span&gt;     x, w, b =&lt;span&gt; cache    
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     dx, dw, db =&lt;span&gt; None, None, None
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     dx = np.dot(dout, w.T)                       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D)&lt;/span&gt;
&lt;span&gt; 65&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; dx维度必须跟x维度相同&lt;/span&gt;
&lt;span&gt; 66&lt;/span&gt;     dx = np.reshape(dx, x.shape)                 &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,d1,...,d_k)&lt;/span&gt;
&lt;span&gt; 67&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 转换成二维矩阵&lt;/span&gt;
&lt;span&gt; 68&lt;/span&gt;     x_row = x.reshape(x.shape[0], -1)            &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D)&lt;/span&gt;
&lt;span&gt; 69&lt;/span&gt;     dw = np.dot(x_row.T, dout)                   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (D,M)&lt;/span&gt;
&lt;span&gt; 70&lt;/span&gt; 
&lt;span&gt; 71&lt;/span&gt;     db = np.sum(dout, axis=0, keepdims=True)     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (1,M)    &lt;/span&gt;
&lt;span&gt; 72&lt;/span&gt; 
&lt;span&gt; 73&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt; 74&lt;/span&gt; 
&lt;span&gt; 75&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; relu_forward(x):   
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; 激活函数，解决sigmoid梯度消失问题，网络性能比sigmoid更好
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    Computes the forward pass for a layer of rectified linear units (ReLUs).    
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    Input:    
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    - x: Inputs, of any shape    
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - out: Output, of the same shape as x    
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;    - cache: x    
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;   
&lt;span&gt; 84&lt;/span&gt;     out =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt;     out =&lt;span&gt; ReLU(x)    
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;     cache =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt; 87&lt;/span&gt; 
&lt;span&gt; 88&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; 
&lt;span&gt; 90&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; relu_backward(dout, cache):   
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;  
&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;    Computes the backward pass for a layer of rectified linear units (ReLUs).   
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;    Input:    
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;    - dout: Upstream derivatives, of any shape    
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;    - cache: Input x, of same shape as dout    
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;    Returns:    
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;    - dx: Gradient with respect to x    
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 99&lt;/span&gt;     dx, x =&lt;span&gt; None, cache    
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt;     dx =&lt;span&gt; dout    
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt;     dx[x &amp;lt;= 0] =&lt;span&gt; 0    
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt; 
&lt;span&gt;103&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt; 
&lt;span&gt;105&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; svm_loss(x, y):   
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;107&lt;/span&gt; &lt;span&gt;    Computes the loss and gradient using for multiclass SVM classification.    
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class         
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; &lt;span&gt;         for the ith input.    
&lt;/span&gt;&lt;span&gt;111&lt;/span&gt; &lt;span&gt;    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and         
&lt;/span&gt;&lt;span&gt;112&lt;/span&gt; &lt;span&gt;         0 &amp;lt;= y[i] &amp;lt; C   
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss   
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt; &lt;span&gt;    - dx: Gradient of the loss with respect to x    
&lt;/span&gt;&lt;span&gt;116&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;117&lt;/span&gt;     N =&lt;span&gt; x.shape[0]   
&lt;/span&gt;&lt;span&gt;118&lt;/span&gt;     correct_class_scores =&lt;span&gt; x[np.arange(N), y]    
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt;     margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt;     margins[np.arange(N), y] =&lt;span&gt; 0   
&lt;/span&gt;&lt;span&gt;121&lt;/span&gt;     loss = np.sum(margins) /&lt;span&gt; N   
&lt;/span&gt;&lt;span&gt;122&lt;/span&gt;     num_pos = np.sum(margins &amp;gt; 0, axis=1&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;123&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)   
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt;     dx[margins &amp;gt; 0] = 1    
&lt;span&gt;125&lt;/span&gt;     dx[np.arange(N), y] -=&lt;span&gt; num_pos    
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt;     dx /=&lt;span&gt; N    
&lt;/span&gt;&lt;span&gt;127&lt;/span&gt; 
&lt;span&gt;128&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; loss, dx
&lt;/span&gt;&lt;span&gt;129&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;130&lt;/span&gt; &lt;span&gt;softmax_loss 求梯度优点: 求梯度运算简单，方便 
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt; &lt;span&gt;softmax: softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt; &lt;span&gt;可以看成概率来理解，从而来进行多分类。
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; &lt;span&gt;Si = exp(i)/[exp(j)求和]
&lt;/span&gt;&lt;span&gt;134&lt;/span&gt; &lt;span&gt;softmax_loss：损失函数，求梯度dx必须用到损失函数，通过梯度下降更新超参
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; &lt;span&gt;Loss = -[Ypred*ln(Sj真实类别位置的概率值)]求和
&lt;/span&gt;&lt;span&gt;136&lt;/span&gt; &lt;span&gt;梯度dx : 对损失函数求一阶偏导
&lt;/span&gt;&lt;span&gt;137&lt;/span&gt; &lt;span&gt;如果 j = i =&amp;gt;dx = Sj - 1
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt; &lt;span&gt;如果 j != i =&amp;gt; dx = Sj
&lt;/span&gt;&lt;span&gt;139&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;140&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; softmax_loss(x, y):    
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;142&lt;/span&gt; &lt;span&gt;    Computes the loss and gradient for softmax classification.    Inputs:    
&lt;/span&gt;&lt;span&gt;143&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class         
&lt;/span&gt;&lt;span&gt;144&lt;/span&gt; &lt;span&gt;    for the ith input.    
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt; &lt;span&gt;    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and         
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt; &lt;span&gt;         0 &amp;lt;= y[i] &amp;lt; C   
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss    
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; &lt;span&gt;    - dx: Gradient of the loss with respect to x   
&lt;/span&gt;&lt;span&gt;150&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;151&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;152&lt;/span&gt; &lt;span&gt;     x - np.max(x, axis=1, keepdims=True) 对数据进行预处理，
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt; &lt;span&gt;     防止np.exp(x - np.max(x, axis=1, keepdims=True))得到结果太分散；
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt; &lt;span&gt;     np.max(x, axis=1, keepdims=True)保证所得结果维度不变；
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;156&lt;/span&gt;     probs = np.exp(x - np.max(x, axis=1, keepdims=&lt;span&gt;True))
&lt;/span&gt;&lt;span&gt;157&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算softmax，准确的说应该是soft，因为还没有选取概率最大值的操作&lt;/span&gt;
&lt;span&gt;158&lt;/span&gt;     probs /= np.sum(probs, axis=1, keepdims=&lt;span&gt;True)
&lt;/span&gt;&lt;span&gt;159&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 样本图片个数&lt;/span&gt;
&lt;span&gt;160&lt;/span&gt;     N =&lt;span&gt; x.shape[0]
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算图片损失&lt;/span&gt;
&lt;span&gt;162&lt;/span&gt;     loss = -np.sum(np.log(probs[np.arange(N), y])) /&lt;span&gt; N
&lt;/span&gt;&lt;span&gt;163&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 复制概率&lt;/span&gt;
&lt;span&gt;164&lt;/span&gt;     dx =&lt;span&gt; probs.copy()
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 针对 i = j 求梯度&lt;/span&gt;
&lt;span&gt;166&lt;/span&gt;     dx[np.arange(N), y] -= 1
&lt;span&gt;167&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算每张样本图片梯度&lt;/span&gt;
&lt;span&gt;168&lt;/span&gt;     dx /=&lt;span&gt; N    
&lt;/span&gt;&lt;span&gt;169&lt;/span&gt; 
&lt;span&gt;170&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; loss, dx
&lt;/span&gt;&lt;span&gt;171&lt;/span&gt; 
&lt;span&gt;172&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; ReLU(x):    
&lt;/span&gt;&lt;span&gt;173&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;ReLU non-linearity.&lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;174&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; np.maximum(0, x)
&lt;/span&gt;&lt;span&gt;175&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;176&lt;/span&gt; &lt;span&gt;功能：获取图片特征
&lt;/span&gt;&lt;span&gt;177&lt;/span&gt; &lt;span&gt;前向卷积：每次用一个3维的卷积核与图片RGB各个通道分别卷积（卷积核1与R进行点积，卷积核2与G点积，卷积核3与B点积）,
&lt;/span&gt;&lt;span&gt;178&lt;/span&gt; &lt;span&gt;然后将3个结果求和（也就是 w*x ）,再加上 b，就是新结果某一位置输出，这是卷积核在图片某一固定小范围内（卷积核大小）的卷积，
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;要想获得整个图片的卷积结果，需要在图片上滑动卷积核（先右后下），直至遍历整个图片。
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; &lt;span&gt;x: 2*3*32*32  每次选取2张图片，图片大小32*32，彩色(3通道)
&lt;/span&gt;&lt;span&gt;181&lt;/span&gt; &lt;span&gt;w: 32*3*7*7   卷积核每个大小是7*7；对应输入x的3通道，所以是3维，有32个卷积核
&lt;/span&gt;&lt;span&gt;182&lt;/span&gt; &lt;span&gt;pad = 3(图片边缘行列补0)，stride = 1(卷积核移动步长)
&lt;/span&gt;&lt;span&gt;183&lt;/span&gt; &lt;span&gt;输出宽*高结果：(32-7+2*3)/1 + 1 = 32
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt; &lt;span&gt;输出大小：2*32*32*32
&lt;/span&gt;&lt;span&gt;185&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;186&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_forward_naive(x, w, b, conv_param):
&lt;/span&gt;&lt;span&gt;187&lt;/span&gt;     stride, pad = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;188&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;189&lt;/span&gt;     F, C, HH, WW =&lt;span&gt; w.shape
&lt;/span&gt;&lt;span&gt;190&lt;/span&gt;     x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;191&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;&lt;span&gt;// : 求整型&lt;/span&gt;&lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;192&lt;/span&gt;     H_new = 1 + (H + 2 * pad - HH) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt;     W_new = 1 + (W + 2 * pad - WW) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;194&lt;/span&gt;     s =&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt;     out =&lt;span&gt; np.zeros((N, F, H_new, W_new))
&lt;/span&gt;&lt;span&gt;196&lt;/span&gt; 
&lt;span&gt;197&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(N):       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; ith image    &lt;/span&gt;
&lt;span&gt;198&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; f &lt;span&gt;in&lt;/span&gt; range(F):   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; fth filter        &lt;/span&gt;
&lt;span&gt;199&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):   
&lt;/span&gt;&lt;span&gt;201&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s].shape&lt;/span&gt;
&lt;span&gt;202&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print w[f].shape  &lt;/span&gt;
&lt;span&gt;203&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print b.shape  &lt;/span&gt;
&lt;span&gt;204&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print np.sum((x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] * w[f]))         &lt;/span&gt;
&lt;span&gt;205&lt;/span&gt;                     out[i, f, j, k] = np.sum(x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] * w[f]) +&lt;span&gt; b[f]
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt; 
&lt;span&gt;207&lt;/span&gt;     cache =&lt;span&gt; (x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;208&lt;/span&gt; 
&lt;span&gt;209&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;210&lt;/span&gt; 
&lt;span&gt;211&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;212&lt;/span&gt; &lt;span&gt;反向传播之卷积：卷积核3*7*7
&lt;/span&gt;&lt;span&gt;213&lt;/span&gt; &lt;span&gt;输入dout:2*32*32*32
&lt;/span&gt;&lt;span&gt;214&lt;/span&gt; &lt;span&gt;输出dx:2*3*32*32
&lt;/span&gt;&lt;span&gt;215&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;216&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_backward_naive(dout, cache):
&lt;/span&gt;&lt;span&gt;217&lt;/span&gt; 
&lt;span&gt;218&lt;/span&gt;     x, w, b, conv_param =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;219&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 边界补0&lt;/span&gt;
&lt;span&gt;220&lt;/span&gt;     pad = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;221&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 步长&lt;/span&gt;
&lt;span&gt;222&lt;/span&gt;     stride = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;223&lt;/span&gt;     F, C, HH, WW =&lt;span&gt; w.shape
&lt;/span&gt;&lt;span&gt;224&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;225&lt;/span&gt;     H_new = 1 + (H + 2 * pad - HH) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;226&lt;/span&gt;     W_new = 1 + (W + 2 * pad - WW) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;227&lt;/span&gt; 
&lt;span&gt;228&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)
&lt;/span&gt;&lt;span&gt;229&lt;/span&gt;     dw =&lt;span&gt; np.zeros_like(w)
&lt;/span&gt;&lt;span&gt;230&lt;/span&gt;     db =&lt;span&gt; np.zeros_like(b)
&lt;/span&gt;&lt;span&gt;231&lt;/span&gt; 
&lt;span&gt;232&lt;/span&gt;     s =&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;233&lt;/span&gt;     x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;234&lt;/span&gt;     dx_padded = np.pad(dx, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;235&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 图片个数&lt;/span&gt;
&lt;span&gt;236&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(N):       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; ith image&lt;/span&gt;
&lt;span&gt;237&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 卷积核滤波个数&lt;/span&gt;
&lt;span&gt;238&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; f &lt;span&gt;in&lt;/span&gt; range(F):   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; fth filter        &lt;/span&gt;
&lt;span&gt;239&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;240&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):
&lt;/span&gt;&lt;span&gt;241&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7&lt;/span&gt;
&lt;span&gt;242&lt;/span&gt;                     window = x_padded[i, :, j*s:HH+j*s, k*s:WW+k*&lt;span&gt;s]
&lt;/span&gt;&lt;span&gt;243&lt;/span&gt;                     db[f] +=&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;244&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7&lt;/span&gt;
&lt;span&gt;245&lt;/span&gt;                     dw[f] += window *&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;246&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7 =&amp;gt; 2*3*38*38&lt;/span&gt;
&lt;span&gt;247&lt;/span&gt;                     dx_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] += w[f] *&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;248&lt;/span&gt; 
&lt;span&gt;249&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Unpad&lt;/span&gt;
&lt;span&gt;250&lt;/span&gt;     dx = dx_padded[:, :, pad:pad+H, pad:pad+&lt;span&gt;W]
&lt;/span&gt;&lt;span&gt;251&lt;/span&gt; 
&lt;span&gt;252&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;253&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;254&lt;/span&gt; &lt;span&gt;功能：减少特征尺寸大小
&lt;/span&gt;&lt;span&gt;255&lt;/span&gt; &lt;span&gt;前向最大池化：在特征矩阵中选取指定大小窗口，获取窗口内元素最大值作为输出窗口映射值，
&lt;/span&gt;&lt;span&gt;256&lt;/span&gt; &lt;span&gt;先有后下遍历，直至获取整个特征矩阵对应的新映射特征矩阵。
&lt;/span&gt;&lt;span&gt;257&lt;/span&gt; &lt;span&gt;输入x：2*32*32*32
&lt;/span&gt;&lt;span&gt;258&lt;/span&gt; &lt;span&gt;池化参数：窗口：2*2，步长：2
&lt;/span&gt;&lt;span&gt;259&lt;/span&gt; &lt;span&gt;输出窗口宽，高：(32-2)/2 + 1 = 16
&lt;/span&gt;&lt;span&gt;260&lt;/span&gt; &lt;span&gt;输出大小：2*32*16*16
&lt;/span&gt;&lt;span&gt;261&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;262&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; max_pool_forward_naive(x, pool_param):
&lt;/span&gt;&lt;span&gt;263&lt;/span&gt;     HH, WW = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;264&lt;/span&gt;     s = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;265&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;266&lt;/span&gt;     H_new = 1 + (H - HH) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;267&lt;/span&gt;     W_new = 1 + (W - WW) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;268&lt;/span&gt;     out =&lt;span&gt; np.zeros((N, C, H_new, W_new))
&lt;/span&gt;&lt;span&gt;269&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(N):    
&lt;/span&gt;&lt;span&gt;270&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(C):        
&lt;/span&gt;&lt;span&gt;271&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;272&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):                
&lt;/span&gt;&lt;span&gt;273&lt;/span&gt;                     window = x[i, j, k*s:HH+k*s, l*s:WW+l*&lt;span&gt;s] 
&lt;/span&gt;&lt;span&gt;274&lt;/span&gt;                     out[i, j, k, l] =&lt;span&gt; np.max(window)
&lt;/span&gt;&lt;span&gt;275&lt;/span&gt; 
&lt;span&gt;276&lt;/span&gt;     cache =&lt;span&gt; (x, pool_param)
&lt;/span&gt;&lt;span&gt;277&lt;/span&gt; 
&lt;span&gt;278&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;279&lt;/span&gt; 
&lt;span&gt;280&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;281&lt;/span&gt; &lt;span&gt;反向传播之池化：增大特征尺寸大小
&lt;/span&gt;&lt;span&gt;282&lt;/span&gt; &lt;span&gt;在缓存中取出前向池化时输入特征，选取某一范围矩阵窗口，
&lt;/span&gt;&lt;span&gt;283&lt;/span&gt; &lt;span&gt;找出最大值所在的位置，根据这个位置将dout值映射到新的矩阵对应位置上，
&lt;/span&gt;&lt;span&gt;284&lt;/span&gt; &lt;span&gt;而新矩阵其他位置都初始化为0.
&lt;/span&gt;&lt;span&gt;285&lt;/span&gt; &lt;span&gt;输入dout:2*32*16*16
&lt;/span&gt;&lt;span&gt;286&lt;/span&gt; &lt;span&gt;输出dx:2*32*32*32
&lt;/span&gt;&lt;span&gt;287&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;288&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; max_pool_backward_naive(dout, cache):
&lt;/span&gt;&lt;span&gt;289&lt;/span&gt;     x, pool_param =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;290&lt;/span&gt;     HH, WW = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;291&lt;/span&gt;     s = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;292&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;293&lt;/span&gt;     H_new = 1 + (H - HH) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;294&lt;/span&gt;     W_new = 1 + (W - WW) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;295&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)
&lt;/span&gt;&lt;span&gt;296&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(N):    
&lt;/span&gt;&lt;span&gt;297&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(C):        
&lt;/span&gt;&lt;span&gt;298&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;299&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):
&lt;/span&gt;&lt;span&gt;300&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 取前向传播时输入的某一池化窗口&lt;/span&gt;
&lt;span&gt;301&lt;/span&gt;                     window = x[i, j, k*s:HH+k*s, l*s:WW+l*&lt;span&gt;s]
&lt;/span&gt;&lt;span&gt;302&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算窗口最大值&lt;/span&gt;
&lt;span&gt;303&lt;/span&gt;                     m =&lt;span&gt; np.max(window)
&lt;/span&gt;&lt;span&gt;304&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 根据最大值所在位置以及dout对应值=&amp;gt;新矩阵窗口数值&lt;/span&gt;
&lt;span&gt;305&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; [false,false&lt;/span&gt;
&lt;span&gt;306&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;  true, false]  * 1 =&amp;gt; [0,0&lt;/span&gt;
&lt;span&gt;307&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;                        1,0]&lt;/span&gt;
&lt;span&gt;308&lt;/span&gt;                     dx[i, j, k*s:HH+k*s, l*s:WW+l*s] = (window == m) *&lt;span&gt; dout[i, j, k, l]
&lt;/span&gt;&lt;span&gt;309&lt;/span&gt; 
&lt;span&gt;310&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt; dx
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;optim.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('47862aed-8f71-4371-8d79-c39ad2879533')&quot; readability=&quot;47&quot;&gt;&lt;img id=&quot;code_img_closed_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('47862aed-8f71-4371-8d79-c39ad2879533',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;89&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; 
&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; sgd(w, dw, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;  5&lt;/span&gt; &lt;span&gt;    Performs vanilla stochastic gradient descent.    
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;  9&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 10&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     w -= config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *&lt;span&gt; dw    
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt; 
&lt;span&gt; 13&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; w, config
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;SGD:随机梯度下降：由梯度计算新的权重矩阵w
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; &lt;span&gt;sgd_momentum 是sgd的改进版，解决sgd更新不稳定，陷入局部最优的问题。
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;增加一个动量因子momentum，可以在一定程度上增加稳定性，
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;从而学习地更快，并且还有一定摆脱局部最优的能力。
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; 
&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 21&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; sgd_momentum(w, dw, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;    Performs stochastic gradient descent with momentum.    
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;    - momentum: Scalar between 0 and 1 giving the momentum value.                
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt; &lt;span&gt;    Setting momentum = 0 reduces to sgd.    
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt; &lt;span&gt;    - velocity（速度）: A numpy array of the same shape as w and dw used to store a moving
&lt;/span&gt;&lt;span&gt; 29&lt;/span&gt; &lt;span&gt;    average of the gradients.   
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;   
&lt;span&gt; 31&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)   
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.9&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 34&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; config 如果存在属性velocity，则获取config['velocity']，否则获取np.zeros_like(w)&lt;/span&gt;
&lt;span&gt; 35&lt;/span&gt;     v = config.get(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;velocity&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(w))    
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt;     next_w =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt;     v = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] * v - config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *&lt;span&gt; dw    
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt;     next_w = w +&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;velocity&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; 
&lt;span&gt; 41&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; next_w, config
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; 
&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; rmsprop(x, dx, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;    Uses the RMSProp update rule, which uses a moving average of squared gradient    
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt; &lt;span&gt;    values to set adaptive per-parameter learning rates.    
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 48&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared                  
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; &lt;span&gt;    gradient cache.    
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;    - epsilon: Small scalar used for smoothing to avoid dividing by zero.    
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    - cache: Moving average of second moments of gradients.   
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 54&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)  
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;decay_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.99&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-8&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 58&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt;     next_x =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt;     cache = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 61&lt;/span&gt;     decay_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;decay_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 62&lt;/span&gt;     learning_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     epsilon = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     cache = decay_rate * cache + (1 - decay_rate) * (dx**2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt;     x += - learning_rate * dx / (np.sqrt(cache) +&lt;span&gt; epsilon)  
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; cache    
&lt;/span&gt;&lt;span&gt; 67&lt;/span&gt;     next_x =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt; 68&lt;/span&gt; 
&lt;span&gt; 69&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; next_x, config
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt; 
&lt;span&gt; 71&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; adam(x, dx, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 73&lt;/span&gt; &lt;span&gt;    Uses the Adam update rule, which incorporates moving averages of both the  
&lt;/span&gt;&lt;span&gt; 74&lt;/span&gt; &lt;span&gt;    gradient and its square and a bias correction term.    
&lt;/span&gt;&lt;span&gt; 75&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    - beta1: Decay rate for moving average of first moment of gradient.    
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    - beta2: Decay rate for moving average of second moment of gradient.   
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    - epsilon: Small scalar used for smoothing to avoid dividing by zero.    
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    - m: Moving average of gradient.    
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - v: Moving average of squared gradient.    
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;    - t: Iteration number.   
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 84&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-3&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.9&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 87&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.999&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-8&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, 0)   
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt;     next_x =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt;     m = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt;     v = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt;     beta1 = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt;     beta2 = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt;     learning_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt;     epsilon = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]   
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt;     t = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt;     t += 1    
&lt;span&gt;101&lt;/span&gt;     m = beta1 * m + (1 - beta1) *&lt;span&gt; dx    
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt;     v = beta2 * v + (1 - beta2) * (dx**2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt;     m_bias = m / (1 - beta1**&lt;span&gt;t)    
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt;     v_bias = v / (1 - beta2**&lt;span&gt;t)    
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt;     x += - learning_rate * m_bias / (np.sqrt(v_bias) +&lt;span&gt; epsilon)    
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     next_x =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; m    
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; t    
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; 
&lt;span&gt;111&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt; next_x, config
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;solver.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('c971387e-35cb-4089-826b-f2676488aa6c')&quot; readability=&quot;81&quot;&gt;&lt;img id=&quot;code_img_closed_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('c971387e-35cb-4089-826b-f2676488aa6c',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;157&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;  3&lt;/span&gt;   &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; optim
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt;   &lt;span&gt;import&lt;/span&gt;&lt;span&gt; optim
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; 
&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; Solver(object):
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;  9&lt;/span&gt; &lt;span&gt;  A Solver encapsulates all the logic necessary for training classification
&lt;/span&gt;&lt;span&gt; 10&lt;/span&gt; &lt;span&gt;  models. The Solver performs stochastic gradient descent using different
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt; &lt;span&gt;  update rules defined in optim.py.
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt; 
&lt;span&gt; 13&lt;/span&gt; &lt;span&gt;  The solver accepts both training and validataion data and labels so it can
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;  periodically check classification accuracy on both training and validation
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;  data to watch out for overfitting.
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; 
&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;  To train a model, you will first construct a Solver instance, passing the
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;  model, dataset, and various optoins (learning rate, batch size, etc) to the
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; &lt;span&gt;  constructor. You will then call the train() method to run the optimization
&lt;/span&gt;&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;  procedure and train the model.
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt;   
&lt;span&gt; 22&lt;/span&gt; &lt;span&gt;  After the train() method returns, model.params will contain the parameters
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;  that performed best on the validation set over the course of training.
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt; &lt;span&gt;  In addition, the instance variable solver.loss_history will contain a list
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;  of all losses encountered during training and the instance variables
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;  solver.train_acc_history and solver.val_acc_history will be lists containing
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt; &lt;span&gt;  the accuracies of the model on the training and validation set at each epoch.
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;   
&lt;span&gt; 29&lt;/span&gt; &lt;span&gt;  Example usage might look something like this:
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;   
&lt;span&gt; 31&lt;/span&gt; &lt;span&gt;  data = {
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt; &lt;span&gt;    'X_train': # training data
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt; &lt;span&gt;    'y_train': # training labels
&lt;/span&gt;&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;    'X_val': # validation data
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; &lt;span&gt;    'X_train': # validation labels
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt; &lt;span&gt;  }
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;  model = MyAwesomeModel(hidden_size=100, reg=10)
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;  solver = Solver(model, data,
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;                  update_rule='sgd',
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; &lt;span&gt;                  optim_config={
&lt;/span&gt;&lt;span&gt; 41&lt;/span&gt; &lt;span&gt;                    'learning_rate': 1e-3,
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; &lt;span&gt;                  },
&lt;/span&gt;&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;                  lr_decay=0.95,
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt; &lt;span&gt;                  num_epochs=10, batch_size=100,
&lt;/span&gt;&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;                  print_every=100)
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt; &lt;span&gt;  solver.train()
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt; 
&lt;span&gt; 48&lt;/span&gt; 
&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;  A Solver works on a model object that must conform to the following API:
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; 
&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;  - model.params must be a dictionary mapping string parameter names to numpy
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    arrays containing parameter values.
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt; 
&lt;span&gt; 54&lt;/span&gt; &lt;span&gt;  - model.loss(X, y) must be a function that computes training-time loss and
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt; &lt;span&gt;    gradients, and test-time classification scores, with the following inputs
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt; &lt;span&gt;    and outputs:
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; 
&lt;span&gt; 58&lt;/span&gt; &lt;span&gt;    Inputs:
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt; &lt;span&gt;    - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt; &lt;span&gt;    - y: Array of labels, of shape (N,) giving labels for X where y[i] is the
&lt;/span&gt;&lt;span&gt; 61&lt;/span&gt; &lt;span&gt;      label for X[i].
&lt;/span&gt;&lt;span&gt; 62&lt;/span&gt; 
&lt;span&gt; 63&lt;/span&gt; &lt;span&gt;    Returns:
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt; &lt;span&gt;    If y is None, run a test-time forward pass and return:
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt; &lt;span&gt;    - scores: Array of shape (N, C) giving classification scores for X where
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt; &lt;span&gt;      scores[i, c] gives the score of class c for X[i].
&lt;/span&gt;&lt;span&gt; 67&lt;/span&gt; 
&lt;span&gt; 68&lt;/span&gt; &lt;span&gt;    If y is not None, run a training time forward and backward pass and return
&lt;/span&gt;&lt;span&gt; 69&lt;/span&gt; &lt;span&gt;    a tuple of:
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss
&lt;/span&gt;&lt;span&gt; 71&lt;/span&gt; &lt;span&gt;    - grads: Dictionary with the same keys as self.params mapping parameter
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt; &lt;span&gt;      names to gradients of the loss with respect to those parameters.
&lt;/span&gt;&lt;span&gt; 73&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 74&lt;/span&gt; 
&lt;span&gt; 75&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self, model, data, **&lt;span&gt;kwargs):
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    Construct a new Solver instance.
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt;     
&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    Required arguments:
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    - model: A model object conforming to the API described above
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - data: A dictionary of training and validation data with the following:
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;      'X_train': Array of shape (N_train, d_1, ..., d_k) giving training images
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt; &lt;span&gt;      'X_val': Array of shape (N_val, d_1, ..., d_k) giving validation images
&lt;/span&gt;&lt;span&gt; 84&lt;/span&gt; &lt;span&gt;      'y_train': Array of shape (N_train,) giving labels for training images
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt; &lt;span&gt;      'y_val': Array of shape (N_val,) giving labels for validation images
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;       
&lt;span&gt; 87&lt;/span&gt; &lt;span&gt;    Optional arguments:
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt; &lt;span&gt;    - update_rule: A string giving the name of an update rule in optim.py.
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; &lt;span&gt;      Default is 'sgd'.
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt; &lt;span&gt;    - optim_config: A dictionary containing hyperparameters that will be
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt; &lt;span&gt;      passed to the chosen update rule. Each update rule requires different
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;      hyperparameters (see optim.py) but all update rules require a
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;      'learning_rate' parameter so that should always be present.
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;    - lr_decay: A scalar for learning rate decay; after each epoch the learning
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;      rate is multiplied by this value.
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;    - batch_size: Size of minibatches used to compute loss and gradient during
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;      training.
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt; &lt;span&gt;    - num_epochs: The number of epochs to run for during training.
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt; &lt;span&gt;    - print_every: Integer; training losses will be printed every print_every
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt; &lt;span&gt;      iterations.
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt; &lt;span&gt;    - verbose: Boolean; if set to false then no output will be printed during
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt; &lt;span&gt;      training.
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;104&lt;/span&gt;     self.model =&lt;span&gt; model
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt;     self.X_train = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     self.y_train = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt;     self.X_val = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt;     self.y_val = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt;     
&lt;span&gt;110&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Unpack keyword arguments&lt;/span&gt;
&lt;span&gt;111&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pop(key, default):删除kwargs对象中key，如果存在该key，返回该key对应的value，否则，返回default值。&lt;/span&gt;
&lt;span&gt;112&lt;/span&gt;     self.update_rule = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;update_rule&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;sgd&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt;     self.optim_config = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;optim_config&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, {})
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt;     self.lr_decay = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;lr_decay&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1.0&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt;     self.batch_size = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;batch_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 2&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;116&lt;/span&gt;     self.num_epochs = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;num_epochs&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 10&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;117&lt;/span&gt; 
&lt;span&gt;118&lt;/span&gt;     self.print_every = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;print_every&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 10&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt;     self.verbose = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;verbose&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, True)
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt; 
&lt;span&gt;121&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Throw an error if there are extra keyword arguments&lt;/span&gt;
&lt;span&gt;122&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 删除kwargs中参数后，校验是否还有多余参数&lt;/span&gt;
&lt;span&gt;123&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; len(kwargs) &amp;gt;&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt;       extra = &lt;span&gt;'&lt;/span&gt;&lt;span&gt;, &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;.join(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;&quot;%s&quot;&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; % k &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; kwargs.keys())
&lt;/span&gt;&lt;span&gt;125&lt;/span&gt;       &lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Unrecognized arguments %s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; extra)
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt; 
&lt;span&gt;127&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make sure the update rule exists, then replace the string&lt;/span&gt;
&lt;span&gt;128&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; name with the actual function&lt;/span&gt;
&lt;span&gt;129&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 检查optim对象中是否有属性或方法名为self.update_rule&lt;/span&gt;
&lt;span&gt;130&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; hasattr(optim, self.update_rule):
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt;       &lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Invalid update_rule &quot;%s&quot;&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; self.update_rule)
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt;     self.update_rule =&lt;span&gt; getattr(optim, self.update_rule)
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; 
&lt;span&gt;134&lt;/span&gt; &lt;span&gt;    self._reset()
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; 
&lt;span&gt;136&lt;/span&gt; 
&lt;span&gt;137&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; _reset(self):
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;139&lt;/span&gt; &lt;span&gt;    Set up some book-keeping variables for optimization. Don't call this
&lt;/span&gt;&lt;span&gt;140&lt;/span&gt; &lt;span&gt;    manually.
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;142&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Set up some variables for book-keeping&lt;/span&gt;
&lt;span&gt;143&lt;/span&gt;     self.epoch =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;144&lt;/span&gt;     self.best_val_acc =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt;     self.best_params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt;     self.loss_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt;     self.train_acc_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt;     self.val_acc_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; 
&lt;span&gt;150&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make a deep copy of the optim_config for each parameter&lt;/span&gt;
&lt;span&gt;151&lt;/span&gt;     self.optim_configs =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;152&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; p &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params:
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt;       d = {k: v &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.optim_config.items()}
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt;       self.optim_configs[p] =&lt;span&gt; d
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt; 
&lt;span&gt;156&lt;/span&gt; 
&lt;span&gt;157&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; _step(self):
&lt;/span&gt;&lt;span&gt;158&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;159&lt;/span&gt; &lt;span&gt;    Make a single gradient update. This is called by train() and should not
&lt;/span&gt;&lt;span&gt;160&lt;/span&gt; &lt;span&gt;    be called manually.
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;162&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make a minibatch of training data&lt;/span&gt;
&lt;span&gt;163&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 500 张图片&lt;/span&gt;
&lt;span&gt;164&lt;/span&gt;     num_train =&lt;span&gt; self.X_train.shape[0]
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 随机选出batch_size：2 张&lt;/span&gt;
&lt;span&gt;166&lt;/span&gt;     batch_mask =&lt;span&gt; np.random.choice(num_train, self.batch_size)
&lt;/span&gt;&lt;span&gt;167&lt;/span&gt; 
&lt;span&gt;168&lt;/span&gt;    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; batch_mask = [t%(num_train//2), num_train//2 + t%(num_train//2)]&lt;/span&gt;
&lt;span&gt;169&lt;/span&gt; 
&lt;span&gt;170&lt;/span&gt; 
&lt;span&gt;171&lt;/span&gt; 
&lt;span&gt;172&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练样本矩阵[2,3,32,32]&lt;/span&gt;
&lt;span&gt;173&lt;/span&gt;     X_batch =&lt;span&gt; self.X_train[batch_mask]
&lt;/span&gt;&lt;span&gt;174&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 标签矩阵[2,] 图片类型&lt;/span&gt;
&lt;span&gt;175&lt;/span&gt;     y_batch =&lt;span&gt; self.y_train[batch_mask]
&lt;/span&gt;&lt;span&gt;176&lt;/span&gt; 
&lt;span&gt;177&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Compute loss and gradient&lt;/span&gt;
&lt;span&gt;178&lt;/span&gt;     loss, grads =&lt;span&gt; self.model.loss(X_batch, y_batch)
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;    self.loss_history.append(loss)
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; 
&lt;span&gt;181&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 更新模型超参(w1,b1),(w2,b2),(w3,b3)，以及保存更新超参时对应参数因子&lt;/span&gt;
&lt;span&gt;182&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Perform a parameter update&lt;/span&gt;
&lt;span&gt;183&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; p, w &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params.items():
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt;       dw =&lt;span&gt; grads[p]
&lt;/span&gt;&lt;span&gt;185&lt;/span&gt;       config =&lt;span&gt; self.optim_configs[p]
&lt;/span&gt;&lt;span&gt;186&lt;/span&gt;       next_w, next_config =&lt;span&gt; self.update_rule(w, dw, config)
&lt;/span&gt;&lt;span&gt;187&lt;/span&gt;       self.model.params[p] =&lt;span&gt; next_w
&lt;/span&gt;&lt;span&gt;188&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 保存参数因子，learning_rate(学习率)，velocity(速度)&lt;/span&gt;
&lt;span&gt;189&lt;/span&gt;       self.optim_configs[p] =&lt;span&gt; next_config
&lt;/span&gt;&lt;span&gt;190&lt;/span&gt; 
&lt;span&gt;191&lt;/span&gt; 
&lt;span&gt;192&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt; check_accuracy(self, X, y, num_samples=None, batch_size=2&lt;span&gt;):
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;194&lt;/span&gt; &lt;span&gt;    Check accuracy of the model on the provided data.
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt;     
&lt;span&gt;196&lt;/span&gt; &lt;span&gt;    Inputs:
&lt;/span&gt;&lt;span&gt;197&lt;/span&gt; &lt;span&gt;    - X: Array of data, of shape (N, d_1, ..., d_k)
&lt;/span&gt;&lt;span&gt;198&lt;/span&gt; &lt;span&gt;    - y: Array of labels, of shape (N,)
&lt;/span&gt;&lt;span&gt;199&lt;/span&gt; &lt;span&gt;    - num_samples: If not None, subsample the data and only test the model
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt; &lt;span&gt;      on num_samples datapoints.
&lt;/span&gt;&lt;span&gt;201&lt;/span&gt; &lt;span&gt;    - batch_size: Split X and y into batches of this size to avoid using too
&lt;/span&gt;&lt;span&gt;202&lt;/span&gt; &lt;span&gt;      much memory.
&lt;/span&gt;&lt;span&gt;203&lt;/span&gt;       
&lt;span&gt;204&lt;/span&gt; &lt;span&gt;    Returns:
&lt;/span&gt;&lt;span&gt;205&lt;/span&gt; &lt;span&gt;    - acc: Scalar giving the fraction of instances that were correctly
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt; &lt;span&gt;      classified by the model.
&lt;/span&gt;&lt;span&gt;207&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;208&lt;/span&gt;     
&lt;span&gt;209&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Maybe subsample the data&lt;/span&gt;
&lt;span&gt;210&lt;/span&gt;     N =&lt;span&gt; X.shape[0]
&lt;/span&gt;&lt;span&gt;211&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; num_samples &lt;span&gt;is&lt;/span&gt; &lt;span&gt;not&lt;/span&gt; None &lt;span&gt;and&lt;/span&gt; N &amp;gt;&lt;span&gt; num_samples:
&lt;/span&gt;&lt;span&gt;212&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 随机选取num_samples张图片，返回选取图片索引&lt;/span&gt;
&lt;span&gt;213&lt;/span&gt;       mask =&lt;span&gt; np.random.choice(N, num_samples)
&lt;/span&gt;&lt;span&gt;214&lt;/span&gt;       N =&lt;span&gt; num_samples
&lt;/span&gt;&lt;span&gt;215&lt;/span&gt;       X =&lt;span&gt; X[mask]
&lt;/span&gt;&lt;span&gt;216&lt;/span&gt;       y =&lt;span&gt; y[mask]
&lt;/span&gt;&lt;span&gt;217&lt;/span&gt; 
&lt;span&gt;218&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Compute predictions in batches&lt;/span&gt;
&lt;span&gt;219&lt;/span&gt;     num_batches = N //&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;220&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; N % batch_size !=&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;221&lt;/span&gt;       num_batches += 1
&lt;span&gt;222&lt;/span&gt;     y_pred =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;223&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(num_batches):
&lt;/span&gt;&lt;span&gt;224&lt;/span&gt;       start = i *&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;225&lt;/span&gt;       end = (i + 1) *&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;226&lt;/span&gt;       scores =&lt;span&gt; self.model.loss(X[start:end])
&lt;/span&gt;&lt;span&gt;227&lt;/span&gt;       y_pred.append(np.argmax(scores, axis=1&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;228&lt;/span&gt;     y_pred =&lt;span&gt; np.hstack(y_pred)
&lt;/span&gt;&lt;span&gt;229&lt;/span&gt;     acc = np.mean(y_pred ==&lt;span&gt; y)
&lt;/span&gt;&lt;span&gt;230&lt;/span&gt; 
&lt;span&gt;231&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; acc
&lt;/span&gt;&lt;span&gt;232&lt;/span&gt; 
&lt;span&gt;233&lt;/span&gt;   &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;234&lt;/span&gt; &lt;span&gt;   训练模型：核心方法
&lt;/span&gt;&lt;span&gt;235&lt;/span&gt; &lt;span&gt;   epoch &amp;gt; batch_size &amp;gt; iteration &amp;gt;= 1
&lt;/span&gt;&lt;span&gt;236&lt;/span&gt; &lt;span&gt;   训练总的次数 = num_epochs * iterations_per_epoch
&lt;/span&gt;&lt;span&gt;237&lt;/span&gt;   &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;238&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; train(self):
&lt;/span&gt;&lt;span&gt;239&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;240&lt;/span&gt; &lt;span&gt;    Run optimization to train the model.
&lt;/span&gt;&lt;span&gt;241&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;242&lt;/span&gt;     num_train =&lt;span&gt; self.X_train.shape[0]
&lt;/span&gt;&lt;span&gt;243&lt;/span&gt;     iterations_per_epoch = max(num_train // self.batch_size, 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;244&lt;/span&gt;     num_iterations = self.num_epochs *&lt;span&gt; iterations_per_epoch
&lt;/span&gt;&lt;span&gt;245&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 迭代总的次数&lt;/span&gt;
&lt;span&gt;246&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; t &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(num_iterations):
&lt;/span&gt;&lt;span&gt;247&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 某次iteration训练&lt;/span&gt;
&lt;span&gt;248&lt;/span&gt; &lt;span&gt;      self._step()
&lt;/span&gt;&lt;span&gt;249&lt;/span&gt; 
&lt;span&gt;250&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Maybe print training loss&lt;/span&gt;
&lt;span&gt;251&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; verbose：是否显示详细信息&lt;/span&gt;
&lt;span&gt;252&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt; self.verbose &lt;span&gt;and&lt;/span&gt; t % self.print_every ==&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;253&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;(Iteration %d / %d) loss: %f&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (
&lt;/span&gt;&lt;span&gt;254&lt;/span&gt;                t + 1, num_iterations, self.loss_history[-1&lt;span&gt;]))
&lt;/span&gt;&lt;span&gt;255&lt;/span&gt; 
&lt;span&gt;256&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; At the end of every epoch, increment the epoch counter and decay the&lt;/span&gt;
&lt;span&gt;257&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; learning rate.&lt;/span&gt;
&lt;span&gt;258&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 每迭代完一次epoch后，更新学习率learning_rate，加快运算效率。&lt;/span&gt;
&lt;span&gt;259&lt;/span&gt;       epoch_end = (t + 1) % iterations_per_epoch ==&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;260&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt;&lt;span&gt; epoch_end:
&lt;/span&gt;&lt;span&gt;261&lt;/span&gt;         self.epoch += 1
&lt;span&gt;262&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.optim_configs:
&lt;/span&gt;&lt;span&gt;263&lt;/span&gt;           self.optim_configs[k][&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *=&lt;span&gt; self.lr_decay
&lt;/span&gt;&lt;span&gt;264&lt;/span&gt; 
&lt;span&gt;265&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Check train and val accuracy on the first iteration, the last&lt;/span&gt;
&lt;span&gt;266&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; iteration, and at the end of each epoch.&lt;/span&gt;
&lt;span&gt;267&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 在第1次迭代，最后1次迭代，或者运行完一个epoch后，校验训练结果。&lt;/span&gt;
&lt;span&gt;268&lt;/span&gt;       first_it = (t ==&lt;span&gt; 0)
&lt;/span&gt;&lt;span&gt;269&lt;/span&gt;       last_it = (t == num_iterations + 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;270&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt; first_it &lt;span&gt;or&lt;/span&gt; last_it &lt;span&gt;or&lt;/span&gt;&lt;span&gt; epoch_end:
&lt;/span&gt;&lt;span&gt;271&lt;/span&gt;         train_acc =&lt;span&gt; self.check_accuracy(self.X_train, self.y_train,
&lt;/span&gt;&lt;span&gt;272&lt;/span&gt;                                         num_samples=4&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;273&lt;/span&gt;         val_acc = self.check_accuracy(self.X_val, self.y_val,num_samples=4&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;274&lt;/span&gt; &lt;span&gt;        self.train_acc_history.append(train_acc)
&lt;/span&gt;&lt;span&gt;275&lt;/span&gt; &lt;span&gt;        self.val_acc_history.append(val_acc)
&lt;/span&gt;&lt;span&gt;276&lt;/span&gt; 
&lt;span&gt;277&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt;&lt;span&gt; self.verbose:
&lt;/span&gt;&lt;span&gt;278&lt;/span&gt;           &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;(Epoch %d / %d) train acc: %f; val_acc: %f&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (
&lt;/span&gt;&lt;span&gt;279&lt;/span&gt; &lt;span&gt;                 self.epoch, self.num_epochs, train_acc, val_acc))
&lt;/span&gt;&lt;span&gt;280&lt;/span&gt; 
&lt;span&gt;281&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Keep track of the best model&lt;/span&gt;
&lt;span&gt;282&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; val_acc &amp;gt;&lt;span&gt; self.best_val_acc:
&lt;/span&gt;&lt;span&gt;283&lt;/span&gt;           self.best_val_acc =&lt;span&gt; val_acc
&lt;/span&gt;&lt;span&gt;284&lt;/span&gt;           self.best_params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;285&lt;/span&gt;           &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params.items():
&lt;/span&gt;&lt;span&gt;286&lt;/span&gt;             self.best_params[k] =&lt;span&gt; v.copy()
&lt;/span&gt;&lt;span&gt;287&lt;/span&gt; 
&lt;span&gt;288&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; At the end of training swap the best params into the model&lt;/span&gt;
&lt;span&gt;289&lt;/span&gt;     self.model.params = self.best_params
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;h2&gt;&lt;span&gt;5.运行结果以及分析&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这里选取500张图片作为训练样本，epoch = 5，batch = 2，每次随机选取2张图片，迭代 5 * 500/2 = 1250次，&lt;/span&gt;&lt;span&gt;测试样本选取50张。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由运行结果可以看出，损失loss是逐步下降的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222911134-710696995.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222926050-864003615.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222944971-72881526.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;但是测试结果有点不尽人意，只有12%左右，某位大神说可以到&lt;/span&gt;54.7%&lt;span&gt;，下次把训练数据集以及测试数据集加大看看，大家也可以发表下高见啊。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;6. 参考文献&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;视觉一只白的博客《常用损失函数小结》&lt;a href=&quot;https://blog.csdn.net/zhangjunp3/article/details/80467350&quot; target=&quot;_blank&quot;&gt;https://blog.csdn.net/zhangjunp3/article/details/80467350&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;理想万岁的博客《Softmax函数详解与推导》：&lt;a href=&quot;http://www.cnblogs.com/zongfa/p/8971213.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/zongfa/p/8971213.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;下路派出所的博客《深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）》&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;a href=&quot;http://www.cnblogs.com/callyblog/p/8299074.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/callyblog/p/8299074.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;不要让懒惰占据你的大脑，不要让妥协拖垮了你的人生。青春就是一张票，能不能赶上时代的快车，你的步伐就掌握在你的脚下。&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 14:34:00 +0000</pubDate>
<dc:creator>w_x_w1985</dc:creator>
<og:description>卷积神经网络(CNN)详解 本文系作者原创，转载请注明出处:https://www.cnblogs.com/further-further-further/p/10430073.html 目录 1.应</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/further-further-further/p/10430073.html</dc:identifier>
</item>
<item>
<title>pytorch模型部署在MacOS或者IOS - 一度逍遥</title>
<link>http://www.cnblogs.com/riddick/p/10434339.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/riddick/p/10434339.html</guid>
<description>&lt;p&gt;pytorch训练出.pth模型如何在MacOS上或者IOS部署，这是个问题。&lt;/p&gt;
&lt;p&gt;然而我们有了onnx，同样我们也有了coreML。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ONNX：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;onnx是一种针对机器学习设计的开放式文件格式，用来存储训练好的模型，并进行多种框架模型间的转换。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;coreML：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apple在2017年 MacOS 10.13以及IOS11+系统上推出了coreML1.0，官网地址：&lt;a href=&quot;https://developer.apple.com/documentation/coreml&quot; target=&quot;_blank&quot;&gt;https://developer.apple.com/documentation/coreml&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;2018年又推出MacOS 10.14以及IOS12系统上的coreML2.0  &lt;a href=&quot;https://www.appcoda.com/coreml2/&quot; target=&quot;_blank&quot;&gt;https://www.appcoda.com/coreml2/&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;coreML框架可以方便的进行深度学习模型的部署，利用模型进行预测，让深度学习可以在apple的移动设备上发光发热。而开发者需要做的仅仅是将model.mlModel拖进xcode工程，xcode工程会自动生成以模型名称命名的object-c类以及多种进行预测所需的类接口。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;pytorch -- ONNX -- coreML&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;没错，就是这个流程。我们有训练好的.pth模型，通过pytorch.onnx.export() 转化为 .onnx模型，然后利用 onnx_coreml.convert()将 .onnx转换为 .mlModel。将.mlModel拖进xcode工程编写预测代码就可以了。&lt;/p&gt;

&lt;p&gt;1.  pytorch -- ONNX&lt;/p&gt;
&lt;p&gt;请先查看pytorch官网的onnx模块：&lt;a href=&quot;https://pytorch.org/docs/stable/onnx.html&quot; target=&quot;_blank&quot;&gt;https://pytorch.org/docs/stable/onnx.html&lt;/a&gt;  。 主要的代码就这一个API, 各个参数意义请查阅文档。&lt;/p&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;span class=&quot;o&quot;&gt;.&lt;span class=&quot;n&quot;&gt;onnx&lt;span class=&quot;o&quot;&gt;.&lt;span class=&quot;n&quot;&gt;export&lt;span class=&quot;p&quot;&gt;(&lt;span class=&quot;n&quot;&gt;model&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;dummy_input&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;alexnet.onnx&quot;&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;verbose&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;kc&quot;&gt;True&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;input_names&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;n&quot;&gt;input_names&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;output_names&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;n&quot; readability=&quot;1&quot;&gt;output_names&lt;span class=&quot;p&quot; readability=&quot;2&quot;&gt;)；&lt;p&gt;转换部分代码如下：&lt;br/&gt;&lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;45&quot;&gt;
&lt;pre&gt;
 batch_size=1&lt;span&gt;
 onnx_model_path &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;onnx_model.onnx&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
 dummy_input &lt;/span&gt;= V(torch.randn(batch_size, 3, 224, 224), requires_grad=&lt;span&gt;True)
 torch_out&lt;/span&gt;= torch.onnx.export(pytorch_model, dummy_input , onnx_model_path, verbose=True,input_names=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;image&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], output_names=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;outTensor&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], export_params=True, training=False )
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;kc&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt; 这里有一个需要注意的地方就是input_names和output_names的设置，如果不设置的情况，输入层和输出层pytorch会自动分配一个数字编号。比如下图(用netron工具查看，真是一个很好用的工具 &lt;a href=&quot;https://pypi.org/project/netron/&quot; target=&quot;_blank&quot;&gt;https://pypi.org/project/netron/&lt;/a&gt;)。 自动分配的输入名称和输出名称是0 和 199。 这样转换成coreML模型后加载到xcode中会出现&quot;initwith0&quot;这样的编译错误，就是模型初始化的时候不能正确处理这个输入名称0。因此最好是在export的时候将其修改一个名称。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;kc&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225215840757-1408564082.png&quot; alt=&quot;&quot; width=&quot;375&quot; height=&quot;615&quot;/&gt;  &lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225215928046-1587433274.png&quot; alt=&quot;&quot; width=&quot;416&quot; height=&quot;739&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;修改之后的模型是这样的，可以看到模型的输入和输出名称都发生的修改：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225220434449-1000784521.png&quot; alt=&quot;&quot; width=&quot;300&quot;/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225220539951-728271199.png&quot; alt=&quot;&quot; width=&quot;300&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 2. onnx -- mlModel&lt;/p&gt;
&lt;p&gt;　　这一部分需要安装onnx, github地址： &lt;a href=&quot;https://github.com/onnx/onnx&quot; target=&quot;_blank&quot;&gt;https://github.com/onnx/onnx&lt;/a&gt;  以及安装一个转换工具onnx_coreML，github地址：&lt;a href=&quot;https://github.com/onnx/onnx-coreml&quot; target=&quot;_blank&quot;&gt;https://github.com/onnx/onnx-coreml&lt;/a&gt;  。里面用到了一个coremltools : &lt;a href=&quot;https://pypi.org/project/coremltools/&quot; target=&quot;_blank&quot;&gt;https://pypi.org/project/coremltools/&lt;/a&gt;，这个tool目前仅支持python2.7环境下使用。&lt;/p&gt;
&lt;p&gt;　　安装好后, import onnx ， import onnx_coreML 就可以使用。转换代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
onnx_model = onnx.load(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;onnx_model.onnx&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
cml_model&lt;/span&gt;=&lt;span&gt; onnx_coreml.convert(onnx_model)
cml_model.save(&lt;/span&gt;&lt;span&gt;&quot;coreML_model&lt;/span&gt;&lt;span&gt;.mlmodel&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt; 　　当然, onnx_coreml.convert有很多参数，可以用来预处理，设置bgr顺序等，请参看github文档介绍。&lt;/p&gt;
&lt;p&gt;　　现在将coreML_model.mlModel拖进xcode工程里，会自动生成一个coreML_model类，这个类有初始化模型，输入 预测 输出等API，编写预测代码即可。&lt;/p&gt;

&lt;p&gt;3. 在最新的coreML2.0中，支持模型的量化. coreML1.0中处理模型是32位，而在coreML2.0中可以将模型量化为16bit, 8bit, 4bit甚至是2bit。 具体请看apple WWDC视频以及PPT。&lt;/p&gt;
&lt;p&gt;  模型量化仍然是使用coreMLtool2.0工具，具体代码请查阅这篇博客，写的很详细：&lt;a href=&quot;https://www.jianshu.com/p/b6e3cb7338bf&quot; target=&quot;_blank&quot;&gt;https://www.jianshu.com/p/b6e3cb7338bf&lt;/a&gt;。 两句代码即可完成量化转换。&lt;/p&gt;

&lt;p&gt;时间仓促，写的粗糙，随后更新。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:28:00 +0000</pubDate>
<dc:creator>一度逍遥</dc:creator>
<og:description>pytorch训练出.pth模型如何在MacOS上或者IOS部署，这是个问题。 然而我们有了onnx，同样我们也有了coreML。 ONNX： onnx是一种针对机器学习设计的开放式文件格式，用来存储</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/riddick/p/10434339.html</dc:identifier>
</item>
<item>
<title>Yaml 文件中Condition If- else 判断的问题 - BUTTERAPPLE</title>
<link>http://www.cnblogs.com/xiyin/p/10434247.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xiyin/p/10434247.html</guid>
<description>&lt;p&gt;在做项目的CI/ CD 时，难免会用到 Travis.CI 和 AppVeyor 以及 CodeCov 来判断测试的覆盖率，今天突然遇到了一个问题，就是我需要在每次做测试的时候判断是否存在一个环境变量，我对于 script 脚本半只半解还不太懂的状态，我最初的打算是这样写的&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;if [-z $ENV_VALUE &amp;amp;&amp;amp; -z $ENV_VALUE]; then
    #do something
else
    #do another 
fi&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;没想到，在windows上跑着正常的，编写到 .yml 文件的 script 中时，在 GitHub 上就报错了，说&lt;code&gt;-z was unexcepted at this time.&lt;/code&gt; 看的我真的是一脸懵啊，什么鬼。去Google 了一下，也没有找到什么可靠的答案，于是我去翻了翻其他语言的项目中是如何写 &lt;code&gt;.yml&lt;/code&gt; 文件的，刚开始其实我也去看了看，只记得里面有个这个命令 &lt;code&gt;test -z $ENV_VALUE -a -z $ENV_VALUE&lt;/code&gt; 。开始没有太注意，后来发现这个 test 命令我在写脚本时怎么从来没见过呢，去Google 了一下，发现新大陆。&lt;/p&gt;
&lt;p&gt;这个 Test 命令的解释是:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;The test command can be used on the Linux command line to compare one element against another, but it is more commonly used in BASH shell scripts as part of conditional statements which control logic and program flow&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下来就简单介绍一下这个神器的用法，比如下面这个基础命令&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;test 1 -eq 2 &amp;amp;&amp;amp; echo &quot;yes&quot; || echo &quot;no&quot;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面这段命令的意思是 1 等于 2 吗？ 如果等于就输出 yes 否则输出 no 显然答案是 no&lt;br/&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/69a9dfdbgy1g0j0lofbiyj209h022dfq.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;具体解剖开来就是&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;test 表示的你将要进行一个比较&lt;/li&gt;
&lt;li&gt;1 是你第一个要比较的数据&lt;/li&gt;
&lt;li&gt;-eq 表示 equal 就是等于&lt;/li&gt;
&lt;li&gt;2 是你第二个要比较的数据&lt;/li&gt;
&lt;li&gt;&amp;amp;&amp;amp; 这个符号后的语句会在表达式为 true时执行&lt;/li&gt;
&lt;li&gt;|| 这个符号后面的语句会在表达式为 false的时候执行&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较的是数字时，还可以又以下其他符号：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;-eq 还有其他比较的符号&lt;/li&gt;
&lt;li&gt;-ge ： 表示 大于等于&lt;/li&gt;
&lt;li&gt;-gt： great than 大于&lt;/li&gt;
&lt;li&gt;-le： less equal than 小于等于&lt;/li&gt;
&lt;li&gt;-lt： 小于&lt;/li&gt;
&lt;li&gt;-ne： 不等于&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较的是 Text时&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;= ：表示的是 string 1 和 stirng2 匹配，相等&lt;/li&gt;
&lt;li&gt;!= ：和上面相反&lt;/li&gt;
&lt;li&gt;-n ：表示这个字符串的长度大于 0&lt;/li&gt;
&lt;li&gt;-z： 表示这个字符串长度等于 0&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较 string 类型或者是 file 文件都有相应的符号来进行比较，对于要在 bash 上脚本中进行比较很是方便。&lt;br/&gt;对于我之前想要在 .yml 文件中进行比较判断的语句，则可以写成以下这个样了：&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;
test -z $ENV_VALUE1 -a -z $ENV_VALUE2 &amp;amp;&amp;amp; dotnet test --filter Category = category1 || dotnet test --filter Category = all

# -a 代表的是 and&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;是不是 So easy 啊，赶紧去试试！&lt;/p&gt;
&lt;p&gt;参考文章：&lt;br/&gt;&lt;a href=&quot;https://www.lifewire.com/test-linux-command-unix-command-4097166&quot;&gt;How to Use Test Conditions Within a Bash Script&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 14:11:00 +0000</pubDate>
<dc:creator>BUTTERAPPLE</dc:creator>
<og:description>在做项目的CI/ CD 时，难免会用到 Travis.CI 和 AppVeyor 以及 CodeCov 来判断测试的覆盖率，今天突然遇到了一个问题，就是我需要在每次做测试的时候判断是否存在一个环境变量</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/xiyin/p/10434247.html</dc:identifier>
</item>
<item>
<title>unity 简易场景切换 - U头</title>
<link>http://www.cnblogs.com/lihangppz/p/10434245.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/lihangppz/p/10434245.html</guid>
<description>&lt;p&gt;　　场景跳转是游戏里面不可缺少的内容，这里写写我这个小白遇到的问题。&lt;/p&gt;
&lt;p&gt;　　场景资源比较大，同步切换场景不现实。场景一般比较多，都加到scenes in build也不太现实。&lt;/p&gt;
&lt;p&gt;　　这里将初始场景和过渡场景加到scenes in build，一般而言是login和loading场景，将其他场景打成uab的包。&lt;/p&gt;
&lt;p&gt;　　这里忽略其他功能，单纯来说场景跳转。&lt;/p&gt;
&lt;p&gt;　　场景同步跳转SceneManager.LoadScene(&quot;loading&quot;, LoadSceneMode.Single);同步跳转有个条件需要加到scenes in build。loading场景做成一个很小的场景，并且带一个进度条。几乎是瞬间就可以加载完成，同时异步加载目标场景，用SceneManager.LoadSceneAsync(&quot;targetSence&quot;, LoadSceneMode.Single);这里也有条件，加到scenes in build，或者uab已被加载。&lt;/p&gt;
&lt;p&gt;　　在loagin场景，也就是初始场景上，挂上下面脚本：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; System.Collections;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine.SceneManagement;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; MainStart : MonoBehaviour
{
    &lt;/span&gt;&lt;span&gt;void&lt;/span&gt;&lt;span&gt; Start()
    {
        DontDestroyOnLoad(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;&lt;span&gt;);
        SceneManager.LoadScene(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;loading&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        StartCoroutine(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;LoadScene&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;异步加载&lt;/span&gt;
&lt;span&gt;    IEnumerator LoadScene()
    {
        WWW download &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; WWW(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;file:///&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target/targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt;&lt;span&gt; download;
        Debug.Log(download.ToString());
        AsyncOperation &lt;/span&gt;&lt;span&gt;async&lt;/span&gt; = SceneManager.LoadSceneAsync(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt; &lt;span&gt;async&lt;/span&gt;&lt;span&gt;;
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　资源打包部分就不细说了，代码放到Editor下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;40&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEditor;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; TestUab
{

    [MenuItem(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Assets/资源打包&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, &lt;span&gt;false&lt;/span&gt;, &lt;span&gt;63&lt;/span&gt;&lt;span&gt;)]
    &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; testUab()
    {
        &lt;/span&gt;&lt;span&gt;string&lt;/span&gt; targetPath = Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        AssetBundleBuild abb &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt;&lt;span&gt; AssetBundleBuild();
        abb.assetBundleName &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        abb.assetNames &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; &lt;span&gt;string&lt;/span&gt;[&lt;span&gt;1&lt;/span&gt;&lt;span&gt;];
        abb.assetNames[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;] = &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Assets/TestUab/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence.unity&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        AssetBundleBuild[] addArr &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; AssetBundleBuild[&lt;span&gt;1&lt;/span&gt;&lt;span&gt;];
        addArr[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;] =&lt;span&gt; abb;
        
        BuildPipeline.BuildAssetBundles(targetPath, addArr, BuildAssetBundleOptions.None, BuildTarget.Android);
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　将login，loading场景加入到scenes in build，并且打包目标场景后，开始运行，你会发现场景切换没有问题了，但是天空盒变成紫色了。刚开始以为材质丢了，点开Lighting界面，材质没丢，但是shader渲染出来的就是紫色。花了很长时间没解决，然后翻自己的线上工程，发现线上工程没有天空盒，一口老血喷到屏幕上了，最后还是请教老大，才知道问题。&lt;/p&gt;
&lt;p&gt;　　在打uab包时，用的是Android平台，BuildPipeline.BuildAssetBundles(targetPath, addArr, BuildAssetBundleOptions.None, BuildTarget.Android);所以打出来的uab包时Android平台下的，shader都是Android平台下的shader，不同平台shader渲染不一样。把dome工程打成apk，一看果然天空盒没问题，没有变成紫色。线上项目在场景加载完毕的时候会切换材质球的shader，以保证在编辑器下显示没有问题。最终代码如下：&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; System.Collections;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine.SceneManagement;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; MainStart : MonoBehaviour
{

    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; Use this for initialization&lt;/span&gt;
    &lt;span&gt;void&lt;/span&gt;&lt;span&gt; Start()
    {
        DontDestroyOnLoad(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;&lt;span&gt;);
        SceneManager.LoadScene(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;loading&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        StartCoroutine(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;LoadScene&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;异步加载&lt;/span&gt;
&lt;span&gt;    IEnumerator LoadScene()
    {
        WWW download &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; WWW(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;file:///&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target/targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt;&lt;span&gt; download;
        Debug.Log(download.ToString());
        AsyncOperation &lt;/span&gt;&lt;span&gt;async&lt;/span&gt; = SceneManager.LoadSceneAsync(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt; &lt;span&gt;async&lt;/span&gt;&lt;span&gt;;
        ChangeShader();
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换材质&lt;/span&gt;
    &lt;span&gt;private&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; ChangeShader()
    {
&lt;/span&gt;&lt;span&gt;#if&lt;/span&gt; UNITY_EDITOR
        &lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换场景物体材质&lt;/span&gt;
        Terrain terrain = GameObject.FindObjectOfType&amp;lt;Terrain&amp;gt;&lt;span&gt;();
        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (terrain != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
        {
            terrain.materialType &lt;/span&gt;=&lt;span&gt; Terrain.MaterialType.BuiltInLegacyDiffuse;
        }
        Object[] objs &lt;/span&gt;= GameObject.FindObjectsOfType(&lt;span&gt;typeof&lt;/span&gt;&lt;span&gt;(GameObject));
        &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; k = &lt;span&gt;0&lt;/span&gt;; k &amp;lt; objs.Length; k++&lt;span&gt;)
        {
            GameObject go &lt;/span&gt;= objs[k] &lt;span&gt;as&lt;/span&gt;&lt;span&gt; GameObject;
            Renderer[] componentsInChildren &lt;/span&gt;= go.GetComponents&amp;lt;Renderer&amp;gt;&lt;span&gt;();
            &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; i = &lt;span&gt;0&lt;/span&gt;; i &amp;lt; componentsInChildren.Length; i++&lt;span&gt;)
            {
                Material[] sharedMaterials &lt;/span&gt;=&lt;span&gt; componentsInChildren[i].sharedMaterials;
                &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; j = &lt;span&gt;0&lt;/span&gt;; j &amp;lt; sharedMaterials.Length; j++&lt;span&gt;)
                {
                    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (sharedMaterials[j] != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                    {
                        Shader shader &lt;/span&gt;=&lt;span&gt; sharedMaterials[j].shader;
                        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (shader != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                        {
                            Shader shader2 &lt;/span&gt;=&lt;span&gt; Shader.Find(shader.name);
                            &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (shader2 != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                            {
                                sharedMaterials[j].shader &lt;/span&gt;=&lt;span&gt; shader2;
                            }
                        }
                    }
                }
            }
        }
        &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换天空盒材质&lt;/span&gt;
        Shader shader02 =&lt;span&gt; RenderSettings.skybox.shader;
        RenderSettings.skybox.shader &lt;/span&gt;=&lt;span&gt; Shader.Find(shader02.name);

        Debug.Log(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;替换材质&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;#endif&lt;/span&gt;&lt;span&gt;
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　这里没有写进度条相关的功能，大家可以自行解决。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:10:00 +0000</pubDate>
<dc:creator>U头</dc:creator>
<og:description>场景跳转是游戏里面不可缺少的内容，这里写写我这个小白遇到的问题。 场景资源比较大，同步切换场景不现实。场景一般比较多，都加到scenes in build也不太现实。 这里将初始场景和过渡场</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/lihangppz/p/10434245.html</dc:identifier>
</item>
<item>
<title>.Net Core跨平台应用研究-CustomSerialPort(增强型跨平台串口类库) - 赫山老妖</title>
<link>http://www.cnblogs.com/flyfire-cn/p/10434171.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/flyfire-cn/p/10434171.html</guid>
<description>&lt;p align=&quot;left&quot;&gt;      在使用SerialPort进行串口协议解析过程中，经常遇到接收单帧协议数据串口接收事件多次触发，协议解析麻烦的问题。针对此情况，基于开源跨平台串口类库SerialPortStrem进行了进一步封装，实现了一种接收超时响应事件机制，简化串口通讯的使用。&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;      最近，写了一篇博文&lt;a href=&quot;https://www.cnblogs.com/flyfire-cn/p/10356991.html&quot; target=&quot;_blank&quot;&gt;《.net core跨平台应用研究-串口篇》&lt;/a&gt;得到了一些园友的好评，文中介绍了在跨平台应用研究过程中，在dotnet core下使用SerialPort类库在linux下不能支持的踩坑经历及解决办法。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      因网上关于SerialPort类库使用的相关文章较多，在该文中，对串口类库的使用，一笔带过。但在实际使用，使用过SerialPort类库的同学，可能遇到过在数据接收时，由于数据接收事件的触发具有不确定性，很多时候，一帧通讯协议数据，会多次触发，造成程序处理协议数据较为麻烦的问题。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      为简化串口通讯类库的使用，笔者结合自己的相关经验，封装了一个自定义增强型跨平台串口类库，以解决一帧协议数据，多次触发的问题。&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;      由于考虑的是跨平台应用，SerialPort类库并不支持linux系统（在前一篇文章中已介绍过踩坑经历），笔者选用了SerialPortStream类库进行封装。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     该类库支持windows系统和Linux系统，但在Linux系统下运行，需要额外编译目标平台支持库并进行相关环境配置。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     相关编译配置说明在&lt;a href=&quot;https://github.com/jcurl/SerialPortStream&quot; target=&quot;_blank&quot;&gt;https://github.com/jcurl/SerialPortStream&lt;/a&gt;已有介绍，也可参考本人的拙作&lt;a href=&quot;https://www.cnblogs.com/flyfire-cn/p/10356991.html&quot; target=&quot;_blank&quot;&gt;《.net core跨平台应用研究-串口篇》&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;创建跨平台类库&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     为了支持跨平台，我们使用Visual Studio017创建一个基于.NET Standard的类库。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225214408091-1681582560.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     NET Standard是一项API规范，每一个特定的版本，都定义了必须实现的基类库。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     .NET Core是一个托管框架，针对构建控制台、云、ASP.NET Core和UWP应用程序进行了优化。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     每一种托管实现（如.NET Core、.NET Framework或Xamarin）都必须遵循.NET Standard实现基类库（BCL）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     关于NET Standard和跨平台的详细说明在此：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     &lt;a href=&quot;https://zhuanlan.zhihu.com/p/30081607&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/30081607&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     笔者也不再啰嗦呵。&lt;/p&gt;
&lt;h2&gt;实现机制/条件&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     通常串口通讯中，发送数据后，会有一段时间用于等待接收方应答，如此一来，两次数据发送之间，必然会有一定的时间间隔。如ModbusRTU协议就规定，两次数据报文发送之间，需要等待超过发送4个字节以上的间隔时间。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     笔者在单片机以及实时性较高的嵌入式系统中，为处理串口接收与协议的无关性，通常采用数据帧接收超时来处理数据帧的接收。根据串口通讯的速率计算出两次通讯之间所需要超时间隔，取两倍超时间隔时间作为超时参数，每接收到一个字节，将数据放入缓冲区并进行计时，当最后一个字节的接收时间超过超时时间，返回接收数据并清空缓存，一次完整接收完成（DMA接收方式不在此讨论）。&lt;/p&gt;
&lt;h2&gt;.net core跨平台实现&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     在自定义的串口类中，订阅基础串口类数据接收事件，在接收事件每次触发后，读出当前可用的缓冲数据到自定义缓冲区，同时，标记最后接收时间Tick为当前系统Tick。判断是否开启了接收超时处理线程，如未开启，则开启一个接收超时处理线程。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     接收超时处理线程中，以一个较小的时间间隔进行判断，如果最后接收时间与当前时间之间的间隔小于设置值（默认128ms），休眠一段时间（默认16ms）后循环检查。如间隔时间大于设定值，触发外部接收订阅事件，传出接收到的数据，退出超时处理线程。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     此处应有流程图。呵呵，懒得画了，大家自行脑补吧。 ^_^&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     在windows系统或linux系统中，因系统的多任务处理的特性，系统实时性较差，通常50ms以下时间间隔的定时任务，较大程序会出现不可靠的情况（任务执行时间都有可能超过调用间隔时间）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     因此，默认超时时间间隔设置为128ms。也可根据实际使用情况调整，但最小间隔不宜低于64ms。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     注：此处为个人经验和理解，如不认同，请直接忽视。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;h2&gt;主要代码&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;       串口接收事件代码：    &lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('93bc91df-e33f-4c11-80e6-9bc658b9ec44')&quot; readability=&quot;40.5&quot;&gt;&lt;img id=&quot;code_img_closed_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('93bc91df-e33f-4c11-80e6-9bc658b9ec44',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;76&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;protected&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; Sp_DataReceived(&lt;span&gt;object&lt;/span&gt;&lt;span&gt; sender, SerialDataReceivedEventArgs e)
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;             &lt;span&gt;int&lt;/span&gt; canReadBytesLen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt;&lt;span&gt; (ReceiveTimeoutEnable)
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;                 &lt;span&gt;while&lt;/span&gt; (sp.BytesToRead &amp;gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;                     canReadBytesLen =&lt;span&gt; sp.BytesToRead;
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (receiveDatalen + canReadBytesLen &amp;gt;&lt;span&gt; BufSize)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;                         receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;                         &lt;span&gt;throw&lt;/span&gt; &lt;span&gt;new&lt;/span&gt; Exception(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Serial port receives buffer overflow!&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;                     &lt;span&gt;var&lt;/span&gt; receiveLen =&lt;span&gt; sp.Read(recviceBuffer, receiveDatalen, canReadBytesLen);
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (receiveLen !=&lt;span&gt; canReadBytesLen)
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;                         receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                         &lt;span&gt;throw&lt;/span&gt; &lt;span&gt;new&lt;/span&gt; Exception(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Serial port receives exception!&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Copy(recviceBuffer, 0, receivedBytes, receiveDatalen, receiveLen);&lt;/span&gt;
&lt;span&gt;21&lt;/span&gt;                     receiveDatalen +=&lt;span&gt; receiveLen;
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;                     lastReceiveTick =&lt;span&gt; Environment.TickCount;
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (!&lt;span&gt;TimeoutCheckThreadIsWork)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;                         TimeoutCheckThreadIsWork = &lt;span&gt;true&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;                         Thread thread = &lt;span&gt;new&lt;/span&gt;&lt;span&gt; Thread(ReceiveTimeoutCheckFunc)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;                        {
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;                             Name = &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;ComReceiveTimeoutCheckThread&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
&lt;span&gt;29&lt;/span&gt; &lt;span&gt;                        };
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; &lt;span&gt;                        thread.Start();
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;             &lt;span&gt;else&lt;/span&gt;
&lt;span&gt;35&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;                 &lt;span&gt;if&lt;/span&gt; (ReceivedEvent != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获取字节长度&lt;/span&gt;
&lt;span&gt;39&lt;/span&gt;                     &lt;span&gt;int&lt;/span&gt; bytesNum =&lt;span&gt; sp.BytesToRead;
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (bytesNum == &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;                         &lt;span&gt;return&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 创建字节数组&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt;                     &lt;span&gt;byte&lt;/span&gt;[] resultBuffer = &lt;span&gt;new&lt;/span&gt; &lt;span&gt;byte&lt;/span&gt;&lt;span&gt;[bytesNum];
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt;                     &lt;span&gt;int&lt;/span&gt; i = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;                     &lt;span&gt;while&lt;/span&gt; (i &amp;lt;&lt;span&gt; bytesNum)
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;                         &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 读取数据到缓冲区&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt;                         &lt;span&gt;int&lt;/span&gt; j = sp.Read(recviceBuffer, i, bytesNum -&lt;span&gt; i);
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;                         i +=&lt;span&gt; j;
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;                     Array.Copy(recviceBuffer, &lt;span&gt;0&lt;/span&gt;, resultBuffer, &lt;span&gt;0&lt;/span&gt;&lt;span&gt;, i);
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt;                     ReceivedEvent(&lt;span&gt;this&lt;/span&gt;&lt;span&gt;, resultBuffer);
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;System.Diagnostics.Debug.WriteLine(&quot;len &quot; + i.ToString() + &quot; &quot; + ByteToHexStr(resultBuffer));&lt;/span&gt;
&lt;span&gt;55&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;                 &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Clear (receivedBytes,0,receivedBytes.Length );&lt;/span&gt;
&lt;span&gt;57&lt;/span&gt;                 receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      接收超时处理线程代码：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('d32b09a6-98fb-45a7-9bf3-d7890a242e6d')&quot; readability=&quot;36&quot;&gt;&lt;img id=&quot;code_img_closed_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('d32b09a6-98fb-45a7-9bf3-d7890a242e6d',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;67&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt; &lt;span&gt;&amp;lt;summary&amp;gt;&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt;&lt;span&gt; 超时返回数据处理线程方法
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt; &lt;span&gt;&amp;lt;/summary&amp;gt;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt;         &lt;span&gt;protected&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; ReceiveTimeoutCheckFunc()
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;             &lt;span&gt;while&lt;/span&gt;&lt;span&gt; (TimeoutCheckThreadIsWork)
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;                 &lt;span&gt;if&lt;/span&gt; (Environment.TickCount - lastReceiveTick &amp;gt;&lt;span&gt; ReceiveTimeout)
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (ReceivedEvent != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;                         &lt;span&gt;byte&lt;/span&gt;[] returnBytes = &lt;span&gt;new&lt;/span&gt; &lt;span&gt;byte&lt;/span&gt;&lt;span&gt;[receiveDatalen];
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;                         Array.Copy(recviceBuffer, &lt;span&gt;0&lt;/span&gt;, returnBytes, &lt;span&gt;0&lt;/span&gt;&lt;span&gt;, receiveDatalen);
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;                         ReceivedEvent(&lt;span&gt;this&lt;/span&gt;&lt;span&gt;, returnBytes);
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Clear (receivedBytes,0,receivedBytes.Length );&lt;/span&gt;
&lt;span&gt;17&lt;/span&gt;                     receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                     TimeoutCheckThreadIsWork = &lt;span&gt;false&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                 &lt;span&gt;else&lt;/span&gt;
&lt;span&gt;21&lt;/span&gt;                     Thread.Sleep(&lt;span&gt;16&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;

&lt;p&gt;    为验证我们的类库是否能够正常工作，我们创建一个使用类库的.net core控制台程序。&lt;/p&gt;
&lt;p&gt;    为啥选择dotnet core,原因很简单，跨平台。本程序分别需在windows和linux系统下进行运行测试。&lt;/p&gt;
&lt;p&gt;    控制台程序主要实现以下功能：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;    显示系统信息（系统标识、程序标识等）&lt;/li&gt;
&lt;li&gt;    列举系统可用串口资源&lt;/li&gt;
&lt;li&gt;    选择串口&lt;/li&gt;
&lt;li&gt;    打开串口/关闭串口&lt;/li&gt;
&lt;li&gt;    串口测试（打开/发送/关闭）&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('a5f9c87b-eec1-40c9-a0fc-bfe477573f5b')&quot; readability=&quot;32.5&quot;&gt;&lt;img id=&quot;code_img_closed_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('a5f9c87b-eec1-40c9-a0fc-bfe477573f5b',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;60&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; Main(&lt;span&gt;string&lt;/span&gt;&lt;span&gt;[] args)
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;            SetLibPath();
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;            ShowWelcome();
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; 
&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;            GetPortNames();
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;            ShowPortNames();
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt; (serailports.Length == &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Press any key to exit&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;                Console.ReadKey();
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt;                 &lt;span&gt;return&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;#if&lt;/span&gt; RunIsService
&lt;span&gt;17&lt;/span&gt; &lt;span&gt;            RunService();
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; &lt;span&gt;#endif&lt;/span&gt;
&lt;span&gt;19&lt;/span&gt; 
&lt;span&gt;20&lt;/span&gt;             &lt;span&gt;bool&lt;/span&gt; quit = &lt;span&gt;false&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;             &lt;span&gt;while&lt;/span&gt; (!&lt;span&gt;quit)
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\r\nPlease Input command Key\r\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;p:Show SerialPort List&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;t:Test Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;o:Open Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;c:Close Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;n:select next serial port&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;q:exit app&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; &lt;span&gt;                Console.WriteLine();
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;                 &lt;span&gt;var&lt;/span&gt; key =&lt;span&gt; Console.ReadKey().KeyChar;
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;                Console.WriteLine();
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; 
&lt;span&gt;34&lt;/span&gt;                 &lt;span&gt;switch&lt;/span&gt;&lt;span&gt; (key)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; (Char)&lt;span&gt;27&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;q&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;Q&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;                         quit = &lt;span&gt;true&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt; &lt;span&gt;                        ShowWelcome();
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;p&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt; &lt;span&gt;                        ShowPortNames();
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;n&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; &lt;span&gt;                        SelectSerialPort();
&lt;/span&gt;&lt;span&gt;49&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;                        TestUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;w&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt; &lt;span&gt;                        TestWinUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; &lt;span&gt;                        OpenUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;c&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt; &lt;span&gt;                        CloseUart();
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;63&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;     笔者使用类库是直接引用类库项目，大家需要使用的话，可在解决方案资源管理器中，项目的依赖项上点击右键&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215501868-1566940166.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      在NuGet包管理器中，搜索SerialPort或flyfire即可找到并安装本类库。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215615906-591043223.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;     类库地址：&lt;a href=&quot;https://www.nuget.org/packages/flyfire.CustomSerialPort&quot; target=&quot;_blank&quot;&gt;https://www.nuget.org/packages/flyfire.CustomSerialPort&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215711328-1690878690.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;

&lt;h2&gt;Windows测试输出界面&lt;/h2&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215727952-603205333.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215750923-416383800.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;ubuntu测试输出界面&lt;/h2&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215841749-630757689.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;     类库源码与例程地址：&lt;a href=&quot;https://github.com/flyfire-cn/flyfire.CustomSerialPort&quot; target=&quot;_blank&quot;&gt;https://github.com/flyfire-cn/flyfire.CustomSerialPort&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     有需要的同学，请自行获取。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:00:00 +0000</pubDate>
<dc:creator>赫山老妖</dc:creator>
<og:description>在使用SerialPort进行串口协议解析过程中，经常遇到接收单帧协议数据串口接收事件多次触发，协议解析麻烦的问题。针对此情况，基于开源跨平台串口类库SerialPortStrem进行了进一步封装，实</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/flyfire-cn/p/10434171.html</dc:identifier>
</item>
<item>
<title>使用minukube部署kubernetes admission webhook实现etcd pod安全删除 - charlieroro</title>
<link>http://www.cnblogs.com/charlieroro/p/10434138.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/charlieroro/p/10434138.html</guid>
<description>&lt;p&gt;本需求来自于一道面试题😂（本环境使用centos 7）&lt;/p&gt;
&lt;p&gt;最好使用阿里云ec2服务器安装minikube，若使用本地pc的vmware可能会出现网络方面的问题。&lt;/p&gt;
&lt;p&gt;使用如下命令安装minikube，参见&lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-minikube/&quot; target=&quot;_blank&quot;&gt;install minikube&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
# curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;amp;&amp;amp; chmod +x minikube&lt;span&gt;
# sudo cp minikube /usr/local/bin &amp;amp;&amp;amp; rm minikube&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;启动minikube可能会遇到docker版本过旧导致启动失败的问题。默认centos下面yum安装的docker版本比较旧，需要安装最新docker，更新docker参见&lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/centos/&quot; target=&quot;_blank&quot;&gt;Get Docker CE for CentOS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如下命令移除已经安装的docker&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;yum remove docker \
                  docker-&lt;span&gt;client \
                  docker-client-&lt;span&gt;latest \
                  docker-&lt;span&gt;common \
                  docker-&lt;span&gt;latest \
                  docker-latest-&lt;span&gt;logrotate \
                  docker-&lt;span&gt;logrotate \
                  docker-engine&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如下命令安装存储驱动和设置docker stable版本的库&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
yum install -y yum-&lt;span&gt;utils \
  device-mapper-persistent-&lt;span&gt;data \
  lvm2

yum-config-&lt;span&gt;manager \
    --add-&lt;span&gt;repo \
    https://download.docker.com/linux/centos/docker-ce.repo&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用如下命令即可更新为最新的docker&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
yum install docker-ce docker-ce-cli containerd.io
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;管理kubernetes上的服务最好使用helm(本次未用到，如无需要可忽略本节)，helm安装如下：&lt;/p&gt;
&lt;p&gt;使用如下方式获取helm的二进制版本&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32.475961538462&quot;&gt;
&lt;pre&gt;
&lt;span&gt;Download your &lt;a href=&quot;https://github.com/helm/helm/releases&quot; target=&quot;_blank&quot;&gt;desired version&lt;/a&gt;
Unpack it (tar -zxvf helm-v2.0.0-linux-&lt;span&gt;amd64.tgz)
Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/helm/helm/blob/master/docs/rbac.md&quot; target=&quot;_blank&quot;&gt; tiller安装&lt;/a&gt;给出了在不同scope下面安装tiller的方法，最简单的是在cluster-admin下面安装即可（生成环境下建议参考&lt;a href=&quot;https://github.com/helm/helm/blob/master/docs/install.md&quot; target=&quot;_blank&quot;&gt;helm安装文档&lt;/a&gt;将权限最小化）&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
# cat rbac-&lt;span&gt;config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-&lt;span&gt;system
---&lt;span&gt;
apiVersion: rbac.authorization.k8s.io/&lt;span&gt;v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-&lt;span&gt;admin
subjects:
  -&lt;span&gt; kind: ServiceAccount
    name: tiller
    namespace: kube-system&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用如下命令创建即可&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
kubectl create -f rbac-&lt;span&gt;config.yaml
serviceaccount &quot;tiller&quot;&lt;span&gt; created
clusterrolebinding &quot;tiller&quot;&lt;span&gt; created
$ helm init --service-account tiller&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;admission webhook原理&lt;/p&gt;
&lt;p&gt;kubernetes的认证和授权是对客户端进行认证以及对资源进行授权，但在资源的使用处理上不够细化。admission webhook是在一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。参见&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&quot; target=&quot;_blank&quot;&gt;官方资料&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225202115299-175005990.png&quot; alt=&quot;&quot; width=&quot;697&quot; height=&quot;296&quot;/&gt;&lt;/p&gt;
&lt;p&gt;如下图，在部署了admission webhook之后，apiserver会发送一个AdmissionReview的json数据到webhook，其中主要包含一个AdmissionRequest的请求，AdmissionRequest.RawExtension.Raw包含了需要处理的kubernetes组件(如pod，deployment等)的详细信息。webhook在接收到该请求之后会根据自定义逻辑进行处理，并返回处理结果AdmissionResponse。admission webhook有两种处理方式：&lt;/p&gt;
&lt;p&gt;MutatingAdmissionWebhook：可以修改可以自定义的策略MutatingAdmissionWebhook的处理一般优先于MutatingAdmissionWebhook，这样前者修改的内容就可以由后者进行校验&lt;/p&gt;
&lt;p&gt;ValidatingAdmissionWebhook: 允许或拒绝客户自定义的策略&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;44&quot;&gt;
&lt;pre&gt;
type AdmissionReview struct&lt;span&gt; {
    metav1.TypeMeta `json:&quot;,inline&quot;&lt;span&gt;`
    // Request describes the attributes for the admission request.
    // +optional
    Request *AdmissionRequest `json:&quot;request,omitempty&quot; protobuf:&quot;bytes,1,opt,name=request&quot;&lt;span&gt;`
    // Response describes the attributes for the admission response.
    // +optional
    Response *AdmissionResponse `json:&quot;response,omitempty&quot; protobuf:&quot;bytes,2,opt,name=response&quot;&lt;span&gt;`
}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225200938176-1213925158.png&quot; alt=&quot;&quot; width=&quot;671&quot; height=&quot;579&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 处理流程图如下，可以看到MutatingAdmissionWebhook的处理优先于MutatingAdmissionWebhook&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225204708180-361997199.png&quot; alt=&quot;&quot; width=&quot;734&quot; height=&quot;325&quot;/&gt;&lt;/p&gt;

&lt;p&gt;使用如下命令启动minikube&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
minikube start --vm-driver=none --extra-config=apiserver.enable-admission-plugins=&quot;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota&quot;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; 使用kubectl api-versions查看是否支持admissionregistration.k8s.io/v1alpha1 API&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://banzaicloud.com/blog/k8s-admission-webhooks/&quot; target=&quot;_blank&quot;&gt;In-depth introduction to Kubernetes admission webhooks&lt;/a&gt;是个很好的例子，里面详细记录了创建admission webhook的方式。&lt;a href=&quot;https://github.com/woodliu/admission-webhook-example&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;参照&lt;a href=&quot;https://banzaicloud.com/blog/k8s-admission-webhooks/&quot; target=&quot;_blank&quot;&gt;In-depth introduction to Kubernetes admission webhooks&lt;/a&gt;实现了根据etcd pod角色(leader和非leader)来删除pod。&lt;/p&gt;
&lt;p&gt;首先使用webhook-create-signed-cert.sh文件来生成自签证书，后续ValidatingWebhookConfiguration中会用到&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
# ./deployment/webhook-create-signed-cert.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;部署admission webhook&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
# kubectl create -f deployment/&lt;span&gt;deployment.yaml&lt;span&gt;
# kubectl create -f deployment/service.yaml&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;生成待CA bundle的config文件以及validatingwebhook.yaml原始文件如下&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
# cat ./deployment/validatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh &amp;gt; ./deployment/validatingwebhook-ca-bundle.yaml
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
apiVersion: admissionregistration.k8s.io/&lt;span&gt;v1beta1
kind: ValidatingWebhookConfiguration
metadata:
  name: validation-webhook-example-&lt;span&gt;cfg
  labels:
    app: admission-webhook-&lt;span&gt;example
webhooks:
  - name: required-&lt;span&gt;labels.banzaicloud.com
    clientConfig:
      service:
        name: admission-webhook-example-&lt;span&gt;svc #service名称
        namespace: default&lt;span&gt;
        path: &quot;/validate&quot; #访问的后缀路径&lt;span&gt;
      caBundle: ${CA_BUNDLE} #上述命令生成的认证字段
    rules:
      - operations: [ &quot;DELETE&quot;&lt;span&gt; ] #操作的动作
        apiGroups: [&quot;apps&quot;, &quot;&quot;&lt;span&gt;]  #api groups
        apiVersions: [&quot;v1&quot;&lt;span&gt;]      #api version
        resources: [&quot;pods&quot;&lt;span&gt;]      #操作的资源
    namespaceSelector:
      matchLabels:
        admission-webhook-example: enabled # 限制default的命名空间&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;给default namespace打上标签&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ kubectl label namespace default admission-webhook-example=enabled
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;创建ValidatingWebhookConfiguration&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ kubectl create -f deployment/validatingwebhook-ca-bundle.yaml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;admission-webhook-example/build用于编译和生成容器镜像&lt;/p&gt;
&lt;p&gt;etcd的部署直接使用admission-webhook-example\deployment\etcd中的配置文件即可，这样在删除etcd的leader时会显示如下内容&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225213902358-517584601.png&quot; alt=&quot;&quot; width=&quot;1361&quot; height=&quot;42&quot;/&gt;&lt;/p&gt;
&lt;p&gt;admission webhook的实现只有2个文件，main.go和webhook.go。main.go中通过mux.HandleFunc(&quot;/validate&quot;, whsvr.serve)来启动一个http服务处理路径/validate(对应validatingwebhook.yaml的webhooks.clientConfig.service.path)的请求。主要逻辑是现在函数func (whsvr *WebhookServer) validate(ar *v1beta1.AdmissionReview) *v1beta1.AdmissionRespons中&lt;/p&gt;

&lt;p&gt;FAQ：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;执行yum install -y socat可解决如下问题：&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
an error occurred forwarding 40546 -&amp;gt; 44134: error forwarding port 44134 to pod 3ea221f842e1446a5fd9da9fc29e7e415a1ecb6dd6d07c56f64fb4788f2c3915, uid : unable to do port forwarding: socat not found.
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;TIPS:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; 使用如下方式可以安装etcdctl命令行工具&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
&lt;span&gt;# choose either URL
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/coreos/etcd/releases/download
DOWNLOAD_URL=&lt;span&gt;${GOOGLE_URL}

rm -f /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz
rm -rf /tmp/etcd-download-test &amp;amp;&amp;amp; mkdir -p /tmp/etcd-download-&lt;span&gt;test

curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1&lt;span&gt;
rm -f /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz

/tmp/etcd-download-test/etcd --&lt;span&gt;version
ETCDCTL_API=3 /tmp/etcd-download-test/&lt;span&gt;etcdctl version
# start a local etcd server
/tmp/etcd-download-test/&lt;span&gt;etcd

# write,read to etcd
ETCDCTL_API=3 /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379&lt;span&gt; put foo bar
ETCDCTL_API=3 /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get foo&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;go get -u github.com/golang/dep/cmd/dep 安装dep&lt;/li&gt;
&lt;li&gt;etcdv3的API使用可以参考&lt;a href=&quot;https://godoc.org/github.com/dverbeek84/etcd/clientv3&quot; target=&quot;_blank&quot;&gt;package clientv3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;/p&gt;
&lt;p&gt;https://github.com/helm/helm/blob/master/docs/install.md&lt;/p&gt;
&lt;p&gt;https://container-solutions.com/some-admission-webhook-basics/&lt;/p&gt;
&lt;p&gt;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&lt;/p&gt;
&lt;p&gt;https://github.com/kubernetes/kubernetes/tree/master/test/e2e/testing-manifests/statefulset/etcd&lt;/p&gt;
&lt;p&gt;https://github.com/morvencao/kube-mutating-webhook-tutorial/blob/master/medium-article.md&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 13:54:00 +0000</pubDate>
<dc:creator>charlieroro</dc:creator>
<og:description>本需求来自于一道面试题😂（本环境使用centos 7） 最好使用阿里云ec2服务器安装minikube，若使用本地pc的vmware可能会出现网络方面的问题。 使用如下命令安装mini</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/charlieroro/p/10434138.html</dc:identifier>
</item>
<item>
<title>如何在ASP.NET Core中使用JSON Patch - LamondLu</title>
<link>http://www.cnblogs.com/lwqlun/p/10433615.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/lwqlun/p/10433615.html</guid>
<description>&lt;blockquote readability=&quot;3.6875&quot;&gt;
&lt;p&gt;原文： &lt;a href=&quot;https://dotnetcoretutorials.com/2017/11/29/json-patch-asp-net-core/&quot;&gt;JSON Patch With ASP.NET Core&lt;/a&gt;&lt;br/&gt;作者：.NET Core Tutorials&lt;br/&gt;译文：如何在ASP.NET Core中使用JSON Patch&lt;br/&gt;地址：&lt;a href=&quot;https://www.cnblogs.com/lwqlun/p/10433615.html&quot; class=&quot;uri&quot;&gt;https://www.cnblogs.com/lwqlun/p/10433615.html&lt;/a&gt;&lt;br/&gt;译者：Lamond Lu&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/65831/201902/65831-20190225213208189-1747876581.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;JSON Patch&lt;/strong&gt;是一种使用API显式更新文档的方法。它本身是一种契约，用于描述如何修改文档（例如：将字段的值替换成另外一个值），而不必同时发送其他未更改的属性值。&lt;/p&gt;

&lt;p&gt;你可以在以下链接（&lt;a href=&quot;http://jsonpatch.com/&quot; class=&quot;uri&quot;&gt;http://jsonpatch.com/&lt;/a&gt;）中找到JSON Patch的官方文档，但是这里我们将进一步研究一下如何在ASP.NET Core中实现JSON Patch。&lt;/p&gt;
&lt;p&gt;为了演示JSON Patch, 我创建了以下C#对象类, 后续我将使用JSON Patch请求来操作这个对象类的实例。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public List&amp;lt;string&amp;gt; Friends { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所有的JSON Patch请求都是遵循一个相似的结构。它有一个固定的“操作”列表。每个操作本身拥有3个属性：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&quot;op&quot; - 定义了你要执行何种操作，例如add, replace, test等。&lt;/li&gt;
&lt;li&gt;&quot;path&quot; - 定义了你要操作对象属性路径。用前面的&lt;code&gt;Person&lt;/code&gt;类为例，如果你希望修改&lt;code&gt;FirstName&lt;/code&gt;属性，那么你使用的操作路径应该是&quot;/FirstName&quot;。&lt;/li&gt;
&lt;li&gt;&quot;value&quot; - 在大部分情况下，这个属性表示你希望在操作中使用的值。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;现在让我们来看一下每一个的操作如何使用。&lt;/p&gt;
&lt;h2 id=&quot;add&quot;&gt;Add&lt;/h2&gt;
&lt;p&gt;Add操作通常意味着你要向对象中添加属性，或者向数组中添加项目。对于前者，在C#中是没有用的，因为C#是强类型语言，所以不能将属性添加到编译时尚未定义的对象上。&lt;/p&gt;
&lt;p&gt;所以这里如果想往数组中添加项目，PATCH请求的内容应该如下所示。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/Friends/1&quot;, &quot;value&quot;: &quot;Mike&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这将在Friends数组的索引1处插入一个&quot;Mike&quot;值。&lt;/p&gt;
&lt;p&gt;或者你还可以使用&quot;-&quot;在数组尾部插入记录。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/Friends/-&quot;, &quot;value&quot;: &quot;Mike&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;remove&quot;&gt;Remove&lt;/h2&gt;
&lt;p&gt;与Add操作类似，删除操作意味着你希望删除对象中属性，或者从数据中删除某一项。但是因为在C#中你无法移除属性，实际操作时，它会将属性的值变更为default(T)。在某些情况下，如果属性是可空的，则会设置属性值为NULL。但是需要小心，因为当在值类型上使用时，例如int, 则该值实际上会重置为&quot;0&quot;。&lt;/p&gt;
&lt;p&gt;如果要在对象上删除某一属性以达到重置的效果，你可以使用一下命令。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/FirstName&quot;}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;当然你也可以使用删除命令删除数组中的某一项。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/Friends/1&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这将删除数组索引为1的项目。但是有时候使用索引从数组中删除数据是非常危险的，因为这里没有一个&quot;where&quot;条件来控制删除， 有可能在删除的时候，数据库中对应数组已经发生变化了。实际上有一个JSON Patch操作可以帮助解决这个问题，后面我会描述它。&lt;/p&gt;
&lt;h2 id=&quot;replace&quot;&gt;Replace&lt;/h2&gt;
&lt;p&gt;Replace操作和它的字面意思完全一样，可以使用它来替换已有值。针对简单属性，你可以使用如下的命令。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Jim&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;你同样可以使用它来替换数组中的对象。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/Friends/1&quot;, &quot;value&quot;: &quot;Bob&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;你甚至可以用它来替换整个数组。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/Friends&quot;, &quot;value&quot;: [&quot;Bob&quot;, &quot;Bill&quot;] }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;copy&quot;&gt;Copy&lt;/h2&gt;
&lt;p&gt;Copy操作可以将值从一个路径复制到另一个路径。这个值可以是属性，对象，或者数据。在下面的例子中，我们将FirstName属性的值复制到了LastName属性上。这个命令的使用场景不是很多。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;copy&quot;, &quot;from&quot;: &quot;/FirstName&quot;, &quot;path&quot; : &quot;/LastName&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;move&quot;&gt;Move&lt;/h2&gt;
&lt;p&gt;Move操作非常类似于Copy操作，但是正如它的字面意思，&quot;from&quot;字段的值将被移除。如果你看一下ASP.NET Core的JSON Patch的底层代码，你会发现，它实际上它会在&quot;from&quot;路径上执行Remove操作，在&quot;path&quot;路径上执行Add操作。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;move&quot;, &quot;from&quot;: &quot;/FirstName&quot;, &quot;path&quot; : &quot;/LastName&quot; }    &lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;test&quot;&gt;Test&lt;/h2&gt;
&lt;p&gt;在当前的ASP.NET Core公开发行版中没有Test操作，但是如果你在Github上查看源代码，你会发现微软已经处理了Test操作。Test操作是一种乐观锁定的方法，或者更简单的说，它会检测数据对象从服务器读取之后，是否发生了更改。&lt;/p&gt;
&lt;p&gt;我们以如下操作为例。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;[
    { &quot;op&quot;: &quot;test&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Bob&quot; }
    { &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Jim&quot; }
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个操作首先会检查&quot;/FirstName&quot;路径的值是否&quot;Bob&quot;, 如果是，就将它改为&quot;Jim&quot;。 如果不是，则什么事情都不会发生。这里你需要注意，在一个Test操作的请求体内可以包含多个Test操作，但是如果其中任何一个Test操作验证失败，所以的变更操作都不会被执行。&lt;/p&gt;

&lt;p&gt;JSON Patch的一大优势在于它的请求操作体很小，只发送对象的更改内容。 但是在ASP.NET Core中使用JSON Patch还有另一个很大的好处，就是C＃是一种强类型语言，无法区分是要将模型的值设置为NULL，还是忽略该属性， 而使用JSON Patch可以解决这个问题。&lt;/p&gt;
&lt;p&gt;这里如果没有好的例子，很难解释。 所以想象一下我从API请求一个“Person”对象。 在C＃中，模型可能如下所示：&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;
public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;当从API返回Json对象时，它看起来可能像这样。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;James&quot;, 
    &quot;lastName&quot; : &quot;Smith&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;现在在前端，如果不使用JSON Patch, 如果我只想更新FirstName, 我可能在请求中附带一下请求体。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;Jim&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;现在当我在C#中反序列化这个模型时，问题就出现了。不要看下面的代码，想一下此时我们的模型中的属性值是什么？&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;public class Person
{
    public string FirstName { get; set; } //Jim
    public string LastName { get; set; } //&amp;lt;Null&amp;gt;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;因为我们发送LastName属性的值，所以它被反序列化为Null。 但这很简单，我们可以忽略NULL的值，只更新我们实际传递的字段。 但这不一定是正确的，如果该字段实际上可以为空呢？ 如果我们发送了以下请求体怎么办？&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;Jim&quot;, 
    &quot;lastName&quot; : null
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所以现在我们实际上已经指定我们想要取消该字段。但是因为C＃是强类型的，所以我们无法在服务器端进行模型绑定的时候，我们无法确定它是否要将该字段的值设置为NULL。&lt;/p&gt;
&lt;p&gt;这似乎是一个奇怪的场景，因为前端可以始终发送完整的数据模型，永远不会省略字段。并且在大多数情况下，前端Web库的模型将始终与API的模型匹配。但有一种情况并非如此，那就是移动应用程序。通常向苹果应用商店提交手机应用，可能需要数周时间才能获得批准。在这个时候，你可能还需要在Web或Android应用程序中使用新模型。在不同平台之间实现同步非常困难，而且通常是不可能。虽然API版本确实对解决这个问题有很长的路要走，但我仍然认为JSON Patch在解决这个问题方面具有很大的实用性。&lt;/p&gt;
&lt;p&gt;最后，让我们使用JSON Patch！我们可以使用以下JSON Patch请求更新Person对象&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;[
    {
      &quot;op&quot;: &quot;replace&quot;,
      &quot;path&quot;: &quot;/FirstName&quot;,
      &quot;value&quot;: &quot;Jim&quot;
    }
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这明确表示我们想要更改名字而不是其他内容。 它准确的告诉我们到底将要发生什么。&lt;/p&gt;

&lt;p&gt;在Visual Studio中，我们可以在Package Manage Console中安装官方的Json Patch库（默认创建的ASP.NET Core项目中没有该库）。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Install-Package Microsoft.AspNetCore.JsonPatch&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;为了演示，我将添加如下的一个控制器类。这里需要注意我们使用的HTTP verb是HttpPatch, 请求参数的类型是&lt;code&gt;JsonPatchDocument&lt;/code&gt;。 为了更新对象，我们只需要简单调用&lt;code&gt;ApplyTo&lt;/code&gt;方法，并传入了需要更新的对象。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;[Route(&quot;api/[controller]&quot;)]
public class PersonController : Controller
{
    private readonly Person _defaultPerson = new Person
    {
        FirstName = &quot;Jim&quot;,
        LastName = &quot;Smith&quot;
    };
 
    [HttpPatch(&quot;update&quot;)]
    public Person Patch([FromBody]JsonPatchDocument&amp;lt;Person&amp;gt; personPatch)
    {
        personPatch.ApplyTo(_defaultPerson);
        return _defaultPerson;
    }
}
 
public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在以上示例中，我们只是使用了存放在控制器中的简单对象并对其进行了更新，但是在正式的API中，我们需要从数据库中拉取数据对象，更新对象，并重新保存到数据库。&lt;/p&gt;
&lt;p&gt;当我们使用如下请求体发送JSON Patch请求时：&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;[
    {&quot;op&quot; : &quot;replace&quot;, &quot;path&quot; : &quot;/FirstName&quot;, &quot;value&quot; : &quot;Bob&quot;}
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;我们可以得到如下响应内容：&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;真棒！ 我们的名字改为Bob！ 使用JSON Patch启动和运行真的很简单。&lt;/p&gt;

&lt;p&gt;针对JSON Patch的使用，最大的问题是，你经常需要从API返回View Model或者DTO, 并生成PATCH请求。但是如果将这些修改请求应用于数据库对象上？大部分情况下，开发人员都挣扎在与此。这里我们可以使用Automapper来帮助完成这个转换的工作。&lt;/p&gt;
&lt;p&gt;例如如下代码:&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;
[HttpPatch(&quot;update/{id}&quot;)]
public Person Patch(int id, [FromBody]JsonPatchDocument&amp;lt;PersonDTO&amp;gt; personPatch)
{
    //获取原始Person对象实例
    PersonDatabase personDatabase = _personRepository.GetById(id); 
    
    //将Person对象实例转换为PersonDTO对象实例
    PersonDTO personDTO = _mapper.Map&amp;lt;PersonDTO&amp;gt;(personDatabase); 
    
    //应用Patch修改
    personPatch.ApplyTo(personDTO);  
    
    //将更新后的PersonDTO对象，重新映射到Person对象实例中
    _mapper.Map(personDTO, personDatabase); 
    
    //将更新后的Person对象实例保存到数据库
    _personRepository.Update(personDatabase); 
 
    return personDTO;
}&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Mon, 25 Feb 2019 13:33:00 +0000</pubDate>
<dc:creator>LamondLu</dc:creator>
<og:description>原文： 'JSON Patch With ASP.NET Core' 作者：.NET Core Tutorials 译文：如何在ASP.NET Core中使用JSON Patch 地址：https:/</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/lwqlun/p/10433615.html</dc:identifier>
</item>
<item>
<title>如何定位前端线上问题（如何排查前端生产问题） - 一步一个脚印一个坑</title>
<link>http://www.cnblogs.com/warm-stranger/p/10430346.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/warm-stranger/p/10430346.html</guid>
<description>&lt;p&gt;　　一直以来，前端的线上问题很难定位，因为它发生于用户的一系列操作之后。错误的原因可能源于机型，网络环境，复杂的操作行为等等，在我们想要去解决的时候很难复现出来，自然也就无法解决。 当然，这些问题并非不能克服，让我们来一起看看如何去定位线上的问题吧。&lt;/p&gt;
&lt;p&gt;　　所谓，工欲善其事必先利其器，你不能撸起袖子蛮干，所以，我们需要一个工具。我们曾经尝试用过很多监控工具去统计这些错误，比如，听云、OneApm、sentry、FundBug、growingIo 等等。 每家工具都各有所长，但也都各有所短，而且要花不少的钱（感觉是痛点，哈哈）。&lt;/p&gt;
&lt;h3&gt;　　一、统计前端错误&lt;/h3&gt;
&lt;p&gt;　　众所周知，我们有办法去统计前端的错误，那就是大名鼎鼎的 window.onerror 方法， 用法如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
&lt;span&gt;    //&lt;/span&gt;&lt;span&gt; 重写 onerror 进行jsError的监听&lt;/span&gt;
    window.onerror = &lt;span&gt;function&lt;/span&gt;&lt;span&gt;(errorMsg, url, lineNumber, columnNumber, errorObj)
    {
      &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; errorStack = errorObj ? errorObj.stack : &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
      &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 分类处理信息&lt;/span&gt;
&lt;span&gt;      siftAndMakeUpMessage(errorMsg, url, lineNumber, columnNumber, errorStack);
    };&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;
　　window.onerror 方法中参数的意义我就不一一介绍了，我相信大家也已经耳熟能详了。 总之它能够为我们记录下线上的很多错误，以及一些额外的信息。我将window.onerror方法收集到的错误信息进行分析统计后的结果如下：
&lt;/pre&gt;
&lt;p&gt;        &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225125611992-1716164198.png&quot; alt=&quot;&quot; width=&quot;868&quot; height=&quot;474&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 　　　如上图所见： 我们统计出了每天的错误量，每个小时的错误量，每天的错误率变化，来鉴定我们线上环境是否健康。我们按照JS错误数量进行分类排序，按照页面进行错误分类。通过上边的数据分析，我们能够清晰地观察到线上项目的报错情况。&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;　　二、分析错误详情&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;　　&lt;span&gt;线上的错误日志统计出来了， 如何解析这些错误日志呢。如下图，解析出用户的机型，版本，系统平台，影响范围，以及具体的错误位置， 从而提高我们解决问题的效率。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　  &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225183852197-522824587.png&quot; alt=&quot;&quot; width=&quot;854&quot; height=&quot;444&quot;/&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;h3&gt; &lt;/h3&gt;
&lt;h3&gt; 　 疑问？&lt;/h3&gt;
&lt;p&gt;　　window.onerror 方法能够利用的地方都已经用的差不多了，但是它真的可以帮我们定位和解决前端线上的问题吗？&lt;/p&gt;
&lt;p&gt;　　线上能够修复的问题我已经尽量修复了，但是线上的问题频发。 当客服反馈一个问题， 你发现没有测试机型，无法复现用户错误的时候，让你来修复这个问题，只能两眼一抹黑，无能为力。&lt;/p&gt;
&lt;p&gt;　　例如：线上用户进过了复杂的链接跳转而发生了错误；用户调用的接口发生了异常或者超时；线上的用户反馈异常根本就跟实际情况不符，等等。这些非直观型的问题该如何解决？ 所以，我们需要用户的行为记录。&lt;/p&gt;
&lt;h3&gt;　　三、记录用户的访问行为&lt;/h3&gt;
&lt;p&gt;　　有些错误是前端页面经过复杂的跳转、回退之后才发生的，就算测试人员也很难测试出这种问题，因为线上的用户的任何行为都有可能出现。往往我们知道的只是他在最后停留的页面发生了错误。 如此，我们记录下用户的跳转日志， 就能够复现出用户的行为， 从而复现BUG&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225203859419-1929436470.png&quot; alt=&quot;&quot; width=&quot;422&quot; height=&quot;235&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　四、记录用户的接口行为&lt;/h3&gt;
&lt;p&gt;　　接口请求是一个前端项目涉及最多的行为，接口的异常包括：后台报错，响应超时，网络环境较差，重复接口数据覆盖等等。这些错误也只有在真实的用户环境中才会发生，是典型的线上问题。我们可以记录下用户的请求时间，参数，响应时间，响应状态等等，可以具体分析出来接口对页面的影响。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225204029452-1229334743.png&quot; alt=&quot;&quot; width=&quot;470&quot; height=&quot;311&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　五、记录用户的点击行为&lt;/h3&gt;
&lt;p&gt;　　用户经过一系列复杂的行为操作之后（主要是点击行为），页面的样子和保存的数据都经过了很多变化，此时此刻最容易发生数据错乱的现象，导致修复bug的时候无从入手，是复现用户行为中重要的一环。&lt;/p&gt;
&lt;p&gt;     &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225210531410-1295469117.png&quot; alt=&quot;&quot; width=&quot;395&quot; height=&quot;177&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　六、记录用户的页面截图&lt;/h3&gt;
&lt;p&gt;　　即使你记录下所有的行为，但是你依然需要看到页面的样子，才能够分析出问题所在，那么我们依然可以通过js截图来看看用户设备上的样子。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225211433703-404715797.png&quot; alt=&quot;&quot; width=&quot;389&quot; height=&quot;227&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　七、分析用户的场外信息&lt;/h3&gt;
&lt;p&gt;　　当用户所有的行为都被我们掌握之后，我们能够复现出用户的行为，甚至能够复现出用户的问题，也许我们还需要一些场外信息才能精准定位问题，比如，用户的机型，地理位置，系统版本，当时的网络环境（这个不准确，我是依据用户当时首次加载页面的时间来判断，只能作为参考依据）&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225211959356-383480033.png&quot; alt=&quot;&quot; width=&quot;354&quot; height=&quot;456&quot;/&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;h3&gt;　　总结：&lt;/h3&gt;
&lt;p&gt;　　 问题产生的原因五花八门，只有把日志做全了，才能够准确的定位和解决问题。&lt;/p&gt;
&lt;p&gt;　　 这是我排查线上问题的经验和实战，分享出来，以求分享和学习。&lt;/p&gt;

&lt;p&gt;　　 说了这么多都没有直接体验直观，这是我自己做的demo, 欢迎指教。 &lt;a href=&quot;https://www.webfunny.cn/&quot; target=&quot;_blank&quot;&gt;Demo地址&lt;/a&gt; &lt;/p&gt;











</description>
<pubDate>Mon, 25 Feb 2019 13:30:00 +0000</pubDate>
<dc:creator>一步一个脚印一个坑</dc:creator>
<og:description>一直以来，前端的线上问题很难定位，因为它发生于用户的一系列操作之后。错误的原因可能源于机型，网络环境，复杂的操作行为等等，在我们想要去解决的时候很难复现出来，自然也就无法解决。 当然，这些问题并非不能</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/warm-stranger/p/10430346.html</dc:identifier>
</item>
<item>
<title>JS引擎线程的执行过程的三个阶段（一） - 奋斗的小舟</title>
<link>http://www.cnblogs.com/BoatGina/p/10433518.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/BoatGina/p/10433518.html</guid>
<description>&lt;p data-source-line=&quot;5&quot;&gt;浏览器首先按顺序加载由&amp;lt;script&amp;gt;标签分割的js代码块，加载js代码块完毕后，立刻进入以下三个阶段，然后再按顺序查找下一个代码块，再继续执行以下三个阶段，无论是外部脚本文件（不异步加载）还是内部脚本代码块，都是一样的原理，并且都在同一个全局作用域中。&lt;/p&gt;
&lt;p data-source-line=&quot;7&quot;&gt;JS引擎线程的执行过程的三个阶段：&lt;/p&gt;
&lt;ul data-source-line=&quot;8&quot;&gt;&lt;li&gt;语法分析&lt;/li&gt;
&lt;li&gt;预编译阶段&lt;/li&gt;
&lt;li&gt;执行阶段&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;一-语法分析&quot; data-source-line=&quot;13&quot;&gt;一. 语法分析&lt;/h2&gt;
&lt;p data-source-line=&quot;14&quot;&gt;分析该js脚本代码块的语法是否正确，如果出现不正确，则向外抛出一个语法错误（SyntaxError），停止该js代码块的执行，然后继续查找并加载下一个代码块；如果语法正确，则进入预编译阶段。&lt;/p&gt;
&lt;p data-source-line=&quot;16&quot;&gt;下面阶段的代码执行不会再进行语法校验，语法分析在代码块加载完毕时统一检验语法。&lt;/p&gt;
&lt;h2 id=&quot;二-预编译阶段&quot; data-source-line=&quot;18&quot;&gt;二. 预编译阶段&lt;/h2&gt;
&lt;h3 id=&quot;1-js的运行环境&quot; data-source-line=&quot;20&quot;&gt;1. js的运行环境&lt;/h3&gt;
&lt;ul data-source-line=&quot;21&quot; readability=&quot;0.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;全局环境（JS代码加载完毕后，进入代码预编译即进入全局环境）&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;函数环境（函数调用执行时，进入该函数环境，不同的函数则函数环境不同）&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;eval（不建议使用，会有安全，性能等问题）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p data-source-line=&quot;27&quot;&gt;每进入一个不同的运行环境都会创建一个相应的执行上下文（Execution Context），那么在一段JS程序中一般都会创建多个执行上下文，js引擎会以栈的方式对这些执行上下文进行处理，形成函数调用栈（call stack），栈底永远是全局执行上下文（Global Execution Context），栈顶则永远是当前执行上下文。&lt;/p&gt;
&lt;h3 id=&quot;2-函数调用栈执行栈&quot; data-source-line=&quot;29&quot;&gt;2. 函数调用栈/执行栈&lt;/h3&gt;
&lt;p data-source-line=&quot;30&quot;&gt;调用栈，也叫执行栈，具有LIFO（后进先出）结构，用于存储在代码执行期间创建的所有执行上下文。&lt;/p&gt;
&lt;p data-source-line=&quot;32&quot;&gt;首次运行JS代码时，会创建一个全局执行上下文并Push到当前的执行栈中。每当发生函数调用，引擎都会为该函数创建一个新的函数执行上下文并Push到当前执行栈的栈顶。&lt;/p&gt;
&lt;p data-source-line=&quot;34&quot;&gt;当栈顶函数运行完成后，其对应的函数执行上下文将会从执行栈中Pop出，上下文控制权将移到当前执行栈的下一个执行上下文。&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
var a = 'Hello World!';

function first() {  
  console.log('Inside first function');  
  second();  
  console.log('Again inside first function');  
}

function second() {  
  console.log('Inside second function');  
}

first();  
console.log('Inside Global Execution Context');

// Inside first function
// Inside second function
// Again inside first function
// Inside Global Execution Context
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;36&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt; &lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;57&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2b271448ad38e8fde43f28db066af7dbe356cbb3/68747470733a2f2f757365722d676f6c642d63646e2e786974752e696f2f323031382f31312f352f313636653235386531643032383161363f696d61676556696577322f302f772f313238302f682f3936302f666f726d61742f776562702f69676e6f72652d6572726f722f31&quot; alt=&quot;函数调用栈&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;3-执行上下文的创建&quot; data-source-line=&quot;59&quot;&gt;3. 执行上下文的创建&lt;/h3&gt;
&lt;p data-source-line=&quot;61&quot;&gt;执行上下文可理解为当前的执行环境，与该运行环境相对应，具体分类如上面所说分为全局执行上下文和函数执行上下文。创建执行上下文的三部曲：&lt;/p&gt;
&lt;h4 id=&quot;31-创建变量对象&quot; data-source-line=&quot;69&quot;&gt;3.1 创建变量对象&lt;/h4&gt;
&lt;p data-source-line=&quot;71&quot;&gt;&lt;img src=&quot;https://heyingye.github.io/2018/03/19/js%E5%BC%95%E6%93%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89/img/VO.jpg&quot; alt=&quot;创建变量对象&quot;/&gt;&lt;/p&gt;
&lt;ul data-source-line=&quot;73&quot; readability=&quot;3&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;创建arguments对象：检查当前上下文中的参数，建立该对象的属性与属性值，仅在函数环境(非箭头函数)中进行，全局环境没有此过程&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;检查当前上下文的函数声明：按代码顺序查找，将找到的函数提前声明，如果当前上下文的变量对象没有该函数名属性，则在该变量对象以函数名建立一个属性，属性值则为指向该函数所在堆内存地址的引用，如果存在，则会被新的引用覆盖。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;检查当前上下文的变量声明：按代码顺序查找，将找到的变量提前声明，如果当前上下文的变量对象没有该变量名属性，则在该变量对象以变量名建立一个属性，属性值为undefined；如果存在，则忽略该变量声明&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p data-source-line=&quot;79&quot;&gt;函数声明提前和变量声明提升是在创建变量对象中进行的，且函数声明优先级高于变量声明。具体是如何函数和变量声明提前的可以看后面。&lt;/p&gt;
&lt;p data-source-line=&quot;82&quot;&gt;创建变量对象发生在预编译阶段，但尚未进入执行阶段，该变量对象都是不能访问的，因为此时的变量对象中的变量属性尚未赋值，值仍为undefined，只有进入执行阶段，变量对象中的变量属性进行赋值后，变量对象（Variable Object）转为活动对象（Active Object）后，才能进行访问，这个过程就是VO –&amp;gt; AO过程。&lt;/p&gt;
&lt;h4 id=&quot;32-建立作用域链&quot; data-source-line=&quot;84&quot;&gt;3.2 建立作用域链&lt;/h4&gt;
&lt;p data-source-line=&quot;86&quot;&gt;通俗理解，作用域链由当前执行环境的变量对象（未进入执行阶段前）与上层环境的一系列活动对象组成，它保证了当前执行环境对符合访问权限的变量和函数的有序访问。&lt;/p&gt;
&lt;p data-source-line=&quot;88&quot;&gt;可以通过一个例子简单理解：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
var num = 30;

function test() {
    var a = 10;

    function innerTest() {
        var b = 20;

        return a + b
    }

    innerTest()
}

test()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;89&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt; &lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;107&quot;&gt;在上面的例子中，当执行到调用innerTest函数，进入innerTest函数环境。全局执行上下文和test函数执行上下文已进入执行阶段，innerTest函数执行上下文在预编译阶段创建变量对象，所以他们的活动对象和变量对象分别是AO(global)，AO(test)和VO(innerTest)，而innerTest的作用域链由当前执行环境的变量对象（未进入执行阶段前）与上层环境的一系列活动对象组成，如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
innerTestEC = {

    //变量对象
    VO: {b: undefined}, 

    //作用域链
    scopeChain: [VO(innerTest), AO(test), AO(global)],  
    
    //this指向
    this: window
}
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;109&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt; &lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;123&quot;&gt;深入理解的话，创建作用域链，也就是创建词法环境，而词法环境有两个组成部分：&lt;/p&gt;
&lt;ul data-source-line=&quot;124&quot;&gt;&lt;li&gt;环境记录：存储变量和函数声明的实际位置&lt;/li&gt;
&lt;li&gt;对外部环境的引用：可以访问其外部词法环境&lt;/li&gt;
&lt;/ul&gt;&lt;p data-source-line=&quot;127&quot;&gt;词法环境类型伪代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
// 第一种类型： 全局环境
GlobalExectionContext = {  // 全局执行上下文
  LexicalEnvironment: {           // 词法环境
    EnvironmentRecord: {                // 环境记录
      Type: &quot;Object&quot;,                      // 全局环境
      // 标识符绑定在这里 
      outer: &amp;lt;null&amp;gt;                          // 对外部环境的引用
  }  
}

// 第二种类型： 函数环境
FunctionExectionContext = { // 函数执行上下文
  LexicalEnvironment: {           // 词法环境
    EnvironmentRecord: {                // 环境记录
      Type: &quot;Declarative&quot;,         // 函数环境
      // 标识符绑定在这里                         // 对外部环境的引用
      outer: &amp;lt;Global or outer function environment reference&amp;gt;  
  }  
}
&lt;/pre&gt;&lt;/div&gt;

&lt;p data-source-line=&quot;151&quot;&gt;在创建变量对象，也就是创建变量环境，而变量环境也是一个词法环境。在 ES6 中，词法&lt;span class=&quot;Apple-converted-space&quot;&gt; 环境和&lt;span class=&quot;Apple-converted-space&quot;&gt; 变量&lt;span class=&quot;Apple-converted-space&quot;&gt; 环境的区别在于前者用于存储函数声明和变量（&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;let&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 和&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;const&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; ）绑定，而后者仅用于存储变量（&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;var&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; ）绑定。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;153&quot;&gt;如例子：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
let a = 20;  
const b = 30;  
var c;

function multiply(e, f) {  
 var g = 20;  
 return e * f * g;  
}

c = multiply(20, 30);
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;155&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt; &lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;167&quot;&gt;执行上下文如下所示&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;51&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
GlobalExectionContext = {

  ThisBinding: &amp;lt;Global Object&amp;gt;,

  LexicalEnvironment: {  
    EnvironmentRecord: {  
      Type: &quot;Object&quot;,  
      // 标识符绑定在这里  
      a: &amp;lt; uninitialized &amp;gt;,  
      b: &amp;lt; uninitialized &amp;gt;,  
      multiply: &amp;lt; func &amp;gt;  
    }  
    outer: &amp;lt;null&amp;gt;  
  },

  VariableEnvironment: {  
    EnvironmentRecord: {  
      Type: &quot;Object&quot;,  
      // 标识符绑定在这里  
      c: undefined,  
    }  
    outer: &amp;lt;null&amp;gt;  
  }  
}

FunctionExectionContext = {  
   
  ThisBinding: &amp;lt;Global Object&amp;gt;,

  LexicalEnvironment: {  
    EnvironmentRecord: {  
      Type: &quot;Declarative&quot;,  
      // 标识符绑定在这里  
      Arguments: {0: 20, 1: 30, length: 2},  
    },  
    outer: &amp;lt;GlobalLexicalEnvironment&amp;gt;  
  },

  VariableEnvironment: {  
    EnvironmentRecord: {  
      Type: &quot;Declarative&quot;,  
      // 标识符绑定在这里  
      g: undefined  
    },  
    outer: &amp;lt;GlobalLexicalEnvironment&amp;gt;  
  }  
}
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;169&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt; &lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;219&quot;&gt;变量提升的具体原因：在创建阶段，函数声明存储在环境中，而变量会被设置为&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;undefined&lt;/code&gt;（在&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;var&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 的情况下）或保持未初始化（在&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;let&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 和&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;const&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 的情况下）。所以这就是为什么可以在声明之前访问&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;var&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 定义的变量（尽管是&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;undefined&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; ），但如果在声明之前访问&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;let&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 和&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;code&gt;const&lt;/code&gt;&lt;span class=&quot;Apple-converted-space&quot;&gt; 定义的变量就会提示引用错误的原因。此时let 和 const处于未初始化状态不能使用，只有进入执行阶段，变量对象中的变量属性进行赋值后，变量对象（Variable Object）转为活动对象（Active Object）后，&lt;code&gt;let&lt;/code&gt;和&lt;code&gt;const&lt;/code&gt;才能进行访问。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;221&quot;&gt;关于函数声明和变量声明，这篇文章讲的很好：&lt;a href=&quot;https://github.com/yygmind/blog/issues/13&quot;&gt;https://github.com/yygmind/blog/issues/13&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;224&quot;&gt;另外关于闭包的理解，如例子：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
function foo() {
    var num = 20;

    function bar() {
        var result = num + 20;

        return result
    }

    bar()
}

foo()
&lt;/pre&gt;&lt;/div&gt;

&lt;pre data-source-line=&quot;225&quot;&gt;
&lt;code class=&quot;hljs&quot;&gt; &lt;/code&gt;
&lt;/pre&gt;
&lt;p data-source-line=&quot;240&quot;&gt;浏览器分析如下：&lt;/p&gt;
&lt;p data-source-line=&quot;242&quot;&gt;&lt;img src=&quot;https://heyingye.github.io/2018/03/19/js%E5%BC%95%E6%93%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89/img/close.jpg&quot; alt=&quot;闭包&quot;/&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;245&quot;&gt;chrome浏览器理解闭包是foo，那么按浏览器的标准是如何定义闭包的，总结为三点：&lt;/p&gt;
&lt;ul data-source-line=&quot;247&quot; readability=&quot;0&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;在函数内部定义新函数&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;新函数访问外层函数的局部变量，即访问外层函数环境的活动对象属性&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;新函数执行，创建新的函数执行上下文，外层函数即为闭包&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;33-this指向&quot; data-source-line=&quot;257&quot;&gt;3.3 this指向&lt;/h4&gt;
&lt;p data-source-line=&quot;259&quot;&gt;比较复杂，后面专门弄一篇文章来整理。&lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt; &lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt; &lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt;第三个阶段请看  &lt;a href=&quot;https://www.cnblogs.com/BoatGina/p/10433551.html&quot; target=&quot;_blank&quot;&gt;JS引擎线程的执行过程的三个阶段（二）&lt;br/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt; &lt;/p&gt;
&lt;p data-source-line=&quot;361&quot;&gt;文章参考：&lt;/p&gt;
&lt;p data-source-line=&quot;363&quot;&gt;&lt;a href=&quot;https://github.com/yygmind/blog/issues/12&quot;&gt;https://github.com/yygmind/blog/issues/12&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;365&quot;&gt;&lt;a href=&quot;https://heyingye.github.io/2018/03/19/js%E5%BC%95%E6%93%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89&quot;&gt;https://heyingye.github.io/2018/03/19/js引擎的执行过程（一）&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;367&quot;&gt;&lt;a href=&quot;https://heyingye.github.io/2018/03/26/js%E5%BC%95%E6%93%8E%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89&quot;&gt;https://heyingye.github.io/2018/03/26/js引擎的执行过程（二）&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;370&quot;&gt;&lt;a href=&quot;https://github.com/yygmind/blog&quot;&gt;https://github.com/yygmind/blog&lt;/a&gt;&lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt; &lt;/p&gt;
&lt;p data-source-line=&quot;259&quot;&gt; &lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 13:16:00 +0000</pubDate>
<dc:creator>奋斗的小舟</dc:creator>
<og:description>浏览器首先按顺序加载由&lt;script&gt;标签分割的js代码块，加载js代码块完毕后，立刻进入以下三个阶段，然后再按顺序查找下一个代码块，再继续执行以下三个阶段，无论是外部脚本文件（不异步加载</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/BoatGina/p/10433518.html</dc:identifier>
</item>
</channel>
</rss>