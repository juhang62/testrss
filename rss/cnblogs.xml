<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>权限管理系统之项目框架搭建并集成日志、mybatis和分页 - 社会主义接班人</title>
<link>http://www.cnblogs.com/5ishare/p/10434943.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/5ishare/p/10434943.html</guid>
<description>&lt;p&gt;前一篇博客中使用LayUI实现了列表页面和编辑页面的显示交互，但列表页面table渲染的数据是固定数据，本篇博客主要是将固定数据变成数据库数据。&lt;/p&gt;
&lt;p&gt;一、项目框架&lt;/p&gt;
&lt;p&gt;首先要解决的是项目框架问题，搭建什么样的框架比较合适，优缺点是什么，扩展性、可读性等方面都要考虑，本项目的框架也是百度参考借鉴网友的，不管是Java还是C#的项目思想都差不多也都是可以互相借鉴的，下图是项目结构图，可能后面还会根据需要再进一步的修改完善。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/733213/201902/733213-20190226010110683-766822177.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;上面目录结构中主要包含8个包，下面对这几个包进行简单介绍。&lt;/p&gt;
&lt;p&gt;com.example：存在main函数类&lt;/p&gt;
&lt;p&gt;com.example.config：配置类包，例如druid多数据源配置类&lt;/p&gt;
&lt;p&gt;com.example.controller：存放controller&lt;/p&gt;
&lt;p&gt;com.example.dao：与数据库交互层，存放mapper接口&lt;/p&gt;
&lt;p&gt;com.example.entity：实体层，这个与com.example.pojo有点类似，不过两个还是有区别的，pojo层主要是与数据库单个数据表对应，entity可能是其他对象的抽象，比如多个表联合查询组成的行对应的类、或者前端页面显示对应的类、一对多、多对多关系。&lt;/p&gt;
&lt;p&gt;com.example.pojo：数据库表的对应类&lt;/p&gt;
&lt;p&gt;com.example.service：业务服务接口层，定义服务接口，优势是什么呢，这样在Controller中注入的是service接口，是面向接口的编程，如果服务的具体实现改变了也不影响其他注入类。&lt;/p&gt;
&lt;p&gt;com.example.service.impl：实现服务接口，业务逻辑的具体实现&lt;/p&gt;
&lt;p&gt;com.example.utils：基础类、工具类层&lt;/p&gt;
&lt;p&gt;二、集成日志&lt;/p&gt;
&lt;p&gt;项目使用的log4j2日志框架，由于SpringBoot自带的有日志框架，所以需要先排除掉，然后在引入log4j2日志。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;org.springframework.boot&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;spring-boot-starter-web&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;exclusions&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
                &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;exclusion&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
                    &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;spring-boot-starter-logging&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
                    &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;org.springframework.boot&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
                &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;exclusion&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;exclusions&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
               &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;org.springframework.boot&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;spring-boot-starter-log4j2&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以在application.properties中设置log4j2的相关配置或者创建log4j2.xml放在application.properties同目录下。这里在创建的log4j2.xml中设置日志存放位置在D:\log\logs，在该目录下可以看下日志文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/733213/201902/733213-20190226015725733-1175143953.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;二、集成mybatis&lt;/p&gt;
&lt;p&gt;集成mybatis主要是引入两个依赖，一个是mysql的，一个是mybatis的。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('84a1bd45-fa7e-4a3d-ae60-7862306a341a')&quot; readability=&quot;32.5&quot;&gt;&lt;img id=&quot;code_img_closed_84a1bd45-fa7e-4a3d-ae60-7862306a341a&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_84a1bd45-fa7e-4a3d-ae60-7862306a341a&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('84a1bd45-fa7e-4a3d-ae60-7862306a341a',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_84a1bd45-fa7e-4a3d-ae60-7862306a341a&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;60&quot;&gt;
&lt;pre&gt;
        &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; https://mvnrepository.com/artifact/org.mybatis.spring.boot/mybatis-spring-boot-starter &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;org.mybatis.spring.boot&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;mybatis-spring-boot-starter&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;1.3.2&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; https://mvnrepository.com/artifact/mysql/mysql-connector-java &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;mysql&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;mysql-connector-java&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;8.0.11&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;项目是使用的xml方式管理sql，所以在该项目的resource目录下创建了mybatis目录，子目录mapper存放sql映射文件，mybatis-config.xml放mybatis的配置，同时还需要在application.properties设置数据库信息、mybatis文件目录信息。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
#mysql
spring.datasource.driverClassName = com.mysql.cj.jdbc.Driver
#spring.datasource.url = jdbc:mysql://localhost:3306/mybatis
spring.datasource.url =jdbc:mysql://127.0.0.1:3306/mybatis?useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;serverTimezone=UTC
spring.datasource.username = root
spring.datasource.password = 123456

#mybatis
mybatis.type-aliases-package=com.example.model
mybatis.config-location=classpath:mybatis/mybatis-config.xml
mybatis.mapper-locations=classpath:mybatis/mapper/*.xml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;三、集成分页pagehelper&lt;/p&gt;
&lt;p&gt;集成pagehelper这里有两种方式，一是spring集成方式，二是SpringBoot集成方式，可参考：https://www.cnblogs.com/1315925303zxz/p/7364552.html 。这里使用的是第一种，另外在这里遇到了一个问题，就是分页操作之后能查出数据但PageInfo的total值一直是0，原来是忘记在mybatis-config.xml配置pagehelper插件并要设置rowBoundsWithCount=true。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;com.github.pagehelper&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;groupId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;pagehelper&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;artifactId&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
            &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;3.4.2&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;version&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;dependency&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('530754df-9d7e-49f4-9018-29204655975b')&quot; readability=&quot;32.5&quot;&gt;&lt;img id=&quot;code_img_closed_530754df-9d7e-49f4-9018-29204655975b&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_530754df-9d7e-49f4-9018-29204655975b&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('530754df-9d7e-49f4-9018-29204655975b',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_530754df-9d7e-49f4-9018-29204655975b&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;60&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&amp;lt;?&lt;/span&gt;&lt;span&gt;xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; &lt;/span&gt;&lt;span&gt;?&amp;gt;&lt;/span&gt;
&lt;span&gt;&amp;lt;!&lt;/span&gt;&lt;span&gt;DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;configuration&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
    &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAliases&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;Integer&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.lang.Integer&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;Long&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.lang.Long&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;HashMap&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.util.HashMap&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;LinkedHashMap&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.util.LinkedHashMap&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;ArrayList&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.util.ArrayList&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;typeAlias &lt;/span&gt;&lt;span&gt;alias&lt;/span&gt;&lt;span&gt;=&quot;LinkedList&quot;&lt;/span&gt;&lt;span&gt; type&lt;/span&gt;&lt;span&gt;=&quot;java.util.LinkedList&quot;&lt;/span&gt; &lt;span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;typeAliases&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;plugins&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
    &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; com.github.pagehelper为PageHelper类所在包名 &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;  
    &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;plugin &lt;/span&gt;&lt;span&gt;interceptor&lt;/span&gt;&lt;span&gt;=&quot;com.github.pagehelper.PageHelper&quot;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; 方言 &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;property &lt;/span&gt;&lt;span&gt;name&lt;/span&gt;&lt;span&gt;=&quot;dialect&quot;&lt;/span&gt;&lt;span&gt; value&lt;/span&gt;&lt;span&gt;=&quot;mysql&quot;&lt;/span&gt;&lt;span&gt;/&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; 该参数默认为false &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;!--&lt;/span&gt;&lt;span&gt; 设置为true时，使用RowBounds分页会进行count查询 &lt;/span&gt;&lt;span&gt;--&amp;gt;&lt;/span&gt;  
        &lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;property &lt;/span&gt;&lt;span&gt;name&lt;/span&gt;&lt;span&gt;=&quot;rowBoundsWithCount&quot;&lt;/span&gt;&lt;span&gt; value&lt;/span&gt;&lt;span&gt;=&quot;true&quot;&lt;/span&gt;&lt;span&gt;/&amp;gt;&lt;/span&gt;  
    &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;plugin&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;  
    &lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;plugins&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt; 
&lt;span&gt;&amp;lt;/&lt;/span&gt;&lt;span&gt;configuration&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;四、数据分页实现&lt;/p&gt;
&lt;p&gt;1.首先数据库准备数据&lt;/p&gt;
&lt;p&gt;这里在数据表中插入了12条数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/733213/201902/733213-20190226015630015-1925889059.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;2.分页&lt;/p&gt;
&lt;p&gt;layui的table填充的数据有4个字段，code、msg、count、data，所以在PageDataResult中定义也定义了4个属性。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('0f825cd4-b023-496f-8ce7-5273367125f6')&quot; readability=&quot;34&quot;&gt;&lt;img id=&quot;code_img_closed_0f825cd4-b023-496f-8ce7-5273367125f6&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_0f825cd4-b023-496f-8ce7-5273367125f6&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('0f825cd4-b023-496f-8ce7-5273367125f6',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_0f825cd4-b023-496f-8ce7-5273367125f6&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;63&quot;&gt;
&lt;pre&gt;
&lt;span&gt;package&lt;/span&gt;&lt;span&gt; com.example.utils;

&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.util.List;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; PageDataResult {
        &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;总记录数量&lt;/span&gt;
        &lt;span&gt;private&lt;/span&gt;&lt;span&gt; Integer totals;
        &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;当前页数据列表&lt;/span&gt;
        &lt;span&gt;private&lt;/span&gt; List&amp;lt;?&amp;gt;&lt;span&gt; list;

        &lt;/span&gt;&lt;span&gt;private&lt;/span&gt; Integer code=200&lt;span&gt;;
        
        &lt;/span&gt;&lt;span&gt;private&lt;/span&gt; String msg=&quot;&quot;&lt;span&gt;;

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; String getMsg() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; msg;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; setMsg(String msg) {
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.msg =&lt;span&gt; msg;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; PageDataResult() {
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; PageDataResult( Integer totals,
                List&lt;/span&gt;&amp;lt;?&amp;gt;&lt;span&gt; list) {
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.totals =&lt;span&gt; totals;
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.list =&lt;span&gt; list;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; Integer getTotals() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; totals;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; setTotals(Integer totals) {
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.totals =&lt;span&gt; totals;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; List&amp;lt;?&amp;gt;&lt;span&gt; getList() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; list;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; setList(List&amp;lt;?&amp;gt;&lt;span&gt; list) {
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.list =&lt;span&gt; list;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; Integer getCode() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; code;
        }

        &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; setCode(Integer code) {
            &lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.code =&lt;span&gt; code;
        }

        @Override &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; String toString() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; &quot;PageDataResult{&quot; + &quot;totals=&quot; + totals + &quot;, list=&quot; +&lt;span&gt; list
                    &lt;/span&gt;+ &quot;, code=&quot; + code + '}'&lt;span&gt;;
        }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;3.分页实现&lt;/p&gt;
&lt;p&gt;主要实现方法也比较简单，使用pagehelper进行分页，然后将结果设置到PageDataResult。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('97564a22-98c8-4c06-9777-7835b2c3c3ad')&quot; readability=&quot;33.5&quot;&gt;&lt;img id=&quot;code_img_closed_97564a22-98c8-4c06-9777-7835b2c3c3ad&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_97564a22-98c8-4c06-9777-7835b2c3c3ad&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('97564a22-98c8-4c06-9777-7835b2c3c3ad',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_97564a22-98c8-4c06-9777-7835b2c3c3ad&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;62&quot;&gt;
&lt;pre&gt;
&lt;span&gt;    @Override
    &lt;/span&gt;&lt;span&gt;public&lt;/span&gt;&lt;span&gt; PageDataResult getUsers(UserSearchDTO userSearch) {

                PageDataResult pdr &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt;&lt;span&gt; PageDataResult();
                PageHelper.startPage(userSearch.getPage(), userSearch.getLimit(),&lt;/span&gt;&lt;span&gt;true&lt;/span&gt;&lt;span&gt;);
                List&lt;/span&gt;&amp;lt;User&amp;gt; urList =&lt;span&gt; userMapper.getUsers(userSearch);
                logger.debug(&lt;/span&gt;&quot;urList:&quot;+&lt;span&gt;urList.size());    
                &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获取分页查询后的数据&lt;/span&gt;
                PageInfo&amp;lt;User&amp;gt; pageInfo = &lt;span&gt;new&lt;/span&gt; PageInfo&amp;lt;&amp;gt;&lt;span&gt;(urList);
                &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 设置获取到的总记录数total：&lt;/span&gt;
                logger.debug(&quot;page:&quot;+userSearch.getPage()+&quot;limit:&quot;+userSearch.getLimit()+&quot;总行数:&quot;+&lt;span&gt;pageInfo.getTotal());
                pdr.setTotals(Long.valueOf(pageInfo.getTotal()).intValue());
                pdr.setList(urList);
                &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; pdr;
                
    }&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;4.替换固定数据&lt;/p&gt;
&lt;p&gt;上一博客是将列表数据写成固定的，这里进行了替换，分页之后返回PageDataResult。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('57220991-c4b7-460a-a176-8b5e093f0972')&quot; readability=&quot;37&quot;&gt;&lt;img id=&quot;code_img_closed_57220991-c4b7-460a-a176-8b5e093f0972&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_57220991-c4b7-460a-a176-8b5e093f0972&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('57220991-c4b7-460a-a176-8b5e093f0972',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_57220991-c4b7-460a-a176-8b5e093f0972&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;69&quot;&gt;
&lt;pre&gt;
    @RequestMapping(value = &quot;/getUsers&quot;, method =&lt;span&gt; RequestMethod.GET)
    @ResponseBody
    &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; PageDataResult getUsers(@RequestParam(&quot;page&quot;&lt;span&gt;) Integer page,
            @RequestParam(&lt;/span&gt;&quot;limit&quot;) Integer limit,@RequestParam(value=&quot;keyword&quot;,required=&lt;span&gt;false&lt;/span&gt;&lt;span&gt;) String keyword) {
        logger.debug(&lt;/span&gt;&quot;分页查询用户列表！,查询条件keyword:&quot;+keyword+&quot;page:&quot; + page+&quot;,每页记录数量limit:&quot; +&lt;span&gt; limit);
        
        PageDataResult pdr &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt;&lt;span&gt; PageDataResult();
        &lt;/span&gt;&lt;span&gt;try&lt;/span&gt;&lt;span&gt; {
            &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (&lt;span&gt;null&lt;/span&gt; ==&lt;span&gt; page) {
                page &lt;/span&gt;= 1&lt;span&gt;;
            }
            &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (&lt;span&gt;null&lt;/span&gt; ==&lt;span&gt; limit) {
                limit &lt;/span&gt;= 10&lt;span&gt;;
            }
            UserSearchDTO userSearch&lt;/span&gt;=&lt;span&gt;new&lt;/span&gt;&lt;span&gt; UserSearchDTO(page,limit,keyword);
            &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获取用户和角色列表&lt;/span&gt;
            pdr =&lt;span&gt; userService.getUsers(userSearch);
            logger.debug(&lt;/span&gt;&quot;用户列表查询=pdr:&quot; +&lt;span&gt; pdr);
            logger.debug(&lt;/span&gt;&quot;用户列表查询数量:&quot; +&lt;span&gt; pdr.getList().size());
        } &lt;/span&gt;&lt;span&gt;catch&lt;/span&gt;&lt;span&gt; (Exception e) {
            e.printStackTrace();
            logger.error(&lt;/span&gt;&quot;用户列表查询异常！&quot;&lt;span&gt;, e);
        }
        &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; pdr;
    }&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/733213/201902/733213-20190226021735994-1229985599.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;五、优化&lt;/p&gt;
&lt;p&gt;由于从数据库返回的user表的sex数据是0、1，所以需要转成男、女，使用了table列的templet，同时id列通过hide=true进行了隐藏，又由于PageDataResult返回结果的key的名字与table所要求的不一致，又使用了response进行映射。下面代码是目前最新的user.js代码，后续可能还会更新。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('4da196b2-e339-4753-a975-5182b62042c0')&quot; readability=&quot;58.5&quot;&gt;&lt;img id=&quot;code_img_closed_4da196b2-e339-4753-a975-5182b62042c0&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_4da196b2-e339-4753-a975-5182b62042c0&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('4da196b2-e339-4753-a975-5182b62042c0',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_4da196b2-e339-4753-a975-5182b62042c0&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;112&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt;&lt;span&gt; table;
&lt;/span&gt;&lt;span&gt;var&lt;/span&gt;&lt;span&gt; layer;
layui.use([ &lt;/span&gt;'layer', 'table', 'element' ], &lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {
    table &lt;/span&gt;=&lt;span&gt; layui.table;
    layer &lt;/span&gt;=&lt;span&gt; layui.layer;
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 执行一个 table 实例&lt;/span&gt;
&lt;span&gt;    table.render({
        elem : &lt;/span&gt;'#user'&lt;span&gt;,
        height:&lt;/span&gt;350&lt;span&gt;,
        url : &lt;/span&gt;'/user/getUsers'&lt;span&gt;,
        method: &lt;/span&gt;'get', &lt;span&gt;//&lt;/span&gt;&lt;span&gt;默认：get请求&lt;/span&gt;
        page :&lt;span&gt;true&lt;/span&gt;, &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 开启分页&lt;/span&gt;
&lt;span&gt;        request: {
            pageName: &lt;/span&gt;'page' &lt;span&gt;//&lt;/span&gt;&lt;span&gt;页码的参数名称，默认：page&lt;/span&gt;
            ,limitName: 'limit' &lt;span&gt;//&lt;/span&gt;&lt;span&gt;每页数据量的参数名，默认：limit&lt;/span&gt;
&lt;span&gt;        },response:{
            statusName: &lt;/span&gt;'code' &lt;span&gt;//&lt;/span&gt;&lt;span&gt;数据状态的字段名称，默认：code&lt;/span&gt;
            ,statusCode: 200 &lt;span&gt;//&lt;/span&gt;&lt;span&gt;成功的状态码，默认：0&lt;/span&gt;
            ,countName: 'totals' &lt;span&gt;//&lt;/span&gt;&lt;span&gt;数据总数的字段名称，默认：count&lt;/span&gt;
            ,dataName: 'list' &lt;span&gt;//&lt;/span&gt;&lt;span&gt;数据列表的字段名称，默认：data         &lt;/span&gt;
&lt;span&gt;        },
        cols : [ [ &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 表头&lt;/span&gt;
&lt;span&gt;            {
                fixed : &lt;/span&gt;'left'&lt;span&gt;,
                type : &lt;/span&gt;'checkbox'&lt;span&gt;
            }, {
                field : &lt;/span&gt;'id'&lt;span&gt;,
                title : &lt;/span&gt;'ID'&lt;span&gt;,
                width : &lt;/span&gt;80&lt;span&gt;,
                fixed : &lt;/span&gt;'left'&lt;span&gt;,
                hide:&lt;/span&gt;&lt;span&gt;true&lt;/span&gt;&lt;span&gt;
            }, {
                field : &lt;/span&gt;'name'&lt;span&gt;,
                title : &lt;/span&gt;'姓名'&lt;span&gt;,
                width : &lt;/span&gt;80&lt;span&gt;
            },
            {
                field : &lt;/span&gt;'age'&lt;span&gt;,
                title : &lt;/span&gt;'年龄'&lt;span&gt;,
                width : &lt;/span&gt;80&lt;span&gt;
            },
            {
                field : &lt;/span&gt;'sex'&lt;span&gt;,
                title : &lt;/span&gt;'性别'&lt;span&gt;,
                width : &lt;/span&gt;80&lt;span&gt;,
                templet : &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(d) {
                    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (d.sex == 1&lt;span&gt;) {
                        &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; '男'&lt;span&gt;;
                    } &lt;/span&gt;&lt;span&gt;else&lt;/span&gt; &lt;span&gt;if&lt;/span&gt; (d.sex == 0&lt;span&gt;) {
                        &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; '女'&lt;span&gt;;
                    }
                }
            },{
                title : &lt;/span&gt;'操作'&lt;span&gt;,
                width : &lt;/span&gt;200&lt;span&gt;,
                align : &lt;/span&gt;'center'&lt;span&gt;,
                toolbar : &lt;/span&gt;'#tools'&lt;span&gt;
            } ] ]

    });

&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 监听工具条&lt;/span&gt;
    table.on('tool(tools)', &lt;span&gt;function&lt;/span&gt;(obj) { &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 注：tool是工具条事件名，test是table原始容器的属性&lt;/span&gt;
        &lt;span&gt;var&lt;/span&gt; data = obj.data &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获得当前行数据&lt;/span&gt;
            , layEvent = obj.event; &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获得 lay-event 对应的值&lt;/span&gt;
        &lt;span&gt;if&lt;/span&gt; ('edit' ==&lt;span&gt; layEvent) {
            addUser(data.id)
        } &lt;/span&gt;&lt;span&gt;else&lt;/span&gt; &lt;span&gt;if&lt;/span&gt; ('del' ==&lt;span&gt; layEvent) {
            del(data.id);
        }
    });
});

&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt; queryUser(){
    &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; keyword = $(&quot;#keyword&quot;&lt;span&gt;).val();
    table.reload(&lt;/span&gt;'user'&lt;span&gt;, {
        where : {
            keyword : keyword
        },
        page : {
            curr : &lt;/span&gt;1&lt;span&gt;
        }
    });
    }

&lt;/span&gt;&lt;span&gt;var&lt;/span&gt;&lt;span&gt; index;
&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt; addUser(id) {
    index &lt;/span&gt;=&lt;span&gt; parent.layer.open({
        type : &lt;/span&gt;2&lt;span&gt;,
        title : &lt;/span&gt;&quot;用户信息&quot;&lt;span&gt;,
        area: [&lt;/span&gt;'550px', '400px'&lt;span&gt;],
        content : &lt;/span&gt;'/user/edit?id=' +&lt;span&gt; id
    });
    layer.full(index);
}

&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt; del(id) {
    parent.layer.open({
            type : &lt;/span&gt;1&lt;span&gt;,
            content : &lt;/span&gt;'&amp;lt;div style=&quot;padding: 20px 80px;&quot;&amp;gt;确定删除记录?&amp;lt;/div&amp;gt;'&lt;span&gt;,
            btn : [ &lt;/span&gt;'确定', '取消'&lt;span&gt; ],
            yes : &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(index, layero) {
                $.ajax({
                    url : &lt;/span&gt;&quot;/user/delete&quot;&lt;span&gt;,
                    data : {
                        &lt;/span&gt;&quot;id&quot;&lt;span&gt; : id
                    },
                    dataType : &lt;/span&gt;&quot;text&quot;&lt;span&gt;,
                    success : &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(data) {
                        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;(data==0&lt;span&gt;){
                            layer.msg(&lt;/span&gt;&quot;删除成功！&quot;&lt;span&gt;);
                            layer.close(index);
                            queryUser();
                        }&lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;{
                            layer.msg(&lt;/span&gt;&quot;删除失败！&quot;&lt;span&gt;);
                        }
                    },
                    error : &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {
                    }
                });
            }
        });

}

&lt;/span&gt;&lt;span&gt;/*&lt;/span&gt;&lt;span&gt;*
 * 获取选中数据
 &lt;/span&gt;&lt;span&gt;*/&lt;/span&gt;
&lt;span&gt;function&lt;/span&gt;&lt;span&gt; getDatas(){
    &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; checkStatus = table.checkStatus('user'&lt;span&gt;);
    &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; data =&lt;span&gt; checkStatus.data;
    &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; id = &quot;&quot;&lt;span&gt;;
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt;(&lt;span&gt;var&lt;/span&gt; i=0;i&amp;lt;data.length;i++&lt;span&gt;){
        id &lt;/span&gt;+=&lt;span&gt; data[i].id;
        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;(i&amp;lt;data.length-1&lt;span&gt;){
            id &lt;/span&gt;+= &quot;,&quot;&lt;span&gt;;
        }
    }
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;(data.length != 0&lt;span&gt;){
        alert(id);
&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;        del(id);&lt;/span&gt;
&lt;span&gt;    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;六.小结&lt;/p&gt;
&lt;p&gt;目前把java部分的项目框架搭建了起来，也集成了日志、mybatis、分页插件pagehelper，实现了table页面动态数据分页显示，后续就是将新增、编辑、删除几个功能实现，同时还要注意删除功能，删除用户后可能还要删除用户与角色关联表，会涉及到事务操作，后续也会加进来。&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 18:19:00 +0000</pubDate>
<dc:creator>社会主义接班人</dc:creator>
<og:description>前一篇博客中使用LayUI实现了列表页面和编辑页面的显示交互，但列表页面table渲染的数据是固定数据，本篇博客主要是将固定数据变成数据库数据。 一、项目框架 首先要解决的是项目框架问题，搭建什么样的</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/5ishare/p/10434943.html</dc:identifier>
</item>
<item>
<title>Springboot 系列（八）动态Banner与图片转字符图案的手动实现 - 雪漫士兵</title>
<link>http://www.cnblogs.com/niumoo/p/10434746.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/niumoo/p/10434746.html</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/2626549-9ec05ad9746acf37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Springboot 启动 banner&quot;/&gt;&lt;/p&gt;
&lt;p&gt;使用过 Springboot 的对上面这个图案肯定不会陌生，Springboot 启动的同时会打印上面的图案，并带有版本号。查看官方文档可以找到关于 banner 的描述&lt;/p&gt;
&lt;blockquote readability=&quot;12&quot;&gt;
&lt;p&gt;The banner that is printed on start up can be changed by adding a banner.txt file to your classpath or by setting the spring.banner.location property to the location of such a file. If the file has an encoding other than UTF-8, you can set spring.banner.charset. In addition to a text file, you can also add a banner.gif, banner.jpg, or banner.png image file to your classpath or set the spring.banner.image.location property. Images are converted into an ASCII art representation and printed above any text banner.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;就不翻译了，直接有道翻译贴过来看个大概意思。&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;可以通过向类路径中添加一个banner.txt文件或设置spring.banner来更改在start up上打印的banner。属性指向此类文件的位置。如果文件的编码不是UTF-8，那么可以设置spring.banner.charset。除了文本文件，还可以添加横幅。将gif、banner.jpg或banner.png图像文件保存到类路径或设置spring.banner.image。位置属性。图像被转换成ASCII艺术形式，并打印在任何文本横幅上面。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;根据官方的描述，可以在类路径中自定义 banner 图案，我们进行尝试在放 resouce 目录下新建文件 banner.txt 并写入内容（&lt;a href=&quot;http://patorjk.com/software/taag/#p=testall&amp;amp;f=Graffiti&amp;amp;t=niumoo&quot;&gt;在线字符生成&lt;/a&gt;）。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;     (_)
  _ __  _ _   _ _ __ ___   ___   ___
 | '_ \| | | | | '_ ` _ \ / _ \ / _ \
 | | | | | |_| | | | | | | (_) | (_) |
 |_| |_|_|\__,_|_| |_| |_|\___/ \___/ 版本：${spring-boot.formatted-version}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;启动 Springboot 在控制台看到下面的输出。&lt;/p&gt;
&lt;pre class=&quot;log&quot;&gt;
&lt;code&gt;     (_)
  _ __  _ _   _ _ __ ___   ___   ___
 | '_ \| | | | | '_ ` _ \ / _ \ / _ \
 | | | | | |_| | | | | | | (_) | (_) |
 |_| |_|_|\__,_|_| |_| |_|\___/ \___/ 版本：(v2.1.3.RELEASE)
2019-02-25 14:00:31.289  INFO 12312 --- [           main] net.codingme.banner.BannerApplication    : Starting BannerApplication on LAPTOP-L1S5MKTA with PID 12312 (D:\IdeaProjectMy\springboot-git\springboot-banner\target\classes started by Niu in D:\IdeaProjectMy\springboot-git\springboot-banner)
2019-02-25 14:00:31.291  INFO 12312 --- [           main] net.codingme.banner.BannerApplication    : No active profile set, falling back to default profiles: default
2019-02-25 14:00:32.087  INFO 12312 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;发现自定义 banner 已经生效了，官方文档的介绍里说还可以放置图片，下面放置图片 banner.jpg 测试。&lt;br/&gt;网上随便找了一个图片。&lt;br/&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26371673/53317728-c7bb3580-3907-11e9-86b4-35143b81d33a.jpg&quot; alt=&quot;Google Log&quot;/&gt;再次启动观察输出。&lt;br/&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26371673/53317827-2e405380-3908-11e9-83f3-a40e902633d8.png&quot; alt=&quot;自定义 Banner&quot;/&gt;Springboot 把图案转成了 ASCII 图案。&lt;/p&gt;

&lt;p&gt;看了上面的例子，发现 Springboot 可以把图片转换成 ASCII 图案，那么它是怎么做的呢？我们或许可以想象出一个大概流程。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;获取图片。&lt;/li&gt;
&lt;li&gt;遍历图片像素点。&lt;/li&gt;
&lt;li&gt;分析像素点，每个像素点根据颜色深度得出一个值，根据明暗度匹配不同的字符。&lt;/li&gt;
&lt;li&gt;输出图案。&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Springboot 对图片 banner 的处理到底是不是我们上面想想的那样呢？直接去源码中寻找答案。&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;/** 位置：org.springframework.boot.SpringApplicationBannerPrinter */
//方法1：
public Banner print(Environment environment, Class&amp;lt;?&amp;gt; sourceClass, Log logger) {
        // 获取 banner  调用方法记为2
        Banner banner = getBanner(environment);
        try {
            logger.info(createStringFromBanner(banner, environment, sourceClass));
        }
        catch (UnsupportedEncodingException ex) {
            logger.warn(&quot;Failed to create String for banner&quot;, ex);
        }
        // 打印 banner
        return new PrintedBanner(banner, sourceClass);
}
// 方法2
private Banner getBanner(Environment environment) {
        Banners banners = new Banners();
        // 获取图片banner，我们只关注这个，调用方法记为3
        banners.addIfNotNull(getImageBanner(environment));
        banners.addIfNotNull(getTextBanner(environment));
        if (banners.hasAtLeastOneBanner()) {
            return banners;
        }
        if (this.fallbackBanner != null) {
            return this.fallbackBanner;
        }
        return DEFAULT_BANNER;
    }
// 方法3
/** 获取自定义banner文件信息 */
private Banner getImageBanner(Environment environment) {
    // BANNER_IMAGE_LOCATION_PROPERTY = &quot;spring.banner.image.location&quot;;
        String location = environment.getProperty(BANNER_IMAGE_LOCATION_PROPERTY);
        if (StringUtils.hasLength(location)) {
            Resource resource = this.resourceLoader.getResource(location);
            return resource.exists() ? new ImageBanner(resource) : null;
        }
        // IMAGE_EXTENSION = { &quot;gif&quot;, &quot;jpg&quot;, &quot;png&quot; };
        for (String ext : IMAGE_EXTENSION) {
            Resource resource = this.resourceLoader.getResource(&quot;banner.&quot; + ext);
            if (resource.exists()) {
                return new ImageBanner(resource);
            }
        }
        return null;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面是寻找自定义图片 banner 文件源码，如果把图片转换成 ASCII 图案继续跟进，追踪方法1中的&lt;code&gt;PrintedBanner(banner, sourceClass)&lt;/code&gt;方法。最终查找输出图案的主要方法。&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;// 位置：org.springframework.boot.ImageBanner#printBanner
private void printBanner(BufferedImage image, int margin, boolean invert,
            PrintStream out) {
        AnsiElement background = invert ? AnsiBackground.BLACK : AnsiBackground.DEFAULT;
        out.print(AnsiOutput.encode(AnsiColor.DEFAULT));
        out.print(AnsiOutput.encode(background));
        out.println();
        out.println();
        AnsiColor lastColor = AnsiColor.DEFAULT;
        // 图片高度遍历
        for (int y = 0; y &amp;lt; image.getHeight(); y++) {
            for (int i = 0; i &amp;lt; margin; i++) {
                out.print(&quot; &quot;);
            }
            // 图片宽度遍历
            for (int x = 0; x &amp;lt; image.getWidth(); x++) {
                // 获取每一个像素点
                Color color = new Color(image.getRGB(x, y), false);
                AnsiColor ansiColor = AnsiColors.getClosest(color);
                if (ansiColor != lastColor) {
                    out.print(AnsiOutput.encode(ansiColor));
                    lastColor = ansiColor;
                }
                // 像素点转换成字符输出，调用方法记为2
                out.print(getAsciiPixel(color, invert));
            }
            out.println();
        }
        out.print(AnsiOutput.encode(AnsiColor.DEFAULT));
        out.print(AnsiOutput.encode(AnsiBackground.DEFAULT));
        out.println();
    }
// 方法2，像素点转换成字符
    private char getAsciiPixel(Color color, boolean dark) {
        // 根据 color 算出一个亮度值
        double luminance = getLuminance(color, dark);
        for (int i = 0; i &amp;lt; PIXEL.length; i++) {
            // 寻找亮度值匹配的字符
            if (luminance &amp;gt;= (LUMINANCE_START - (i * LUMINANCE_INCREMENT))) {
                // PIXEL = { ' ', '.', '*', ':', 'o', '&amp;amp;', '8', '#', '@' };
                return PIXEL[i];
            }
        }
        return PIXEL[PIXEL.length - 1];
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;通过查看源码，发现 Springboot 的图片 banner 的转换和我们预想的大致一致，这么有趣的功能我们能不能自己写一个呢？&lt;/p&gt;

&lt;p&gt;根据上面的分析，总结一下思路，我们也可以手动写一个图片转 ASCII 字符图案。&lt;br/&gt;思路如下：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;图片大小缩放，调整到合适大小。&lt;/li&gt;
&lt;li&gt;遍历图片像素。&lt;/li&gt;
&lt;li&gt;获取图片像素点亮度（RGB颜色通过公式可以得到亮度数值）。&lt;/li&gt;
&lt;li&gt;匹配字符。&lt;/li&gt;
&lt;li&gt;输出图案。&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;上面的5个步骤直接使用 Java 代码就可以完整实现，下面是编写的源码。&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;import java.awt.*;
import java.awt.image.BufferedImage;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;

import javax.imageio.ImageIO;

/**
 * &amp;lt;p&amp;gt;
 * 根据图片生成字符图案
 * 1.图片大小缩放
 * 2.遍历图片像素点
 * 3.获取图片像素点亮度
 * 4.匹配字符
 * 5.输出图案
 *
 * @author  niujinpeng
 * @website www.codingme.net
 * @date 2019-02-25 23:03:01
 */
public class GeneratorTextImage {
    private static final char[] PIXEL = {'@', '#', '8', '&amp;amp;', 'o', ':', '*', '.', ' '};
    public static void main(String[] args) throws Exception {
        // 图片缩放
        BufferedImage bufferedImage = makeSmallImage(&quot;src/main/resources/banner.jpg&quot;);
        // 输出
        printImage(bufferedImage);
    }

    public static void printImage(BufferedImage image) throws IOException {
        int width = image.getWidth();
        int height = image.getHeight();
        for (int i = 0; i &amp;lt; height; i++) {
            for (int j = 0; j &amp;lt; width; j++) {
                int rgb = image.getRGB(j, i);
                Color color = new Color(rgb);
                int red = color.getRed();
                int green = color.getGreen();
                int blue = color.getBlue();
                // 一个用于计算RGB像素点亮度的公式
                Double luminace = 0.2126 * red + 0.7152 * green + 0.0722 * blue;
                double index = luminace / (Math.ceil(255 / PIXEL.length) + 0.5);
                System.out.print(PIXEL[(int)(Math.floor(index))]);
            }
            System.out.println();
        }
    }

    public static BufferedImage makeSmallImage(String srcImageName) throws Exception {
        File srcImageFile = new File(srcImageName);
        if (srcImageFile == null) {
            System.out.println(&quot;文件不存在&quot;);
            return null;
        }
        FileOutputStream fileOutputStream = null;
        BufferedImage tagImage = null;
        Image srcImage = null;
        try {
            srcImage = ImageIO.read(srcImageFile);
            int srcWidth = srcImage.getWidth(null);// 原图片宽度
            int srcHeight = srcImage.getHeight(null);// 原图片高度
            int dstMaxSize = 90;// 目标缩略图的最大宽度/高度，宽度与高度将按比例缩写
            int dstWidth = srcWidth;// 缩略图宽度
            int dstHeight = srcHeight;// 缩略图高度
            float scale = 0;
            // 计算缩略图的宽和高
            if (srcWidth &amp;gt; dstMaxSize) {
                dstWidth = dstMaxSize;
                scale = (float)srcWidth / (float)dstMaxSize;
                dstHeight = Math.round((float)srcHeight / scale);
            }
            srcHeight = dstHeight;
            if (srcHeight &amp;gt; dstMaxSize) {
                dstHeight = dstMaxSize;
                scale = (float)srcHeight / (float)dstMaxSize;
                dstWidth = Math.round((float)dstWidth / scale);
            }
            // 生成缩略图
            tagImage = new BufferedImage(dstWidth, dstHeight, BufferedImage.TYPE_INT_RGB);
            tagImage.getGraphics().drawImage(srcImage, 0, 0, dstWidth, dstHeight, null);
            return tagImage;
        } finally {
            if (fileOutputStream != null) {
                try {
                    fileOutputStream.close();
                } catch (Exception e) {
                }
                fileOutputStream = null;
            }
            tagImage = null;
            srcImage = null;
            System.gc();
        }
    }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;还是拿上面的 Google log 图片作为实验对象，运行得到字符图案输出。&lt;br/&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/26371673/53331160-2e9e1600-392b-11e9-8417-2d3e4f2a060f.png&quot; alt=&quot;图片转 ASCII 字符&quot;/&gt;&lt;/p&gt;
&lt;p&gt;文章代码已经上传到 GitHub &lt;a href=&quot;https://github.com/niumoo/springboot/tree/master/&quot;&gt;Spring Boot&lt;/a&gt;。&lt;br/&gt;&amp;lt;完&amp;gt;&lt;br/&gt;欢迎点赞关注！&lt;br/&gt;本文原发于个人博客：&lt;a href=&quot;https://www.codingme.net/&quot; class=&quot;uri&quot;&gt;https://www.codingme.net&lt;/a&gt; 转载请注明出处&lt;br/&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/2626549-e1b06fdf782b78e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/600&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 16:27:00 +0000</pubDate>
<dc:creator>雪漫士兵</dc:creator>
<og:description>使用过 Springboot 的对上面这个图案肯定不会陌生，Springboot 启动的同时会打印上面的图案，并带有版本号。查看官方文档可以找到关于 banner 的描述 The banner tha</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/niumoo/p/10434746.html</dc:identifier>
</item>
<item>
<title>深度学习之卷积神经网络(CNN)详解 - w_x_w1985</title>
<link>http://www.cnblogs.com/further-further-further/p/10430073.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/further-further-further/p/10430073.html</guid>
<description>&lt;p&gt;&lt;span&gt;                                           本文系作者原创，转载请注明出处:&lt;a id=&quot;Editor_Edit_hlEntryLink&quot; title=&quot;view: 深度学习之卷积神经网络(CNN)详解&quot; href=&quot;https://www.cnblogs.com/further-further-further/p/10430073.html&quot; target=&quot;_blank&quot;&gt;https://www.cnblogs.com/further-further-further/p/10430073.html&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;目录&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;1.应用场景&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;2.卷积神经网络结构&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt; 2.1 卷积（convelution）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.2 Relu激活函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.3 池化（pool）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.4 全连接（full connection）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.5 损失函数（softmax_loss）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.6 前向传播（forward propagation）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.7 反向传播（backford propagation）&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;3.代码实现流程图以及介绍&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;4.代码实现（python3.6）&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;5.运行结果以及分析&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;6.参考文献&lt;/span&gt;&lt;/h2&gt;

&lt;h2&gt;&lt;span&gt;1.应用场景&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;卷积神经网络的应用不可谓不广泛，主要有两大类，数据预测和图片处理。数据预测自然不需要多说，图片处理主要包含有图像分类，检测，识别，以及分割方面的应用。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像分类：场景分类，目标分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像检测：显著性检测，物体检测，语义检测等等&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像识别：人脸识别，字符识别，车牌识别，行为识别，步态识别等等&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图像分割：前景分割，语义分割&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225110007640-734520127.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;2.卷积神经网络结构&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;卷积神经网络主要是由输入层、卷积层、激活函数、池化层、全连接层、损失函数组成，表面看比较复杂，其实质就是&lt;span&gt;特征提取&lt;/span&gt;以及&lt;span&gt;决策推断&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;要使特征提取尽量准确，就需要将这些网络层结构进行组合，比如经典的卷积神经网络模型AlexNet:5个卷积层+3个池化层+3个连接层结构。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.1 卷积（convolution）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;卷积的作用就是提取特征，因为一次卷积可能提取的特征比较粗糙，所以多次卷积，以及层层纵深卷积，层层提取特征（千万要区别于多次卷积，因为每一层里含有多次卷积）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里可能就有小伙伴问：为什么要进行层层纵深卷积，而且还要每层多次？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你可以理解为物质A有自己的多个特征（高、矮、胖、瘦、、、），所以在物质A上需要多次提取，得到不同的特征，然后这些特征组合后发生化学反应生成物质B，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;而物质B又有一些新的专属于自己的特征，所以需要进一步卷积。这是我个人的理解，不对的话或者有更形象的比喻还请不吝赐教啊。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225143917459-1247536480.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在卷积层中，每一层的卷积核是不一样的。比如AlexNet&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一层：96*11*11（96表示卷积核个数，11表示卷积核矩阵宽*高） stride（步长） = 4  pad（边界补零） = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二层：256*5*5 stride（步长） = 1  pad（边界补零） = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三，四层：384*3*3 stride（步长） = 1  pad（边界补零） = 1&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第五层：256*3*3 stride（步长） = 1  pad（边界补零） = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积的篇幅说了这么多，那么到底是如何进行运算的呢，虽说网络上关于卷积运算原理铺天盖地，但是个人总感觉讲得不够透彻，或者说本人智商有待提高，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;希望通过如下这幅图（某位大神的杰作）来使各位看官们能够真正理解。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225154557731-1535384866.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这里举的例子是一个输入图片（5*5*3），卷积核（3*3*3），有两个（Filter W0，W1），偏置b也有两个（Bios b0，b1），卷积结果Output Volumn（3*3*2），步长stride = 2。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;输入：7*7*3 是因为 pad = 1 （在图片边界行和列都补零，补零的行和的数目是1），&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;（对于彩色图片，一般都是RGB3种颜色，号称3通道，7*7指图片高h * 宽w）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;，补零的作用是能够提取图片边界的特征。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核深度为什么要设置成3呢？这是因为输入是3通道，所以卷积核深度必须与输入的深度相同。至于卷积核宽w，高h则是可以变化的，但是宽高必须相等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,0] = 3 （Output Volumn下浅绿色框结果），这个结果是如何得到的呢？ 其实关键就是矩阵&lt;span&gt;对应位置相乘在相加&lt;span&gt;（千万不要跟矩阵乘法搞混淆啦）&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;=&amp;gt; w0[:,:,0] * x[:,:,0]蓝色区域矩阵(R通道) +  w0[:,:,1] * x[:,:,1]蓝色区域矩阵（G通道）+  w0[:,:,2] * x[:,:,2]蓝色区域矩阵（B通道） + &lt;span&gt;b0（千万不能丢，因为 y = w * x + b）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一项  =&amp;gt; 0 * 1 + 0 * 1 + 0 * 1 + 0 * (-1) + 1 * (-1) + 1 * 0 + 0 * (-1) + 1 * 1 + 1 * 0  =  0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二项 =&amp;gt; 0 * (-1) + 0 * (-1) + 0 * 1 + 0 * (-1) + 0 * 1 + 1 * 0 + 0 * (-1) + 2 * 1 + 2 * 0 = 2&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三项 =&amp;gt; 0 * 1 + 0 * 0 + 0 * (-1) + 0 * 0 + 2 * 0 + 2 * 0 + 0 * 1 + 0 * (-1) + 0 * (-1) = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,0] = &amp;gt; 第一项 + 第二项 + 第三项 + b0 = 0 + 2 + 0 + 1 = 3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;o[0,0,1] = -5 又是如何得到的呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因为这里的stride = 2 ，所以 输入的窗口就要滑动两个步长，也就是红色框的区域，而运算跟之前是一样的&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一项  =&amp;gt; 0 * 1 + 0 * 1 + 0 * 1 + 1 * (-1) + 2 * (-1) + 2 * 0 + 1 * (-1) + 1 * 1 + 2 * 0 = -3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二项 =&amp;gt; 0 * (-1) + 0 * (-1) + 0 * 1 + 1 * (-1) + 2 * 1 + 0 * 0 + 2 * (-1) + 1 * 1 + 1 * 0 = 0&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三项 =&amp;gt; 0 * 1 + 0 * 0 + 0 * (-1) + 2 * 0 + 0 * 0 + 1 * 0 + 0 * 1 + 2 * (-1) + 1 * (-1)  = - 3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积核输出o[0,0,1] = &amp;gt; 第一项 + 第二项 + 第三项 + b0 = (-3) + 0 + (-3) + 1 = -5&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;之后以此卷积核窗口大小在输入图片上滑动，卷积求出结果，因为有两个卷积核，所有就有两个输出结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里小伙伴可能有个疑问，输出窗口是如何得到的呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这里有一个公式：输出窗口宽 w = (输入窗口宽 w - 卷积核宽 w + 2 * pad)/stride  + 1 ，输出高 h  = 输出窗口宽 w&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;以上面例子， 输出窗口宽 w = ( 5 - 3 + 2 * 1)/2 + 1 = 3 ，则输出窗口大小为 3 * 3，因为有2个输出，所以是 3*3*2。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.2 Relu激活函数&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225163325379-1450551226.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;相信看过卷积神经网络结构（CNN）的伙伴们都知道，激活函数无处不在，特别是CNN中，在卷积层后，全连接（FC）后都有激活函数Relu的身影，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么这就自然不得不让我们产生疑问：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;问题1、为什么要用激活函数？它的作用是什么？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;问题2、在CNN中为什么要用Relu，相比于sigmoid，tanh，它的优势在什么地方？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于第1个问题：由 y = w * x + b 可知，如果不用激活函数，每个网络层的输出都是一种线性输出，而我们所处的现实场景，其实更多的是各种非线性的分布。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这也说明了激活函数的作用是将线性分布转化为非线性分布，能更逼近我们的真实场景。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于第2个问题： 先看sigmoid，tanh分布&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225164757082-386473751.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他们在 x -&amp;gt;&lt;img title=&quot;&quot; src=&quot;https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D15/sign=d11bc50915d8bc3ec20802cf838b1dd3/f9198618367adab4f7776b7087d4b31c8701e45a.jpg&quot; alt=&quot;&quot; width=&quot;15&quot; height=&quot;7&quot; align=&quot;absmiddle&quot;/&gt; 时，输出就变成了恒定值，因为求梯度时需要对函数求一阶偏导数，而不论是sigmoid，还是tanhx，他们的偏导都为0，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;也就是存在所谓的&lt;span&gt;梯度消失问题&lt;span&gt;，最终也就会导致权重参数w ， b 无法更新。相比之下，Relu就不存在这样的问题，另外在 x &amp;gt; 0 时，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Relu求导 = 1，这对于反向传播计算dw，db，是能够大大的简化运算的。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;使用sigmoid还会存在梯度爆炸的问题，比如在进行前向传播和反向传播迭代次数非常多的情况下，sigmoid因为是指数函数，其结果中&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;某些值会在迭代中累计，并成指数级增长，最终会出现NaN而导致溢出。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;2.3 池化&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;池化层一般在卷积层+ Relu之后，它的作用是：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;1、减小输入矩阵的大小（只是宽和高，而不是深度），提取主要特征。（不可否认的是，在池化后，特征会有一定的损失，所以，有些经典模型就去掉了池化这一层）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;它的目的是显而易见的，就是在后续操作时能降低运算。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;2、一般采用mean_pooling（均值池化）和max_pooling（最大值池化），对于输入矩阵有translation（平移），rotation（旋转），能够保证特征的不变性。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;mean_pooling 就是输入矩阵池化区域求均值，这里要注意的是池化窗口在输入矩阵滑动的步长跟stride有关，一般stride = 2.（图片是直接盗过来，这里感谢原创）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;最右边7/4 =&amp;gt; (1 + 1 + 2 + 3)/4&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225172838359-1167296564.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;max_pooling 最大值池化，就是每个池化区域的最大值放在输出对应位置上。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225172637537-860553733.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.4 全连接（full connection）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：分类器角色，将特征映射到样本标记空间，本质是矩阵变换（affine）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;至于变换的实现见后面的代码流程图，或者最好是跟一下代码，这样理解更透彻。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.5 损失函数（softmax_loss）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：计算损失loss，从而求出梯度grad。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;常用损失函数有:MSE均方误差，SVM（支持向量机）合页损失函数，Cross Entropy交叉熵损失函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这几种损失函数目前还看不出谁优谁劣，估计只有在具体的应用场景中去验证了。至于这几种损失函数的介绍，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大家可以去参考《常用损失函数小结》&lt;a href=&quot;https://blog.csdn.net/zhangjunp3/article/details/80467350&quot; target=&quot;_blank&quot;&gt;https://blog.csdn.net/zhangjunp3/article/details/80467350&lt;/a&gt;，这个哥们写得比较详细。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在后面的代码实例中，用到的是softmax_loss，它属于Cross Entropy交叉熵损失函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;softmax计算公式：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225185252832-1147975865.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;其中&lt;/span&gt;，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bz_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;span&gt; 是要计算的类别 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 的网络输出，分母是网络输出所有类别之和（共有 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20k&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;10&quot; height=&quot;16&quot;/&gt; 个类别），&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bp_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示第 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类的概率。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;交叉熵损失：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225190245199-312658746.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20y_%7Bi%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是类别 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 的真实标签，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5CLARGE%20_%7Bp_%7Bi%7D%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示第 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类的概率，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20N&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是样本总数，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20k&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 是类别数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;梯度:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;   &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20grad_%7Bj%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;47&quot; height=&quot;21&quot;/&gt;  =  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;50&quot; height=&quot;19&quot;/&gt;      当  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; != &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;   &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20grad_%7Bj%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;49&quot; height=&quot;22&quot;/&gt;  =  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot; width=&quot;45&quot; height=&quot;17&quot;/&gt;  - 1   当  &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; = &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20i&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中 &lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20ypred_%7Bi%7D%5E%7B%7D&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 表示&lt;span&gt;真实标签对应索引&lt;/span&gt;下预测的目标值，&lt;img id=&quot;equationview&quot; title=&quot;This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program.&quot; src=&quot;https://latex.codecogs.com/gif.latex?%5Clarge%20j&quot; alt=&quot;&quot; name=&quot;equationview&quot;/&gt; 类别索引。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个有点折磨人，原理讲解以及推导请大家可以参考这位大神的博客：&lt;a href=&quot;http://www.cnblogs.com/zongfa/p/8971213.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/zongfa/p/8971213.html&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.6 前向传播（forward propagation）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;前向传播包含之前的卷积，Relu激活函数，池化（pool），全连接(fc)，可以说，在损失函数之前操作都属于前向传播。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;主要是权重参数w , b 初始化，迭代，以及更新w, b,生成分类器模型。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.7 反向传播（back propagation）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;反向传播包含损失函数，通过梯度计算dw，db，Relu激活函数逆变换，反池化，反全连接。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;2.8 随机梯度下降（sgd_momentum）&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;作用：由梯度grad计算新的权重矩阵w&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;sgd公式：&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225200258307-64239916.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中，&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-39&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-40&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-41&quot; class=&quot;mi&quot;&gt;η为学习率，g&lt;sub&gt;t&lt;/sub&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;为x在t时刻的梯度。 &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;一般我们是将整个数据集分成n个epoch，每个epoch再分成m个batch，每次更新都利用一个batch的数据，而非整个训练集。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;优点：batch的方法可以减少机器的压力，并且可以更快地收敛。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;缺点：其更新方向完全依赖于当前的batch，因而其更新十分不稳定。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为了解决这个问题，momentum就横空出世了，具体原理详解见下路派出所（这名字霸气）的博客&lt;a href=&quot;http://www.cnblogs.com/callyblog/p/8299074.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/callyblog/p/8299074.html&lt;/a&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225201006070-1516485048.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中，&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-74&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-75&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-76&quot; class=&quot;mi&quot;&gt;ρ 即momentum，表示要在多大程度上保留原来的更新方向，这个值在0-1之间，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-74&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-75&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-76&quot; class=&quot;mi&quot;&gt;当梯度不那么大时，改为0.9。&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-77&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-78&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-79&quot; class=&quot;mi&quot;&gt;η 是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-80&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-81&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-82&quot; class=&quot;mi&quot;&gt;ρ 与 &lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span id=&quot;MathJax-Span-83&quot; class=&quot;math&quot;&gt;&lt;span id=&quot;MathJax-Span-84&quot; class=&quot;mrow&quot;&gt;&lt;span id=&quot;MathJax-Span-85&quot; class=&quot;mi&quot;&gt;η 之和不一定为1。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;3.代码实现流程图以及介绍&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;代码流程图：费了老大劲，终于弄完了，希望对各位看官们有所帮助，建议对比流程图和跟踪代码，加深对原理的理解。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;特别是前向传播和反向传播维度的变换，需要重点关注。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225221658952-232231034.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;4.代码实现&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;当然，代码的整个实现是某位大神实现的，我只是在上面做了些小改动以及重点函数做了些注释，有不妥之处也希望大家不吝指教。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;因为原始图片数据集太大，不好上传，大家可以直接在&lt;a href=&quot;http://www.cs.toronto.edu/~kriz/cifar.html&quot; target=&quot;_blank&quot;&gt;http://www.cs.toronto.edu/~kriz/cifar.html&lt;/a&gt;下载CIFAR-10 python version，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;有163M，放在代码文件同路径下即可。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;span class=&quot;MathJax_Preview&quot;&gt;&lt;span class=&quot;MathJax&quot;&gt;&lt;span class=&quot;math&quot;&gt;&lt;span class=&quot;mrow&quot;&gt;&lt;span class=&quot;mi&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222131668-2140712885.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; start.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('7779ba27-1e37-4387-bbe9-eba910650fb9')&quot; readability=&quot;50&quot;&gt;&lt;img id=&quot;code_img_closed_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('7779ba27-1e37-4387-bbe9-eba910650fb9',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_7779ba27-1e37-4387-bbe9-eba910650fb9&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;95&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; matplotlib.pyplot as plt
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;&lt;span&gt;同路径下py模块引用&lt;/span&gt;&lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; solver
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; cnn
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; solver
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; cnn
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 获取样本数据&lt;/span&gt;
&lt;span&gt;16&lt;/span&gt; data =&lt;span&gt; data_utils.get_CIFAR10_data()
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; model初始化（权重因子以及对应偏置 w1,b1 ,w2,b2 ,w3,b3，数量取决于网络层数）&lt;/span&gt;
&lt;span&gt;18&lt;/span&gt; model = cnn.ThreeLayerConvNet(reg=0.9&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; solver =&lt;span&gt; solver.Solver(model, data,
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                 lr_decay=0.95&lt;span&gt;,                
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;                 print_every=10, num_epochs=5, batch_size=2&lt;span&gt;, 
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;                 update_rule=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;sgd_momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,                
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                 optim_config={&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 5e-4, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 0.9&lt;span&gt;})
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练，获取最佳model&lt;/span&gt;
&lt;span&gt;25&lt;/span&gt; &lt;span&gt;solver.train()                 
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; 
&lt;span&gt;27&lt;/span&gt; plt.subplot(2, 1, 1&lt;span&gt;) 
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; plt.title(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Training loss&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt; plt.plot(solver.loss_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; plt.xlabel(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Iteration&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; 
&lt;span&gt;32&lt;/span&gt; plt.subplot(2, 1, 2&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; plt.title(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Accuracy&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; plt.plot(solver.train_acc_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;-o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, label=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; plt.plot(solver.val_acc_history, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;-o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, label=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; plt.plot([0.5] * len(solver.val_acc_history), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;k--&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt; plt.xlabel(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Epoch&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt; plt.legend(loc=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;lower right&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt; plt.gcf().set_size_inches(15, 12&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; &lt;span&gt;plt.show()
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt; 
&lt;span&gt;42&lt;/span&gt; 
&lt;span&gt;43&lt;/span&gt; best_model =&lt;span&gt; model
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; y_test_pred = np.argmax(best_model.loss(data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt; y_val_pred = np.argmax(best_model.loss(data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Validation set accuracy: &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,(y_val_pred == data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]).mean())
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Test set accuracy: &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, (y_test_pred == data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]).mean())
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Validation set accuracy:  about 52.9%&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Test set accuracy:  about 54.7%&lt;/span&gt;
&lt;span&gt;50&lt;/span&gt; 
&lt;span&gt;51&lt;/span&gt; 
&lt;span&gt;52&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Visualize the weights of the best network&lt;/span&gt;
&lt;span&gt;53&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;54&lt;/span&gt; &lt;span&gt;from vis_utils import visualize_grid
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt; 
&lt;span&gt;56&lt;/span&gt; &lt;span&gt;def show_net_weights(net):    
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; &lt;span&gt;    W1 = net.params['W1']    
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt; &lt;span&gt;    W1 = W1.reshape(3, 32, 32, -1).transpose(3, 1, 2, 0)    
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt; &lt;span&gt;    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))   
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt; &lt;span&gt;    plt.gca().axis('off')    
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt; &lt;span&gt;show_net_weights(best_model)
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt; &lt;span&gt;plt.show()
&lt;/span&gt;&lt;span&gt;63&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p&gt; cnn.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('6c8f719a-23b6-458e-b55b-b295adc1ba2a')&quot; readability=&quot;62&quot;&gt;&lt;img id=&quot;code_img_closed_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('6c8f719a-23b6-458e-b55b-b295adc1ba2a',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_6c8f719a-23b6-458e-b55b-b295adc1ba2a&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;119&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layer_utils
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;     &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layer_utils
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt;  ThreeLayerConvNet(object):
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;12&lt;/span&gt; &lt;span&gt;    A three-layer convolutional network with the following architecture:       
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;       conv - relu - 2x2 max pool - affine - relu - affine - softmax
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt; 
&lt;span&gt;16&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7&lt;span&gt;,             
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;                  hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                  dtype=&lt;span&gt;np.float32):
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;         self.params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         self.reg =&lt;span&gt; reg
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         self.dtype =&lt;span&gt; dtype
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Initialize weights and biases&lt;/span&gt;
&lt;span&gt;24&lt;/span&gt;         C, H, W =&lt;span&gt; input_dim
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale *&lt;span&gt; np.random.randn(num_filters, C, filter_size, filter_size)
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(num_filters)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale * np.random.randn(num_filters*H*W//4&lt;span&gt;, hidden_dim)
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(hidden_dim)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = weight_scale *&lt;span&gt; np.random.randn(hidden_dim, num_classes)
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;         self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; np.zeros(num_classes)
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; 
&lt;span&gt;32&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.params.items():    
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             self.params[k] =&lt;span&gt; v.astype(dtype)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; 
&lt;span&gt;35&lt;/span&gt; 
&lt;span&gt;36&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; loss(self, X, y=&lt;span&gt;None):
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;         W1, b1 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;         W2, b2 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;         W3, b3 = self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], self.params[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pass conv_param to the forward pass for the convolutional layer&lt;/span&gt;
&lt;span&gt;42&lt;/span&gt;         filter_size = W1.shape[2&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;         conv_param = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: (filter_size - 1) // 2&lt;span&gt;}
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pass pool_param to the forward pass for the max-pooling layer&lt;/span&gt;
&lt;span&gt;46&lt;/span&gt;         pool_param = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: 2&lt;span&gt;}
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; 
&lt;span&gt;48&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; compute the forward pass&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt;         a1, cache1 =&lt;span&gt; layer_utils.conv_relu_pool_forward(X, W1, b1, conv_param, pool_param)
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;         a2, cache2 =&lt;span&gt; layer_utils.affine_relu_forward(a1, W2, b2)
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt;         scores, cache3 =&lt;span&gt; layers.affine_forward(a2, W3, b3)
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt; 
&lt;span&gt;53&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; y &lt;span&gt;is&lt;/span&gt;&lt;span&gt; None:    
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;             &lt;span&gt;return&lt;/span&gt;&lt;span&gt; scores
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt; 
&lt;span&gt;56&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; compute the backward pass&lt;/span&gt;
&lt;span&gt;57&lt;/span&gt;         data_loss, dscores =&lt;span&gt; layers.softmax_loss(scores, y)
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt;         da2, dW3, db3 =&lt;span&gt; layers.affine_backward(dscores, cache3)
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;         da1, dW2, db2 =&lt;span&gt; layer_utils.affine_relu_backward(da2, cache2)
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt;         dX, dW1, db1 =&lt;span&gt; layer_utils.conv_relu_pool_backward(da1, cache1)
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt; 
&lt;span&gt;62&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Add regularization 引入修正因子，重新计算损失，梯度&lt;/span&gt;
&lt;span&gt;63&lt;/span&gt;         dW1 += self.reg *&lt;span&gt; W1
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;         dW2 += self.reg *&lt;span&gt; W2
&lt;/span&gt;&lt;span&gt;65&lt;/span&gt;         dW3 += self.reg *&lt;span&gt; W3
&lt;/span&gt;&lt;span&gt;66&lt;/span&gt;         reg_loss = 0.5 * self.reg * sum(np.sum(W * W) &lt;span&gt;for&lt;/span&gt; W &lt;span&gt;in&lt;/span&gt;&lt;span&gt; [W1, W2, W3])
&lt;/span&gt;&lt;span&gt;67&lt;/span&gt; 
&lt;span&gt;68&lt;/span&gt;         loss = data_loss +&lt;span&gt; reg_loss
&lt;/span&gt;&lt;span&gt;69&lt;/span&gt;         grads = {&lt;span&gt;'&lt;/span&gt;&lt;span&gt;W1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: db1, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;W2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: db2, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;W3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: dW3, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b3&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: db3}
&lt;/span&gt;&lt;span&gt;70&lt;/span&gt; 
&lt;span&gt;71&lt;/span&gt;         &lt;span&gt;return&lt;/span&gt; loss, grads
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;data.utils.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('2c746635-9f3b-4008-9c7e-77cc52d615f0')&quot; readability=&quot;97&quot;&gt;&lt;img id=&quot;code_img_closed_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('2c746635-9f3b-4008-9c7e-77cc52d615f0',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_2c746635-9f3b-4008-9c7e-77cc52d615f0&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;189&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt;  2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle 
&lt;/span&gt;&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; os
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt; 
&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt;from scipy.misc import imread&lt;/span&gt;
&lt;span&gt;  7&lt;/span&gt; 
&lt;span&gt;  8&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_CIFAR_batch(filename):
&lt;/span&gt;&lt;span&gt;  9&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; load single batch of cifar &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 10&lt;/span&gt;   with open(filename, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;rb&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) as f:
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     datadict = pickle.load(f, encoding=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;bytes&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt;     X = datadict[b&lt;span&gt;'&lt;/span&gt;&lt;span&gt;data&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt; 13&lt;/span&gt;     Y = datadict[b&lt;span&gt;'&lt;/span&gt;&lt;span&gt;labels&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt;     X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;float&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt;     Y =&lt;span&gt; np.array(Y)
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; X, Y
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; 
&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_CIFAR10(ROOT):
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; load all of cifar &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 20&lt;/span&gt;   xs =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt;   ys =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt;   &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt; range(1,2&lt;span&gt;):
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt;     f = os.path.join(ROOT, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;data_batch_%d&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (b, ))
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt;     X, Y =&lt;span&gt; load_CIFAR_batch(f)
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;    xs.append(X)
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;    ys.append(Y)    
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt;   Xtr =&lt;span&gt; np.concatenate(xs)
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;   Ytr =&lt;span&gt; np.concatenate(ys)
&lt;/span&gt;&lt;span&gt; 29&lt;/span&gt;   &lt;span&gt;del&lt;/span&gt;&lt;span&gt; X, Y
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;   Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;test_batch&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;))
&lt;/span&gt;&lt;span&gt; 31&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; Xtr, Ytr, Xte, Yte
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt; 
&lt;span&gt; 33&lt;/span&gt; 
&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; get_CIFAR10_data(num_training=500, num_validation=50, num_test=50&lt;span&gt;):
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; 
&lt;span&gt; 36&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;    it for classifiers. These are the same steps as we used for the SVM, but
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;    condensed to a single function.
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 41&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Load the raw CIFAR-10 data&lt;/span&gt;
&lt;span&gt; 42&lt;/span&gt; 
&lt;span&gt; 43&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;cifar10_dir = 'C://download//cifar-10-python//cifar-10-batches-py//'&lt;/span&gt;
&lt;span&gt; 44&lt;/span&gt;     cifar10_dir = &lt;span&gt;'&lt;/span&gt;&lt;span&gt;.\\cifar-10-batches-py\\&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;span&gt; 45&lt;/span&gt;     X_train, y_train, X_test, y_test =&lt;span&gt; load_CIFAR10(cifar10_dir)
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;&lt;span&gt; (X_train.shape)
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Subsample the data&lt;/span&gt;
&lt;span&gt; 48&lt;/span&gt;     mask = range(num_training, num_training +&lt;span&gt; num_validation)
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt;     X_val =&lt;span&gt; X_train[mask]
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt;     y_val =&lt;span&gt; y_train[mask]
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt;     mask =&lt;span&gt; range(num_training)
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt;     X_train =&lt;span&gt; X_train[mask]
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt;     y_train =&lt;span&gt; y_train[mask]
&lt;/span&gt;&lt;span&gt; 54&lt;/span&gt;     mask =&lt;span&gt; range(num_test)
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt;     X_test =&lt;span&gt; X_test[mask]
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt;     y_test =&lt;span&gt; y_test[mask]
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; 
&lt;span&gt; 58&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 标准化数据，求样本均值，然后 样本 - 样本均值，作用：使样本数据更收敛一些，便于后续处理&lt;/span&gt;
&lt;span&gt; 59&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Normalize the data: subtract the mean image&lt;/span&gt;
&lt;span&gt; 60&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果2维空间 m*n np.mean()后 =&amp;gt; 1*n&lt;/span&gt;
&lt;span&gt; 61&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 对于4维空间 m*n*k*j np.mean()后 =&amp;gt; 1*n*k*j&lt;/span&gt;
&lt;span&gt; 62&lt;/span&gt;     mean_image = np.mean(X_train, axis=&lt;span&gt;0)
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     X_train -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     X_val -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt;     X_test -=&lt;span&gt; mean_image
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt; 
&lt;span&gt; 67&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 把通道channel 提前&lt;/span&gt;
&lt;span&gt; 68&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Transpose so that channels come first&lt;/span&gt;
&lt;span&gt; 69&lt;/span&gt;     X_train = X_train.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt;     X_val = X_val.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 71&lt;/span&gt;     X_test = X_test.transpose(0, 3, 1, 2&lt;span&gt;).copy()
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt; 
&lt;span&gt; 73&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Package data into a dictionary&lt;/span&gt;
&lt;span&gt; 74&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 75&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_train, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_train,
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_val, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_val,
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt;       &lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: X_test, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;: y_test,
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt;     
&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;def load_tiny_imagenet(path, dtype=np.float32):
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt;   
&lt;span&gt; 83&lt;/span&gt; &lt;span&gt;  Load TinyImageNet. Each of TinyImageNet-100-A, TinyImageNet-100-B, and
&lt;/span&gt;&lt;span&gt; 84&lt;/span&gt; &lt;span&gt;  TinyImageNet-200 have the same directory structure, so this can be used
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt; &lt;span&gt;  to load any of them.
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt; 
&lt;span&gt; 87&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt; &lt;span&gt;  - path: String giving path to the directory to load.
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; &lt;span&gt;  - dtype: numpy datatype used to load the data.
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt; 
&lt;span&gt; 91&lt;/span&gt; &lt;span&gt;  Returns: A tuple of
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;  - class_names: A list where class_names[i] is a list of strings giving the
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;    WordNet names for class i in the loaded dataset.
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;  - X_train: (N_tr, 3, 64, 64) array of training images
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;  - y_train: (N_tr,) array of training labels
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;  - X_val: (N_val, 3, 64, 64) array of validation images
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;  - y_val: (N_val,) array of validation labels
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt; &lt;span&gt;  - X_test: (N_test, 3, 64, 64) array of testing images.
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt; &lt;span&gt;  - y_test: (N_test,) array of test labels; if test labels are not available
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt; &lt;span&gt;    (such as in student code) then y_test will be None.
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt;   
&lt;span&gt;102&lt;/span&gt; &lt;span&gt;  # First load wnids
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'wnids.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt; &lt;span&gt;    wnids = [x.strip() for x in f]
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt; 
&lt;span&gt;106&lt;/span&gt; &lt;span&gt;  # Map wnids to integer labels
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt; &lt;span&gt;  wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt; 
&lt;span&gt;109&lt;/span&gt; &lt;span&gt;  # Use words.txt to get names for each class
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'words.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;111&lt;/span&gt; &lt;span&gt;    wnid_to_words = dict(line.split('\t') for line in f)
&lt;/span&gt;&lt;span&gt;112&lt;/span&gt; &lt;span&gt;    for wnid, words in wnid_to_words.iteritems():
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt; &lt;span&gt;      wnid_to_words[wnid] = [w.strip() for w in words.split(',')]
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt; &lt;span&gt;  class_names = [wnid_to_words[wnid] for wnid in wnids]
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt; 
&lt;span&gt;116&lt;/span&gt; &lt;span&gt;  # Next load training data.
&lt;/span&gt;&lt;span&gt;117&lt;/span&gt; &lt;span&gt;  X_train = []
&lt;/span&gt;&lt;span&gt;118&lt;/span&gt; &lt;span&gt;  y_train = []
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt; &lt;span&gt;  for i, wnid in enumerate(wnids):
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt; &lt;span&gt;    if (i + 1) % 20 == 0:
&lt;/span&gt;&lt;span&gt;121&lt;/span&gt; &lt;span&gt;      print 'loading training data for synset %d / %d' % (i + 1, len(wnids))
&lt;/span&gt;&lt;span&gt;122&lt;/span&gt; &lt;span&gt;    # To figure out the filenames we need to open the boxes file
&lt;/span&gt;&lt;span&gt;123&lt;/span&gt; &lt;span&gt;    boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt; &lt;span&gt;    with open(boxes_file, 'r') as f:
&lt;/span&gt;&lt;span&gt;125&lt;/span&gt; &lt;span&gt;      filenames = [x.split('\t')[0] for x in f]
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt; &lt;span&gt;    num_images = len(filenames)
&lt;/span&gt;&lt;span&gt;127&lt;/span&gt;     
&lt;span&gt;128&lt;/span&gt; &lt;span&gt;    X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;129&lt;/span&gt; &lt;span&gt;    y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)
&lt;/span&gt;&lt;span&gt;130&lt;/span&gt; &lt;span&gt;    for j, img_file in enumerate(filenames):
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt; &lt;span&gt;      img_file = os.path.join(path, 'train', wnid, 'images', img_file)
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt; &lt;span&gt;      img = imread(img_file)
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; &lt;span&gt;      if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;134&lt;/span&gt; &lt;span&gt;        ## grayscale file
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; &lt;span&gt;        img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;136&lt;/span&gt; &lt;span&gt;      X_train_block[j] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;137&lt;/span&gt; &lt;span&gt;    X_train.append(X_train_block)
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt; &lt;span&gt;    y_train.append(y_train_block)
&lt;/span&gt;&lt;span&gt;139&lt;/span&gt;       
&lt;span&gt;140&lt;/span&gt; &lt;span&gt;  # We need to concatenate all training data
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt; &lt;span&gt;  X_train = np.concatenate(X_train, axis=0)
&lt;/span&gt;&lt;span&gt;142&lt;/span&gt; &lt;span&gt;  y_train = np.concatenate(y_train, axis=0)
&lt;/span&gt;&lt;span&gt;143&lt;/span&gt;   
&lt;span&gt;144&lt;/span&gt; &lt;span&gt;  # Next load validation data
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt; &lt;span&gt;  with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt; &lt;span&gt;    img_files = []
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt; &lt;span&gt;    val_wnids = []
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt; &lt;span&gt;    for line in f:
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; &lt;span&gt;      img_file, wnid = line.split('\t')[:2]
&lt;/span&gt;&lt;span&gt;150&lt;/span&gt; &lt;span&gt;      img_files.append(img_file)
&lt;/span&gt;&lt;span&gt;151&lt;/span&gt; &lt;span&gt;      val_wnids.append(wnid)
&lt;/span&gt;&lt;span&gt;152&lt;/span&gt; &lt;span&gt;    num_val = len(img_files)
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt; &lt;span&gt;    y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt; &lt;span&gt;    X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt; &lt;span&gt;    for i, img_file in enumerate(img_files):
&lt;/span&gt;&lt;span&gt;156&lt;/span&gt; &lt;span&gt;      img_file = os.path.join(path, 'val', 'images', img_file)
&lt;/span&gt;&lt;span&gt;157&lt;/span&gt; &lt;span&gt;      img = imread(img_file)
&lt;/span&gt;&lt;span&gt;158&lt;/span&gt; &lt;span&gt;      if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;159&lt;/span&gt; &lt;span&gt;        img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;160&lt;/span&gt; &lt;span&gt;      X_val[i] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt; 
&lt;span&gt;162&lt;/span&gt; &lt;span&gt;  # Next load test images
&lt;/span&gt;&lt;span&gt;163&lt;/span&gt; &lt;span&gt;  # Students won't have test labels, so we need to iterate over files in the
&lt;/span&gt;&lt;span&gt;164&lt;/span&gt; &lt;span&gt;  # images directory.
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt; &lt;span&gt;  img_files = os.listdir(os.path.join(path, 'test', 'images'))
&lt;/span&gt;&lt;span&gt;166&lt;/span&gt; &lt;span&gt;  X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)
&lt;/span&gt;&lt;span&gt;167&lt;/span&gt; &lt;span&gt;  for i, img_file in enumerate(img_files):
&lt;/span&gt;&lt;span&gt;168&lt;/span&gt; &lt;span&gt;    img_file = os.path.join(path, 'test', 'images', img_file)
&lt;/span&gt;&lt;span&gt;169&lt;/span&gt; &lt;span&gt;    img = imread(img_file)
&lt;/span&gt;&lt;span&gt;170&lt;/span&gt; &lt;span&gt;    if img.ndim == 2:
&lt;/span&gt;&lt;span&gt;171&lt;/span&gt; &lt;span&gt;      img.shape = (64, 64, 1)
&lt;/span&gt;&lt;span&gt;172&lt;/span&gt; &lt;span&gt;    X_test[i] = img.transpose(2, 0, 1)
&lt;/span&gt;&lt;span&gt;173&lt;/span&gt; 
&lt;span&gt;174&lt;/span&gt; &lt;span&gt;  y_test = None
&lt;/span&gt;&lt;span&gt;175&lt;/span&gt; &lt;span&gt;  y_test_file = os.path.join(path, 'test', 'test_annotations.txt')
&lt;/span&gt;&lt;span&gt;176&lt;/span&gt; &lt;span&gt;  if os.path.isfile(y_test_file):
&lt;/span&gt;&lt;span&gt;177&lt;/span&gt; &lt;span&gt;    with open(y_test_file, 'r') as f:
&lt;/span&gt;&lt;span&gt;178&lt;/span&gt; &lt;span&gt;      img_file_to_wnid = {}
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;      for line in f:
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; &lt;span&gt;        line = line.split('\t')
&lt;/span&gt;&lt;span&gt;181&lt;/span&gt; &lt;span&gt;        img_file_to_wnid[line[0]] = line[1]
&lt;/span&gt;&lt;span&gt;182&lt;/span&gt; &lt;span&gt;    y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]
&lt;/span&gt;&lt;span&gt;183&lt;/span&gt; &lt;span&gt;    y_test = np.array(y_test)
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt;   
&lt;span&gt;185&lt;/span&gt; &lt;span&gt;  return class_names, X_train, y_train, X_val, y_val, X_test, y_test
&lt;/span&gt;&lt;span&gt;186&lt;/span&gt; 
&lt;span&gt;187&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;188&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; load_models(models_dir):
&lt;/span&gt;&lt;span&gt;189&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;190&lt;/span&gt; &lt;span&gt;  Load saved models from disk. This will attempt to unpickle all files in a
&lt;/span&gt;&lt;span&gt;191&lt;/span&gt; &lt;span&gt;  directory; any files that give errors on unpickling (such as README.txt) will
&lt;/span&gt;&lt;span&gt;192&lt;/span&gt; &lt;span&gt;  be skipped.
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt; 
&lt;span&gt;194&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt; &lt;span&gt;  - models_dir: String giving the path to a directory containing model files.
&lt;/span&gt;&lt;span&gt;196&lt;/span&gt; &lt;span&gt;    Each model file is a pickled dictionary with a 'model' field.
&lt;/span&gt;&lt;span&gt;197&lt;/span&gt; 
&lt;span&gt;198&lt;/span&gt; &lt;span&gt;  Returns:
&lt;/span&gt;&lt;span&gt;199&lt;/span&gt; &lt;span&gt;  A dictionary mapping model file names to models.
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;201&lt;/span&gt;   models =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;202&lt;/span&gt;   &lt;span&gt;for&lt;/span&gt; model_file &lt;span&gt;in&lt;/span&gt;&lt;span&gt; os.listdir(models_dir):
&lt;/span&gt;&lt;span&gt;203&lt;/span&gt;     with open(os.path.join(models_dir, model_file), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;rb&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) as f:
&lt;/span&gt;&lt;span&gt;204&lt;/span&gt;       &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;205&lt;/span&gt;         models[model_file] = pickle.load(f)[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;model&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt;       &lt;span&gt;except&lt;/span&gt;&lt;span&gt; pickle.UnpicklingError:
&lt;/span&gt;&lt;span&gt;207&lt;/span&gt;         &lt;span&gt;continue&lt;/span&gt;
&lt;span&gt;208&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt; models
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;layer.utils.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('9472e966-549f-4722-ad84-39f2ef8b3ad3')&quot; readability=&quot;65&quot;&gt;&lt;img id=&quot;code_img_closed_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('9472e966-549f-4722-ad84-39f2ef8b3ad3',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_9472e966-549f-4722-ad84-39f2ef8b3ad3&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;125&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;   &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;   &lt;span&gt;import&lt;/span&gt;&lt;span&gt; layers
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; 
&lt;span&gt; 7&lt;/span&gt; 
&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_relu_forward(x, w, b):
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;12&lt;/span&gt; &lt;span&gt;  Convenience layer that perorms an affine transform followed by a ReLU
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;  - x: Input to the affine layer
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;  - w, b: Weights for the affine layer
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; 
&lt;span&gt;18&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;  - out: Output from the ReLU
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;22&lt;/span&gt;   a, fc_cache =&lt;span&gt; layers.affine_forward(x, w, b)
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;   out, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;   cache =&lt;span&gt; (fc_cache, relu_cache)
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; 
&lt;span&gt;27&lt;/span&gt; 
&lt;span&gt;28&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_relu_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;30&lt;/span&gt; &lt;span&gt;  Backward pass for the affine-relu convenience layer
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;32&lt;/span&gt;   fc_cache, relu_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(dout, relu_cache)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.affine_backward(da, fc_cache)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; 
&lt;span&gt;37&lt;/span&gt; 
&lt;span&gt;38&lt;/span&gt; &lt;span&gt;pass&lt;/span&gt;
&lt;span&gt;39&lt;/span&gt; 
&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_forward(x, w, b, conv_param):
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt; &lt;span&gt;  A convenience layer that performs a convolution followed by a ReLU.
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; &lt;span&gt;  - x: Input to the convolutional layer
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;  - w, b, conv_param: Weights and parameters for the convolutional layer
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;   
&lt;span&gt;49&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt; &lt;span&gt;  - out: Output from the ReLU
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;53&lt;/span&gt;   a, conv_cache =&lt;span&gt; layers.conv_forward_fast(x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;   out, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;   cache =&lt;span&gt; (conv_cache, relu_cache)
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; 
&lt;span&gt;58&lt;/span&gt; 
&lt;span&gt;59&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;61&lt;/span&gt; &lt;span&gt;  Backward pass for the conv-relu convenience layer.
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;63&lt;/span&gt;   conv_cache, relu_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(dout, relu_cache)
&lt;/span&gt;&lt;span&gt;65&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.conv_backward_fast(da, conv_cache)
&lt;/span&gt;&lt;span&gt;66&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;67&lt;/span&gt; 
&lt;span&gt;68&lt;/span&gt; 
&lt;span&gt;69&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_pool_forward(x, w, b, conv_param, pool_param):
&lt;/span&gt;&lt;span&gt;70&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;71&lt;/span&gt; &lt;span&gt;  Convenience layer that performs a convolution, a ReLU, and a pool.
&lt;/span&gt;&lt;span&gt;72&lt;/span&gt; 
&lt;span&gt;73&lt;/span&gt; &lt;span&gt;  Inputs:
&lt;/span&gt;&lt;span&gt;74&lt;/span&gt; &lt;span&gt;  - x: Input to the convolutional layer
&lt;/span&gt;&lt;span&gt;75&lt;/span&gt; &lt;span&gt;  - w, b, conv_param: Weights and parameters for the convolutional layer
&lt;/span&gt;&lt;span&gt;76&lt;/span&gt; &lt;span&gt;  - pool_param: Parameters for the pooling layer
&lt;/span&gt;&lt;span&gt;77&lt;/span&gt; 
&lt;span&gt;78&lt;/span&gt; &lt;span&gt;  Returns a tuple of:
&lt;/span&gt;&lt;span&gt;79&lt;/span&gt; &lt;span&gt;  - out: Output from the pooling layer
&lt;/span&gt;&lt;span&gt;80&lt;/span&gt; &lt;span&gt;  - cache: Object to give to the backward pass
&lt;/span&gt;&lt;span&gt;81&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;82&lt;/span&gt;   a, conv_cache =&lt;span&gt; layers.conv_forward_naive(x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;83&lt;/span&gt;   s, relu_cache =&lt;span&gt; layers.relu_forward(a)
&lt;/span&gt;&lt;span&gt;84&lt;/span&gt;   out, pool_cache =&lt;span&gt; layers.max_pool_forward_naive(s, pool_param)
&lt;/span&gt;&lt;span&gt;85&lt;/span&gt;   cache =&lt;span&gt; (conv_cache, relu_cache, pool_cache)
&lt;/span&gt;&lt;span&gt;86&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;87&lt;/span&gt; 
&lt;span&gt;88&lt;/span&gt; 
&lt;span&gt;89&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_relu_pool_backward(dout, cache):
&lt;/span&gt;&lt;span&gt;90&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;91&lt;/span&gt; &lt;span&gt;  Backward pass for the conv-relu-pool convenience layer
&lt;/span&gt;&lt;span&gt;92&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;93&lt;/span&gt;   conv_cache, relu_cache, pool_cache =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;94&lt;/span&gt;   ds =&lt;span&gt; layers.max_pool_backward_naive(dout, pool_cache)
&lt;/span&gt;&lt;span&gt;95&lt;/span&gt;   da =&lt;span&gt; layers.relu_backward(ds, relu_cache)
&lt;/span&gt;&lt;span&gt;96&lt;/span&gt;   dx, dw, db =&lt;span&gt; layers.conv_backward_naive(da, conv_cache)
&lt;/span&gt;&lt;span&gt;97&lt;/span&gt;   &lt;span&gt;return&lt;/span&gt; dx, dw, db
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;layers.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('3f8ab327-6695-4ef0-943b-74fdc83786ed')&quot; readability=&quot;150&quot;&gt;&lt;img id=&quot;code_img_closed_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('3f8ab327-6695-4ef0-943b-74fdc83786ed',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_3f8ab327-6695-4ef0-943b-74fdc83786ed&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;295&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; 
&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;全连接层：矩阵变换，获取对应目标相同的行与列
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt; &lt;span&gt;输入x: 2*32*16*16 
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;输入x_row: 2*8192
&lt;/span&gt;&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;超参w：8192*100
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt; &lt;span&gt;输出：矩阵乘法 2*8192 -&amp;gt;8192*100 =&amp;gt;2*100
&lt;/span&gt;&lt;span&gt;  9&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 10&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_forward(x, w, b):   
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 12&lt;/span&gt; &lt;span&gt;    Computes the forward pass for an affine (fully-connected) layer. 
&lt;/span&gt;&lt;span&gt; 13&lt;/span&gt; &lt;span&gt;    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N   
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;    examples, where each example x[i] has shape (d_1, ..., d_k). We will    
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;    reshape each input into a vector of dimension D = d_1 * ... * d_k, and    
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; &lt;span&gt;    then transform it to an output vector of dimension M.    
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)    
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; &lt;span&gt;    - w: A numpy array of weights, of shape (D, M)    
&lt;/span&gt;&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;    - b: A numpy array of biases, of shape (M,)   
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt; &lt;span&gt;    - out: output, of shape (N, M)    
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;    - cache: (x, w, b)   
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 25&lt;/span&gt;     out =&lt;span&gt; None
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Reshape x into rows&lt;/span&gt;
&lt;span&gt; 27&lt;/span&gt;     N =&lt;span&gt; x.shape[0]
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;     x_row = x.reshape(N, -1)         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D) -1表示不知道多少列，指定行，就能算出列 = 2 * 32 * 16 * 16/2 = 8192&lt;/span&gt;
&lt;span&gt; 29&lt;/span&gt;     out = np.dot(x_row, w) + b       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,M) 2*8192 8192*100 =&amp;gt;2 * 100&lt;/span&gt;
&lt;span&gt; 30&lt;/span&gt;     cache =&lt;span&gt; (x, w, b)
&lt;/span&gt;&lt;span&gt; 31&lt;/span&gt; 
&lt;span&gt; 32&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;反向传播之affine矩阵变换
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; &lt;span&gt;根据dout求出dx,dw,db
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt; &lt;span&gt;由 out = w * x =&amp;gt;
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;dx = dout * w
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;dw = dout * x
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;db = dout * 1
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; &lt;span&gt;因为dx 与 x，dw 与 w，db 与 b 大小（维度）必须相同
&lt;/span&gt;&lt;span&gt; 41&lt;/span&gt; &lt;span&gt;dx = dout * wT  矩阵乘法
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; &lt;span&gt;dw = dxT * dout 矩阵乘法
&lt;/span&gt;&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;db = dout 按列求和
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; affine_backward(dout, cache):   
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 47&lt;/span&gt; &lt;span&gt;    Computes the backward pass for an affine layer.    
&lt;/span&gt;&lt;span&gt; 48&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;    - dout: Upstream derivative, of shape (N, M)    
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; &lt;span&gt;    - cache: Tuple of: 
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, d_1, ... d_k)    
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    - w: Weights, of shape (D, M)    
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt; &lt;span&gt;    Returns a tuple of:   
&lt;/span&gt;&lt;span&gt; 54&lt;/span&gt; &lt;span&gt;    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt; &lt;span&gt;      dx = dout * w
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt; &lt;span&gt;    - dw: Gradient with respect to w, of shape (D, M)
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; &lt;span&gt;      dw = dout * x
&lt;/span&gt;&lt;span&gt; 58&lt;/span&gt; &lt;span&gt;    - db: Gradient with respect to b, of shape (M,)
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt; &lt;span&gt;      db = dout * 1
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 61&lt;/span&gt; 
&lt;span&gt; 62&lt;/span&gt;     x, w, b =&lt;span&gt; cache    
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     dx, dw, db =&lt;span&gt; None, None, None
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     dx = np.dot(dout, w.T)                       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D)&lt;/span&gt;
&lt;span&gt; 65&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; dx维度必须跟x维度相同&lt;/span&gt;
&lt;span&gt; 66&lt;/span&gt;     dx = np.reshape(dx, x.shape)                 &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,d1,...,d_k)&lt;/span&gt;
&lt;span&gt; 67&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 转换成二维矩阵&lt;/span&gt;
&lt;span&gt; 68&lt;/span&gt;     x_row = x.reshape(x.shape[0], -1)            &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (N,D)&lt;/span&gt;
&lt;span&gt; 69&lt;/span&gt;     dw = np.dot(x_row.T, dout)                   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (D,M)&lt;/span&gt;
&lt;span&gt; 70&lt;/span&gt; 
&lt;span&gt; 71&lt;/span&gt;     db = np.sum(dout, axis=0, keepdims=True)     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (1,M)    &lt;/span&gt;
&lt;span&gt; 72&lt;/span&gt; 
&lt;span&gt; 73&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt; 74&lt;/span&gt; 
&lt;span&gt; 75&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; relu_forward(x):   
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; 激活函数，解决sigmoid梯度消失问题，网络性能比sigmoid更好
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    Computes the forward pass for a layer of rectified linear units (ReLUs).    
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    Input:    
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    - x: Inputs, of any shape    
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - out: Output, of the same shape as x    
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;    - cache: x    
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;   
&lt;span&gt; 84&lt;/span&gt;     out =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt;     out =&lt;span&gt; ReLU(x)    
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;     cache =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt; 87&lt;/span&gt; 
&lt;span&gt; 88&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; 
&lt;span&gt; 90&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; relu_backward(dout, cache):   
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;  
&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;    Computes the backward pass for a layer of rectified linear units (ReLUs).   
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;    Input:    
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;    - dout: Upstream derivatives, of any shape    
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;    - cache: Input x, of same shape as dout    
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;    Returns:    
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;    - dx: Gradient with respect to x    
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 99&lt;/span&gt;     dx, x =&lt;span&gt; None, cache    
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt;     dx =&lt;span&gt; dout    
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt;     dx[x &amp;lt;= 0] =&lt;span&gt; 0    
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt; 
&lt;span&gt;103&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt; 
&lt;span&gt;105&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; svm_loss(x, y):   
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;107&lt;/span&gt; &lt;span&gt;    Computes the loss and gradient using for multiclass SVM classification.    
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt; &lt;span&gt;    Inputs:    
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class         
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; &lt;span&gt;         for the ith input.    
&lt;/span&gt;&lt;span&gt;111&lt;/span&gt; &lt;span&gt;    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and         
&lt;/span&gt;&lt;span&gt;112&lt;/span&gt; &lt;span&gt;         0 &amp;lt;= y[i] &amp;lt; C   
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss   
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt; &lt;span&gt;    - dx: Gradient of the loss with respect to x    
&lt;/span&gt;&lt;span&gt;116&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;117&lt;/span&gt;     N =&lt;span&gt; x.shape[0]   
&lt;/span&gt;&lt;span&gt;118&lt;/span&gt;     correct_class_scores =&lt;span&gt; x[np.arange(N), y]    
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt;     margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt;     margins[np.arange(N), y] =&lt;span&gt; 0   
&lt;/span&gt;&lt;span&gt;121&lt;/span&gt;     loss = np.sum(margins) /&lt;span&gt; N   
&lt;/span&gt;&lt;span&gt;122&lt;/span&gt;     num_pos = np.sum(margins &amp;gt; 0, axis=1&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;123&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)   
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt;     dx[margins &amp;gt; 0] = 1    
&lt;span&gt;125&lt;/span&gt;     dx[np.arange(N), y] -=&lt;span&gt; num_pos    
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt;     dx /=&lt;span&gt; N    
&lt;/span&gt;&lt;span&gt;127&lt;/span&gt; 
&lt;span&gt;128&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; loss, dx
&lt;/span&gt;&lt;span&gt;129&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;130&lt;/span&gt; &lt;span&gt;softmax_loss 求梯度优点: 求梯度运算简单，方便 
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt; &lt;span&gt;softmax: softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt; &lt;span&gt;可以看成概率来理解，从而来进行多分类。
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; &lt;span&gt;Si = exp(i)/[exp(j)求和]
&lt;/span&gt;&lt;span&gt;134&lt;/span&gt; &lt;span&gt;softmax_loss：损失函数，求梯度dx必须用到损失函数，通过梯度下降更新超参
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; &lt;span&gt;Loss = -[Ypred*ln(Sj真实类别位置的概率值)]求和
&lt;/span&gt;&lt;span&gt;136&lt;/span&gt; &lt;span&gt;梯度dx : 对损失函数求一阶偏导
&lt;/span&gt;&lt;span&gt;137&lt;/span&gt; &lt;span&gt;如果 j = i =&amp;gt;dx = Sj - 1
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt; &lt;span&gt;如果 j != i =&amp;gt; dx = Sj
&lt;/span&gt;&lt;span&gt;139&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;140&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; softmax_loss(x, y):    
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;142&lt;/span&gt; &lt;span&gt;    Computes the loss and gradient for softmax classification.    Inputs:    
&lt;/span&gt;&lt;span&gt;143&lt;/span&gt; &lt;span&gt;    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class         
&lt;/span&gt;&lt;span&gt;144&lt;/span&gt; &lt;span&gt;    for the ith input.    
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt; &lt;span&gt;    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and         
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt; &lt;span&gt;         0 &amp;lt;= y[i] &amp;lt; C   
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt; &lt;span&gt;    Returns a tuple of:    
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss    
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; &lt;span&gt;    - dx: Gradient of the loss with respect to x   
&lt;/span&gt;&lt;span&gt;150&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;151&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;152&lt;/span&gt; &lt;span&gt;     x - np.max(x, axis=1, keepdims=True) 对数据进行预处理，
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt; &lt;span&gt;     防止np.exp(x - np.max(x, axis=1, keepdims=True))得到结果太分散；
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt; &lt;span&gt;     np.max(x, axis=1, keepdims=True)保证所得结果维度不变；
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;156&lt;/span&gt;     probs = np.exp(x - np.max(x, axis=1, keepdims=&lt;span&gt;True))
&lt;/span&gt;&lt;span&gt;157&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算softmax，准确的说应该是soft，因为还没有选取概率最大值的操作&lt;/span&gt;
&lt;span&gt;158&lt;/span&gt;     probs /= np.sum(probs, axis=1, keepdims=&lt;span&gt;True)
&lt;/span&gt;&lt;span&gt;159&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 样本图片个数&lt;/span&gt;
&lt;span&gt;160&lt;/span&gt;     N =&lt;span&gt; x.shape[0]
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算图片损失&lt;/span&gt;
&lt;span&gt;162&lt;/span&gt;     loss = -np.sum(np.log(probs[np.arange(N), y])) /&lt;span&gt; N
&lt;/span&gt;&lt;span&gt;163&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 复制概率&lt;/span&gt;
&lt;span&gt;164&lt;/span&gt;     dx =&lt;span&gt; probs.copy()
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 针对 i = j 求梯度&lt;/span&gt;
&lt;span&gt;166&lt;/span&gt;     dx[np.arange(N), y] -= 1
&lt;span&gt;167&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算每张样本图片梯度&lt;/span&gt;
&lt;span&gt;168&lt;/span&gt;     dx /=&lt;span&gt; N    
&lt;/span&gt;&lt;span&gt;169&lt;/span&gt; 
&lt;span&gt;170&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; loss, dx
&lt;/span&gt;&lt;span&gt;171&lt;/span&gt; 
&lt;span&gt;172&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; ReLU(x):    
&lt;/span&gt;&lt;span&gt;173&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;ReLU non-linearity.&lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;174&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; np.maximum(0, x)
&lt;/span&gt;&lt;span&gt;175&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;176&lt;/span&gt; &lt;span&gt;功能：获取图片特征
&lt;/span&gt;&lt;span&gt;177&lt;/span&gt; &lt;span&gt;前向卷积：每次用一个3维的卷积核与图片RGB各个通道分别卷积（卷积核1与R进行点积，卷积核2与G点积，卷积核3与B点积）,
&lt;/span&gt;&lt;span&gt;178&lt;/span&gt; &lt;span&gt;然后将3个结果求和（也就是 w*x ）,再加上 b，就是新结果某一位置输出，这是卷积核在图片某一固定小范围内（卷积核大小）的卷积，
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;要想获得整个图片的卷积结果，需要在图片上滑动卷积核（先右后下），直至遍历整个图片。
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; &lt;span&gt;x: 2*3*32*32  每次选取2张图片，图片大小32*32，彩色(3通道)
&lt;/span&gt;&lt;span&gt;181&lt;/span&gt; &lt;span&gt;w: 32*3*7*7   卷积核每个大小是7*7；对应输入x的3通道，所以是3维，有32个卷积核
&lt;/span&gt;&lt;span&gt;182&lt;/span&gt; &lt;span&gt;pad = 3(图片边缘行列补0)，stride = 1(卷积核移动步长)
&lt;/span&gt;&lt;span&gt;183&lt;/span&gt; &lt;span&gt;输出宽*高结果：(32-7+2*3)/1 + 1 = 32
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt; &lt;span&gt;输出大小：2*32*32*32
&lt;/span&gt;&lt;span&gt;185&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;186&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_forward_naive(x, w, b, conv_param):
&lt;/span&gt;&lt;span&gt;187&lt;/span&gt;     stride, pad = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;188&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;189&lt;/span&gt;     F, C, HH, WW =&lt;span&gt; w.shape
&lt;/span&gt;&lt;span&gt;190&lt;/span&gt;     x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;191&lt;/span&gt;     &lt;span&gt;'''&lt;/span&gt;&lt;span&gt;// : 求整型&lt;/span&gt;&lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;192&lt;/span&gt;     H_new = 1 + (H + 2 * pad - HH) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt;     W_new = 1 + (W + 2 * pad - WW) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;194&lt;/span&gt;     s =&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt;     out =&lt;span&gt; np.zeros((N, F, H_new, W_new))
&lt;/span&gt;&lt;span&gt;196&lt;/span&gt; 
&lt;span&gt;197&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(N):       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; ith image    &lt;/span&gt;
&lt;span&gt;198&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; f &lt;span&gt;in&lt;/span&gt; range(F):   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; fth filter        &lt;/span&gt;
&lt;span&gt;199&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):   
&lt;/span&gt;&lt;span&gt;201&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s].shape&lt;/span&gt;
&lt;span&gt;202&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print w[f].shape  &lt;/span&gt;
&lt;span&gt;203&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print b.shape  &lt;/span&gt;
&lt;span&gt;204&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;print np.sum((x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] * w[f]))         &lt;/span&gt;
&lt;span&gt;205&lt;/span&gt;                     out[i, f, j, k] = np.sum(x_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] * w[f]) +&lt;span&gt; b[f]
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt; 
&lt;span&gt;207&lt;/span&gt;     cache =&lt;span&gt; (x, w, b, conv_param)
&lt;/span&gt;&lt;span&gt;208&lt;/span&gt; 
&lt;span&gt;209&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;210&lt;/span&gt; 
&lt;span&gt;211&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;212&lt;/span&gt; &lt;span&gt;反向传播之卷积：卷积核3*7*7
&lt;/span&gt;&lt;span&gt;213&lt;/span&gt; &lt;span&gt;输入dout:2*32*32*32
&lt;/span&gt;&lt;span&gt;214&lt;/span&gt; &lt;span&gt;输出dx:2*3*32*32
&lt;/span&gt;&lt;span&gt;215&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;216&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; conv_backward_naive(dout, cache):
&lt;/span&gt;&lt;span&gt;217&lt;/span&gt; 
&lt;span&gt;218&lt;/span&gt;     x, w, b, conv_param =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;219&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 边界补0&lt;/span&gt;
&lt;span&gt;220&lt;/span&gt;     pad = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pad&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;221&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 步长&lt;/span&gt;
&lt;span&gt;222&lt;/span&gt;     stride = conv_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;223&lt;/span&gt;     F, C, HH, WW =&lt;span&gt; w.shape
&lt;/span&gt;&lt;span&gt;224&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;225&lt;/span&gt;     H_new = 1 + (H + 2 * pad - HH) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;226&lt;/span&gt;     W_new = 1 + (W + 2 * pad - WW) //&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;227&lt;/span&gt; 
&lt;span&gt;228&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)
&lt;/span&gt;&lt;span&gt;229&lt;/span&gt;     dw =&lt;span&gt; np.zeros_like(w)
&lt;/span&gt;&lt;span&gt;230&lt;/span&gt;     db =&lt;span&gt; np.zeros_like(b)
&lt;/span&gt;&lt;span&gt;231&lt;/span&gt; 
&lt;span&gt;232&lt;/span&gt;     s =&lt;span&gt; stride
&lt;/span&gt;&lt;span&gt;233&lt;/span&gt;     x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;234&lt;/span&gt;     dx_padded = np.pad(dx, ((0, 0), (0, 0), (pad, pad), (pad, pad)), &lt;span&gt;'&lt;/span&gt;&lt;span&gt;constant&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;235&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 图片个数&lt;/span&gt;
&lt;span&gt;236&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(N):       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; ith image&lt;/span&gt;
&lt;span&gt;237&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 卷积核滤波个数&lt;/span&gt;
&lt;span&gt;238&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; f &lt;span&gt;in&lt;/span&gt; range(F):   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; fth filter        &lt;/span&gt;
&lt;span&gt;239&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;240&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):
&lt;/span&gt;&lt;span&gt;241&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7&lt;/span&gt;
&lt;span&gt;242&lt;/span&gt;                     window = x_padded[i, :, j*s:HH+j*s, k*s:WW+k*&lt;span&gt;s]
&lt;/span&gt;&lt;span&gt;243&lt;/span&gt;                     db[f] +=&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;244&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7&lt;/span&gt;
&lt;span&gt;245&lt;/span&gt;                     dw[f] += window *&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;246&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 3*7*7 =&amp;gt; 2*3*38*38&lt;/span&gt;
&lt;span&gt;247&lt;/span&gt;                     dx_padded[i, :, j*s:HH+j*s, k*s:WW+k*s] += w[f] *&lt;span&gt; dout[i, f, j, k]
&lt;/span&gt;&lt;span&gt;248&lt;/span&gt; 
&lt;span&gt;249&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Unpad&lt;/span&gt;
&lt;span&gt;250&lt;/span&gt;     dx = dx_padded[:, :, pad:pad+H, pad:pad+&lt;span&gt;W]
&lt;/span&gt;&lt;span&gt;251&lt;/span&gt; 
&lt;span&gt;252&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; dx, dw, db
&lt;/span&gt;&lt;span&gt;253&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;254&lt;/span&gt; &lt;span&gt;功能：减少特征尺寸大小
&lt;/span&gt;&lt;span&gt;255&lt;/span&gt; &lt;span&gt;前向最大池化：在特征矩阵中选取指定大小窗口，获取窗口内元素最大值作为输出窗口映射值，
&lt;/span&gt;&lt;span&gt;256&lt;/span&gt; &lt;span&gt;先有后下遍历，直至获取整个特征矩阵对应的新映射特征矩阵。
&lt;/span&gt;&lt;span&gt;257&lt;/span&gt; &lt;span&gt;输入x：2*32*32*32
&lt;/span&gt;&lt;span&gt;258&lt;/span&gt; &lt;span&gt;池化参数：窗口：2*2，步长：2
&lt;/span&gt;&lt;span&gt;259&lt;/span&gt; &lt;span&gt;输出窗口宽，高：(32-2)/2 + 1 = 16
&lt;/span&gt;&lt;span&gt;260&lt;/span&gt; &lt;span&gt;输出大小：2*32*16*16
&lt;/span&gt;&lt;span&gt;261&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;262&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; max_pool_forward_naive(x, pool_param):
&lt;/span&gt;&lt;span&gt;263&lt;/span&gt;     HH, WW = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;264&lt;/span&gt;     s = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;265&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;266&lt;/span&gt;     H_new = 1 + (H - HH) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;267&lt;/span&gt;     W_new = 1 + (W - WW) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;268&lt;/span&gt;     out =&lt;span&gt; np.zeros((N, C, H_new, W_new))
&lt;/span&gt;&lt;span&gt;269&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(N):    
&lt;/span&gt;&lt;span&gt;270&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(C):        
&lt;/span&gt;&lt;span&gt;271&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;272&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):                
&lt;/span&gt;&lt;span&gt;273&lt;/span&gt;                     window = x[i, j, k*s:HH+k*s, l*s:WW+l*&lt;span&gt;s] 
&lt;/span&gt;&lt;span&gt;274&lt;/span&gt;                     out[i, j, k, l] =&lt;span&gt; np.max(window)
&lt;/span&gt;&lt;span&gt;275&lt;/span&gt; 
&lt;span&gt;276&lt;/span&gt;     cache =&lt;span&gt; (x, pool_param)
&lt;/span&gt;&lt;span&gt;277&lt;/span&gt; 
&lt;span&gt;278&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; out, cache
&lt;/span&gt;&lt;span&gt;279&lt;/span&gt; 
&lt;span&gt;280&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;281&lt;/span&gt; &lt;span&gt;反向传播之池化：增大特征尺寸大小
&lt;/span&gt;&lt;span&gt;282&lt;/span&gt; &lt;span&gt;在缓存中取出前向池化时输入特征，选取某一范围矩阵窗口，
&lt;/span&gt;&lt;span&gt;283&lt;/span&gt; &lt;span&gt;找出最大值所在的位置，根据这个位置将dout值映射到新的矩阵对应位置上，
&lt;/span&gt;&lt;span&gt;284&lt;/span&gt; &lt;span&gt;而新矩阵其他位置都初始化为0.
&lt;/span&gt;&lt;span&gt;285&lt;/span&gt; &lt;span&gt;输入dout:2*32*16*16
&lt;/span&gt;&lt;span&gt;286&lt;/span&gt; &lt;span&gt;输出dx:2*32*32*32
&lt;/span&gt;&lt;span&gt;287&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;288&lt;/span&gt; &lt;span&gt;def&lt;/span&gt;&lt;span&gt; max_pool_backward_naive(dout, cache):
&lt;/span&gt;&lt;span&gt;289&lt;/span&gt;     x, pool_param =&lt;span&gt; cache
&lt;/span&gt;&lt;span&gt;290&lt;/span&gt;     HH, WW = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_height&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pool_width&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;291&lt;/span&gt;     s = pool_param[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;stride&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;292&lt;/span&gt;     N, C, H, W =&lt;span&gt; x.shape
&lt;/span&gt;&lt;span&gt;293&lt;/span&gt;     H_new = 1 + (H - HH) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;294&lt;/span&gt;     W_new = 1 + (W - WW) //&lt;span&gt; s
&lt;/span&gt;&lt;span&gt;295&lt;/span&gt;     dx =&lt;span&gt; np.zeros_like(x)
&lt;/span&gt;&lt;span&gt;296&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(N):    
&lt;/span&gt;&lt;span&gt;297&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(C):        
&lt;/span&gt;&lt;span&gt;298&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(H_new):            
&lt;/span&gt;&lt;span&gt;299&lt;/span&gt;                 &lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(W_new):
&lt;/span&gt;&lt;span&gt;300&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 取前向传播时输入的某一池化窗口&lt;/span&gt;
&lt;span&gt;301&lt;/span&gt;                     window = x[i, j, k*s:HH+k*s, l*s:WW+l*&lt;span&gt;s]
&lt;/span&gt;&lt;span&gt;302&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算窗口最大值&lt;/span&gt;
&lt;span&gt;303&lt;/span&gt;                     m =&lt;span&gt; np.max(window)
&lt;/span&gt;&lt;span&gt;304&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 根据最大值所在位置以及dout对应值=&amp;gt;新矩阵窗口数值&lt;/span&gt;
&lt;span&gt;305&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; [false,false&lt;/span&gt;
&lt;span&gt;306&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;  true, false]  * 1 =&amp;gt; [0,0&lt;/span&gt;
&lt;span&gt;307&lt;/span&gt;                     &lt;span&gt;#&lt;/span&gt;&lt;span&gt;                        1,0]&lt;/span&gt;
&lt;span&gt;308&lt;/span&gt;                     dx[i, j, k*s:HH+k*s, l*s:WW+l*s] = (window == m) *&lt;span&gt; dout[i, j, k, l]
&lt;/span&gt;&lt;span&gt;309&lt;/span&gt; 
&lt;span&gt;310&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt; dx
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;optim.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('47862aed-8f71-4371-8d79-c39ad2879533')&quot; readability=&quot;47&quot;&gt;&lt;img id=&quot;code_img_closed_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('47862aed-8f71-4371-8d79-c39ad2879533',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_47862aed-8f71-4371-8d79-c39ad2879533&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;89&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; 
&lt;span&gt;  3&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; sgd(w, dw, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;  5&lt;/span&gt; &lt;span&gt;    Performs vanilla stochastic gradient descent.    
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt;  9&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 10&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt;     w -= config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *&lt;span&gt; dw    
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt; 
&lt;span&gt; 13&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; w, config
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;SGD:随机梯度下降：由梯度计算新的权重矩阵w
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; &lt;span&gt;sgd_momentum 是sgd的改进版，解决sgd更新不稳定，陷入局部最优的问题。
&lt;/span&gt;&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;增加一个动量因子momentum，可以在一定程度上增加稳定性，
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;从而学习地更快，并且还有一定摆脱局部最优的能力。
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; 
&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt; 21&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; sgd_momentum(w, dw, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 22&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;    Performs stochastic gradient descent with momentum.    
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;    - momentum: Scalar between 0 and 1 giving the momentum value.                
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt; &lt;span&gt;    Setting momentum = 0 reduces to sgd.    
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt; &lt;span&gt;    - velocity（速度）: A numpy array of the same shape as w and dw used to store a moving
&lt;/span&gt;&lt;span&gt; 29&lt;/span&gt; &lt;span&gt;    average of the gradients.   
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;   
&lt;span&gt; 31&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)   
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.9&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 34&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; config 如果存在属性velocity，则获取config['velocity']，否则获取np.zeros_like(w)&lt;/span&gt;
&lt;span&gt; 35&lt;/span&gt;     v = config.get(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;velocity&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(w))    
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt;     next_w =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt;     v = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;momentum&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] * v - config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *&lt;span&gt; dw    
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt;     next_w = w +&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;velocity&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; 
&lt;span&gt; 41&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; next_w, config
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; 
&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; rmsprop(x, dx, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;    Uses the RMSProp update rule, which uses a moving average of squared gradient    
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt; &lt;span&gt;    values to set adaptive per-parameter learning rates.    
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 48&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared                  
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; &lt;span&gt;    gradient cache.    
&lt;/span&gt;&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;    - epsilon: Small scalar used for smoothing to avoid dividing by zero.    
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    - cache: Moving average of second moments of gradients.   
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 54&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-2&lt;span&gt;)  
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;decay_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.99&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-8&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 58&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt;     next_x =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt;     cache = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 61&lt;/span&gt;     decay_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;decay_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 62&lt;/span&gt;     learning_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 63&lt;/span&gt;     epsilon = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt;     cache = decay_rate * cache + (1 - decay_rate) * (dx**2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt;     x += - learning_rate * dx / (np.sqrt(cache) +&lt;span&gt; epsilon)  
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;cache&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; cache    
&lt;/span&gt;&lt;span&gt; 67&lt;/span&gt;     next_x =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt; 68&lt;/span&gt; 
&lt;span&gt; 69&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; next_x, config
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt; 
&lt;span&gt; 71&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; adam(x, dx, config=&lt;span&gt;None):    
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 73&lt;/span&gt; &lt;span&gt;    Uses the Adam update rule, which incorporates moving averages of both the  
&lt;/span&gt;&lt;span&gt; 74&lt;/span&gt; &lt;span&gt;    gradient and its square and a bias correction term.    
&lt;/span&gt;&lt;span&gt; 75&lt;/span&gt; &lt;span&gt;    config format:    
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt; &lt;span&gt;    - learning_rate: Scalar learning rate.    
&lt;/span&gt;&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    - beta1: Decay rate for moving average of first moment of gradient.    
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt; &lt;span&gt;    - beta2: Decay rate for moving average of second moment of gradient.   
&lt;/span&gt;&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    - epsilon: Small scalar used for smoothing to avoid dividing by zero.    
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    - m: Moving average of gradient.    
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - v: Moving average of squared gradient.    
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;    - t: Iteration number.   
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;    
&lt;span&gt; 84&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; config &lt;span&gt;is&lt;/span&gt; None: config =&lt;span&gt; {}    
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-3&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.9&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 87&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 0.999&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1e-8&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, np.zeros_like(x))    
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt;     config.setdefault(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, 0)   
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt;     next_x =&lt;span&gt; None    
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt;     m = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt;     v = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt;     beta1 = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta1&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt;     beta2 = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;beta2&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt;     learning_rate = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt;     epsilon = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;epsilon&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]   
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt;     t = config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]    
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt;     t += 1    
&lt;span&gt;101&lt;/span&gt;     m = beta1 * m + (1 - beta1) *&lt;span&gt; dx    
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt;     v = beta2 * v + (1 - beta2) * (dx**2&lt;span&gt;)    
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt;     m_bias = m / (1 - beta1**&lt;span&gt;t)    
&lt;/span&gt;&lt;span&gt;104&lt;/span&gt;     v_bias = v / (1 - beta2**&lt;span&gt;t)    
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt;     x += - learning_rate * m_bias / (np.sqrt(v_bias) +&lt;span&gt; epsilon)    
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     next_x =&lt;span&gt; x    
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;m&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; m    
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;v&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; v    
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt;     config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] =&lt;span&gt; t    
&lt;/span&gt;&lt;span&gt;110&lt;/span&gt; 
&lt;span&gt;111&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt; next_x, config
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;solver.py&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('c971387e-35cb-4089-826b-f2676488aa6c')&quot; readability=&quot;81&quot;&gt;&lt;img id=&quot;code_img_closed_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('c971387e-35cb-4089-826b-f2676488aa6c',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_c971387e-35cb-4089-826b-f2676488aa6c&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;157&quot;&gt;
&lt;pre&gt;
&lt;span&gt;  1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;  2&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;  3&lt;/span&gt;   &lt;span&gt;from&lt;/span&gt; . &lt;span&gt;import&lt;/span&gt;&lt;span&gt; optim
&lt;/span&gt;&lt;span&gt;  4&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception:
&lt;/span&gt;&lt;span&gt;  5&lt;/span&gt;   &lt;span&gt;import&lt;/span&gt;&lt;span&gt; optim
&lt;/span&gt;&lt;span&gt;  6&lt;/span&gt; 
&lt;span&gt;  7&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; Solver(object):
&lt;/span&gt;&lt;span&gt;  8&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;  9&lt;/span&gt; &lt;span&gt;  A Solver encapsulates all the logic necessary for training classification
&lt;/span&gt;&lt;span&gt; 10&lt;/span&gt; &lt;span&gt;  models. The Solver performs stochastic gradient descent using different
&lt;/span&gt;&lt;span&gt; 11&lt;/span&gt; &lt;span&gt;  update rules defined in optim.py.
&lt;/span&gt;&lt;span&gt; 12&lt;/span&gt; 
&lt;span&gt; 13&lt;/span&gt; &lt;span&gt;  The solver accepts both training and validataion data and labels so it can
&lt;/span&gt;&lt;span&gt; 14&lt;/span&gt; &lt;span&gt;  periodically check classification accuracy on both training and validation
&lt;/span&gt;&lt;span&gt; 15&lt;/span&gt; &lt;span&gt;  data to watch out for overfitting.
&lt;/span&gt;&lt;span&gt; 16&lt;/span&gt; 
&lt;span&gt; 17&lt;/span&gt; &lt;span&gt;  To train a model, you will first construct a Solver instance, passing the
&lt;/span&gt;&lt;span&gt; 18&lt;/span&gt; &lt;span&gt;  model, dataset, and various optoins (learning rate, batch size, etc) to the
&lt;/span&gt;&lt;span&gt; 19&lt;/span&gt; &lt;span&gt;  constructor. You will then call the train() method to run the optimization
&lt;/span&gt;&lt;span&gt; 20&lt;/span&gt; &lt;span&gt;  procedure and train the model.
&lt;/span&gt;&lt;span&gt; 21&lt;/span&gt;   
&lt;span&gt; 22&lt;/span&gt; &lt;span&gt;  After the train() method returns, model.params will contain the parameters
&lt;/span&gt;&lt;span&gt; 23&lt;/span&gt; &lt;span&gt;  that performed best on the validation set over the course of training.
&lt;/span&gt;&lt;span&gt; 24&lt;/span&gt; &lt;span&gt;  In addition, the instance variable solver.loss_history will contain a list
&lt;/span&gt;&lt;span&gt; 25&lt;/span&gt; &lt;span&gt;  of all losses encountered during training and the instance variables
&lt;/span&gt;&lt;span&gt; 26&lt;/span&gt; &lt;span&gt;  solver.train_acc_history and solver.val_acc_history will be lists containing
&lt;/span&gt;&lt;span&gt; 27&lt;/span&gt; &lt;span&gt;  the accuracies of the model on the training and validation set at each epoch.
&lt;/span&gt;&lt;span&gt; 28&lt;/span&gt;   
&lt;span&gt; 29&lt;/span&gt; &lt;span&gt;  Example usage might look something like this:
&lt;/span&gt;&lt;span&gt; 30&lt;/span&gt;   
&lt;span&gt; 31&lt;/span&gt; &lt;span&gt;  data = {
&lt;/span&gt;&lt;span&gt; 32&lt;/span&gt; &lt;span&gt;    'X_train': # training data
&lt;/span&gt;&lt;span&gt; 33&lt;/span&gt; &lt;span&gt;    'y_train': # training labels
&lt;/span&gt;&lt;span&gt; 34&lt;/span&gt; &lt;span&gt;    'X_val': # validation data
&lt;/span&gt;&lt;span&gt; 35&lt;/span&gt; &lt;span&gt;    'X_train': # validation labels
&lt;/span&gt;&lt;span&gt; 36&lt;/span&gt; &lt;span&gt;  }
&lt;/span&gt;&lt;span&gt; 37&lt;/span&gt; &lt;span&gt;  model = MyAwesomeModel(hidden_size=100, reg=10)
&lt;/span&gt;&lt;span&gt; 38&lt;/span&gt; &lt;span&gt;  solver = Solver(model, data,
&lt;/span&gt;&lt;span&gt; 39&lt;/span&gt; &lt;span&gt;                  update_rule='sgd',
&lt;/span&gt;&lt;span&gt; 40&lt;/span&gt; &lt;span&gt;                  optim_config={
&lt;/span&gt;&lt;span&gt; 41&lt;/span&gt; &lt;span&gt;                    'learning_rate': 1e-3,
&lt;/span&gt;&lt;span&gt; 42&lt;/span&gt; &lt;span&gt;                  },
&lt;/span&gt;&lt;span&gt; 43&lt;/span&gt; &lt;span&gt;                  lr_decay=0.95,
&lt;/span&gt;&lt;span&gt; 44&lt;/span&gt; &lt;span&gt;                  num_epochs=10, batch_size=100,
&lt;/span&gt;&lt;span&gt; 45&lt;/span&gt; &lt;span&gt;                  print_every=100)
&lt;/span&gt;&lt;span&gt; 46&lt;/span&gt; &lt;span&gt;  solver.train()
&lt;/span&gt;&lt;span&gt; 47&lt;/span&gt; 
&lt;span&gt; 48&lt;/span&gt; 
&lt;span&gt; 49&lt;/span&gt; &lt;span&gt;  A Solver works on a model object that must conform to the following API:
&lt;/span&gt;&lt;span&gt; 50&lt;/span&gt; 
&lt;span&gt; 51&lt;/span&gt; &lt;span&gt;  - model.params must be a dictionary mapping string parameter names to numpy
&lt;/span&gt;&lt;span&gt; 52&lt;/span&gt; &lt;span&gt;    arrays containing parameter values.
&lt;/span&gt;&lt;span&gt; 53&lt;/span&gt; 
&lt;span&gt; 54&lt;/span&gt; &lt;span&gt;  - model.loss(X, y) must be a function that computes training-time loss and
&lt;/span&gt;&lt;span&gt; 55&lt;/span&gt; &lt;span&gt;    gradients, and test-time classification scores, with the following inputs
&lt;/span&gt;&lt;span&gt; 56&lt;/span&gt; &lt;span&gt;    and outputs:
&lt;/span&gt;&lt;span&gt; 57&lt;/span&gt; 
&lt;span&gt; 58&lt;/span&gt; &lt;span&gt;    Inputs:
&lt;/span&gt;&lt;span&gt; 59&lt;/span&gt; &lt;span&gt;    - X: Array giving a minibatch of input data of shape (N, d_1, ..., d_k)
&lt;/span&gt;&lt;span&gt; 60&lt;/span&gt; &lt;span&gt;    - y: Array of labels, of shape (N,) giving labels for X where y[i] is the
&lt;/span&gt;&lt;span&gt; 61&lt;/span&gt; &lt;span&gt;      label for X[i].
&lt;/span&gt;&lt;span&gt; 62&lt;/span&gt; 
&lt;span&gt; 63&lt;/span&gt; &lt;span&gt;    Returns:
&lt;/span&gt;&lt;span&gt; 64&lt;/span&gt; &lt;span&gt;    If y is None, run a test-time forward pass and return:
&lt;/span&gt;&lt;span&gt; 65&lt;/span&gt; &lt;span&gt;    - scores: Array of shape (N, C) giving classification scores for X where
&lt;/span&gt;&lt;span&gt; 66&lt;/span&gt; &lt;span&gt;      scores[i, c] gives the score of class c for X[i].
&lt;/span&gt;&lt;span&gt; 67&lt;/span&gt; 
&lt;span&gt; 68&lt;/span&gt; &lt;span&gt;    If y is not None, run a training time forward and backward pass and return
&lt;/span&gt;&lt;span&gt; 69&lt;/span&gt; &lt;span&gt;    a tuple of:
&lt;/span&gt;&lt;span&gt; 70&lt;/span&gt; &lt;span&gt;    - loss: Scalar giving the loss
&lt;/span&gt;&lt;span&gt; 71&lt;/span&gt; &lt;span&gt;    - grads: Dictionary with the same keys as self.params mapping parameter
&lt;/span&gt;&lt;span&gt; 72&lt;/span&gt; &lt;span&gt;      names to gradients of the loss with respect to those parameters.
&lt;/span&gt;&lt;span&gt; 73&lt;/span&gt;   &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 74&lt;/span&gt; 
&lt;span&gt; 75&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;(self, model, data, **&lt;span&gt;kwargs):
&lt;/span&gt;&lt;span&gt; 76&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 77&lt;/span&gt; &lt;span&gt;    Construct a new Solver instance.
&lt;/span&gt;&lt;span&gt; 78&lt;/span&gt;     
&lt;span&gt; 79&lt;/span&gt; &lt;span&gt;    Required arguments:
&lt;/span&gt;&lt;span&gt; 80&lt;/span&gt; &lt;span&gt;    - model: A model object conforming to the API described above
&lt;/span&gt;&lt;span&gt; 81&lt;/span&gt; &lt;span&gt;    - data: A dictionary of training and validation data with the following:
&lt;/span&gt;&lt;span&gt; 82&lt;/span&gt; &lt;span&gt;      'X_train': Array of shape (N_train, d_1, ..., d_k) giving training images
&lt;/span&gt;&lt;span&gt; 83&lt;/span&gt; &lt;span&gt;      'X_val': Array of shape (N_val, d_1, ..., d_k) giving validation images
&lt;/span&gt;&lt;span&gt; 84&lt;/span&gt; &lt;span&gt;      'y_train': Array of shape (N_train,) giving labels for training images
&lt;/span&gt;&lt;span&gt; 85&lt;/span&gt; &lt;span&gt;      'y_val': Array of shape (N_val,) giving labels for validation images
&lt;/span&gt;&lt;span&gt; 86&lt;/span&gt;       
&lt;span&gt; 87&lt;/span&gt; &lt;span&gt;    Optional arguments:
&lt;/span&gt;&lt;span&gt; 88&lt;/span&gt; &lt;span&gt;    - update_rule: A string giving the name of an update rule in optim.py.
&lt;/span&gt;&lt;span&gt; 89&lt;/span&gt; &lt;span&gt;      Default is 'sgd'.
&lt;/span&gt;&lt;span&gt; 90&lt;/span&gt; &lt;span&gt;    - optim_config: A dictionary containing hyperparameters that will be
&lt;/span&gt;&lt;span&gt; 91&lt;/span&gt; &lt;span&gt;      passed to the chosen update rule. Each update rule requires different
&lt;/span&gt;&lt;span&gt; 92&lt;/span&gt; &lt;span&gt;      hyperparameters (see optim.py) but all update rules require a
&lt;/span&gt;&lt;span&gt; 93&lt;/span&gt; &lt;span&gt;      'learning_rate' parameter so that should always be present.
&lt;/span&gt;&lt;span&gt; 94&lt;/span&gt; &lt;span&gt;    - lr_decay: A scalar for learning rate decay; after each epoch the learning
&lt;/span&gt;&lt;span&gt; 95&lt;/span&gt; &lt;span&gt;      rate is multiplied by this value.
&lt;/span&gt;&lt;span&gt; 96&lt;/span&gt; &lt;span&gt;    - batch_size: Size of minibatches used to compute loss and gradient during
&lt;/span&gt;&lt;span&gt; 97&lt;/span&gt; &lt;span&gt;      training.
&lt;/span&gt;&lt;span&gt; 98&lt;/span&gt; &lt;span&gt;    - num_epochs: The number of epochs to run for during training.
&lt;/span&gt;&lt;span&gt; 99&lt;/span&gt; &lt;span&gt;    - print_every: Integer; training losses will be printed every print_every
&lt;/span&gt;&lt;span&gt;100&lt;/span&gt; &lt;span&gt;      iterations.
&lt;/span&gt;&lt;span&gt;101&lt;/span&gt; &lt;span&gt;    - verbose: Boolean; if set to false then no output will be printed during
&lt;/span&gt;&lt;span&gt;102&lt;/span&gt; &lt;span&gt;      training.
&lt;/span&gt;&lt;span&gt;103&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;104&lt;/span&gt;     self.model =&lt;span&gt; model
&lt;/span&gt;&lt;span&gt;105&lt;/span&gt;     self.X_train = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;106&lt;/span&gt;     self.y_train = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;107&lt;/span&gt;     self.X_val = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;X_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;108&lt;/span&gt;     self.y_val = data[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;y_val&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
&lt;/span&gt;&lt;span&gt;109&lt;/span&gt;     
&lt;span&gt;110&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Unpack keyword arguments&lt;/span&gt;
&lt;span&gt;111&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pop(key, default):删除kwargs对象中key，如果存在该key，返回该key对应的value，否则，返回default值。&lt;/span&gt;
&lt;span&gt;112&lt;/span&gt;     self.update_rule = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;update_rule&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, &lt;span&gt;'&lt;/span&gt;&lt;span&gt;sgd&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;113&lt;/span&gt;     self.optim_config = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;optim_config&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, {})
&lt;/span&gt;&lt;span&gt;114&lt;/span&gt;     self.lr_decay = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;lr_decay&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 1.0&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;115&lt;/span&gt;     self.batch_size = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;batch_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 2&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;116&lt;/span&gt;     self.num_epochs = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;num_epochs&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 10&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;117&lt;/span&gt; 
&lt;span&gt;118&lt;/span&gt;     self.print_every = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;print_every&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, 10&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;119&lt;/span&gt;     self.verbose = kwargs.pop(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;verbose&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, True)
&lt;/span&gt;&lt;span&gt;120&lt;/span&gt; 
&lt;span&gt;121&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Throw an error if there are extra keyword arguments&lt;/span&gt;
&lt;span&gt;122&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 删除kwargs中参数后，校验是否还有多余参数&lt;/span&gt;
&lt;span&gt;123&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; len(kwargs) &amp;gt;&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;124&lt;/span&gt;       extra = &lt;span&gt;'&lt;/span&gt;&lt;span&gt;, &lt;/span&gt;&lt;span&gt;'&lt;/span&gt;.join(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;&quot;%s&quot;&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; % k &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; kwargs.keys())
&lt;/span&gt;&lt;span&gt;125&lt;/span&gt;       &lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Unrecognized arguments %s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; extra)
&lt;/span&gt;&lt;span&gt;126&lt;/span&gt; 
&lt;span&gt;127&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make sure the update rule exists, then replace the string&lt;/span&gt;
&lt;span&gt;128&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; name with the actual function&lt;/span&gt;
&lt;span&gt;129&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 检查optim对象中是否有属性或方法名为self.update_rule&lt;/span&gt;
&lt;span&gt;130&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; hasattr(optim, self.update_rule):
&lt;/span&gt;&lt;span&gt;131&lt;/span&gt;       &lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Invalid update_rule &quot;%s&quot;&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; self.update_rule)
&lt;/span&gt;&lt;span&gt;132&lt;/span&gt;     self.update_rule =&lt;span&gt; getattr(optim, self.update_rule)
&lt;/span&gt;&lt;span&gt;133&lt;/span&gt; 
&lt;span&gt;134&lt;/span&gt; &lt;span&gt;    self._reset()
&lt;/span&gt;&lt;span&gt;135&lt;/span&gt; 
&lt;span&gt;136&lt;/span&gt; 
&lt;span&gt;137&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; _reset(self):
&lt;/span&gt;&lt;span&gt;138&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;139&lt;/span&gt; &lt;span&gt;    Set up some book-keeping variables for optimization. Don't call this
&lt;/span&gt;&lt;span&gt;140&lt;/span&gt; &lt;span&gt;    manually.
&lt;/span&gt;&lt;span&gt;141&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;142&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Set up some variables for book-keeping&lt;/span&gt;
&lt;span&gt;143&lt;/span&gt;     self.epoch =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;144&lt;/span&gt;     self.best_val_acc =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;145&lt;/span&gt;     self.best_params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;146&lt;/span&gt;     self.loss_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;147&lt;/span&gt;     self.train_acc_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;148&lt;/span&gt;     self.val_acc_history =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;149&lt;/span&gt; 
&lt;span&gt;150&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make a deep copy of the optim_config for each parameter&lt;/span&gt;
&lt;span&gt;151&lt;/span&gt;     self.optim_configs =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;152&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; p &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params:
&lt;/span&gt;&lt;span&gt;153&lt;/span&gt;       d = {k: v &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.optim_config.items()}
&lt;/span&gt;&lt;span&gt;154&lt;/span&gt;       self.optim_configs[p] =&lt;span&gt; d
&lt;/span&gt;&lt;span&gt;155&lt;/span&gt; 
&lt;span&gt;156&lt;/span&gt; 
&lt;span&gt;157&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; _step(self):
&lt;/span&gt;&lt;span&gt;158&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;159&lt;/span&gt; &lt;span&gt;    Make a single gradient update. This is called by train() and should not
&lt;/span&gt;&lt;span&gt;160&lt;/span&gt; &lt;span&gt;    be called manually.
&lt;/span&gt;&lt;span&gt;161&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;162&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Make a minibatch of training data&lt;/span&gt;
&lt;span&gt;163&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 500 张图片&lt;/span&gt;
&lt;span&gt;164&lt;/span&gt;     num_train =&lt;span&gt; self.X_train.shape[0]
&lt;/span&gt;&lt;span&gt;165&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 随机选出batch_size：2 张&lt;/span&gt;
&lt;span&gt;166&lt;/span&gt;     batch_mask =&lt;span&gt; np.random.choice(num_train, self.batch_size)
&lt;/span&gt;&lt;span&gt;167&lt;/span&gt; 
&lt;span&gt;168&lt;/span&gt;    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; batch_mask = [t%(num_train//2), num_train//2 + t%(num_train//2)]&lt;/span&gt;
&lt;span&gt;169&lt;/span&gt; 
&lt;span&gt;170&lt;/span&gt; 
&lt;span&gt;171&lt;/span&gt; 
&lt;span&gt;172&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练样本矩阵[2,3,32,32]&lt;/span&gt;
&lt;span&gt;173&lt;/span&gt;     X_batch =&lt;span&gt; self.X_train[batch_mask]
&lt;/span&gt;&lt;span&gt;174&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 标签矩阵[2,] 图片类型&lt;/span&gt;
&lt;span&gt;175&lt;/span&gt;     y_batch =&lt;span&gt; self.y_train[batch_mask]
&lt;/span&gt;&lt;span&gt;176&lt;/span&gt; 
&lt;span&gt;177&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Compute loss and gradient&lt;/span&gt;
&lt;span&gt;178&lt;/span&gt;     loss, grads =&lt;span&gt; self.model.loss(X_batch, y_batch)
&lt;/span&gt;&lt;span&gt;179&lt;/span&gt; &lt;span&gt;    self.loss_history.append(loss)
&lt;/span&gt;&lt;span&gt;180&lt;/span&gt; 
&lt;span&gt;181&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 更新模型超参(w1,b1),(w2,b2),(w3,b3)，以及保存更新超参时对应参数因子&lt;/span&gt;
&lt;span&gt;182&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Perform a parameter update&lt;/span&gt;
&lt;span&gt;183&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; p, w &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params.items():
&lt;/span&gt;&lt;span&gt;184&lt;/span&gt;       dw =&lt;span&gt; grads[p]
&lt;/span&gt;&lt;span&gt;185&lt;/span&gt;       config =&lt;span&gt; self.optim_configs[p]
&lt;/span&gt;&lt;span&gt;186&lt;/span&gt;       next_w, next_config =&lt;span&gt; self.update_rule(w, dw, config)
&lt;/span&gt;&lt;span&gt;187&lt;/span&gt;       self.model.params[p] =&lt;span&gt; next_w
&lt;/span&gt;&lt;span&gt;188&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 保存参数因子，learning_rate(学习率)，velocity(速度)&lt;/span&gt;
&lt;span&gt;189&lt;/span&gt;       self.optim_configs[p] =&lt;span&gt; next_config
&lt;/span&gt;&lt;span&gt;190&lt;/span&gt; 
&lt;span&gt;191&lt;/span&gt; 
&lt;span&gt;192&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt; check_accuracy(self, X, y, num_samples=None, batch_size=2&lt;span&gt;):
&lt;/span&gt;&lt;span&gt;193&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;194&lt;/span&gt; &lt;span&gt;    Check accuracy of the model on the provided data.
&lt;/span&gt;&lt;span&gt;195&lt;/span&gt;     
&lt;span&gt;196&lt;/span&gt; &lt;span&gt;    Inputs:
&lt;/span&gt;&lt;span&gt;197&lt;/span&gt; &lt;span&gt;    - X: Array of data, of shape (N, d_1, ..., d_k)
&lt;/span&gt;&lt;span&gt;198&lt;/span&gt; &lt;span&gt;    - y: Array of labels, of shape (N,)
&lt;/span&gt;&lt;span&gt;199&lt;/span&gt; &lt;span&gt;    - num_samples: If not None, subsample the data and only test the model
&lt;/span&gt;&lt;span&gt;200&lt;/span&gt; &lt;span&gt;      on num_samples datapoints.
&lt;/span&gt;&lt;span&gt;201&lt;/span&gt; &lt;span&gt;    - batch_size: Split X and y into batches of this size to avoid using too
&lt;/span&gt;&lt;span&gt;202&lt;/span&gt; &lt;span&gt;      much memory.
&lt;/span&gt;&lt;span&gt;203&lt;/span&gt;       
&lt;span&gt;204&lt;/span&gt; &lt;span&gt;    Returns:
&lt;/span&gt;&lt;span&gt;205&lt;/span&gt; &lt;span&gt;    - acc: Scalar giving the fraction of instances that were correctly
&lt;/span&gt;&lt;span&gt;206&lt;/span&gt; &lt;span&gt;      classified by the model.
&lt;/span&gt;&lt;span&gt;207&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;208&lt;/span&gt;     
&lt;span&gt;209&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Maybe subsample the data&lt;/span&gt;
&lt;span&gt;210&lt;/span&gt;     N =&lt;span&gt; X.shape[0]
&lt;/span&gt;&lt;span&gt;211&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; num_samples &lt;span&gt;is&lt;/span&gt; &lt;span&gt;not&lt;/span&gt; None &lt;span&gt;and&lt;/span&gt; N &amp;gt;&lt;span&gt; num_samples:
&lt;/span&gt;&lt;span&gt;212&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 随机选取num_samples张图片，返回选取图片索引&lt;/span&gt;
&lt;span&gt;213&lt;/span&gt;       mask =&lt;span&gt; np.random.choice(N, num_samples)
&lt;/span&gt;&lt;span&gt;214&lt;/span&gt;       N =&lt;span&gt; num_samples
&lt;/span&gt;&lt;span&gt;215&lt;/span&gt;       X =&lt;span&gt; X[mask]
&lt;/span&gt;&lt;span&gt;216&lt;/span&gt;       y =&lt;span&gt; y[mask]
&lt;/span&gt;&lt;span&gt;217&lt;/span&gt; 
&lt;span&gt;218&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Compute predictions in batches&lt;/span&gt;
&lt;span&gt;219&lt;/span&gt;     num_batches = N //&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;220&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; N % batch_size !=&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;221&lt;/span&gt;       num_batches += 1
&lt;span&gt;222&lt;/span&gt;     y_pred =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;223&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(num_batches):
&lt;/span&gt;&lt;span&gt;224&lt;/span&gt;       start = i *&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;225&lt;/span&gt;       end = (i + 1) *&lt;span&gt; batch_size
&lt;/span&gt;&lt;span&gt;226&lt;/span&gt;       scores =&lt;span&gt; self.model.loss(X[start:end])
&lt;/span&gt;&lt;span&gt;227&lt;/span&gt;       y_pred.append(np.argmax(scores, axis=1&lt;span&gt;))
&lt;/span&gt;&lt;span&gt;228&lt;/span&gt;     y_pred =&lt;span&gt; np.hstack(y_pred)
&lt;/span&gt;&lt;span&gt;229&lt;/span&gt;     acc = np.mean(y_pred ==&lt;span&gt; y)
&lt;/span&gt;&lt;span&gt;230&lt;/span&gt; 
&lt;span&gt;231&lt;/span&gt;     &lt;span&gt;return&lt;/span&gt;&lt;span&gt; acc
&lt;/span&gt;&lt;span&gt;232&lt;/span&gt; 
&lt;span&gt;233&lt;/span&gt;   &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;234&lt;/span&gt; &lt;span&gt;   训练模型：核心方法
&lt;/span&gt;&lt;span&gt;235&lt;/span&gt; &lt;span&gt;   epoch &amp;gt; batch_size &amp;gt; iteration &amp;gt;= 1
&lt;/span&gt;&lt;span&gt;236&lt;/span&gt; &lt;span&gt;   训练总的次数 = num_epochs * iterations_per_epoch
&lt;/span&gt;&lt;span&gt;237&lt;/span&gt;   &lt;span&gt;'''&lt;/span&gt;
&lt;span&gt;238&lt;/span&gt;   &lt;span&gt;def&lt;/span&gt;&lt;span&gt; train(self):
&lt;/span&gt;&lt;span&gt;239&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;240&lt;/span&gt; &lt;span&gt;    Run optimization to train the model.
&lt;/span&gt;&lt;span&gt;241&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;242&lt;/span&gt;     num_train =&lt;span&gt; self.X_train.shape[0]
&lt;/span&gt;&lt;span&gt;243&lt;/span&gt;     iterations_per_epoch = max(num_train // self.batch_size, 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;244&lt;/span&gt;     num_iterations = self.num_epochs *&lt;span&gt; iterations_per_epoch
&lt;/span&gt;&lt;span&gt;245&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 迭代总的次数&lt;/span&gt;
&lt;span&gt;246&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt; t &lt;span&gt;in&lt;/span&gt;&lt;span&gt; range(num_iterations):
&lt;/span&gt;&lt;span&gt;247&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 某次iteration训练&lt;/span&gt;
&lt;span&gt;248&lt;/span&gt; &lt;span&gt;      self._step()
&lt;/span&gt;&lt;span&gt;249&lt;/span&gt; 
&lt;span&gt;250&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Maybe print training loss&lt;/span&gt;
&lt;span&gt;251&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; verbose：是否显示详细信息&lt;/span&gt;
&lt;span&gt;252&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt; self.verbose &lt;span&gt;and&lt;/span&gt; t % self.print_every ==&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;253&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;(Iteration %d / %d) loss: %f&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (
&lt;/span&gt;&lt;span&gt;254&lt;/span&gt;                t + 1, num_iterations, self.loss_history[-1&lt;span&gt;]))
&lt;/span&gt;&lt;span&gt;255&lt;/span&gt; 
&lt;span&gt;256&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; At the end of every epoch, increment the epoch counter and decay the&lt;/span&gt;
&lt;span&gt;257&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; learning rate.&lt;/span&gt;
&lt;span&gt;258&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 每迭代完一次epoch后，更新学习率learning_rate，加快运算效率。&lt;/span&gt;
&lt;span&gt;259&lt;/span&gt;       epoch_end = (t + 1) % iterations_per_epoch ==&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;260&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt;&lt;span&gt; epoch_end:
&lt;/span&gt;&lt;span&gt;261&lt;/span&gt;         self.epoch += 1
&lt;span&gt;262&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.optim_configs:
&lt;/span&gt;&lt;span&gt;263&lt;/span&gt;           self.optim_configs[k][&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] *=&lt;span&gt; self.lr_decay
&lt;/span&gt;&lt;span&gt;264&lt;/span&gt; 
&lt;span&gt;265&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Check train and val accuracy on the first iteration, the last&lt;/span&gt;
&lt;span&gt;266&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; iteration, and at the end of each epoch.&lt;/span&gt;
&lt;span&gt;267&lt;/span&gt;       &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 在第1次迭代，最后1次迭代，或者运行完一个epoch后，校验训练结果。&lt;/span&gt;
&lt;span&gt;268&lt;/span&gt;       first_it = (t ==&lt;span&gt; 0)
&lt;/span&gt;&lt;span&gt;269&lt;/span&gt;       last_it = (t == num_iterations + 1&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;270&lt;/span&gt;       &lt;span&gt;if&lt;/span&gt; first_it &lt;span&gt;or&lt;/span&gt; last_it &lt;span&gt;or&lt;/span&gt;&lt;span&gt; epoch_end:
&lt;/span&gt;&lt;span&gt;271&lt;/span&gt;         train_acc =&lt;span&gt; self.check_accuracy(self.X_train, self.y_train,
&lt;/span&gt;&lt;span&gt;272&lt;/span&gt;                                         num_samples=4&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;273&lt;/span&gt;         val_acc = self.check_accuracy(self.X_val, self.y_val,num_samples=4&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;274&lt;/span&gt; &lt;span&gt;        self.train_acc_history.append(train_acc)
&lt;/span&gt;&lt;span&gt;275&lt;/span&gt; &lt;span&gt;        self.val_acc_history.append(val_acc)
&lt;/span&gt;&lt;span&gt;276&lt;/span&gt; 
&lt;span&gt;277&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt;&lt;span&gt; self.verbose:
&lt;/span&gt;&lt;span&gt;278&lt;/span&gt;           &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;'&lt;/span&gt;&lt;span&gt;(Epoch %d / %d) train acc: %f; val_acc: %f&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %&lt;span&gt; (
&lt;/span&gt;&lt;span&gt;279&lt;/span&gt; &lt;span&gt;                 self.epoch, self.num_epochs, train_acc, val_acc))
&lt;/span&gt;&lt;span&gt;280&lt;/span&gt; 
&lt;span&gt;281&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Keep track of the best model&lt;/span&gt;
&lt;span&gt;282&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; val_acc &amp;gt;&lt;span&gt; self.best_val_acc:
&lt;/span&gt;&lt;span&gt;283&lt;/span&gt;           self.best_val_acc =&lt;span&gt; val_acc
&lt;/span&gt;&lt;span&gt;284&lt;/span&gt;           self.best_params =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;285&lt;/span&gt;           &lt;span&gt;for&lt;/span&gt; k, v &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.model.params.items():
&lt;/span&gt;&lt;span&gt;286&lt;/span&gt;             self.best_params[k] =&lt;span&gt; v.copy()
&lt;/span&gt;&lt;span&gt;287&lt;/span&gt; 
&lt;span&gt;288&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; At the end of training swap the best params into the model&lt;/span&gt;
&lt;span&gt;289&lt;/span&gt;     self.model.params = self.best_params
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;h2&gt;&lt;span&gt;5.运行结果以及分析&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;这里选取500张图片作为训练样本，epoch = 5，batch = 2，每次随机选取2张图片，迭代 5 * 500/2 = 1250次，&lt;/span&gt;&lt;span&gt;测试样本选取50张。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由运行结果可以看出，损失loss是逐步下降的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222911134-710696995.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222926050-864003615.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/797382/201902/797382-20190225222944971-72881526.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;但是测试结果有点不尽人意，只有12%左右，某位大神说可以到&lt;/span&gt;54.7%&lt;span&gt;，下次把训练数据集以及测试数据集加大看看，大家也可以发表下高见啊。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;6. 参考文献&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;视觉一只白的博客《常用损失函数小结》&lt;a href=&quot;https://blog.csdn.net/zhangjunp3/article/details/80467350&quot; target=&quot;_blank&quot;&gt;https://blog.csdn.net/zhangjunp3/article/details/80467350&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;理想万岁的博客《Softmax函数详解与推导》：&lt;a href=&quot;http://www.cnblogs.com/zongfa/p/8971213.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/zongfa/p/8971213.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;下路派出所的博客《深度学习（九） 深度学习最全优化方法总结比较（SGD，Momentum，Nesterov Momentum，Adagrad，Adadelta，RMSprop，Adam）》&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;a href=&quot;http://www.cnblogs.com/callyblog/p/8299074.html&quot; target=&quot;_blank&quot;&gt;http://www.cnblogs.com/callyblog/p/8299074.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;



&lt;p&gt;不要让懒惰占据你的大脑，不要让妥协拖垮了你的人生。青春就是一张票，能不能赶上时代的快车，你的步伐就掌握在你的脚下。&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 14:34:00 +0000</pubDate>
<dc:creator>w_x_w1985</dc:creator>
<og:description>卷积神经网络(CNN)详解 本文系作者原创，转载请注明出处:https://www.cnblogs.com/further-further-further/p/10430073.html 目录 1.应</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/further-further-further/p/10430073.html</dc:identifier>
</item>
<item>
<title>pytorch模型部署在MacOS或者IOS - 一度逍遥</title>
<link>http://www.cnblogs.com/riddick/p/10434339.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/riddick/p/10434339.html</guid>
<description>&lt;p&gt;pytorch训练出.pth模型如何在MacOS上或者IOS部署，这是个问题。&lt;/p&gt;
&lt;p&gt;然而我们有了onnx，同样我们也有了coreML。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ONNX：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;onnx是一种针对机器学习设计的开放式文件格式，用来存储训练好的模型，并进行多种框架模型间的转换。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;coreML：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Apple在2017年 MacOS 10.13以及IOS11+系统上推出了coreML1.0，官网地址：&lt;a href=&quot;https://developer.apple.com/documentation/coreml&quot; target=&quot;_blank&quot;&gt;https://developer.apple.com/documentation/coreml&lt;/a&gt; 。&lt;/p&gt;
&lt;p&gt;2018年又推出MacOS 10.14以及IOS12系统上的coreML2.0  &lt;a href=&quot;https://www.appcoda.com/coreml2/&quot; target=&quot;_blank&quot;&gt;https://www.appcoda.com/coreml2/&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;coreML框架可以方便的进行深度学习模型的部署，利用模型进行预测，让深度学习可以在apple的移动设备上发光发热。而开发者需要做的仅仅是将model.mlModel拖进xcode工程，xcode工程会自动生成以模型名称命名的object-c类以及多种进行预测所需的类接口。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;pytorch -- ONNX -- coreML&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;没错，就是这个流程。我们有训练好的.pth模型，通过pytorch.onnx.export() 转化为 .onnx模型，然后利用 onnx_coreml.convert()将 .onnx转换为 .mlModel。将.mlModel拖进xcode工程编写预测代码就可以了。&lt;/p&gt;

&lt;p&gt;1.  pytorch -- ONNX&lt;/p&gt;
&lt;p&gt;请先查看pytorch官网的onnx模块：&lt;a href=&quot;https://pytorch.org/docs/stable/onnx.html&quot; target=&quot;_blank&quot;&gt;https://pytorch.org/docs/stable/onnx.html&lt;/a&gt;  。 主要的代码就这一个API, 各个参数意义请查阅文档。&lt;/p&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;torch&lt;span class=&quot;o&quot;&gt;.&lt;span class=&quot;n&quot;&gt;onnx&lt;span class=&quot;o&quot;&gt;.&lt;span class=&quot;n&quot;&gt;export&lt;span class=&quot;p&quot;&gt;(&lt;span class=&quot;n&quot;&gt;model&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;dummy_input&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;alexnet.onnx&quot;&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;verbose&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;kc&quot;&gt;True&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;input_names&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;n&quot;&gt;input_names&lt;span class=&quot;p&quot;&gt;, &lt;span class=&quot;n&quot;&gt;output_names&lt;span class=&quot;o&quot;&gt;=&lt;span class=&quot;n&quot; readability=&quot;1&quot;&gt;output_names&lt;span class=&quot;p&quot; readability=&quot;2&quot;&gt;)；&lt;p&gt;转换部分代码如下：&lt;br/&gt;&lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;45&quot;&gt;
&lt;pre&gt;
 batch_size=1&lt;span&gt;
 onnx_model_path &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;onnx_model.onnx&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
 dummy_input &lt;/span&gt;= V(torch.randn(batch_size, 3, 224, 224), requires_grad=&lt;span&gt;True)
 torch_out&lt;/span&gt;= torch.onnx.export(pytorch_model, dummy_input , onnx_model_path, verbose=True,input_names=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;image&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], output_names=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;outTensor&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], export_params=True, training=False )
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;kc&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt; 这里有一个需要注意的地方就是input_names和output_names的设置，如果不设置的情况，输入层和输出层pytorch会自动分配一个数字编号。比如下图(用netron工具查看，真是一个很好用的工具 &lt;a href=&quot;https://pypi.org/project/netron/&quot; target=&quot;_blank&quot;&gt;https://pypi.org/project/netron/&lt;/a&gt;)。 自动分配的输入名称和输出名称是0 和 199。 这样转换成coreML模型后加载到xcode中会出现&quot;initwith0&quot;这样的编译错误，就是模型初始化的时候不能正确处理这个输入名称0。因此最好是在export的时候将其修改一个名称。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;kc&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;o&quot;&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;p&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225215840757-1408564082.png&quot; alt=&quot;&quot; width=&quot;375&quot; height=&quot;615&quot;/&gt;  &lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225215928046-1587433274.png&quot; alt=&quot;&quot; width=&quot;416&quot; height=&quot;739&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;修改之后的模型是这样的，可以看到模型的输入和输出名称都发生的修改：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225220434449-1000784521.png&quot; alt=&quot;&quot; width=&quot;300&quot;/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1138496/201902/1138496-20190225220539951-728271199.png&quot; alt=&quot;&quot; width=&quot;300&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 2. onnx -- mlModel&lt;/p&gt;
&lt;p&gt;　　这一部分需要安装onnx, github地址： &lt;a href=&quot;https://github.com/onnx/onnx&quot; target=&quot;_blank&quot;&gt;https://github.com/onnx/onnx&lt;/a&gt;  以及安装一个转换工具onnx_coreML，github地址：&lt;a href=&quot;https://github.com/onnx/onnx-coreml&quot; target=&quot;_blank&quot;&gt;https://github.com/onnx/onnx-coreml&lt;/a&gt;  。里面用到了一个coremltools : &lt;a href=&quot;https://pypi.org/project/coremltools/&quot; target=&quot;_blank&quot;&gt;https://pypi.org/project/coremltools/&lt;/a&gt;，这个tool目前仅支持python2.7环境下使用。&lt;/p&gt;
&lt;p&gt;　　安装好后, import onnx ， import onnx_coreML 就可以使用。转换代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
onnx_model = onnx.load(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;onnx_model.onnx&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
cml_model&lt;/span&gt;=&lt;span&gt; onnx_coreml.convert(onnx_model)
cml_model.save(&lt;/span&gt;&lt;span&gt;&quot;coreML_model&lt;/span&gt;&lt;span&gt;.mlmodel&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt; 　　当然, onnx_coreml.convert有很多参数，可以用来预处理，设置bgr顺序等，请参看github文档介绍。&lt;/p&gt;
&lt;p&gt;　　现在将coreML_model.mlModel拖进xcode工程里，会自动生成一个coreML_model类，这个类有初始化模型，输入 预测 输出等API，编写预测代码即可。&lt;/p&gt;

&lt;p&gt;3. 在最新的coreML2.0中，支持模型的量化. coreML1.0中处理模型是32位，而在coreML2.0中可以将模型量化为16bit, 8bit, 4bit甚至是2bit。 具体请看apple WWDC视频以及PPT。&lt;/p&gt;
&lt;p&gt;  模型量化仍然是使用coreMLtool2.0工具，具体代码请查阅这篇博客，写的很详细：&lt;a href=&quot;https://www.jianshu.com/p/b6e3cb7338bf&quot; target=&quot;_blank&quot;&gt;https://www.jianshu.com/p/b6e3cb7338bf&lt;/a&gt;。 两句代码即可完成量化转换。&lt;/p&gt;

&lt;p&gt;时间仓促，写的粗糙，随后更新。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:28:00 +0000</pubDate>
<dc:creator>一度逍遥</dc:creator>
<og:description>pytorch训练出.pth模型如何在MacOS上或者IOS部署，这是个问题。 然而我们有了onnx，同样我们也有了coreML。 ONNX： onnx是一种针对机器学习设计的开放式文件格式，用来存储</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/riddick/p/10434339.html</dc:identifier>
</item>
<item>
<title>Yaml 文件中Condition If- else 判断的问题 - BUTTERAPPLE</title>
<link>http://www.cnblogs.com/xiyin/p/10434247.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xiyin/p/10434247.html</guid>
<description>&lt;p&gt;在做项目的CI/ CD 时，难免会用到 Travis.CI 和 AppVeyor 以及 CodeCov 来判断测试的覆盖率，今天突然遇到了一个问题，就是我需要在每次做测试的时候判断是否存在一个环境变量，我对于 script 脚本半只半解还不太懂的状态，我最初的打算是这样写的&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;if [-z $ENV_VALUE &amp;amp;&amp;amp; -z $ENV_VALUE]; then
    #do something
else
    #do another 
fi&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;没想到，在windows上跑着正常的，编写到 .yml 文件的 script 中时，在 GitHub 上就报错了，说&lt;code&gt;-z was unexcepted at this time.&lt;/code&gt; 看的我真的是一脸懵啊，什么鬼。去Google 了一下，也没有找到什么可靠的答案，于是我去翻了翻其他语言的项目中是如何写 &lt;code&gt;.yml&lt;/code&gt; 文件的，刚开始其实我也去看了看，只记得里面有个这个命令 &lt;code&gt;test -z $ENV_VALUE -a -z $ENV_VALUE&lt;/code&gt; 。开始没有太注意，后来发现这个 test 命令我在写脚本时怎么从来没见过呢，去Google 了一下，发现新大陆。&lt;/p&gt;
&lt;p&gt;这个 Test 命令的解释是:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;The test command can be used on the Linux command line to compare one element against another, but it is more commonly used in BASH shell scripts as part of conditional statements which control logic and program flow&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下来就简单介绍一下这个神器的用法，比如下面这个基础命令&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;test 1 -eq 2 &amp;amp;&amp;amp; echo &quot;yes&quot; || echo &quot;no&quot;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面这段命令的意思是 1 等于 2 吗？ 如果等于就输出 yes 否则输出 no 显然答案是 no&lt;br/&gt;&lt;img src=&quot;http://ww1.sinaimg.cn/large/69a9dfdbgy1g0j0lofbiyj209h022dfq.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;具体解剖开来就是&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;test 表示的你将要进行一个比较&lt;/li&gt;
&lt;li&gt;1 是你第一个要比较的数据&lt;/li&gt;
&lt;li&gt;-eq 表示 equal 就是等于&lt;/li&gt;
&lt;li&gt;2 是你第二个要比较的数据&lt;/li&gt;
&lt;li&gt;&amp;amp;&amp;amp; 这个符号后的语句会在表达式为 true时执行&lt;/li&gt;
&lt;li&gt;|| 这个符号后面的语句会在表达式为 false的时候执行&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较的是数字时，还可以又以下其他符号：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;-eq 还有其他比较的符号&lt;/li&gt;
&lt;li&gt;-ge ： 表示 大于等于&lt;/li&gt;
&lt;li&gt;-gt： great than 大于&lt;/li&gt;
&lt;li&gt;-le： less equal than 小于等于&lt;/li&gt;
&lt;li&gt;-lt： 小于&lt;/li&gt;
&lt;li&gt;-ne： 不等于&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较的是 Text时&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;= ：表示的是 string 1 和 stirng2 匹配，相等&lt;/li&gt;
&lt;li&gt;!= ：和上面相反&lt;/li&gt;
&lt;li&gt;-n ：表示这个字符串的长度大于 0&lt;/li&gt;
&lt;li&gt;-z： 表示这个字符串长度等于 0&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;当比较 string 类型或者是 file 文件都有相应的符号来进行比较，对于要在 bash 上脚本中进行比较很是方便。&lt;br/&gt;对于我之前想要在 .yml 文件中进行比较判断的语句，则可以写成以下这个样了：&lt;/p&gt;
&lt;pre class=&quot;script&quot;&gt;
&lt;code&gt;
test -z $ENV_VALUE1 -a -z $ENV_VALUE2 &amp;amp;&amp;amp; dotnet test --filter Category = category1 || dotnet test --filter Category = all

# -a 代表的是 and&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;是不是 So easy 啊，赶紧去试试！&lt;/p&gt;
&lt;p&gt;参考文章：&lt;br/&gt;&lt;a href=&quot;https://www.lifewire.com/test-linux-command-unix-command-4097166&quot;&gt;How to Use Test Conditions Within a Bash Script&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 14:11:00 +0000</pubDate>
<dc:creator>BUTTERAPPLE</dc:creator>
<og:description>在做项目的CI/ CD 时，难免会用到 Travis.CI 和 AppVeyor 以及 CodeCov 来判断测试的覆盖率，今天突然遇到了一个问题，就是我需要在每次做测试的时候判断是否存在一个环境变量</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/xiyin/p/10434247.html</dc:identifier>
</item>
<item>
<title>unity 简易场景切换 - U头</title>
<link>http://www.cnblogs.com/lihangppz/p/10434245.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/lihangppz/p/10434245.html</guid>
<description>&lt;p&gt;　　场景跳转是游戏里面不可缺少的内容，这里写写我这个小白遇到的问题。&lt;/p&gt;
&lt;p&gt;　　场景资源比较大，同步切换场景不现实。场景一般比较多，都加到scenes in build也不太现实。&lt;/p&gt;
&lt;p&gt;　　这里将初始场景和过渡场景加到scenes in build，一般而言是login和loading场景，将其他场景打成uab的包。&lt;/p&gt;
&lt;p&gt;　　这里忽略其他功能，单纯来说场景跳转。&lt;/p&gt;
&lt;p&gt;　　场景同步跳转SceneManager.LoadScene(&quot;loading&quot;, LoadSceneMode.Single);同步跳转有个条件需要加到scenes in build。loading场景做成一个很小的场景，并且带一个进度条。几乎是瞬间就可以加载完成，同时异步加载目标场景，用SceneManager.LoadSceneAsync(&quot;targetSence&quot;, LoadSceneMode.Single);这里也有条件，加到scenes in build，或者uab已被加载。&lt;/p&gt;
&lt;p&gt;　　在loagin场景，也就是初始场景上，挂上下面脚本：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; System.Collections;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine.SceneManagement;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; MainStart : MonoBehaviour
{
    &lt;/span&gt;&lt;span&gt;void&lt;/span&gt;&lt;span&gt; Start()
    {
        DontDestroyOnLoad(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;&lt;span&gt;);
        SceneManager.LoadScene(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;loading&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        StartCoroutine(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;LoadScene&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;异步加载&lt;/span&gt;
&lt;span&gt;    IEnumerator LoadScene()
    {
        WWW download &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; WWW(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;file:///&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target/targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt;&lt;span&gt; download;
        Debug.Log(download.ToString());
        AsyncOperation &lt;/span&gt;&lt;span&gt;async&lt;/span&gt; = SceneManager.LoadSceneAsync(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt; &lt;span&gt;async&lt;/span&gt;&lt;span&gt;;
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　资源打包部分就不细说了，代码放到Editor下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;40&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEditor;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; TestUab
{

    [MenuItem(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Assets/资源打包&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, &lt;span&gt;false&lt;/span&gt;, &lt;span&gt;63&lt;/span&gt;&lt;span&gt;)]
    &lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; testUab()
    {
        &lt;/span&gt;&lt;span&gt;string&lt;/span&gt; targetPath = Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        AssetBundleBuild abb &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt;&lt;span&gt; AssetBundleBuild();
        abb.assetBundleName &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        abb.assetNames &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; &lt;span&gt;string&lt;/span&gt;[&lt;span&gt;1&lt;/span&gt;&lt;span&gt;];
        abb.assetNames[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;] = &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Assets/TestUab/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence.unity&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
        AssetBundleBuild[] addArr &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; AssetBundleBuild[&lt;span&gt;1&lt;/span&gt;&lt;span&gt;];
        addArr[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;] =&lt;span&gt; abb;
        
        BuildPipeline.BuildAssetBundles(targetPath, addArr, BuildAssetBundleOptions.None, BuildTarget.Android);
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　将login，loading场景加入到scenes in build，并且打包目标场景后，开始运行，你会发现场景切换没有问题了，但是天空盒变成紫色了。刚开始以为材质丢了，点开Lighting界面，材质没丢，但是shader渲染出来的就是紫色。花了很长时间没解决，然后翻自己的线上工程，发现线上工程没有天空盒，一口老血喷到屏幕上了，最后还是请教老大，才知道问题。&lt;/p&gt;
&lt;p&gt;　　在打uab包时，用的是Android平台，BuildPipeline.BuildAssetBundles(targetPath, addArr, BuildAssetBundleOptions.None, BuildTarget.Android);所以打出来的uab包时Android平台下的，shader都是Android平台下的shader，不同平台shader渲染不一样。把dome工程打成apk，一看果然天空盒没问题，没有变成紫色。线上项目在场景加载完毕的时候会切换材质球的shader，以保证在编辑器下显示没有问题。最终代码如下：&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; System.Collections;
&lt;/span&gt;&lt;span&gt;using&lt;/span&gt;&lt;span&gt; UnityEngine.SceneManagement;

&lt;/span&gt;&lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; MainStart : MonoBehaviour
{

    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; Use this for initialization&lt;/span&gt;
    &lt;span&gt;void&lt;/span&gt;&lt;span&gt; Start()
    {
        DontDestroyOnLoad(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;&lt;span&gt;);
        SceneManager.LoadScene(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;loading&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        StartCoroutine(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;LoadScene&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;异步加载&lt;/span&gt;
&lt;span&gt;    IEnumerator LoadScene()
    {
        WWW download &lt;/span&gt;= &lt;span&gt;new&lt;/span&gt; WWW(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;file:///&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; + Application.dataPath + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/target/targetSence.uab&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt;&lt;span&gt; download;
        Debug.Log(download.ToString());
        AsyncOperation &lt;/span&gt;&lt;span&gt;async&lt;/span&gt; = SceneManager.LoadSceneAsync(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;targetSence&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, LoadSceneMode.Single);
        &lt;/span&gt;&lt;span&gt;yield&lt;/span&gt; &lt;span&gt;return&lt;/span&gt; &lt;span&gt;async&lt;/span&gt;&lt;span&gt;;
        ChangeShader();
    }
    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换材质&lt;/span&gt;
    &lt;span&gt;private&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; ChangeShader()
    {
&lt;/span&gt;&lt;span&gt;#if&lt;/span&gt; UNITY_EDITOR
        &lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换场景物体材质&lt;/span&gt;
        Terrain terrain = GameObject.FindObjectOfType&amp;lt;Terrain&amp;gt;&lt;span&gt;();
        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (terrain != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
        {
            terrain.materialType &lt;/span&gt;=&lt;span&gt; Terrain.MaterialType.BuiltInLegacyDiffuse;
        }
        Object[] objs &lt;/span&gt;= GameObject.FindObjectsOfType(&lt;span&gt;typeof&lt;/span&gt;&lt;span&gt;(GameObject));
        &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; k = &lt;span&gt;0&lt;/span&gt;; k &amp;lt; objs.Length; k++&lt;span&gt;)
        {
            GameObject go &lt;/span&gt;= objs[k] &lt;span&gt;as&lt;/span&gt;&lt;span&gt; GameObject;
            Renderer[] componentsInChildren &lt;/span&gt;= go.GetComponents&amp;lt;Renderer&amp;gt;&lt;span&gt;();
            &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; i = &lt;span&gt;0&lt;/span&gt;; i &amp;lt; componentsInChildren.Length; i++&lt;span&gt;)
            {
                Material[] sharedMaterials &lt;/span&gt;=&lt;span&gt; componentsInChildren[i].sharedMaterials;
                &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; (&lt;span&gt;int&lt;/span&gt; j = &lt;span&gt;0&lt;/span&gt;; j &amp;lt; sharedMaterials.Length; j++&lt;span&gt;)
                {
                    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (sharedMaterials[j] != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                    {
                        Shader shader &lt;/span&gt;=&lt;span&gt; sharedMaterials[j].shader;
                        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (shader != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                        {
                            Shader shader2 &lt;/span&gt;=&lt;span&gt; Shader.Find(shader.name);
                            &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (shader2 != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
                            {
                                sharedMaterials[j].shader &lt;/span&gt;=&lt;span&gt; shader2;
                            }
                        }
                    }
                }
            }
        }
        &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;替换天空盒材质&lt;/span&gt;
        Shader shader02 =&lt;span&gt; RenderSettings.skybox.shader;
        RenderSettings.skybox.shader &lt;/span&gt;=&lt;span&gt; Shader.Find(shader02.name);

        Debug.Log(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;替换材质&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;#endif&lt;/span&gt;&lt;span&gt;
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　这里没有写进度条相关的功能，大家可以自行解决。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:10:00 +0000</pubDate>
<dc:creator>U头</dc:creator>
<og:description>场景跳转是游戏里面不可缺少的内容，这里写写我这个小白遇到的问题。 场景资源比较大，同步切换场景不现实。场景一般比较多，都加到scenes in build也不太现实。 这里将初始场景和过渡场</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/lihangppz/p/10434245.html</dc:identifier>
</item>
<item>
<title>.Net Core跨平台应用研究-CustomSerialPort(增强型跨平台串口类库) - 赫山老妖</title>
<link>http://www.cnblogs.com/flyfire-cn/p/10434171.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/flyfire-cn/p/10434171.html</guid>
<description>&lt;p align=&quot;left&quot;&gt;      在使用SerialPort进行串口协议解析过程中，经常遇到接收单帧协议数据串口接收事件多次触发，协议解析麻烦的问题。针对此情况，基于开源跨平台串口类库SerialPortStrem进行了进一步封装，实现了一种接收超时响应事件机制，简化串口通讯的使用。&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;      最近，写了一篇博文&lt;a href=&quot;https://www.cnblogs.com/flyfire-cn/p/10356991.html&quot; target=&quot;_blank&quot;&gt;《.net core跨平台应用研究-串口篇》&lt;/a&gt;得到了一些园友的好评，文中介绍了在跨平台应用研究过程中，在dotnet core下使用SerialPort类库在linux下不能支持的踩坑经历及解决办法。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      因网上关于SerialPort类库使用的相关文章较多，在该文中，对串口类库的使用，一笔带过。但在实际使用，使用过SerialPort类库的同学，可能遇到过在数据接收时，由于数据接收事件的触发具有不确定性，很多时候，一帧通讯协议数据，会多次触发，造成程序处理协议数据较为麻烦的问题。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      为简化串口通讯类库的使用，笔者结合自己的相关经验，封装了一个自定义增强型跨平台串口类库，以解决一帧协议数据，多次触发的问题。&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;      由于考虑的是跨平台应用，SerialPort类库并不支持linux系统（在前一篇文章中已介绍过踩坑经历），笔者选用了SerialPortStream类库进行封装。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     该类库支持windows系统和Linux系统，但在Linux系统下运行，需要额外编译目标平台支持库并进行相关环境配置。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     相关编译配置说明在&lt;a href=&quot;https://github.com/jcurl/SerialPortStream&quot; target=&quot;_blank&quot;&gt;https://github.com/jcurl/SerialPortStream&lt;/a&gt;已有介绍，也可参考本人的拙作&lt;a href=&quot;https://www.cnblogs.com/flyfire-cn/p/10356991.html&quot; target=&quot;_blank&quot;&gt;《.net core跨平台应用研究-串口篇》&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;创建跨平台类库&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     为了支持跨平台，我们使用Visual Studio017创建一个基于.NET Standard的类库。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225214408091-1681582560.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     NET Standard是一项API规范，每一个特定的版本，都定义了必须实现的基类库。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     .NET Core是一个托管框架，针对构建控制台、云、ASP.NET Core和UWP应用程序进行了优化。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     每一种托管实现（如.NET Core、.NET Framework或Xamarin）都必须遵循.NET Standard实现基类库（BCL）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     关于NET Standard和跨平台的详细说明在此：&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     &lt;a href=&quot;https://zhuanlan.zhihu.com/p/30081607&quot; target=&quot;_blank&quot;&gt;https://zhuanlan.zhihu.com/p/30081607&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     笔者也不再啰嗦呵。&lt;/p&gt;
&lt;h2&gt;实现机制/条件&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     通常串口通讯中，发送数据后，会有一段时间用于等待接收方应答，如此一来，两次数据发送之间，必然会有一定的时间间隔。如ModbusRTU协议就规定，两次数据报文发送之间，需要等待超过发送4个字节以上的间隔时间。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     笔者在单片机以及实时性较高的嵌入式系统中，为处理串口接收与协议的无关性，通常采用数据帧接收超时来处理数据帧的接收。根据串口通讯的速率计算出两次通讯之间所需要超时间隔，取两倍超时间隔时间作为超时参数，每接收到一个字节，将数据放入缓冲区并进行计时，当最后一个字节的接收时间超过超时时间，返回接收数据并清空缓存，一次完整接收完成（DMA接收方式不在此讨论）。&lt;/p&gt;
&lt;h2&gt;.net core跨平台实现&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;     在自定义的串口类中，订阅基础串口类数据接收事件，在接收事件每次触发后，读出当前可用的缓冲数据到自定义缓冲区，同时，标记最后接收时间Tick为当前系统Tick。判断是否开启了接收超时处理线程，如未开启，则开启一个接收超时处理线程。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     接收超时处理线程中，以一个较小的时间间隔进行判断，如果最后接收时间与当前时间之间的间隔小于设置值（默认128ms），休眠一段时间（默认16ms）后循环检查。如间隔时间大于设定值，触发外部接收订阅事件，传出接收到的数据，退出超时处理线程。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     此处应有流程图。呵呵，懒得画了，大家自行脑补吧。 ^_^&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     在windows系统或linux系统中，因系统的多任务处理的特性，系统实时性较差，通常50ms以下时间间隔的定时任务，较大程序会出现不可靠的情况（任务执行时间都有可能超过调用间隔时间）。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     因此，默认超时时间间隔设置为128ms。也可根据实际使用情况调整，但最小间隔不宜低于64ms。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     注：此处为个人经验和理解，如不认同，请直接忽视。&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;h2&gt;主要代码&lt;/h2&gt;
&lt;p align=&quot;left&quot;&gt;       串口接收事件代码：    &lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('93bc91df-e33f-4c11-80e6-9bc658b9ec44')&quot; readability=&quot;40.5&quot;&gt;&lt;img id=&quot;code_img_closed_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('93bc91df-e33f-4c11-80e6-9bc658b9ec44',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_93bc91df-e33f-4c11-80e6-9bc658b9ec44&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;76&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;protected&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; Sp_DataReceived(&lt;span&gt;object&lt;/span&gt;&lt;span&gt; sender, SerialDataReceivedEventArgs e)
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;             &lt;span&gt;int&lt;/span&gt; canReadBytesLen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt;&lt;span&gt; (ReceiveTimeoutEnable)
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;                 &lt;span&gt;while&lt;/span&gt; (sp.BytesToRead &amp;gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;                     canReadBytesLen =&lt;span&gt; sp.BytesToRead;
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (receiveDatalen + canReadBytesLen &amp;gt;&lt;span&gt; BufSize)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;                         receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;                         &lt;span&gt;throw&lt;/span&gt; &lt;span&gt;new&lt;/span&gt; Exception(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Serial port receives buffer overflow!&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;                     &lt;span&gt;var&lt;/span&gt; receiveLen =&lt;span&gt; sp.Read(recviceBuffer, receiveDatalen, canReadBytesLen);
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (receiveLen !=&lt;span&gt; canReadBytesLen)
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;                         receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                         &lt;span&gt;throw&lt;/span&gt; &lt;span&gt;new&lt;/span&gt; Exception(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Serial port receives exception!&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Copy(recviceBuffer, 0, receivedBytes, receiveDatalen, receiveLen);&lt;/span&gt;
&lt;span&gt;21&lt;/span&gt;                     receiveDatalen +=&lt;span&gt; receiveLen;
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;                     lastReceiveTick =&lt;span&gt; Environment.TickCount;
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (!&lt;span&gt;TimeoutCheckThreadIsWork)
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;                         TimeoutCheckThreadIsWork = &lt;span&gt;true&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;                         Thread thread = &lt;span&gt;new&lt;/span&gt;&lt;span&gt; Thread(ReceiveTimeoutCheckFunc)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;                        {
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;                             Name = &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;ComReceiveTimeoutCheckThread&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
&lt;span&gt;29&lt;/span&gt; &lt;span&gt;                        };
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; &lt;span&gt;                        thread.Start();
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;             &lt;span&gt;else&lt;/span&gt;
&lt;span&gt;35&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;                 &lt;span&gt;if&lt;/span&gt; (ReceivedEvent != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 获取字节长度&lt;/span&gt;
&lt;span&gt;39&lt;/span&gt;                     &lt;span&gt;int&lt;/span&gt; bytesNum =&lt;span&gt; sp.BytesToRead;
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (bytesNum == &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;                         &lt;span&gt;return&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 创建字节数组&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt;                     &lt;span&gt;byte&lt;/span&gt;[] resultBuffer = &lt;span&gt;new&lt;/span&gt; &lt;span&gt;byte&lt;/span&gt;&lt;span&gt;[bytesNum];
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt; 
&lt;span&gt;45&lt;/span&gt;                     &lt;span&gt;int&lt;/span&gt; i = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;                     &lt;span&gt;while&lt;/span&gt; (i &amp;lt;&lt;span&gt; bytesNum)
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;                         &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 读取数据到缓冲区&lt;/span&gt;
&lt;span&gt;49&lt;/span&gt;                         &lt;span&gt;int&lt;/span&gt; j = sp.Read(recviceBuffer, i, bytesNum -&lt;span&gt; i);
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;                         i +=&lt;span&gt; j;
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;                     Array.Copy(recviceBuffer, &lt;span&gt;0&lt;/span&gt;, resultBuffer, &lt;span&gt;0&lt;/span&gt;&lt;span&gt;, i);
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt;                     ReceivedEvent(&lt;span&gt;this&lt;/span&gt;&lt;span&gt;, resultBuffer);
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;System.Diagnostics.Debug.WriteLine(&quot;len &quot; + i.ToString() + &quot; &quot; + ByteToHexStr(resultBuffer));&lt;/span&gt;
&lt;span&gt;55&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;                 &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Clear (receivedBytes,0,receivedBytes.Length );&lt;/span&gt;
&lt;span&gt;57&lt;/span&gt;                 receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      接收超时处理线程代码：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('d32b09a6-98fb-45a7-9bf3-d7890a242e6d')&quot; readability=&quot;36&quot;&gt;&lt;img id=&quot;code_img_closed_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('d32b09a6-98fb-45a7-9bf3-d7890a242e6d',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_d32b09a6-98fb-45a7-9bf3-d7890a242e6d&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;67&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt; &lt;span&gt;&amp;lt;summary&amp;gt;&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt;&lt;span&gt; 超时返回数据处理线程方法
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;         &lt;span&gt;///&lt;/span&gt; &lt;span&gt;&amp;lt;/summary&amp;gt;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt;         &lt;span&gt;protected&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; ReceiveTimeoutCheckFunc()
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;             &lt;span&gt;while&lt;/span&gt;&lt;span&gt; (TimeoutCheckThreadIsWork)
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;                 &lt;span&gt;if&lt;/span&gt; (Environment.TickCount - lastReceiveTick &amp;gt;&lt;span&gt; ReceiveTimeout)
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;                     &lt;span&gt;if&lt;/span&gt; (ReceivedEvent != &lt;span&gt;null&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;                    {
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;                         &lt;span&gt;byte&lt;/span&gt;[] returnBytes = &lt;span&gt;new&lt;/span&gt; &lt;span&gt;byte&lt;/span&gt;&lt;span&gt;[receiveDatalen];
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;                         Array.Copy(recviceBuffer, &lt;span&gt;0&lt;/span&gt;, returnBytes, &lt;span&gt;0&lt;/span&gt;&lt;span&gt;, receiveDatalen);
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;                         ReceivedEvent(&lt;span&gt;this&lt;/span&gt;&lt;span&gt;, returnBytes);
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;                    }
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Array.Clear (receivedBytes,0,receivedBytes.Length );&lt;/span&gt;
&lt;span&gt;17&lt;/span&gt;                     receiveDatalen = &lt;span&gt;0&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;                     TimeoutCheckThreadIsWork = &lt;span&gt;false&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;                 &lt;span&gt;else&lt;/span&gt;
&lt;span&gt;21&lt;/span&gt;                     Thread.Sleep(&lt;span&gt;16&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;

&lt;p&gt;    为验证我们的类库是否能够正常工作，我们创建一个使用类库的.net core控制台程序。&lt;/p&gt;
&lt;p&gt;    为啥选择dotnet core,原因很简单，跨平台。本程序分别需在windows和linux系统下进行运行测试。&lt;/p&gt;
&lt;p&gt;    控制台程序主要实现以下功能：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;    显示系统信息（系统标识、程序标识等）&lt;/li&gt;
&lt;li&gt;    列举系统可用串口资源&lt;/li&gt;
&lt;li&gt;    选择串口&lt;/li&gt;
&lt;li&gt;    打开串口/关闭串口&lt;/li&gt;
&lt;li&gt;    串口测试（打开/发送/关闭）&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('a5f9c87b-eec1-40c9-a0fc-bfe477573f5b')&quot; readability=&quot;32.5&quot;&gt;&lt;img id=&quot;code_img_closed_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('a5f9c87b-eec1-40c9-a0fc-bfe477573f5b',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_a5f9c87b-eec1-40c9-a0fc-bfe477573f5b&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;60&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;         &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt; Main(&lt;span&gt;string&lt;/span&gt;&lt;span&gt;[] args)
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;            SetLibPath();
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;            ShowWelcome();
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; 
&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;            GetPortNames();
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;            ShowPortNames();
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt; (serailports.Length == &lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Press any key to exit&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;                Console.ReadKey();
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt;                 &lt;span&gt;return&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;#if&lt;/span&gt; RunIsService
&lt;span&gt;17&lt;/span&gt; &lt;span&gt;            RunService();
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; &lt;span&gt;#endif&lt;/span&gt;
&lt;span&gt;19&lt;/span&gt; 
&lt;span&gt;20&lt;/span&gt;             &lt;span&gt;bool&lt;/span&gt; quit = &lt;span&gt;false&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;             &lt;span&gt;while&lt;/span&gt; (!&lt;span&gt;quit)
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\r\nPlease Input command Key\r\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;p:Show SerialPort List&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;t:Test Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;o:Open Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;                 Console.WriteLine($&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;c:Close Uart:\&quot;{selectedComPort}\&quot;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;n:select next serial port&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;                 Console.WriteLine(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;q:exit app&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; &lt;span&gt;                Console.WriteLine();
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;                 &lt;span&gt;var&lt;/span&gt; key =&lt;span&gt; Console.ReadKey().KeyChar;
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;                Console.WriteLine();
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt; 
&lt;span&gt;34&lt;/span&gt;                 &lt;span&gt;switch&lt;/span&gt;&lt;span&gt; (key)
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; &lt;span&gt;                {
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; (Char)&lt;span&gt;27&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;q&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;Q&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;                         quit = &lt;span&gt;true&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;s&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt; &lt;span&gt;                        ShowWelcome();
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;p&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt; &lt;span&gt;                        ShowPortNames();
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;n&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; &lt;span&gt;                        SelectSerialPort();
&lt;/span&gt;&lt;span&gt;49&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;t&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; &lt;span&gt;                        TestUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;w&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt; &lt;span&gt;                        TestWinUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;o&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt; &lt;span&gt;                        OpenUart(selectedComPort);
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;                     &lt;span&gt;case&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;c&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt; &lt;span&gt;                        CloseUart();
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt;                         &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt; &lt;span&gt;                }
&lt;/span&gt;&lt;span&gt;63&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;64&lt;/span&gt;         }
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;     笔者使用类库是直接引用类库项目，大家需要使用的话，可在解决方案资源管理器中，项目的依赖项上点击右键&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215501868-1566940166.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;      在NuGet包管理器中，搜索SerialPort或flyfire即可找到并安装本类库。&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215615906-591043223.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;     类库地址：&lt;a href=&quot;https://www.nuget.org/packages/flyfire.CustomSerialPort&quot; target=&quot;_blank&quot;&gt;https://www.nuget.org/packages/flyfire.CustomSerialPort&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215711328-1690878690.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt; &lt;/p&gt;

&lt;h2&gt;Windows测试输出界面&lt;/h2&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215727952-603205333.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215750923-416383800.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;ubuntu测试输出界面&lt;/h2&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1525067/201902/1525067-20190225215841749-630757689.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;     类库源码与例程地址：&lt;a href=&quot;https://github.com/flyfire-cn/flyfire.CustomSerialPort&quot; target=&quot;_blank&quot;&gt;https://github.com/flyfire-cn/flyfire.CustomSerialPort&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;     有需要的同学，请自行获取。&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 14:00:00 +0000</pubDate>
<dc:creator>赫山老妖</dc:creator>
<og:description>在使用SerialPort进行串口协议解析过程中，经常遇到接收单帧协议数据串口接收事件多次触发，协议解析麻烦的问题。针对此情况，基于开源跨平台串口类库SerialPortStrem进行了进一步封装，实</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/flyfire-cn/p/10434171.html</dc:identifier>
</item>
<item>
<title>使用minukube部署kubernetes admission webhook实现etcd pod安全删除 - charlieroro</title>
<link>http://www.cnblogs.com/charlieroro/p/10434138.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/charlieroro/p/10434138.html</guid>
<description>&lt;p&gt;本需求来自于一道面试题😂（本环境使用centos 7）&lt;/p&gt;
&lt;p&gt;最好使用阿里云ec2服务器安装minikube，若使用本地pc的vmware可能会出现网络方面的问题。&lt;/p&gt;
&lt;p&gt;使用如下命令安装minikube，参见&lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-minikube/&quot; target=&quot;_blank&quot;&gt;install minikube&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
# curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;amp;&amp;amp; chmod +x minikube&lt;span&gt;
# sudo cp minikube /usr/local/bin &amp;amp;&amp;amp; rm minikube&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;启动minikube可能会遇到docker版本过旧导致启动失败的问题。默认centos下面yum安装的docker版本比较旧，需要安装最新docker，更新docker参见&lt;a href=&quot;https://docs.docker.com/install/linux/docker-ce/centos/&quot; target=&quot;_blank&quot;&gt;Get Docker CE for CentOS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如下命令移除已经安装的docker&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;yum remove docker \
                  docker-&lt;span&gt;client \
                  docker-client-&lt;span&gt;latest \
                  docker-&lt;span&gt;common \
                  docker-&lt;span&gt;latest \
                  docker-latest-&lt;span&gt;logrotate \
                  docker-&lt;span&gt;logrotate \
                  docker-engine&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如下命令安装存储驱动和设置docker stable版本的库&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
yum install -y yum-&lt;span&gt;utils \
  device-mapper-persistent-&lt;span&gt;data \
  lvm2

yum-config-&lt;span&gt;manager \
    --add-&lt;span&gt;repo \
    https://download.docker.com/linux/centos/docker-ce.repo&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用如下命令即可更新为最新的docker&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
yum install docker-ce docker-ce-cli containerd.io
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;管理kubernetes上的服务最好使用helm(本次未用到，如无需要可忽略本节)，helm安装如下：&lt;/p&gt;
&lt;p&gt;使用如下方式获取helm的二进制版本&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32.475961538462&quot;&gt;
&lt;pre&gt;
&lt;span&gt;Download your &lt;a href=&quot;https://github.com/helm/helm/releases&quot; target=&quot;_blank&quot;&gt;desired version&lt;/a&gt;
Unpack it (tar -zxvf helm-v2.0.0-linux-&lt;span&gt;amd64.tgz)
Find the helm binary in the unpacked directory, and move it to its desired destination (mv linux-amd64/helm /usr/local/bin/helm)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/helm/helm/blob/master/docs/rbac.md&quot; target=&quot;_blank&quot;&gt; tiller安装&lt;/a&gt;给出了在不同scope下面安装tiller的方法，最简单的是在cluster-admin下面安装即可（生成环境下建议参考&lt;a href=&quot;https://github.com/helm/helm/blob/master/docs/install.md&quot; target=&quot;_blank&quot;&gt;helm安装文档&lt;/a&gt;将权限最小化）&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
# cat rbac-&lt;span&gt;config.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-&lt;span&gt;system
---&lt;span&gt;
apiVersion: rbac.authorization.k8s.io/&lt;span&gt;v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-&lt;span&gt;admin
subjects:
  -&lt;span&gt; kind: ServiceAccount
    name: tiller
    namespace: kube-system&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用如下命令创建即可&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
kubectl create -f rbac-&lt;span&gt;config.yaml
serviceaccount &quot;tiller&quot;&lt;span&gt; created
clusterrolebinding &quot;tiller&quot;&lt;span&gt; created
$ helm init --service-account tiller&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;admission webhook原理&lt;/p&gt;
&lt;p&gt;kubernetes的认证和授权是对客户端进行认证以及对资源进行授权，但在资源的使用处理上不够细化。admission webhook是在一种在改变资源的持久化之前（比如某些资源的创建或删除，修改等之前）的机制。参见&lt;a href=&quot;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&quot; target=&quot;_blank&quot;&gt;官方资料&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225202115299-175005990.png&quot; alt=&quot;&quot; width=&quot;697&quot; height=&quot;296&quot;/&gt;&lt;/p&gt;
&lt;p&gt;如下图，在部署了admission webhook之后，apiserver会发送一个AdmissionReview的json数据到webhook，其中主要包含一个AdmissionRequest的请求，AdmissionRequest.RawExtension.Raw包含了需要处理的kubernetes组件(如pod，deployment等)的详细信息。webhook在接收到该请求之后会根据自定义逻辑进行处理，并返回处理结果AdmissionResponse。admission webhook有两种处理方式：&lt;/p&gt;
&lt;p&gt;MutatingAdmissionWebhook：可以修改可以自定义的策略MutatingAdmissionWebhook的处理一般优先于MutatingAdmissionWebhook，这样前者修改的内容就可以由后者进行校验&lt;/p&gt;
&lt;p&gt;ValidatingAdmissionWebhook: 允许或拒绝客户自定义的策略&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;44&quot;&gt;
&lt;pre&gt;
type AdmissionReview struct&lt;span&gt; {
    metav1.TypeMeta `json:&quot;,inline&quot;&lt;span&gt;`
    // Request describes the attributes for the admission request.
    // +optional
    Request *AdmissionRequest `json:&quot;request,omitempty&quot; protobuf:&quot;bytes,1,opt,name=request&quot;&lt;span&gt;`
    // Response describes the attributes for the admission response.
    // +optional
    Response *AdmissionResponse `json:&quot;response,omitempty&quot; protobuf:&quot;bytes,2,opt,name=response&quot;&lt;span&gt;`
}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225200938176-1213925158.png&quot; alt=&quot;&quot; width=&quot;671&quot; height=&quot;579&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 处理流程图如下，可以看到MutatingAdmissionWebhook的处理优先于MutatingAdmissionWebhook&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225204708180-361997199.png&quot; alt=&quot;&quot; width=&quot;734&quot; height=&quot;325&quot;/&gt;&lt;/p&gt;

&lt;p&gt;使用如下命令启动minikube&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
minikube start --vm-driver=none --extra-config=apiserver.enable-admission-plugins=&quot;NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota&quot;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; 使用kubectl api-versions查看是否支持admissionregistration.k8s.io/v1alpha1 API&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://banzaicloud.com/blog/k8s-admission-webhooks/&quot; target=&quot;_blank&quot;&gt;In-depth introduction to Kubernetes admission webhooks&lt;/a&gt;是个很好的例子，里面详细记录了创建admission webhook的方式。&lt;a href=&quot;https://github.com/woodliu/admission-webhook-example&quot; target=&quot;_blank&quot;&gt;这里&lt;/a&gt;参照&lt;a href=&quot;https://banzaicloud.com/blog/k8s-admission-webhooks/&quot; target=&quot;_blank&quot;&gt;In-depth introduction to Kubernetes admission webhooks&lt;/a&gt;实现了根据etcd pod角色(leader和非leader)来删除pod。&lt;/p&gt;
&lt;p&gt;首先使用webhook-create-signed-cert.sh文件来生成自签证书，后续ValidatingWebhookConfiguration中会用到&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
# ./deployment/webhook-create-signed-cert.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;部署admission webhook&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
# kubectl create -f deployment/&lt;span&gt;deployment.yaml&lt;span&gt;
# kubectl create -f deployment/service.yaml&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;生成待CA bundle的config文件以及validatingwebhook.yaml原始文件如下&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
# cat ./deployment/validatingwebhook.yaml | ./deployment/webhook-patch-ca-bundle.sh &amp;gt; ./deployment/validatingwebhook-ca-bundle.yaml
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
apiVersion: admissionregistration.k8s.io/&lt;span&gt;v1beta1
kind: ValidatingWebhookConfiguration
metadata:
  name: validation-webhook-example-&lt;span&gt;cfg
  labels:
    app: admission-webhook-&lt;span&gt;example
webhooks:
  - name: required-&lt;span&gt;labels.banzaicloud.com
    clientConfig:
      service:
        name: admission-webhook-example-&lt;span&gt;svc #service名称
        namespace: default&lt;span&gt;
        path: &quot;/validate&quot; #访问的后缀路径&lt;span&gt;
      caBundle: ${CA_BUNDLE} #上述命令生成的认证字段
    rules:
      - operations: [ &quot;DELETE&quot;&lt;span&gt; ] #操作的动作
        apiGroups: [&quot;apps&quot;, &quot;&quot;&lt;span&gt;]  #api groups
        apiVersions: [&quot;v1&quot;&lt;span&gt;]      #api version
        resources: [&quot;pods&quot;&lt;span&gt;]      #操作的资源
    namespaceSelector:
      matchLabels:
        admission-webhook-example: enabled # 限制default的命名空间&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;给default namespace打上标签&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ kubectl label namespace default admission-webhook-example=enabled
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;创建ValidatingWebhookConfiguration&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ kubectl create -f deployment/validatingwebhook-ca-bundle.yaml
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;admission-webhook-example/build用于编译和生成容器镜像&lt;/p&gt;
&lt;p&gt;etcd的部署直接使用admission-webhook-example\deployment\etcd中的配置文件即可，这样在删除etcd的leader时会显示如下内容&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1334952/201902/1334952-20190225213902358-517584601.png&quot; alt=&quot;&quot; width=&quot;1361&quot; height=&quot;42&quot;/&gt;&lt;/p&gt;
&lt;p&gt;admission webhook的实现只有2个文件，main.go和webhook.go。main.go中通过mux.HandleFunc(&quot;/validate&quot;, whsvr.serve)来启动一个http服务处理路径/validate(对应validatingwebhook.yaml的webhooks.clientConfig.service.path)的请求。主要逻辑是现在函数func (whsvr *WebhookServer) validate(ar *v1beta1.AdmissionReview) *v1beta1.AdmissionRespons中&lt;/p&gt;

&lt;p&gt;FAQ：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;执行yum install -y socat可解决如下问题：&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
an error occurred forwarding 40546 -&amp;gt; 44134: error forwarding port 44134 to pod 3ea221f842e1446a5fd9da9fc29e7e415a1ecb6dd6d07c56f64fb4788f2c3915, uid : unable to do port forwarding: socat not found.
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;TIPS:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; 使用如下方式可以安装etcdctl命令行工具&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
&lt;span&gt;# choose either URL
GOOGLE_URL=https://storage.googleapis.com/etcd
GITHUB_URL=https://github.com/coreos/etcd/releases/download
DOWNLOAD_URL=&lt;span&gt;${GOOGLE_URL}

rm -f /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz
rm -rf /tmp/etcd-download-test &amp;amp;&amp;amp; mkdir -p /tmp/etcd-download-&lt;span&gt;test

curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz
tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /tmp/etcd-download-test --strip-components=1&lt;span&gt;
rm -f /tmp/etcd-${ETCD_VER}-linux-&lt;span&gt;amd64.tar.gz

/tmp/etcd-download-test/etcd --&lt;span&gt;version
ETCDCTL_API=3 /tmp/etcd-download-test/&lt;span&gt;etcdctl version
# start a local etcd server
/tmp/etcd-download-test/&lt;span&gt;etcd

# write,read to etcd
ETCDCTL_API=3 /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379&lt;span&gt; put foo bar
ETCDCTL_API=3 /tmp/etcd-download-test/etcdctl --endpoints=localhost:2379 get foo&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;go get -u github.com/golang/dep/cmd/dep 安装dep&lt;/li&gt;
&lt;li&gt;etcdv3的API使用可以参考&lt;a href=&quot;https://godoc.org/github.com/dverbeek84/etcd/clientv3&quot; target=&quot;_blank&quot;&gt;package clientv3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;/p&gt;
&lt;p&gt;https://github.com/helm/helm/blob/master/docs/install.md&lt;/p&gt;
&lt;p&gt;https://container-solutions.com/some-admission-webhook-basics/&lt;/p&gt;
&lt;p&gt;https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/&lt;/p&gt;
&lt;p&gt;https://github.com/kubernetes/kubernetes/tree/master/test/e2e/testing-manifests/statefulset/etcd&lt;/p&gt;
&lt;p&gt;https://github.com/morvencao/kube-mutating-webhook-tutorial/blob/master/medium-article.md&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 13:54:00 +0000</pubDate>
<dc:creator>charlieroro</dc:creator>
<og:description>本需求来自于一道面试题😂（本环境使用centos 7） 最好使用阿里云ec2服务器安装minikube，若使用本地pc的vmware可能会出现网络方面的问题。 使用如下命令安装mini</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/charlieroro/p/10434138.html</dc:identifier>
</item>
<item>
<title>如何在ASP.NET Core中使用JSON Patch - LamondLu</title>
<link>http://www.cnblogs.com/lwqlun/p/10433615.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/lwqlun/p/10433615.html</guid>
<description>&lt;blockquote readability=&quot;3.6875&quot;&gt;
&lt;p&gt;原文： &lt;a href=&quot;https://dotnetcoretutorials.com/2017/11/29/json-patch-asp-net-core/&quot;&gt;JSON Patch With ASP.NET Core&lt;/a&gt;&lt;br/&gt;作者：.NET Core Tutorials&lt;br/&gt;译文：如何在ASP.NET Core中使用JSON Patch&lt;br/&gt;地址：&lt;a href=&quot;https://www.cnblogs.com/lwqlun/p/10433615.html&quot; class=&quot;uri&quot;&gt;https://www.cnblogs.com/lwqlun/p/10433615.html&lt;/a&gt;&lt;br/&gt;译者：Lamond Lu&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/65831/201902/65831-20190225213208189-1747876581.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;JSON Patch&lt;/strong&gt;是一种使用API显式更新文档的方法。它本身是一种契约，用于描述如何修改文档（例如：将字段的值替换成另外一个值），而不必同时发送其他未更改的属性值。&lt;/p&gt;

&lt;p&gt;你可以在以下链接（&lt;a href=&quot;http://jsonpatch.com/&quot; class=&quot;uri&quot;&gt;http://jsonpatch.com/&lt;/a&gt;）中找到JSON Patch的官方文档，但是这里我们将进一步研究一下如何在ASP.NET Core中实现JSON Patch。&lt;/p&gt;
&lt;p&gt;为了演示JSON Patch, 我创建了以下C#对象类, 后续我将使用JSON Patch请求来操作这个对象类的实例。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
    public List&amp;lt;string&amp;gt; Friends { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所有的JSON Patch请求都是遵循一个相似的结构。它有一个固定的“操作”列表。每个操作本身拥有3个属性：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&quot;op&quot; - 定义了你要执行何种操作，例如add, replace, test等。&lt;/li&gt;
&lt;li&gt;&quot;path&quot; - 定义了你要操作对象属性路径。用前面的&lt;code&gt;Person&lt;/code&gt;类为例，如果你希望修改&lt;code&gt;FirstName&lt;/code&gt;属性，那么你使用的操作路径应该是&quot;/FirstName&quot;。&lt;/li&gt;
&lt;li&gt;&quot;value&quot; - 在大部分情况下，这个属性表示你希望在操作中使用的值。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;现在让我们来看一下每一个的操作如何使用。&lt;/p&gt;
&lt;h2 id=&quot;add&quot;&gt;Add&lt;/h2&gt;
&lt;p&gt;Add操作通常意味着你要向对象中添加属性，或者向数组中添加项目。对于前者，在C#中是没有用的，因为C#是强类型语言，所以不能将属性添加到编译时尚未定义的对象上。&lt;/p&gt;
&lt;p&gt;所以这里如果想往数组中添加项目，PATCH请求的内容应该如下所示。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/Friends/1&quot;, &quot;value&quot;: &quot;Mike&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这将在Friends数组的索引1处插入一个&quot;Mike&quot;值。&lt;/p&gt;
&lt;p&gt;或者你还可以使用&quot;-&quot;在数组尾部插入记录。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/Friends/-&quot;, &quot;value&quot;: &quot;Mike&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;remove&quot;&gt;Remove&lt;/h2&gt;
&lt;p&gt;与Add操作类似，删除操作意味着你希望删除对象中属性，或者从数据中删除某一项。但是因为在C#中你无法移除属性，实际操作时，它会将属性的值变更为default(T)。在某些情况下，如果属性是可空的，则会设置属性值为NULL。但是需要小心，因为当在值类型上使用时，例如int, 则该值实际上会重置为&quot;0&quot;。&lt;/p&gt;
&lt;p&gt;如果要在对象上删除某一属性以达到重置的效果，你可以使用一下命令。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/FirstName&quot;}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;当然你也可以使用删除命令删除数组中的某一项。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/Friends/1&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这将删除数组索引为1的项目。但是有时候使用索引从数组中删除数据是非常危险的，因为这里没有一个&quot;where&quot;条件来控制删除， 有可能在删除的时候，数据库中对应数组已经发生变化了。实际上有一个JSON Patch操作可以帮助解决这个问题，后面我会描述它。&lt;/p&gt;
&lt;h2 id=&quot;replace&quot;&gt;Replace&lt;/h2&gt;
&lt;p&gt;Replace操作和它的字面意思完全一样，可以使用它来替换已有值。针对简单属性，你可以使用如下的命令。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Jim&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;你同样可以使用它来替换数组中的对象。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/Friends/1&quot;, &quot;value&quot;: &quot;Bob&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;你甚至可以用它来替换整个数组。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/Friends&quot;, &quot;value&quot;: [&quot;Bob&quot;, &quot;Bill&quot;] }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;copy&quot;&gt;Copy&lt;/h2&gt;
&lt;p&gt;Copy操作可以将值从一个路径复制到另一个路径。这个值可以是属性，对象，或者数据。在下面的例子中，我们将FirstName属性的值复制到了LastName属性上。这个命令的使用场景不是很多。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;copy&quot;, &quot;from&quot;: &quot;/FirstName&quot;, &quot;path&quot; : &quot;/LastName&quot; }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;move&quot;&gt;Move&lt;/h2&gt;
&lt;p&gt;Move操作非常类似于Copy操作，但是正如它的字面意思，&quot;from&quot;字段的值将被移除。如果你看一下ASP.NET Core的JSON Patch的底层代码，你会发现，它实际上它会在&quot;from&quot;路径上执行Remove操作，在&quot;path&quot;路径上执行Add操作。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{ &quot;op&quot;: &quot;move&quot;, &quot;from&quot;: &quot;/FirstName&quot;, &quot;path&quot; : &quot;/LastName&quot; }    &lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;test&quot;&gt;Test&lt;/h2&gt;
&lt;p&gt;在当前的ASP.NET Core公开发行版中没有Test操作，但是如果你在Github上查看源代码，你会发现微软已经处理了Test操作。Test操作是一种乐观锁定的方法，或者更简单的说，它会检测数据对象从服务器读取之后，是否发生了更改。&lt;/p&gt;
&lt;p&gt;我们以如下操作为例。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;[
    { &quot;op&quot;: &quot;test&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Bob&quot; }
    { &quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/FirstName&quot;, &quot;value&quot;: &quot;Jim&quot; }
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个操作首先会检查&quot;/FirstName&quot;路径的值是否&quot;Bob&quot;, 如果是，就将它改为&quot;Jim&quot;。 如果不是，则什么事情都不会发生。这里你需要注意，在一个Test操作的请求体内可以包含多个Test操作，但是如果其中任何一个Test操作验证失败，所以的变更操作都不会被执行。&lt;/p&gt;

&lt;p&gt;JSON Patch的一大优势在于它的请求操作体很小，只发送对象的更改内容。 但是在ASP.NET Core中使用JSON Patch还有另一个很大的好处，就是C＃是一种强类型语言，无法区分是要将模型的值设置为NULL，还是忽略该属性， 而使用JSON Patch可以解决这个问题。&lt;/p&gt;
&lt;p&gt;这里如果没有好的例子，很难解释。 所以想象一下我从API请求一个“Person”对象。 在C＃中，模型可能如下所示：&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;
public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;当从API返回Json对象时，它看起来可能像这样。&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;James&quot;, 
    &quot;lastName&quot; : &quot;Smith&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;现在在前端，如果不使用JSON Patch, 如果我只想更新FirstName, 我可能在请求中附带一下请求体。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;Jim&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;现在当我在C#中反序列化这个模型时，问题就出现了。不要看下面的代码，想一下此时我们的模型中的属性值是什么？&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;public class Person
{
    public string FirstName { get; set; } //Jim
    public string LastName { get; set; } //&amp;lt;Null&amp;gt;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;因为我们发送LastName属性的值，所以它被反序列化为Null。 但这很简单，我们可以忽略NULL的值，只更新我们实际传递的字段。 但这不一定是正确的，如果该字段实际上可以为空呢？ 如果我们发送了以下请求体怎么办？&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot; : &quot;Jim&quot;, 
    &quot;lastName&quot; : null
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所以现在我们实际上已经指定我们想要取消该字段。但是因为C＃是强类型的，所以我们无法在服务器端进行模型绑定的时候，我们无法确定它是否要将该字段的值设置为NULL。&lt;/p&gt;
&lt;p&gt;这似乎是一个奇怪的场景，因为前端可以始终发送完整的数据模型，永远不会省略字段。并且在大多数情况下，前端Web库的模型将始终与API的模型匹配。但有一种情况并非如此，那就是移动应用程序。通常向苹果应用商店提交手机应用，可能需要数周时间才能获得批准。在这个时候，你可能还需要在Web或Android应用程序中使用新模型。在不同平台之间实现同步非常困难，而且通常是不可能。虽然API版本确实对解决这个问题有很长的路要走，但我仍然认为JSON Patch在解决这个问题方面具有很大的实用性。&lt;/p&gt;
&lt;p&gt;最后，让我们使用JSON Patch！我们可以使用以下JSON Patch请求更新Person对象&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;[
    {
      &quot;op&quot;: &quot;replace&quot;,
      &quot;path&quot;: &quot;/FirstName&quot;,
      &quot;value&quot;: &quot;Jim&quot;
    }
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这明确表示我们想要更改名字而不是其他内容。 它准确的告诉我们到底将要发生什么。&lt;/p&gt;

&lt;p&gt;在Visual Studio中，我们可以在Package Manage Console中安装官方的Json Patch库（默认创建的ASP.NET Core项目中没有该库）。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Install-Package Microsoft.AspNetCore.JsonPatch&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;为了演示，我将添加如下的一个控制器类。这里需要注意我们使用的HTTP verb是HttpPatch, 请求参数的类型是&lt;code&gt;JsonPatchDocument&lt;/code&gt;。 为了更新对象，我们只需要简单调用&lt;code&gt;ApplyTo&lt;/code&gt;方法，并传入了需要更新的对象。&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;[Route(&quot;api/[controller]&quot;)]
public class PersonController : Controller
{
    private readonly Person _defaultPerson = new Person
    {
        FirstName = &quot;Jim&quot;,
        LastName = &quot;Smith&quot;
    };
 
    [HttpPatch(&quot;update&quot;)]
    public Person Patch([FromBody]JsonPatchDocument&amp;lt;Person&amp;gt; personPatch)
    {
        personPatch.ApplyTo(_defaultPerson);
        return _defaultPerson;
    }
}
 
public class Person
{
    public string FirstName { get; set; }
    public string LastName { get; set; }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在以上示例中，我们只是使用了存放在控制器中的简单对象并对其进行了更新，但是在正式的API中，我们需要从数据库中拉取数据对象，更新对象，并重新保存到数据库。&lt;/p&gt;
&lt;p&gt;当我们使用如下请求体发送JSON Patch请求时：&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;[
    {&quot;op&quot; : &quot;replace&quot;, &quot;path&quot; : &quot;/FirstName&quot;, &quot;value&quot; : &quot;Bob&quot;}
]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;我们可以得到如下响应内容：&lt;/p&gt;
&lt;pre class=&quot;json&quot;&gt;
&lt;code&gt;{
    &quot;firstName&quot;: &quot;Bob&quot;,
    &quot;lastName&quot;: &quot;Smith&quot;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;真棒！ 我们的名字改为Bob！ 使用JSON Patch启动和运行真的很简单。&lt;/p&gt;

&lt;p&gt;针对JSON Patch的使用，最大的问题是，你经常需要从API返回View Model或者DTO, 并生成PATCH请求。但是如果将这些修改请求应用于数据库对象上？大部分情况下，开发人员都挣扎在与此。这里我们可以使用Automapper来帮助完成这个转换的工作。&lt;/p&gt;
&lt;p&gt;例如如下代码:&lt;/p&gt;
&lt;pre class=&quot;c#&quot;&gt;
&lt;code&gt;
[HttpPatch(&quot;update/{id}&quot;)]
public Person Patch(int id, [FromBody]JsonPatchDocument&amp;lt;PersonDTO&amp;gt; personPatch)
{
    //获取原始Person对象实例
    PersonDatabase personDatabase = _personRepository.GetById(id); 
    
    //将Person对象实例转换为PersonDTO对象实例
    PersonDTO personDTO = _mapper.Map&amp;lt;PersonDTO&amp;gt;(personDatabase); 
    
    //应用Patch修改
    personPatch.ApplyTo(personDTO);  
    
    //将更新后的PersonDTO对象，重新映射到Person对象实例中
    _mapper.Map(personDTO, personDatabase); 
    
    //将更新后的Person对象实例保存到数据库
    _personRepository.Update(personDatabase); 
 
    return personDTO;
}&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Mon, 25 Feb 2019 13:33:00 +0000</pubDate>
<dc:creator>LamondLu</dc:creator>
<og:description>JSON Patch是一种使用API显式更新文档的方法。它本身是一种契约，用于描述如何修改文档（例如：将字段的值替换成另外一个值），而不必同时发送其他未更改的属性值。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/lwqlun/p/10433615.html</dc:identifier>
</item>
<item>
<title>如何定位前端线上问题（如何排查前端生产问题） - 一步一个脚印一个坑</title>
<link>http://www.cnblogs.com/warm-stranger/p/10430346.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/warm-stranger/p/10430346.html</guid>
<description>&lt;p&gt;　　一直以来，前端的线上问题很难定位，因为它发生于用户的一系列操作之后。错误的原因可能源于机型，网络环境，复杂的操作行为等等，在我们想要去解决的时候很难复现出来，自然也就无法解决。 当然，这些问题并非不能克服，让我们来一起看看如何去定位线上的问题吧。&lt;/p&gt;
&lt;p&gt;　　所谓，工欲善其事必先利其器，你不能撸起袖子蛮干，所以，我们需要一个工具。我们曾经尝试用过很多监控工具去统计这些错误，比如，听云、OneApm、sentry、FundBug、growingIo 等等。 每家工具都各有所长，但也都各有所短，而且要花不少的钱（感觉是痛点，哈哈）。&lt;/p&gt;
&lt;h3&gt;　　一、统计前端错误（&lt;a href=&quot;http://www.webfunny.cn/webfunny/javascriptError&quot; target=&quot;_blank&quot;&gt;Demo&lt;/a&gt;）&lt;/h3&gt;
&lt;p&gt;　　众所周知，我们有办法去统计前端的错误，那就是大名鼎鼎的 window.onerror 方法， 用法如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
&lt;span&gt;    //&lt;/span&gt;&lt;span&gt; 重写 onerror 进行jsError的监听&lt;/span&gt;
    window.onerror = &lt;span&gt;function&lt;/span&gt;&lt;span&gt;(errorMsg, url, lineNumber, columnNumber, errorObj)
    {
      &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; errorStack = errorObj ? errorObj.stack : &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
      &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 分类处理信息&lt;/span&gt;
&lt;span&gt;      siftAndMakeUpMessage(errorMsg, url, lineNumber, columnNumber, errorStack);
    };&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;
　　window.onerror 方法中参数的意义我就不一一介绍了，我相信大家也已经耳熟能详了。 总之它能够为我们记录下线上的很多错误，以及一些额外的信息。我将window.onerror方法收集到的错误信息进行分析统计后的结果如下：
&lt;/pre&gt;
&lt;p&gt;        &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225125611992-1716164198.png&quot; alt=&quot;&quot; width=&quot;868&quot; height=&quot;474&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 　　　如上图所见： 我们统计出了每天的错误量，每个小时的错误量，每天的错误率变化，来鉴定我们线上环境是否健康。我们按照JS错误数量进行分类排序，按照页面进行错误分类。通过上边的数据分析，我们能够清晰地观察到线上项目的报错情况。&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;　　二、分析错误详情&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;　　&lt;span&gt;线上的错误日志统计出来了， 如何解析这些错误日志呢。如下图，解析出用户的机型，版本，系统平台，影响范围，以及具体的错误位置， 从而提高我们解决问题的效率。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　  &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225183852197-522824587.png&quot; alt=&quot;&quot; width=&quot;854&quot; height=&quot;444&quot;/&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;h3&gt; &lt;/h3&gt;
&lt;h3&gt; 　 疑问？&lt;/h3&gt;
&lt;p&gt;　　window.onerror 方法能够利用的地方都已经用的差不多了，但是它真的可以帮我们定位和解决前端线上的问题吗？&lt;/p&gt;
&lt;p&gt;　　线上能够修复的问题我已经尽量修复了，但是线上的问题频发。 当客服反馈一个问题， 你发现没有测试机型，无法复现用户错误的时候，让你来修复这个问题，只能两眼一抹黑，无能为力。&lt;/p&gt;
&lt;p&gt;　　例如：线上用户进过了复杂的链接跳转而发生了错误；用户调用的接口发生了异常或者超时；线上的用户反馈异常根本就跟实际情况不符，等等。这些非直观型的问题该如何解决？ 所以，我们需要用户的行为记录。&lt;/p&gt;
&lt;h3&gt;　　三、记录用户的访问行为（&lt;a href=&quot;http://www.webfunny.cn/webfunny/behaviors&quot; target=&quot;_blank&quot;&gt;Demo&lt;/a&gt;）&lt;/h3&gt;
&lt;p&gt;　　有些错误是前端页面经过复杂的跳转、回退之后才发生的，就算测试人员也很难测试出这种问题，因为线上的用户的任何行为都有可能出现。往往我们知道的只是他在最后停留的页面发生了错误。 如此，我们记录下用户的跳转日志， 就能够复现出用户的行为， 从而复现BUG&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225203859419-1929436470.png&quot; alt=&quot;&quot; width=&quot;422&quot; height=&quot;235&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　四、记录用户的接口行为&lt;/h3&gt;
&lt;p&gt;　　接口请求是一个前端项目涉及最多的行为，接口的异常包括：后台报错，响应超时，网络环境较差，重复接口数据覆盖等等。这些错误也只有在真实的用户环境中才会发生，是典型的线上问题。我们可以记录下用户的请求时间，参数，响应时间，响应状态等等，可以具体分析出来接口对页面的影响。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225204029452-1229334743.png&quot; alt=&quot;&quot; width=&quot;470&quot; height=&quot;311&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　五、记录用户的点击行为&lt;/h3&gt;
&lt;p&gt;　　用户经过一系列复杂的行为操作之后（主要是点击行为），页面的样子和保存的数据都经过了很多变化，此时此刻最容易发生数据错乱的现象，导致修复bug的时候无从入手，是复现用户行为中重要的一环。&lt;/p&gt;
&lt;p&gt;     &lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225210531410-1295469117.png&quot; alt=&quot;&quot; width=&quot;395&quot; height=&quot;177&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　六、记录用户的页面截图&lt;/h3&gt;
&lt;p&gt;　　即使你记录下所有的行为，但是你依然需要看到页面的样子，才能够分析出问题所在，那么我们依然可以通过js截图来看看用户设备上的样子。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225211433703-404715797.png&quot; alt=&quot;&quot; width=&quot;389&quot; height=&quot;227&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;　　七、分析用户的场外信息&lt;/h3&gt;
&lt;p&gt;　　当用户所有的行为都被我们掌握之后，我们能够复现出用户的行为，甚至能够复现出用户的问题，也许我们还需要一些场外信息才能精准定位问题，比如，用户的机型，地理位置，系统版本，当时的网络环境（这个不准确，我是依据用户当时首次加载页面的时间来判断，只能作为参考依据）&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/712333/201902/712333-20190225211959356-383480033.png&quot; alt=&quot;&quot; width=&quot;354&quot; height=&quot;456&quot;/&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;h3&gt;　　总结：&lt;/h3&gt;
&lt;p&gt;　　 问题产生的原因五花八门，只有把日志做全了，才能够准确的定位和解决问题。&lt;/p&gt;
&lt;p&gt;　　 这是我排查线上问题的经验和实战，分享出来，以求分享和学习。&lt;/p&gt;

&lt;p&gt;　　 说了这么多都没有直接体验直观，请移步。 &lt;a href=&quot;https://www.webfunny.cn/&quot; target=&quot;_blank&quot;&gt;Demo地址&lt;/a&gt; &lt;/p&gt;











</description>
<pubDate>Mon, 25 Feb 2019 13:30:00 +0000</pubDate>
<dc:creator>一步一个脚印一个坑</dc:creator>
<og:description>一直以来，前端的线上问题很难定位，因为它发生于用户的一系列操作之后。错误的原因可能源于机型，网络环境，复杂的操作行为等等，在我们想要去解决的时候很难复现出来，自然也就无法解决。 当然，这些问题并非不能</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/warm-stranger/p/10430346.html</dc:identifier>
</item>
</channel>
</rss>