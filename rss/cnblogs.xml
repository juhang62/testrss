<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>.Net微信网页开发之使用微信JS-SDK调用微信扫一扫功能 - 追逐时光</title>
<link>http://www.cnblogs.com/Can-daydayup/p/11135954.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/Can-daydayup/p/11135954.html</guid>
<description>&lt;h2&gt;前言：&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;　　之前有个项目需要调用微信扫描二维码的功能，通过调用微信扫码二维码功能，然后去获取到系统中生成的二维码信息。正好微信JS-SDK提供了调用微信扫一扫的功能接口，下面让我们来看看是如何实现的吧。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;第一步、微信JS-SDK的使用步骤，配置信息的生成获取讲解：&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;　　关于JS-SDK的使用步骤和timestamp(时间戳)，nonceStr(随机串)，signature(签名)，access_token(接口调用凭据)生成获取的详细说明在这里：&lt;a href=&quot;https://www.cnblogs.com/Can-daydayup/p/11124092.html&quot;&gt;https://www.cnblogs.com/Can-daydayup/p/11124092.html&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;第二步、调用微信扫一扫，获取到二维码中的内容：&lt;/h2&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;43&quot;&gt;
&lt;pre&gt;
 &amp;lt;input id=&quot;scanbutton&quot; type=&quot;button&quot; onclick=&quot;scan()&quot; style=&quot;background-color: #07CAF4&quot; value=&quot;扫描二维码&quot;&amp;gt;


&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
&lt;span&gt;//&lt;/span&gt;&lt;span&gt;注入权限验证配置&lt;/span&gt;
&lt;span&gt;wx.config({
    debug: &lt;/span&gt;&lt;span&gt;true&lt;/span&gt;, &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 开启调试模式,调用的所有api的返回值会在客户端alert出来，若要查看传入的参数，可以在pc端打开，参数信息会通过log打出，仅在pc端时才会打印。&lt;/span&gt;
    appId: '', &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 必填，公众号的唯一标识&lt;/span&gt;
    timestamp: , &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 必填，生成签名的时间戳&lt;/span&gt;
    nonceStr: '', &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 必填，生成签名的随机串&lt;/span&gt;
    signature: '',&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 必填，签名&lt;/span&gt;
    jsApiList: ['scanQRCode'] &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 必填，调用微信扫一扫接口&lt;/span&gt;
&lt;span&gt;});


&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;调用微信扫一扫接口&lt;/span&gt;
&lt;span&gt;function&lt;/span&gt;&lt;span&gt; scan() {
&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;首先判断是否使用微信内，因为微信JS-SDK只有在微信环境下才有用&lt;/span&gt;
&lt;span&gt;var&lt;/span&gt; environmental=&lt;span&gt; window.navigator.userAgent.toLowerCase();
 &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (environmental.match(/MicroMessenger/i) == 'micromessenger'&lt;span&gt;) {
wx.ready(&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {
 &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;获取本地跳转地址&lt;/span&gt;
&lt;span&gt;wx.scanQRCode({
needResult: &lt;/span&gt;1,&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 默认为0，扫描结果由微信处理，1则直接返回扫描结果，&lt;/span&gt;
desc: 'scanQRCode desc',&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 可以指定扫二维码还是一维码，默认二者都有&lt;/span&gt;
success: &lt;span&gt;function&lt;/span&gt;&lt;span&gt;(res) {
&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;alert(JSON.stringify(res));&lt;/span&gt;
 $(&quot;#Code&quot;&lt;span&gt;).val(res.resultStr);
}
});
});
}
&lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt; 
{
 alert(&lt;/span&gt;&quot;请在微信中登录！&quot;&lt;span&gt;);
}
}
&lt;/span&gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

</description>
<pubDate>Sat, 27 Jul 2019 00:01:00 +0000</pubDate>
<dc:creator>追逐时光</dc:creator>
<og:description>前言： 之前有个项目需要调用微信扫描二维码的功能，通过调用微信扫码二维码功能，然后去获取到系统中生成的二维码信息。正好微信JS-SDK提供了调用微信扫一扫的功能接口，下面让我们来看看是如何实现的吧。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/Can-daydayup/p/11135954.html</dc:identifier>
</item>
<item>
<title>微服务之初了解（一） - Vincent-yuan</title>
<link>http://www.cnblogs.com/Vincent-yuan/p/11253512.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/Vincent-yuan/p/11253512.html</guid>
<description>
&lt;h4&gt;一.什么是微服务&lt;/h4&gt;
&lt;p&gt;微服务就是一些协同工作的小而自治的服务。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 服务要足够小&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在使用微服务的时候，内聚性是一个很重要的概念。Robert C. Martion对 单一职责原则 有个论述是： 把相同原因而变化的东西聚合到一起，而把不同原因而变化的东西分离开。这个论述很好的强调了内聚性这个概念。&lt;/p&gt;
&lt;p&gt;那么服务要多小才算足够小呢？&lt;/p&gt;
&lt;p&gt;要考虑这些因素：服务越小，微服务架构的优点和缺点也就越明显。使用的服务越小，独立性带来的好处就越多。但是管理大量服务也会越复杂，之后会将这一复杂性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. 自治性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一个微服务就是一个独立的实体。它可以独立的部署在PAAS(Platform As A Service,平台即服务)上，也可以作为一个操作系统进程存在。&lt;/p&gt;
&lt;p&gt;我们要尽量避免把多个服务部署到一台机器上。&lt;/p&gt;
&lt;p&gt;服务之间通过网络调用进行通信，从而加强服务之间的隔离性，避免紧耦合。&lt;/p&gt;
&lt;p&gt;这些服务应该可以彼此间独立的进行修改，并且某个服务的部署不应该引起该服务消费方的变动。&lt;/p&gt;
&lt;p&gt;服务会暴露出API(Application Programming Interface , 应用编程接口)，然后服务之间通过这些API进行通信。API的实现技术应该避免与消费方耦合，这就意味着应该选择与具体技术不相关的API实现方式，以保证技术的选择不被限制。&lt;/p&gt;

&lt;p&gt;如果系统没有很好的解耦，那么一旦出现问题，所有的功能都将不可用。有个黄金法则是：你是否能修改一个服务并对其进行部署，而不影响其他任何服务？&lt;/p&gt;
&lt;p&gt;为了达到解耦的目的，你需要正确的建模服务和API。&lt;/p&gt;
&lt;h4&gt;二.主要好处&lt;/h4&gt;
&lt;p&gt;它的主要好处包括：技术异构性、弹性、扩展、简化部署、与组织结构相匹配、可组合性、对可替代性的优化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 技术异构性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在一个由多个服务相互协作的系统中，可以在不同的服务中使用最适合该服务的技术。尝试使用一种适合所有场景的标准化技术，会使得所有的场景都无法得到很好的支持。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1182288/201907/1182288-20190726212427007-378873471.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.弹性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;弹性工程学的一个关键概念是&lt;span&gt;舱壁&lt;/span&gt;。如果系统中的一个组件不可用了，但并没有导致级联故障，那么系统的其他部分还可以正常运行。服务边界就是一个很显然的舱壁。&lt;/p&gt;
&lt;p&gt;在单块系统中，如果服务不可用，那么所有功能都会不可用。&lt;/p&gt;
&lt;p&gt;对于单块服务的系统而言，可以通过将相同的实例运行在不同的机器上来降低功能完全不可用的概率，&lt;/p&gt;
&lt;p&gt;然而微服务系统本身就能够很好的处理服务不可用和功能降级问题。&lt;/p&gt;
&lt;p&gt;微服务系统可以改进弹性，但你还是需要谨慎对待，因为一旦使用了分布式系统，网络会是一个问题，而且还有机器，我们需要了解问题出现时应该如何对用户进行展示。&lt;/p&gt;
&lt;blockquote readability=&quot;20.805049668874&quot;&gt;
&lt;p&gt; 舱壁模式：&lt;/p&gt;
&lt;pre readability=&quot;8&quot;&gt;
舱壁模式（Bulkhead）隔离了每个工作负载或服务的关键资源，如连接池、内存和CPU。
使用舱壁避免了单个工作负载（或服务）消耗掉所有资源，从而导致其他服务出现故障的场景。
这种模式主要是通过防止由一个服务引起的级联故障来增加系统的弹性。&lt;p&gt;下图描述了实施舱壁的简单的示例场景：在左侧，微服务A，用同一个连接池去请求X和Y两个服务。&lt;br/&gt;如果服务X或服务的Y其中任何一个行为异常，这会影响连接池的整体行为。&lt;br/&gt;如果舱壁模式实现，如该图所示的右侧，即使微服务X被错误操作，只有池X将受到影响。&lt;br/&gt;微服务Y可以继续为应用程序提供服务.
&lt;/p&gt;&lt;/pre&gt;
&lt;img src=&quot;https://img2018.cnblogs.com/blog/1182288/201907/1182288-20190726213745615-1453396525.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;舱壁的概念在软件开发中可以被应用在隔离资源上。&lt;/p&gt;
&lt;p&gt;而在工业中使用舱壁将船舶划分为几个部分，以便在船体破坏的情况下，可以将船舶各个部件密封起来。&lt;/p&gt;
&lt;p&gt;泰坦尼克号沉没的主要原因之一是其舱壁设计失败，水可以通过上面的甲板倒在舱壁的顶部，最后整个船淹没&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1182288/201907/1182288-20190726214003418-1252852853.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;参考网址：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://lvjun106.iteye.com/blog/2427353&quot;&gt;https://lvjun106.iteye.com/blog/2427353&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/lfs2640666960/p/9543096.html&quot;&gt;https://www.cnblogs.com/lfs2640666960/p/9543096.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt; 3.扩展&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;庞大的单块服务只能作为一个整体进行扩展。即使系统中只有一小部分存在性能问题，也需要对整个服务进行扩展。&lt;/p&gt;
&lt;p&gt;如果使用较小的多个服务，则可以只对需要扩展的服务进行扩展，这样就可以把那些不需要扩展的服务运行在更小的，性能稍差的硬件上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1182288/201907/1182288-20190726214659016-1681303160.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在使用类似Amazon云服务之类的平台时，也可以只对需要的服务进行扩展，从而节省成本。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.简化部署&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在有几百万行的单块应用程序中，即使只修改一行代码，也需要重新部署整个应用程序才能够发布该变更。&lt;/p&gt;
&lt;p&gt;这种部署的影响很大，风险很高，因此相关干系人不敢轻易做部署。于是在实际操作中，部署的频率就会变的很低。&lt;/p&gt;
&lt;p&gt;这意味着在两次发布之间我们对软件做了很多功能增强，但直到最后一刻才把这些大量的变更一次性发布到生成环境中。&lt;/p&gt;
&lt;p&gt;这时，另外一个问题就显现出来了：两次发布之间的差异越大，出错的可能性就更大。&lt;/p&gt;

&lt;p&gt;在微服务架构中，各个服务的部署时独立的，这样就可以更快的多特定部分的代码进行部署。&lt;/p&gt;
&lt;p&gt;如果真的出了问题，也只会影响一个服务，并且容器快速回滚，也意味着客户可以更快的使用我们开发的新功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.与组织结构相匹配&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;微服务的架构可以很好的将架构和组织结构相匹配，避免出现过大的代码库，从而获得理想的团队大小和生产力。&lt;/p&gt;
&lt;p&gt;服务的所有权也可以在团队之间迁移，从而避免异地团队的出现。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. 可组合性&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;分布式系统和面向服务架构 声称的主要好处是易于重用已有功能。&lt;/p&gt;
&lt;p&gt;在微服务架构中，根据不同的目的，人们可以通过不同的方式使用同一个功能，在考虑客户如何使用该软件时，这一点尤其重要。&lt;/p&gt;
&lt;p&gt;现在我们需要考虑的应用种类包括Web , 原生应用、移动端Web 、平板应用及可穿戴设备等，针对每一种都应该考虑如何对已有的功能进行组合来实现这些应用。&lt;/p&gt;
&lt;p&gt;在微服务架构中，系统会开放很多接口供外部使用。当情况发生改变时，可以使用不同的方式构建应用，而整体化应用程序只能提供一个非常粗粒度的接口供外部使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7.可替代性的优化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使用微服务架构的团队可以在需要时轻易重写服务，或者删除不再使用的服务。当一个代码库只有几百行时，人们也不会对它有太多感情上的依赖，所以很容易替换它。&lt;/p&gt;
&lt;h4&gt;三. 面向服务的架构&lt;/h4&gt;
&lt;p&gt;SOA(Service-Oriented Architecture, 面向服务的架构)是一种设计方法，其中包含多个服务，而服务之间通过配合最终会提供一系列功能。&lt;/p&gt;
&lt;p&gt;一个服务 通常以独立的形式存在于操作系统进程中。服务之间通过网络调用，而非采用进程内调用的方式进行通信。&lt;/p&gt;
&lt;p&gt;它的目标是在不影响其他任何人的情况下透明的替换一个服务，只要替换之后的服务的外部接口没有太大变化即可。这种性质能够大大简化软件运维甚至软件重写的过程。&lt;/p&gt;
&lt;p&gt;实施SOA时，会遇到这些问题：通信协议(例如SOAP)如何选择、第三方中间件如何选择、服务粒度如何确定等。&lt;/p&gt;
&lt;h4&gt;四. 其他分解技术&lt;/h4&gt;
&lt;p&gt;当你开始使用微服务时会发现，很多基于微服务的架构主要有两个优势：&lt;/p&gt;
&lt;p&gt;首先它具有较小的粒度，其次它能够在解决问题的方法上给予你更多的选择。&lt;/p&gt;
&lt;p&gt;那么其他的分解技术是否也有相应的好处呢？&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. 共享库&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;基本上所有的语言都支持将整个代码库分解成为多个库，这是一种非常标准的分解技术。这些库可以由第三方或者自己的组织提供。&lt;/p&gt;
&lt;p&gt;不同的团队和服务可以通过库的形式共享功能。&lt;/p&gt;
&lt;p&gt;团队可以围绕库来进行组织，但是库有一些缺点。&lt;/p&gt;
&lt;p&gt;　　首先，你无法选择异构的技术。一般来说，这些库只能在同一种语言中，或者至少在同一个平台上使用。&lt;/p&gt;
&lt;p&gt;　　其次，除非你使用的是动态链接库，否则每次当库有更新的时候，都需要重新部署整个进程，以至于无法独立的部署变更。&lt;/p&gt;
&lt;p&gt;　　而最糟糕的影响可能是你会缺乏一个比较明显的接缝来建立架构的安全性保护措施，从而无法确保系统的弹性。&lt;/p&gt;
&lt;p&gt;如果使用共享代码来做服务之间的通信的话，那么它会成为一个耦合点。&lt;/p&gt;
&lt;p&gt;服务之间可以并且应该大量使用第三方库来重用公共代码，但有时候效果不太好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.模块&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;除了简单的库之外，有些语言提供了自己的模块分解技术。它们允许对模块进行生命周期管理，这样就可以把模块部署到运行的进程中，并且可以不停止整个进程的前提下对某个模块进行修改。&lt;/p&gt;
&lt;p&gt;作为一个与具体技术相关的模块分解技术，OSGI (Open Source Gateway ,Initiative , 开放服务网关协议)值得一提。&lt;/p&gt;
&lt;h4&gt;五. 没有银弹&lt;/h4&gt;
&lt;p&gt;（没有银弹指的是没有任何一项技术或方法可使软件工程的生产力在十年内提高十倍。）&lt;/p&gt;
&lt;p&gt;如果你之后想要使用微服务，那么你需要面对分布式系统需要面对的复杂性；&lt;/p&gt;
&lt;p&gt;如果你过去的经验更多的是关于单块系统，那么为了得到微服务的好处，你需要在部署，测试，监控等方面做很多工作。&lt;/p&gt;
&lt;p&gt;你还需要考虑如何扩展系统，并且保证它们的弹性。&lt;/p&gt;
&lt;p&gt;如果你发现还需要处理类似分布式事务或者与CAP相关的问题，也不要感到惊讶。&lt;/p&gt;

</description>
<pubDate>Fri, 26 Jul 2019 23:22:00 +0000</pubDate>
<dc:creator>Vincent-yuan</dc:creator>
<og:description>一.什么是微服务 微服务就是一些协同工作的小而自治的服务。 1. 服务要足够小 在使用微服务的时候，内聚性是一个很重要的概念。Robert C. Martion对 单一职责原则 有个论述是： </og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/Vincent-yuan/p/11253512.html</dc:identifier>
</item>
<item>
<title>Flink实战(八) - Streaming Connectors 编程 - JavaEdge</title>
<link>http://www.cnblogs.com/JavaEdge/p/11253682.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/JavaEdge/p/11253682.html</guid>
<description>&lt;h2 id=&quot;预定义的源和接收器&quot;&gt;1.1 预定义的源和接收器&lt;/h2&gt;
&lt;p&gt;Flink内置了一些基本数据源和接收器，并且始终可用。该预定义的数据源包括文件，目录和插socket，并从集合和迭代器摄取数据。该预定义的数据接收器支持写入文件和标准输入输出及socket。&lt;/p&gt;
&lt;h2 id=&quot;绑定连接器&quot;&gt;1.2 绑定连接器&lt;/h2&gt;
&lt;p&gt;连接器提供用于与各种第三方系统连接的代码。目前支持这些系统：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Apache Kafka (source/sink)&lt;/li&gt;
&lt;li&gt;Apache Cassandra (sink)&lt;/li&gt;
&lt;li&gt;Amazon Kinesis Streams (source/sink)&lt;/li&gt;
&lt;li&gt;Elasticsearch (sink)&lt;/li&gt;
&lt;li&gt;Hadoop FileSystem (sink)&lt;/li&gt;
&lt;li&gt;RabbitMQ (source/sink)&lt;/li&gt;
&lt;li&gt;Apache NiFi (source/sink)&lt;/li&gt;
&lt;li&gt;Twitter Streaming API (source)&lt;/li&gt;
&lt;li&gt;Google PubSub (source/sink)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;要在应用程序中使用其中一个连接器，通常需要其他第三方组件，例如数据存储或消息队列的服务器。&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;虽然本节中列出的流连接器是Flink项目的一部分，并且包含在源版本中，但它们不包含在二进制分发版中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;apache-bahir中的连接器&quot;&gt;1.3 Apache Bahir中的连接器&lt;/h2&gt;
&lt;p&gt;Flink的其他流处理连接器正在通过Apache Bahir发布，包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Apache ActiveMQ (source/sink)&lt;/li&gt;
&lt;li&gt;Apache Flume (sink)&lt;/li&gt;
&lt;li&gt;Redis (sink)&lt;/li&gt;
&lt;li&gt;Akka (sink)&lt;/li&gt;
&lt;li&gt;Netty (source)&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;其他连接到flink的方法&quot;&gt;1.4 其他连接到Flink的方法&lt;/h2&gt;
&lt;h3 id=&quot;通过异步i-o进行数据渲染&quot;&gt;1.4.1 通过异步I / O进行数据渲染&lt;/h3&gt;
&lt;p&gt;使用连接器不是将数据输入和输出Flink的唯一方法。一种常见的模式是在一个Map或多个FlatMap 中查询外部数据库或Web服务以渲染主数据流。&lt;/p&gt;
&lt;p&gt;Flink提供了一个用于异步I / O的API， 以便更有效，更稳健地进行这种渲染。&lt;/p&gt;
&lt;h3 id=&quot;可查询状态&quot;&gt;1.4.2 可查询状态&lt;/h3&gt;
&lt;p&gt;当Flink应用程序将大量数据推送到外部数据存储时，这可能会成为I / O瓶颈。如果所涉及的数据具有比写入更少的读取，则更好的方法可以是外部应用程序从Flink获取所需的数据。在可查询的状态界面，允许通过Flink被管理的状态，按需要查询支持这个。&lt;/p&gt;

&lt;p&gt;此连接器提供一个Sink，可将分区文件写入任一Hadoop文件系统支持的文件系统 。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;要使用此连接器，请将以下依赖项添加到项目中：&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/ag6nihqu87.png&quot;/&gt;&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/nff2qx4gs2.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;请注意，流连接器当前不是二进制发布的一部分&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;bucketing-file-sink&quot;&gt;2.1 Bucketing File Sink&lt;/h2&gt;
&lt;p&gt;可以配置分段行为以及写入，但我们稍后会介绍。这是可以创建一个默认情况下汇总到按时间拆分的滚动文件的存储槽的方法&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Java&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/x2kpj2lm23.png&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Scala&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/b4ryyl9i9s.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;唯一必需的参数是存储桶的基本路径。可以通过指定自定义bucketer，写入器和批量大小来进一步配置接收器。&lt;/p&gt;
&lt;p&gt;默认情况下，当数据元到达时，分段接收器将按当前系统时间拆分，并使用日期时间模式&quot;yyyy-MM-dd--HH&quot;命名存储区。这种模式传递给 DateTimeFormatter使用当前系统时间和JVM的默认时区来形成存储桶路径。用户还可以为bucketer指定时区以格式化存储桶路径。每当遇到新日期时，都会创建一个新存储桶。&lt;/p&gt;
&lt;p&gt;例如，如果有一个包含分钟作为最精细粒度的模式，将每分钟获得一个新桶。每个存储桶本身都是一个包含多个部分文件的目录：接收器的每个并行实例将创建自己的部件文件，当部件文件变得太大时，接收器也会在其他文件旁边创建新的部件文件。当存储桶变为非活动状态时，将刷新并关闭打开的部件文件。如果存储桶最近未写入，则视为非活动状态。默认情况下，接收器每分钟检查一次非活动存储桶，并关闭任何超过一分钟未写入的存储桶。setInactiveBucketCheckInterval()并 setInactiveBucketThreshold()在一个BucketingSink。&lt;/p&gt;
&lt;p&gt;也可以通过指定自定义bucketer setBucketer()上BucketingSink。如果需要，bucketer可以使用数据元或元组的属性来确定bucket目录。&lt;/p&gt;
&lt;p&gt;默认编写器是StringWriter。这将调用toString()传入的数据元并将它们写入部分文件，由换行符分隔。在a setWriter() 上指定自定义编写器使用BucketingSink。如果要编写Hadoop SequenceFiles，可以使用提供的 SequenceFileWriter，也可以配置为使用压缩。&lt;/p&gt;
&lt;p&gt;有两个配置选项指定何时应关闭零件文件并启动新零件文件：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;通过设置批量大小（默认部件文件大小为384 MB）&lt;/li&gt;
&lt;li&gt;通过设置批次滚动时间间隔（默认滚动间隔为&lt;code&gt;Long.MAX_VALUE&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;当满足这两个条件中的任何一个时，将启动新的部分文件。看如下例子：&lt;/p&gt;
&lt;p&gt;然而这种方式创建了太多小文件，不适合HDFS！仅供娱乐！&lt;/p&gt;

&lt;h2 id=&quot;简介&quot;&gt;3.1 简介&lt;/h2&gt;
&lt;p&gt;此连接器提供对Apache Kafka服务的事件流的访问。&lt;/p&gt;
&lt;p&gt;Flink提供特殊的Kafka连接器，用于从/向Kafka主题读取和写入数据。Flink Kafka Consumer集成了Flink的检查点机制，可提供一次性处理语义。为实现这一目标，Flink并不完全依赖Kafka的消费者群体偏移跟踪，而是在内部跟踪和检查这些偏移。&lt;/p&gt;
&lt;p&gt;为用例和环境选择一个包（maven artifact id）和类名。对于大多数用户来说，FlinkKafkaConsumer08（部分flink-connector-kafka）是合适的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/3bovukxvkf.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;然后，导入maven项目中的连接器:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/vcelq8duyv.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_33589510/article/details/97064338&quot;&gt;环境配置参考&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;zookeeper安装及配置&quot;&gt;3.2 ZooKeeper安装及配置&lt;/h2&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;由于Kafka控制台脚本对于基于Unix和Windows的平台不同，因此在Windows平台上使用bin  windows \而不是bin /，并将脚本扩展名更改为.bat。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;step-1下载代码&quot;&gt;Step 1:下载代码&lt;/h3&gt;
&lt;h3 id=&quot;step-2-启动服务器&quot;&gt;Step 2: 启动服务器&lt;/h3&gt;
&lt;p&gt;Kafka使用ZooKeeper，因此如果还没有ZooKeeper服务器，则需要先启动它。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;后台模式启动&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/0019k9uuv2.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;step-3-创建一个主题&quot;&gt;Step 3: 创建一个主题&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;创建topic&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/mvmr36phlf.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;step-4-发送一些消息&quot;&gt;Step 4: 发送一些消息&lt;/h3&gt;
&lt;p&gt;Kafka附带一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。 默认情况下，每行将作为单独的消息发送。&lt;/p&gt;
&lt;p&gt;运行生产者，然后在控制台中键入一些消息以发送到服务器。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;启动生产者&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/9w7adksobj.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;step-5-启动一个消费者&quot;&gt;Step 5: 启动一个消费者&lt;/h3&gt;
&lt;p&gt;Kafka还有一个命令行使用者，它会将消息转储到标准输出。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;分屏，新建消费端&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/9hy3pxpykr.png&quot;/&gt;&lt;/li&gt;
&lt;li&gt;在不同的终端中运行上述每个命令，那么现在应该能够在生产者终端中键入消息并看到它们出现在消费者终端中&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/geg6jzbjvx.png&quot;/&gt;&lt;br/&gt;所有命令行工具都有其他选项; 运行不带参数的命令将显示更详细地记录它们的使用信息。&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;kafka-1.0.0-connector&quot;&gt;3.4 Kafka 1.0.0+ Connector&lt;/h2&gt;
&lt;p&gt;从Flink 1.7开始，有一个新的通用Kafka连接器，它不跟踪特定的Kafka主要版本。 相反，它在Flink发布时跟踪最新版本的Kafka。&lt;/p&gt;
&lt;p&gt;如果您的Kafka代理版本是1.0.0或更高版本，则应使用此Kafka连接器。 如果使用旧版本的Kafka（0.11,0.10,0.9或0.8），则应使用与代理版本对应的连接器。&lt;/p&gt;
&lt;h3 id=&quot;兼容性&quot;&gt;兼容性&lt;/h3&gt;
&lt;p&gt;通过Kafka客户端API和代理的兼容性保证，通用Kafka连接器与较旧和较新的Kafka代理兼容。 它与版本0.11.0或更高版本兼容，具体取决于所使用的功能。&lt;/p&gt;
&lt;h3 id=&quot;将kafka-connector从0.11迁移到通用v1.10新增&quot;&gt;将Kafka Connector从0.11迁移到通用(V1.10新增）&lt;/h3&gt;
&lt;p&gt;要执行迁移，请参阅&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/ops/upgrading.html&quot;&gt;升级作业和Flink版本指南&lt;/a&gt;和&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;在整个过程中使用Flink 1.9或更新版本。&lt;/li&gt;
&lt;li&gt;不要同时升级Flink和操作符。&lt;/li&gt;
&lt;li&gt;确保您作业中使用的Kafka Consumer和/或Kafka Producer分配了唯一标识符（uid）：&lt;/li&gt;
&lt;li&gt;使用stop with savepoint功能获取保存点（例如，使用stop --withSavepoint）CLI命令。&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;用法&quot;&gt;用法&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;要使用通用Kafka连接器，请为其添加依赖关系：&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/0r7ynpj3jy.png&quot;/&gt;&lt;br/&gt;然后实例化新源（FlinkKafkaConsumer）&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/f2sdrm6oac.png&quot;/&gt;&lt;br/&gt;Flink Kafka Consumer是一个流数据源，可以从Apache Kafka中提取并行数据流。 使用者可以在多个并行实例中运行，每个实例都将从一个或多个Kafka分区中提取数据。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Flink Kafka Consumer参与了检查点，并保证在故障期间没有数据丢失，并且计算处理元素“恰好一次”。（注意：这些保证自然会假设Kafka本身不会丢失任何数据。）&lt;/p&gt;
&lt;p&gt;请注意，Flink在内部将偏移量作为其分布式检查点的一部分进行快照。 承诺给Kafka的抵消只是为了使外部的进展观与Flink对进展的看法同步。 这样，监控和其他工作可以了解Flink Kafka消费者在多大程度上消耗了一个主题。&lt;/p&gt;
&lt;p&gt;和接收器（FlinkKafkaProducer）。&lt;/p&gt;
&lt;p&gt;除了从模块和类名中删除特定的Kafka版本之外，API向后兼容Kafka 0.11连接器。&lt;/p&gt;
&lt;h2 id=&quot;kafka消费者&quot;&gt;3.5 Kafka消费者&lt;/h2&gt;
&lt;p&gt;Flink的Kafka消费者被称为FlinkKafkaConsumer08（或09Kafka 0.9.0.x等）。它提供对一个或多个Kafka主题的访问。&lt;/p&gt;
&lt;p&gt;构造函数接受以下参数：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;主题名称/主题名称列表&lt;/li&gt;
&lt;li&gt;DeserializationSchema / KeyedDeserializationSchema用于反序列化来自Kafka的数据&lt;/li&gt;
&lt;li&gt;Kafka消费者的属性。需要以下属性：
&lt;ul&gt;&lt;li&gt;“bootstrap.servers”（以逗号分隔的Kafka经纪人名单）&lt;/li&gt;
&lt;li&gt;“zookeeper.connect”（逗号分隔的Zookeeper服务器列表）（仅Kafka 0.8需要）&lt;/li&gt;
&lt;li&gt;“group.id”消费者群组的ID&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/dnfv7v69xk.png&quot;/&gt;&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/z3m8tclzea.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/yi7k2ughpa.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/dihfy67k7v.png&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-deserializationschema&quot;&gt;The DeserializationSchema&lt;/h3&gt;
&lt;p&gt;Flink Kafka Consumer需要知道如何将Kafka中的二进制数据转换为Java / Scala对象。&lt;/p&gt;
&lt;p&gt;在 DeserializationSchema允许用户指定这样的一个架构。T deserialize(byte[] message) 为每个Kafka消息调用该方法，从Kafka传递值。&lt;/p&gt;
&lt;p&gt;从它开始通常很有帮助AbstractDeserializationSchema，它负责将生成的Java / Scala类型描述为Flink的类型系统。实现vanilla的用户DeserializationSchema需要自己实现该getProducedType(...)方法。&lt;/p&gt;
&lt;p&gt;为了访问Kafka消息的键和值，KeyedDeserializationSchema具有以下deserialize方法&lt;code&gt;T deserialize（byte [] messageKey，byte [] message，String topic，int partition，long offset）&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;为方便起见，Flink提供以下模式：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;TypeInformationSerializationSchema（和TypeInformationKeyValueSerializationSchema）创建基于Flink的模式TypeInformation。如果Flink编写和读取数据，这将非常有用。此模式是其他通用序列化方法的高性能Flink替代方案。&lt;/li&gt;
&lt;li&gt;JsonDeserializationSchema（和JSONKeyValueDeserializationSchema）将序列化的JSON转换为ObjectNode对象，可以使用objectNode.get（“field”）作为（Int / String / ...）（）从中访问字段。KeyValue objectNode包含一个“key”和“value”字段，其中包含所有字段，以及一个可选的“元数据”字段，用于公开此消息的偏移量/分区/主题。&lt;/li&gt;
&lt;li&gt;AvroDeserializationSchema它使用静态提供的模式读取使用Avro格式序列化的数据。它可以从Avro生成的类（AvroDeserializationSchema.forSpecific(...)）中推断出模式，也可以GenericRecords 使用手动提供的模式（with AvroDeserializationSchema.forGeneric(...)）。此反序列化架构要求序列化记录不包含嵌入式架构。&lt;br/&gt;- 还有一个可用的模式版本，可以在Confluent Schema Registry中查找编写器的模式（用于编写记录的 模式）。使用这些反序列化模式记录将使用从模式注册表中检索的模式进行读取，并转换为静态提供的模式（通过 ConfluentRegistryAvroDeserializationSchema.forGeneric(...)或ConfluentRegistryAvroDeserializationSchema.forSpecific(...)）。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;要使用此反序列化模式，必须添加以下附加依赖项：&lt;/p&gt;
&lt;p&gt;当遇到因任何原因无法反序列化的损坏消息时，有两个选项 - 从deserialize(...)方法中抛出异常将导致作业失败并重新启动，或者返回null以允许Flink Kafka使用者以静默方式跳过损坏的消息。请注意，由于使用者的容错能力（请参阅下面的部分以获取更多详细信息），因此对损坏的消息执行失败将使消费者尝试再次反序列化消息。因此，如果反序列化仍然失败，则消费者将在该损坏的消息上进入不间断重启和失败循环。&lt;/p&gt;
&lt;h2 id=&quot;kafka生产者&quot;&gt;3.6 Kafka生产者&lt;/h2&gt;
&lt;p&gt;Flink的Kafka Producer被称为FlinkKafkaProducer011（或010 对于Kafka 0.10.0.x版本。或者直接就是FlinkKafkaProducer，对于Kafka&amp;gt;=1.0.0的版本来说）。&lt;/p&gt;
&lt;p&gt;它允许将记录流写入一个或多个Kafka主题。&lt;/p&gt;
&lt;h3 id=&quot;自应用&quot;&gt;自应用&lt;/h3&gt;
&lt;p&gt;上面的示例演示了创建Flink Kafka Producer以将流写入单个Kafka目标主题的基本用法。对于更高级的用法，还有其他构造函数变体允许提供以下内容：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;strong&gt;提供自定义属性&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;生产者允许为内部的KafkaProducer提供自定义属性配置。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;自定义分区程序&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;将记录分配给特定分区，可以为FlinkKafkaPartitioner构造函数提供实现。将为流中的每个记录调用此分区程序，以确定应将记录发送到的目标主题的确切分区。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;高级序列化模式&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;与消费者类似，生产者还允许使用调用的高级序列化模式KeyedSerializationSchema，该模式允许单独序列化键和值。它还允许覆盖目标主题，以便一个生产者实例可以将数据发送到多个主题。&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;kafka消费者开始位置配置&quot;&gt;3.8 Kafka消费者开始位置配置&lt;/h2&gt;
&lt;p&gt;Flink Kafka Consumer允许配置如何确定Kafka分区的起始位置。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Java&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/6ojgorfku0.png&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Scala&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/rihywds3uf.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Flink Kafka Consumer的所有版本都具有上述明确的起始位置配置方法。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;setStartFromGroupOffsets（默认行为）&lt;br/&gt;从group.idKafka代理（或Zookeeper for Kafka 0.8）中的消费者组（在消费者属性中设置）提交的偏移量开始读取分区。如果找不到分区的偏移量，auto.offset.reset将使用属性中的设置。&lt;/li&gt;
&lt;li&gt;setStartFromEarliest()/ setStartFromLatest()&lt;br/&gt;从最早/最新记录开始。在这些模式下，Kafka中的承诺偏移将被忽略，不会用作起始位置。&lt;/li&gt;
&lt;li&gt;setStartFromTimestamp(long)&lt;br/&gt;从指定的时间戳开始。对于每个分区，时间戳大于或等于指定时间戳的记录将用作起始位置。如果分区的最新记录早于时间戳，则只会从最新记录中读取分区。在此模式下，Kafka中的已提交偏移将被忽略，不会用作起始位置。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;还可以指定消费者应从每个分区开始的确切偏移量：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Java&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/4hkgxd2zd4.png&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Scala&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/ijcese7inm.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;上面的示例将使用者配置为从主题的分区0,1和2的指定偏移量开始myTopic。偏移值应该是消费者应为每个分区读取的下一条记录。请注意，如果使用者需要读取在提供的偏移量映射中没有指定偏移量的分区，则它将回退到setStartFromGroupOffsets()该特定分区的默认组偏移行为（即）。&lt;/p&gt;
&lt;p&gt;请注意，当作业从故障中自动恢复或使用保存点手动恢复时，这些起始位置配置方法不会影响起始位置。在恢复时，每个Kafka分区的起始位置由存储在保存点或检查点中的偏移量确定。&lt;/p&gt;
&lt;h2 id=&quot;kafka生产者和容错&quot;&gt;3.9 Kafka生产者和容错&lt;/h2&gt;
&lt;h3 id=&quot;kafka-0.8&quot;&gt;Kafka 0.8&lt;/h3&gt;
&lt;p&gt;在0.9之前，Kafka没有提供任何机制来保证至少一次或恰好一次的语义。&lt;/p&gt;
&lt;h3 id=&quot;kafka-0.9和0.10&quot;&gt;Kafka 0.9和0.10&lt;/h3&gt;
&lt;p&gt;启用Flink的检查点时，FlinkKafkaProducer09和FlinkKafkaProducer010 能提供&lt;strong&gt;至少一次&lt;/strong&gt;传输保证。&lt;/p&gt;
&lt;p&gt;除了开启Flink的检查点，还应该配置setter方法：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;strong&gt;setLogFailuresOnly(boolean)&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;默认为false。启用此选项将使生产者仅记录失败日志而不是捕获和重新抛出它们。这大体上就是计数已成功的记录，即使它从未写入目标Kafka主题。这必须设为false对于确保 &lt;em&gt;&lt;strong&gt;至少一次&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;setFlushOnCheckpoint(boolean)&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;默认为true。启用此函数后，Flink的检查点将在检查点成功之前等待检查点时的任何动态记录被Kafka确认。这可确保检查点之前的所有记录都已写入Kafka。必须开启，对于确保 &lt;em&gt;&lt;strong&gt;至少一次&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;总之，默认情况下，Kafka生成器对版本0.9和0.10具有&lt;strong&gt;至少一次&lt;/strong&gt;保证，即&lt;/p&gt;
&lt;p&gt;setLogFailureOnly设置为false和setFlushOnCheckpoint设置为true。&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;默认情况下，重试次数设置为“0”。这意味着当setLogFailuresOnly设置为时false，生产者会立即失败，包括Leader更改。&lt;br/&gt;默认情况下，该值设置为“0”，以避免重试导致目标主题中出现重复消息。对于经常更改代理的大多数生产环境，建议将重试次数设置为更高的值。&lt;br/&gt;Kafka目前没有生产者事务，因此Flink在Kafka主题里无法保证恰好一次交付&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;kafka-0.11&quot;&gt;Kafka &amp;gt;= 0.11&lt;/h3&gt;
&lt;p&gt;启用Flink的检查点后，FlinkKafkaProducer011&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;对于Kafka &amp;gt;= 1.0.0版本是FlinkKafkaProduce&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以提供准确的一次交付保证。&lt;/p&gt;
&lt;p&gt;除了启用Flink的检查点，还可以通过将适当的语义参数传递给FlinkKafkaProducer011,选择三种不同的算子操作模式&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;strong&gt;Semantic.NONE&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/9jom38mdbi.png&quot;/&gt;Flink啥都不保证。生成的记录可能会丢失，也可能会重复。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Semantic.AT_LEAST_ONCE（默认设置）&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/8iio0or1i1.png&quot;/&gt;&lt;br/&gt;类似于setFlushOnCheckpoint(true)在 FlinkKafkaProducer010。这可以保证不会丢失任何记录（尽管它们可以重复）。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;strong&gt;Semantic.EXACTLY_ONCE&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/zwbecndnsc.png&quot;/&gt;使用Kafka事务提供恰好一次的语义。每当您使用事务写入Kafka时，不要忘记为任何从Kafka消费记录的应用程序设置所需的isolation.level（read_committed 或read_uncommitted- 后者为默认值）。&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;注意事项&quot;&gt;注意事项&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Semantic.EXACTLY_ONCE&lt;/strong&gt;&lt;/em&gt; 模式依赖于在从所述检查点恢复之后提交在获取检查点之前启动的事务的能力。如果Flink应用程序崩溃和完成重启之间的时间较长，那么Kafka的事务超时将导致数据丢失（Kafka将自动中止超过超时时间的事务）。考虑到这一点，请根据预期的停机时间适当配置事务超时。&lt;/p&gt;
&lt;p&gt;Kafka broker默认 &lt;em&gt;transaction.max.timeout.ms&lt;/em&gt; 设置为15分钟。此属性不允许为生产者设置大于其值的事务超时。&lt;/p&gt;
&lt;p&gt;FlinkKafkaProducer011默认情况下，将_transaction.timeout.msproducer config_中的属性设置为1小时，因此_transaction.max.timeout.ms_在使用 &lt;em&gt;&lt;strong&gt;Semantic.EXACTLY_ONCE&lt;/strong&gt;&lt;/em&gt; 模式之前应该增加 该属性。&lt;/p&gt;
&lt;p&gt;在_read_committed_模式中KafkaConsumer，任何未完成的事务（既不中止也不完成）将阻止来自给定Kafka主题的所有读取超过任何未完成的事务。换言之，遵循以下事件顺序：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;用户事务1开启并写记录&lt;/li&gt;
&lt;li&gt;用户事务2开启并写了一些其他记录&lt;/li&gt;
&lt;li&gt;用户提交事务2&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;即使事务2已经提交了记录，在事务1提交或中止之前，消费者也不会看到它们。这有两个含义：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;首先，在Flink应用程序的正常工作期间，用户可以预期Kafka主题中生成的记录的可见性会延迟，等于已完成检查点之间的平均时间。&lt;/li&gt;
&lt;li&gt;其次，在Flink应用程序失败的情况下，读者将阻止此应用程序编写的主题，直到应用程序重新启动或配置的事务超时时间过去为止。此注释仅适用于有多个代理/应用程序写入同一Kafka主题的情况。&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Semantic.EXACTLY_ONCE&lt;/strong&gt;&lt;/em&gt; 模式为每个FlinkKafkaProducer011实例使用固定大小的KafkaProducers池。每个检查点使用其中一个生产者。如果并发检查点的数量超过池大小，FlinkKafkaProducer011 将引发异常并将使整个应用程序失败。请相应地配置最大池大小和最大并发检查点数。&lt;br/&gt;&lt;em&gt;&lt;strong&gt;Semantic.EXACTLY_ONCE&lt;/strong&gt;&lt;/em&gt; 采取所有可能的措施，不要留下任何阻碍消费者阅读Kafka主题的延迟事务，这是必要的。但是，如果Flink应用程序在第一个检查点之前失败，则在重新启动此类应用程序后，系统中没有关于先前池大小的信息。因此，在第一个检查点完成之前按比例缩小Flink应用程序是不安全的 &lt;em&gt;&lt;strong&gt;FlinkKafkaProducer011.SAFE_SCALE_DOWN_FACTOR&lt;/strong&gt;&lt;/em&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;kafka消费者及其容错&quot;&gt;3.10 Kafka消费者及其容错&lt;/h2&gt;
&lt;p&gt;启用Flink的检查点后，Flink Kafka Consumer将使用主题中的记录，并以一致的方式定期检查其所有Kafka偏移以及其他 算子操作的状态。如果作业失败，Flink会将流式程序恢复到最新检查点的状态，并从存储在检查点中的偏移量开始重新使用来自Kafka的记录。&lt;/p&gt;
&lt;p&gt;因此，绘制检查点的间隔定义了程序在发生故障时最多可以返回多少。&lt;/p&gt;
&lt;h3 id=&quot;检查点常用参数&quot;&gt;检查点常用参数&lt;/h3&gt;
&lt;h4 id=&quot;enablecheckpointing&quot;&gt;enableCheckpointing&lt;/h4&gt;
&lt;p&gt;启用流式传输作业的检查点。 将定期快照流式数据流的分布式状态。 如果发生故障，流数据流将从最新完成的检查点重新启动。&lt;/p&gt;
&lt;p&gt;该作业在给定的时间间隔内定期绘制检查点。 状态将存储在配置的状态后端。&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;此刻未正确支持检查点迭代流数据流。 如果“force”参数设置为true，则系统仍将执行作业。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/zxijcyy22k.png&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;setcheckpointingmode&quot;&gt;setCheckpointingMode&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/a4p6877waw.png&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;setcheckpointtimeout&quot;&gt;setCheckpointTimeout&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/b8q32hhifz.png&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;setmaxconcurrentcheckpoints&quot;&gt;setMaxConcurrentCheckpoints&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/a0vqjm0u46.png&quot;/&gt;&lt;/h4&gt;
&lt;p&gt;要使用容错的Kafka使用者，需要在运行环境中启用拓扑的检查点：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Scala&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/3rbkt5t5uc.png&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Java&lt;br/&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/1752328/mqmuwrzdt8.png&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;另请注意，如果有足够的处理插槽可用于重新启动拓扑，则Flink只能重新启动拓扑。因此，如果拓扑由于丢失了TaskManager而失败，那么之后仍然必须有足够的可用插槽。YARN上的Flink支持自动重启丢失的YARN容器。&lt;/p&gt;
&lt;p&gt;如果未启用检查点，Kafka使用者将定期向Zookeeper提交偏移量。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/dev/connectors/kafka.html&quot;&gt;Streaming Connectors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://kafka.apache.org/documentation/&quot;&gt;Kafka官方文档&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 26 Jul 2019 20:46:00 +0000</pubDate>
<dc:creator>JavaEdge</dc:creator>
<og:description>1 概览 1.1 预定义的源和接收器 Flink内置了一些基本数据源和接收器，并且始终可用。该预定义的数据源包括文件，目录和插socket，并从集合和迭代器摄取数据。该预定义的数据接收器支持写入文件和</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/JavaEdge/p/11253682.html</dc:identifier>
</item>
<item>
<title>使用redis分布式锁解决并发线程资源共享问题 - 保军Baojun</title>
<link>http://www.cnblogs.com/wangbaojun/p/11251403.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/wangbaojun/p/11251403.html</guid>
<description>&lt;p&gt;众所周知, 在多线程中，因为共享全局变量,会导致资源修改结果不一致，所以需要加锁来解决这个问题，保证同一时间只有一个线程对资源进行操作&lt;/p&gt;
&lt;p&gt;但是在分布式架构中，我们的服务可能会有n个实例，但线程锁只对同一个实例有效，就需要用到&lt;em&gt;&lt;strong&gt;分布式锁----redis setnx&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt; &lt;/h3&gt;
&lt;h3&gt;&lt;strong&gt;原理：&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;　　&lt;/strong&gt;修改某个资源时, 在redis中设置一个key，value根据实际情况自行决定如何表示&lt;/p&gt;
&lt;p&gt;　　我们既然要通过检查key是否存在（存在表示有线程在修改资源，资源上锁，其他线程不可同时操作，若key不存在，表示资源未被线程占用，允许线程抢占，然后将通过setnx设置vlaue，表示资源上锁，其他线程不可同时操作）&lt;/p&gt;
&lt;p&gt;　　图示：&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1317274/201907/1317274-20190726162306207-1022330238.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;分析:&lt;/h3&gt;
&lt;p&gt;　　我们的服务处于一个集群中，如果只是简单的的使用线程锁来解决以上问题，是存在问题的：因为线程是基于进程的，两个web server处于不同的进程空间&lt;/p&gt;
&lt;p&gt;　　也就是说,user1的请求发往web server1，那只能与web server1的其他请求进行锁的操作，而不能对web server2的请求产生影响&lt;/p&gt;
&lt;p&gt;　　上面的图中，user1发往web server1的请求负责处理的线程为Thread1,同理负责处理user2发往web server2的请求的线程thread2&lt;/p&gt;
&lt;p&gt;　　在同一时刻1，两个线程都读取了mysql中residue_ticket的值为100，对应上图 (1)(2)， 各自对100进行-1操作，更新到数据库，对应(3)(4)&lt;/p&gt;
&lt;p&gt;　　我们预期的情况是residue_ticket值被减少了两次，应该为98,但是实际情况下，两个线程都做了100-1=99的操作，并都将mysql中的值改为了99, 的这就会导致最终数据不一致，所以就要用到分布式锁。&lt;/p&gt;
&lt;h4&gt;为什么用redis?&lt;/h4&gt;
&lt;p&gt;　　因为redis是单线程的，不存在多线程资源竞争，并且它真的很快&lt;/p&gt;
&lt;h4&gt;为什么用setnx 而不是set?&lt;/h4&gt;
&lt;p&gt;　　setnx表示只有在key不存在时才能设置成功，但是set会在key存在的情况下修改value&lt;/p&gt;

&lt;p&gt;利用setnx的特性，我们可以这样这样设计:&lt;/p&gt;
&lt;p&gt;　　伪代码:&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
　　&lt;span&gt;#&lt;/span&gt;&lt;span&gt; 设置redis锁的&lt;/span&gt;
　　redis key = &lt;span&gt;'&lt;/span&gt;&lt;span&gt;residue_ticket_lock&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;

　　&lt;span&gt;#&lt;/span&gt;&lt;span&gt; get_ticket是处理购票的逻辑&lt;/span&gt;
　　&lt;span&gt;def&lt;/span&gt;&lt;span&gt; get_ticket()：
　　　　time_out &lt;/span&gt;= 5    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 为了防止线程过多，当前线程获取不到锁，长时间处于循环中而导致的性能影响，我们设置一个超时时间，如果当前线程在超时时间内还没有抢占到分布式锁，就返回失败的结果&lt;/span&gt;
　　　　&lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
　　　　　　　&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; redis.setnx(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;residue_ticket_lock&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;，&lt;span&gt;'&lt;/span&gt;&lt;span&gt;lock&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,5&lt;span&gt;):
　　　　　　　　　　&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果setnx返回True, 表示此刻没有其他线程在操作数据库,当前线程可以上锁成功,注意不仅设置了value=lock,还设置了过期时间，这是必要的，为了防止上锁的线程异常崩掉导致不能释放(删除key)而导致其他所有线程永远拿不到操作权&lt;/span&gt;
　　　　　　　　　  residue_ticket = mysql.get(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;residue_ticket&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;)     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 从mysql中获取当前剩余票数&lt;/span&gt;
　　　　　　　　　  mysql.update(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;residue_ticket&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,residue_ticket-1)   &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 订购成功，将票数-1，更新数据到mysql&lt;/span&gt;
　　　　　　　　　　&lt;span&gt;#&lt;/span&gt;&lt;span&gt; 删除key，释放锁&lt;/span&gt;
　　　　　　　　　　redis.&lt;span&gt;del&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;residue_ticket&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
　　　　　　　　　  &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; True
　　　　　　　&lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
　　　　　　　　　　&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果setnx返回False，表示有其他线程对在操作,当前线程等待0.01s，并继续循环&lt;/span&gt;
　　　　　　　　　　time.sleep(0.01&lt;span&gt;)
　　　　　　　　　　time_out &lt;/span&gt;-= 0.01
　　　　　　　　　　&lt;span&gt;continue&lt;/span&gt;
　　　　&lt;span&gt;return&lt;/span&gt; False
&lt;/pre&gt;&lt;/div&gt;






</description>
<pubDate>Fri, 26 Jul 2019 16:55:00 +0000</pubDate>
<dc:creator>保军Baojun</dc:creator>
<og:description>众所周知, 在多线程中，因为共享全局变量,会导致资源修改结果不一致，所以需要加锁来解决这个问题，保证同一时间只有一个线程对资源进行操作 但是在分布式架构中，我们的服务可能会有n个实例，但线程锁只对同一</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/wangbaojun/p/11251403.html</dc:identifier>
</item>
<item>
<title>头部姿态估计 - OpenCV/Dlib/Ceres - Bemfoo</title>
<link>http://www.cnblogs.com/bemfoo/p/11253450.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/bemfoo/p/11253450.html</guid>
<description>&lt;h2 id=&quot;基本思想&quot;&gt;基本思想&lt;/h2&gt;
&lt;p&gt;通过Dlib获得当前人脸的特征点，然后通过旋转平移标准模型的特征点进行拟合，计算标准模型求得的特征点与Dlib获得的特征点之间的差，使用Ceres不断迭代优化，最终得到最佳的旋转和平移参数。&lt;/p&gt;
&lt;h2 id=&quot;使用环境&quot;&gt;使用环境&lt;/h2&gt;
&lt;p&gt;系统环境：Ubuntu 18.04&lt;br/&gt;使用语言：C++&lt;br/&gt;编译工具：CMake&lt;/p&gt;
&lt;h2 id=&quot;第三方工具&quot;&gt;第三方工具&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/davisking/dlib&quot;&gt;Dlib&lt;/a&gt;：用于获得人脸特征点&lt;br/&gt;&lt;a href=&quot;https://github.com/ceres-solver/ceres-solver&quot;&gt;Ceres&lt;/a&gt;：用于进行非线性优化&lt;br/&gt;&lt;a href=&quot;https://github.com/devernay/cminpack&quot;&gt;CMinpack&lt;/a&gt;：用于进行非线性优化 &lt;em&gt;（OPTIONAL）&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;源代码&quot;&gt;源代码&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Great-Keith/head-pose-estimation&quot; class=&quot;uri&quot;&gt;https://github.com/Great-Keith/head-pose-estimation&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;基础概念&quot;&gt;基础概念&lt;/h2&gt;
&lt;h3 id=&quot;旋转矩阵&quot;&gt;旋转矩阵&lt;/h3&gt;
&lt;p&gt;头部的任意姿态可以转化为6个参数（yaw, roll, pitch, tx, ty, tz），前三个为旋转参数，后三个为平移参数。&lt;br/&gt;平移参数好理解，原坐标加上对应的变化值即可；旋转参数需要构成旋转矩阵，三个参数分别对应了绕y轴旋转的角度、绕z轴旋转的角度和绕x轴旋转的角度。&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1368985/201907/1368985-20190726232444067-1278059563.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;具体代码实现我们可以通过Dlib已经封装好的API，&lt;code&gt;rotate_around_x/y/z(angle)&lt;/code&gt;。该函数返回的类型是&lt;code&gt;dlib::point_transform_affine3d&lt;/code&gt;，可以通过括号进行三维的变形，我们将其封装成一个rotate函数使用如下：&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;void rotate(std::vector&amp;lt;point3f&amp;gt;&amp;amp; points, const double yaw, const double pitch, const double roll) 
{
    dlib::point_transform_affine3d around_z = rotate_around_z(roll * pi / 180);
    dlib::point_transform_affine3d around_y = rotate_around_y(yaw * pi / 180);
    dlib::point_transform_affine3d around_x = rotate_around_x(pitch * pi / 180);
    for(std::vector&amp;lt;point3f&amp;gt;::iterator iter=points.begin(); iter!=points.end(); ++iter)
        *iter = around_z(around_y(around_x(*iter)));
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;[NOTE] 其中point3f是我自己定义的一个三维点坐标类型，因为Dlib中并没有提供，而使用OpenCV中的cv::Point3f会与dlib::point定义起冲突。定义如下：&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;typedef dlib::vector&amp;lt;double, 3&amp;gt; point3f;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;[NOTE] Dlib中的dlib::vector不是std::vector，注意二者区分。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;lm算法&quot;&gt;LM算法&lt;/h3&gt;
&lt;p&gt;这边不进行赘诉，建议跟着推导一遍高斯牛顿法，LM算法类似于高斯牛顿法的进阶，用于迭代优化求解非线性最小二乘问题。在该程序中使用Ceres/CMinpack封装好的API（具体使用见后文）。&lt;/p&gt;
&lt;h3 id=&quot;三维空间到二维平面的映射&quot;&gt;三维空间到二维平面的映射&lt;/h3&gt;
&lt;p&gt;根据针孔相机模型我们可以轻松的得到三维坐标到二维坐标的映射：&lt;br/&gt;&lt;span class=&quot;math inline&quot;&gt;\(X^{2d}=f_x(\frac{X^{3d}}{Z^{3d}})+c_x\)&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;math inline&quot;&gt;\(Y^{2d}=f_y(\frac{Y^{3d}}{Z^{3d}})+c_y\)&lt;/span&gt;&lt;br/&gt;&lt;em&gt;[NOTE] 使用上角标来表示3维坐标还是2维坐标，下同。&lt;/em&gt;&lt;br/&gt;其中&lt;span class=&quot;math inline&quot;&gt;\(f_x, f_y, c_x, c_y\)&lt;/span&gt;为相机的内参，我们通过&lt;a href=&quot;https://github.com/opencv/opencv/blob/master/samples/cpp/calibration.cpp&quot;&gt;OpenCV官方提供的Calibration样例&lt;/a&gt;进行获取：&lt;br/&gt;例如我的电脑所获得的结果如下：&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1368985/201907/1368985-20190726232502547-875414455.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;从图中矩阵对应关系可以获得对应的参数值。&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;#define FX 1744.327628674942
#define FY 1747.838275588676
#define CX 800
#define CY 600&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;[NOTE] 本程序不考虑外参。&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;具体步骤&quot;&gt;具体步骤&lt;/h2&gt;
&lt;h3 id=&quot;获得标准模型的特征点&quot;&gt;获得标准模型的特征点&lt;/h3&gt;
&lt;p&gt;该部分可见前一篇文章：&lt;a href=&quot;https://www.cnblogs.com/bemfoo/p/11215643.html&quot;&gt;BFM使用 - 获取平均脸模型的68个特征点坐标&lt;/a&gt;&lt;br/&gt;我们将获得的特征点保存在文件 &lt;code&gt;landmarks.txt&lt;/code&gt; 当中。&lt;/p&gt;
&lt;h3 id=&quot;使用dlib获得人脸特征点&quot;&gt;使用Dlib获得人脸特征点&lt;/h3&gt;
&lt;p&gt;该部分不进行赘诉，官方有给出了详细的样例。&lt;br/&gt;具体可以参考如下样例：&lt;/p&gt;
&lt;p&gt;其中使用官方提供的预先训练好的模型，下载地址：&lt;a href=&quot;http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2&quot; class=&quot;uri&quot;&gt;http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;具体在代码中使用如下：&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;    cv::Mat temp;
    if(!cap.read(temp))
        break;
    dlib::cv_image&amp;lt;bgr_pixel&amp;gt; img(temp);
    std::vector&amp;lt;rectangle&amp;gt; dets = detector(img);
    cout &amp;lt;&amp;lt; &quot;Number of faces detected: &quot; &amp;lt;&amp;lt; dets.size() &amp;lt;&amp;lt; endl;
    std::vector&amp;lt;full_object_detection&amp;gt; shapes;
    for (unsigned long j = 0; j &amp;lt; dets.size(); ++j) {
        /* Use dlib to get landmarks */
        full_object_detection shape = sp(img, dets[j]);
        /* ... */
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中&lt;code&gt;shape.part&lt;/code&gt;就存放着我们通过Dlib获得的当前人脸的特征点二维点序列。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;[NOTE] 在最后CMake配置的时候，需要使用&lt;code&gt;Release&lt;/code&gt;版本（最重要），以及增加选项&lt;code&gt;USE_AVX_INSTRUCTIONS&lt;/code&gt;和&lt;code&gt;USE_SSE2_INSTRUCTIONS&lt;/code&gt;/&lt;code&gt;USE_SSE4_INSTRUCTIONS&lt;/code&gt;，否则因为Dlib的检测耗时较长，使用摄像头即时拟合会有严重的卡顿。&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;使用ceres进行非线性优化&quot;&gt;使用Ceres进行非线性优化&lt;/h3&gt;
&lt;p&gt;Ceres的使用官方也提供了详细的样例，在此我们使用的是数值差分的方法，可参考：&lt;a href=&quot;https://github.com/ceres-solver/ceres-solver/blob/master/examples/helloworld_numeric_diff.cc&quot; class=&quot;uri&quot;&gt;https://github.com/ceres-solver/ceres-solver/blob/master/examples/helloworld_numeric_diff.cc&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;    Problem problem;
    CostFunction* cost_function = new NumericDiffCostFunction&amp;lt;CostFunctor, ceres::RIDDERS, LANDMARK_NUM, 6&amp;gt;(new CostFunctor(shape));
    problem.AddResidualBlock(cost_function, NULL, x);
    Solver::Options options;
    options.minimizer_progress_to_stdout = true;
    Solver::Summary summary;
    Solve(options, &amp;amp;problem, &amp;amp;summary);
    std::cout &amp;lt;&amp;lt; summary.BriefReport() &amp;lt;&amp;lt; endl;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这里我直接使用了数值差分的方法（&lt;code&gt;NumericDiffCostFunction&lt;/code&gt;），而不是使用自动差分（&lt;code&gt;AutoDiffCostFunction&lt;/code&gt;），是因为自动差分的CostFunctor是通过Template实现的，利用Template来实现Jacobian矩阵的计算使用的同一个结构，这样的话下方旋转矩阵就不能直接通过调用Dlib提供的三维坐标旋转接口，而是要将整个矩阵拆解开来实现（这边暂时没有细想到底能不能实现），因此出于简便，使用数值差分，在准确性上是会受到影响的。&lt;br/&gt;并且注意到，具体的方法使用了Ridders（&lt;code&gt;ceres::RIDDERS&lt;/code&gt;），而不是向前差分（&lt;code&gt;ceres::FORWARD&lt;/code&gt;）或者中分（&lt;code&gt;ceres::CENTRAL&lt;/code&gt;）,因为用后两者进行处理的时候，LM算法&lt;span class=&quot;math inline&quot;&gt;\(\beta_{k+1}=\beta_k-(J^TJ+\lambda I)^{-1}J^Tr)\)&lt;/span&gt;的更新项为0，无法进行迭代，暂时没有想到原因，之前这里也被卡了很久。&lt;br/&gt;&lt;em&gt;[NOTE] 源代码中还有使用了CMinpack的版本，该版本不可用的原因也是使用了封装最浅的&lt;code&gt;lmdif1_&lt;/code&gt;调用（返回结果INFO=4），该版本下使用的向前差分，如果改为使用&lt;code&gt;lmdif_&lt;/code&gt;对其中的一些参数进行调整应该是可以实现的。&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&quot;costfunctor的构建&quot;&gt;CostFunctor的构建&lt;/h4&gt;
&lt;p&gt;CostFunctor的构建是Ceres，也是这个程序，最重要的部分。首先我们需要先把想要计算的式子写出来：&lt;br/&gt;&lt;span class=&quot;math inline&quot;&gt;\(Q=\sum_i^{LANDMARK\_NUM} \|q_i^{2d}-p_i^{2d}\|^2\)&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;math inline&quot;&gt;\(Q=\sum_i^{LANDMARK\_NUM} \|q_i^{2d}-Map(R(yaw, roll, pitch)p_i^{3d}+T(t_x, t_y, t_z))\|^2\)&lt;/span&gt;。&lt;br/&gt;其中：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;LANDMARK_NUM：该程序中为68，因为Dlib算法获得的特征点数为68；；&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(q_i^{2d}\)&lt;/span&gt;：通过Dlib获得的2维特征点坐标，大小为68的vector&amp;lt;dlib::point&amp;gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(p_i^{2d}\)&lt;/span&gt;：经过一系列变换得到的标准模型的2维特征点坐标，大小为68的vector&amp;lt;dlib::point&amp;gt;，具体计算方法是通过&lt;span class=&quot;math inline&quot;&gt;\(p_i^{2d}=Map(R(yaw, roll, pitch)(p_i^{3d})+T(t_x, t_y, t_z))\)&lt;/span&gt;；&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(p_i^{3d}\)&lt;/span&gt;：标准模型的三维3维特征点坐标，大小为68的vector&amp;lt;point3f&amp;gt;；&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(R(yaw, roll, pitch)\)&lt;/span&gt;：旋转矩阵；&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(T(t_x, t_y, t_z)\)&lt;/span&gt;：平移矩阵；&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(Map()\)&lt;/span&gt;：3维点转2维点的映射，如上所描述通过相机内参获得。&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\|·\|\)&lt;/span&gt;：因为是两个2维点的差，我们使用欧几里得距离来作为2点的差。&lt;br/&gt;Ceres当中的CostFunctor只需要写入平方以内的内容，因此我们如下构建：&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;struct CostFunctor {
public:
    CostFunctor(full_object_detection _shape){ shape = _shape; }
    bool operator()(const double* const x, double* residual) const {
        /* Init landmarks to be transformed */
        fitting_landmarks.clear();
        for(std::vector&amp;lt;point3f&amp;gt;::iterator iter=model_landmarks.begin(); iter!=model_landmarks.end(); ++iter)
            fitting_landmarks.push_back(*iter);
        transform(fitting_landmarks, x);
        std::vector&amp;lt;point&amp;gt; model_landmarks_2d;
        landmarks_3d_to_2d(fitting_landmarks, model_landmarks_2d);

        /* Calculate the energe (Euclid distance from two points) */
        for(int i=0; i&amp;lt;LANDMARK_NUM; i++) {
            long tmp1 = shape.part(i).x() - model_landmarks_2d.at(i).x();
            long tmp2 = shape.part(i).y() - model_landmarks_2d.at(i).y();
            residual[i] = sqrt(tmp1 * tmp1 + tmp2 * tmp2);
        }
        return true;
    }
private:
    full_object_detection shape;    /* 3d landmarks coordinates got from dlib */
};&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中的参数x是一个长度为6的数组，对应了我们要获得的6个参数。&lt;/p&gt;
&lt;h4 id=&quot;初始值的选定&quot;&gt;初始值的选定&lt;/h4&gt;
&lt;p&gt;当前并没有多考虑这个因素，在landmark-fitting-cam程序中除了第一帧的初始值是提前设置好的以外，后续的初始值都是前一帧的最优值。&lt;br/&gt;后面的表现都很好，但这第一帧确实会存在紊乱的情况。&lt;br/&gt;因此后续优化可以考虑使用一个粗估计的初始值，因为对于这些迭代优化方法，初始值的选择决定了会不会陷入局部最优的情况。&lt;/p&gt;
&lt;h2 id=&quot;测试结果&quot;&gt;测试结果&lt;/h2&gt;
&lt;p&gt;脸部效果：&lt;br/&gt;&lt;img src=&quot;https://github.com/Great-Keith/head-pose-estimation/raw/master/assets/only_face.gif&quot; alt=&quot;only_face&quot;/&gt;&lt;br/&gt;输出工作环境：&lt;br/&gt;&lt;img src=&quot;https://github.com/Great-Keith/head-pose-estimation/raw/master/assets/work_place.gif&quot; alt=&quot;work_place&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 26 Jul 2019 15:28:00 +0000</pubDate>
<dc:creator>Bemfoo</dc:creator>
<og:description>基本思想 通过Dlib获得当前人脸的特征点，然后通过旋转平移标准模型的特征点进行拟合，计算标准模型求得的特征点与Dlib获得的特征点之间的差，使用Ceres不断迭代优化，最终得到最佳的旋转和平移参数。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/bemfoo/p/11253450.html</dc:identifier>
</item>
<item>
<title>SQL数据同步到ElasticSearch（三）- 使用Logstash+LastModifyTime同步数据 - 人生无赖</title>
<link>http://www.cnblogs.com/baiyunchen/p/11253404.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/baiyunchen/p/11253404.html</guid>
<description>&lt;p&gt;在系列开篇，我提到了四种将SQL SERVER数据同步到ES中的方案，本文将采用最简单的一种方案，即使用LastModifyTime来追踪DB中在最近一段时间发生了变更的数据。&lt;/p&gt;
&lt;h2&gt;安装Java&lt;/h2&gt;
&lt;p&gt;安装部分的官方文档在这里：&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/installing-logstash.html&quot;&gt;https://www.elastic.co/guide/en/logstash/current/installing-logstash.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以直接查看官方文档。&lt;/p&gt;
&lt;p&gt;我这里使用的还是之前文章中所述的CentOS来进行安装。&lt;/p&gt;
&lt;p&gt;首先需要安装Java（万物源于Java）&lt;/p&gt;
&lt;p&gt;输入命令找到的OpenJDK 1.8.X版本（截止我尝试时，在Java11上会有问题）：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
yum search java | grep -i --color JDK
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用Yum进行安装：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
yum install java-1.8.0-openjdk
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;配置环境变量JAVA_HOME、CLASSPATH、PATH。&lt;/p&gt;
&lt;p&gt;打开/etc/profile文件:&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot;&gt;
&lt;pre&gt;
vi /etc/profile
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;将下面几行代码粘贴到该文件的最后：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;&lt;span&gt;&lt;strong&gt;--这句要自己到/usr/lib/jvm下面找对应的目录,不能直接copy&lt;/strong&gt;&lt;/span&gt;
&lt;pre&gt;
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-0.el7_6.x86_64/ &lt;span&gt;
export CLASSPATH&lt;/span&gt;=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;保存并关闭，然后执行下列命令让设置立即生效。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot;&gt;
&lt;pre&gt;
source /etc/profile
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以输入下面的命令查看是否已生效：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;java –-version&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span&gt;echo $JAVA_HOME
echo $CLASSPATH
echo $PATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;首先注册ELK官方的GPG-KEY:&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;6.6031746031746&quot;&gt;
&lt;pre&gt;
rpm --import &lt;a href=&quot;https://artifacts.elastic.co/GPG-KEY-elasticsearch&quot;&gt;https:&lt;span&gt;//&lt;/span&gt;&lt;/a&gt;&lt;span&gt;&lt;a&gt;artifacts.elastic.co/GPG-KEY-elasticsearch&lt;/a&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后cd /etc/yum.repos.d/文件夹下，创建一个logstash.repo文件，并将下面一段内容粘贴到该文件中保存：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
[logstash-7&lt;span&gt;.x]
name&lt;/span&gt;=Elastic repository &lt;span&gt;for&lt;/span&gt; 7&lt;span&gt;.x packages
baseurl&lt;/span&gt;=https:&lt;span&gt;//&lt;/span&gt;&lt;span&gt;artifacts.elastic.co/packages/7.x/yum&lt;/span&gt;
gpgcheck=1&lt;span&gt;
gpgkey&lt;/span&gt;=https:&lt;span&gt;//&lt;/span&gt;&lt;span&gt;artifacts.elastic.co/GPG-KEY-elasticsearch&lt;/span&gt;
enabled=1&lt;span&gt;
autorefresh&lt;/span&gt;=1&lt;span&gt;
type&lt;/span&gt;=rpm-md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后执行安装命令：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
sudo yum install logstash
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以上步骤可能比较慢，还有另外一种办法，就是通过下载来安装LogStash:&lt;/p&gt;
&lt;p&gt;官方文档在这里：&lt;a href=&quot;https://www.elastic.co/cn/downloads/logstash&quot;&gt;https://www.elastic.co/cn/downloads/logstash&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;首先在上面的链接中下载LogStash的tar.gz包，这个过程有可能也很慢，我的解决方案是在自己机器上使用迅雷进行下载，完事儿Copy到Linux服务器中。&lt;/p&gt;
&lt;p&gt;下载完成后，执行解压操作：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
sudo tar -xvf logstash-7.2.0.tar.gz
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;解压完成后，进入解压后的logstash-7.2.0文件夹。&lt;/p&gt;
&lt;p&gt;接着我们安装Logstash-input-jdbc插件：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
bin/logstash-plugin &lt;span&gt;install&lt;/span&gt; logstash-input-jdbc
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;下载SQL SERVER jbdc组件，这里我们从微软官网下载：&lt;a href=&quot;https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server?view=sql-server-2017&quot;&gt;https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server?view=sql-server-2017&lt;/a&gt; ，当然这个链接只是目前的，如果你在尝试时这个链接失效了，那就自行百度搜索吧~&lt;/p&gt;
&lt;p&gt;下载完成后，解压到logstash下面的lib目录下，这里我自己为了方便，把微软默认给jdbc外面包的一层语言名称的文件夹给去掉了。&lt;/p&gt;
&lt;p&gt;接着，我们到/config文件夹，新建一个logstash.conf文件，内容大概如下：&lt;/p&gt;
&lt;p&gt;下面的每一个参数含义都可以在官方文档中找到：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span&gt;input {
    jdbc {
        jdbc_driver_library &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/usr/local/logstash-7.2.0/lib/mssql-jdbc-7.2.2/mssql-jdbc-7.2.2.jre8.jar&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 这里请灵活应变，能找到我们上一步下载的jdbc jar包即可&lt;/span&gt;
        jdbc_driver_class =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;com.microsoft.sqlserver.jdbc.SQLServerDriver&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 这个名字是固定的&lt;/span&gt;
        jdbc_connection_string =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;jdbc:sqlserver: //数据库ServerIP:1433;databaseName=数据库名;&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        jdbc_user &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;数据库账号&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        jdbc_password &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;数据库密码&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        schedule &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;* * * * *&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt; Corn 表达式，请自行百度写法&lt;/span&gt;
        jdbc_default_timezone =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Asia/Shanghai&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        jdbc_page_size &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;500&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt; 每一批传输的数量&lt;/span&gt;
        record_last_run =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;true&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;是否保存状态 &lt;/span&gt;
        use_column_value =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;true&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//设置为时true，使用定义的 tracking_column值作为:sql_last_value。设置为时false，:sql_last_value反映上次执行查询的时间。&lt;/span&gt;
        tracking_column =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;LastModificationTime&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;配合use_column_value使用&lt;/span&gt;
        last_run_metadata_path =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;/usr/opt/logstash/config/last_id&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;记录:sql_last_value的文件&lt;/span&gt;
        lowercase_column_names =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;false&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;将DB中的列名自动转换为小写&lt;/span&gt;
        tracking_column_type =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;timestamp&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;tracking_column的数据类型，只能是numberic和timestamp&lt;/span&gt;
        clean_run =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;false&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;是否应保留先前的运行状态，其实我也不知道这个字段干啥用的~~&lt;/span&gt;
        statement =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;SELECT * FROM 表 WITH(NOLOCK) WHERE LastModificationTime &amp;gt; :sql_last_value&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;从DB中抓数据的SQL脚本&lt;/span&gt;
&lt;span&gt;    }
}
output {
    elasticsearch {
        index &lt;/span&gt;=&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;test&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;  &lt;span&gt;//&lt;/span&gt;&lt;span&gt;ES集群的索引名称     &lt;/span&gt;
        document_id =&amp;gt; &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;%{Id}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;//&lt;/span&gt;&lt;span&gt;Id是表里面的主键，为了拿这个主键在ES中生成document ID&lt;/span&gt;
        hosts =&amp;gt; [&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;http://192.168.154.135:9200&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;]&lt;span&gt;//&lt;/span&gt;&lt;span&gt; ES集群的地址&lt;/span&gt;
&lt;span&gt;    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面的被注释搞的乱糟糟的，给你们一个可以复制的版本吧：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span&gt;input {
    jdbc {
        jdbc_driver_library &lt;/span&gt;=&amp;gt; &quot;/usr/local/logstash-7.2.0/lib/mssql-jdbc-7.2.2/mssql-jdbc-7.2.2.jre8.jar&quot;&lt;span&gt;
        jdbc_driver_class &lt;/span&gt;=&amp;gt; &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;&lt;span&gt;
        jdbc_connection_string &lt;/span&gt;=&amp;gt; &quot;jdbc:sqlserver://SERVER_IP:1433;databaseName=DBName;&quot;&lt;span&gt;
        jdbc_user &lt;/span&gt;=&amp;gt; &quot;xxx&quot;&lt;span&gt;
        jdbc_password &lt;/span&gt;=&amp;gt; &quot;password&quot;&lt;span&gt;
        schedule &lt;/span&gt;=&amp;gt; &quot;* * * * *&quot;&lt;span&gt;
        jdbc_default_timezone &lt;/span&gt;=&amp;gt; &quot;Asia/Shanghai&quot;&lt;span&gt;
        jdbc_page_size &lt;/span&gt;=&amp;gt; &quot;50000&quot;&lt;span&gt;
        record_last_run &lt;/span&gt;=&amp;gt; &quot;true&quot;&lt;span&gt;
        use_column_value &lt;/span&gt;=&amp;gt; &quot;true&quot;&lt;span&gt;
        tracking_column &lt;/span&gt;=&amp;gt; &quot;LastModificationTime&quot;&lt;span&gt;
        last_run_metadata_path &lt;/span&gt;=&amp;gt; &quot;/usr/local/logstash-7.2.0/config/last_id&quot;&lt;span&gt;
        lowercase_column_names &lt;/span&gt;=&amp;gt; &quot;false&quot;&lt;span&gt;
        tracking_column_type &lt;/span&gt;=&amp;gt; &quot;timestamp&quot;&lt;span&gt;
        clean_run &lt;/span&gt;=&amp;gt; &quot;false&quot;&lt;span&gt;
        statement &lt;/span&gt;=&amp;gt; &quot;SELECT * FROM xxx WITH(NOLOCK) WHERE LastModificationTime &amp;gt; :sql_last_value&quot;&lt;span&gt;
    }
}
output {
    elasticsearch {
        index &lt;/span&gt;=&amp;gt; &quot;item&quot;&lt;span&gt;       
        document_id &lt;/span&gt;=&amp;gt; &quot;%{Id}&quot;&lt;span&gt;
        hosts &lt;/span&gt;=&amp;gt; &lt;span&gt;[&lt;/span&gt;&lt;span&gt;&quot;http://ES集群IP:9200&quot;&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;
    }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;回头来说一下这个LogStash的整体思路吧，其实我的理解，LogStash就是一个数据搬运工，他的搬运数据，分为三个大的阶段：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;读取数据（input）&lt;/li&gt;
&lt;li&gt;过滤数据（filter）&lt;/li&gt;
&lt;li&gt;输出数据(output)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;对应的官方文档：&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/current/pipeline.html&quot;&gt;https://www.elastic.co/guide/en/logstash/current/pipeline.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;而这每一个阶段，都是通过一些插件来实现的，比如在上述的配置文件中，我们有：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;读取数据即input部分，这部分由于我们是需要从数据库读取数据，所以使用了一个可以执行SQL语句的jdbc-input插件，这里如果我们的数据源是其他的部分，就需要使用其他的一些插件来实现。&lt;/li&gt;
&lt;li&gt;也有输出数据部分，这部分我们是将数据写入到ElasticSearch，所以我们使用了一个elasticsearch-output插件。这里也可以将数据写入到kafka等其他的一些产品中，也是需要一些插件即可搞定。&lt;/li&gt;
&lt;li&gt;可以发现我们上面的部分没有涉及到filter插件，其实如果我们想对数据做一些过滤、规范化处理等，都可以使用filter插件来进行处理，具体的还需要进一步去探索啦~&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;剩下的部分就简单了，切换目录到logstash的目录下，执行命令：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
bin/logstash -f config/logstash.conf
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后执行的效果图大概如下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231020045-2114758556.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231026951-657084553.png&quot; alt=&quot;image&quot; width=&quot;1541&quot; height=&quot;461&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;可以使用Elasticsearch-Head等插件来查看是否同步正常：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231034587-1972537914.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231040635-1043974127.png&quot; alt=&quot;image&quot; width=&quot;912&quot; height=&quot;306&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231045903-286993847.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;https://img2018.cnblogs.com/blog/602207/201907/602207-20190726231051971-685158046.png&quot; alt=&quot;image&quot; width=&quot;1440&quot; height=&quot;932&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大概就是这样啦，后续我这边会继续尝试使用其他方式来进行数据同步，欢迎大家关注~&lt;/p&gt;
</description>
<pubDate>Fri, 26 Jul 2019 15:11:00 +0000</pubDate>
<dc:creator>人生无赖</dc:creator>
<og:description>在系列开篇，我提到了四种将SQL SERVER数据同步到ES中的方案，本文将采用最简单的一种方案，即使用LastModifyTime来追踪DB中在最近一段时间发生了变更的数据。 安装Java 安装部分</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/baiyunchen/p/11253404.html</dc:identifier>
</item>
<item>
<title> 翻译 2.2 CMake 编程 - 数据管理乐园</title>
<link>http://www.cnblogs.com/hejiang/p/11253387.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/hejiang/p/11253387.html</guid>
<description>&lt;h2 id=&quot;流程控制&quot;&gt;流程控制&lt;/h2&gt;
&lt;p&gt;CMake有一个 &lt;a href=&quot;https://cmake.org/cmake/help/latest/command/if.html&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;if&lt;/code&gt;&lt;/a&gt; 语句, 经年累月之后，现在它已经相当复杂。 您可以在 &lt;code&gt;if&lt;/code&gt; 语句中使用全大写字母书写一系列关键字，并且您通常可以直接通过名称（if语句在历史上出现早于变量扩展）或使用 &lt;code&gt;${}&lt;/code&gt; 语法来引用变量。 下面是 &lt;code&gt;if&lt;/code&gt; 语句的示例：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if(variable)
    &lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;如果您明确地进行变量扩展（例如 &lt;code&gt;${variable}&lt;/code&gt;，由于扩展的潜在扩展），这可能会有点混乱。 在CMake 3.1+ 中添加了一个策略(&lt;a href=&quot;https://cmake.org/cmake/help/latest/policy/CMP0054.html&quot; target=&quot;_blank&quot;&gt;CMP0054&lt;/a&gt;)，使得被引号引起来的变量扩展不被递归扩展。 因此，只要 CMake 的最低版本是 3.1+，您就可以：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if(&lt;span class=&quot;hljs-string&quot;&gt;&quot;${variable}&quot;)
    &lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这里还有一些可以在 &lt;code&gt;if&lt;/code&gt; 命令中使用的关键字，例如：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;一元关键字: &lt;code&gt;NOT&lt;/code&gt;, &lt;code&gt;TARGET&lt;/code&gt;, &lt;code&gt;EXISTS&lt;/code&gt; (文件), &lt;code&gt;DEFINED&lt;/code&gt;, 等等&lt;/li&gt;
&lt;li&gt;二元关键字: &lt;code&gt;STREQUAL&lt;/code&gt;, &lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, &lt;code&gt;MATCHES&lt;/code&gt; (正则表达式), &lt;code&gt;VERSION_LESS&lt;/code&gt;, &lt;code&gt;VERSION_LESS_EQUAL&lt;/code&gt;(CMake 3.7+), 等等&lt;/li&gt;
&lt;li&gt;括号可以用于分组&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://cmake.org/cmake/help/latest/manual/cmake-Generator-expressions.7.html&quot; target=&quot;_blank&quot;&gt;Generator-expressions&lt;/a&gt; 非常强大，但也有点奇怪和专业。 大多数 CMake 命令在配置时运行，包括上面看到的 &lt;code&gt;if&lt;/code&gt; 语句。 但是如果你需要在构建时甚至安装时间执行逻辑呢？ 为了这个目的 CMake 添加了生成器表达式(&lt;a href=&quot;https://cmake.org/cmake/help/latest/manual/cmake-Generator-expressions.7.html&quot; target=&quot;_blank&quot;&gt;Generator-expressions&lt;/a&gt;)。&lt;a id=&quot;reffn_1&quot; href=&quot;http://localhost:4000/chapters/basics/functions.html#fn_1&quot;&gt;1&lt;/a&gt; 它们在目标属性中进行计算和执行。&lt;/p&gt;
&lt;p&gt;最简单的生成器表达式是信息表达式，具有这样 &lt;code&gt;$&amp;lt;KEYWORD&amp;gt;&lt;/code&gt; 的形式; 他们评估(计算和执行)一条与当前配置相关的信息。 另一种形式是 &lt;code&gt;$&amp;lt;KEYWORD:value&amp;gt;&lt;/code&gt;，KEYWORD 关键字控制评估，&lt;code&gt;value&lt;/code&gt; 是要评估的项目（这里也允许使用信息表达式关键字）。 如果 &lt;code&gt;KEYWORD&lt;/code&gt; 是一个计算结果为 0 或 1 的生成器表达式或变量，&lt;code&gt;value&lt;/code&gt; 则替换为 0 或 1。 您可以嵌套生成器表达式，并且可以使用变量来使嵌套变量可读。 某些表达式允许使用逗号分隔的多个值。&lt;a id=&quot;reffn_2&quot; href=&quot;http://localhost:4000/chapters/basics/functions.html#fn_2&quot;&gt;2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;例如，如果你想要设置仅在 &lt;code&gt;DEBUG&lt;/code&gt; 配置下生效的编译标志，则可以这样写：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;target_compile_options(MyTarget PRIVATE &quot;$&amp;lt;$&amp;lt;CONFIG:Debug&amp;gt;:--my-flag&amp;gt;&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这是一种比使用专用 &lt;code&gt;*_DEBUG&lt;/code&gt; 变量更新，更好的方式，并且可以推广到生成器表达式支持的所有情况。 请注意，您永远不应该使用配置时的值作为当前配置，因为 IDE 之类的多配置生成器并没有“当前”配置，仅在构建时通过生成器表达式和自定义的 &lt;code&gt;*_&amp;lt;CONFIG&amp;gt;&lt;/code&gt; 变量起作用。&lt;/p&gt;
&lt;p&gt;生成器表达式的其他常见用法：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;将项目限制为仅限某种语言（例如 CXX），以避免它与 CUDA 等混在一起，或者根据目标语言将其包装以使其不同。&lt;/li&gt;
&lt;li&gt;访问配置相关属性，例如目标文件位置。&lt;/li&gt;
&lt;li&gt;为构建和安装目录提供不同的位置。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;最后一个很常见。几乎每个支持安装的软件包都会看到类似的内容：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt; target_include_directories(
     MyTarget
     PUBLIC
     $&amp;lt;BUILD_INTERFACE:&lt;span class=&quot;hljs-string&quot;&gt;&quot;${CMAKE_CURRENT_SOURCE_DIR}/include&quot;&amp;gt;
     $&amp;lt;INSTALL_INTERFACE:&lt;span class=&quot;hljs-keyword&quot;&gt;include&amp;gt;
     )
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;blockquote id=&quot;fn_1&quot; readability=&quot;5.7674418604651&quot;&gt;
&lt;p&gt;1. 它们就像在构建/安装时评估它们一样，但实际上它们是针对每个构建配置分别进行评估的。&lt;a title=&quot;Jump back to footnote [1] in the text.&quot; href=&quot;http://localhost:4000/chapters/basics/functions.html#reffn_1&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote id=&quot;fn_2&quot; readability=&quot;4.6323529411765&quot;&gt;
&lt;p&gt;2. CMake 文档将表达式拆分为信息，逻辑和输出。&lt;a title=&quot;Jump back to footnote [2] in the text.&quot; href=&quot;http://localhost:4000/chapters/basics/functions.html#reffn_2&quot;&gt; ↩&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;宏和函数&quot;&gt;宏和函数&lt;/h2&gt;
&lt;p&gt;你可以方便的定义你自己的 CMake 函数 &lt;a href=&quot;https://cmake.org/cmake/help/latest/command/function.html&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;function&lt;/code&gt;&lt;/a&gt; 和宏 &lt;a href=&quot;https://cmake.org/cmake/help/latest/command/macro.html&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;macro&lt;/code&gt;&lt;/a&gt; 。 函数和宏之间的唯一区别是作用范围：宏没有作用范围。 因此，如果您在函数中设置了变量并希望它在外部可见，那么您将需要用 &lt;code&gt;PARENT_SCOPE&lt;/code&gt;。 由于您必须在每个函数中明确用 &lt;code&gt;PARENT_SCOPE&lt;/code&gt; 设置您希望外部世界可见的变量，嵌套函数有点儿烦。 但是，函数不会像宏一样 “泄漏” 变量。 在以下示例中，我将使用函数。&lt;/p&gt;
&lt;p&gt;一个简单使用函数的例子如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;function(SIMPLE REQUIRED_ARG)
    &lt;span class=&quot;hljs-keyword&quot;&gt;message(STATUS &lt;span class=&quot;hljs-string&quot;&gt;&quot;Simple arguments: ${REQUIRED_ARG}, followed by ${ARGV}&quot;)
    &lt;span class=&quot;hljs-keyword&quot;&gt;set(&lt;span class=&quot;hljs-variable&quot;&gt;${REQUIRED_ARG} &lt;span class=&quot;hljs-string&quot;&gt;&quot;From SIMPLE&quot; PARENT_SCOPE)
&lt;span class=&quot;hljs-keyword&quot;&gt;endfunction()

simple(This)
&lt;span class=&quot;hljs-keyword&quot;&gt;message(&lt;span class=&quot;hljs-string&quot;&gt;&quot;Output: ${This}&quot;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;如果你想用位置参数，它们需要明确列出，并且所有其他参数都被放在 &lt;code&gt;ARGN&lt;/code&gt; 中（&lt;code&gt;ARGV&lt;/code&gt; 中保存有所有参数，甚至那些你已经列出的参数）。 您只能通过设置变量来解决 CMake 中函数没有返回值的问题。 在上面的示例中，您可以显式指定要设置的变量名称。&lt;/p&gt;
&lt;h2 id=&quot;参数&quot;&gt;参数&lt;/h2&gt;
&lt;p&gt;你已经在 CMake 函数的大部分使用中看到，CMake 有一个命名变量系统。 您可以将它与 &lt;a href=&quot;https://cmake.org/cmake/help/latest/command/cmake_parse_arguments.html&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;cmake_parse_arguments&lt;/code&gt;&lt;/a&gt; 函数一起使用。 如果要支持 3.5 以下的 CMake 版本，您还需要包含 &lt;a href=&quot;https://cmake.org/cmake/help/latest/module/CMakeParseArguments.html&quot; target=&quot;_blank&quot;&gt;CMakeParseArguments&lt;/a&gt; 模块，该模块在成为内置命令之前就已存在。&lt;/p&gt;
&lt;p&gt;以下是如何使用它的示例：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;function(COMPLEX)
cmake_parse_arguments(
    COMPLEX_PREFIX
    &lt;span class=&quot;hljs-string&quot;&gt;&quot;SINGLE;ANOTHER&quot;
    &lt;span class=&quot;hljs-string&quot;&gt;&quot;ONE_VALUE;ALSO_ONE_VALUE&quot;
    &lt;span class=&quot;hljs-string&quot;&gt;&quot;MULTI_VALUES&quot;
    &lt;span class=&quot;hljs-variable&quot;&gt;${ARGN}
)

&lt;span class=&quot;hljs-keyword&quot;&gt;endfunction()

complex(SINGLE ONE_VALUE value MULTI_VALUES some other values)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在此调用 &lt;a href=&quot;https://cmake.org/cmake/help/latest/command/cmake_parse_arguments.html&quot; target=&quot;_blank&quot;&gt;&lt;code&gt;cmake_parse_arguments&lt;/code&gt;&lt;/a&gt; 函数后，在 &lt;code&gt;complex&lt;/code&gt; 函数内部，给我们产生了这些变量：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;lang-cmake&quot;&gt;COMPLEX_PREFIX_SINGLE = &lt;span class=&quot;hljs-keyword&quot;&gt;TRUE
COMPLEX_PREFIX_ANOTHER = &lt;span class=&quot;hljs-keyword&quot;&gt;FALSE
COMPLEX_PREFIX_ONE_VALUE = &lt;span class=&quot;hljs-string&quot;&gt;&quot;value&quot;
COMPLEX_PREFIX_ALSO_ONE_VALUE = &amp;lt;UNDEFINED&amp;gt;
COMPLEX_PREFIX_MULTI_VALUES = &lt;span class=&quot;hljs-string&quot;&gt;&quot;some;other;values&quot;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;如果你查看官方页面，你会看到一个稍微不同的使用 &lt;code&gt;set&lt;/code&gt; 的方法避免在列表中明确写分号的方法; 随意使用您最喜欢的结构。 您也可以将它与上面列出的位置参数混合使用; 任何剩余的参数（包括可选的位置参数）都会保存在 &lt;code&gt;COMPLEX_PREFIX_UNPARSED_ARGUMENTS&lt;/code&gt; 中。&lt;/p&gt;
</description>
<pubDate>Fri, 26 Jul 2019 15:06:00 +0000</pubDate>
<dc:creator>数据管理乐园</dc:creator>
<og:description>&lt;&lt;Modern CMake&gt;&gt; 翻译 2.2 CMake 编程 流程控制 CMake有一个 if 语句, 经年累月之后，现在它已经相当复杂。 您</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/hejiang/p/11253387.html</dc:identifier>
</item>
<item>
<title>iOS 类知乎”分页”效果的实现? - jgCho</title>
<link>http://www.cnblogs.com/jgCho/p/11253314.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/jgCho/p/11253314.html</guid>
<description>&lt;p&gt;我们先看张gif图看一下效果(LICEcap录制的有点卡, 凑合看)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223614131-946845494.gif&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;好像还是卡, 怼个视频演示链接吧: &lt;a href=&quot;https://m.weibo.cn/1990517135/4398431764047996&quot;&gt;https://m.weibo.cn/1990517135/4398431764047996&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们先来分析一下页面结构, 然后分析具体动画实现.&lt;/p&gt;

&lt;p&gt;页面结构: 可以将当前页面和下个页面复用, 下个作为作为当前页面的chilldViewController, 大概长这个样子&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223651908-300243722.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;那么项目目录大概长这样: &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223713798-1446635689.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;div readability=&quot;20.5&quot;&gt;
&lt;div readability=&quot;36&quot;&gt;
&lt;p&gt;接下来就是给baseView添加对应的tableView,refeshHeader和footer&lt;/p&gt;
&lt;p&gt;下面来分析当前页滑动到底部, 切换下页的动画实现:&lt;/p&gt;
&lt;p&gt;动画分解: 当前view滑出屏幕 + 下页view滑入屏幕, &lt;/p&gt;
&lt;p&gt;那么如何通过view和childView完成切换呢? &lt;/p&gt;
&lt;p&gt;先来说当前view滑出屏幕,&lt;/p&gt;
&lt;p&gt;我们可以用一种”欺骗”式的把戏来完成, 既通过生成裁剪当前view生成的screenShotview添加到当前屏幕, 来完成当前view上滑的效果;&lt;/p&gt;
&lt;p&gt;接下来是childView滑入屏幕,&lt;/p&gt;
&lt;p&gt;当前view添加childView后,可以给childView添加对应的transform动画, 从底部弹出, 就完成了弹出效果动画的实现.&lt;/p&gt;

&lt;p&gt;上面动画做完后, 页面布局大概长这样:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223739931-485319109.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;下面给出动画实现关键代码:&lt;/p&gt;
&lt;p&gt;首先判断tableView滑动偏移量达到了临界值:&lt;/p&gt;

&lt;p&gt;通过tableView以下代理方法判断当前偏移量 &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223757380-823741191.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;临界值的判断如下:&lt;/p&gt;

&lt;p&gt;下滑临界值: &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223817305-424924341.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 上滑临界值:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223837850-1944291429.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;当满足对应的临界值偏移量, 我们就要进行view切换.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;view中的代码处理(下滑处理:当前view滑出屏幕, childView底部弹出; 上滑处理: 当前view滑出屏幕, 新view从顶部滑入屏幕) &lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223857767-1234925018.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;childView中的代码处理: &lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/826619/201907/826619-20190726223916247-1451979783.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;大概思路就是这样, 有需要demo的可以评论留言. &lt;/p&gt;
</description>
<pubDate>Fri, 26 Jul 2019 14:40:00 +0000</pubDate>
<dc:creator>jgCho</dc:creator>
<og:description>我们先看张gif图看一下效果(LICEcap录制的有点卡, 凑合看) 好像还是卡, 怼个视频演示链接吧: https://m.weibo.cn/1990517135/439843176404</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/jgCho/p/11253314.html</dc:identifier>
</item>
<item>
<title>JAVA-注解(2)-自定义注解及反射注解 - Mr丶L</title>
<link>http://www.cnblogs.com/xiaoluohao/p/11253307.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xiaoluohao/p/11253307.html</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;自定义注解开发&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;　　&lt;em&gt;1.开发一个注解类&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　　　开发一个注解类的过程，非常类似于开发一个接口，只不过需要通过@interface关键字来声明&lt;/p&gt;
&lt;p&gt;　　&lt;em&gt;&lt;strong&gt;2.使用元注解修饰注解的声明&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　所谓的原注解是用来修饰注解声明的注释，可以控制被修饰的注解的特性。&lt;/p&gt;
&lt;p&gt;　　　　&lt;strong&gt;@Target&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　用来声明被修饰的注解可以用在什么位置。&lt;/p&gt;
&lt;p&gt;　　　　　　可以在@Target的属性中设置Element Type类型的数组来指定可以使用的位置。&lt;/p&gt;
&lt;p&gt;　　　　　　如果不使用此原注解修饰，默认注解可以用在任意位置。&lt;/p&gt;
&lt;p&gt;　　　　&lt;strong&gt;@Retention&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　用来声明被修饰的注释会被保留到什么阶段。&lt;/p&gt;
&lt;p&gt;　　　　　　可以在该注解的属性中通过RetentionPolicy类型的值来指定注释被保留到何时。&lt;/p&gt;
&lt;p&gt;　　　　　　　　（1）&lt;em&gt;RetentionPolicy.SOURCE&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　此注解将会被保留到源码阶段，java中，在编译过程中被删除。&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　这种类型的注解通常是给编译器看的。&lt;/p&gt;
&lt;p&gt;　　　　　　　　（2）&lt;em&gt;Retention Policy.CLASS&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　此注解将会被保留到源码阶段和编译阶段，.java和.class中，在类加载的过程中被删除。&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　这种类型的注解通常是给类加载器看的。&lt;/p&gt;
&lt;p&gt;　　　　　　　　（3）&lt;em&gt;RetentionPolicy.RUNTIME&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　此注解将会被保留在源码阶段、编译阶段和运行阶段，.java  .class和内存中的字节码中都会存在。&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　这种类型的注解通常在运行阶段进行反射，控制程序运行过程。&lt;/p&gt;
&lt;p&gt;　　　　　　　　　***只有RUNTIME级别的注解才可以通过反射技术进行反射。&lt;/p&gt;
&lt;p&gt;　　　　&lt;strong&gt;@Documented&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　用来声明被修饰注解是否要被文档提取工具提取到文档中。&lt;/p&gt;
&lt;p&gt;　　　　　　　　默认不提取&lt;/p&gt;
&lt;p&gt;　　　　&lt;strong&gt;@Inherited&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　被修饰的注释是否具有继承性&lt;/p&gt;
&lt;p&gt;　　　　　　　　默认没有继承性&lt;/p&gt;
&lt;p&gt;　　&lt;em&gt;&lt;strong&gt;3.为注解增加属性&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　注解类中还可以有声明属性。&lt;/p&gt;
&lt;p&gt;　　　　为注解类声明属性的过程非常类似于接口定义方法。&lt;/p&gt;
&lt;p&gt;　　　　但要求，注解中的所有的属性必须是public的，可以显式声明，也可以不声明，不声明默认就是public的。&lt;/p&gt;
&lt;p&gt;　　　　注解中的属性只能是八种基本数据类型、String类型、Class类型、其他注解类型及以上类型的一维数组。&lt;/p&gt;
&lt;p&gt;　　　　注解中声明的属性 需要在使用注解时，为其赋值，赋值的方式就是在使用注解时，在注解后跟上一对小括号，在括号中通过  属性名=属性值   的方式指定属性的值&lt;/p&gt;
&lt;p&gt;　　　　也可以在声明注解时，在注解的属性后面通过default关键字，声明属性的默认值，这样一来，使用注解时不赋值，将会使用default默认值，当然也可以手动赋值，覆盖默认值。&lt;/p&gt;
&lt;p&gt;　　　　如果属性是一维数组类型，在传入的数组中，只有一个值，则包括数组的大括号也可以省略。&lt;/p&gt;
&lt;p&gt;　　　　如果注解的属性只有一个需要赋值，且该属性的名称叫做value，则在使用注解时，value==可以不写&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;反射注解&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;　　&lt;em&gt;&lt;strong&gt;1.反射注解的原理&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;　　　　RetentionPolicy.RUNTIME级别的注解会保留到运行中，可以通过反射技术获取，从而可以根据是否有注解 或 注解属性值的不同来控制程序按照不同方式运行。&lt;/p&gt;
&lt;p&gt;　　　　以下反射相关的类型中都提供了反射注解的方法：&lt;/p&gt;
&lt;p&gt;　　　　　　类Class&amp;lt;T&amp;gt;、类Method、类FIeld、类Constructor&amp;lt;T&amp;gt;、类Package&lt;/p&gt;
&lt;p&gt;　　　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1236612/201907/1236612-20190726223150101-2068840112.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;　　&lt;em&gt;&lt;strong&gt;2.反射注解案例&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;package&lt;/span&gt;&lt;span&gt; cn.tedu.test;
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; 
&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.lang.annotation.ElementType;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.lang.annotation.Retention;
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.lang.annotation.RetentionPolicy;
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.lang.annotation.Target;
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; 
&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;@Target(ElementType.TYPE)
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;@Retention(RetentionPolicy.RUNTIME)
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; @&lt;span&gt;interface&lt;/span&gt;&lt;span&gt; level{
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;    String value();
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;}
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; @level(&quot;刑警&quot;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; Police{
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;}
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; 
&lt;span&gt;18&lt;/span&gt; &lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; AnnoTest02 {
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;     &lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; main(String[] args) {
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         System.out.println(&quot;敬了个礼，您好，您超速了，罚款200元。。。&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt;(Police.&lt;span&gt;class&lt;/span&gt;.isAnnotationPresent(level.&lt;span&gt;class&lt;/span&gt;&lt;span&gt;)){
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;             level anno = Police.&lt;span&gt;class&lt;/span&gt;.getAnnotation(level.&lt;span&gt;class&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt;(&quot;协警&quot;&lt;span&gt;.equals(anno.value())){
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;                 System.out.println(&quot;哥们少罚点，50块得了~~&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;             }&lt;span&gt;else&lt;/span&gt; &lt;span&gt;if&lt;/span&gt;(&quot;交警&quot;&lt;span&gt;.equals(anno.value())){
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;                 System.out.println(&quot;哥们抽根烟，这是200块，收好我走人~~&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt;             }&lt;span&gt;else&lt;/span&gt; &lt;span&gt;if&lt;/span&gt;(&quot;刑警&quot;&lt;span&gt;.equals(anno.value())){
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt;                 System.out.println(&quot;赶紧交钱走人，别查出 别的事。。。&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;             }&lt;span&gt;else&lt;/span&gt;&lt;span&gt;{
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;                 System.out.println(&quot;xxx&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;         }&lt;span&gt;else&lt;/span&gt;&lt;span&gt;{
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             System.out.println(&quot;打一顿，扭送警察局。。&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; }
&lt;/pre&gt;&lt;/div&gt;

</description>
<pubDate>Fri, 26 Jul 2019 14:38:00 +0000</pubDate>
<dc:creator>Mr丶L</dc:creator>
<og:description>自定义注解开发 1.开发一个注解类 开发一个注解类的过程，非常类似于开发一个接口，只不过需要通过@interface关键字来声明 2.使用元注解修饰注解的声明 所谓的原注解是用来修饰注解声明的注释，可</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/xiaoluohao/p/11253307.html</dc:identifier>
</item>
<item>
<title>Spring Cloud 之 Config与动态路由. - JMCui</title>
<link>http://www.cnblogs.com/jmcui/p/11252170.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/jmcui/p/11252170.html</guid>
<description>&lt;h2 id=&quot;一简介&quot;&gt;一、简介&lt;/h2&gt;
&lt;p&gt; Spring Cloud Confg 是用来为分布式系统中的基础设施和微服务应用提供集中化的外部配置支持，它分为服务端与客户端两个部分。其中服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置仓库并为客户端提供获取配置信息、加密/解密信息等访问接口；而客户端则是微服务架构中的各个微服务应用或基础设施，它们通过指定的配置中心来管理应用资源与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息。&lt;/p&gt;
&lt;h2 id=&quot;二spring-config-server&quot;&gt;二、Spring Config Server&lt;/h2&gt;
&lt;p&gt;搭建一个 Config Server，首先需要一个仓库，作为分布式配置中心的存储。这里我们选择了 Github 作为我们的仓库：&lt;a href=&quot;https://github.com/JMCuixy/cloud-config-server/tree/master/config-repo&quot;&gt;&lt;span&gt;https://github.com/JMCuixy/cloud-config-server/tree/master/config-repo&lt;/span&gt;&lt;/a&gt;&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1153954/201907/1153954-20190726182916673-1921807946.png&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;pom.xml&quot;&gt;1. pom.xml&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-cloud-config-server&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;!--启动 security 保护，不需要可不添加--&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-security&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-client&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;application.yml&quot;&gt;2. application.yml&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;server:
  port: 7001

spring:
  application:
    name: cloud-config-server

  # 配置完成后可访问的 url 如下，比如：http://localhost:7001/env/default
  # /{application}/{profile} [/{label}]
  # /{application}-{profile}.yml
  # /{label}/{application}-{profile}.yml
  # /{application}-{profile}.properties
  # /{label}/{application}-{profile}.properties
  cloud:
    config:
      # 为配置中心提供安全保护
      username: user
      password: password
      server:
        git:
          # 仓库地址
          uri: https://github.com/JMCuixy/cloud-config-server.git
          # 搜索路径
          search-paths: config-repo
        # 访问 http://localhost:7001/actuator/health 可以获取配置中心健康指标
        health:
          repositories:
            env:
              name: env
              profiles: default
              label: master
            env-dev:
              name: env-dev
              profiles: dev
              label: master
            env-test:
              name: env-test
              profiles: test
              label: master
            env-prod:
              name: env-prod
              profiles: prod
              label: master

  # 提供 security 保护
  security:
    user:
      name: user
      password: password

management:
  endpoint:
    health:
      enabled: true
      show-details: always

eureka:
  client:
    service-url:
      defaultZone: http://user:password@localhost:1111/eureka/&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这里我没有配置 Github 的 username 和 password，用的是 SSH key 的方式。&lt;/p&gt;
&lt;h3 id=&quot;configapplication.java&quot;&gt;3. ConfigApplication.java&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;// 开启 Spring Cloud Config 的 Server 功能
@EnableConfigServer
@EnableDiscoveryClient
@SpringBootApplication
public class ConfigApplication {

    public static void main(String[] args) {
        SpringApplication.run(ConfigApplication.class, args);
    }

}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;至此，一个 Spring Cloud Config Server 就搭建完成了。上面的配置中，我们将 Config Server 注册到 Eureka Server 中，当作整个系统服务的一部分，所以Config Client 只要利用 Eureka 的服务发现维持与 Config Server 通信就可以了。&lt;/p&gt;
&lt;p&gt;在Config Server 的文件系统中，每次客户端请求获取配置信息时，Confg Server 从 Git 仓库中获取最新配置到本地，然后在本地 Git 仓库中读取并返回。当远程仓库无法获取时，直接将本地内容返回。&lt;/p&gt;
&lt;h2 id=&quot;二spring-config-client&quot;&gt;二、Spring Config Client&lt;/h2&gt;
&lt;p&gt;Spring Cloud Confg 的客户端在启动的时候，默认会从工程的 classpath 中加载配置信息并启动应用。只有当我们配置 spring.cloud.config.uri（或者spring.cloud.config.discovery） 的时候，客户端应用才会尝试连接 Spring Cloud Confg 的服务端来获取远程配置信息并初始化 Spring 环境配置。同时，我们必须将该参数配置在bootstrap.yml、环境变量或是其他优先级高于应用 Jar 包内的配置信息中，才能正确加载到远程配置。&lt;/p&gt;
&lt;h3 id=&quot;pom.xml-1&quot;&gt;1. pom.xml&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;    &amp;lt;dependencies&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-cloud-config-client&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.cloud&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-cloud-starter-netflix-eureka-client&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;!-- 当连接 config-server 失败的时候，可增加重试--&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-aop&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.retry&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-retry&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;

        &amp;lt;!--配置动态刷新--&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-actuator&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
    &amp;lt;/dependencies&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;bootstrap.yml-和-application.yml&quot;&gt;2. bootstrap.yml 和 application.yml&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;bootstrap.yml&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;spring:
  application:
    # 对应配置文件规则中的 {application} 部分
    name: env
  cloud:
    config:
      name: env
      # uri: http://localhost:7001
      discovery:
        enabled: true
        service-id: cloud-config-server
      # 环境变量  
      profile: default
      # 分支
      label: master
      # config Server 配置的安全信息
      username: user
      password: password
      # 快速失败响应（当发现 config-server 连接失败时，就不做连接的准备工作，直接返回失败）
      fail-fast: true
      # 失败重试
      retry:
        # 初始重试间隔时间，毫秒
        initial-interval: 1000
        # 下一间隔的乘数
        multiplier: 1.1
        # 最大间隔时间
        max-interval: 2000
        # 最多重试次数
        max-attempts: 6&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;bootstrap 配置会系统会优先加载，加载优先级比 application 高。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;application.yml&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;server:
  port: 7002

spring:
  application:
    name: cloud-config-client

eureka:
  client:
    service-url:
      defaultZone: http://user:password@localhost:1111/eureka/

management:
  endpoints:
    web:
      exposure:
        # 开启指定端点
        # 配置刷新地址：POST http://127.0.0.1:7002/actuator/refresh
        include: 'refresh'&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;configclientapplication.java&quot;&gt;3. ConfigClientApplication.java&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;@EnableDiscoveryClient
@SpringBootApplication
public class ConfigClientApplication {

    public static void main(String[] args) {
        SpringApplication.run(ConfigClientApplication.class, args);
    }

}&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;应用&quot;&gt;4. 应用&lt;/h3&gt;
&lt;p&gt;接下来瞅瞅客户端要怎么读到服务器的配置项呢？&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;@RefreshScope
@RestController
public class ConfigClientAdmin {

    @Value(&quot;${from:default}&quot;)
    private String from;

    @Autowired
    private Environment environment;


    @RequestMapping(&quot;/from&quot;)
    public String from() {
        String fromEnv = environment.getProperty(&quot;from&quot;);
        return from + &quot;_&quot; + fromEnv;
    }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;如上，我们可以使用 @Value 注解注入配置信息，或者使用 Environment Bean 来获取配置项。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;需要注意的是&lt;/span&gt;，当服务端的配置项更新的时候，客户端并不会同步获得更新，需要 Post 方法执行 &quot;/actuator/refresh&quot; 来刷新配置项。&lt;/p&gt;
&lt;p&gt;@RefreshScope 注解使配置的内容动态化，当使用 http://127.0.0.1:7002/actuator/refresh 刷新配置的时候，会刷新带有 @RefreshScope 的 Bean。&lt;/p&gt;
&lt;h2 id=&quot;三动态路由&quot;&gt;三、动态路由&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/jmcui/p/11219820.html&quot;&gt;&lt;span&gt;上一篇文章&lt;/span&gt;&lt;/a&gt; 我们尝试用 Spring Cloud Zuul 搭建了网关服务，但是我们发现路由信息都配置在 application.yml 中，这对网关的高可用是个不小的打击，因为网关作为系统流量的路口，总不能因为改个路由信息天天重启网关吧？所以动态路由的实现，就变得迫不及待了，好在我们现在有了 Spring Cloud Config。&lt;/p&gt;
&lt;p&gt;首先，我们将 Spring Cloud Zuul 的路由信息，配置在 Config Server 的 env.yml 中：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;zuul:
  routes:
    client-1:
      # ?:匹配任意单个数量字符；*：匹配任意多个数量字符；**：匹配任意多个数量字符，支持多级目录
      # 使用 url 的配置没有线程隔离和断路器的自我保护功能，不推荐使用
      path: /client-1/**
      url: http://localhost:2222/
      # 敏感头信息设置为空，表示不过滤敏感头信息，允许敏感头信息渗透到下游服务器
      sensitiveHeaders: &quot;&quot;
      customSensitiveHeaders: true
    client-2:
      path: /client-2/**
      serviceId: cloud-eureka-client
    # zuul.routes.&amp;lt;serviceid&amp;gt; = &amp;lt;path&amp;gt;
    cloud-eureka-client: /client-3/**
    client-4:
      path: /client-4/**
      # 请求转发 —— 仅限转发到本地接口
      url: forward:/local

  # Zuul 将对所有的服务都不自动创建路由规则
  ignored-services: &quot;*&quot;
  # 对某些 url 设置不经过路由选择
  ignored-patterns: {&quot;/**/world/**&quot;,&quot;/**/zuul/**&quot;}
  # Spring Cloud Zuul在请求路由时，会过滤掉 HTTP 请求头（Cookie、Set-Cookie、Authorization）信息中的一些敏感信息，
  sensitive-headers: {&quot;Cookie&quot;, &quot;Set-Cookie&quot;, &quot;Authorization&quot;}
  # 网关在进行路由转发时为请求设置 Host 头信息（保持在路由转发过程中 host 头信息不变）
  add-host-header: true
  # 请求转发时加上 X-Forwarded-*头域
  add-proxy-headers: true
  # 是否开启重试，默认关闭
  retryable: true
  # 通过 /zuul 路径访问的请求会绕过 dispatcherServlet, 被 Zuu1Servlet 处理，主要用来应对处理大文件上传的情况。
  servlet-path: /zuul
  # 禁用某个过滤器 zuul.&amp;lt;SimpleClassName&amp;gt;.&amp;lt;filterTye&amp;gt;.disable=true
  TokenFilter:
    pre:
      disable: true&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;然后，我们将网关服务注册为 Config Client（配置项与上面类似，就不赘述了），从 Config Server 获取路由信息：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;@EnableZuulProxy
@EnableDiscoveryClient
@SpringBootApplication
public class DynamicRouteApplication {

    public static void main(String[] args) {
        SpringApplication.run(DynamicRouteApplication.class, args);
    }

    /**
     * 刷新地址：POST http://127.0.0.1:5006/actuator/refresh
     * 路由查看地址：GET http://127.0.0.1:5006/actuator/routes
     *
     * @return
     */
    @Bean
    @Primary
    //该注解来使 zuul 的配置内容动态化
    @RefreshScope
    @ConfigurationProperties(prefix = &quot;zuul&quot;)
    public ZuulProperties zuulProperties() {
        return new ZuulProperties();
    }

}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这样，就把我们的路由信息交给 Config Server 去管理了~~&lt;/p&gt;
&lt;h4 id=&quot;演示源代码-httpsgithub.comjmcuixyspring-cloud-demo&quot;&gt;演示源代码 ：&lt;a href=&quot;https://github.com/JMCuixy/spring-cloud-demo&quot;&gt;&lt;span&gt;https://github.com/JMCuixy/spring-cloud-demo&lt;/span&gt;&lt;/a&gt;&lt;/h4&gt;
&lt;h4 id=&quot;内容参考spring-cloud-微服务实战&quot;&gt;内容参考：《Spring Cloud 微服务实战》&lt;/h4&gt;
</description>
<pubDate>Fri, 26 Jul 2019 14:38:00 +0000</pubDate>
<dc:creator>JMCui</dc:creator>
<og:description>一、简介  Spring Cloud Confg 是用来为分布式系统中的基础设施和微服务应用提供集中化的外部配置支持，它分为服务端与客户端两个部分。其中服务端也称为分布式配置中心，它是一个独</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/jmcui/p/11252170.html</dc:identifier>
</item>
</channel>
</rss>