<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>数据库智能运维探索与实践 - 美团技术团队</title>
<link>http://www.cnblogs.com/meituantech/p/10119875.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/meituantech/p/10119875.html</guid>
<description>&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;从自动化到智能化运维过渡时，美团DBA团队进行了哪些思考、探索与实践？本文根据赵应钢在“第九届中国数据库技术大会”上的演讲内容整理而成，部分内容有更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;-&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;近些年，传统的数据库运维方式已经越来越难于满足业务方对数据库的稳定性、可用性、灵活性的要求。随着数据库规模急速扩大，各种NewSQL系统上线使用，运维逐渐跟不上业务发展，各种矛盾暴露的更加明显。在业务的驱动下，美团点评DBA团队经历了从“人肉”运维到工具化、产品化、自助化、自动化的转型之旅，也开始了智能运维在数据库领域的思考和实践。&lt;/p&gt;
&lt;p&gt;本文将介绍美团点评整个数据库平台的演进历史，以及我们当前的情况和面临的一些挑战，最后分享一下我们从自动化到智能化运维过渡时，所进行的思考、探索与实践。&lt;/p&gt;
&lt;h2 id=&quot;-&quot;&gt;数据库平台的演变&lt;/h2&gt;
&lt;p&gt;我们数据库平台的演进大概经历了五个大的阶段：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/01.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;第一个是脚本化阶段，这个阶段，我们人少，集群少，服务流量也比较小，脚本化的模式足以支撑整个服务。&lt;/p&gt;
&lt;p&gt;第二个是工具化阶段，我们把一些脚本包装成工具，围绕CMDB管理资产和服务，并完善了监控系统。这时，我们的工具箱也逐渐丰富起来，包括DDL变更工具、SQL Review工具、慢查询采集分析工具和备份闪回工具等等。&lt;/p&gt;
&lt;p&gt;第三个是产品化阶段，工具化阶段可能还是单个的工具，但是在完成一些复杂操作时，就需要把这些工具组装起来形成一个产品。当然，并不是说这个产品一定要做成Web系统的形式，而是工具组装起来形成一套流程之后，就可以保证所有DBA的操作行为，对流程的理解以及对线上的影响都是一致的。我们会在易用性和安全性层面不断进行打磨。而工具产品化的主要受益者是DBA，其定位是提升运维服务的效率，减少事故的发生，并方便进行快速统一的迭代。&lt;/p&gt;
&lt;p&gt;第四个是打造私有云平台阶段，随着美团点评业务的高速发展，仅靠十几、二十个DBA越来越难以满足业务发展的需要。所以我们就把某些日常操作开放授权，让开发人员自助去做，将DBA从繁琐的操作中解放出来。当时整个平台每天执行300多次改表操作；自助查询超过1万次；自助申请账号、授权并调整监控；自助定义敏感数据并授权给业务方管理员自助审批和管理；自定义业务的高峰和低峰时间段等等；自助下载、查询日志等等。&lt;/p&gt;
&lt;p&gt;第五个是自动化阶段，对这个阶段的理解，其实是“仁者见仁，智者见智”。大多数人理解的自动化，只是通过Web平台来执行某些操作，但我们认为这只是半自动化，所谓的自动化应该是完全不需要人参与。目前，我们很多操作都还处于半自动化阶段，下一个阶段我们需要从半自动过渡到全自动。以MySQL系统为例，从运维角度看包括主从的高可用、服务过载的自我保护、容量自动诊断与评估以及集群的自动扩缩容等等。&lt;/p&gt;
&lt;h2 id=&quot;-&quot;&gt;现状和面临的挑战&lt;/h2&gt;
&lt;p&gt;下图是我们平台的现状，以关系数据库RDS平台为例，其中集成了很多管理的功能，例如主从的高可用、MGW的管理、DNS的变更、备份系统、升级流程、流量分配和切换系统、账号管理、数据归档、服务与资产的流转系统等等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/02.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而且我们按照逻辑对平台设计进行了划分，例如以用户维度划分的RDS自助平台，DBA管理平台和测试环境管理平台；以功能维度划分的运维、运营和监控；以存储类型为维度划分的关系型数据库MySQL、分布式KV缓存、分布式KV存储，以及正在建设中的NewSQL数据库平台等等。未来，我们希望打造成“MySQL+NoSQL+NewSQL，存储+缓存的一站式服务平台”。&lt;/p&gt;
&lt;h3 id=&quot;-rootcause-&quot;&gt;挑战一：RootCause定位难&lt;/h3&gt;
&lt;p&gt;即便我们打造了一个很强大的平台，但还是发现有很多问题难以搞定。第一个就是故障定位，如果是简单的故障，我们有类似天网、雷达这样的系统去发现和定位。但是如果故障发生在数据库内部，那就需要专业的数据库知识，去定位和查明到底是什么原因导致了故障。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/03.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;通常来讲，故障的轨迹是一个链，但也可能是一个“多米诺骨牌”的连环。可能因为一些原因导致SQL执行变慢，引起连接数的增长，进而导致业务超时，而业务超时又会引发业务不断重试，结果会产生更多的问题。当我们收到一个报警时，可能已经过了30秒甚至更长时间，DBA再去查看时，已经错过了最佳的事故处理时机。所以，我们要在故障发生之后，制定一些应对策略，例如快速切换主库、自动屏蔽下线问题从库等等。除此之外，还有一个比较难的问题，就是如何避免相似的故障再次出现。&lt;/p&gt;
&lt;h3 id=&quot;-&quot;&gt;挑战二：人力和发展困境&lt;/h3&gt;
&lt;p&gt;第二个挑战是人力和发展的困境，当服务流量成倍增长时，其成本并不是以相同的速度对应增长的。当业务逻辑越来越复杂时，每增加一块钱的营收，其后面对应的数据库QPS可能是2倍甚至5倍，业务逻辑越复杂，服务支撑的难度越大。另外，传统的关系型数据库在容量、延时、响应时间以及数据量等方面很容易达到瓶颈，这就需要我们不断拆分集群，同时开发诉求也多种多样，当我们尝试使用平台化的思想去解决问题时，还要充分思考如何满足研发人员多样化的需求。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/04.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;人力困境这一问题，从DBA的角度来说，时间被严重的碎片化，自身的成长就会遇到瓶颈，比如经常会做一些枯燥的重复操作；另外，业务咨询量暴增，尽管我们已经在尝试平台化的方法，但是还是跟不上业务发展的速度。还有一个就是专业的DBA越来越匮乏，越来越贵，关键是根本招聘不到人手。&lt;/p&gt;
&lt;p&gt;在这种背景下，我们必须去思考：如何突破困局？如何朝着智能化转型？传统运维苦在哪里？智能化运维又能解决哪些问题？&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/05.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;首先从故障产生的原因来说，传统运维是故障触发，而智能运维是隐患驱动。换句话来说，智能运维不用报警，通过看报表就能知道可能要出事了，能够把故障消灭在“萌芽”阶段；第二，传统运维是被动接受，而智能运维是主动出击。但主动出击不一定是通过DBA去做，可能是系统或者机器人操作；第三，传统运维是由DBA发起和解决的，而智能运维是系统发起、RD自助；第四，传统运维属于“人肉救火”，而智能运维属于“智能决策执行”；最后一点，传统运维需要DBA亲临事故现场，而智能运维DBA只需要“隐身幕后”。&lt;/p&gt;
&lt;h2 id=&quot;-&quot;&gt;从自动化到智能化&lt;/h2&gt;
&lt;p&gt;那么，如何从半自动化过渡到自动化，进而发展到智能化运维呢？在这个过程中，我们会面临哪些痛点呢?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/06.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;我们的目标是为整个公司的业务系统提供高效、稳定、快速的存储服务，这也是DBA存在的价值。业务并不关心后面是MySQL还是NoSQL，只关心数据是否没丢，服务是否可用，出了问题之后多长时间能够恢复等等。所以我们尽可能做到把这些东西对开发人员透明化，提供稳定高效快速的服务。而站在公司的角度，就是在有限的资源下，提升效率，降低成本，尽可能长远地解决问题。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/07.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;上图是传统运维和智能运维的特点分析，左边属于传统运维，右边属于智能运维。传统运维在采集这一块做的不够，所以它没有太多的数据可供参考，其分析和预警能力是比较弱的。而智能运维刚好是反过来，重采集，很多功夫都在平时做了，包括分析、预警和执行，智能分析并推送关键报表。&lt;/p&gt;
&lt;p&gt;而我们的目标，是让智能运维中的“报警+分析+执行”的比重占据的越来越少。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/08.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;决策执行如何去做呢？我们都知道，预警重要但不紧急，但报警是紧急且重要的，如果你不能够及时去处理的话，事态可能会扩大，甚至会给公司带来直接的经济损失。&lt;/p&gt;
&lt;p&gt;预警通常代表我们已经定位了一个问题，它的决策思路是非常清晰的，可以使用基于规则或AI的方式去解决，相对难度更小一些。而报警依赖于现场的链路分析，变量多、路径长，所以决策难，间接导致任何决策的风险可能都变大。所以说我们的策略就是全面的采集数据，然后增多预警，率先实现预警发现和处理的智能化。就像我们既有步枪，也有手枪和刺刀，能远距离解决敌人的，就尽量不要短兵相接、肉搏上阵。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/09.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;数据采集，从数据库角度来说，我们产生的数据分成四块，Global Status、Variable，Processlist、InnoDB Status，Slow、Error、General Log和Binlog；从应用侧来说，包含端到端成功率、响应时间95线、99线、错误日志和吞吐量；从系统层面，支持秒级采样、操作系统各项指标；从变更侧来看，包含集群拓扑调整、在线DDL、DML变更、DB平台操作日志和应用端发布记录等等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/10.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;数据分析，首先是围绕集群分析，接着是实例、库，最后是表，其中每个对象都可以在多项指标上同比和环比，具体对比项可参考上图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/11.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;通过上面的步骤，我们基本可以获得数据库的画像，并且帮助我们从整体上做资源规划和服务治理。例如，有些集群实例数特别多且有继续增加的趋势，那么服务器需要scale up；读增加迅猛，读写比变大，那么应考虑存储KV化；利用率和分布情况会影响到服务器采购和预算制定；哪几类报警最多，就专项治理，各个击破。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/12.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;从局部来说，我们根据分析到的一些数据，可以做一个集群的健康体检，例如数据库的某些指标是否超标、如何做调整等等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/13.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;数据库预警，通过分析去发现隐患，把报警转化为预警。上图是我们实际情况下的报警统计分析结果，其中主从延迟占比最大。假设load.1minPerCPU比较高，我们怎么去解决？那么，可能需要采购CPU单核性能更高的机器，而不是采用更多的核心。再比如说磁盘空间，当我们发现3T的磁盘空间普遍不够时，我们下次可以采购6T或更大空间的磁盘。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/14.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;针对空间预警问题，什么时候需要拆分集群？MySQL数据库里，拆分或迁移数据库，花费的时间可能会很久。所以需要评估当前集群，按目前的增长速度还能支撑多长时间，进而反推何时要开始拆分、扩容等操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/15.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;针对慢查询的预警问题，我们会统计红黑榜，上图是统计数据，也有利用率和出轨率的数据。假设这是一个金融事业群的数据库，假设有业务需要访问且是直连，那么这时就会产生几个问题：第一个，有没有数据所有者的授权；第二个，如果不通过服务化方式或者接口，发生故障时，它可能会导致整个金融的数据库挂，如何进行降级？所以，我们会去统计出轨率跟慢查询，如果某数据库正被以一种非法的方式访问，那么我们就会扫描出来，再去进行服务治理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/16.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;从运维的层面来说，我们做了故障快速转移，包括自动生成配置文件，自动判断是否启用监控，切换后自动重写配置，以及从库可自动恢复上线等等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/17.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;报警自动处理，目前来说大部分的处理工作还是基于规则，在大背景下拟定规则，触发之后，按照满足的前提条件触发动作，随着库的规则定义的逐渐完善和丰富，可以逐步解决很多简单的问题，这部分就不再需要人的参与。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tech.meituan.com/img/Intelligent_Operation_Practice_in_meituan/18.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;-&quot;&gt;展望&lt;/h2&gt;
&lt;p&gt;未来我们还会做一个故障诊断平台，类似于“扁鹊”，实现日志的采集、入库和分析，同时提供接口，供全链路的故障定位和分析、服务化治理。&lt;/p&gt;
&lt;p&gt;展望智能运维，应该是在自动化和智能化上交叠演进，在ABC（AI、Big Data、Cloud Computing）三个方向上深入融合。在数据库领域，NoSQL和SQL界限正变得模糊，软硬结合、存储计算分离架构也被越来越多的应用，智能运维正当其时，我们也面临更多新的挑战。我们的目标是，希望通过DB平台的不断建设加固，平台能自己发现问题，自动定位问题，并智能的解决问题。&lt;/p&gt;
&lt;h2 id=&quot;-&quot;&gt;作者简介&lt;/h2&gt;
&lt;p&gt;应钢，美团点评研究员，数据库专家。曾就职于百度、新浪、去哪儿网等，10年数据库自动化运维开发、数据库性能优化、大规模数据库集群技术保障和架构优化经验。精通主流的SQL与NoSQL系统，现专注于公司业务在NewSQL领域的创新和落地。&lt;/p&gt;
&lt;div class=&quot;hidden-mobile&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;strong&gt;发现文章有错误、对内容有疑问，都可以关注美团技术团队微信公众号（meituantech），在后台给我们留言。我们每周会挑选出一位热心小伙伴，送上一份精美的小礼品。快来扫码关注我们吧！&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;article__footer__img&quot;&gt;&lt;img src=&quot;https://tech.meituan.com/img/meituanjishutuandui.png&quot; alt=&quot;公众号二维码&quot; width=&quot;455&quot; align=&quot;center&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 14 Dec 2018 07:56:00 +0000</pubDate>
<dc:creator>美团技术团队</dc:creator>
<og:description>从自动化到智能化运维过渡时，美团DBA团队进行了哪些思考、探索与实践？本文根据赵应钢在“第九届中国数据库技术大会”上的演讲内容整理而成，部分内容有更新。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/meituantech/p/10119875.html</dc:identifier>
</item>
<item>
<title>Flask script 内的Shell 类 使用 - 暮良文王</title>
<link>http://www.cnblogs.com/liangmingshen/p/10119882.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/liangmingshen/p/10119882.html</guid>
<description>&lt;h2&gt;&lt;span&gt;1.集成Python shell&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;每次自动shell会话都要导入数据库实例和模型，很烦人。&lt;/span&gt;&lt;br/&gt;&lt;span&gt;为了避免一直重复导入，&lt;/span&gt;&lt;br/&gt;&lt;span&gt;我们可以做些配置让Flask-Script的Shell命令&lt;span&gt;自动导入特定的对象&lt;/span&gt;。&lt;/span&gt;&lt;br/&gt;&lt;span&gt;若想把对象添加到导入列表中，我们要&lt;span&gt;为shell命令注册一个make_context回调函数&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;例子：&lt;/span&gt;&lt;br/&gt;&lt;span&gt;hello.py: 为shell命令添加一个上下文&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;39&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt;from&lt;/span&gt; flask_script &lt;span&gt;import&lt;/span&gt;&lt;span&gt; Shell&lt;br/&gt;&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; make_ shell context():
　　&lt;/span&gt;&lt;span&gt;return&lt;/span&gt; dict(app=app, db=db, User=User, Role=&lt;span&gt;Role)&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span&gt;manager.add_command('shell', Shell(make_context=make_shell_context))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;span&gt;可能错误提示：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;TypeError: &amp;lt;flask_script.commands.Shell object at 0x000000000478E908&amp;gt;: 'dict' ob&lt;/span&gt;&lt;span&gt;ject is not callable&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;是因为&lt;span&gt;多写了括号&lt;/span&gt;，让定义的 make_ shell context 函数直接执行了&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;span&gt;manager.add_command('shell', Shell(make_context=make_shell_context&lt;span&gt;()&lt;/span&gt;))&lt;/span&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;span&gt;继续：&lt;/span&gt;&lt;br/&gt;&lt;span&gt;make_shell _context() 函数注册了程序、数据库实例以及模型，因此这些对象能直接导入shell:&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt;$ python hello.py shell
&lt;/span&gt;&amp;gt;&amp;gt;&amp;gt;&lt;span&gt; app
&lt;/span&gt;&amp;lt;Flask &lt;span&gt;'&lt;/span&gt;&lt;span&gt;app&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt;&lt;span&gt; db
&lt;/span&gt;&amp;lt;SQLAlchemy engine=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;sqlite:////home/flask/flasky/data.sqlite&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&amp;gt;
&amp;gt;&amp;gt;&amp;gt;&lt;span&gt; User
&lt;/span&gt;&amp;lt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;app,User&lt;/span&gt;&lt;span&gt;'&amp;gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;2.补充&lt;/h2&gt;
&lt;h3&gt;（1）shell作用&lt;/h3&gt;
&lt;p&gt;&lt;span&gt; shell 是个好东西，在平时需要手动做一些应用的操作的时候，Flask 的 Shell 简直是神助攻，尤其是当需要查找一个 Model 的数据的时候更爽了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Flask-Script 的 Shell 其实就是一个加载了 Flask 应用上下文的&lt;strong&gt;交互式环境&lt;/strong&gt;，通过 shell， 我们可以像启动应用一样操作动态数据。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;简单说：方便自己调试！&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;（2）&lt;span&gt;&lt;span&gt;&lt;code&gt;make_context&lt;/code&gt;&lt;/span&gt;作用：&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;在启动的 shell 中添加默认的变量，例如上面添加了 db、User 这些，也就是说在启动 shell 之后就可以直接像访问默认函数/变量一样直接用，不用自己导入对象。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;比如可以直接查询数据库：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
&lt;span&gt;db.User.query.all()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;如果不加这个 &lt;code&gt;make_context&lt;/code&gt; 参数的话，还得麻烦的自己导入：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:python;gutter:true;&quot;&gt;
&lt;span&gt;from application.app import db
db.User.query.all()
&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;简单说：不用自己导入对象（如数据库模型等）！&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 14 Dec 2018 07:56:00 +0000</pubDate>
<dc:creator>暮良文王</dc:creator>
<og:description>1.集成Python shell 每次自动shell会话都要导入数据库实例和模型，很烦人。为了避免一直重复导入，我们可以做些配置让Flask-Script的Shell命令自动导入特定的对象。若想把对象</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/liangmingshen/p/10119882.html</dc:identifier>
</item>
<item>
<title>在vue中子组件修改props引发的对js深拷贝和浅拷贝的思考 - 彼岸花在开</title>
<link>http://www.cnblogs.com/hutuzhu/p/10119698.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/hutuzhu/p/10119698.html</guid>
<description>&lt;p&gt;不管是react还是vue，父级组件与子组件的通信都是通过props来实现的，在vue中父组件的props遵循的是单向数据流，用官方的话说就是，父级的props的更新会向下流动到子组件中，反之则不行。也就是说，子组件不应该去修改props。但实际开发过程中，可能会有一些情况试图去修改props数据：&lt;/p&gt;
&lt;p&gt;1、这个props只是传递一个初始值，子组件把它当做一个局部变量来使用，这种情况一般定义一个本地的data属性，将props的值赋值给它。如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
props: ['initialCounter'],
data: function () {
  return {
    counter: this.initialCounter
  }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2、这个props的值以原始数据传入，但是子组件对其需要转换。这种情况，最好使用computed来定义一个计算属性，如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
props: [&lt;span&gt;'&lt;/span&gt;&lt;span&gt;size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;],
computed: {
  normalizedSize: function () {
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; &lt;span&gt;this&lt;/span&gt;&lt;span&gt;.size.trim().toLowerCase()
  }
}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;以上两种情况，传递的值都是基本数据类型，但是大多数情况下，我们需要向子组件传递一个引用类型数据，那么问题就来了。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;JavaScript 中对象和数组是通过引用传入的，所以对于一个数组或对象类型的 prop 来说，在子组件中改变这个对象或数组本身将会影响到父组件的状态。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;比如，在父组件中有一个列表，双击其中一个元素进行编辑，该元素的数据作为props传递给一个子组件，在子组件中需要对该数据进行编辑，你会发现如上所说，编辑后父组件的值也发生了变化。实际上我们想父组件影响子组件，但是子组件修改不要影响父组件。vue官网上貌似没说明这种情况应该如何处理。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214144536329-331399854.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这里情况相对简单点，在传递props时用Object.assign拷贝一份数据（这里数据是一个单层级对象），然后在子组件里面对其进行编辑。Object.assign能实现对象的合并，但是它是浅拷贝，也就是说如果对象的熟悉也是对象就不行。&lt;/p&gt;
&lt;p&gt;于是查阅了相关资料，再次巩固下JS中深拷贝与浅拷贝的相关知识。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;1、基本数据类型和引用数据类型的存储位置&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 基本数据类型是存储在栈内存中&lt;/strong&gt;，比如 var a=1;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214145815657-148172792.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;当进行复制操作b=a时，会在栈内存中再开一个内存，如下&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214145857698-161919722.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;变量a和变量b的存储互补影响，如果此时修改b的值不会影响a的值。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;引用类型数据存储在堆内存中，引用类型的名是存储在栈内存中，值是存储在堆内存中，但是栈内存会提供引用地址指向堆内存中的值。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214150227097-500206286.png&quot; alt=&quot;&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当进行b=a的复制操作时，复制的是引用地址，而不是堆内存中的值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214150318234-1518627287.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而当我们&lt;strong&gt;a[0]=1&lt;/strong&gt;时进行数组修改时，由于a与b指向的是同一个地址，所以自然b也受了影响，这就是所谓的浅拷贝了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214150359242-1265013958.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;而实际上我们希望的效果应该是这样：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214150421195-101771432.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;好，到这里，&lt;span&gt;&lt;strong&gt;到底什么是深浅拷贝：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;对于仅仅是复制了引用（地址），换句话说，复制了之后，原来的变量和新的变量指向同一个东西，彼此之间的操作会互相影响，为 &lt;strong&gt;浅拷贝&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而如果是在堆中重新分配内存，拥有不同的地址，但是值是一样的，复制后的对象与原来的对象是完全隔离，互不影响，为 &lt;strong&gt;深拷贝&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;回顾下JS里实现拷贝的方法有哪些：&lt;/p&gt;
&lt;p&gt;针对数组有这些方法：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Array.slice()&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
var a=[1,2,3];
var b=a.slice();&lt;br/&gt;b[0]=4;&lt;br/&gt;console.log(b);//[4,2,3]&lt;br/&gt;console.log(a);//[1,2,3]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Array.concat&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
var a=[1,2,3];
var b=a.concat();
b[0]=4;
console.log(b);//[4,2,3]
console.log(a);//[1,2,3]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; 当然，也可以遍历数组赋值。&lt;/p&gt;
&lt;p&gt;但是以上两种只对单级结构的数组有效，如果数组的元素是一个引用类型，就不行了，比如：&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
let a=[0,1,[2,3],4],
        b=a.slice();
a[0]=1;
a[2][0]=1;
console.log(a,b);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　&lt;img src=&quot;https://img2018.cnblogs.com/blog/570037/201812/570037-20181214151453795-959433000.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;修改二维数组的元素还是会影响原数组，也就是说slice和concat实际上是浅拷贝。&lt;/p&gt;
&lt;p&gt;针对对象：&lt;/p&gt;
&lt;p&gt;Object.assign()&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;34&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
var a={
  &quot;name&quot;:&quot;张三&quot;  
}；
b=Object.assign({},a);
b.name=&quot;李四&quot;；
console.log(b.name);//李四
console.log(a.name);//张三
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;同样该方法也是浅拷贝，如果对象属性值是引用类型也不行；&lt;/p&gt;
&lt;p&gt;那么到底有哪些办法可以实现深拷贝呢&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1、递归&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;40&quot;&gt;
&lt;pre&gt;
&lt;span&gt;function deepClone(obj){
    let objClone &lt;/span&gt;= Array.isArray(obj)?&lt;span&gt;[]:{};
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;(obj &amp;amp;&amp;amp; &lt;span&gt;typeof&lt;/span&gt; obj===&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;object&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;){
        &lt;/span&gt;&lt;span&gt;for&lt;/span&gt;(key &lt;span&gt;in&lt;/span&gt;&lt;span&gt; obj){
            &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;&lt;span&gt;(obj.hasOwnProperty(key)){
                &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;判断ojb子元素是否为对象，如果是，递归复制&lt;/span&gt;
                &lt;span&gt;if&lt;/span&gt;(obj[key]&amp;amp;&amp;amp;&lt;span&gt;typeof&lt;/span&gt; obj[key] ===&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;object&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;){
                    objClone[key] &lt;/span&gt;=&lt;span&gt; deepClone(obj[key]);
                }&lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;{
                    &lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;如果不是，简单复制&lt;/span&gt;
                    objClone[key] =&lt;span&gt; obj[key];
                }
            }
        }
    }
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; objClone;
}    
let a&lt;/span&gt;=[&lt;span&gt;1&lt;/span&gt;,&lt;span&gt;2&lt;/span&gt;,&lt;span&gt;3&lt;/span&gt;,&lt;span&gt;4&lt;/span&gt;&lt;span&gt;],
    b&lt;/span&gt;=&lt;span&gt;deepClone(a);
a[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;]=&lt;span&gt;2&lt;/span&gt;&lt;span&gt;;
console.log(a,b);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;2、jquery中的$.extend();&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;40&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt; obj = {name:&lt;span&gt;'&lt;/span&gt;&lt;span&gt;xixi&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,age:&lt;span&gt;20&lt;/span&gt;,company : { name : &lt;span&gt;'&lt;/span&gt;&lt;span&gt;腾讯&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, address : &lt;span&gt;'&lt;/span&gt;&lt;span&gt;深圳&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;} };
&lt;/span&gt;&lt;span&gt;var&lt;/span&gt; obj_extend = $.extend(&lt;span&gt;true&lt;/span&gt;,{}, obj); &lt;span&gt;//&lt;/span&gt;&lt;span&gt;extend方法，第一个参数为true，为深拷贝，为false，或者没有为浅拷贝。&lt;/span&gt;
console.log(obj ===&lt;span&gt; obj_extend);
obj.company.name &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;ali&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
obj.name &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;hei&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
console.log(obj);
console.log(obj_extend);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;3、JSON对象的JSON.parse()和JSON.stringify();&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt; obj = {name:&lt;span&gt;'&lt;/span&gt;&lt;span&gt;xixi&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;,age:&lt;span&gt;20&lt;/span&gt;,company : { name : &lt;span&gt;'&lt;/span&gt;&lt;span&gt;腾讯&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, address : &lt;span&gt;'&lt;/span&gt;&lt;span&gt;深圳&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;} };
&lt;/span&gt;&lt;span&gt;var&lt;/span&gt; obj_json =&lt;span&gt; JSON.parse(JSON.stringify(obj));
console.log(obj &lt;/span&gt;===&lt;span&gt; obj_json);
obj.company.name &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;ali&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
obj.name &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;hei&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;;
console.log(obj);
console.log(obj_json);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;4、Lodash中的_.cloneDeep()&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt; objects = [{ &lt;span&gt;'&lt;/span&gt;&lt;span&gt;a&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: &lt;span&gt;1&lt;/span&gt; }, { &lt;span&gt;'&lt;/span&gt;&lt;span&gt;b&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;: &lt;span&gt;2&lt;/span&gt;&lt;span&gt; }];
 
&lt;/span&gt;&lt;span&gt;var&lt;/span&gt; deep =&lt;span&gt; _.cloneDeep(objects);
console.log(deep[&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;] === objects[&lt;span&gt;0&lt;/span&gt;&lt;span&gt;]);
&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt; =&amp;gt; false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;虽然通过拷贝props数据解决了问题，但是拷贝后修改新数据的属性并不会触发vue的更新机制，需要强制更新$forceUpdate()，总觉得很奇怪，不知道大家有什么更好的办法没有，欢迎大家留言讨论。&lt;/p&gt;

&lt;p&gt;参考文章：&lt;/p&gt;
&lt;p&gt;https://zhuanlan.zhihu.com/p/26282765&lt;/p&gt;
&lt;p&gt;https://zhuanlan.zhihu.com/p/26282765&lt;/p&gt;

</description>
<pubDate>Fri, 14 Dec 2018 07:32:00 +0000</pubDate>
<dc:creator>彼岸花在开</dc:creator>
<og:description>不管是react还是vue，父级组件与子组件的通信都是通过props来实现的，在vue中父组件的props遵循的是单向数据流，用官方的话说就是，父级的props的更新会向下流动到子组件中，反之则不行。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/hutuzhu/p/10119698.html</dc:identifier>
</item>
<item>
<title>Linux VFS机制简析（二） - 舰队</title>
<link>http://www.cnblogs.com/jimbo17/p/10119567.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/jimbo17/p/10119567.html</guid>
<description>&lt;p&gt;接上一篇&lt;a href=&quot;https://www.cnblogs.com/jimbo17/p/10107318.html&quot;&gt;Linux VFS机制简析（一）&lt;/a&gt;，本篇继续介绍有关Address space和address operations、file和file operations、dentry和dentry operations和dentry cache API。&lt;/p&gt;
&lt;h2 id=&quot;address-space&quot;&gt;Address Space&lt;/h2&gt;
&lt;p&gt;Address Space用于管理page caches里的page页，它关联某个文件的所有pages，并管理文件的内容到进程地址空间的映射。它还提供了内存管理接口（page回收等）、根据地址查找page、跟踪page的tags（如dirty和writeback）等等功能。&lt;br/&gt;VM模块会调用-&amp;gt;write_page方法去尝试将脏页刷盘，以及调用-&amp;gt;releasepage方法将clean page释放。带有PagePrivate标记的clean page（引用为0）会被VM直接释放而不通知Address Space。&lt;br/&gt;为了实现这个功能，Address Space通过lru_cache_add()将page放入LRU，并通过mark_page_active()标记page正在使用。&lt;/p&gt;
&lt;p&gt;Pages通过-&amp;gt;index保存在一个radix树里，该radix树维护page的PG_Dirty和PG_Writeback信息，因此查找这两个标识的pages变得非常快。&lt;br/&gt;Dirty标记（&lt;em&gt;PAGECACHE_TAG_DIRTY&lt;/em&gt;）主要由-&amp;gt;writepages（默认方法mpage_writepages()）方法使用。它使用该标记查找脏页并调用-&amp;gt;writepage方法。如果Address operations实现了自己的-&amp;gt;writepages（不使用mpage_writepages），则Dirty标记将几乎没有作用。write_inode_now()和sync_inode()通过Dirty标记来检查-&amp;gt;writepages是否成功完成。&lt;br/&gt;Writeback标记主要是由filemap_&lt;em&gt;wait&lt;/em&gt; 方法和sync_page* 方法使用，通过调用filemap_fdatawait_range()等待所有的writeback完成。如果定义了-&amp;gt;sync_page，则会调用它来等待所有需要writeback的page结束。&lt;/p&gt;
&lt;p&gt;Address Space Handler可以通过page的&lt;em&gt;private&lt;/em&gt;字段保存额外的数据，此时需要设置PG_Private标识。这样VM的相关操作会调用address的handler处理这些数据。&lt;/p&gt;
&lt;p&gt;上面说的这么多Page相关的管理，其实Address Space最核心的作用是充当存储和应用程序的中间缓存。数据从存储侧以page为单位读入address space，通过拷贝或者mapping的方式提供给应用层。应用写入数据到address space，然后通过writeback机制写入到存储。&lt;br/&gt;读操作的核心是&lt;em&gt;readpage()&lt;/em&gt;。写操作稍微复杂些，可以通过&lt;em&gt;write_begin&lt;/em&gt;/&lt;em&gt;write_end&lt;/em&gt;或者&lt;em&gt;set_page_dirty&lt;/em&gt;写入数据到address space，再通过&lt;em&gt;writepage&lt;/em&gt;、&lt;em&gt;sync_page&lt;/em&gt;和&lt;em&gt;writepages&lt;/em&gt;写入数据到存储。从address space里增加删除page由inode的&lt;em&gt;i_mutex&lt;/em&gt;锁保护。&lt;/p&gt;
&lt;p&gt;当数据写入到page，需要设置PG_Dirty标识，当writepage准备写入存储时清除PG_Dirty，并设置PG_Writeback标识，知道数据完全写入存储后清除PG_Writeback。&lt;/p&gt;
&lt;h3 id=&quot;struct-address_space_operations&quot;&gt;struct address_space_operations&lt;/h3&gt;
&lt;p&gt;struct address_space_operations的定义如下：&lt;/p&gt;
&lt;pre class=&quot;clang&quot;&gt;
&lt;code&gt;struct address_space_operations {
    int (*writepage)(struct page *page, struct writeback_control *wbc);
    int (*readpage)(struct file *, struct page *);
    int (*sync_page)(struct page *);
    int (*writepages)(struct address_space *, struct writeback_control *);
    int (*set_page_dirty)(struct page *page);
    int (*readpages)(struct file *filp, struct address_space *mapping,
                    struct list_head *pages, unsigned nr_pages);
    int (*write_begin)(struct file *, struct address_space *mapping,
                            loff_t pos, unsigned len, unsigned flags,
                            struct page **pagep, void **fsdata);
    int (*write_end)(struct file *, struct address_space *mapping,
                            loff_t pos, unsigned len, unsigned copied,
                            struct page *page, void *fsdata);
    sector_t (*bmap)(struct address_space *, sector_t);
    int (*invalidatepage) (struct page *, unsigned long);
    int (*releasepage) (struct page *, int);
    void (*freepage)(struct page *);
    ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov,
                    loff_t offset, unsigned long nr_segs);
    struct page* (*get_xip_page)(struct address_space *, sector_t,
                    int);
    /* migrate the contents of a page to the specified target */
    int (*migratepage) (struct page *, struct page *);
    int (*launder_page) (struct page *);
    int (*error_remove_page) (struct mapping *mapping, struct page *page);
    int (*swap_activate)(struct file *);
    int (*swap_deactivate)(struct file *);
};&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;writepage&lt;/em&gt;：VM调用，用于将脏页写入后端存储。参数wbc-&amp;gt;sync_mode显示是什么原因触发，'sync'或者'flush'（释放内存）。调用时PG_Dirty已经被清除，并且PageLocked已经设置。&lt;em&gt;writepage&lt;/em&gt;开始写入数据时需要设置PG_Writeback，并且写入结束时清除该标记。无论是同步还是异步写入，都要保证函数返回时page处于unlocked状态。&lt;br/&gt;如果wbc-&amp;gt;sync_mode是WB_SYNC_NONE（不等待），则&lt;em&gt;writepage&lt;/em&gt;遇到困难时可以不那么努力的写入，而是返回AOP_WRITEPAGE_ACTIVATE，这样VM不会老是来写该page。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;readpage&lt;/em&gt;：VM调用，用于从后端存储读取数据。调用时，page处于lock状态，并且在读取结束时需要设置为unlock状态，并设置uptodate。如果&lt;em&gt;readpage&lt;/em&gt;处理过程中需要unlock page，则unlcok之后需要返回AOP_TRUNCATED_PAGE，调用者将重新定位page并重新lock，成功之后会再次调用&lt;em&gt;readpage&lt;/em&gt;。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;sync_page&lt;/em&gt;：VM调用，用于通知后端存储处理该page的I/O。该page所属address space的其他Pages的I/O也可能被处理。该函数是可选的，仅用于等待PG_Writeback的page处理完成。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;writepages&lt;/em&gt;：VM调用，将address space里所有Dirty的pages写入后端存储。如果wbc-&amp;gt;sync_mode是WBC_SYNC_ALL，则writeback_control会选取一个范围的pages必须写入。如果是WBC_SYNC_NONE，则根据参数nr_to_write尽可能写入这么多pages。如果没有设置，则默认调用mpage_writepages()。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;set_page_dirty&lt;/em&gt;：VM调用，用于设置page为dirty。通常用于address space里有新的数据写入，如memory mapping的page被修改。该函数将设置PageDirty标记，并在Radix树里设置PAGECACHE_TAG_DIRTY标识。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;readpages&lt;/em&gt;：VM调用，用于读取address space里的指定pages。主要是通过调用&lt;em&gt;readpage&lt;/em&gt;将一组pages读取。通常用于预读，因此读取失败的错误码可能会被忽略。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;write_begin&lt;/em&gt;：由通用的buffered写流程调用，写入len长度数据到文件的offset处。address space可能需要申请额外的存储空间来保证写操作可以完成，或者需要从后端存储读取不在缓存里的pages。该函数返回的pagep要处于locked状态，调用者将直接写入数据。返回参数fsdata用于私有数据指针，它将传递给&lt;em&gt;write_end&lt;/em&gt;函数。如果函数返回&amp;lt;0，则&lt;em&gt;write_end&lt;/em&gt;将不会调用。&lt;br/&gt;&lt;em&gt;write_end&lt;/em&gt;：数据拷贝到&lt;em&gt;write_begin&lt;/em&gt;返回的page后，调用&lt;em&gt;write_end&lt;/em&gt;将page unlock，递减引用计数并更新i_size字段。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;bmap&lt;/em&gt;：VFS调用用于映射逻辑块的偏移和物理块编号。该方法由FIBMAP ioctl使用，并且是swap文件。swap系统不直接进入文件系统，而是通过BMAP方式建立内存地址和文件的块映射，然后直接使用内存地址。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;invalidatepage&lt;/em&gt;：如果设置了PagePrivate，则当Page部分或者全部从address space里删除时调用该方法。通常是因为address space里执行了一个截断或者是失效所有数据。和page关联的私有信息需要更新，或者直接被释放（如果失效的offset为0的话，整个page将被释放）。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;releasepage&lt;/em&gt;：用于将PagePrivate pages释放，它将把私有数据释放，然后清除PagePrivate标识。&lt;em&gt;releasepage&lt;/em&gt;有两种使用场景，一是VM发现没有引用计数的clean page，想将其变成free page。通过调用&lt;em&gt;releasepage&lt;/em&gt;将其从address space里摘掉变为clean page。二是有invalid请求需要将address space里的部分或全部pages失效。通常是fadvice系统调用或者文件系统自己认为缓存里的数据已经不是最新的了，此时通过调用invalidate_inode_pages2()将pages释放。调动该函数前，需要保证pages已经是invalidate的。如果释放私有数据失败，则需要在返回错误之前将PageUptodate清除。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;freepage&lt;/em&gt;：用于将不在pagecache里的page释放，page必须不属于任何address space。通常有内存回收处理程序调用。&lt;br/&gt;&lt;em&gt;direct_IO&lt;/em&gt;：由通用读写流程调用，绕过pagecache，DIO方式读取数据。&lt;br/&gt;&lt;em&gt;get_xip_page&lt;/em&gt;：VM调用，将block number转换为page。支持XIP（execute in place）的文件系统需要实现该函数。&lt;br/&gt;&lt;em&gt;migrate_page&lt;/em&gt;：在old page和new page之间迁移数据，通常用于内存整理（减少碎片）。迁移时需要将私有数据和引用一起迁移。&lt;br/&gt;&lt;em&gt;launder_page&lt;/em&gt;：在free之前调用，用于writeback dirty page。为了防止再次被设置dirty，操作过程中持有page lock。&lt;br/&gt;&lt;em&gt;error_remove_page&lt;/em&gt;：用于内存分配失败的处理，如果address space支持truncation，通常设置为generic_error_remove_page()。&lt;br/&gt;&lt;em&gt;swap_activate&lt;/em&gt; and &lt;em&gt;swap_deactivate&lt;/em&gt;：用于swapon在一个文件上时，分配空间并将block信息保存在内存中。以及swapoff时释放空间。&lt;/p&gt;
&lt;h2 id=&quot;file&quot;&gt;File&lt;/h2&gt;
&lt;p&gt;一个File数据结构代表一个进程里打开的一个文件。所以File结构是跟进程相关的，不同的进程打开同一个文件会在每个进程里都有一个File对象，对应到进程的文件句柄。&lt;br/&gt;同一个文件File结构指向的inode是同一个，所以通过pagecache缓存进行数据读写的时候，使用的是inode里同一个address space，保证文件数据在不同进程里的一致性。&lt;/p&gt;
&lt;h3 id=&quot;struct-file_operations&quot;&gt;struct file_operations&lt;/h3&gt;
&lt;p&gt;struct file_operations的定义如下：&lt;/p&gt;
&lt;pre class=&quot;clang&quot;&gt;
&lt;code&gt;struct file_operations {
    struct module *owner;
    loff_t (*llseek) (struct file *, loff_t, int);
    ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
    ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
    ssize_t (*aio_read) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
    ssize_t (*aio_write) (struct kiocb *, const struct iovec *, unsigned long, loff_t);
    int (*readdir) (struct file *, void *, filldir_t);
    unsigned int (*poll) (struct file *, struct poll_table_struct *);
    long (*unlocked_ioctl) (struct file *, unsigned int, unsigned long);
    long (*compat_ioctl) (struct file *, unsigned int, unsigned long);
    int (*mmap) (struct file *, struct vm_area_struct *);
    int (*open) (struct inode *, struct file *);
    int (*flush) (struct file *);
    int (*release) (struct inode *, struct file *);
    int (*fsync) (struct file *, loff_t, loff_t, int datasync);
    int (*aio_fsync) (struct kiocb *, int datasync);
    int (*fasync) (int, struct file *, int);
    int (*lock) (struct file *, int, struct file_lock *);
    ssize_t (*readv) (struct file *, const struct iovec *, unsigned long, loff_t *);
    ssize_t (*writev) (struct file *, const struct iovec *, unsigned long, loff_t *);
    ssize_t (*sendfile) (struct file *, loff_t *, size_t, read_actor_t, void *);
    ssize_t (*sendpage) (struct file *, struct page *, int, size_t, loff_t *, int);
    unsigned long (*get_unmapped_area)(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);
    int (*check_flags)(int);
    int (*flock) (struct file *, int, struct file_lock *);
    ssize_t (*splice_write)(struct pipe_inode_info *, struct file *, size_t, unsigned int);
    ssize_t (*splice_read)(struct file *, struct pipe_inode_info *, size_t, unsigned int);
    int (*setlease)(struct file *, long arg, struct file_lock **);
    long (*fallocate)(struct file *, int mode, loff_t offset, loff_t len);
};&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;同样，如果没有特别说明，则所有操作都在没有锁持有的情况下调用。&lt;br/&gt;file_operations里大部分函数跟POSIX文件系统接口语义一样，就不单独列出了。&lt;/p&gt;
&lt;h2 id=&quot;dentry&quot;&gt;Dentry&lt;/h2&gt;
&lt;p&gt;dcache（dentry cache）用于缓存dentry，每个dentry用于索引filename和inode number。dentry也有一套操作合集dentry operations用于管理dentry。底层文件系统可以选择实现自己的dentry operations来替换默认的operations。&lt;/p&gt;
&lt;h3 id=&quot;struct-dentry_operations&quot;&gt;struct dentry_operations&lt;/h3&gt;
&lt;p&gt;struct dentry_operations的定义如下：&lt;/p&gt;
&lt;pre class=&quot;clang&quot;&gt;
&lt;code&gt;struct dentry_operations {
    int (*d_revalidate)(struct dentry *, unsigned int);
    int (*d_weak_revalidate)(struct dentry *, unsigned int);
    int (*d_hash)(const struct dentry *, const struct inode *,
                    struct qstr *);
    int (*d_compare)(const struct dentry *, const struct inode *,
                    const struct dentry *, const struct inode *,
                    unsigned int, const char *, const struct qstr *);
    int (*d_delete)(const struct dentry *);
    void (*d_release)(struct dentry *);
    void (*d_iput)(struct dentry *, struct inode *);
    char *(*d_dname)(struct dentry *, char *, int);
    struct vfsmount *(*d_automount)(struct path *);
    int (*d_manage)(struct dentry *, bool);
};&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;d_revalidate&lt;/em&gt;：VFS用于检查在dcache里找到的dentry是否有效。通常设置为NULL，则只要在dcache找到即认为是有效的。但对网络文件系统如NFS来说，dentry可能在一段时间之后就会失效，因此需要实现该函数用于检查是否有效。如果有效，函数需要返回一个正数。&lt;br/&gt;&lt;em&gt;d_revalidate&lt;/em&gt;可能在rcu-walk模式（flags &amp;amp; LOOKUP_RCU）下被调用。此时该函数里不能阻塞也不能写入数据到dentry，并且d_parent和d_inode不能使用，因为他们可能瞬间就可能被修改。如果在rcu-walk模式遇到困难，则返回-ECHILD，将在ref-walk模式下重新调用。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_weak_revalidate&lt;/em&gt;：用于检查'jumped'的dentry，即那些不是通过lookup获取的dentry，如'', '.'或者'..'。这种场景只需要检查dentry对应inode是否OK即可。该函数不会在rcu-walk模式下调用，所以可以放心的使用inode。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_hash&lt;/em&gt;：用于VFS将dentry放入HASH列表。并不清楚HASH表用来做啥，通常不需要设置它，使用VFS默认的即可。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_compare&lt;/em&gt;：用于比较dentry name和指定的name。该函数必须是可重入的，即每次的返回结果一样。&lt;br/&gt;&lt;em&gt;d_delete&lt;/em&gt;：用于引用计数递减为0时调用，返回1则dcache立即删除dentry，返回0则继续缓存该dentry。默认为NULL，则总是将dentry进行缓存。该函数必须是可重入的，即每次的返回结果一样。&lt;br/&gt;&lt;em&gt;d_release&lt;/em&gt;：用于释放dentry资源。&lt;br/&gt;&lt;em&gt;d_iput&lt;/em&gt;：用于释放dentry对应inode引用计数。该函数在释放dentry之前调用。如果为NULL，则VFS默认调用iput()。&lt;br/&gt;&lt;em&gt;d_dname&lt;/em&gt;：用于生成dentry的pathname，主要是一些伪文件系统（sockfs, pipefs等）用于延迟生成pathname。一般文件系统不实现该函数，因为其dentry存在于dcache的hash表里（通过pathname做hash），所以并不希望pathname变化。&lt;br/&gt;&lt;em&gt;d_automount&lt;/em&gt;：可选函数，用于穿越到一个自动挂载的dentry。它会创建一个新的vfsmount记录，并将其返回，成功后调用者将根据vfsmount去尝试mount它到挂载点。&lt;br/&gt;&lt;em&gt;d_manage&lt;/em&gt;：可选函数，用于管理从dentry进行transition。&lt;/p&gt;
&lt;h3 id=&quot;directory-entry-cache-api&quot;&gt;Directory Entry Cache API&lt;/h3&gt;
&lt;p&gt;以下函数是VFS提供给文件系统参与维护和管理的dentry cache的API接口。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;dget&lt;/em&gt;：用于增加dentry引用计数。&lt;br/&gt;&lt;em&gt;dput&lt;/em&gt;：递减引用计数，如果减为0，则调用&lt;em&gt;d_delete&lt;/em&gt;判断是否留在缓存里。如果判断为否，或者该dentry已经不在其父目录hash列表里，则将其删除。如果判断为是，则dentry放入LRU链表，并在触发内存回收时删除。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_drop&lt;/em&gt;：将dentry从其父目录的hash列表里删除。随后如果引用计数减为0，该dentry将被删除。&lt;br/&gt;&lt;em&gt;d_delete&lt;/em&gt;：将dentry删除。如果引用计数不为0，则调用&lt;em&gt;d_drop&lt;/em&gt;。如果为0，则调用&lt;em&gt;d_iput&lt;/em&gt;将dentry搞成nagtive dentry。注意该函数不是dentry operations-&amp;gt;d_delete函数指针，而是VFS提供的API接口。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_add&lt;/em&gt;：将dentry加入到父目录的hash列表里，并调用&lt;em&gt;d_instantiate&lt;/em&gt;。&lt;br/&gt;&lt;em&gt;d_instantiate&lt;/em&gt;：将dentry加入到对应的inode的hash列表里，并更新其d_inode字段。inode的引用计数i_count字段需要递增。该函数通常用于新创建inode给一个nagtive dentry。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;d_lookup&lt;/em&gt;：根据pathname，查找父目录dentry下的某个dentry。如果找到，则增加引用计数并返回dentry。调用用完该dentry之后需要通过&lt;em&gt;dput&lt;/em&gt;将引用计数递减。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;VFS的角色包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;管理可用的文件系统类型，将设备和文件系统实例进行关联。&lt;/li&gt;
&lt;li&gt;处理文件系统的相关操作，为应用程序提供标准文件系统接口。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;VFS和具体的文件系统系统之间主要通过几个数据结构：super_block, inode, dentry, file和address space以及对应的operations: sb_ops, i_ops, d_ops, f_ops和a_ops来实现文件系统的功能。&lt;/p&gt;
&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://tomoyo.osdn.jp/cgi-bin/lxr/source/Documentation/filesystems/vfs.txt?v=linux-3.10.108&quot;&gt;Linux Documentation: VFS&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 14 Dec 2018 07:15:00 +0000</pubDate>
<dc:creator>舰队</dc:creator>
<og:description>Linux VFS机制简析（二） 接上一篇</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/jimbo17/p/10119567.html</dc:identifier>
</item>
<item>
<title>统计学习方法ｃ++实现之一　感知机 - bobxxxl</title>
<link>http://www.cnblogs.com/bobxxxl/p/10119130.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/bobxxxl/p/10119130.html</guid>
<description>&lt;h2 id=&quot;感知机&quot;&gt;感知机&lt;/h2&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;最近学习了c++，俗话说‘光说不练假把式’，所以决定用c++将《统计学习方法》里面的经典模型全部实现一下，&lt;a href=&quot;https://github.com/bBobxx/statistical-learning&quot;&gt;&lt;strong&gt;代码&lt;/strong&gt;&lt;/a&gt;在这里，请大家多多指教。&lt;/p&gt;
&lt;p&gt;感知机虽然简单，但是他可以为学习其他模型提供基础，现在先简单回顾一下基础知识。&lt;/p&gt;
&lt;h2 id=&quot;感知机模型&quot;&gt;感知机模型&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Ncell.png/300px-Ncell.png&quot; alt=&quot;感知机&quot;/&gt;&lt;/p&gt;
&lt;p&gt;首先，感知机是用来分类的模型，上图就是简单的感知机模型，其中&lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt; 我们一般取符号函数&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[sign(x)=\begin{cases} -1,\quad x&amp;lt;0 \\\\ +1,\quad x\geq0 \end{cases} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;所以感知机的数学形式就是&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[y=sign(wx+b)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;其中w和x都是n维的向量。当n为２时，&lt;span class=&quot;math inline&quot;&gt;\(sign\)&lt;/span&gt;里面的公式有没有特别熟悉?就是直线的公式，n&amp;gt;2就是超平面，用一下课本里面的图就是如下图&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/2666154-bb097804935bf5ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/608/format/webp&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这就是分类的根据，必须要注意，感知机只能分离&lt;strong&gt;线性可分数据&lt;/strong&gt;，非线性的不行。&lt;/p&gt;
&lt;h2 id=&quot;感知机学习策略&quot;&gt;感知机学习策略&lt;/h2&gt;
&lt;p&gt;提到学习就不得不提到梯度下降算法。感知机的学习策略就是随机梯度下降算法。&lt;/p&gt;
&lt;p&gt;具体的在书中讲的很详细，我这里就不赘述了，直接看学习算法吧：&lt;/p&gt;
&lt;p&gt;(1) 选取初值w,b。&lt;/p&gt;
&lt;p&gt;(2) 选取一组训练数据(x, y)。&lt;/p&gt;
&lt;p&gt;(3) 如果&lt;span class=&quot;math inline&quot;&gt;\(y(wx+b)\leq0\)&lt;/span&gt;,则&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[ w += lr*yx\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[b+=lr*y\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(4)转至(2)直到没有误分类点。&lt;/p&gt;
&lt;h2 id=&quot;c实现感知机&quot;&gt;c++实现感知机&lt;/h2&gt;
&lt;p&gt;首先我有一个基类Base,为了以后的算法继承用的，它包含一个run()的纯虚函数，这样以后就可以在main里面实现多态。&lt;/p&gt;
&lt;p&gt;我的数据都存储在私有成员里：&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;    std::vector&amp;lt;std::vector&amp;lt;double&amp;gt;&amp;gt; inData;//从文件都的数据
    std::vector&amp;lt;std::vector&amp;lt;double&amp;gt;&amp;gt; trainData;//分割后的训练数据，里面包含真值
    std::vector&amp;lt;std::vector&amp;lt;double&amp;gt;&amp;gt; testData;
    unsigned long indim = 0;
    std::vector&amp;lt;double&amp;gt; paraData;
    std::vector&amp;lt;std::vector&amp;lt;double&amp;gt;&amp;gt; trainDataF;//真正的训练数据，特征
    std::vector&amp;lt;std::vector&amp;lt;double&amp;gt;&amp;gt; testDataF;
    std::vector&amp;lt;double&amp;gt; trainDataGT;//真值
    std::vector&amp;lt;double&amp;gt; testDataGT;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在main函数里只需要调用每个模型的run()方法，声明的是基类指针:&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;int main() {
    Base* obj = new Perceptron();
    obj-&amp;gt;run();
    delete obj;
    return 0;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;第一步，读取数据并分割。这里用的vector存储。&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;    getData(&quot;../data/perceptrondata.txt&quot;);
    splitData(0.6);//below is split data , and store it in　trainData, testData&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;第二步初始化&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;    std::vector&amp;lt;double&amp;gt; init = {1.0,1.0,1.0};
    initialize(init);&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;第三步进行训练。&lt;/p&gt;
&lt;p&gt;在训练时，函数调用顺序如下:&lt;/p&gt;
&lt;ul readability=&quot;17&quot;&gt;&lt;li readability=&quot;25&quot;&gt;
&lt;p&gt;调用computeGradient，进行梯度的计算。对于满足&lt;span class=&quot;math inline&quot;&gt;\(y(wx+b)&amp;gt;0\)&lt;/span&gt;的数据我们把梯度设为０。&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;std::pair&amp;lt;std::vector&amp;lt;double&amp;gt;, double&amp;gt; Perceptron::computeGradient(const std::vector&amp;lt;double&amp;gt;&amp;amp; inputData, const double&amp;amp; groundTruth) {
    double lossVal = loss(inputData, groundTruth);
    std::vector&amp;lt;double&amp;gt; w;
    double b;
    if (lossVal &amp;gt; 0.0)
    {
        for(auto indata:inputData) {
            w.push_back(indata*groundTruth);
        }
        b = groundTruth;
    }
    else{
        for(auto indata:inputData) {
            w.push_back(0.0);
        }
        b = 0.0;
    }
    return std::pair&amp;lt;std::vector&amp;lt;double&amp;gt;, double&amp;gt;(w, b);//here, for understandable, we use pair to represent w and b.
                           //you also could return a vector which contains w and b.
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在调用computeGradient时又调用了loss，即计算&lt;span class=&quot;math inline&quot;&gt;\(-y(wx+b)\)&lt;/span&gt;,loss里调用了inference，用来计算&lt;span class=&quot;math inline&quot;&gt;\(wx+b\)&lt;/span&gt;,看起来有点多余对吧，inference函数存在的目的是为了后面预测时候用的。&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;double Perceptron::loss(const std::vector&amp;lt;double&amp;gt;&amp;amp; inputData, const double&amp;amp; groundTruth){
    double loss = -1.0 * groundTruth * inference(inputData);
    std::cout&amp;lt;&amp;lt;&quot;loss is &quot;&amp;lt;&amp;lt; loss &amp;lt;&amp;lt;std::endl;
    return loss;
}


double Perceptron::inference(const std::vector&amp;lt;double&amp;gt;&amp;amp; inputData){
    //just compute wx+b , for compute loss and predict.
    if (inputData.size()!=indim){
        std::cout&amp;lt;&amp;lt;&quot;input dimension is incorrect. &quot;&amp;lt;&amp;lt;std::endl;
        throw inputData.size();
    }

    double sum_tem = 0.0;
    for(int i=0; i&amp;lt;indim; ++i){
        sum_tem += inputData[i]*paraData[i];
    }
    sum_tem += paraData[indim];
    return sum_tem;
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/li&gt;
&lt;li readability=&quot;7&quot;&gt;
&lt;p&gt;根据计算的梯度更新w, b&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;void Perceptron::train(const int &amp;amp; step, const float &amp;amp; lr) {
    int count = 0;
    createFeatureGt();
    for(int i=0; i&amp;lt;step; ++i){
        if (count==trainDataF.size()-1)
            count = 0;
        count++;
        std::vector&amp;lt;double&amp;gt; inputData = trainDataF[count];
        double groundTruth = trainDataGT[count];
        auto grad = computeGradient(inputData, groundTruth);
        auto grad_w = grad.first;
        double grad_b = grad.second;
        for (int j=0; j&amp;lt;indim;++j){//这里更新参数
            paraData[j] += lr * (grad_w[j]);
        }
        paraData[indim] += lr * (grad_b);
    }
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;预测用的数据也是之前就分割好的，注意这里的参数始终存在&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;std::vector&amp;lt;double&amp;gt; paraData; &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;进行预测的代码&lt;/p&gt;
&lt;pre class=&quot;cpp&quot;&gt;
&lt;code&gt;int Perceptron::predict(const std::vector&amp;lt;double&amp;gt;&amp;amp; inputData, const double&amp;amp; GT) {

    double out = inference(inputData);
    std::cout&amp;lt;&amp;lt;&quot;The right class is &quot;&amp;lt;&amp;lt;GT&amp;lt;&amp;lt;std::endl;
    if(out&amp;gt;=0.0){
        std::cout&amp;lt;&amp;lt;&quot;The predict class is 1&quot;&amp;lt;&amp;lt;std::endl;
        return 1;
    }
    else{
        std::cout&amp;lt;&amp;lt;&quot;The right class is -1&quot;&amp;lt;&amp;lt;std::endl;
        return -1;
    }&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Fri, 14 Dec 2018 06:10:00 +0000</pubDate>
<dc:creator>bobxxxl</dc:creator>
<og:description>感知机 前言 最近学习了c++，俗话说‘光说不练假把式’，所以决定用c++将《统计学习方法》里面的经典模型全部实现一下，</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/bobxxxl/p/10119130.html</dc:identifier>
</item>
<item>
<title>AlphaZero并行五子棋AI - initial_h</title>
<link>http://www.cnblogs.com/initial-h/p/10118871.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/initial-h/p/10118871.html</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1428973/201812/1428973-20181214131414867-718580985.gif&quot; width=&quot;400&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Github : &lt;a href=&quot;https://github.com/initial-h/AlphaZero_Gomoku_MPI&quot;&gt;AlphaZero-Gomoku-MPI&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This repo is based on &lt;a href=&quot;https://github.com/junxiaosong/AlphaZero_Gomoku&quot;&gt;junxiaosong/AlphaZero_Gomoku&lt;/a&gt;, sincerely grateful for it.&lt;/p&gt;
&lt;p&gt;I do these things:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Implement asynchronous self-play training pipeline in parallel like AlphaGo Zero's way&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Write a root parallel mcts (vote a move using ensemble way)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Use ResNet structure to train the model and set a transfer learning API to train a larger board model based on small board's model (like pre-training way in order to save time)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Strength&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Current model is on 11x11 board, and playout 400 times when test&lt;/li&gt;
&lt;li&gt;Play with &lt;a href=&quot;https://github.com/yzhq97/AlphaGomokuZero&quot;&gt;this model&lt;/a&gt;, can always win regardless of black or white&lt;/li&gt;
&lt;li&gt;Play with &lt;a href=&quot;http://gomocup.org/download-gomoku-ai/&quot;&gt;gomocup's AI&lt;/a&gt;, can rank around 20th-30th for some rough tests&lt;/li&gt;
&lt;li&gt;When I play white, I can't win AI. When I play black, end up with tie/lose for most of my time&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;References&lt;/p&gt;
&lt;p&gt;Blog&lt;/p&gt;
&lt;h2 id=&quot;installation-dependencies&quot;&gt;Installation Dependencies&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Python3&lt;/li&gt;
&lt;li&gt;tensorflow&amp;gt;=1.8.0&lt;/li&gt;
&lt;li&gt;tensorlayer&amp;gt;=1.8.5&lt;/li&gt;
&lt;li&gt;mpi4py (parallel train and play)&lt;/li&gt;
&lt;li&gt;pygame (GUI)&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;how-to-install&quot;&gt;How to Install&lt;/h2&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;tensorflow/tensorlayer/pygame install :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
&lt;code&gt;conda install tensorflow
conda install tensorlayer
conda install pygame&lt;/code&gt;
&lt;/pre&gt;
&lt;blockquote readability=&quot;4.4074074074074&quot;&gt;
&lt;p&gt;mpi4py install &lt;a href=&quot;https://www.jianshu.com/p/ba6f7c9415a0&quot;&gt;click here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;mpi4py on windows &lt;a href=&quot;https://blog.csdn.net/mengmengz07/article/details/70163140&quot;&gt;click here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;how-to-run&quot;&gt;How to Run&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Play with AI&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;python human_play.py&lt;/code&gt;
&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;Play with parallel AI (-np : set number of processings, take care of OOM !)&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;mpiexec -np 3 python -u human_play_mpi.py &lt;/code&gt;
&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;Train from scratch&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;python train.py&lt;/code&gt;
&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;Train in parallel&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;mpiexec -np 43 python -u train_mpi.py&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;It's almost no difference between AlphaGo Zero except APV-MCTS.&lt;br/&gt;A PPT can be found in dir &lt;a href=&quot;https://github.com/initial-h/AlphaZero_Gomoku_MPI/tree/master/demo/slides&quot;&gt;demo/slides&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1428973/201812/1428973-20181214131856754-1649208691.png&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;p&gt;Most settings are the same with AlphaGo Zero, details as follow :&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Network Structure
&lt;ul&gt;&lt;li&gt;Current model uses 19 residual blocks, more blocks means more accurate prediction but also slower speed&lt;/li&gt;
&lt;li&gt;The number of filters in convolutional layer shows in the follow picture&lt;br/&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1428973/201812/1428973-20181214131925707-735850373.jpg&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Feature Planes
&lt;ul&gt;&lt;li&gt;In AlphaGo Zero paper, there are 19 feature planes: 8 for current player's stones, 8 for opponent's stones, and the final feature plane represents the colour to play&lt;/li&gt;
&lt;li&gt;Here I only use 4 for each player, it can be easily changed in &lt;code&gt;game_board.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dirichlet Noise
&lt;ul&gt;&lt;li&gt;I add dirichlet noises in each node, it's different from paper that only add noises in root node. I guess AlphaGo Zero discard the whole tree after each move and rebuild a new tree, while here I keep the nodes under the chosen action, it's a little different&lt;/li&gt;
&lt;li&gt;Weights between prior probabilities and noises are not changed here (0.75/0.25), though I think maybe 0.8/0.2 or even 0.9/0.1 is better because noises are added in every node&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;parameters in detail
&lt;ul readability=&quot;-0.5&quot;&gt;&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;I try to maintain the original parameters in AlphaGo Zero paper, so as to testify it's generalization. Besides, I also take training time and computer configuration into consideration.&lt;/p&gt;
&lt;table&gt;&lt;thead/&gt;&lt;tbody&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;MPI num&lt;/td&gt;
&lt;td&gt;43&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;c_puct&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;n_playout&lt;/td&gt;
&lt;td&gt;400&lt;/td&gt;
&lt;td&gt;1600&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;blocks&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;19/39&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;buffer size&lt;/td&gt;
&lt;td&gt;500,000(data)&lt;/td&gt;
&lt;td&gt;500,000(games)&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;batch_size&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;lr&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;annealed&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;optimizer&lt;/td&gt;
&lt;td&gt;Adam&lt;/td&gt;
&lt;td&gt;SGD with momentum&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;dirichlet noise&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;0.03&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;even&quot;&gt;&lt;td&gt;weight of noise&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;odd&quot;&gt;&lt;td&gt;first n move&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Training detials
&lt;ul&gt;&lt;li&gt;I train the model for about 100,000 games and takes 800 hours or so&lt;/li&gt;
&lt;li&gt;Computer configuration : 2 CPU and 2 1080ti GPU&lt;/li&gt;
&lt;li&gt;We can easily find the computation gap with DeepMind and rich people can do some future work&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;some-tips&quot;&gt;Some Tips&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Network
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;ZeroPadding with Input&lt;/strong&gt; : Sometimes when play with AI, it's unaware of the risk at the edge of board even though I'm three/four in a row. ZeroPadding data input can mitigate the problem&lt;/li&gt;
&lt;li&gt;Put the network on GPU : If the network is shallow, it's not matter CPU/GPU to use, otherwise it's faster to use GPU when self-play&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dirichlet Noise
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Add Noise in Node&lt;/strong&gt; : In &lt;a href=&quot;https://github.com/junxiaosong/AlphaZero_Gomoku&quot;&gt;junxiaosong/AlphaZero_Gomoku&lt;/a&gt;, noises are added outside the tree, seemingly like DQN's &lt;span class=&quot;math inline&quot;&gt;\(\epsilon-greedy\)&lt;/span&gt; way. It's ok when I test on 6x6 and 8x8 board, but when on 11x11 some problems occur. After a long time training on 11x11, black player will always play the first stone in the middle place with policy probability equal to 1. It's very rational for black to play here, however, the white player will never see other kifu that play in the other place at first stone. So, when I play black with AI and place somewhere not the middle place, AI will get very stupid because it has never seen this way at all. Add noise in node can mitigate the problem&lt;/li&gt;
&lt;li&gt;Smaller Weight with Noise : As I said before, I think maybe 0.8/0.2 or even 0.9/0.1 is a better choice between prior probabilities and noises' weights, because noises are added in every node&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Randomness
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Dihedral Reflection or Rotation&lt;/strong&gt; : When use the network to output probabilities/value, it's better to do as paper said: The leaf node &lt;span class=&quot;math inline&quot;&gt;\(s_L\)&lt;/span&gt; is added to a queue for neural network evaluation, &lt;span class=&quot;math inline&quot;&gt;\((d_i(p),v)=f_{\theta}(d_i(s_L))\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(d_i\)&lt;/span&gt; is a dihedral reflection or rotation selected uniformly at random from &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; in &lt;span class=&quot;math inline&quot;&gt;\([1..8]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Add Randomness when Test : I add the dihedral reflection or rotation also when play with it, so as to avoid to play the same game all the time&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Tradeoffs
&lt;ul&gt;&lt;li&gt;Network Depth : If the network is too shallow, loss will increase. If too deep, it's slow when train and test. (My network is still a little slow when play with it, I think maybe 9 blocks is all right)&lt;/li&gt;
&lt;li&gt;Buffer Size : If the size is small, it's easy to fit by network but can't guarantee it's performance for only learning from these few data. If it's too large, much longer time and deeper network structure should be taken&lt;/li&gt;
&lt;li&gt;Playout Number : If small, it's quick to finish a self-play game but can't guarantee kifu's quality. On the contrary with more playout times, better kifu will get but also take longer time&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;future-work-can-try&quot;&gt;Future Work Can Try&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Continue to train (a larger board) and increase the playout number&lt;/li&gt;
&lt;li&gt;Try some other parameters for better performance&lt;/li&gt;
&lt;li&gt;Alter network structure&lt;/li&gt;
&lt;li&gt;Alter feature planes&lt;/li&gt;
&lt;li&gt;Implement APV-MCTS&lt;/li&gt;
&lt;li&gt;Train on standard/renju rule&lt;/li&gt;
&lt;/ul&gt;</description>
<pubDate>Fri, 14 Dec 2018 05:34:00 +0000</pubDate>
<dc:creator>initial_h</dc:creator>
<og:description>AlphaZero Gomoku MPI Link Github :</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/initial-h/p/10118871.html</dc:identifier>
</item>
<item>
<title>万恶之源 - Python包的应用 - Meets</title>
<link>http://www.cnblogs.com/guobaoyuan/p/10118636.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/guobaoyuan/p/10118636.html</guid>
<description>&lt;h3 class=&quot;md-end-block&quot;&gt;包的简介&lt;/h3&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span class=&quot;md-expand&quot;&gt;你们听到的包,可不是女同胞疯狂喜欢的那个包,我们来看看这个是啥包 &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;官方解释:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
Packages are a way of structuring Python’s module namespace by using “dotted module names”
包是一种通过使用‘.模块名’来组织python模块名称空间的方式。
​
#具体的：包就是一个包含有__init__.py文件的文件夹，所以其实我们创建包的目的就是为了用文件夹将文件/模块组织起来
​
#需要强调的是：
　　1. 在python3中，即使包下没有__init__.py文件，import 包仍然不会报错，而在python2中，包下一定要有该文件，否则import 包报错
​
　　2. 创建包的目的不是为了运行，而是被导入使用，记住，包只是模块的一种形式而已，包的本质就是一种模块
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;为什么要使用包呢? &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;包的本质就是一个文件夹，那么文件夹唯一的功能就是将文件组织起来 &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;随着功能越写越多，我们无法将所以功能都放到一个文件中，于是我们使用模块去组织功能，而随着模块越来越多，我们就需要用文件夹将模块文件组织起来，以此来提高程序的结构性和可维护性&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;注意事项&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#1.关于包相关的导入语句也分为import和from ... import ...两种，但是无论哪种，无论在什么位置，在导入时都必须遵循一个原则：凡是在导入时带点的，点的左边都必须是一个包，否则非法。可以带有一连串的点，如item.subitem.subsubitem,但都必须遵循这个原则。但对于导入后，在使用时就没有这种限制了，点的左边可以是包,模块，函数，类(它们都可以用点的方式调用自己的属性)。
​
#2、import导入文件时，产生名称空间中的名字来源于文件，import 包，产生的名称空间的名字同样来源于文件，即包下的__init__.py，导入包本质就是在导入该文件
​
#3、包A和包B下有同名模块也不会冲突，如A.a与B.a来自俩个命名空间
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;包的使用&lt;/span&gt;&lt;/h3&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;&lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;示例文件&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
glance/                   #Top-level package
​
├── __init__.py      #Initialize the glance package
​
├── api                  #Subpackage for api
​
│   ├── __init__.py
​
│   ├── policy.py
​
│   └── versions.py
​
├── cmd                #Subpackage for cmd
​
│   ├── __init__.py
​
│   └── manage.py
​
└── db                  #Subpackage for db
​
    ├── __init__.py
​
    └── models.py
    
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;span&gt;文件内容&lt;/span&gt;&lt;/h3&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#policy.py
def get():
    print('from policy.py')
​
#versions.py
def create_resource(conf):
    print('from version.py: ',conf)
​
#manage.py
def main():
    print('from manage.py')
​
#models.py
def register_models(engine):
    print('from models.py: ',engine)
​
包所包含的文件内容
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;执行文件与示范文件在同级目录下&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;包的使用之import&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
1 import glance.db.models
2 glance.db.models.register_models('mysql') 
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;单独导入包名称时不会导入包中所有包含的所有子模块，如&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#在与glance同级的test.py中
import glance
glance.cmd.manage.main()
​
'''
执行结果：
AttributeError: module 'glance' has no attribute 'cmd'
​
'''
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;解决方法：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
1 #glance/__init__.py
2 from . import cmd
3 
4 #glance/cmd/__init__.py
5 from . import manage
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block md-focus&quot;&gt;&lt;span class=&quot;md-expand&quot;&gt;执行：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
1 #在于glance同级的test.py中
2 import glance
3 glance.cmd.manage.main()
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;包的使用之from ... import ...&lt;/span&gt;&lt;/h3&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;需要注意的是from后import导入的模块，必须是明确的一个不能带点，否则会有语法错误，如：from a import b.c是错误语法&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
from glance.api import *
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;在讲模块时，我们已经讨论过了从一个模块内导入所有，此处我们研究从一个包导入所有。&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;此处是想从包api中导入所有，实际上该语句只会导入包api下&lt;span&gt;&lt;em&gt;_init**.py文件中定义的名字，我们可以在这个文件中定义&lt;/em&gt;&lt;span&gt;all**:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#在__init__.py中定义
x=10
​
def func():
    print('from api.__init.py')
​
__all__=['x','func','policy']
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;此时我们在于glance同级的文件中执行from glance.api import *就导入&lt;span&gt;&lt;strong&gt;all&lt;/strong&gt;&lt;span&gt;中的内容（versions仍然不能导入）。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#在__init__.py中定义
x=10
​
def func():
    print('from api.__init.py')
​
__all__=['x','func','policy']
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;此时我们在于glance同级的文件中执行from glance.api import *就导入&lt;span&gt;&lt;strong&gt;all&lt;/strong&gt;&lt;span&gt;中的内容（versions仍然不能导入）。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;练习：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#执行文件中的使用效果如下，请处理好包的导入
from glance import *
​
get()
create_resource('a.conf')
main()
register_models('mysql')
​
#在glance.__init__.py中
from .api.policy import get
from .api.versions import create_resource
​
from .cmd.manage import main
from .db.models import  register_models
​
__all__=['get','create_resource','main','register_models']
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;绝对导入和相对导入&lt;/span&gt;&lt;/h3&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;&lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;我们的最顶级包glance是写给别人用的，然后在glance包内部也会有彼此之间互相导入的需求，这时候就有绝对导入和相对导入两种方式：&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;绝对导入：以glance作为起始&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;相对导入：用.或者..的方式最为起始（只能在一个包中使用，不能用于不同目录内）&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;例如：我们在glance/api/version.py中想要导入glance/cmd/manage.py&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
1 在glance/api/version.py
2 
3 #绝对导入
4 from glance.cmd import manage
5 manage.main()
6 
7 #相对导入
8 from ..cmd import manage
9 manage.main()
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;测试结果：注意一定要在于glance同级的文件中测试&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
1 from glance.api import versions 
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;包以及包所包含的模块都是用来被导入的，而不是被直接执行的。而环境变量都是以执行文件为准的&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;比如我们想在glance/api/versions.py中导入glance/api/policy.py，有的同学一抽这俩模块是在同一个目录下，十分开心的就去做了，它直接这么做&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
1 #在version.py中
2 
3 import policy
4 policy.get()
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;没错，我们单独运行version.py是一点问题没有的，运行version.py的路径搜索就是从当前路径开始的，于是在导入policy时能在当前目录下找到&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;但是你想啊，你子包中的模块version.py极有可能是被一个glance包同一级别的其他文件导入，比如我们在于glance同级下的一个test.py文件中导入version.py，如下&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:false;&quot;&gt;
 1 from glance.api import versions
 2 
 3 '''
 4 执行结果:
 5 ImportError: No module named 'policy'
 6 '''
 7 
 8 '''
 9 分析:
10 此时我们导入versions在versions.py中执行
11 import policy需要找从sys.path也就是从当前目录找policy.py,
12 这必然是找不到的
13 '''
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;绝对导入与相对导入总结&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;45&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
绝对导入与相对导入
​
# 绝对导入: 以执行文件的sys.path为起始点开始导入,称之为绝对导入
#        优点: 执行文件与被导入的模块中都可以使用
#        缺点: 所有导入都是以sys.path为起始点,导入麻烦
​
# 相对导入: 参照当前所在文件的文件夹为起始开始查找,称之为相对导入
#        符号: .代表当前所在文件的文件加,..代表上一级文件夹,...代表上一级的上一级文件夹
#        优点: 导入更加简单
#        缺点: 只能在导入包中的模块时才能使用
　　　　  #注意:
　　　　　　　　1. 相对导入只能用于包内部模块之间的相互导入,导入者与被导入者都必须存在于一个包内
　　　　　　　　2. attempted relative import beyond top-level package # 试图在顶级包之外使用相对导入是错误的,言外之意,必须在顶级包内使用相对导入,每增加一个.代表跳到上一级文件夹,而上一级不应该超出顶级包
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;random模块&lt;/span&gt;&lt;/h3&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;61&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
&amp;gt;&amp;gt;&amp;gt; import random
#随机小数
&amp;gt;&amp;gt;&amp;gt; random.random()      # 大于0且小于1之间的小数
0.7664338663654585
&amp;gt;&amp;gt;&amp;gt; random.uniform(1,3) #大于1小于3的小数
1.6270147180533838
#恒富：发红包
​
#随机整数
&amp;gt;&amp;gt;&amp;gt; random.randint(1,5)  # 大于等于1且小于等于5之间的整数
&amp;gt;&amp;gt;&amp;gt; random.randrange(1,10,2) # 大于等于1且小于10之间的奇数
​
​
#随机选择一个返回
&amp;gt;&amp;gt;&amp;gt; random.choice([1,'23',[4,5]])  # #1或者23或者[4,5]
#随机选择多个返回，返回的个数为函数的第二个参数
&amp;gt;&amp;gt;&amp;gt; random.sample([1,'23',[4,5]],2) # #列表元素任意2个组合
[[4, 5], '23']
​
​
#打乱列表顺序
&amp;gt;&amp;gt;&amp;gt; item=[1,3,5,7,9]
&amp;gt;&amp;gt;&amp;gt; random.shuffle(item) # 打乱次序
&amp;gt;&amp;gt;&amp;gt; item
[5, 1, 3, 7, 9]
&amp;gt;&amp;gt;&amp;gt; random.shuffle(item)
&amp;gt;&amp;gt;&amp;gt; item
[5, 9, 7, 1, 3]
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;练习：生成随机验证码&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import random
​
def v_code():
​
    code = ''
    for i in range(5):
​
        num=random.randint(0,9)
        alf=chr(random.randint(65,90))
        add=random.choice([num,alf])
        code=&quot;&quot;.join([code,str(add)])
​
    return code
​
print(v_code())
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;打印进度条&lt;/span&gt;&lt;/h3&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;39&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
#=========知识储备==========
#进度条的效果
[#             ]
[##            ]
[###           ]
[####          ]
​
#指定宽度
print('[%-15s]' %'#')
print('[%-15s]' %'##')
print('[%-15s]' %'###')
print('[%-15s]' %'####')
​
#打印%
print('%s%%' %(100)) #第二个%号代表取消第一个%的特殊意义
​
#可传参来控制宽度
print('[%%-%ds]' %50) #[%-50s]
print(('[%%-%ds]' %50) %'#')
print(('[%%-%ds]' %50) %'##')
print(('[%%-%ds]' %50) %'###')
​
​
#=========实现打印进度条函数==========
import sys
import time
​
def progress(percent,width=50):
    if percent &amp;gt;= 1:
        percent=1
    show_str = ('%%-%ds' % width) % (int(width*percent)*'|')
    print('\r%s %d%%' %(show_str, int(100*percent)), end='')
​
​
#=========应用==========
data_size=1025
recv_size=0
while recv_size &amp;lt; data_size:
    time.sleep(0.1) #模拟数据的传输延迟
    recv_size+=1024 #每次收1024
​
    percent=recv_size/data_size #接收的比例
    progress(percent,width=70) #进度条的宽度70
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil&lt;/span&gt;&lt;/h3&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;&lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;shutil模块 高级的文件、文件夹、压缩包 处理模块&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.copyfileobj(fsrc, fdst[, length]) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;将文件内容拷贝到另一个文件中&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil
 
shutil.copyfileobj(open('old.xml','r'), open('new.xml', 'w'))
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.copyfile(src, dst) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;拷贝文件&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
shutil.copyfile('f1.log', 'f2.log') #目标文件无需存在
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.copymode(src, dst) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;仅拷贝权限。内容、组、用户均不变&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
shutil.copymode('f1.log', 'f2.log') #目标文件必须存在
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.copystat(src, dst) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;仅拷贝状态的信息，包括：mode bits, atime, mtime, flags&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
shutil.copystat('f1.log', 'f2.log') #目标文件必须存在
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.copy(src, dst) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;拷贝文件和权限&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;34&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil 
shutil.copy('f1.log', 'f2.log')
shutil.copy2(src, dst)
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;拷贝文件和状态信息&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil
 
shutil.copy2('f1.log', 'f2.log')
shutil.ignore_patterns(*patterns)
shutil.copytree(src, dst, symlinks=False, ignore=None)
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;递归的去拷贝文件夹&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;37&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil

shutil.copytree('folder1', 'folder2', ignore=shutil.ignore_patterns('*.pyc', 'tmp*')) #目标目录不能存在，注意对folder2目录父级目录要有可写权限，ignore的意思是排除
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;拷贝软连接&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil
​
shutil.copytree('f1', 'f2', symlinks=True, ignore=shutil.ignore_patterns('*.pyc', 'tmp*'))
​
'''
通常的拷贝都把软连接拷贝成硬链接，即对待软连接来说，创建新的文件
'''
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil.rmtree(path[, ignore_errors[, onerror]]) &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;递归的去删除文件&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;33&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil
 
shutil.rmtree('folder1')
shutil.move(src, dst)
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;递归的去移动文件，它类似mv命令，其实就是重命名。&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import shutil
  
shutil.move('folder1', 'folder3')
shutil.make_archive(base_name, format,...)
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;创建压缩包并返回文件路径，例如：zip、tar&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;41&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
base_name： 压缩包的文件名，也可以是压缩包的路径。只是文件名时，则保存至当前目录，否则保存至指定路径，
如 data_bak                       =&amp;gt;保存至当前路径
如：/tmp/data_bak =&amp;gt;保存至/tmp/
format： 压缩包种类，“zip”, “tar”, “bztar”，“gztar”
root_dir：   要压缩的文件夹路径（默认当前目录）
owner：  用户，默认当前用户
group：  组，默认当前组
logger： 用于记录日志，通常是logging.Logger对象
​
#将 /data 下的文件打包放置当前程序目录
import shutil
ret = shutil.make_archive(&quot;data_bak&quot;, 'gztar', root_dir='/data')
  
  
#将 /data下的文件打包放置 /tmp/目录
import shutil
ret = shutil.make_archive(&quot;/tmp/data_bak&quot;, 'gztar', root_dir='/data')
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;shutil 对压缩包的处理是调用 ZipFile 和 TarFile 两个模块来进行的，详细： &lt;span class=&quot;md-softbreak&quot;&gt;&lt;span&gt;zipfile压缩解压缩&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;35&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import zipfile

# 压缩
z = zipfile.ZipFile('laxi.zip', 'w')
z.write('a.log')
z.write('data.data')
z.close()

# 解压
z = zipfile.ZipFile('laxi.zip', 'r')
z.extractall(path='.')
z.close()
&lt;/pre&gt;&lt;/div&gt;
&lt;p class=&quot;md-end-block&quot;&gt;&lt;span&gt;tarfile压缩解压缩&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;38&quot;&gt;
&lt;pre class=&quot;brush:csharp;gutter:true;&quot;&gt;
import tarfile
​
# 压缩
&amp;gt;&amp;gt;&amp;gt; t=tarfile.open('/tmp/egon.tar','w')
&amp;gt;&amp;gt;&amp;gt; t.add('/test1/a.py',arcname='a.bak')
&amp;gt;&amp;gt;&amp;gt; t.add('/test1/b.py',arcname='b.bak')
&amp;gt;&amp;gt;&amp;gt; t.close()
​
​
# 解压
&amp;gt;&amp;gt;&amp;gt; t=tarfile.open('/tmp/egon.tar','r')
&amp;gt;&amp;gt;&amp;gt; t.extractall('/egon')
&amp;gt;&amp;gt;&amp;gt; t.close()
&lt;/pre&gt;&lt;/div&gt;

</description>
<pubDate>Fri, 14 Dec 2018 04:02:00 +0000</pubDate>
<dc:creator>Meets</dc:creator>
<og:description>包的简介 你们听到的包,可不是女同胞疯狂喜欢的那个包,我们来看看这个是啥包 官方解释: 为什么要使用包呢? 包的本质就是一个文件夹，那么文件夹唯一的功能就是将文件组织起来 随着功能越写越多，我们无法将</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/guobaoyuan/p/10118636.html</dc:identifier>
</item>
<item>
<title>分布式Session共享解决方案 - SimpleWu</title>
<link>http://www.cnblogs.com/SimpleWu/p/10118674.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/SimpleWu/p/10118674.html</guid>
<description>&lt;pre&gt;
&lt;code&gt;Author:SimpleWu&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;分布式Session一致性？&lt;/p&gt;
&lt;p&gt;说白了就是服务器集群Session共享的问题&lt;/p&gt;
&lt;p&gt;Session的作用？&lt;/p&gt;
&lt;p&gt;Session 是客户端与服务器通讯会话跟踪技术，服务器与客户端保持整个通讯的会话基本信息。&lt;/p&gt;
&lt;p&gt;客户端在第一次访问服务端的时候，服务端会响应一个sessionId并且将它存入到本地cookie中，在之后的访问会将cookie中的sessionId放入到请求头中去访问服务器，如果通过这个sessionid没有找到对应的数据那么服务器会创建一个新的sessionid并且响应给客户端。&lt;/p&gt;
&lt;h5 id=&quot;分布式session存在的问题&quot;&gt;分布式Session存在的问题？&lt;/h5&gt;
&lt;p&gt;假设第一次访问服务A生成一个sessionid并且存入cookie中，第二次却访问服务B客户端会在cookie中读取sessionid加入到请求头中，如果在服务B通过sessionid没有找到对应的数据那么它创建一个新的并且将sessionid返回给客户端,这样并不能共享我们的Session无法达到我们想要的目的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决方案：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;使用cookie来完成（很明显这种不安全的操作并不可靠）&lt;/li&gt;
&lt;li&gt;使用Nginx中的ip绑定策略，同一个ip只能在指定的同一个机器访问（不支持负载均衡）&lt;/li&gt;
&lt;li&gt;利用数据库同步session（效率不高）&lt;/li&gt;
&lt;li&gt;使用tomcat内置的session同步（同步可能会产生延迟）&lt;/li&gt;
&lt;li&gt;使用token代替session&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;我们使用spring-session以及集成好的解决方案，存放在redis中&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h5 id=&quot;目前项目中存在的问题&quot;&gt;目前项目中存在的问题&lt;/h5&gt;
&lt;p&gt;启动两个项目端口号分别为8080,8081。&lt;/p&gt;
&lt;p&gt;依赖：&lt;/p&gt;
&lt;pre class=&quot;xml&quot;&gt;
&lt;code&gt;&amp;lt;!--springboot父项目--&amp;gt;
&amp;lt;parent&amp;gt;
        &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;spring-boot-starter-parent&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;2.1.1.RELEASE&amp;lt;/version&amp;gt;
        &amp;lt;relativePath/&amp;gt; &amp;lt;!-- lookup parent from repository --&amp;gt;
&amp;lt;/parent&amp;gt;
&amp;lt;dependencies&amp;gt;
        &amp;lt;!--web依赖--&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;创建测试类：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;/**
 * Author: SimpleWu
 * date: 2018/12/14
 */
@RestController
public class TestSessionController {

    @Value(&quot;${server.port}&quot;)
    private Integer projectPort;// 项目端口

    @RequestMapping(&quot;/createSession&quot;)
    public String createSession(HttpSession session, String name) {
        session.setAttribute(&quot;name&quot;, name);
        return &quot;当前项目端口：&quot; + projectPort + &quot; 当前sessionId :&quot; + session.getId() + &quot;在Session中存入成功！&quot;;
    }

    @RequestMapping(&quot;/getSession&quot;)
    public String getSession(HttpSession session) {
        return &quot;当前项目端口：&quot; + projectPort + &quot; 当前sessionId :&quot; + session.getId() + &quot;  获取的姓名:&quot; + session.getAttribute(&quot;name&quot;);
    }

}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;yml配置：&lt;/p&gt;
&lt;pre class=&quot;yaml&quot;&gt;
&lt;code&gt;server:
  port: 8080 &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;修改映射文件&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;#将本机ip映射到www.hello.com上
127.0.0.1 www.hello.com&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在这里我们开启nginx集群，修改配置：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;#加入
#默认使用轮询,
upstream backserver{
        server 127.0.0.1:8080;
        server 127.0.0.1:8081;
}
#修改server中的local
location / {
            proxy_pass  http://backserver;
            index  index.html index.htm;
        }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;我们直接通过轮询机制来访问首先向Session中存入一个姓名，http://www.hello.com/createSession?name=SimpleWu&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;当前项目端口：8081 当前sessionId :0F20F73170AE6780B1EC06D9B06210DB在Session中存入成功！&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;因为我们使用的是默认的轮询机制那么下次肯定访问的是8080端口，我们直接获取以下刚才存入的值http://www.hello.com/getSession&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;当前项目端口：8080 当前sessionId :C6663EA93572FB8DAE27736A553EAB89 获取的姓名:null&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个时候发现8080端口中并没有我们存入的值，并且sessionId也是与8081端口中的不同。&lt;/p&gt;
&lt;p&gt;别急继续访问，因为轮询机制这个时候我们是8081端口的服务器，那么之前我们是在8081中存入了一个姓名。那么我们现在来访问以下看看是否能够获取到我们存入的姓名：SimpleWu,继续访问：http://www.hello.com/getSession&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;当前项目端口：8081 当前sessionId :005EE6198C30D7CD32FBD8B073531347 获取的姓名:null&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;8080端口我们没有存入连8081端口存入的都没有呢？&lt;/p&gt;
&lt;p&gt;我们仔细观察一下第三次访问8081的端口sessionid都不一样了，是因为我们在第二次去访问的时候访问的是8080端口这个时候客户端在cookie中获取8081的端口去8080服务器上去找，没有找到后重新创建了一个session并且将sessionid响应给客户端，客户端又保持到cookid中替换了之前8081的sessionid，那么第三次访问的时候拿着第二次访问的sessionid去找又找不到然后又创建。一直反复循环。&lt;/p&gt;
&lt;h5 id=&quot;如何解决这两个服务之间的共享问题呢&quot;&gt;如何解决这两个服务之间的共享问题呢？&lt;/h5&gt;
&lt;p&gt;spring已经给我们想好了问题并且已经提供出解决方案：spring-session 不了解的可以去百度了解下。&lt;/p&gt;
&lt;p&gt;我们首先打开redis并且在pom.xml中添加依赖：&lt;/p&gt;
&lt;pre class=&quot;xml&quot;&gt;
&lt;code&gt;&amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;com.alibaba&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;fastjson&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;1.2.47&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-boot-starter-data-redis&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;!--spring session 与redis应用基本环境配置,需要开启redis后才可以使用，不然启动Spring boot会报错 --&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.springframework.session&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;spring-session-data-redis&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.apache.commons&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;commons-pool2&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;
        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;redis.clients&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;jedis&amp;lt;/artifactId&amp;gt;
        &amp;lt;/dependency&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;修改yml配置文件:&lt;/p&gt;
&lt;pre class=&quot;yaml&quot;&gt;
&lt;code&gt;server:
  port: 8081
spring:
  redis:
    database: 0
    host: localhost
    port: 6379
    jedis:
      pool:
        max-active: 8
        max-wait: -1
        max-idle: 8
        min-idle: 0
    timeout: 10000
redis:
 hostname: localhost
 port: 6379
 #password: 123456&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;添加Session配置类&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;/**
 * Author: SimpleWu
 * date: 2018/12/14
 */
//这个类用配置redis服务器的连接
//maxInactiveIntervalInSeconds为SpringSession的过期时间（单位：秒）
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 1800)
public class SessionConfig {

    // 冒号后的值为没有配置文件时，制动装载的默认值
    @Value(&quot;${redis.hostname:localhost}&quot;)
    private String hostName;
    @Value(&quot;${redis.port:6379}&quot;)
    private int port;
   // @Value(&quot;${redis.password}&quot;)
   // private String password;

    @Bean
    public JedisConnectionFactory connectionFactory() {
        JedisConnectionFactory connection = new JedisConnectionFactory();
        connection.setPort(port);
        connection.setHostName(hostName);
        //connection.setPassword(password);
        // connection.setDatabase(0);
        return connection;
    }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;初始化Session配置&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;/**
 * Author: SimpleWu
 * date: 2018/12/14
 */
//初始化Session配置
public class SessionInitializer extends AbstractHttpSessionApplicationInitializer {
    public SessionInitializer() {
        super(SessionConfig.class);
    }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;然后我们继续启动8080,8081来进行测试：&lt;/p&gt;
&lt;p&gt;首先存入一个姓名http://www.hello.com/createSession?name=SimpleWu：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;当前项目端口：8080 当前sessionId :cf5c029a-2f90-4b7e-8345-bf61e0279254在Session中存入成功！&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;应该轮询机制那么下次一定是8081，竟然已经解决session共享问题了那么肯定能够获取到了，竟然这样那么我们直接来获取下姓名http://www.hello.com/getSession：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;当前项目端口：8081 当前sessionId :cf5c029a-2f90-4b7e-8345-bf61e0279254 获取的姓名:SimpleWu&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个时候我们发现不仅能够获取到值而且连sessionid都一致了。&lt;/p&gt;
&lt;p&gt;实现原理：&lt;/p&gt;
&lt;p&gt;就是当Web服务器接收到http请求后，当请求进入对应的Filter进行过滤，将原本需要由web服务器创建会话的过程转交给Spring-Session进行创建，本来创建的会话保存在Web服务器内存中，通过Spring-Session创建的会话信息可以保存第三方的服务中，如：redis,mysql等。Web服务器之间通过连接第三方服务来共享数据，实现Session共享！&lt;/p&gt;
</description>
<pubDate>Fri, 14 Dec 2018 04:00:00 +0000</pubDate>
<dc:creator>SimpleWu</dc:creator>
<og:description>分布式Session解决方案 分布式Session一致性？ 说白了就是服务器集群Session共享的问题 Session的作用？ Session 是客户端与服务器通讯会话跟踪技术，服务器与客户端保持整</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/SimpleWu/p/10118674.html</dc:identifier>
</item>
<item>
<title>ElasticSearch实战：Linux日志对接Kibana - 腾讯云+社区</title>
<link>http://www.cnblogs.com/qcloud1001/p/10118639.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/qcloud1001/p/10118639.html</guid>
<description>&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;本文由云+社区发表&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTFul web接口。ElasticSearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。ElasticSearch常用于全文检索，结构化检索，数据分析等。&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;下面，我们以ElasticSearch接管Linux日志（/var/log/xxx.log）为例，详细介绍如何进行配置与部署。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/bae5h3fio2.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;总体架构图&lt;/p&gt;

&lt;h2 id=&quot;cvm及elasticsearch&quot;&gt;&lt;strong&gt;1，CVM及ElasticSearch&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;在腾讯云帐号下，申请一台CVM（Linux操作系统）、一个ElasticSearch集群（后面简称ES），使用最简配置即可；申请的CVM和ES，必须在同一个VPC的同一个子网下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/080c94bgoz.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;CVM详情信息&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/gb5gbn3yv1.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;ElasticSearch详情信息&lt;/p&gt;
&lt;h2 id=&quot;filebeat工具&quot;&gt;&lt;strong&gt;2，Filebeat工具&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;为了将Linux日志提取到ES中，我们需要使用Filebeat工具。Filebeat是一个日志文件托运工具，在你的服务器上安装客户端后，Filebeat会监控日志目录或者指定的日志文件，追踪读取这些文件（追踪文件的变化，不停的读），并且转发这些信息到ElasticSearch或者logstarsh中存放。当你开启Filebeat程序的时候，它会启动一个或多个探测器（prospectors）去检测你指定的日志目录或文件，对于探测器找出的每一个日志文件，Filebeat启动收割进程（harvester），每一个收割进程读取一个日志文件的新内容，并发送这些新的日志数据到处理程序（spooler），处理程序会集合这些事件，最后Filebeat会发送集合的数据到你指定的地点。&lt;/p&gt;
&lt;p&gt;官网简介：https://www.elastic.co/products/beats/filebeat&lt;/p&gt;

&lt;h2 id=&quot;filebeat下载与安装&quot;&gt;&lt;strong&gt;1，Filebeat下载与安装&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;首先，登录待接管日志的CVM，在CVM上下载Filebeat工具：&lt;/p&gt;
&lt;pre class=&quot;bash&quot;&gt;
&lt;code&gt;[root@VM_3_7_centos ~]# cd /opt/
[root@VM_3_7_centos opt]# ll
total 4
drwxr-xr-x. 2 root root 4096 Sep  7  2017 rh
[root@VM_3_7_centos opt]# wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.2-x86_64.rpm
--2018-12-10 20:24:26--  https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.2.2-x86_64.rpm
Resolving artifacts.elastic.co (artifacts.elastic.co)... 107.21.202.15, 107.21.127.184, 54.225.214.74, ...
Connecting to artifacts.elastic.co (artifacts.elastic.co)|107.21.202.15|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 12697788 (12M) [binary/octet-stream]
Saving to: ‘filebeat-6.2.2-x86_64.rpm’

100%[=================================================================================================&amp;gt;] 12,697,788   160KB/s   in 1m 41s 

2018-12-10 20:26:08 (123 KB/s) - ‘filebeat-6.2.2-x86_64.rpm’ saved [12697788/12697788]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;然后，进行安装filebeat：&lt;/p&gt;
&lt;pre class=&quot;bash&quot;&gt;
&lt;code&gt;[root@VM_3_7_centos opt]# rpm -vi filebeat-6.2.2-x86_64.rpm
warning: filebeat-6.2.2-x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID d88e42b4: NOKEY
Preparing packages...
filebeat-6.2.2-1.x86_64
[root@VM_3_7_centos opt]#&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;至此，Filebeat安装完成。&lt;/p&gt;
&lt;h2 id=&quot;filebeat配置&quot;&gt;&lt;strong&gt;2，Filebeat配置&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;进入Filebeat配置文件目录：/etc/filebeat/&lt;/p&gt;
&lt;pre class=&quot;bash&quot;&gt;
&lt;code&gt;[root@VM_3_7_centos opt]# cd /etc/filebeat/
[root@VM_3_7_centos filebeat]# ll
total 108
-rw-r--r-- 1 root root 44384 Feb 17  2018 fields.yml
-rw-r----- 1 root root 52193 Feb 17  2018 filebeat.reference.yml
-rw------- 1 root root  7264 Feb 17  2018 filebeat.yml
drwxr-xr-x 2 root root  4096 Dec 10 20:35 modules.d
[root@VM_3_7_centos filebeat]#&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中，filebeat.yml就是我们需要修改的配置文件。建议修改配置前，先备份此文件。&lt;/p&gt;
&lt;p&gt;然后，确认需要对接ElasticSearch的Linux的日志目录，我们以下图（&lt;strong&gt;/var/log/secure&lt;/strong&gt;）为例。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/dvacjpavr4.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;/var/log/secure日志文件&lt;/p&gt;
&lt;p&gt;使用vim打开/etc/filebeat/filebeat.yml文件，修改其中的：&lt;/p&gt;
&lt;p&gt;1）Filebeat prospectors类目中，enable默认为false，我们要改为&lt;strong&gt;true&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;2）paths，默认为/var/log/*.log，我们要改为待接管的日志路径：&lt;strong&gt;/var/log/secure&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3）Outputs类目中，有ElasticSearchoutput配置，其中hosts默认为&quot;localhost:9200&quot;，需要我们手工修改为上面申请的ES子网地址和端口，即&lt;strong&gt;&quot;10.0.3.8:9200&quot;&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;修改好上述内容后，保存退出。&lt;/p&gt;
&lt;p&gt;修改好的配置文件全文如下：&lt;/p&gt;
&lt;pre class=&quot;bash&quot;&gt;
&lt;code&gt;[root@VM_3_7_centos /]# vim /etc/filebeat/filebeat.yml
[root@VM_3_7_centos /]# cat /etc/filebeat/filebeat.yml
###################### Filebeat Configuration Example #########################

# This file is an example configuration file highlighting only the most common
# options. The filebeat.reference.yml file from the same directory contains all the
# supported options with more comments. You can use it as a reference.
#
# You can find the full configuration reference here:
# https://www.elastic.co/guide/en/beats/filebeat/index.html

# For more available modules and options, please see the filebeat.reference.yml sample
# configuration file.

#=========================== Filebeat prospectors =============================

filebeat.prospectors:

# Each - is a prospector. Most options can be set at the prospector level, so
# you can use different prospectors for various configurations.
# Below are the prospector specific configurations.

- type: log

  # Change to true to enable this prospector configuration.
  enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/secure
    #- c:\programdata\elasticsearch\logs\*

  # Exclude lines. A list of regular expressions to match. It drops the lines that are
  # matching any regular expression from the list.
  #exclude_lines: ['^DBG']

  # Include lines. A list of regular expressions to match. It exports the lines that are
  # matching any regular expression from the list.
  #include_lines: ['^ERR', '^WARN']

  # Exclude files. A list of regular expressions to match. Filebeat drops the files that
  # are matching any regular expression from the list. By default, no files are dropped.
  #exclude_files: ['.gz$']

  # Optional additional fields. These fields can be freely picked
  # to add additional information to the crawled log files for filtering
  #fields:
  #  level: debug
  #  review: 1

  ### Multiline options

  # Mutiline can be used for log messages spanning multiple lines. This is common
  # for Java Stack Traces or C-Line Continuation

  # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
  #multiline.pattern: ^\[

  # Defines if the pattern set under pattern should be negated or not. Default is false.
  #multiline.negate: false

  # Match can be set to &quot;after&quot; or &quot;before&quot;. It is used to define if lines should be append to a pattern
  # that was (not) matched before or after or as long as a pattern is not matched based on negate.
  # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
  #multiline.match: after


#============================= Filebeat modules ===============================

filebeat.config.modules:
  # Glob pattern for configuration loading
  path: ${path.config}/modules.d/*.yml

  # Set to true to enable config reloading
  reload.enabled: false

  # Period on which files under path should be checked for changes
  #reload.period: 10s

#==================== Elasticsearch template setting ==========================

setup.template.settings:
  index.number_of_shards: 3
  #index.codec: best_compression
  #_source.enabled: false

#================================ General =====================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their own field with each
# transaction published.
#tags: [&quot;service-X&quot;, &quot;web-tier&quot;]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging


#============================== Dashboards =====================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here, or by using the `-setup` CLI flag or the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

#============================== Kibana =====================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: &quot;localhost:5601&quot;

#============================= Elastic Cloud ==================================

# These settings simplify using filebeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `&amp;lt;user&amp;gt;:&amp;lt;pass&amp;gt;`.
#cloud.auth:

#================================ Outputs =====================================

# Configure what output to use when sending the data collected by the beat.

#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: [&quot;10.0.3.8:9200&quot;]

  # Optional protocol and basic auth credentials.
  #protocol: &quot;https&quot;
  #username: &quot;elastic&quot;
  #password: &quot;changeme&quot;

#----------------------------- Logstash output --------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: [&quot;localhost:5044&quot;]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;]

  # Certificate for SSL client authentication
  #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot;

  # Client Certificate Key
  #ssl.key: &quot;/etc/pki/client/cert.key&quot;

#================================ Logging =====================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use [&quot;*&quot;]. Examples of other selectors are &quot;beat&quot;,
# &quot;publish&quot;, &quot;service&quot;.
#logging.selectors: [&quot;*&quot;]

#============================== Xpack Monitoring ===============================
# filebeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#xpack.monitoring.enabled: false

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well. Any setting that is not set is
# automatically inherited from the Elasticsearch output configuration, so if you
# have the Elasticsearch output configured, you can simply uncomment the
# following line.
#xpack.monitoring.elasticsearch:
[root@VM_3_7_centos /]# &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;执行下列命令启动filebeat&lt;/p&gt;
&lt;pre class=&quot;js&quot;&gt;
&lt;code&gt;[root@VM_3_7_centos /]# sudo /etc/init.d/filebeat start
Starting filebeat (via systemctl):                         [  OK  ]
[root@VM_3_7_centos /]# &lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;kibana配置&quot;&gt;&lt;strong&gt;3，Kibana配置&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;进入ElasticSearch对应的Kibana管理页，如下图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/qkzxcnxq76.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;首次访问Kibana默认会显示管理页&lt;/p&gt;
&lt;p&gt;首次登陆，会默认进入Management页面，我们需要将Index pattern内容修改为：filebeat-*，然后页面会自动填充&lt;strong&gt;Time Filter field name，&lt;/strong&gt;不需手动设置，直接点击Create即可。点击Create后，页面需要一定时间来加载配置和数据，请稍等。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/kjuqr2xc6f.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;将Index pattern内容修改为：filebeat-*，然后点击Create&lt;/p&gt;
&lt;p&gt;至此，CVM上，/var/log/secure日志文件，已对接到ElasticSearch中，历史日志可以通过Kibana进行查询，最新产生的日志也会实时同步到Kibana中。&lt;/p&gt;

&lt;p&gt;日志接管已完成配置，如何使用呢？&lt;/p&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/3k6w45klpc.png?imageView2/2/w/1620&quot; alt=&quot;img&quot;/&gt;在Index Patterns中可以看到我们配置过的filebeat-*&lt;/p&gt;
&lt;p&gt;点击Discover，即可看到secure中的所有日志，页面上方的搜索框中输入关键字，即可完成日志的检索。如下图（点击图片，可查看高清大图）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/draft/2276073/1nnrbwmuxy.gif&quot; alt=&quot;img&quot;/&gt;使用Kibana进行日志检索&lt;/p&gt;
&lt;p&gt;实际上，检索只是Kibana提供的诸多功能之一，还有其他功能如可视化、分词检索等，还有待后续研究。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;此文已由作者授权腾讯云+社区发布&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;搜索关注公众号「云加社区」，第一时间获取技术干货，关注后回复1024 送你一份技术课程大礼包！&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 14 Dec 2018 03:53:00 +0000</pubDate>
<dc:creator>腾讯云+社区</dc:creator>
<og:description>本文由云+社区发表 ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTFul web接口。ElasticSearch是用Java开发</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/qcloud1001/p/10118639.html</dc:identifier>
</item>
<item>
<title>Kafka提交offset机制 - Harvard_Fly</title>
<link>http://www.cnblogs.com/FG123/p/10091599.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/FG123/p/10091599.html</guid>
<description>&lt;p&gt;&lt;span&gt;在kafka的消费者中，有一个非常关键的机制，那就是offset机制。它使得Kafka在消费的过程中即使挂了或者引发再均衡问题重新分配Partation，当下次重新恢复消费时仍然可以知道从哪里开始消费。它好比看一本书中的书签标记，每次通过书签标记(offset)就能快速找到该从哪里开始看(消费)。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Kafka对于offset的处理有两种提交方式：(1) 自动提交(默认的提交方式)   (2) 手动提交(可以灵活地控制offset)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;(1) 自动提交偏移量:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Kafka中偏移量的自动提交是由参数&lt;/span&gt;&lt;span&gt;enable_auto_commit和&lt;/span&gt;&lt;span&gt;auto_commit_interval_ms&lt;/span&gt;&lt;span&gt;控制的，当enable_auto_commit=True时，Kafka在消费的过程中会以频率为auto_commit_interval_ms向Kafka自带的topic&lt;/span&gt;&lt;span&gt;(__consumer_offsets)进行偏移量提交，具体提交到哪个Partation是以算法：partation=&lt;/span&gt;&lt;span&gt;hash(group_id)%50来计算的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如：group_id=&lt;/span&gt;&lt;span&gt;test_group_1，则partation=&lt;/span&gt;&lt;span&gt;hash(&quot;test_group_1&quot;)%50=28&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;自动提交偏移量示例：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;55&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; uuid
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;from&lt;/span&gt; kafka &lt;span&gt;import&lt;/span&gt;&lt;span&gt; KafkaConsumer
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; consumer =&lt;span&gt; KafkaConsumer(
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     bootstrap_servers=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;192.168.33.11:9092&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;],
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;     group_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;test_group_1&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;,
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;     client_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;{}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(str(uuid.uuid4())),
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;     max_poll_records=500&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;     enable_auto_commit=True,  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 默认为True 表示自动提交偏移量&lt;/span&gt;
&lt;span&gt;11&lt;/span&gt;     auto_commit_interval_ms=100,  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 控制自动提交偏移量的频率 单位ms 默认是5000ms&lt;/span&gt;
&lt;span&gt;12&lt;/span&gt;     key_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; k: pickle.loads(k),
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;     value_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; v: pickle.loads(v)
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt; &lt;span&gt;)
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; 
&lt;span&gt;16&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 订阅消费round_topic这个主题&lt;/span&gt;
&lt;span&gt;17&lt;/span&gt; consumer.subscribe(topics=(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;round_topic&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,))
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; 
&lt;span&gt;19&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;     &lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;         consumer_records_dict = consumer.poll(timeout_ms=1000&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; consumer.assignment()可以获取每个分区的offset&lt;/span&gt;
&lt;span&gt;24&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; partition &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer.assignment():
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;主题:{} 分区:{},需要从下面的offset开始消费:{}&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;.format(
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; &lt;span&gt;                str(partition.topic),
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;                str(partition.partition),
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; &lt;span&gt;                consumer.position(partition)
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt; &lt;span&gt;            ))
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt; 
&lt;span&gt;31&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 处理逻辑.&lt;/span&gt;
&lt;span&gt;32&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;&lt;span&gt;(k)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt;                 &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;topic = {},partition = {},offset = {},key = {},value = {}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; &lt;span&gt;                    record.topic, record.partition, record.offset, record.key, record.value)
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt; &lt;span&gt;                )
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt; 
&lt;span&gt;39&lt;/span&gt; &lt;span&gt;finally&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;     &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 调用close方法的时候会触发偏移量的自动提交 close默认autocommit=True&lt;/span&gt;
&lt;span&gt;41&lt;/span&gt;     consumer.close()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt; 返回结果：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/739231/201812/739231-20181213180630107-297898732.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在上述代码中，最后调用&lt;/span&gt;&lt;span&gt;consumer.close()时候也会触发自动提交，因为它默认&lt;/span&gt;&lt;span&gt;autocommit=True，&lt;/span&gt;&lt;span&gt;源码如下：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 1&lt;/span&gt;     &lt;span&gt;def&lt;/span&gt; close(self, autocommit=&lt;span&gt;True):
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt;         &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Close the consumer, waiting indefinitely for any needed cleanup.
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; 
&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;        Keyword Arguments:
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;            autocommit (bool): If auto-commit is configured for this consumer,
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;                this optional flag causes the consumer to attempt to commit any
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;                pending consumed offsets prior to close. Default: True
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;         &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 9&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt;&lt;span&gt; self._closed:
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;             &lt;span&gt;return&lt;/span&gt;
&lt;span&gt;11&lt;/span&gt;         log.debug(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Closing the KafkaConsumer.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;         self._closed =&lt;span&gt; True
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;         self._coordinator.close(autocommit=&lt;span&gt;autocommit)
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt; &lt;span&gt;        self._metrics.close()
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;        self._client.close()
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;         &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;             self.config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;key_deserializer&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].close()
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         &lt;span&gt;except&lt;/span&gt;&lt;span&gt; AttributeError:
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt;             &lt;span&gt;pass&lt;/span&gt;
&lt;span&gt;20&lt;/span&gt;         &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt;             self.config[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;value_deserializer&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].close()
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;         &lt;span&gt;except&lt;/span&gt;&lt;span&gt; AttributeError:
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;             &lt;span&gt;pass&lt;/span&gt;
&lt;span&gt;24&lt;/span&gt;         log.debug(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;The KafkaConsumer has closed.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;span&gt;对于自动提交偏移量，如果&lt;/span&gt;&lt;span&gt;auto_commit_interval_ms的值设置的过大，当&lt;/span&gt;&lt;span&gt;消费者在自动提交偏移量之前异常退出，将导致kafka未提交偏移量，进而出现重复消费的问题，所以建议auto_commit_interval_ms的值越小越好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;(2) 手动提交偏移量:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;鉴于Kafka自动提交offset的不灵活性和不精确性(只能是按指定频率的提交)，Kafka提供了手动提交offset策略。手动提交能对偏移量更加灵活精准地控制，以保证消息不被重复消费以及消息不被丢失。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于手动提交offset主要有3种方式：1.同步提交  2.异步提交  3.异步+同步 组合的方式提交&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 1.同步手动提交偏移量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;同步模式下提交失败的时候一直尝试提交，直到遇到无法重试的情况下才会结束，同时同步方式下消费者线程在拉取消息会被阻塞，在broker对提交的请求做出响应之前，会一直阻塞直到偏移量提交操作成功或者在提交过程中发生异常，限制了消息的吞吐量。&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;52&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;同步的方式10W条消息  4.58s
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; uuid
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; time
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;from&lt;/span&gt; kafka &lt;span&gt;import&lt;/span&gt;&lt;span&gt; KafkaConsumer
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; consumer =&lt;span&gt; KafkaConsumer(
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     bootstrap_servers=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;192.168.33.11:9092&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;],
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     group_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;test_group_1&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;     client_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;{}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(str(uuid.uuid4())),
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;     enable_auto_commit=False,  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 设置为手动提交偏移量.&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt;     key_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; k: pickle.loads(k),
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     value_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; v: pickle.loads(v)
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; &lt;span&gt;)
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; 
&lt;span&gt;19&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 订阅消费round_topic这个主题&lt;/span&gt;
&lt;span&gt;20&lt;/span&gt; consumer.subscribe(topics=(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;round_topic&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,))
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; 
&lt;span&gt;22&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;     start_time =&lt;span&gt; time.time()
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;     &lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         consumer_records_dict = consumer.poll(timeout_ms=100)  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 在轮询中等待的毫秒数&lt;/span&gt;
&lt;span&gt;26&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;获取下一轮&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; 
&lt;span&gt;28&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; key, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;                 record_num += 1
&lt;span&gt;32&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;----&amp;gt;当前批次获取到的消息个数是:{}&amp;lt;----&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(record_num))
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; 
&lt;span&gt;35&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;                 &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;topic = {},partition = {},offset = {},key = {},value = {}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt; &lt;span&gt;                    record.topic, record.partition, record.offset, record.key, record.value)
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt; &lt;span&gt;                )
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt;         &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;             &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 轮询一个batch 手动提交一次&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt;             consumer.commit()  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 提交当前批次最新的偏移量. 会阻塞  执行完后才会下一轮poll&lt;/span&gt;
&lt;span&gt;44&lt;/span&gt;             end_time =&lt;span&gt; time.time()
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt;             time_counts = end_time -&lt;span&gt; start_time
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;&lt;span&gt;(time_counts)
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt;         &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception as e:
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;commit failed&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, str(e))
&lt;/span&gt;&lt;span&gt;49&lt;/span&gt; 
&lt;span&gt;50&lt;/span&gt; &lt;span&gt;finally&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt;     consumer.close()  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 手动提交中close对偏移量提交没有影响&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/739231/201812/739231-20181214095625576-61103101.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;从上述可以看出，每轮循一个批次，手动提交一次，只有当前批次的消息提交完成时才会触发poll来获取下一轮的消息，经测试10W条消息耗时4.58s&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 2.异步手动提交偏移量+回调函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 异步手动提交offset时，消费者线程不会阻塞，提交失败的时候也不会进行重试，并且可以配合回调函数在broker做出响应的时候记录错误信息。&lt;/span&gt;&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;53&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;异步的方式手动提交偏移量(异步+回调函数的模式) 10W条消息 3.09s
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; uuid
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; time
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;from&lt;/span&gt; kafka &lt;span&gt;import&lt;/span&gt;&lt;span&gt; KafkaConsumer
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; consumer =&lt;span&gt; KafkaConsumer(
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     bootstrap_servers=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;192.168.33.11:9092&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;],
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     group_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;test_group_1&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;     client_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;{}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(str(uuid.uuid4())),
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;     enable_auto_commit=False,  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 设置为手动提交偏移量.&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt;     key_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; k: pickle.loads(k),
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     value_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; v: pickle.loads(v)
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; &lt;span&gt;)
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; 
&lt;span&gt;19&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 订阅消费round_topic这个主题&lt;/span&gt;
&lt;span&gt;20&lt;/span&gt; consumer.subscribe(topics=(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;round_topic&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,))
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; 
&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; _on_send_response(*args, **&lt;span&gt;kwargs):
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;25&lt;/span&gt; &lt;span&gt;    提交偏移量涉及回调函数
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; &lt;span&gt;    :param args: args[0] --&amp;gt; {TopicPartition:OffsetAndMetadata}  args[1] --&amp;gt; Exception
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;    :param kwargs:
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; &lt;span&gt;    :return:
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;30&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; isinstance(args[1&lt;span&gt;], Exception):
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;偏移量提交异常. {}&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;.format(args[1&lt;span&gt;]))
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;     &lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;偏移量提交成功&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; 
&lt;span&gt;35&lt;/span&gt; 
&lt;span&gt;36&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;     start_time =&lt;span&gt; time.time()
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;     &lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;         consumer_records_dict = consumer.poll(timeout_ms=10&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; key, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt;                 record_num += 1
&lt;span&gt;45&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;当前批次获取到的消息个数是:{}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(record_num))
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt; 
&lt;span&gt;47&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.values():
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;49&lt;/span&gt;                 &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;topic = {},partition = {},offset = {},key = {},value = {}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt; &lt;span&gt;                    record.topic, record.partition, record.offset, record.key, record.value))
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt; 
&lt;span&gt;52&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 避免频繁提交&lt;/span&gt;
&lt;span&gt;53&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt; record_num !=&lt;span&gt; 0:
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt;             &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;55&lt;/span&gt;                 consumer.commit_async(callback=&lt;span&gt;_on_send_response)
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;             &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception as e:
&lt;/span&gt;&lt;span&gt;57&lt;/span&gt;                 &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;commit failed&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, str(e))
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt; 
&lt;span&gt;59&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt; 
&lt;span&gt;61&lt;/span&gt; &lt;span&gt;finally&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt;     consumer.close()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/739231/201812/739231-20181214103751541-482671348.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于args参数：args[0]是一个dict，key是&lt;/span&gt;&lt;span&gt;TopicPartition，value是OffsetAndMetadata，表示该主题下的partition对应的offset；args[1]在提交成功是True，提交失败时是一个Exception类。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于异步提交，由于不会进行失败重试，当消费者异常关闭或者触发了再均衡前，如果偏移量还未提交就会造成偏移量丢失。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 3.异步+同步 组合的方式提交偏移量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;针对异步提交偏移量丢失的问题，通过对消费者进行异步批次提交并且在关闭时同步提交的方式，&lt;/span&gt;&lt;span&gt;这样即使上一次的异步提交失败，通过同步提交还能够进行补救，同步会一直重试，直到提交成功。&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;56&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;同步和异步组合的方式提交偏移量
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; pickle
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; uuid
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; time
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;from&lt;/span&gt; kafka &lt;span&gt;import&lt;/span&gt;&lt;span&gt; KafkaConsumer
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; 
&lt;span&gt;10&lt;/span&gt; consumer =&lt;span&gt; KafkaConsumer(
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     bootstrap_servers=[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;192.168.33.11:9092&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;],
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     group_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;test_group_1&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;,
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt;     client_id=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;{}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(str(uuid.uuid4())),
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;     enable_auto_commit=False,  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 设置为手动提交偏移量.&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt;     key_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; k: pickle.loads(k),
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     value_deserializer=&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; v: pickle.loads(v)
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt; &lt;span&gt;)
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt; 
&lt;span&gt;19&lt;/span&gt; &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 订阅消费round_topic这个主题&lt;/span&gt;
&lt;span&gt;20&lt;/span&gt; consumer.subscribe(topics=(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;round_topic&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;,))
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; 
&lt;span&gt;22&lt;/span&gt; 
&lt;span&gt;23&lt;/span&gt; &lt;span&gt;def&lt;/span&gt; _on_send_response(*args, **&lt;span&gt;kwargs):
&lt;/span&gt;&lt;span&gt;24&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;25&lt;/span&gt; &lt;span&gt;    提交偏移量涉及的回调函数
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt; &lt;span&gt;    :param args:
&lt;/span&gt;&lt;span&gt;27&lt;/span&gt; &lt;span&gt;    :param kwargs:
&lt;/span&gt;&lt;span&gt;28&lt;/span&gt; &lt;span&gt;    :return:
&lt;/span&gt;&lt;span&gt;29&lt;/span&gt;     &lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span&gt;30&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; isinstance(args[1&lt;span&gt;], Exception):
&lt;/span&gt;&lt;span&gt;31&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;偏移量提交异常. {}&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;.format(args[1&lt;span&gt;]))
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;     &lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;偏移量提交成功&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; 
&lt;span&gt;35&lt;/span&gt; 
&lt;span&gt;36&lt;/span&gt; &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;     start_time =&lt;span&gt; time.time()
&lt;/span&gt;&lt;span&gt;38&lt;/span&gt;     &lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
&lt;/span&gt;&lt;span&gt;39&lt;/span&gt;         consumer_records_dict = consumer.poll(timeout_ms=100&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt; 
&lt;span&gt;41&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; key, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;43&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt;                 record_num += 1
&lt;span&gt;45&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;----&amp;gt;当前批次获取到的消息个数是:&amp;lt;----&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(record_num))
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;         record_num =&lt;span&gt; 0
&lt;/span&gt;&lt;span&gt;47&lt;/span&gt; 
&lt;span&gt;48&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt; k, record_list &lt;span&gt;in&lt;/span&gt;&lt;span&gt; consumer_records_dict.items():
&lt;/span&gt;&lt;span&gt;49&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;&lt;span&gt;(k)
&lt;/span&gt;&lt;span&gt;50&lt;/span&gt;             &lt;span&gt;for&lt;/span&gt; record &lt;span&gt;in&lt;/span&gt;&lt;span&gt; record_list:
&lt;/span&gt;&lt;span&gt;51&lt;/span&gt;                 &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;topic = {},partition = {},offset = {},key = {},value = {}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(
&lt;/span&gt;&lt;span&gt;52&lt;/span&gt; &lt;span&gt;                    record.topic, record.partition, record.offset, record.key, record.value)
&lt;/span&gt;&lt;span&gt;53&lt;/span&gt; &lt;span&gt;                )
&lt;/span&gt;&lt;span&gt;54&lt;/span&gt; 
&lt;span&gt;55&lt;/span&gt;         &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;56&lt;/span&gt;             &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 轮询一个batch 手动提交一次&lt;/span&gt;
&lt;span&gt;57&lt;/span&gt;             consumer.commit_async(callback=&lt;span&gt;_on_send_response)
&lt;/span&gt;&lt;span&gt;58&lt;/span&gt;             end_time =&lt;span&gt; time.time()
&lt;/span&gt;&lt;span&gt;59&lt;/span&gt;             time_counts = end_time -&lt;span&gt; start_time
&lt;/span&gt;&lt;span&gt;60&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;&lt;span&gt;(time_counts)
&lt;/span&gt;&lt;span&gt;61&lt;/span&gt;         &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception as e:
&lt;/span&gt;&lt;span&gt;62&lt;/span&gt;             &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;commit failed&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;, str(e))
&lt;/span&gt;&lt;span&gt;63&lt;/span&gt; 
&lt;span&gt;64&lt;/span&gt; &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception as e:
&lt;/span&gt;&lt;span&gt;65&lt;/span&gt;     &lt;span&gt;print&lt;/span&gt;&lt;span&gt;(str(e))
&lt;/span&gt;&lt;span&gt;66&lt;/span&gt; &lt;span&gt;finally&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;67&lt;/span&gt;     &lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
&lt;/span&gt;&lt;span&gt;68&lt;/span&gt;         &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 同步提交偏移量,在消费者异常退出的时候再次提交偏移量,确保偏移量的提交.&lt;/span&gt;
&lt;span&gt;69&lt;/span&gt; &lt;span&gt;        consumer.commit()
&lt;/span&gt;&lt;span&gt;70&lt;/span&gt;         &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;同步补救提交成功&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;71&lt;/span&gt;     &lt;span&gt;except&lt;/span&gt;&lt;span&gt; Exception as e:
&lt;/span&gt;&lt;span&gt;72&lt;/span&gt;         consumer.close()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;通过finally在最后不管是否异常都会触发consumer.commit()来同步补救一次，确保偏移量不会丢失&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 14 Dec 2018 03:51:00 +0000</pubDate>
<dc:creator>Harvard_Fly</dc:creator>
<og:description>在kafka的消费者中，有一个非常关键的机制，那就是offset机制。它使得Kafka在消费的过程中即使挂了或者引发再均衡问题重新分配Partation，当下次重新恢复消费时仍然可以知道从哪里开始消费</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/FG123/p/10091599.html</dc:identifier>
</item>
</channel>
</rss>