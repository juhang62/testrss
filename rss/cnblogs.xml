<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>面试官：对象可能会迟到，但它永远不会缺席 - 山禾说</title>
<link>http://www.cnblogs.com/viyoung/p/13515711.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/viyoung/p/13515711.html</guid>
<description>&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题一：简单聊一下关于你对&lt;code&gt;Object&lt;/code&gt;的理解&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在 Java 中，只有基本数据类型不是对象，比如，数值，布尔和字符类型的值都不是对象。而其余的数据类型都是继承自一个名为&lt;code&gt;Object&lt;/code&gt;的类，这个类是所有类的始祖，每个类都是由&lt;code&gt;Object&lt;/code&gt;类扩展而来。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;如果一个类继承自&lt;code&gt;Object&lt;/code&gt;类，我们可以将&lt;code&gt;extends Object&lt;/code&gt;给省略掉，如果在一个类的定义中没有明确的指出哪个是它的父类，那么&lt;code&gt;Object&lt;/code&gt;类就认为是这个类的父类。&lt;/p&gt;
&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/2020-08-14-123909.jpg&quot; alt=&quot;=&quot;/&gt;=
&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题二：&lt;code&gt;Object&lt;/code&gt;类中有一个&lt;code&gt;registerNatives&lt;/code&gt;方法，对此你了解多少？&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;从方法的命名上我们就可以看出，该方法是用于注册本地（native）方法，主要是为了服务于JNI的，它主要是提供了 java 类中的方法与对应 C++ 代码中的方法的&lt;strong&gt;映射&lt;/strong&gt;，方便jvm去查找调用 C++ 中的方法。&lt;/p&gt;
&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/blog/2020-08-15-033936.png&quot; alt=&quot;img&quot;/&gt;img
&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题三：&lt;code&gt;Object&lt;/code&gt;类中有&lt;code&gt;clone&lt;/code&gt;方法，聊聊你对这个方法的认识&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;clone&lt;/code&gt;方法是&lt;code&gt;Object&lt;/code&gt;类的一个&lt;code&gt;protected&lt;/code&gt;的方法，我们可以这样去应用这个方法&lt;/p&gt;
&lt;ol data-tool=&quot;mdnice编辑器&quot;&gt;&lt;li&gt;
&lt;section&gt;&lt;p&gt;实现&lt;code&gt;Cloneable&lt;/code&gt;接口&lt;/p&gt;
&lt;/section&gt;&lt;/li&gt;
&lt;li readability=&quot;-2&quot;&gt;
&lt;section readability=&quot;2&quot;&gt;&lt;p&gt;重写&lt;code&gt;clone&lt;/code&gt;方法，并指定&lt;code&gt;public&lt;/code&gt;修饰符。&lt;/p&gt;
&lt;/section&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/blog/2020-08-16-032428.png&quot; alt=&quot;&quot;/&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题四：为什么我们一定要去实现&lt;code&gt;Cloneable&lt;/code&gt;接口，而不是直接去重写这个方法呢？&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们通过源码可以发现这是一个空的接口，&lt;code&gt;clone&lt;/code&gt;是从&lt;code&gt;Object&lt;/code&gt;类继承的。这个接口只是作为一个标记，指示类设计者了解克隆继承。对象对于克隆也很&quot;偏执&quot;，如果一个对象请求克隆，但没有实现这个接口，就会生成一个异常。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在 Java 中，&lt;code&gt;Cloneable&lt;/code&gt;这样的接口叫做标记接口，标记接口不包括任何方法，它的唯一作用就是允许在类型查询的时候使用&lt;code&gt;instanceof&lt;/code&gt;：&lt;/p&gt;
&lt;section class=&quot;code-snippet__fix code-snippet__js&quot; data-tool=&quot;mdnice编辑器&quot; readability=&quot;2&quot;&gt;&lt;ul class=&quot;code-snippet__line-index code-snippet__js&quot;&gt;&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;Java&quot;&gt;
&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;if (obj instanceof Cloneable){&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    //TODO&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/section&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题五：说一说你对关于深克隆和浅克隆的认识&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;首先来说一下&lt;code&gt;Object&lt;/code&gt;类是如何实现&lt;code&gt;clone&lt;/code&gt;，它对这个对象一无所知，所以只能逐个域的进行拷贝。如果对象中的所有数据域都是数值或其他基本类型，拷贝这些域没有任何问题，但是如果对象中包含子对象的引用，拷贝域就会得到相同子对象的另一个引用，这样一来，原对象和克隆对象仍然会去共享一些信息。这种&lt;code&gt;Object&lt;/code&gt;类默认实现的&lt;code&gt;clone&lt;/code&gt;方法称为&lt;strong&gt;浅拷贝&lt;/strong&gt;（Shallow Clone）。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里需要注意，关于浅克隆的安全性，如果原对象和浅克隆对象共享的子对象是不可变的，那么这种共享就是安全的。如果子对象属于一个不可变的类，如&lt;code&gt;String&lt;/code&gt;，就是这种情况。或者在对象的生命期中，子对象一直包含不变的常量 ，没有更改器方法会改变它，也没有方法会生成它的引用，这种情况同样是安全的。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;不过子类对象通常是可变的，这时我们就需要定义&lt;strong&gt;深拷贝&lt;/strong&gt;（Deep Clone），来克隆这个类的所有子对象。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;具体实现方法如下：&lt;/p&gt;
&lt;section class=&quot;code-snippet__fix code-snippet__js&quot; data-tool=&quot;mdnice编辑器&quot; readability=&quot;3&quot;&gt;&lt;ul class=&quot;code-snippet__line-index code-snippet__js&quot;&gt;&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;ounter(line&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;pre class=&quot;code-snippet__js&quot; data-lang=&quot;Java&quot;&gt;
&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;public Test clone() throws CloneNotSupportedException{&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;     //拷贝该对象&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    Test cloned = (Test)super.clone();&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;     //拷贝该对象中的可变域&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    cloned.time = (Date) time.clone();&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;    return cloned;&lt;/span&gt;&lt;/code&gt;&lt;code&gt;&lt;span class=&quot;code-snippet_outer&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/section&gt;&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这里需要提到的一点是：&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;虽然我们已经学习了&lt;code&gt;clone&lt;/code&gt;的两种用法，但是在实际的编码中还是尽量少用这个方法，它具有天生的不稳定性，仅仅了解即可。即使是Java的标准库中也只有5%的类实现了这个方法。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;我们可以使用Java的对象串行化特性来实现克隆对象，虽然效率不高，但是很安全，而且很容易实现。&lt;/p&gt;
&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/blog/2020-08-16-032446.png&quot; alt=&quot;&quot;/&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题六： 关于&lt;code&gt;equals&lt;/code&gt;方法，说说是什么？&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;&lt;code&gt;Object&lt;/code&gt;类中的&lt;code&gt;equals&lt;/code&gt;方法用于检测一个对象是否等于另一个对象。在&lt;code&gt;Object&lt;/code&gt;类中，这个方法将判断两个对象是否具有相同的引用。如果两个对象具有相同的引用，它们一定是相等的。&lt;/p&gt;
&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/blog/2020-08-16-032458.png&quot; alt=&quot;&quot;/&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题七：有没有自己去重写过&lt;code&gt;equals&lt;/code&gt;方法呢？&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;当然，这个我有笔记～&lt;/p&gt;
&lt;img src=&quot;https://viyoungblog.oss-cn-beijing.aliyuncs.com/blog/2020-08-16-032530.png&quot; alt=&quot;&quot;/&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题八：不限于&lt;code&gt;Object&lt;/code&gt;，聊聊&lt;code&gt;hashCode&lt;/code&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;在Java中，hash code是&lt;strong&gt;由对象导出的一个整型值&lt;/strong&gt;，以下是几个常见哈希值的算法：&lt;/p&gt;
&lt;ol data-tool=&quot;mdnice编辑器&quot;&gt;&lt;li readability=&quot;-1.5&quot;&gt;
&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;&lt;code&gt;Object&lt;/code&gt;类的&lt;code&gt;hashCode()&lt;/code&gt;。返回对象的内存地址经过处理后的结构，由于每个对象的内存地址都不一样，所以哈希码也不一样。&lt;/p&gt;
&lt;/section&gt;&lt;/li&gt;
&lt;li readability=&quot;-1.5&quot;&gt;
&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;&lt;code&gt;String&lt;/code&gt;类的&lt;code&gt;hashCode()&lt;/code&gt;。根据&lt;code&gt;String&lt;/code&gt;类包含的字符串的内容，根据一种特殊算法返回哈希码，只要字符串所在的堆空间相同，返回的哈希码也相同。&lt;/p&gt;
&lt;/section&gt;&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;section readability=&quot;4&quot;&gt;&lt;p&gt;&lt;code&gt;Integer&lt;/code&gt;类，返回的哈希码就是&lt;code&gt;Integer&lt;/code&gt;对象里所包含的那个整数的数值，例如&lt;code&gt;Integer i1=new Integer(100&lt;/code&gt;)，&lt;code&gt;i1.hashCode&lt;/code&gt;的值就是100 。由此可见，2个一样大小的&lt;code&gt;Integer&lt;/code&gt;对象，返回的哈希码也一样。&lt;/p&gt;
&lt;/section&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3 data-tool=&quot;mdnice编辑器&quot;&gt;&lt;span class=&quot;content&quot;&gt;问题九：说说&lt;code&gt;Equals&lt;/code&gt;和 &lt;code&gt;Hashcode&lt;/code&gt;的关系&lt;/span&gt;&lt;/h3&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;这两个其实确切意义上并没有什么联系，前提是我们不会在HashSet，HashMap这种本质是散列表的数据结构中使用，如果我们要在HashSet，HashMap这种本质是散列表的数据结构中使用，在重写equals方法的同时也要重写hashCode方法，以便用户将对象插入到散列表中，否则会导致数据不唯一，内存泄漏等各种问题。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;1.&lt;code&gt;hashCode&lt;/code&gt;是为了提高在散列结构存储中查找的效率，在线性表中没有作用。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;2.&lt;code&gt;equals()&lt;/code&gt;和&lt;code&gt;hashCode()&lt;/code&gt;需要同时覆盖，而且定义必须一致，也就是说equals比较了哪些域，hashCode就会对哪些域进行hash值的处理。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;3.若两个对象&lt;code&gt;equals()&lt;/code&gt;返回true，则&lt;code&gt;hashCode()&lt;/code&gt;有必要也返回相同的值。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;4.若两个对象&lt;code&gt;equals()&lt;/code&gt;返回false，则&lt;code&gt;hashCode()&lt;/code&gt;不一定返回不同的值。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;5.若两个对象&lt;code&gt;hashCode()&lt;/code&gt;返回相同的值，则&lt;code&gt;equals()&lt;/code&gt;不一定返回true。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;6.若两个对象&lt;code&gt;hashCode()&lt;/code&gt;返回不同值，则&lt;code&gt;equals()&lt;/code&gt;一定返回false。&lt;/p&gt;
&lt;p data-tool=&quot;mdnice编辑器&quot;&gt;7.同一对象在执行期间若已经存储在集合中，则不能修改影响hashCode值的相关信息，否则会导致内存泄露问题。&lt;/p&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:58:00 +0000</pubDate>
<dc:creator>山禾说</dc:creator>
<og:description>问题一：简单聊一下关于你对Object的理解 在 Java 中，只有基本数据类型不是对象，比如，数值，布尔和字符类型的值都不是对象。而其余的数据类型都是继承自一个名为Object的类，这个类是所有类的</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/viyoung/p/13515711.html</dc:identifier>
</item>
<item>
<title>LeetCode 到底怎么刷？GitHub 上多位大厂程序员亲测的高效刷题方式 - 削微寒</title>
<link>http://www.cnblogs.com/xueweihan/p/13513371.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xueweihan/p/13513371.html</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173147484-1826405470.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;作者：HelloGitHub-&lt;strong&gt;小鱼干&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在众多的诸如阿里、腾讯等大厂之中，最看中面试者刷题技能的大概要数有“链表厂”之称的字节跳动了。作为一个新晋大厂，字节跳动以高薪、技术大佬云集吸引了众多的程序员呢，问题来了，怎么才能进入“链表厂”呢？答案之一：刷题！&lt;/p&gt;
&lt;p&gt;刷题就不得不提 LeetCode 了，如何高效地刷 LeetCode 便是本文要说的事情了。&lt;/p&gt;
&lt;h2 id=&quot;助力-gopher-金九银十：leetcode-cookbook&quot;&gt;助力 gopher 金九银十：LeetCode Cookbook&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GitHub Star 数&lt;/strong&gt; ：2642&lt;/p&gt;
&lt;p&gt;LeetCode Cookbook 是 &lt;a href=&quot;https://github.com/halfrost&quot;&gt;@halfrost&lt;/a&gt; 去年刷的 LeetCode 整理出的 520 题，每道题都写了解题思路，全部都是 GO 实现的，并且每题都 runtime beats 100% 了。此外，halfrost 也制作了在线阅读小册，支持了 PWA，可以像 Mac 应用一样沉浸式阅读。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;：halfrost，iOS 开发届的大佬级别人物。从 2019 年 3 月 25 号开始刷题，到 2020 年 3 月 25 号，整整一年的时间。原计划是每天一题。实际上每天有时候不止一题，最终完成了 600+： ​​​​&lt;/p&gt;
&lt;blockquote readability=&quot;1.4545454545455&quot;&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/halfrost/LeetCode-Go&quot;&gt;https://github.com/halfrost/LeetCode-Go&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173205386-1809701696.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;图解-leetcode：leetcodeanimation&quot;&gt;图解 LeetCode：LeetCodeAnimation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GitHub Star 数&lt;/strong&gt; ：57k+&lt;/p&gt;
&lt;p&gt;LeetCodeAnimation 是一个用动画的形式呈现解 LeetCode 题目的思路的项目，采用图解形式能极大地降低大家理解解题思路的门槛，快速了解到解题之法。由于作者一个人精力有限，项目一度处于停更状态，今年四月作者招募同样热爱刷题的小伙伴重维护 LeetCodeAnimation。目前项目有 15+ Contributor。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;：吴师兄，五分钟学算法公众号维护者。在 18 年领悟了想进大厂终究还是得掌握算法与数据结构，于是花了半年时间，每天花 4 个小时学习和刷题，最终在 18 年的时候拿下了 8 个 offer。除了图解 LeetCode 项目之外，吴师兄还有一个在线项目：&lt;a href=&quot;https://www.cxyxiaowu.com/soup.html&quot;&gt;毒鸡汤&lt;/a&gt;，刷题之余也可以补充下“能量”——“生活中很多人喜欢小题大作，其实真的没有必要，要想想大题怎么办。”&lt;/p&gt;
&lt;blockquote readability=&quot;1.2698412698413&quot;&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/MisterBooo/LeetCodeAnimation&quot;&gt;https://github.com/MisterBooo/LeetCodeAnimation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173216035-176571378.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;刷题的套路：fucking-algorithm&quot;&gt;刷题的套路：fucking-algorithm&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GitHub Star 数&lt;/strong&gt; ：52k+&lt;/p&gt;
&lt;p&gt;fucking-algorithm 是今年二月开源的一个刷题项目，开源当周便获得了 2k+ star，之后一直蝉联 GitHub Trending 榜单，除了它小别致的项目名之外，内容也是圈粉的原因。fucking-algorithm 总共 60 多篇原创文章，都是基于 LeetCode 的题目，涵盖了所有题型和技巧，而且一定要做到举一反三，通俗易懂，绝不是简单的代码堆砌。项目目前已被翻译成英文，拯救海外和我们一样深陷刷题苦海的程序员。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;：labuladong 是一名低调的程序员 &lt;del&gt;小鱼干不才找不到相关作者介绍&lt;/del&gt;，有一个周访问量过 100k+ 的网站，如果你想传播技术不妨向他投稿参与「&lt;a href=&quot;https://labuladong.gitbook.io/algo/di-wu-zhang-ji-shu-wen-zhang-xi-lie/gitbook&quot;&gt;优质作者扶持计划&lt;/a&gt;」计划&lt;/p&gt;
&lt;blockquote readability=&quot;1.2698412698413&quot;&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/labuladong/fucking-algorithm&quot;&gt;https://github.com/labuladong/fucking-algorithm&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173225525-1891464871.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;最科学的刷题方式：algorithm-pattern&quot;&gt;最科学的刷题方式：algorithm-pattern&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GitHub Star 数&lt;/strong&gt; ：8.3k+&lt;/p&gt;
&lt;p&gt;algorithm-pattern 又名算法模板，用最科学的刷题方式、快速的刷题路径。掌握了刷题模板之后，刷题也变得好玩起来了~这个模板主要是介绍了一些通用的刷题模板，以及一些常见问题，如到底要刷多少题，按什么顺序来刷题，如何提高刷题效率等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;作者介绍&lt;/strong&gt;：greyireland 从 4 月份找工作开始，从 0 开始刷 LeetCode，中间大概花了一个半月（6 周）左右时间刷完 240 题。&lt;/p&gt;
&lt;blockquote readability=&quot;1.25&quot;&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/greyireland/algorithm-pattern&quot;&gt;https://github.com/greyireland/algorithm-pattern&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173235141-464080136.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;文末福利大厂面试题：leetcodetop&quot;&gt;文末福利大厂面试题：LeetcodeTop&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GitHub Star 数&lt;/strong&gt; ：2,404&lt;/p&gt;
&lt;p&gt;LeetcodeTop 这个仓库用于汇总互联网公司技术岗考察 LeetCode题目的热度，帮助同学们更加有针对性地准备面试。&lt;/p&gt;
&lt;blockquote readability=&quot;1.4285714285714&quot;&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/afatcoder/LeetcodeTop&quot;&gt;https://github.com/afatcoder/LeetcodeTop&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200816173244270-274568908.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后，希望上面的 Repo 能帮你在面试中过关斩将，拿到心仪 offer，如果你有更好的刷题 Repo 在手，记得和 HelloGitHub 资源共享你下哟~~&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/759200/202008/759200-20200813230159561-969610802.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关注公众号加入我们&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:55:00 +0000</pubDate>
<dc:creator>削微寒</dc:creator>
<og:description>作者：HelloGitHub-小鱼干 在众多的诸如阿里、腾讯等大厂之中，最看中面试者刷题技能的大概要数有“链表厂”之称的字节跳动了。作为一个新晋大厂，字节跳动以高薪、技术大佬云集吸引了众多的程序员呢，</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/xueweihan/p/13513371.html</dc:identifier>
</item>
<item>
<title>5分钟快速了解MySQL索引的各种类型 - 万猫学社</title>
<link>http://www.cnblogs.com/heihaozi/p/13515690.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/heihaozi/p/13515690.html</guid>
<description>&lt;p&gt;之所以在索引在面试中经常被问到，就是因为：索引是数据库的良好性能表现的关键，也是对查询能优化最有效的手段。索引能够轻易地把查询性能提高几个数量级。&lt;/p&gt;&lt;div id=&quot;cnblogs_post_body&quot; readability=&quot;162.5&quot;&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;文章持续更新，微信搜索「&lt;strong&gt;万猫学社&lt;/strong&gt;」第一时间阅读。&lt;br/&gt;关注后回复「&lt;strong&gt;电子书&lt;/strong&gt;」，免费获取12本Java必读技术书籍。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;什么是索引？&quot;&gt;什么是索引？&lt;/h3&gt;
&lt;p&gt;索引是数据库存储引擎用于快速查找到指定数据的一种数据结构。&lt;/p&gt;
&lt;p&gt;可以用新华字典做类比：如果新华字典中对每个字的详细解释是数据库中表的记录，那么按部首或拼音等排序的目录就是索引，使用它可以让我们快速查找的某一个字详细解释的位置。&lt;/p&gt;
&lt;p&gt;在MySQL中，存储引擎也是用了类似的方法，先在索引中找到对应的值，然后再根据匹配的索引值找到对应表中记录的位置。&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;onemorestudy&quot;&gt;文章持续更新，微信搜索「&lt;strong&gt;万猫学社&lt;/strong&gt;第一时间阅读，关注后回复「&lt;strong&gt;电子书&lt;/strong&gt;」，免费获取12本Java必读技术书籍。&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;面试中为什么问索引？&quot;&gt;面试中为什么问索引？&lt;/h3&gt;
&lt;p&gt;之所以在索引在面试中经常被问到，就是因为：索引是数据库的良好性能表现的关键，也是对查询能优化最有效的手段。索引能够轻易地把查询性能提高几个数量级。&lt;/p&gt;
&lt;p&gt;然而，糟糕的索引也同样会影响查询性能，当表中的数据量越来越多的时候，索引对性能的影响就越大。在数据量比较少并且负责比较低的时候，糟糕的索引对性能的影响可能不明显，但是当数据量逐渐增多的时候，性能会急剧下降。&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;onemorestudy&quot;&gt;文章持续更新，微信搜索「&lt;strong&gt;万猫学社&lt;/strong&gt;第一时间阅读，关注后回复「&lt;strong&gt;电子书&lt;/strong&gt;」，免费获取12本Java必读技术书籍。&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;索引的类型&quot;&gt;索引的类型&lt;/h3&gt;
&lt;p&gt;经过前面的介绍，我们就进入正题，了解一下MySQL支持的索引类型，以及它们的原理和用法。&lt;/p&gt;
&lt;p&gt;不同类型的索引，可以为不同场景提供更好的性能。在MySQL中，索引是在存储引擎层面实现的，而不是在服务器层面实现的。正如大家所知道，MySQL支持多种类型的存储引擎。所以，在不同存储引擎中索引的实现方式并不是一样的，也不是所有类型的索引都被所有存储引擎支持的，即使多个存储引擎支持同一种类型的索引，它底层的实现也有可能是不相同的。&lt;/p&gt;
&lt;h3 id=&quot;b-tree索引&quot;&gt;B-Tree索引&lt;/h3&gt;
&lt;p&gt;B-Tree索引是被大多数MySQL存储引擎支持的，在我们讨论索引时，假如没有特别地说明类型，那么大概率说的就是B-Tree索引了。我们使用B-Tree这个词，是因为MySQL在创建表和其他语句中就使用这个关键字。&lt;/p&gt;
&lt;p&gt;然而，在不同存储引擎的底层可能使用不同的数据结构和算法，比如：InnoDB存储引擎内部使用的是B+Tree结构，NDB集群存储引擎内部使用的是T-Tree结构。不同存储引擎用以不同的方式使用B-Tree索引，性能也可能不同，比如：InnoDB的索引上存储的是原数据格式，而MyISAM存储引擎使用前缀压缩技术使索引更小，InnoDB索引的行存储的数据行的主键引用，而MyISAM存储引擎的索引的行存储的是数据行的物理位置。&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;onemorestudy&quot;&gt;文章持续更新，微信搜索「&lt;strong&gt;万猫学社&lt;/strong&gt;第一时间阅读，关注后回复「&lt;strong&gt;电子书&lt;/strong&gt;」，免费获取12本Java必读技术书籍。&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;b-tree索引的原理&quot;&gt;B-Tree索引的原理&lt;/h3&gt;
&lt;p&gt;B-Tree索引能够加快访问数据的速度，因为不需要全表扫描就可以快速检索的需要的数据。那么B-Tree索引是怎么做到的呢？我们通过一个简单的例子了解一下InnoDB的B-Tree索引是怎么工作的：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;CREATE TABLE `om_address`  (
  `province_name` varchar(255) NOT NULL COMMENT '省',
  `city_name` varchar(255) NOT NULL COMMENT '市',
  `district_name` varchar(255) NOT NULL COMMENT '区',
  `detailed_address` varchar(255) NULL DEFAULT NULL COMMENT '详细地址',
  INDEX `index_province_city_district`(`province_name`, `city_name`, `district_name`) USING BTREE
) ENGINE = InnoDB;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个表中共有4个字段，分别表示省、市、区和详细地址，还有一个B-Tree索引，其中包含了省、市、区三个字段。因为索引的所有值都是按照顺序存储的，即：节点的左子树比当前节点小，节点的右子树比当前节点大。那么当查询数据时，从索引的根节点开始搜索，根据比较当前节点的索引值向子树进行查找，直到找到对应的索引值，或者根本没有找到。&lt;/p&gt;
&lt;h3 id=&quot;b-tree索引的用法&quot;&gt;B-Tree索引的用法&lt;/h3&gt;
&lt;p&gt;根据B-Tree索引的特点，它可以用于全值匹配、值范围匹配和最左前缀匹配。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;全值匹配是指和索引中所有的字段进行匹配，比如：查询黑龙江省哈尔滨市南岗区的数据。&lt;/li&gt;
&lt;li&gt;值范围匹配是指索引中字段的某一范围进行匹配，但是必须满足前面字段的全匹配，比如：第一个字段province_name省名称的全匹配，第二个字段city_name城市名称的范围匹配。&lt;/li&gt;
&lt;li&gt;最左前缀匹配是指索引中字段的某一开头部分进行匹配，但是必须满足前面字段的全匹配，比如：第一个字段province_name省名称为内蒙古，第二个字段city_name城市名称以“呼”开头。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span class=&quot;onemorestudy&quot;&gt;文章持续更新，微信搜索「&lt;strong&gt;万猫学社&lt;/strong&gt;第一时间阅读，关注后回复「&lt;strong&gt;电子书&lt;/strong&gt;」，免费获取12本Java必读技术书籍。&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;哈希索引&quot;&gt;哈希索引&lt;/h3&gt;
&lt;p&gt;哈希索引是基于哈希表实现的，用于精确匹配索引所指向的数据。存储引擎对每一行数据的所有索引字段计算出一个哈希码，哈希码是一个比较小的值，并且不同的数据计算出来的哈希码一般情况下也不一样。哈希索引中存放了这个哈希码和指向这个数据行的指针。&lt;/p&gt;
&lt;p&gt;在MySQL中，只有Memory存储引擎支持哈希索引，也是Memory存储引擎的默认索引类型。另外，在InnoDB存储引擎中也运用了哈希索引，叫做自适应哈希索引。当某些索引中被非常频繁的使用时，InnoDB存储引擎会在内存中基于B-Tree索引之上再创建一个哈希索引，这样一来使得B-Tree索引也具有的快速哈希查找的优点。&lt;/p&gt;
&lt;p&gt;哈希索引因为只需存放对应数据的哈希值，所以索引的结构非常紧凑，占用空间小，同时查询速度也非常快。不过，哈希索引只支持全值等值查询，不能索引字段范围匹配和部分索引字段匹配。&lt;/p&gt;
&lt;h3 id=&quot;空间数据索引&quot;&gt;空间数据索引&lt;/h3&gt;
&lt;p&gt;空间数据索引（R-Tree）主要用于地理数据的存储，会从所有维度来索引数据，查询时可以有效的使用任意维度进行组合查询。 目前，MyISAM存储引擎支持空间数据索引，不过必须使用MySQL的GIS相关的函数来维护数据。&lt;/p&gt;
&lt;p&gt;在MySQL中，空间索引只能建立在空间数据类型上，如：GEOMETRY、POINT、LINESTRING等。&lt;/p&gt;
&lt;h3 id=&quot;全文索引&quot;&gt;全文索引&lt;/h3&gt;
&lt;p&gt;全文索引不像之前介绍的索引那样直接比较索引中的值，而是直接比较查找的文本中的关键词，它类似于搜索引擎做的事情，不是简单的where条件匹配。&lt;/p&gt;
&lt;p&gt;在相同的字段上，可以同时创建全文索引和B-Tree索引，不会有冲突。全文索引适用于match和against操作，不是普通的where条件操作。在MySQL中，只能在类型为CHAR、VARCHAR、TEXT的字段上创建全文索引。&lt;/p&gt;
&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;
&lt;p&gt;索引是数据库存储引擎用于快速查找到指定数据的一种数据结构，它包括B-Tree索引、哈希索引、空间数据索引、全文索引，其中B-Tree索引是我们最常用到的，InnoDB存储引擎内部使用的是B+Tree结构；哈希索引是基于哈希表实现的，用于精确匹配索引所指向的数据；空间数据索引从所有维度来索引数据，查询时可以有效的使用任意维度进行组合查询；全文索引是直接比较查找的文本中的关键词，类似于搜索引擎。&lt;/p&gt;
&lt;div class=&quot;onemorestudy&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;微信公众号：万猫学社&lt;/p&gt;
&lt;p&gt;微信扫描二维码&lt;/p&gt;
&lt;p&gt;关注后回复「电子书」&lt;/p&gt;
&lt;p&gt;获取12本Java必读技术书籍&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://images.cnblogs.com/cnblogs_com/heihaozi/1575453/o_onemore.jpg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
<pubDate>Mon, 17 Aug 2020 00:50:00 +0000</pubDate>
<dc:creator>万猫学社</dc:creator>
<og:description>之所以在索引在面试中经常被问到，就是因为：索引是数据库的良好性能表现的关键，也是对查询能优化最有效的手段。索引能够轻易地把查询性能提高几个数量级。</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/heihaozi/p/13515690.html</dc:identifier>
</item>
<item>
<title>MySQL优化 - 蚂蚁style</title>
<link>http://www.cnblogs.com/zhaoletian/p/13500755.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/zhaoletian/p/13500755.html</guid>
<description>&lt;p&gt;&lt;span&gt;MySQL常见性能瓶颈&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;　　CPU:CPU在饱和的时候一般发生在数据装入内存或从磁盘上读取数据的时候&lt;/p&gt;
&lt;p&gt;　　硬件：top, free, iostat和vmstat等命令来查看系统性能状态&lt;/p&gt;
&lt;p&gt;　　I/O:磁盘I/O瓶颈发生装入数据远大于内存容量的时候&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;SQL：程序中SQL语句问题，Explain查看执行计划。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Explain简介&lt;/span&gt;：&lt;/p&gt;
&lt;p&gt;　　EXPLAIN 命令是查看查询优化器如何决定执行查询的主要方法,MYSQL会在查询上设置一个标记,当执行查询时,这个标记会使其返回关于在执行计划中每一步的信息,从而可以从分析结果中找到查询语句或是表结构的性能瓶颈&lt;br/&gt;Explain能干什么？&lt;/p&gt;
&lt;p&gt;　　1：分析出表的执行顺序&lt;/p&gt;
&lt;p&gt;　　2：数据读取操作的操作类型&lt;/p&gt;
&lt;p&gt;　　3：哪些索引可以使用&lt;/p&gt;
&lt;p&gt;　　4：哪些索引被实际使用&lt;/p&gt;
&lt;p&gt;　　5：表之间的引用&lt;/p&gt;
&lt;p&gt;&lt;span&gt;EXPLAIN结果参数含义：id&lt;/span&gt;&lt;br/&gt;　　样例：explain select * from tbl_dept;&lt;br/&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814091829603-729950440.png&quot; width=&quot;764&quot; height=&quot;112&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 1.id:  id代表执行select子句或操作表的顺序,id分别有三种不同的执行结果&lt;br/&gt;id相同,执行顺序由上至下&lt;br/&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814091947238-1141124637.png&quot; width=&quot;771&quot; height=&quot;112&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 2.id不同,如果是子查询,id的序号会递增,id值越大,优先级越高,越先被执行&lt;br/&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814092231730-1322895340.png&quot; width=&quot;771&quot; height=&quot;127&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 3. id相同和不同,同时存在,遵从优先级高的优先执行,优先级相同的按照由上至下的顺序执行&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814092341653-819877681.png&quot; width=&quot;771&quot; height=&quot;131&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;EXPLAIN结果参数含义：select_type&lt;/span&gt;&lt;br/&gt;　　查询的类型,主要用于区别普通查询,联合查询,子查询等复杂查询&lt;br/&gt;　　SIMPLE：简单的select查询,查询中不包含子查询或union查询&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814093414300-983634433.png&quot; width=&quot;771&quot; height=&quot;44&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt; 　　PRIMARY/UNION： PRIMARY：查询中最外层的SELECT（如两表做UNION或者存在子查询的外层的表操作为PRIMARY，内层的操作为UNION）&lt;/p&gt;
&lt;p&gt; 　　SUBQUERY： 在select 或where 列表中包含了子查询，子查询中首个SELECT（如果有多个子查询存在）&lt;/p&gt;
&lt;p&gt;　&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814093547906-415768713.png&quot; width=&quot;412&quot; height=&quot;112&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; EXPLAIN结果参数含义：type&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　查询类型从最好到最差依次是:system&amp;gt;const&amp;gt;eq_ref&amp;gt;ref&amp;gt;range&amp;gt;index&amp;gt;All,    一般情况下,得至少保证达到range级别,最好能达到ref&lt;/p&gt;
&lt;p&gt;　　eq_ref:唯一性索引扫描,对于每个索引键,表中只有一条记录与之匹配,常见于主键或唯一索引扫描&lt;br/&gt;　　ref:非唯一性索引扫描,返回匹配某个单独值的行,它可能会找到多个符合条件的行,所以他应该属于查找和扫描的混合体&lt;br/&gt;　　range:只检索给定范围的行,使用一个索引来选择行,如where语句中出现了between,&amp;lt;,&amp;gt;,in等查询,这种范围扫描索引比全表扫描要好，因为它只需要开始于索引的某一点，而结束于另一点，不用扫描全部索引。&lt;br/&gt;　　index:index类型只遍历索引树,这通常比All快,因为索引文件通常比数据文件小,index是从索引中读取,all从硬盘中读取&lt;br/&gt;　　all:全表扫描,是最差的一种查询类型&lt;/p&gt;
&lt;p&gt;&lt;span&gt;EXPLAIN结果参数含义：possibles_keys&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　显示可能应用在这张表中的索引,一个或多个,查询到的索引不一定是真正被用到的.&lt;br/&gt;&lt;span&gt;EXPLAIN结果参数含义：key&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　实际使用的索引,如果为null,则没有使用索引,因此会出现possible_keys列有可能被用到的索引,但是key列为null,表示实际没用索引。&lt;br/&gt;&lt;span&gt;EXPLAIN结果参数含义：key_len&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　表示索引中使用的字节数,而通过该列计算查询中使用的 索引长度,在不损失精确性的情况下,长度越短越好,key_len显示的值为索引字段的最大可能长度,并非实际使用长度,即,key_len是根据表定义计算而得么不是通过表内检索出的&lt;br/&gt;&lt;span&gt;EXPLAIN结果参数含义：ref&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　显示索引的哪一列被使用了,如果可能的话是一个常数,哪些列或常量被用于查找索引列上的值&lt;br/&gt;&lt;span&gt;EXPLAIN结果参数含义：rows&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　根据表统计信息及索引选用情况,大致估算出找到所需的记录所需要读取的行数&lt;/p&gt;
&lt;p&gt;&lt;span&gt;EXPLAIN结果参数含义：Extra &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　不适合在其他列显示的额外信息&lt;/p&gt;
&lt;p&gt;　　Using temporary ：使用了临时表保存中间结果,mysql在对查询结果排序时使用临时表,常见于order by和分组查询group by&lt;br/&gt;　　Using index：表示相应的select操作中使用了覆盖索引（Covering Index），避免访问了表的数据行，效率不错。如果同时出现using where，表明索引被用来执行索引键值的查找；如果没有同时出现using where，表明索引用来读取数据而非执行查找动作。 其中的覆盖索引含义是所查询的列是和建立的索引字段和个数是一一对应的&lt;br/&gt;　　Using where:表明使用了where过滤&lt;br/&gt;　　Using join buffer：表明使用了连接缓存,如在查询的时候会有多次join,则可能会产生临时表&lt;br/&gt;　　impossible where：表示where子句的值总是false,不能用来获取任何元祖。如下例：&lt;/p&gt;
&lt;p&gt;　　　　select * from t1 where id='1' and id='2';&lt;br/&gt;　　select tables optimized away：在没有GROUPBY子句的情况下，基于索引优化MIN/MAX操作或者对于MyISAM存储引擎优化 ，COUNT(*)操作，不必等到执行阶段再进行计算，查询执行计划生成的阶段即完成优化。&lt;/p&gt;
&lt;p&gt;　　distinct：优化distinct操作，在找到第一匹配的元组后即停止找同样值的动作，即一旦MySQL找到了与行相联合匹配的行，就不再搜索了。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;索引介绍&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　索引（Index） 是帮助MySQL高效获取数据的数据结构&lt;/p&gt;
&lt;p&gt;　　索引为什么是一种数据结构，它又是怎么提高查询的速度？我们拿最常用的二叉树来分析索引的工作原理。看下面的图片：　　&lt;img src=&quot;https://img2020.cnblogs.com/blog/1639414/202008/1639414-20200814100024654-1175307844.png&quot; width=&quot;615&quot; height=&quot;335&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; 创建索引的优势&lt;/span&gt;&lt;br/&gt;      1 提高数据的检索速度，降低数据库IO成本：使用索引的意义就是通过缩小表中需要查询的记录的数目从而加快搜索的速度。&lt;br/&gt;      2 降低数据排序的成本，降低CPU消耗：索引之所以查的快，是因为先将数据排好序，若该字段正好需要排序，则真好降低了排序的成本。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;创建索引的劣势&lt;/span&gt;&lt;br/&gt;        1 占用存储空间：索引实际上也是一张表，记录了主键与索引字段，一般以索引文件的形式存储在磁盘上。&lt;br/&gt;        2 降低更新表的速度：表的数据发生了变化，对应的索引也需要一起变更，从而减低的更新速度。否则索引指向的物理数据可能不对，这也是索引失效的原因之一。&lt;br/&gt;        3 优质索引创建难：索引的创建并非一日之功，也并非一直不变。需要频繁根据用户的行为和具体的业务逻辑去创建最佳的索引。&lt;br/&gt;常见索引类型有哪些？&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;普通索引&lt;/span&gt;：普通索引(由关键字KEY或INDEX定义的索引)的唯一任务是加快对数据的访问速度。因此，应该只为那些最经常出现在查询条件(WHERE column = …)或排序条件(ORDER BY column)中的数据列创建索引。只要有可能，就应该选择一个数据最整齐、最紧凑的数据列(如一个整数类型的数据列)来创建索引。&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;唯一索引&lt;/span&gt;：唯一性索引的值是唯一的，可以更快速的通过该索引来确定某条记录。&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;主键索引&lt;/span&gt;：是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;组合索引&lt;/span&gt;：指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用组合索引时遵循最左前缀集合，比如说，INDEX(A, B, C)可以当做A或(A, B)的索引来使用，但不能当做B、C或(B, C)的索引来使用&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;全文索引(倒排文档技术)&lt;/span&gt;： 主要用来查找文本中的关键字，而不是直接与索引中的值相比较。fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的where语句的参数匹配。fulltext索引配合match against操作使用，而不是一般的where语句加like。它可以在create table，alter table ，create index使用，不过目前只有char、varchar，text 列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建fulltext索引，要比先为一张表建立fulltext然后再将数据写入的速度快很多。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;数据库索引的设计原则:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　1．选择唯一性索引&lt;br/&gt;例如，学生表中学号是具有唯一性的字段。为该字段建立唯一性索引可以很快的确定某个学生的信息。如果使用姓名的话，可能存在同名现象，从而降低查询速度。&lt;/p&gt;
&lt;p&gt;　　2．为经常需要排序、分组和联合操作的字段建立索引&lt;/p&gt;
&lt;p&gt;　　 经常需要ORDER BY、GROUP BY、DISTINCT和UNION等操作的字段，排序操作会浪费很多时间。如果为其建立索引，可以有效地避免排序操作。&lt;/p&gt;
&lt;p&gt;　　3．为常作为查询条件的字段建立索引&lt;br/&gt;　　 如果某个字段经常用来做查询条件，那么该字段的查询速度会影响整个表的查询速度。因此，为这样的字段建立索引，可以提高整个表的查询速度&lt;/p&gt;
&lt;p&gt;　　4．限制索引的数目&lt;br/&gt;      索引的数目不是越多越好。每个索引都需要占用磁盘空间，索引越多，需要的磁盘空间就越大。修改表时，对索引的重构和更新很麻烦。越多的索引，会使更新表变得很浪费时间。&lt;br/&gt;　　5．尽量使用数据量少的索引&lt;br/&gt; 　　如果索引的值很长，那么查询的速度会受到影响。例如，对一个CHAR（100）类型的字段进行全文，检索需要的时间肯定要比对CHAR（10）类型的字段需要的时间要多。&lt;br/&gt;　　6．尽量使用前缀来索引&lt;br/&gt;　　 如果索引字段的值很长，最好使用值的前缀来索引。例如，TEXT和BLOG类型的字段，进行全文检索会很浪费时间。如果只检索字段的前面的若干个字符，这样可以提高检索速度&lt;br/&gt;　　7．删除不再使用或者很少使用的索引&lt;br/&gt;      表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。&lt;br/&gt;　　8.小表不应建立索引，包含大量的列并且不需要搜索非空值的时候可以考虑不建索引  &lt;br/&gt;　　9.尽量不要对数据库中某个含有大量重复的值的字段建立索引。&lt;br/&gt;　　　　  对于一个ENUM类型的字段来说，出现大量重复值是很有可能的情况，例如“sex”字段，在这样的字段上建立索引将不会有什么帮助；相反，还有可能降低数据库的性能。&lt;br/&gt;&lt;span&gt;索引中需要注意的事项&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　1.索引不会包含有null值的列&lt;br/&gt;        只要列中包含有null值都将不会被包含在索引中，复合索引中只要有一列含有null值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为null。&lt;/p&gt;
&lt;p&gt;　　2.以%开头的LIKE查询不能使用索引&lt;br/&gt;       explain select deptName,remark,createDate from dept where deptName like'%2' and remark = 'test'and createDate ='2018-07-22'; type index&lt;br/&gt;　　SELECT * FROM `houdunwang` WHERE `uname` LIKE'金蝶%' -- 走索引&lt;br/&gt;　　SELECT * FROM `houdunwang` WHERE `uname` LIKE &quot;%金蝶%&quot; -- 不走索引&lt;/p&gt;
&lt;p&gt;　　3.数据类型出现隐式转换时也不能使用索引&lt;br/&gt;       explain select deptName,remark,createDate from dept where deptName =2 and remark = 'test'and createDate ='2018-07-22'; type index&lt;/p&gt;
&lt;p&gt;　　4.复合索引时 不符合最左匹配原则&lt;br/&gt;　　增加联合索引 ALTER TABLE dept ADD INDEX index_all (`deptName`,`remark`,`createDate`);&lt;br/&gt;    　explain select deptName,remark,createDate from dept where remark = 'test'and createDate ='2018-07-22'; type index&lt;/p&gt;
&lt;p&gt;　　5. 用or分隔开的条件，如果or前的条件中的列有索引，后面的列中没有索引，那么涉及到的索引都不会使用到&lt;br/&gt;　　增加联合索引 ALTER TABLE dept ADD INDEX index_all (`deptName`,`remark`,`createDate`);&lt;br/&gt;    　explain select deptName,remark,createDate from dept where deptName ='2' and remark = 'test'and createDate ='2018-07-22' or salary =200; type ALL&lt;/p&gt;
&lt;p&gt;　　6.order by 字段混合使用DESC ASC 不会使用索引&lt;br/&gt;   　 select * from table order by key1 desc,key2 asc (尽量不要出现这种混合排序)&lt;/p&gt;
&lt;p&gt;　　7.Where条件过滤的关键字和Order by中所使用的不同 不会使用索引&lt;br/&gt;    　select * from table where key2 = ? order by key1 (order by 出现的关键字 尽量 在where条件中也出现)&lt;/p&gt;
&lt;p&gt;　　8.多表连接的时候 join on(a.id=b.id2) 连接外键id、id2 必须加索引。MySQL规定作为外键的字段必须有索引 也是为了让我们表连接的时候 用到索引&lt;/p&gt;
&lt;p&gt;　　9.  SELECT `sname` FROM `stu` WHERE `age`+10=30;-- 不会使用索引，因为所有索引列参与了计算&lt;br/&gt;　　10.  SELECT `sname` FROM `stu` WHERE LEFT(`date`,4) &amp;lt;1990; -- 不会使用索引，因为使用了函数运算&lt;br/&gt;　　11.字符串与数字比较不使用索引;&lt;br/&gt;    　　EXPLAIN SELECT * FROM `a` WHERE `a`=&quot;1&quot; -- 走索引&lt;br/&gt;    　　EXPLAIN SELECT * FROM `a` WHERE `a`=1 -- 不走索引，同样也是使用了函数运算&lt;/p&gt;
&lt;p&gt;&lt;span&gt;总结:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　索引的建立必须慎重，对每个索引的必要性都应该经过仔细分析，要有建立的依据。因为太多的索引与不充分、不正确的索引对性能都毫无益处：在表上建立的每个索引都会增加存储开销，索引对于插入、删除、更新操作也会增加处理上的开销。另外，过多的复合索引，在有单字段索引的情况下，一般都是没有存在价值的；相反，还会降低数据增加删除时的性能，特别是对频繁更新的表来说，负面影响更大.&lt;/p&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:47:00 +0000</pubDate>
<dc:creator>蚂蚁style</dc:creator>
<og:description>MySQL常见性能瓶颈： CPU:CPU在饱和的时候一般发生在数据装入内存或从磁盘上读取数据的时候 硬件：top, free, iostat和vmstat等命令来查看系统性能状态 I/O:磁盘I/O瓶</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/zhaoletian/p/13500755.html</dc:identifier>
</item>
<item>
<title>利用 Github Actions 自动更新 docfx 文档 - WeihanLi</title>
<link>http://www.cnblogs.com/weihanli/p/docfx-docs-auto-update.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/weihanli/p/docfx-docs-auto-update.html</guid>
<description>&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;
&lt;p&gt;docfx 是微软出品一个 .NET API 文档框架，有一个理念是代码即文档，会根据项目代码自动生成 API 文档，即使没有写任何注释也会生成 API 文档，也有一些默认的主题可以配置，也可以自定义主题配置，详细介绍可以参考官方介绍 &lt;a href=&quot;https://dotnet.github.io/docfx/&quot;&gt;https://dotnet.github.io/docfx/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;目前也有很多项目在使用 docfx 来生成文档，比如前段时间介绍过的 &lt;code&gt;Reserver-Proxy&lt;/code&gt; 项目，也是看到了 reservse-proxy 项目配置了一个 Github Actions 来自动更新文档所以在我自己的项目里也增加了类似的配置，除了微软的项目还有很多社区开源项目在用，如果你也在做一些 .NET 类库类的开源项目，可以尝试一下&lt;/p&gt;
&lt;p&gt;docfx 怎么使用可以参考官方文档，本文主要介绍如何使用 Github Actions 实现自动更新文档&lt;/p&gt;
&lt;h2 id=&quot;文档示例&quot;&gt;文档示例&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/489462/202008/489462-20200817083043207-2046838401.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/489462/202008/489462-20200817083114953-62514374.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;更多可以参考: &lt;a href=&quot;https://weihanli.github.io/WeihanLi.Npoi/index.html&quot;&gt;https://weihanli.github.io/WeihanLi.Npoi/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;自动更新文档 commit 示例&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/489462/202008/489462-20200817084517857-1045868571.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;自动更新文档流程&quot;&gt;自动更新文档流程&lt;/h2&gt;
&lt;ol&gt;&lt;li&gt;检出要使用的用于生成文档的分支代码&lt;/li&gt;
&lt;li&gt;安装 docfx 命令行工具，推荐使用 choco 安装，因为执行 build 的 agent 上已经安装了 Chocolatey&lt;/li&gt;
&lt;li&gt;使用 docfx 生成文档&lt;/li&gt;
&lt;li&gt;检出 gh-pages 分支，用于托管文档的分支&lt;/li&gt;
&lt;li&gt;删除 gh-pages 之前的文件（&lt;code&gt;.git&lt;/code&gt;目录包含git信息，不能删除）&lt;/li&gt;
&lt;li&gt;把第三步操作生成的文档复制到 gh-pages 分支下&lt;/li&gt;
&lt;li&gt;commit &amp;amp;&amp;amp; push，提交代码并推送更新在线文档&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;github-actions-示例配置&quot;&gt;Github Actions 示例配置&lt;/h2&gt;
&lt;p&gt;Actions 示例，源链接:&lt;a href=&quot;https://github.com/WeihanLi/WeihanLi.Npoi/blob/dev/.github/workflows/docfx.yml&quot;&gt;https://github.com/WeihanLi/WeihanLi.Npoi/blob/dev/.github/workflows/docfx.yml&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-yaml&quot;&gt;name: docfx build
on:
  push:
    branches:
      - dev
jobs:
  build:
    name: Build
    runs-on: windows-latest
    steps:
      # Check out the branch that triggered this workflow to the 'source' subdirectory
      - name: Checkout Code
        uses: actions/checkout@v2
        with:
          ref: dev
          path: source
      - name: install DocFX
        run: &quot;&amp;amp; choco install docfx -y&quot;
      # Run a build
      - name: Build docs
        run: &quot;&amp;amp; docfx ./docfx.json&quot;
        working-directory: ./source
      # Check out gh-pages branch to the 'docs' subdirectory
      - name: Checkout docs
        uses: actions/checkout@v2
        with:
          ref: gh-pages
          path: docs
      # Sync the site
      - name: Clear docs repo
        run: Get-ChildItem -Force -Exclude .git | ForEach-Object { Remove-Item -Recurse -Verbose -Force $_ }
        working-directory: ./docs
      - name: Sync new content
        run: Copy-Item -Recurse -Verbose -Force &quot;$env:GITHUB_WORKSPACE/source/_site/*&quot; &quot;$env:GITHUB_WORKSPACE/docs&quot;
        working-directory: ./docs
        # update docs
      - name: Commit to gh-pages and push
        run: |
          $ErrorActionPreference = &quot;Continue&quot;
          git add -A
          git diff HEAD --exit-code
          if ($LASTEXITCODE -eq 0) {
            Write-Host &quot;No changes to commit!&quot;
          } else {
            git config --global user.name &quot;github-actions-docfx[bot]&quot;
            git config --global user.email &quot;weihanli@outlook.com&quot;
            git commit -m &quot;Updated docs from commit $env:GITHUB_SHA on $env:GITHUB_REF&quot;
            git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}
            git push origin gh-pages
          }
        working-directory: ./docs
&lt;/code&gt;
&lt;/pre&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;我这里是只要 dev 分支更新了就更新，你也可以根据需要当 master 分支更新时再更新，修改分支名称即可&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;
&lt;p&gt;现在用的还是 2.x 版本，3.x 版本还没发布，3.x版本发布之后可以直接通过 &lt;code&gt;dotnet tool&lt;/code&gt; 来安装更加方便和可扩展，目前 2.x 使用 &lt;code&gt;choco&lt;/code&gt; 来安装命令行工具，需要依赖 Chocolatey，如果是 &lt;code&gt;dotnet tool&lt;/code&gt; 有 dotnet 环境就可以了，就可以方便很多了&lt;/p&gt;
&lt;p&gt;不仅仅是 docfx 生成文档，你也可以扩展其他类似的需求，使用 Github Actions 实现自动同步，更新&lt;/p&gt;
&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:35:00 +0000</pubDate>
<dc:creator>WeihanLi</dc:creator>
<og:description>利用 Github Actions 自动更新 docfx 文档</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/weihanli/p/docfx-docs-auto-update.html</dc:identifier>
</item>
<item>
<title>如何构建一个生产环境的推荐系统 - 哥不是小萝莉</title>
<link>http://www.cnblogs.com/smartloli/p/13509861.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/smartloli/p/13509861.html</guid>
<description>&lt;p&gt;前面介绍过什么是推荐系统，以及推荐系统中的用例，比如基于用户的协同过滤来构建推荐系统。今天给大家介绍如何构建一个生产环境的推荐系统。&lt;/p&gt;

&lt;p&gt;现在互联网上的内容很多，我们可能每天都会接受来自不同消息。例如，电商网站、阅读博客、各类新闻文章等。但是，这些消息并不是所有的内容你都感兴趣，可能你只对技术博客感兴趣，或者某些新闻感兴趣等等。而这么内容如何去满足用户的需求呢？我们需要一个精准的解决方案来简化用户的发现过程。&lt;/p&gt;
&lt;h2&gt;2.1 推荐系统的作用是啥？&lt;/h2&gt;
&lt;p&gt;简而言之，推荐系统就是一个发现用户喜好的系统。系统从数据中学习并向用户提供有效的建议。如果用户没有特意搜索某项物品，则系统会自动将该项带出。这样看起很神奇，比如，你在电商网站上浏览过某个品牌的鞋子，当你在用一些社交软件、短视频软件、视频软件时，你会惊奇的发现在你所使用的这些软件中，会给你推荐你刚刚在电商网站上浏览的过的鞋子。&lt;/p&gt;
&lt;p&gt;其实，这得益于推荐系统的过滤功能。我们来看看一张简图，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815085025170-185857211.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 从上图中，我们可以简单的总结出，整个数据流程如下：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;数据来源：负责提供数据来源，比如用户在电商网站、新闻、视频等上的用户行为，作为推荐训练的数据来源；&lt;/li&gt;
&lt;li&gt;数据采集：用户产生了数据，我们需要将这些数据进行收集，比如SDK埋点采集、Nginx上报、爬虫等方式来获取数据；&lt;/li&gt;
&lt;li&gt;数据存储：获取这些数据后，需要对这些数据进行分类存储、清洗等，比如大数据里面用的最多的HDFS，或者构建数据仓库Hive表等；&lt;/li&gt;
&lt;li&gt;推荐系统：数据分类、清洗后好，有了推荐系统需要的数据，然后使用推荐系统中的各种模型、比如协同过滤、内容过滤、相似过滤、用户矩阵等，来训练这些用户数据，得到训练结果；&lt;/li&gt;
&lt;li&gt;目标用户：通过推荐系统，对用户数据进行训练后，得出训练结果，将这些结果，推荐给目标用户。&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;2.2 依赖准备&lt;/h2&gt;
&lt;p&gt;我们使用Python来够构建推荐系统模型，需要依赖如下的Python依赖包：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
pip &lt;span&gt;install&lt;/span&gt;&lt;span&gt; numpy
pip &lt;/span&gt;&lt;span&gt;install&lt;/span&gt;&lt;span&gt; scipy
pip &lt;/span&gt;&lt;span&gt;install&lt;/span&gt;&lt;span&gt; pandas
pip &lt;/span&gt;&lt;span&gt;install&lt;/span&gt;&lt;span&gt; jupyter
pip &lt;/span&gt;&lt;span&gt;install&lt;/span&gt; requests
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里为简化Python的依赖环境，推荐使用Anaconda3。这里面集成了很多Python的依赖库，不用我们在额外去关注Python的环境准备。&lt;/p&gt;
&lt;p&gt;接着，我们加载数据源，代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;import pandas as pd
import numpy as np

&lt;/span&gt;&lt;span&gt;df&lt;/span&gt; = pd.read_csv(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;resource/events.csv&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;df&lt;/span&gt;&lt;span&gt;.shape
print(&lt;/span&gt;&lt;span&gt;df&lt;/span&gt;.&lt;span&gt;head&lt;/span&gt;())
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815111125077-558918413.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;使用df.head()会打印数据前5行数据：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;timestamp：时间戳&lt;/li&gt;
&lt;li&gt;visitorid：用户ID&lt;/li&gt;
&lt;li&gt;event：事件类型&lt;/li&gt;
&lt;li&gt;itemid：物品ID&lt;/li&gt;
&lt;li&gt;transactionid：事务ID&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;使用如下代码，查看事件类型有哪些：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot;&gt;
&lt;pre&gt;
print(&lt;span&gt;df&lt;/span&gt;.event.unique())
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815120316107-811423638.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;从上图可知，类型有三种，分别是：view、addtocart、transaction。&lt;/p&gt;
&lt;p&gt;为了简化起见，以transaction类型为例子。代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
trans = &lt;span&gt;df&lt;/span&gt;[&lt;span&gt;df&lt;/span&gt;[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;event&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] == &lt;span&gt;'&lt;/span&gt;&lt;span&gt;transaction&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
trans.shape
print(trans.&lt;/span&gt;&lt;span&gt;head&lt;/span&gt;())
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815120703212-1247045844.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;接着，我们来看看用户和物品的相关数据，代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
visitors = trans[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].unique()
items &lt;/span&gt;= trans[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;itemid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].unique()
print(visitors.shape)
print(items.shape)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815120950503-2072842690.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 我们可以获得11719个去重用户和12025个去重物品。&lt;/p&gt;
&lt;p&gt;构建一个简单而有效的推荐系统的经验法则是在不损失精准度的情况下减少数据的样本。这意味着，你只能为每个用户获取大约50个最新的事务样本，并且我们仍然可以得到期望中的结果。&lt;/p&gt;
&lt;p&gt;代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
trans2 = trans.groupby([&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]).&lt;span&gt;head&lt;/span&gt;(&lt;span&gt;50&lt;/span&gt;&lt;span&gt;)
print(trans2.shape)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815121350757-948483761.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 真实场景中，用户ID和物品ID是一个海量数字，人为很难记住，比如如下代码：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
trans2[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitors&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = trans2[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;].apply(lambda x : np.argwhere(visitors == x)[&lt;span&gt;0&lt;/span&gt;][&lt;span&gt;0&lt;/span&gt;&lt;span&gt;])
trans2[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;items&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = trans2[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;itemid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;].apply(lambda x : np.argwhere(items == x)[&lt;span&gt;0&lt;/span&gt;][&lt;span&gt;0&lt;/span&gt;&lt;span&gt;])

print(trans2)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815122020773-1811697672.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;h2&gt;2.3 构建矩阵&lt;/h2&gt;
&lt;h3&gt;2.3.1 构建用户-物品矩阵 &lt;/h3&gt;
&lt;p&gt;从上面的代码执行的结果来看，目前样本数据中有11719个去重用户和12025个去重物品，因此，我们接下来构建一个稀疏矩阵。需要用到如下Python依赖：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
from scipy.sparse import csr_matrix
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;实现代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;40&quot;&gt;
&lt;pre&gt;
occurences = csr_matrix((visitors.shape[&lt;span&gt;0&lt;/span&gt;], items.shape[&lt;span&gt;0&lt;/span&gt;]), dtype=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;int8&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
def set_occurences(visitor, item):
    occurences[visitor, item] &lt;/span&gt;+= &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
trans2.apply(lambda row: set_occurences(row[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitors&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], row[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;items&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
print(occurences)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('96ef15de-35e5-469d-ad56-72872440ab27')&quot; readability=&quot;57.5&quot;&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; id=&quot;code_img_closed_96ef15de-35e5-469d-ad56-72872440ab27&quot; class=&quot;code_img_closed&quot;/&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; id=&quot;code_img_opened_96ef15de-35e5-469d-ad56-72872440ab27&quot; class=&quot;code_img_opened&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_96ef15de-35e5-469d-ad56-72872440ab27&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;110&quot;&gt;
&lt;pre&gt;
(&lt;span&gt;0&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;37&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;72&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;108&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;130&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;131&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;132&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;133&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;162&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;163&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;164&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;, &lt;span&gt;2&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;, &lt;span&gt;3&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;, &lt;span&gt;161&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;, &lt;span&gt;4&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;, &lt;span&gt;40&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;6&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;18&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;19&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;54&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;101&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;111&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;, &lt;span&gt;113&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  :     :
  (&lt;/span&gt;&lt;span&gt;11695&lt;/span&gt;, &lt;span&gt;383&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11696&lt;/span&gt;, &lt;span&gt;12007&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11696&lt;/span&gt;, &lt;span&gt;12021&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11697&lt;/span&gt;, &lt;span&gt;12008&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11698&lt;/span&gt;, &lt;span&gt;12011&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11699&lt;/span&gt;, &lt;span&gt;1190&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11700&lt;/span&gt;, &lt;span&gt;506&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11701&lt;/span&gt;, &lt;span&gt;11936&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11702&lt;/span&gt;, &lt;span&gt;10796&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11703&lt;/span&gt;, &lt;span&gt;12013&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11704&lt;/span&gt;, &lt;span&gt;12016&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11705&lt;/span&gt;, &lt;span&gt;12017&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11706&lt;/span&gt;, &lt;span&gt;674&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11707&lt;/span&gt;, &lt;span&gt;3653&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11708&lt;/span&gt;, &lt;span&gt;12018&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11709&lt;/span&gt;, &lt;span&gt;12019&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11710&lt;/span&gt;, &lt;span&gt;1330&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11711&lt;/span&gt;, &lt;span&gt;4184&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11712&lt;/span&gt;, &lt;span&gt;3595&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11713&lt;/span&gt;, &lt;span&gt;12023&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11714&lt;/span&gt;, &lt;span&gt;3693&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11715&lt;/span&gt;, &lt;span&gt;5690&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11716&lt;/span&gt;, &lt;span&gt;6280&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11717&lt;/span&gt;, &lt;span&gt;3246&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11718&lt;/span&gt;, &lt;span&gt;2419&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;h3&gt;2.3.2 构建物品-物品共生矩阵&lt;/h3&gt;
&lt;p&gt;构建一个物品与物品矩阵，其中每个元素表示一个用户购买两个物品的次数，可以认为是一个共生矩阵。要构建一个共生矩阵，需要将发生矩阵的转置与自身进行点乘。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
cooc =&lt;span&gt; occurences.transpose().dot(occurences)
cooc.setdiag(&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
print(cooc)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('231f6923-e8f3-436e-9a97-65d7f62d11da')&quot; readability=&quot;57.5&quot;&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; id=&quot;code_img_closed_231f6923-e8f3-436e-9a97-65d7f62d11da&quot; class=&quot;code_img_closed&quot;/&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; id=&quot;code_img_opened_231f6923-e8f3-436e-9a97-65d7f62d11da&quot; class=&quot;code_img_opened&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_231f6923-e8f3-436e-9a97-65d7f62d11da&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;110&quot;&gt;
&lt;pre&gt;
(&lt;span&gt;0&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;164&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;163&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;162&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;133&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;132&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;131&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;130&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;108&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;72&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;37&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;, &lt;span&gt;2&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;161&lt;/span&gt;, &lt;span&gt;3&lt;/span&gt;)      &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;, &lt;span&gt;3&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;, &lt;span&gt;4&lt;/span&gt;)       &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;, &lt;span&gt;4&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8228&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8197&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8041&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8019&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8014&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8009&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8008&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;7985&lt;/span&gt;, &lt;span&gt;5&lt;/span&gt;)     &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  :     :
  (&lt;/span&gt;&lt;span&gt;11997&lt;/span&gt;, &lt;span&gt;12022&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;2891&lt;/span&gt;, &lt;span&gt;12022&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;12023&lt;/span&gt;, &lt;span&gt;12023&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;12024&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)        &lt;span&gt;0&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11971&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;11880&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;10726&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)        &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;8694&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4984&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4770&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4767&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4765&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4739&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4720&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4716&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4715&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;4306&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;2630&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;2133&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;) &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;978&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;887&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;851&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;768&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;734&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
  (&lt;/span&gt;&lt;span&gt;220&lt;/span&gt;, &lt;span&gt;12024&lt;/span&gt;)  &lt;span&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;这样一个稀疏矩阵就构建好了，并使用setdiag函数将对角线设置为0（即忽略第一项的值）。&lt;/p&gt;
&lt;p&gt;接下来会用到一个和余弦相似度的算法类似的算法LLR（Log-Likelihood Ratio）。LLR算法的核心是分析事件的计数，特别是事件同时发生的计数。而我们需要的技术一般包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;两个事件同时发生的次数（K_11）&lt;/li&gt;
&lt;li&gt;一个事件发生而另外一个事件没有发生的次数（K_12、K_21）&lt;/li&gt;
&lt;li&gt;两个事件都没有发生（K_22）&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;表格表示如下：&lt;/p&gt;
&lt;table border=&quot;0&quot;&gt;&lt;tbody readability=&quot;5&quot;&gt;&lt;tr&gt;&lt;td&gt; &lt;/td&gt;
&lt;td&gt;事件A&lt;/td&gt;
&lt;td&gt;事件B&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;事件B&lt;/td&gt;
&lt;td&gt;A和B同时发生（K_11）&lt;/td&gt;
&lt;td&gt;B发生，单A不发生（K_12）&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;任何事件但不包含B&lt;/td&gt;
&lt;td&gt;A发生，但是B不发生（K_21）&lt;/td&gt;
&lt;td&gt;A和B都不发生（K_22）&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;通过上述表格描述，我们可以较为简单的计算LLR的分数，公式如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
LLR=&lt;span&gt;2&lt;/span&gt; &lt;span&gt;sum&lt;/span&gt;(k)(H(k)-H(rowSums(k))-H(colSums(k)))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;那回到本案例来，实现代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;52&quot;&gt;
&lt;pre&gt;
&lt;span&gt;def xLogX(x):
    return x &lt;/span&gt;* np.log(x) &lt;span&gt;if&lt;/span&gt; x != &lt;span&gt;0&lt;/span&gt; &lt;span&gt;else&lt;/span&gt; &lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
def entropy(x1, x2&lt;/span&gt;=&lt;span&gt;0&lt;/span&gt;, x3=&lt;span&gt;0&lt;/span&gt;, x4=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;):
    return xLogX(x1 &lt;/span&gt;+ x2 + x3 + x4) - xLogX(x1) - xLogX(x2) - xLogX(x3) -&lt;span&gt; xLogX(x4)
def LLR(k11, k12, k21, k22):
    rowEntropy &lt;/span&gt;= entropy(k11 + k12, k21 +&lt;span&gt; k22)
    columnEntropy &lt;/span&gt;= entropy(k11 + k21, k12 +&lt;span&gt; k22)
    matrixEntropy &lt;/span&gt;=&lt;span&gt; entropy(k11, k12, k21, k22)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; rowEntropy + columnEntropy &amp;lt;&lt;span&gt; matrixEntropy:
        return &lt;/span&gt;&lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
    return &lt;/span&gt;&lt;span&gt;2.0&lt;/span&gt; * (rowEntropy + columnEntropy -&lt;span&gt; matrixEntropy)
def rootLLR(k11, k12, k21, k22):
    llr &lt;/span&gt;=&lt;span&gt; LLR(k11, k12, k21, k22)
    sqrt &lt;/span&gt;=&lt;span&gt; np.sqrt(llr)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; k11 * &lt;span&gt;1.0&lt;/span&gt; / (k11 + k12) &amp;lt; k21 * &lt;span&gt;1.0&lt;/span&gt; / (k21 +&lt;span&gt; k22):
        sqrt &lt;/span&gt;= -&lt;span&gt;sqrt
    return sqrt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;代码中的K11、K12、K21、K22分别代表的含义如下：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;K11：两个事件都发送&lt;/li&gt;
&lt;li&gt;K12：事件B发送，而事件A不发生&lt;/li&gt;
&lt;li&gt;K21：事件A发送，而事件B不发生&lt;/li&gt;
&lt;li&gt;K22：事件A和B都不发生&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;那我们计算的公式，实现的代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;48&quot;&gt;
&lt;pre&gt;
row_sum = np.&lt;span&gt;sum&lt;/span&gt;(cooc, axis=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;).A.flatten()
column_sum &lt;/span&gt;= np.&lt;span&gt;sum&lt;/span&gt;(cooc, axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;).A.flatten()
total &lt;/span&gt;= np.&lt;span&gt;sum&lt;/span&gt;(row_sum, axis=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
pp_score &lt;/span&gt;= csr_matrix((cooc.shape[&lt;span&gt;0&lt;/span&gt;], cooc.shape[&lt;span&gt;1&lt;/span&gt;]), dtype=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;double&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
cx &lt;/span&gt;=&lt;span&gt; cooc.tocoo()
&lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i,j,v &lt;span&gt;in&lt;/span&gt; &lt;span&gt;zip&lt;/span&gt;&lt;span&gt;(cx.row, cx.col, cx.data):
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; v != &lt;span&gt;0&lt;/span&gt;&lt;span&gt;:
        k11 &lt;/span&gt;=&lt;span&gt; v
        k12 &lt;/span&gt;= row_sum[i] -&lt;span&gt; k11
        k21 &lt;/span&gt;= column_sum[j] -&lt;span&gt; k11
        k22 &lt;/span&gt;= total - k11 - k12 -&lt;span&gt; k21
        pp_score[i,j] &lt;/span&gt;= rootLLR(k11, k12, k21, k22)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后，我们对结果进行排序，让每一项的最高LLR分数位于每行的第一列，实现代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
result = np.flip(np.&lt;span&gt;sort&lt;/span&gt;(pp_score.A, axis=&lt;span&gt;1&lt;/span&gt;), axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
result_indices &lt;/span&gt;= np.flip(np.argsort(pp_score.A, axis=&lt;span&gt;1&lt;/span&gt;), axis=&lt;span&gt;1&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;例如我们来看看其中一项结果，代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
print(result[&lt;span&gt;8456&lt;/span&gt;&lt;span&gt;])
print(result_indices[&lt;/span&gt;&lt;span&gt;8456&lt;/span&gt;])
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;结果如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815164036578-1585418032.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;实际情况中，我们会根据经验对LLR分数进行一些限制，因此将不重要的指标会进行删除。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
minLLR = &lt;span&gt;5&lt;/span&gt;&lt;span&gt;
indicators &lt;/span&gt;= result[:, :&lt;span&gt;50&lt;/span&gt;&lt;span&gt;]
indicators[indicators &lt;/span&gt;&amp;lt; minLLR] = &lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
indicators_indices &lt;/span&gt;= result_indices[:, :&lt;span&gt;50&lt;/span&gt;&lt;span&gt;]
max_indicator_indices &lt;/span&gt;= (indicators==&lt;span&gt;0&lt;/span&gt;).argmax(axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
max &lt;/span&gt;=&lt;span&gt; max_indicator_indices.max()
indicators &lt;/span&gt;= indicators[:, :max+&lt;span&gt;1&lt;/span&gt;&lt;span&gt;]
indicators_indices &lt;/span&gt;= indicators_indices[:, :max+&lt;span&gt;1&lt;/span&gt;]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;训练出结果后，我们可以将其放入到ElasticSearch中进行实时检索。使用到的Python依赖库如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;import requests
import json&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里使用ElasticSearch的批量更新API，创建一个新的索引，实现代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;42&quot;&gt;
&lt;pre&gt;
actions =&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(indicators.shape[&lt;span&gt;0&lt;/span&gt;&lt;span&gt;]):
    length &lt;/span&gt;= indicators[i].nonzero()[&lt;span&gt;0&lt;/span&gt;].shape[&lt;span&gt;0&lt;/span&gt;&lt;span&gt;]
    real_indicators &lt;/span&gt;= items[indicators_indices[i, :length]].astype(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;int&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;).tolist()
    &lt;/span&gt;&lt;span&gt;id&lt;/span&gt; =&lt;span&gt; items[i]
    
    action &lt;/span&gt;= { &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;index&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : { &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;_index&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;items2&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;_id&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : str(&lt;span&gt;id&lt;/span&gt;&lt;span&gt;) } }
    
    data &lt;/span&gt;=&lt;span&gt; {
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;id&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;: &lt;span&gt;int&lt;/span&gt;(&lt;span&gt;id&lt;/span&gt;&lt;span&gt;),
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;indicators&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;: real_indicators
    }
    
    actions.append(json.dumps(action))
    actions.append(json.dumps(data))
    
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(actions) == &lt;span&gt;200&lt;/span&gt;&lt;span&gt;:
        actions_string &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;.&lt;span&gt;join&lt;/span&gt;(actions) + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        actions &lt;/span&gt;=&lt;span&gt; []
        
        url &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;http://127.0.0.1:9200/_bulk/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        headers &lt;/span&gt;=&lt;span&gt; {
            &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Content-Type&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;application/x-ndjson&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        }
        requests.post(url, headers&lt;/span&gt;=headers, data=&lt;span&gt;actions_string)
&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(actions) &amp;gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;:
    actions_string &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;.&lt;span&gt;join&lt;/span&gt;(actions) + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    actions &lt;/span&gt;=&lt;span&gt; []
    url &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;http://127.0.0.1:9200/_bulk/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    headers &lt;/span&gt;=&lt;span&gt; {
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Content-Type&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;application/x-ndjson&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    }
    requests.post(url, headers&lt;/span&gt;=headers, data=actions_string)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在浏览器中访问地址http://127.0.0.1:9200/items2/_count，结果如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815181206901-498486656.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;接下来，我们可以尝试将访问地址切换为这个http://127.0.0.1:9200/items2/240708，结果如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/666745/202008/666745-20200815181405034-517160381.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;


&lt;p&gt;构建一个面向生产环境的推荐系统并不困难，目前现有的技术组件可以满足我们构建这样一个生产环境的推荐系统。比如Hadoop、Hive、HBase、Kafka、ElasticSearch等这些成熟的开源组件来构建我们的生产环境推荐系统。本案例的完整代码如下所示：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('0d9ee6ce-c25d-4bb4-8191-acf93bada4b8')&quot; readability=&quot;58&quot;&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; id=&quot;code_img_closed_0d9ee6ce-c25d-4bb4-8191-acf93bada4b8&quot; class=&quot;code_img_closed&quot;/&gt;&lt;img src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; id=&quot;code_img_opened_0d9ee6ce-c25d-4bb4-8191-acf93bada4b8&quot; class=&quot;code_img_opened&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_0d9ee6ce-c25d-4bb4-8191-acf93bada4b8&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;111&quot;&gt;
&lt;pre&gt;
&lt;span&gt;import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
import requests
import json

&lt;/span&gt;&lt;span&gt;df&lt;/span&gt; = pd.read_csv(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;resource/events.csv&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
# print(&lt;/span&gt;&lt;span&gt;df&lt;/span&gt;&lt;span&gt;.shape)
# print(&lt;/span&gt;&lt;span&gt;df&lt;/span&gt;.&lt;span&gt;head&lt;/span&gt;&lt;span&gt;())
# print(&lt;/span&gt;&lt;span&gt;df&lt;/span&gt;&lt;span&gt;.event.unique())
trans &lt;/span&gt;= &lt;span&gt;df&lt;/span&gt;[&lt;span&gt;df&lt;/span&gt;[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;event&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] == &lt;span&gt;'&lt;/span&gt;&lt;span&gt;transaction&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
# print(trans.shape)
# print(trans.&lt;/span&gt;&lt;span&gt;head&lt;/span&gt;&lt;span&gt;())

visitors &lt;/span&gt;= trans[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].unique()
items &lt;/span&gt;= trans[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;itemid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;].unique()
# print(visitors.shape)
# print(items.shape)

trans2 &lt;/span&gt;= trans.groupby([&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]).&lt;span&gt;head&lt;/span&gt;(&lt;span&gt;50&lt;/span&gt;&lt;span&gt;)
# print(trans2.shape)

trans2[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitors&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = trans2[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitorid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;].apply(lambda x : np.argwhere(visitors == x)[&lt;span&gt;0&lt;/span&gt;][&lt;span&gt;0&lt;/span&gt;&lt;span&gt;])
trans2[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;items&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] = trans2[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;itemid&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;].apply(lambda x : np.argwhere(items == x)[&lt;span&gt;0&lt;/span&gt;][&lt;span&gt;0&lt;/span&gt;&lt;span&gt;])

# print(trans2)
occurences &lt;/span&gt;= csr_matrix((visitors.shape[&lt;span&gt;0&lt;/span&gt;], items.shape[&lt;span&gt;0&lt;/span&gt;]), dtype=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;int8&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
def set_occurences(visitor, item):
    occurences[visitor, item] &lt;/span&gt;+= &lt;span&gt;1&lt;/span&gt;&lt;span&gt;
trans2.apply(lambda row: set_occurences(row[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;visitors&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], row[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;items&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;]), axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
# print(occurences)

cooc &lt;/span&gt;=&lt;span&gt; occurences.transpose().dot(occurences)
cooc.setdiag(&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
# print(cooc)

def xLogX(x):
    return x &lt;/span&gt;* np.log(x) &lt;span&gt;if&lt;/span&gt; x != &lt;span&gt;0&lt;/span&gt; &lt;span&gt;else&lt;/span&gt; &lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
def entropy(x1, x2&lt;/span&gt;=&lt;span&gt;0&lt;/span&gt;, x3=&lt;span&gt;0&lt;/span&gt;, x4=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;):
    return xLogX(x1 &lt;/span&gt;+ x2 + x3 + x4) - xLogX(x1) - xLogX(x2) - xLogX(x3) -&lt;span&gt; xLogX(x4)
def LLR(k11, k12, k21, k22):
    rowEntropy &lt;/span&gt;= entropy(k11 + k12, k21 +&lt;span&gt; k22)
    columnEntropy &lt;/span&gt;= entropy(k11 + k21, k12 +&lt;span&gt; k22)
    matrixEntropy &lt;/span&gt;=&lt;span&gt; entropy(k11, k12, k21, k22)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; rowEntropy + columnEntropy &amp;lt;&lt;span&gt; matrixEntropy:
        return &lt;/span&gt;&lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
    return &lt;/span&gt;&lt;span&gt;2.0&lt;/span&gt; * (rowEntropy + columnEntropy -&lt;span&gt; matrixEntropy)
def rootLLR(k11, k12, k21, k22):
    llr &lt;/span&gt;=&lt;span&gt; LLR(k11, k12, k21, k22)
    sqrt &lt;/span&gt;=&lt;span&gt; np.sqrt(llr)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; k11 * &lt;span&gt;1.0&lt;/span&gt; / (k11 + k12) &amp;lt; k21 * &lt;span&gt;1.0&lt;/span&gt; / (k21 +&lt;span&gt; k22):
        sqrt &lt;/span&gt;= -&lt;span&gt;sqrt
    return sqrt

row_sum &lt;/span&gt;= np.&lt;span&gt;sum&lt;/span&gt;(cooc, axis=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;).A.flatten()
column_sum &lt;/span&gt;= np.&lt;span&gt;sum&lt;/span&gt;(cooc, axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;).A.flatten()
total &lt;/span&gt;= np.&lt;span&gt;sum&lt;/span&gt;(row_sum, axis=&lt;span&gt;0&lt;/span&gt;&lt;span&gt;)
pp_score &lt;/span&gt;= csr_matrix((cooc.shape[&lt;span&gt;0&lt;/span&gt;], cooc.shape[&lt;span&gt;1&lt;/span&gt;]), dtype=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;double&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
cx &lt;/span&gt;=&lt;span&gt; cooc.tocoo()
&lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i,j,v &lt;span&gt;in&lt;/span&gt; &lt;span&gt;zip&lt;/span&gt;&lt;span&gt;(cx.row, cx.col, cx.data):
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; v != &lt;span&gt;0&lt;/span&gt;&lt;span&gt;:
        k11 &lt;/span&gt;=&lt;span&gt; v
        k12 &lt;/span&gt;= row_sum[i] -&lt;span&gt; k11
        k21 &lt;/span&gt;= column_sum[j] -&lt;span&gt; k11
        k22 &lt;/span&gt;= total - k11 - k12 -&lt;span&gt; k21
        pp_score[i,j] &lt;/span&gt;=&lt;span&gt; rootLLR(k11, k12, k21, k22)
    
result &lt;/span&gt;= np.flip(np.&lt;span&gt;sort&lt;/span&gt;(pp_score.A, axis=&lt;span&gt;1&lt;/span&gt;), axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
result_indices &lt;/span&gt;= np.flip(np.argsort(pp_score.A, axis=&lt;span&gt;1&lt;/span&gt;), axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
print(result.shape)

print(result[&lt;/span&gt;&lt;span&gt;8456&lt;/span&gt;&lt;span&gt;])
print(result_indices[&lt;/span&gt;&lt;span&gt;8456&lt;/span&gt;&lt;span&gt;])

minLLR &lt;/span&gt;= &lt;span&gt;5&lt;/span&gt;&lt;span&gt;
indicators &lt;/span&gt;= result[:, :&lt;span&gt;50&lt;/span&gt;&lt;span&gt;]
indicators[indicators &lt;/span&gt;&amp;lt; minLLR] = &lt;span&gt;0.0&lt;/span&gt;&lt;span&gt;
indicators_indices &lt;/span&gt;= result_indices[:, :&lt;span&gt;50&lt;/span&gt;&lt;span&gt;]
max_indicator_indices &lt;/span&gt;= (indicators==&lt;span&gt;0&lt;/span&gt;).argmax(axis=&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
max &lt;/span&gt;=&lt;span&gt; max_indicator_indices.max()
indicators &lt;/span&gt;= indicators[:, :max+&lt;span&gt;1&lt;/span&gt;&lt;span&gt;]
indicators_indices &lt;/span&gt;= indicators_indices[:, :max+&lt;span&gt;1&lt;/span&gt;&lt;span&gt;]

actions &lt;/span&gt;=&lt;span&gt; []
&lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; range(indicators.shape[&lt;span&gt;0&lt;/span&gt;&lt;span&gt;]):
    length &lt;/span&gt;= indicators[i].nonzero()[&lt;span&gt;0&lt;/span&gt;].shape[&lt;span&gt;0&lt;/span&gt;&lt;span&gt;]
    real_indicators &lt;/span&gt;= items[indicators_indices[i, :length]].astype(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;int&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;).tolist()
    &lt;/span&gt;&lt;span&gt;id&lt;/span&gt; =&lt;span&gt; items[i]
    
    action &lt;/span&gt;= { &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;index&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : { &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;_index&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;items2&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;_id&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : str(&lt;span&gt;id&lt;/span&gt;&lt;span&gt;) } }
    
    data &lt;/span&gt;=&lt;span&gt; {
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;id&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;: &lt;span&gt;int&lt;/span&gt;(&lt;span&gt;id&lt;/span&gt;&lt;span&gt;),
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;indicators&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;: real_indicators
    }
    
    actions.append(json.dumps(action))
    actions.append(json.dumps(data))
    
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(actions) == &lt;span&gt;200&lt;/span&gt;&lt;span&gt;:
        actions_string &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;.&lt;span&gt;join&lt;/span&gt;(actions) + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        actions &lt;/span&gt;=&lt;span&gt; []
        
        url &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;http://127.0.0.1:9200/_bulk/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        headers &lt;/span&gt;=&lt;span&gt; {
            &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Content-Type&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;application/x-ndjson&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
        }
        requests.post(url, headers&lt;/span&gt;=headers, data=&lt;span&gt;actions_string)
&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(actions) &amp;gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;:
    actions_string &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;.&lt;span&gt;join&lt;/span&gt;(actions) + &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;\n&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    actions &lt;/span&gt;=&lt;span&gt; []
    url &lt;/span&gt;= &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;http://127.0.0.1:9200/_bulk/&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    headers &lt;/span&gt;=&lt;span&gt; {
        &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Content-Type&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; : &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;application/x-ndjson&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;
    }
    requests.post(url, headers&lt;/span&gt;=headers, data=actions_string)
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;这篇博客就和大家分享到这里，如果大家在研究学习的过程当中有什么问题，可以加群进行讨论或发送邮件给我，我会尽我所能为您解答，与君共勉！&lt;/p&gt;
&lt;p&gt;另外，博主出书了《&lt;a href=&quot;https://item.jd.com/12455361.html&quot; target=&quot;_blank&quot;&gt;Kafka并不难学&lt;/a&gt;》和《&lt;a href=&quot;https://item.jd.com/12371763.html&quot; target=&quot;_blank&quot;&gt;Hadoop大数据挖掘从入门到进阶实战&lt;/a&gt;》，喜欢的朋友或同学， 可以在公告栏那里点击购买链接购买博主的书进行学习，在此感谢大家的支持。关注下面公众号，根据提示，可免费获取书籍的教学视频。&lt;/p&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:30:00 +0000</pubDate>
<dc:creator>哥不是小萝莉</dc:creator>
<og:description>1.概述 前面介绍过什么是推荐系统，以及推荐系统中的用例，比如基于用户的协同过滤来构建推荐系统。今天给大家介绍如何构建一个生产环境的推荐系统。 2.内容 现在互联网上的内容很多，我们可能每天都会接受来</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/smartloli/p/13509861.html</dc:identifier>
</item>
<item>
<title>初识ABP vNext（3）：vue对接ABP基本思路 - xhznl</title>
<link>http://www.cnblogs.com/xhznl/p/13508356.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xhznl/p/13508356.html</guid>
<description>&lt;p&gt;上一篇介绍了ABP的启动模板以及AbpHelper工具的基本使用，这一篇将进入项目实战部分。因为目前ABP的官方模板只支持MVC和Angular，MVC的话咱.NET开发人员来写还可以，专业前端估计很少会用这个。。。Angular我本人不熟，所以选择vue来做UI。&lt;/p&gt;

&lt;p&gt;我使用&lt;a href=&quot;https://github.com/PanJiaChen/vue-element-admin&quot;&gt;vue-element-admin&lt;/a&gt;来作为模板，这个项目貌似很多人用，选择他的&lt;a href=&quot;https://github.com/PanJiaChen/vue-element-admin/tree/i18n&quot;&gt;i18n&lt;/a&gt;分支，因为我需要国际化功能。在开始编码前，需要先分析几个重要问题：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;用户登录/token&lt;/li&gt;
&lt;li&gt;用户权限控制&lt;/li&gt;
&lt;li&gt;应用程序本地化/语言切换&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;好在ABP模板提供了Angular版本，我们可以参考Angular版本来做。&lt;/p&gt;
&lt;h2 id=&quot;登录&quot;&gt;登录&lt;/h2&gt;
&lt;p&gt;因为ABP的授权模块是使用IdentityServer4，所以IdentityServer4的一些默认端点在ABP里也是同样有效的，可以参考下&lt;a href=&quot;https://identityserver4.readthedocs.io/&quot;&gt;IdentityServer4官网&lt;/a&gt;。运行ABP模板项目，看一下IdentityServer4发现文档，uri是：&lt;code&gt;/.well-known/openid-configuration&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810163749436-980878802.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;可以看到token端点是&lt;code&gt;/connect/token&lt;/code&gt;，这是IdentityServer4默认的，通过这个端点就可以登录用户获取token。后面请求接口时，把token放到HTTP Header的authorization字段即可。&lt;/p&gt;
&lt;h2 id=&quot;权限&quot;&gt;权限&lt;/h2&gt;
&lt;p&gt;进入ABP的&lt;code&gt;/swagger&lt;/code&gt;界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810165447203-1444129284.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;ABP内置了一个&lt;code&gt;/api/abp/application-configuration&lt;/code&gt;接口，它用于返回本地化文本，权限和一些系统设置信息。看一下数据格式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810170048204-401146819.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;auth.policies&lt;/strong&gt;字段中包含了系统的所有权限，&lt;strong&gt;auth.grantedPolicies&lt;/strong&gt;字段则包含了当前用户所拥有的权限，因为我现在没登录所以是空的。通过这两个字段就可以和vue-element-admin的菜单权限对应起来，实现权限控制。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810171247311-1143536186.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;currentUser&lt;/strong&gt;字段表示当前用户信息，没登录时就是空的，&lt;strong&gt;isAuthenticated&lt;/strong&gt;为false，这个字段也可以作为用户是否登录（token是否有效）的判断依据。&lt;/p&gt;
&lt;h2 id=&quot;本地化&quot;&gt;本地化&lt;/h2&gt;
&lt;p&gt;本地化对于大部分的小型系统可能都用不上，不过ABP作为一个优秀且全面的框架，必然会支持本地化功能。同样的，本地化信息也可以通过&lt;code&gt;/api/abp/application-configuration&lt;/code&gt;接口来获取：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810171826125-307001118.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;localization.languages&lt;/strong&gt;字段表示系统所支持的语言类型，前端的语言切换选项就可以使用这个字段。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810172528544-1773818737.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;localization.values&lt;/strong&gt;字段就是本地化的文本信息了，你在后端配置的本地化文本都可以从这里获取到，通过这个字段结合vue-element-admin的国际化功能，就可以让你的系统支持多语言。vue-element-admin的国际化方案是通过 &lt;a href=&quot;https://github.com/kazupon/vue-i18n&quot;&gt;vue-i18n&lt;/a&gt;来实现，你也可以直接在前端部分来做国际化，如果你只有一个前端应用的话，但是在后端做复用性更好一些。&lt;/p&gt;
&lt;p&gt;语言切换时只需要把对应的语言名称放到HTTP Header的accept-language字段就行。&lt;/p&gt;
&lt;h2 id=&quot;创建项目&quot;&gt;创建项目&lt;/h2&gt;
&lt;p&gt;在开始编码前，先创建好前后端的模板项目。&lt;/p&gt;
&lt;h3 id=&quot;abp&quot;&gt;ABP&lt;/h3&gt;
&lt;p&gt;这里直接用Abp CLI命令来创建解决方案吧：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-powershell&quot;&gt;abp new &quot;Xhznl.HelloAbp&quot; 
-t app 
-u none --separate-identity-server 
-m none 
-d ef -cs &quot;Server=localhost;User Id=sa;Password=Password@2020;Database=HelloAbp;MultipleActiveResultSets=true&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;创建一个名为&quot;Xhznl.HelloAbp&quot;的解决方案，使用app作为模板，不需要UI，并且将Identity Server应用程序与API host应用程序分开，使用Entity Framework Core作为数据库提供程序，并指定连接字符串。创建完成后会得到一个aspnet-core文件夹。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811094435880-828476414.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;项目结构如下，因为指定了&lt;code&gt;--separate-identity-server&lt;/code&gt;参数，所以多了个IdentityServer项目，如果不指定的话它是集成在HttpApi.Host中的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811105932795-339687787.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;通常小型系统没必要把Identity Server应用程序与API host应用程序分开，会增加运维成本，这里只是为了演示分布式部署的情况，为后面的微服务做准备。&lt;/p&gt;
&lt;p&gt;ABP还支持为每个模块单独配置数据库（如果你不需要分库，可以忽略以下内容），修改DbMigrator、IdentityServer项目的appsettings.json配置文件：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811160628452-1933296949.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在ConnectionStrings中添加AbpIdentityServer配置，为Identity Server配置独立的数据库连接字符串，不配置的话默认使用Default配置。AbpIdentityServer这个key是来自ABP的IdentityServer模块中的一个常量，具体请参考源码。&lt;/p&gt;
&lt;p&gt;在开发环境光定义连接字符串还不够，因为HelloAbpIdsDB数据库还不存在，需要使用EF Core Code Frist迁移系统创建和维护这个数据库。新建一个项目：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200812171158565-132408323.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;步骤比较多，具体流程请参考官网：&lt;a href=&quot;https://docs.abp.io/zh-Hans/abp/latest/Entity-Framework-Core-Migrations#%E4%BD%BF%E7%94%A8%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%BA%93&quot;&gt;数据库迁移&lt;/a&gt;，这里就不重复介绍了，你也可以选择不分库。&lt;/p&gt;
&lt;p&gt;完成以上步骤，最终会生成2个数据库，并且包含了一些默认的种子数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811163403948-1392973109.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;然后验证一下HttpApi.Host和IdentityServer项目是否可以正常运行，前提是你电脑需要有sqlserver，redis。&lt;/p&gt;
&lt;p&gt;HttpApi.Host：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811172838923-1221899843.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;IdentityServer：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200811172903263-416062701.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;vue-element-admin&quot;&gt;vue-element-admin&lt;/h3&gt;
&lt;p&gt;vue-element-admin的基本使用就不介绍了，相信很多人见过这个，不了解的可以自己去搜索学习一下。&lt;/p&gt;
&lt;p&gt;去GitHub下载&lt;a href=&quot;https://github.com/PanJiaChen/vue-element-admin/tree/i18n&quot;&gt;i18n&lt;/a&gt;分支的代码，或者直接用git clone命令。&lt;/p&gt;
&lt;p&gt;在项目根目录下执行指令：&lt;/p&gt;
&lt;p&gt;安装依赖：&lt;code&gt;npm install&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;启动项目：&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;启动正常的话可以看到这个界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/610959/202008/610959-20200810190011711-1997461720.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;本篇先做准备工作，下一篇将从登录功能开始编码实现。。。代码已上传至GitHub：&lt;a href=&quot;https://github.com/xiajingren/HelloAbp%EF%BC%8C%E6%AC%A2%E8%BF%8Estar%E3%80%82&quot;&gt;https://github.com/xiajingren/HelloAbp，欢迎star。&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 17 Aug 2020 00:24:00 +0000</pubDate>
<dc:creator>xhznl</dc:creator>
<og:description>前言 上一篇介绍了ABP的启动模板以及AbpHelper工具的基本使用，这一篇将进入项目实战部分。因为目前ABP的官方模板只支持MVC和Angular，MVC的话咱.NET开发人员来写还可以，专业前端</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/xhznl/p/13508356.html</dc:identifier>
</item>
<item>
<title>[源码解析] 当 Java Stream 遇见 Flink - 罗西的思考</title>
<link>http://www.cnblogs.com/rossiXYZ/p/13515612.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/rossiXYZ/p/13515612.html</guid>
<description>&lt;p&gt;在分析Alink源码的时候，发现Alink使用了 Java Stream，又去Flink源码搜索，发现Flink也有大量使用。一时兴起，想看看 Java Stream 和 Flink 这种流处理框架的异同点。当然这种比较还是注重于理念和设计思路上的。因为就应用领域和复杂程度来说， Java Stream 和 Flink 属于数量级别的差距。&lt;/p&gt;&lt;div id=&quot;cnblogs_post_body&quot; readability=&quot;613.17655475768&quot;&gt;


&lt;h2 id=&quot;0x00-摘要&quot;&gt;0x00 摘要&lt;/h2&gt;
&lt;p&gt;在分析Alink源码的时候，发现Alink使用了 Java Stream，又去Flink源码搜索，发现Flink也有大量使用。&lt;u&gt;一时兴起，想看看 Java Stream 和 Flink 这种流处理框架的异同点&lt;/u&gt;。当然这种比较还是注重于理念和设计思路上的。因为就应用领域和复杂程度来说， Java Stream 和 Flink 属于数量级别的差距。&lt;/p&gt;
&lt;p&gt;因为Flink的分析文章我写了一些，所以本文源码的分析重点在Java Stream上。&lt;/p&gt;
&lt;h2 id=&quot;0x01-领域&quot;&gt;0x01 领域&lt;/h2&gt;
&lt;h3 id=&quot;11-flink&quot;&gt;1.1 Flink&lt;/h3&gt;
&lt;p&gt;从几个权威来源可以看看Flink本质：&lt;/p&gt;
&lt;ul readability=&quot;2&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;我们直接从官网找出Flink本质：&lt;strong&gt;Apache Flink® — Stateful Computations over Data Streams&lt;/strong&gt;，即 &lt;strong&gt;数据流上的有状态计算&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;从github上看：Apache Flink is an open source stream processing framework with powerful stream- and batch-processing capabilities.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;从百度百科上看：Flink 其核心是用Java和Scala编写的分布式流数据流引擎。Flink以数据并行和流水线方式执行任意流数据程序，Flink的流水线运行时系统可以执行批处理和流处理程序。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;u&gt;因此可以总结如下：Flink 是分布式流数据计算，引擎，框架，系统，各种高大上 ……&lt;/u&gt; 。&lt;/p&gt;
&lt;h3 id=&quot;12-java-stream&quot;&gt;1.2 Java Stream&lt;/h3&gt;
&lt;p&gt;直接看 java doc&lt;/p&gt;
&lt;p&gt;Stream ：A sequence of elements supporting sequential and parallel aggregate operations.&lt;/p&gt;
&lt;p&gt;从其他网址看:&lt;/p&gt;
&lt;p&gt;Java 8 API 添加了一个新的抽象称为流Stream，可以让你以一种声明的方式处理数据。&lt;/p&gt;
&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;这种风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;u&gt;因此可以看到，Java Stream 是流抽象API，可以使用并行操作。&lt;/u&gt;&lt;/p&gt;
&lt;h3 id=&quot;13-探寻角度&quot;&gt;1.3 探寻角度&lt;/h3&gt;
&lt;p&gt;因此我们可以看出，&lt;strong&gt;Flink 和 Java Stream 最值得比较的三个方面就是：数据流模型，流水线，数据并行&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;下面我们就从这三个角度来分析。&lt;/p&gt;
&lt;h2 id=&quot;0x02-数据流模型&quot;&gt;0x02 数据流模型&lt;/h2&gt;
&lt;h3 id=&quot;21-java-stream&quot;&gt;2.1 Java Stream&lt;/h3&gt;
&lt;p&gt;Stream编程风格将要处理的元素集合看作一种流， 流在管道中传输， 并且可以在管道的节点上进行处理， 比如筛选， 排序，聚合等。&lt;/p&gt;
&lt;p&gt;元素流在管道中经过中间操作（intermediate operation）的处理，最后由最终操作(terminal operation)得到前面处理的结果。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;+--------------------+       +------+   +------+   +---+   +-------+
| stream of elements +-----&amp;gt; |filter+-&amp;gt; |sorted+-&amp;gt; |map+-&amp;gt; |collect|
+--------------------+       +------+   +------+   +---+   +-------+
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;以上的流程转换为 Java 代码为：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;List&amp;lt;Integer&amp;gt; transactionsIds = widgets.stream()
             .filter(b -&amp;gt; b.getColor() == RED)
             .sorted((x,y) -&amp;gt; x.getWeight() - y.getWeight())
             .mapToInt(Widget::getWeight)
             .sum();
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;22-flink&quot;&gt;2.2 Flink&lt;/h3&gt;
&lt;p&gt;官方样例如下&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;case class WordWithCount(word: String, count: Long)

val text = env.socketTextStream(host, port, '\n')

val windowCounts = text.flatMap { w =&amp;gt; w.split(&quot;\\s&quot;) }
  .map { w =&amp;gt; WordWithCount(w, 1) }
  .keyBy(&quot;word&quot;)
  .timeWindow(Time.seconds(5))
  .sum(&quot;count&quot;)

windowCounts.print()
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;23-分析&quot;&gt;2.3 分析&lt;/h3&gt;
&lt;p&gt;可以看出来，大家思路都很类似，就是&lt;u&gt;用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对运算和表达的高阶抽象。这种抽象其实在目前已经是很多框架和语言的必备了&lt;/u&gt;。用起来都很爽，调试起来都崩溃。&lt;/p&gt;
&lt;h2 id=&quot;0x03-流水线&quot;&gt;0x03 流水线&lt;/h2&gt;
&lt;p&gt;&lt;u&gt;本部分以 Java Stream为主，如果和Flink比较则会重点指出&lt;/u&gt;。&lt;/p&gt;
&lt;h3 id=&quot;31-总体对比&quot;&gt;3.1 总体对比&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 的流水线是在JVM内部，各种用户自定义函数都是在JVM中随意访问。&lt;/p&gt;
&lt;p&gt;而&lt;strong&gt;Flink&lt;/strong&gt;的流水线节点可能分布在不同机器的JVM上，用户jar包需要提交给不同的JVM。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt; 中的执行图可以分成四层：&lt;strong&gt;StreamGraph —&amp;gt; JobGraph —&amp;gt; ExecutionGraph -&amp;gt; 物理执行图&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;StreamGraph&lt;/strong&gt;：是对用户逻辑的映射，代表程序的拓扑结构，是根据用户通过 Stream API 编写的代码生成的最初的图。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;JobGraph&lt;/strong&gt;：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，&lt;strong&gt;将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ExecutionGraph&lt;/strong&gt;：JobManager 根据 JobGraph 生成ExecutionGraph。&lt;strong&gt;ExecutionGraph是JobGraph的并行化版本&lt;/strong&gt;，是调度层最核心的数据结构。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;物理执行图&lt;/strong&gt;：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 的流水线可以分为两层：&lt;strong&gt;Stage —&amp;gt; Sink&lt;/strong&gt;，即 &quot;流水线构建阶段&quot; 和 &quot;流水线执行阶段&quot;。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Stage&lt;/strong&gt;: Stage是概念上的构建。&lt;u&gt;此阶段类似于Flink的StreamGraph，每一个Stage相当于StreamNode&lt;/u&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sink&lt;/strong&gt;: Sink 接口是执行阶段用到的。&lt;u&gt;类似于Flink中的ExecutionGraph，每一个Sink相当于ExecutionVertex&lt;/u&gt;。&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;32-示例代码&quot;&gt;3.2 示例代码&lt;/h3&gt;
&lt;p&gt;这里的示例代码如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;import com.google.common.collect.Lists;
import java.util.List;
import java.util.stream.Collectors;

public class Java8Stream {
    public static void main(String[] args) {
        List&amp;lt;String&amp;gt; list = Lists.newArrayList(
                &quot;bcd&quot;, &quot;cde&quot;, &quot;def&quot;, &quot;abc&quot;);
        List&amp;lt;String&amp;gt; result = list.stream()
                .filter(e -&amp;gt; e.length() &amp;gt;= 3)
                .map(e -&amp;gt; e.charAt(0))
                .map(e -&amp;gt; String.valueOf(e))
                .collect(Collectors.toList());
        System.out.println(result);
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;33-stream操作分类&quot;&gt;3.3 Stream操作分类&lt;/h3&gt;
&lt;p&gt;Java Stream上的所有操作分为两类：中间操作和结束操作。&lt;u&gt;Flink算子其实也是这么区分，只不过没有像 Java Stream 这么做而已&lt;/u&gt;。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;中间操作只是一种标记，只有结束操作才会触发实际计算。中间操作又可以分为无状态的(&lt;em&gt;Stateless&lt;/em&gt;)和有状态的(&lt;em&gt;Stateful&lt;/em&gt;)，无状态中间操作是指元素的处理不受前面元素的影响，而有状态的中间操作必须等到所有元素处理之后才知道最终结果，比如排序是有状态操作，在读取所有元素之前并不能确定排序结果；&lt;/li&gt;
&lt;li&gt;结束操作又可以分为短路操作和非短路操作，短路操作是指不用处理全部元素就可以返回结果，比如&lt;em&gt;找到第一个满足条件的元素&lt;/em&gt;。之所以要进行如此精细的划分，是因为底层对每一种情况的处理方式不同。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;具体如下：&lt;/p&gt;
&lt;table readability=&quot;7.5&quot;&gt;&lt;tr&gt;&lt;td colspan=&quot;3&quot; align=&quot;center&quot; border=&quot;0&quot;&gt;Stream操作分类&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td rowspan=&quot;2&quot; border=&quot;1&quot;&gt;中间操作(Intermediate operations)&lt;/td&gt;
&lt;td&gt;无状态(Stateless)&lt;/td&gt;
&lt;td&gt;unordered() filter() map() mapToInt() mapToLong() mapToDouble() flatMap() flatMapToInt() flatMapToLong() flatMapToDouble() peek()&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;有状态(Stateful)&lt;/td&gt;
&lt;td&gt;distinct() sorted() sorted() limit() skip()&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td rowspan=&quot;2&quot; border=&quot;1&quot;&gt;结束操作(Terminal operations)&lt;/td&gt;
&lt;td&gt;非短路操作&lt;/td&gt;
&lt;td&gt;forEach() forEachOrdered() toArray() reduce() collect() max() min() count()&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;短路操作(short-circuiting)&lt;/td&gt;
&lt;td&gt;anyMatch() allMatch() noneMatch() findFirst() findAny()&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h3 id=&quot;34-stage&quot;&gt;3.4 Stage&lt;/h3&gt;
&lt;h4 id=&quot;341-分类&quot;&gt;3.4.1 分类&lt;/h4&gt;
&lt;p&gt;&lt;u&gt;源码中把 Stream 的一个操作称为一个 &lt;code&gt;stage&lt;/code&gt;，即由很多Stage构成了流水线&lt;/u&gt;。&lt;u&gt;Flink则要比它复杂很多&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;Stage包括三类Head，StatefulOp，StatelessOp，它们的继承链是这样的：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;Head -----&amp;gt; ReferencePipeline -----&amp;gt; AbstractPipeline -------&amp;gt; PipelineHelper
    
StatefulOp -----&amp;gt; ReferencePipeline -----&amp;gt; AbstractPipeline -------&amp;gt; PipelineHelper
    
StatelessOp -----&amp;gt; ReferencePipeline -----&amp;gt; AbstractPipeline -------&amp;gt; PipelineHelper
    
ReferencePipeline 继承了 AbstractPipeline 和 Stream   
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;PipelineHelper&lt;/strong&gt;主要用于Stream执行过程中相关结构的构建。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AbstractPipeline&lt;/strong&gt;是流水线的核心抽象类，用于构建和管理流水线。它的实现类就是流水线的节点。&lt;/p&gt;
&lt;p&gt;AbstractPipeline的直接实现类为ReferencePipeline，而Head 、StatefulOp 、StatelessOp又继承了ReferencePipeline类。因此Head / StatefulOp / StatelessOp 本身也是AbstractPipeline类型的。&lt;/p&gt;
&lt;h4 id=&quot;342-abstractpipeline&quot;&gt;3.4.2 AbstractPipeline&lt;/h4&gt;
&lt;p&gt;&lt;u&gt;AbstractPipeline是流水线的核心&lt;/u&gt;，每一个stage就是一个AbstractPipeline的实例，这里的每一个pipeline都是一个节点。AbstractPipeline中定义了三个AbstractPipeline类型的变量：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;sourceStage（源阶段），即保存的 &lt;code&gt;Head&lt;/code&gt; 头节点引用，用于获取保存在头节点关于整个 Stream 处理流程中的关键信息，如是否是并行模式；&lt;/li&gt;
&lt;li&gt;previousStage（上游pipeline，前一阶段），即当前中间操作节点的上一个节点，因为 &lt;code&gt;Head&lt;/code&gt; 为整个双向链表最上游，故其前一个节点为 null；&lt;/li&gt;
&lt;li&gt;nextStage（下一阶段）；&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;能看到 prev，next 这就是指向前后两个stage，用来构建一个双向链表。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;abstract class AbstractPipeline&amp;lt;E_IN, E_OUT, S extends BaseStream&amp;lt;E_OUT, S&amp;gt;&amp;gt; extends PipelineHelper&amp;lt;E_OUT&amp;gt; implements BaseStream&amp;lt;E_OUT, S&amp;gt; {
    private final AbstractPipeline sourceStage;
    private final AbstractPipeline previousStage;
    private AbstractPipeline nextStage;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其他比较重要的属性如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;&lt;li&gt;&lt;code&gt;sourceSpliterator&lt;/code&gt; 数据源的可分解迭代器，并行流中分解任务所需&lt;/li&gt;
&lt;li&gt;&lt;code&gt;depth&lt;/code&gt; 当前节点的深度，&lt;code&gt;Head&lt;/code&gt; 头节点深度为 0，该值在并行流大任务fork()分解子任务时可用于维护任务层级&lt;/li&gt;
&lt;li&gt;&lt;code&gt;parallel&lt;/code&gt; 是否是并行模式，决定了是否启用 &lt;code&gt;ForkJoinPool&lt;/code&gt; 用于并行执行任务&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;h3 id=&quot;35-流水线构建阶段&quot;&gt;3.5 流水线构建阶段&lt;/h3&gt;
&lt;p&gt;这部分只是&lt;u&gt;Stage这里这是概念上的构建。类似于Flink的StreamGraph&lt;/u&gt;。也为后续的运行做了准备。&lt;/p&gt;
&lt;p&gt;示例代码中，通过&lt;code&gt;Collection.stream()&lt;/code&gt;方法得到&lt;em&gt;Head&lt;/em&gt;也就是stage0，紧接着调用一系列的中间操作，不断产生新的Stream。&lt;strong&gt;这些Stream对象以双向链表的形式组织在一起，构成整个流水线，由于每个Stage都记录了前一个Stage和本次的操作以及回调函数，依靠这种结构就能建立起对数据源的所有操作&lt;/strong&gt;。这就是Stream记录操作的方式。&lt;/p&gt;
&lt;p&gt;每一步&lt;strong&gt;Stream&lt;/strong&gt;的方法调用都产生一个新的&lt;strong&gt;stage&lt;/strong&gt;，这些&lt;strong&gt;stage&lt;/strong&gt;会以&lt;strong&gt;双向链表&lt;/strong&gt;的方式链接，而每个&lt;strong&gt;stage&lt;/strong&gt;都记录了每一个阶段的操作，这样我们就可以依赖这种数据结构来保存对数据源的所有操作了。&lt;/p&gt;
&lt;h4 id=&quot;351-head&quot;&gt;3.5.1 Head&lt;/h4&gt;
&lt;p&gt;Head 用于表示第一个Stage，也就是source stage，调用诸如Collection.stream()方法产生的Stage，很显然这个Stage里不包含任何操作；&lt;u&gt;Head就类似于Flink的Source&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;从程序开始看起。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;list.stream()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;会调用到&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;default Stream&amp;lt;E&amp;gt; stream() {
        return StreamSupport.stream(spliterator(), false);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;然后会建立一个Stream，这个Stream就是一个 ReferencePipeline.Head。这里Head也是一个ReferencePipeline。 &lt;code&gt;static class Head&amp;lt;E_IN, E_OUT&amp;gt; extends ReferencePipeline&amp;lt;E_IN, E_OUT&amp;gt;&lt;/code&gt; 。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public static &amp;lt;T&amp;gt; Stream&amp;lt;T&amp;gt; stream(Spliterator&amp;lt;T&amp;gt; spliterator, boolean parallel) {
        return new ReferencePipeline.Head&amp;lt;&amp;gt;(spliterator,
                                           StreamOpFlag.fromCharacteristics(spliterator),
                                           parallel);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;352-中间操作&quot;&gt;3.5.2 中间操作&lt;/h4&gt;
&lt;p&gt;StatelessOp和StatefulOp分别表示无状态和有状态的Stage，对应于无状态和有状态的&lt;u&gt;中间操作&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;这种属于算子的逻辑概念，Flink对应的算子也具有类似的区别&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;示例代码中，可以看到 filter 返回了一个无状态stage，也是一个AbstractPipeline、stream，即是流水线的一个阶段。同时还实现了AbstractPipeline定义的opWrapSink方法。其重写的 &lt;code&gt;opWrapSink()&lt;/code&gt; 规定了该操作的下游操作的&lt;code&gt;Sink&lt;/code&gt;是如何组织数据处理逻辑的。&lt;/p&gt;
&lt;p&gt;后续的filter，map都分别构建了一个StatelessOp。这里需要注意的是：&lt;u&gt;每个StatelessOp都在其内部有opWrapSink函数，如果调用opWrapSink时候，就会生成一个Sink，其作用我们当分析到程序运行时候会讲解&lt;/u&gt;。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public final Stream&amp;lt;P_OUT&amp;gt; filter(Predicate&amp;lt;? super P_OUT&amp;gt; predicate) {
        Objects.requireNonNull(predicate);
    
        return new StatelessOp&amp;lt;P_OUT, P_OUT&amp;gt;(this, StreamShape.REFERENCE,
                                     StreamOpFlag.NOT_SIZED) {
            @Override
            Sink&amp;lt;P_OUT&amp;gt; opWrapSink(int flags, Sink&amp;lt;P_OUT&amp;gt; sink) {
                return new Sink.ChainedReference&amp;lt;P_OUT, P_OUT&amp;gt;(sink) {
                    @Override
                    public void begin(long size) {
                        downstream.begin(-1);
                    }

                    @Override
                    public void accept(P_OUT u) {
                        if (predicate.test(u))
                            downstream.accept(u);
                    }
                };
            }
        };
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这里要注意的是构建了一个双向链表。比如filter的构建最终调用到：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;AbstractPipeline(AbstractPipeline&amp;lt;?, E_IN, ?&amp;gt; previousStage, int opFlags) {
        previousStage.nextStage = this; // 构建双向链表
        this.previousStage = previousStage; // 构建双向链表
        this.sourceStage = previousStage.sourceStage;
        ......
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;所以我们最终得到的，用stage标识的流水线就是如下，注意这里都是双向链表。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;+------------+       
| Collection |
+------------+                  
      │ 
      │ stream()
      │ 
+------------+       
|    Head    |
+------------+     
      │ 
      │ filter()
      │
+-------------+       
| StatelessOp |
+-------------+     
      │ 
      │ map()
      │ 
+-------------+       
| StatelessOp |
+-------------+     
      │ 
      │ map()
      │  
+-------------+       
| StatelessOp |
+-------------+     
      │ 
      │ collect()
      │
+------------+       
| TerminalOp |
+------------+     
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;运行时的流水线摘录如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;this = {ReferencePipeline$2$1@767} 

 this$1 = {ReferencePipeline$2@688} 
  predicate = {Java8Stream$lambda@681} 
  this$0 = {ReferencePipeline$Head@682} 
  sourceStage = {ReferencePipeline$Head@682} 
  previousStage = {ReferencePipeline$Head@682} 
  sourceOrOpFlags = 128
  nextStage = {ReferencePipeline$3@691} 
  depth = 1
  combinedFlags = 159
  sourceSpliterator = null
  sourceSupplier = null
  linkedOrConsumed = true
  sourceAnyStateful = false
  sourceCloseAction = null
  parallel = false
    
 downstream = {ReferencePipeline$3$1@768} 
  this$1 = {ReferencePipeline$3@691} 
  downstream = {ReferencePipeline$3$1@770} 
   this$1 = {ReferencePipeline$3@694} 
   downstream = {ReduceOps$3ReducingSink@771} 
    supplier = {Collectors$lambda@772} 
    accumulator = {Collectors$lambda@773} 
    combiner = {Collectors$lambda@774} 
    state = {ArrayList@775}  size = 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;353-vs-flink-streamgraph&quot;&gt;3.5.3 vs Flink StreamGraph&lt;/h4&gt;
&lt;p&gt;&lt;u&gt;Java Stream相对简单，使用 Stage 一个数据结构就都搞定（比如双向链表本身就是Stage双向链表），而Flink则要复杂多了，比如：&lt;/u&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;StreamNode&lt;/strong&gt; 是用来描述 operator 的逻辑节点，并具有所有相关的属性，如并发度、入边和出边等。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;StreamEdge&lt;/strong&gt; 是用来描述两个 StreamNode（operator） 逻辑的链接边。&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;36-流水线执行阶段&quot;&gt;3.6 流水线执行阶段&lt;/h3&gt;
&lt;p&gt;因为Stream 是一个惰性求值的系统，所以直到当执行如下时候，才会进行最后求值。&lt;u&gt;这一步骤就相当于Flink程序需要加一个 print，env.execute 才能运行&lt;/u&gt;。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;.collect(Collectors.toList());
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;调用时候涉及到的部分调用栈如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;makeSink:180, ReduceOps$3 (java.util.stream)
makeSink:177, ReduceOps$3 (java.util.stream)
evaluateSequential:708, ReduceOps$ReduceOp (java.util.stream)
evaluate:234, AbstractPipeline (java.util.stream)
collect:499, ReferencePipeline (java.util.stream)
main:20, Java8Stream (com.alibaba.alink)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这就牵扯出来Java Stream的另外一部分操作：结束操作(Terminal operations)。&lt;/p&gt;
&lt;h4 id=&quot;361-terminalop&quot;&gt;3.6.1 TerminalOp&lt;/h4&gt;
&lt;p&gt;TerminalOp是流水线上的一个算子，其完成了最后的计算操作。在FindOp、ForEachOp、MatchOp 和 ReduceOp 中会覆盖其evaluateParallel函数。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;注意：终结操作不会添加节点&lt;/u&gt;。&lt;/p&gt;
&lt;h4 id=&quot;362-reduceop&quot;&gt;3.6.2 ReduceOp&lt;/h4&gt;
&lt;p&gt;ReduceOp是TerminalOp的一个具体实现，其执行了一个reduce操作。可以看到 &lt;u&gt;makeSink()&lt;/u&gt; 这里做了一个Sink。&lt;/p&gt;
&lt;p&gt;每个Stage都会将自己的操作封装到一个Sink里，前一个Stage只需调用后一个Stage的&lt;code&gt;accept()&lt;/code&gt;方法即可，并不需要知道其内部是如何处理的。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;private static abstract class ReduceOp&amp;lt;T, R, S extends AccumulatingSink&amp;lt;T, R, S&amp;gt;&amp;gt;
            implements TerminalOp&amp;lt;T, R&amp;gt; {
        private final StreamShape inputShape;

        public abstract S makeSink();

        @Override
        public &amp;lt;P_IN&amp;gt; R evaluateSequential(PipelineHelper&amp;lt;T&amp;gt; helper,
                                           Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
            return helper.wrapAndCopyInto(makeSink(), spliterator).get();
        }

        @Override
        public &amp;lt;P_IN&amp;gt; R evaluateParallel(PipelineHelper&amp;lt;T&amp;gt; helper,
                                         Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
            return new ReduceTask&amp;lt;&amp;gt;(this, helper, spliterator).invoke().get();
        }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;363-sink&quot;&gt;3.6.3 Sink&lt;/h4&gt;
&lt;p&gt;Sink接口是执行阶段用到的。&lt;u&gt;类似于Flink中的ExecutionVertex&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;在上一步已经在stage中记录了每一步操作，此时并没有执行。但是stage只是保存了当前的操作，并不能确定下一个stage需要何种操作，何种数据。&lt;/p&gt;
&lt;p&gt;JDK为此定义了&lt;u&gt;Sink接口来处理具体操作&lt;/u&gt;。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;interface Sink&amp;lt;T&amp;gt; extends Consumer&amp;lt;T&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Sink接口，其中只有begin()、end()、cancellationRequested()、accept()四个接口，其中间操作的子类中包含一个指向下游sink的指针。&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;方法名&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;6&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;void begin(long size)&lt;/td&gt;
&lt;td&gt;开始遍历元素之前调用该方法，通知Sink做好准备。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;void end()&lt;/td&gt;
&lt;td&gt;所有元素遍历完成之后调用，通知Sink没有更多的元素了。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;boolean cancellationRequested()&lt;/td&gt;
&lt;td&gt;是否可以结束操作，可以让短路操作尽早结束。&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;void accept(T t)&lt;/td&gt;
&lt;td&gt;遍历元素时调用，接受一个待处理元素，并对元素进行处理。Stage把自己包含的操作和回调方法封装到该方法里，前一个Stage只需要调用当前Stage.accept(T t)方法就行了。&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h4 id=&quot;364-构建执行链&quot;&gt;3.6.4 构建执行链&lt;/h4&gt;
&lt;p&gt;具体以ReduceOp为例，执行关键其实就是调用到了 &lt;code&gt;AbstractPipeline#wrapAndCopyInto()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public &amp;lt;P_IN&amp;gt; R evaluateSequential(PipelineHelper&amp;lt;T&amp;gt; helper,
                                   Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
        return helper.wrapAndCopyInto(makeSink(), spliterator).get();
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;wrapAndCopyInto其实现如下，正如其名字示意，主要包含了两个步骤：&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;code&gt;wrapSink()&lt;/code&gt; 从操作链表的尾部开始，调用操作对象自身重写的 opWrapSink()方法将每一个操作对象中的数据处理逻辑封装成 Sink.ChainedReference，并将传入的 Sink 作为新建 Sink 的 downStream，从而形成单向调用链。&lt;u&gt;这部分属于构建阶段&lt;/u&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;copyInto()&lt;/code&gt;从调用链头部开始执行中间操作数据处理逻辑封装成的 &lt;code&gt;Sink&lt;/code&gt; 对象的方法，完成对数据源的处理。&lt;u&gt;&lt;strong&gt;其实这部分就是执行阶段&lt;/strong&gt;&lt;/u&gt;。&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;final &amp;lt;P_IN, S extends Sink&amp;lt;E_OUT&amp;gt;&amp;gt; S wrapAndCopyInto(S sink, Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
        copyInto(wrapSink(Objects.requireNonNull(sink)), spliterator);
        return sink;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h5 id=&quot;3641-构建sink链条&quot;&gt;3.6.4.1 构建Sink链条&lt;/h5&gt;
&lt;p&gt;当终结操作触发时，以终结操作本身的数据处理逻辑的封装对象 &lt;code&gt;Sink&lt;/code&gt; 为起点，从操作链表尾部 &lt;code&gt;stage&lt;/code&gt; 逆向遍历，将操作动作中封装的数据处理逻辑封装成 &lt;code&gt;ChaineReference&lt;/code&gt; 对象，并将传入的上一个 &lt;code&gt;Sink 引用&lt;/code&gt;赋值给新建 Sink 的 downStream 变量，从而形成单向的调用链。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;+------+   downStream    +------+  downStream  +------+  downStream  +------+ 
| Sink +---------------&amp;gt; | Sink +------------&amp;gt; | Sink +------------&amp;gt; | Sink +
+------+                 +------+              +------+              +------+
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;从操作链表的尾部开始，调用操作对象自身重写的 opWrapSink()方法将每一个操作对象中的数据处理逻辑封装成 Sink.ChainedReference，并将传入的 Sink 作为新建 Sink 的 downStream，从而形成单向调用链。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;final &amp;lt;P_IN&amp;gt; Sink&amp;lt;P_IN&amp;gt; wrapSink(Sink&amp;lt;E_OUT&amp;gt; sink) {
        for ( @SuppressWarnings(&quot;rawtypes&quot;) AbstractPipeline p=AbstractPipeline.this; p.depth &amp;gt; 0; p=p.previousStage) {
            sink = p.opWrapSink(p.previousStage.combinedFlags, sink); // 从后往前处理
        }
        return (Sink&amp;lt;P_IN&amp;gt;) sink; // 返回单向调用链
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;以map算子为例，就是生成 return 了一个Sink.ChainedReference（其也是一个Sink），这些Sink最后会串联在一起，形成Sink链。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public final &amp;lt;R&amp;gt; Stream&amp;lt;R&amp;gt; map(Function&amp;lt;? super P_OUT, ? extends R&amp;gt; mapper) {
        Objects.requireNonNull(mapper);
        return new StatelessOp&amp;lt;P_OUT, R&amp;gt;(this, StreamShape.REFERENCE,
                  StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) {
            @Override
            Sink&amp;lt;P_OUT&amp;gt; opWrapSink(int flags, Sink&amp;lt;R&amp;gt; sink) {
                return new Sink.ChainedReference&amp;lt;P_OUT, R&amp;gt;(sink) { // 这里返回Sink
                    @Override
                    public void accept(P_OUT u) {
                        downstream.accept(mapper.apply(u));
                    }
                };
            }
        };
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ChainedReference包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;begin：在遍历元素前调用，做好遍历准备；&lt;/li&gt;
&lt;li&gt;accept：遍历每个元素的时候调用，包含每个stage的操作和回掉函数；&lt;/li&gt;
&lt;li&gt;end：遍历结束后调用&lt;/li&gt;
&lt;li&gt;cancellationRequested：是否能够尽早结束遍历，用于短路操作&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;每个stage都把操作实现在Sink里，上游stage调用下游stage的accept方法，达到按顺序执行每个操作的目的。&lt;/p&gt;
&lt;p&gt;可以看到调用完成之后&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;sink = {ReferencePipeline$2$1@741} 
     this$1 = {ReferencePipeline$2@687} 
     downstream = {ReferencePipeline$3$1@742} 
          this$1 = {ReferencePipeline$3@693} 
          downstream = {ReferencePipeline$3$1@739} 
               this$1 = {ReferencePipeline$3@714} 
               downstream = {ReduceOps$3ReducingSink@735} 
                    supplier = {Collectors$lambda@723} 
                    accumulator = {Collectors$lambda@722} 
                    combiner = {Collectors$lambda@721} 
                    state = null
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;从结束操作的&lt;strong&gt;sink&lt;/strong&gt;开始，一层一层包装&lt;strong&gt;sink&lt;/strong&gt;，最后第一个中间操作的&lt;strong&gt;sink&lt;/strong&gt;在最外层，在每个操作的&lt;strong&gt;opWrapSink&lt;/strong&gt;方法里返回的&lt;strong&gt;sink&lt;/strong&gt;都维护了一个&lt;strong&gt;downstream&lt;/strong&gt;指向后一个操作，这样，双向链表的结构就完成了。这样我们在&lt;strong&gt;copyInto&lt;/strong&gt;方法里调用&lt;strong&gt;begin&lt;/strong&gt;、&lt;strong&gt;accept&lt;/strong&gt;、&lt;strong&gt;end&lt;/strong&gt;的时候就会通过&lt;strong&gt;downstream&lt;/strong&gt;一层一层的调用下去，最终在结束操作执行实际计算。&lt;/p&gt;
&lt;h4 id=&quot;365-vs-flink-executiongraph&quot;&gt;3.6.5 vs Flink ExecutionGraph&lt;/h4&gt;
&lt;p&gt;Flink要复杂太多。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ExecutionGraph：&lt;/strong&gt; JobManager根据JobGraph生成ExecutionGraph.ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。下面只列举和 Java Stream大致能对应的模块。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;ExecutionJobVertex：和JobGraph中的JobVertex一一对应。每一个ExecutionJobVertex都有和并发度一样多的ExecutionVertex。&lt;/li&gt;
&lt;li&gt;ExecutionVertex：表示ExecutionJobVertex的其中一个并发子任务，输入是ExecutionEdge，输出是IntermediateResultPartition。&lt;u&gt;一个JobVertex/ExecutionJobVertex代表的是一个operator，而具体的ExecutionVertex则代表了每一个Task。&lt;/u&gt;&lt;/li&gt;
&lt;li&gt;IntermediateResult：和JobGraph中的IntermediateDataSet一一对应每一个IntermediateResult有与下游ExecutionJobVertex相同并发数的IntermediateResultPartition。&lt;/li&gt;
&lt;li&gt;IntermediateResultPartition：表示ExecutionVertex的一个输出分区，生产者是ExecutionVertex，消费者是若干个ExecutionEdge。&lt;/li&gt;
&lt;li&gt;ExecutionEdge：表示ExecutionVertex的输入，源是IntermediateResultPartition，目标是ExecutionVertex.source和目标都只能是一个。&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;37-执行调用sink链&quot;&gt;3.7 执行调用Sink链&lt;/h4&gt;
&lt;h4 id=&quot;371-调用执行&quot;&gt;3.7.1 调用执行&lt;/h4&gt;
&lt;p&gt;有了Sink对操作的包装，Stage之间的调用问题就解决了，&lt;u&gt;执行时只需要从流水线的head开始对数据源依次调用每个Stage对应的Sink.{ begin(), accept(), cancellationRequested(), end() }方法就可以了&lt;/u&gt;。一种可能的Sink.accept()方法流程是这样的：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-Java&quot;&gt;void accept(U u){
    1. 使用当前Sink包装的回调函数处理 u
    2. 将处理结果传递给流水线下游的Sink
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Sink接口的其他几个方法也是按照这种[处理-&amp;gt;转发]的模型实现。&lt;/p&gt;
&lt;p&gt;这就是我们前面提到的 &quot;&lt;u&gt;copyInto() 从调用链头部开始执行中间操作数据处理逻辑封装成的 Sink 对象的方法，完成对数据源的处理&lt;/u&gt;&quot;，具体调用如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;final &amp;lt;P_IN&amp;gt; void copyInto(Sink&amp;lt;P_IN&amp;gt; wrappedSink, Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
        if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) {
            wrappedSink.begin(spliterator.getExactSizeIfKnown());
            spliterator.forEachRemaining(wrappedSink);  // 调用到这里
            wrappedSink.end();
        }
        else {
            copyIntoWithCancel(wrappedSink, spliterator);
        }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;会调用到 ArrayListSpliterator 的 forEachRemaining。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public void forEachRemaining(Consumer&amp;lt;? super E&amp;gt; action) {
         for (; i &amp;lt; hi; ++i) {
              @SuppressWarnings(&quot;unchecked&quot;) E e = (E) a[i];
              action.accept(e);
         }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;以map算子为例，就是调用到了之前生成的 Sink.ChainedReference（其也是一个Sink）中的 accept 函数，执行本算子的业务操作，然后传递给下游stream调用。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public final &amp;lt;R&amp;gt; Stream&amp;lt;R&amp;gt; map(Function&amp;lt;? super P_OUT, ? extends R&amp;gt; mapper) {
        Objects.requireNonNull(mapper);
        return new StatelessOp&amp;lt;P_OUT, R&amp;gt;(this, StreamShape.REFERENCE,
                  StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) {
            @Override
            Sink&amp;lt;P_OUT&amp;gt; opWrapSink(int flags, Sink&amp;lt;R&amp;gt; sink) {
                return new Sink.ChainedReference&amp;lt;P_OUT, R&amp;gt;(sink) {
                    @Override
                    public void accept(P_OUT u) {
                        downstream.accept(mapper.apply(u)); // 调用到下游算子
                    }
                };
            }
        };
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;具体调用栈如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;lambda$main$0:14, Java8Stream (com.alibaba.alink)
test:-1, 13138721 (com.alibaba.alink.Java8Stream$$Lambda$1)
accept:174, ReferencePipeline$2$1 (java.util.stream)
forEachRemaining:1374, ArrayList$ArrayListSpliterator (java.util)
copyInto:481, AbstractPipeline (java.util.stream)
wrapAndCopyInto:471, AbstractPipeline (java.util.stream)
evaluateSequential:708, ReduceOps$ReduceOp (java.util.stream)
evaluate:234, AbstractPipeline (java.util.stream)
collect:499, ReferencePipeline (java.util.stream)
main:20, Java8Stream (com.alibaba.alink)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;372-vs-flink-物理执行图&quot;&gt;3.7.2 vs Flink 物理执行图&lt;/h4&gt;
&lt;p&gt;这里也只列举大致可对应或者可参考的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;物理执行图：&lt;/strong&gt; JobManager根据ExecutionGraph对工作进行调度后，在各个TaskManager上部署任务后形成的“图”，并不是一个具体的数据结构。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;任务：执行被调度后在分配的TaskManager中启动对应的Task。Task包裹了具有用户执行逻辑的运算符。&lt;/li&gt;
&lt;li&gt;ResultPartition：代表由一个任务的生成的数据，和ExecutionGraph中的IntermediateResultPartition一一对应。&lt;/li&gt;
&lt;li&gt;ResultSubpartition：是ResultPartition的一个子分区。每个ResultPartition包含多个ResultSubpartition，其数目要由下游消费任务数和DistributionPattern来决定。&lt;/li&gt;
&lt;li&gt;InputGate：代表任务的输入封装，和JobGraph中JobEdge一一对应每个InputGate消费了一个或多个的ResultPartition。&lt;/li&gt;
&lt;li&gt;InputChannel：每个InputGate会包含一个以上的InputChannel，和ExecutionGraph中的ExecutionEdge一一对应，也和ResultSubpartition一对一地相连，即一个InputChannel接收一个ResultSubpartition的输出。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Flink会根据ExecutionJobVertices的数量创建异步任务。并且给每个ExecutionJobVertices分配适当的slot，然后调用execution.deploy();方法。&lt;/p&gt;
&lt;p&gt;从Execution Graph到真正物理执行图转换时候，会将IntermediateResultPartition转化成ResultPartition，ExecutionEdge转成InputChannelDeploymentDescriptor（最终会在执行时转化成InputGate）。&lt;/p&gt;
&lt;p&gt;最后通过RPC方法提交task，实际会调用到&lt;code&gt;TaskExecutor.submitTask&lt;/code&gt;方法中。这个方法会创建真正的Task，然后调用&lt;code&gt;task.startTaskThread();&lt;/code&gt;开始task的执行。&lt;/p&gt;
&lt;h2 id=&quot;0x04-数据并行&quot;&gt;0x04 数据并行&lt;/h2&gt;
&lt;h3 id=&quot;41-对比&quot;&gt;4.1 对比&lt;/h3&gt;
&lt;h4 id=&quot;411-范畴&quot;&gt;4.1.1 范畴&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 的并行指的是在JVM内部并行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt; 并行的范畴就大得多。首先Task Manager是JVM层级，在Task Manager内部又有多个slot任务槽可以并行。其次多个Task Manager即可在同一个机器上，也可以在不同机器上。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Flink&lt;/code&gt;中的执行资源是通过任务槽定义。每个&lt;code&gt;TaskManager&lt;/code&gt;都有一个或多个任务槽，每个任务槽可以运行一个并行任务的流水线(pipeline)。流水线由多个连续的任务组成，例如 &lt;code&gt;MapFunction&lt;/code&gt; 的第n个并行实例和 &lt;code&gt;ReduceFunction&lt;/code&gt; 的第n个并行实例。&lt;/p&gt;
&lt;p&gt;所以Flink 并行的范畴包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;JVM内部Slot概念。&lt;/li&gt;
&lt;li&gt;同一个机器的JVM之间。&lt;/li&gt;
&lt;li&gt;不同机器的JVM之间。&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;412-并行度影响因素&quot;&gt;4.1.2 并行度影响因素&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 并行流内部使用了默认的ForkJoinPool线程池，所以它默认的线程数量就是处理器的数量，通过&lt;code&gt;Runtime.getRuntime().availableProcessors()&lt;/code&gt;可以得到这个值。如果需修改则需设置-Djava.util.concurrent.ForkJoinPool.common.parallelism=xxx。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt; 并行度具体设置取决于部署模式。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;如果Standalone模式，则并行度是通过配置来调整。&lt;/li&gt;
&lt;li&gt;如果是Yarn来控制资源调度，则Flink on YARN时的容器数量——亦即TaskManager数量——将由程序的并行度自动推算。&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;42-java-stream实现&quot;&gt;4.2 Java Stream实现&lt;/h3&gt;
&lt;p&gt;parallelStream是一个并行执行的流，其使用 fork/join （ForkJoinPool）并行方式来拆分任务和加速处理过程。研究parallelStream之前，搞清楚ForkJoinPool是很有必要的。&lt;/p&gt;
&lt;p&gt;ForkJoinPool的核心是采用分治法的思想，将一个大任务拆分为若干互不依赖的子任务，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务。&lt;/p&gt;
&lt;p&gt;同时，为了最大限度地提高并行处理能力，采用了工作窃取算法来运行任务，也就是说当某个线程处理完自己工作队列中的任务后，尝试当其他线程的工作队列中窃取一个任务来执行，直到所有任务处理完毕。所以为了减少线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。&lt;/p&gt;
&lt;p&gt;我们修改下代码，增加 &lt;code&gt;.parallel()&lt;/code&gt; 调用，这样就从串行进化成了并行。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;import com.google.common.collect.Lists;
import java.util.List;
import java.util.stream.Collectors;

public class Java8Stream {
    public static void main(String[] args) {
        List&amp;lt;String&amp;gt; list = Lists.newArrayList(
                &quot;bcd&quot;, &quot;cde&quot;, &quot;def&quot;, &quot;abc&quot;);
        List&amp;lt;String&amp;gt; result = list.stream()
                .parallel()
                .filter(e -&amp;gt; e.length() &amp;gt;= 3)
                .map(e -&amp;gt; e.charAt(0))
                .map(e -&amp;gt; String.valueOf(e))
                .collect(Collectors.toList());
        System.out.println(result);
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;AbstractPipeline 中能看到，就是标记个并行的标记，设置为true。sourceStage其实就是自身代表的算子。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;private boolean parallel;
private final AbstractPipeline sourceStage;

public final S parallel() {
        sourceStage.parallel = true;
        return (S) this;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;程序是在collect处开始执行的。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;public final &amp;lt;R, A&amp;gt; R collect(Collector&amp;lt;? super P_OUT, A, R&amp;gt; collector) {
        A container;
        container = evaluate(ReduceOps.makeRef(collector));
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;执行时候如果设置了并行，就会并行调用。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;    final &amp;lt;R&amp;gt; R evaluate(TerminalOp&amp;lt;E_OUT, R&amp;gt; terminalOp) {
        assert getOutputShape() == terminalOp.inputShape();
        if (linkedOrConsumed)
            throw new IllegalStateException(MSG_STREAM_LINKED);
        linkedOrConsumed = true;

        return isParallel()
               ? terminalOp.evaluateParallel(this, sourceSpliterator(terminalOp.getOpFlags()))
               : terminalOp.evaluateSequential(this, sourceSpliterator(terminalOp.getOpFlags()));
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;evaluateParallel 此处并行调用主要是通过 ReduceOp ---&amp;gt; ReduceTask 来完成的。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;private static abstract class ReduceOp&amp;lt;T, R, S extends AccumulatingSink&amp;lt;T, R, S&amp;gt;&amp;gt;        implements TerminalOp&amp;lt;T, R&amp;gt; {
        @Override
        public &amp;lt;P_IN&amp;gt; R evaluateParallel(PipelineHelper&amp;lt;T&amp;gt; helper,
                                         Spliterator&amp;lt;P_IN&amp;gt; spliterator) {
            return new ReduceTask&amp;lt;&amp;gt;(this, helper, spliterator).invoke().get();
        }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这时候会发现，该方法中new了一个ReduceTask类，然后调用了它的invoke()方法，看看ReduceTask类相关信息，最后会发现它的继承链是这样的：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;ReduceTask -----&amp;gt; AbstractTask -----&amp;gt; CountedCompleter -------&amp;gt; ForkJoinTask
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;可以看出所有的Task 都继承自Jdk7 中引入的ForkJoin 并行框架的ForkJoinTask。所以我们可以看出Stream 的并行是依赖于ForkJoin 框架的。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;abstract class AbstractTask&amp;lt;P_IN, P_OUT, R, K extends AbstractTask&amp;lt;P_IN, P_OUT, R, K&amp;gt;&amp;gt;        extends CountedCompleter&amp;lt;R&amp;gt; {
    @Override
    public void compute() {
        Spliterator&amp;lt;P_IN&amp;gt; rs = spliterator, ls; // right, left spliterators
        long sizeEstimate = rs.estimateSize();
        long sizeThreshold = getTargetSize(sizeEstimate);
        boolean forkRight = false;
        @SuppressWarnings(&quot;unchecked&quot;) K task = (K) this;
        while (sizeEstimate &amp;gt; sizeThreshold &amp;amp;&amp;amp; (ls = rs.trySplit()) != null) {
            K leftChild, rightChild, taskToFork;
            task.leftChild  = leftChild = task.makeChild(ls);
            task.rightChild = rightChild = task.makeChild(rs);
            task.setPendingCount(1);
            if (forkRight) {
                forkRight = false;
                rs = ls;
                task = leftChild;
                taskToFork = rightChild;
            }
            else {
                forkRight = true;
                task = rightChild;
                taskToFork = leftChild;
            }
            taskToFork.fork();
            sizeEstimate = rs.estimateSize();
        }
        task.setLocalResult(task.doLeaf());
        task.tryComplete();
    }    
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;调用栈是：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-java&quot;&gt;compute:297, AbstractTask (java.util.stream)
exec:731, CountedCompleter (java.util.concurrent)
doExec:289, ForkJoinTask (java.util.concurrent)
doInvoke:401, ForkJoinTask (java.util.concurrent)
invoke:734, ForkJoinTask (java.util.concurrent)
evaluateParallel:714, ReduceOps$ReduceOp (java.util.stream)
evaluate:233, AbstractPipeline (java.util.stream)
collect:499, ReferencePipeline (java.util.stream)
main:20, Java8Stream (com.alibaba.alink)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这里面的主要逻辑就是&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;先调用当前splititerator 方法的estimateSize 方法，预估这个分片中的数据量。&lt;/li&gt;
&lt;li&gt;根据预估的数据量获取最小处理单元的大小阈值，即当数据量已经小于这个阈值的时候进行计算，否则进行fork 将任务划分成更小的数据块，进行求解。这里值得注意的是，getTargetSize 在第一次调用的时候会设置 预测数据量大小 / (默认并发度 * 4) 的结果作为最小执行单元的数量（配置的默认值是cpu 数 – 1，可以通过java.util.concurrent.ForkJoinPool.common.parallelism设置）。&lt;/li&gt;
&lt;li&gt;如果当前分片大小仍然大于处理数据单元的阈值，且分片继续尝试切分成功，那么就继续切分，分别将左右分片的任务创建为新的Task，并且将当前的任务关联为两个新任务的父级任务（逻辑在makeChild 里面）。&lt;/li&gt;
&lt;li&gt;先后对左右子节点的任务进行fork，对另外的分区进行分解。同时设定pending 为1，这代表一个task 实际上只会有一个等待的子节点（被fork）。&lt;/li&gt;
&lt;li&gt;当任务已经分解到足够小的时候退出循环，尝试进行结束。调用子类实现的doLeaf方法，完成最小计算单元的计算任务，并设置到当前任务的localResult中。&lt;/li&gt;
&lt;li&gt;调用tryComplete 方法进行最终任务的扫尾工作，如果该任务pending 值不等于0，则原子的减1，如果已经等于0，说明任务都已经完成，则调用onCompletion 回调，如果该任务是叶子任务，则直接销毁中间数据结束；如果是中间节点会将左右子节点的结果进行合并。&lt;/li&gt;
&lt;li&gt;检查如果这个任务已经没有父级任务了，则将该任务置为正常结束，如果还有则尝试递归的去调用父级节点的onCompletion回调，逐级进行任务的合并。&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;0x05-java-stream在flink的应用&quot;&gt;0x05 Java Stream在Flink的应用&lt;/h2&gt;
&lt;p&gt;我从Flink的各个release版本找了下，发现Flink从1.5开始才引入 Java Stream，源码中&lt;u&gt;只有三处使用到&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;但是&lt;u&gt;最新的代码则有几百处调用&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;这说明起初，Flink开发者中大概只有一个兄弟一时兴起实验了 Java Stream，结果发现很好用，就陆续推广开来。&lt;/p&gt;
&lt;p&gt;我们还要发现，Flink在 Java Stream 的用法上，并没有使用其并行版本。&lt;/p&gt;
&lt;p&gt;个人觉得，Flink框架中使用 Java Stream 的并行版本对于框架性能提高意义不大，反而会造成调试差错的难度（ 需要时刻考虑线程安全问题。否则可能造成程序死锁，或数据的准确性。造成的后果完全取决于使用非线程安全类的效果 ），所以Flink没有使用其并行版本。但是用户在自己代码中可以使用其并行版本。&lt;/p&gt;
&lt;h2 id=&quot;0x06-总结&quot;&gt;0x06 总结&lt;/h2&gt;
&lt;p&gt;这里我们再总结下。&lt;strong&gt;Flink 和 Java Stream 最值得比较的三个方面就是：数据流模型，流水线，数据并行&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&quot;61-数据流模型&quot;&gt;6.1 数据流模型&lt;/h3&gt;
&lt;p&gt;大家思路都很类似，就是用一种类似用 SQL 语句从数据库查询数据的直观方式来提供一种对运算和表达的高阶抽象。这种抽象其实在目前已经是很多框架和语言的必备了。&lt;/p&gt;
&lt;h3 id=&quot;62-流水线&quot;&gt;6.2 流水线&lt;/h3&gt;
&lt;p&gt;Flink 中的执行图（ Flink这里形成了图结构 ）可以分成四层：&lt;strong&gt;StreamGraph —&amp;gt; JobGraph —&amp;gt; ExecutionGraph -&amp;gt; 物理执行图&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Java Stream 的流水线可以分为两层：&lt;strong&gt;Stage —&amp;gt; Sink&lt;/strong&gt;，即 &quot;流水线构建阶段&quot; 和 &quot;流水线执行阶段&quot;。&lt;/p&gt;
&lt;p&gt;Java Stream Stage部分只是&lt;u&gt;概念上的构建。类似于Flink的StreamGraph&lt;/u&gt;。&lt;u&gt;Head就类似于Flink的Source&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;Java Stream Sink 接口是执行阶段用到的。类似于Flink中的ExecutionGraph，每一个Sink相当于ExecutionVertex。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sink&lt;/strong&gt;: Sink 接口是执行阶段用到的。&lt;u&gt;类似于Flink中的ExecutionGraph，每一个Sink相当于ExecutionVertex&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;Java Stream 有无状态和有状态的&lt;u&gt;中间操作&lt;/u&gt;。&lt;u&gt;这种属于算子的逻辑概念，Flink对应的算子也具有类似的区别&lt;/u&gt;。&lt;/p&gt;
&lt;p&gt;因为 Java Stream 是一个惰性求值的系统，所以直到当执行如下时候，才会进行最后求值。&lt;u&gt;这一步骤就相当于Flink程序需要加一个 print，env.execute 才能运行&lt;/u&gt;。&lt;/p&gt;
&lt;h3 id=&quot;63-数据并行&quot;&gt;6.3 数据并行&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 的并行指的是在JVM内部并行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt; 并行的范畴就大得多。Flink的范畴包括：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;JVM内部Slot概念&lt;/li&gt;
&lt;li&gt;同一个机器的JVM之间&lt;/li&gt;
&lt;li&gt;不同机器的JVM之间&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Java Stream&lt;/strong&gt; 并行流内部使用了默认的ForkJoinPool线程池，所以它默认的线程数量就是处理器的数量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flink&lt;/strong&gt; 并行度具体设置取决于部署模式。&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;如果Standalone模式，则并行度是通过配置来调整。&lt;/li&gt;
&lt;li&gt;如果是Yarn来控制资源调度，则Flink on YARN时的容器数量——亦即TaskManager数量——将由程序的并行度自动推算。&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;0xff-参考&quot;&gt;0xFF 参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.runoob.com/java/java8-streams.html&quot;&gt;Java 8 Stream&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[&lt;a href=&quot;https://www.cnblogs.com/noteless/p/9505098.html&quot;&gt;三]java8 函数式编程Stream 概念深入理解 Stream 运行原理 Stream设计思路&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/qq_36372507/article/details/78946818&quot;&gt;java8学习总结——Stream的理解&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/lcgoing/article/details/87918010&quot;&gt;深入理解Java8中Stream的实现原理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jianshu.com/p/dd5fb725331b&quot;&gt;浅析Java8 Stream原理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/xiaoxiongcanguan/p/10511233.html&quot;&gt;java8 Stream的实现原理 (从零开始实现一个stream流)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/u013898617/article/details/79146389&quot;&gt;Java8 Stream 并行计算实现的原理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/Dorae/p/7779246.html&quot;&gt;java8Stream原理深度解析&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_38106322/article/details/107549552&quot;&gt;为什么说Java8的Stream并行流底层使用了Fork/Join框架&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_34092455/article/details/92738384&quot;&gt;记一次java8 parallelStream使用不当引发的血案&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/CarpenterLee/p/6637118.html&quot;&gt;深入理解Java Stream流水线&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000010521782&quot;&gt;java8 Stream Pipelines 浅析&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_45505313/article/details/106150967&quot;&gt;Java 8 Stream(2)-原理解析&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Mon, 17 Aug 2020 00:17:00 +0000</pubDate>
<dc:creator>罗西的思考</dc:creator>
<og:description>在分析Alink源码的时候，发现Alink使用了 Java Stream，又去Flink源码搜索，发现Flink也有大量使用。一时兴起，想看看 Java Stream 和 Flink 这种流处理框架的</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/rossiXYZ/p/13515612.html</dc:identifier>
</item>
<item>
<title>基于Prometheus和Grafana打造业务监控看板 - Catcher8</title>
<link>http://www.cnblogs.com/catcher1994/p/13513184.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/catcher1994/p/13513184.html</guid>
<description>&lt;p&gt;基于Prometheus和Grafana打造业务监控看板...&lt;/p&gt;&lt;div id=&quot;cnblogs_post_body&quot; readability=&quot;286.63593706873&quot;&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;业务监控对许许多多的场景都是十分有意义，业务监控看板可以让我们比较直观的看到当前业务的实时情况，然后运营人员可以根据这些情况及时对业务进行调整操作，避免业务出现大问题。&lt;/p&gt;
&lt;p&gt;老黄曾经遇到过一次比较尴尬的“事故”。&lt;/p&gt;
&lt;p&gt;其中一条业务线，服务着的其中一个商家，把大部分流量切到另外一个地方去了，而我们的运营人员在当天却是完全不知情，第二天看了昨天的统计报表之后才发现这个商家的量少了很多，才能跟进协调处理。&lt;/p&gt;
&lt;p&gt;ps: 当时实时报表比较欠缺，都是第二天凌晨生成昨天的数据报表，也没有告警机制。&lt;/p&gt;
&lt;p&gt;后面就弄了个大屏幕做了业务监控的实时看板，看一眼就知道有什么风吹草动了。&lt;/p&gt;
&lt;p&gt;先来看一下最终的效果图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171124537-837099042.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这个图里面主要包含了下面几个内容。&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;总的订单数量&lt;/li&gt;
&lt;li&gt;退单的数量&lt;/li&gt;
&lt;li&gt;创建订单的频率&lt;/li&gt;
&lt;li&gt;不同渠道的订单量&lt;/li&gt;
&lt;li&gt;不同渠道的退单量&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;下面就介绍一下如何实现这样的业务监控。&lt;/p&gt;
&lt;h2 id=&quot;搭建基础设施&quot;&gt;搭建基础设施&lt;/h2&gt;
&lt;p&gt;这里涉及的基础设施就有两个，一个是prometheus，另一个是grafana。&lt;/p&gt;
&lt;p&gt;先启动prometheus，这里直接用docker启动。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-ps&quot;&gt;$base = Split-Path -Parent $MyInvocation.MyCommand.Definition
$prometheusyml = Join-Path $base prometheus.yml
$fileconfig = Join-Path $base &quot;config&quot;

write-host $prometheusyml
write-host $fileconfig

docker run `
    --name prom `
    -p 9090:9090 `
    -v ${prometheusyml}:/etc/prometheus/prometheus.yml `
    -v ${fileconfig}:/etc/prometheus/fileconfig `
    prom/prometheus:v2.20.1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;下面是prometheus.yml&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-yml&quot;&gt;global:
  scrape_interval:     15s 
  evaluation_interval: 15s
  
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      # - alertmanager:9093
      
rule_files:

scrape_configs:
  - job_name: 'file_ds'
    file_sd_configs:
    - refresh_interval: 10s
      files:
      - ./fileconfig/*.yml
&lt;/code&gt;
&lt;/pre&gt;
&lt;blockquote readability=&quot;3.0674157303371&quot;&gt;
&lt;p&gt;这里用了基于文件的发现机制，没有用静态的。更多其他方式，参见 &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config&quot;&gt;https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个时候prometheus已经是运行起来了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171139267-1815434911.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;再来就是grafana了，启动这个更加简单。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-ps&quot;&gt;docker run -d --name grafana -p 3000:3000 grafana/grafana:7.1.3
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;执行完，访问 localhost:3000 就可以看到登录界面了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171149279-262448056.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;确定业务指标metrics&quot;&gt;确定业务指标(metrics)&lt;/h2&gt;
&lt;p&gt;确定指标可以说是整个业务监控中最最最最最为主要的一个环节了，只有明确了我们要监控什么，我们才可以在业务上去进行埋点，拿到想要的数据。&lt;/p&gt;
&lt;p&gt;这个其实和我们平时面对的需求是一个样的，需求明确了，做出来的东西才可能是我们想要的，需求不明确，做出来的东西可能就不会是我们想要的了。&lt;/p&gt;
&lt;p&gt;为了帮助大家简单的理解相关的内容，这里举个监控的例子，监控不同渠道的下单和退单量。&lt;/p&gt;
&lt;p&gt;涉及到量的，在一天内基本上是属于只增不减的，这个时候我们一般会选用 &lt;strong&gt;counter&lt;/strong&gt; 类型来处理。&lt;/p&gt;
&lt;p&gt;一个是下单，一个是退单，所以这里定义两个&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;yyyorder_created_total&lt;/li&gt;
&lt;li&gt;yyyorder_canceled_total&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;counter类型的，一般在命名的时候最好都用_total作为结尾。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;还有不同渠道呢？&lt;/p&gt;
&lt;p&gt;渠道我们就用 lable 来标识一下。&lt;/p&gt;
&lt;p&gt;最后展现格式大致如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;yyyorder_created_total{appkey=&quot;mt&quot;,opreator=&quot;cw&quot;} 1
yyyorder_canceled_total{appkey=&quot;pdd&quot;,opreator=&quot;cw&quot;} 2
&lt;/code&gt;
&lt;/pre&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;这里也要注意一个问题，确定指标的时候，也要避免定义太多指标出来，如果可以，考虑用label去进行区分同性质的内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;业务埋点&quot;&gt;业务埋点&lt;/h2&gt;
&lt;p&gt;在明确了业务指标之后，就要在对应的业务上去进行埋点操作，会对业务代码有一定的侵入性，当然如果业务代码写得足够好，耦合的东西少，或许可以借助AOP来埋点，从而降低侵入性。&lt;/p&gt;
&lt;p&gt;后面就写个简单的例子来模拟业务埋点这一块。&lt;/p&gt;
&lt;p&gt;创建一个ASP.NET Core的项目，并安装&lt;strong&gt;prometheus-net.AspNetCore&lt;/strong&gt;这个nuget包。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;ItemGroup&amp;gt;
    &amp;lt;PackageReference Include=&quot;prometheus-net.AspNetCore&quot; Version=&quot;3.6.0&quot; /&amp;gt;
&amp;lt;/ItemGroup&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其次是启用 ASP.NET Core exporter middleware&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-cs&quot;&gt;public void Configure(IApplicationBuilder app, IWebHostEnvironment env)
{
    if (env.IsDevelopment())
    {
        app.UseDeveloperExceptionPage();
    }

    app.UseRouting();
    app.UseAuthorization();

    app.UseEndpoints(endpoints =&amp;gt;
    {
        // 这一句。
        endpoints.MapMetrics();
        endpoints.MapControllers();
    });
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;最后就是埋点了。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-cs&quot;&gt;[ApiController]
[Route(&quot;&quot;)]
public class HomeController : ControllerBase
{
    private static readonly Counter OrderCreatedCount = Metrics
        .CreateCounter(&quot;yyyorder_created_total&quot;, &quot;Number of created orders.&quot;, new CounterConfiguration
        {
             LabelNames= new [] { &quot;appkey&quot;, &quot;opreator&quot; }
        });

    private static readonly Counter OrderCanceledCount = Metrics
        .CreateCounter(&quot;yyyorder_canceled_total&quot;, &quot;Number of canceled orders.&quot;, new CounterConfiguration
        {
            LabelNames = new[] { &quot;appkey&quot;, &quot;opreator&quot; }
        });

    [HttpGet]
    public string Get()
    {
        var appKeys = new[] { &quot;ali&quot;, &quot;pdd&quot;, &quot;mt&quot; };
        var opreators = new[] { &quot;cw&quot;, &quot;pz&quot; };

        var rd = new Random((int)DateTimeOffset.Now.ToUnixTimeMilliseconds()).Next(0, 2000);
        var appKeyidx = rd % 3;
        var opreatoidx = rd % 2;
        OrderCreatedCount.WithLabels(appKeys[appKeyidx], opreators[opreatoidx]).Inc();

        var cRd = new Random((int)DateTimeOffset.Now.ToUnixTimeMilliseconds()).NextDouble();

        if (cRd &amp;lt; 0.3d)
        {
            OrderCanceledCount.WithLabels(appKeys[appKeyidx], opreators[opreatoidx]).Inc();
        }

        return &quot;ok&quot;;
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面这个控制器中，创建了两个&lt;strong&gt;Counter&lt;/strong&gt;，就是上面确定业务指标中定义好的。&lt;/p&gt;
&lt;p&gt;这里是每访问一次，就创建一个订单，同时生成一个随机数，如果是小于0.3，那么就当它是退单的，这样就可以把两种指标都模拟出来了。&lt;/p&gt;
&lt;p&gt;程序刚启动是有部分默认指标的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171207157-1511278739.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;当我们访问埋点的地址后，可以发现我们自定义的业务指标也已经有数据了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171217948-1371730882.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;到这里，数据已经有了，我们要怎么呈现呢？&lt;/p&gt;
&lt;p&gt;要想呈现数据，我们需要先让prometheus来保存我们的业务指标数据。&lt;/p&gt;
&lt;h2 id=&quot;数据写入&quot;&gt;数据写入&lt;/h2&gt;
&lt;p&gt;把数据写入prometheus有两钟方式，一种是pull，一种是push。&lt;/p&gt;
&lt;p&gt;pull是让prometheus主动去拉取我们产生的数据，只要我们暴露一个地址出来即可，这中也是比较推荐的做法。&lt;/p&gt;
&lt;p&gt;push方式要借助pushgateway，埋点数据要先主动推送到pushgateway，后面在由pushgateway把数据写进prometheus。&lt;/p&gt;
&lt;p&gt;默认情况下，当我们用了&lt;code&gt;endpoints.MapControllers();&lt;/code&gt;之后，就会把数据暴露在 &lt;strong&gt;&lt;a href=&quot;http://ip&quot;&gt;http://ip&lt;/a&gt;:port/metrics&lt;/strong&gt; 这个地址上。&lt;/p&gt;
&lt;p&gt;知道要用pull的方式后，还要做什么呢？当然就是要去配置promethues了。&lt;/p&gt;
&lt;p&gt;前面我们的 &lt;code&gt;scrape_configs&lt;/code&gt; 是通过文件去自动发现的，所以只要在挂载的路径上面加一个对应的yml文件就可以了。&lt;/p&gt;
&lt;p&gt;老黄这里加了一个&lt;code&gt;nc-service.yml&lt;/code&gt;，具体内容如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-yml&quot;&gt;- labels:
    service: nc
    project: demo
  targets: 
  - 192.168.1.103:9874
  - 192.168.1.103:9875
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这个时候就可以在Targets里面看到我们这两个地址的信息了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171230918-1923297201.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;通过prometheus的默认界面，也可以发现数据已经正常读取了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171242503-1603359750.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;后面就是真正的数据查询和展示了。&lt;/p&gt;
&lt;h2 id=&quot;数据展示&quot;&gt;数据展示&lt;/h2&gt;
&lt;p&gt;通过上面的步骤，我们已经保证数据可以正常写入和查询了，现在就在grafana中创建一个业务监控看板了。&lt;/p&gt;
&lt;p&gt;在grafana中先配置我们的数据源。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171253401-1344691162.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这里填上我们prometheuse的地址保存就可以了，可以看到那个绿色的提示，告诉我们这个数据源是正常工作的了。&lt;/p&gt;
&lt;p&gt;先来一个总的订单数。&lt;/p&gt;
&lt;p&gt;创建一个新的dashboard，再创建一个Panel。&lt;/p&gt;
&lt;p&gt;我们在panel中填写我们的信息还有就是选择要的图形。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171309628-944402272.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;然后就是写上查询条件，就可以看到我们要的结果了。&lt;/p&gt;
&lt;p&gt;订单总数这个查询如下：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;sum(ceil(increase(yyyorder_created_total[1d])))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;里面用到了， sum、ceil、increase这三个函数。&lt;/p&gt;
&lt;p&gt;其中 &lt;strong&gt;increase&lt;/strong&gt; 是用来统计一段时间范围内的增量。后面带了 [1d] 这个范围表明这里是查看1天内的增量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ceil&lt;/strong&gt;是用来把increase的结果进行四舍五入的，可能有人会好奇，怎么还会要四舍五入呢？&lt;/p&gt;
&lt;p&gt;看看下面这个图，大家就会发现，非常多的小数点。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171326945-271560792.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;其实这个和prometheus的统计方法是有关系的，这里不展开，先这样用着。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;sum&lt;/strong&gt; 这个是用来求和的，指标中还包含了很多label，我们还要把每个label的求和，才是真正的结果。&lt;/p&gt;
&lt;p&gt;所以这里就得到了下面这个结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171338759-1901176339.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;退单总数的查询和订单总数一样，只是把名字换成退单的即可。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;sum(ceil(increase(yyyorder_canceled_total[1d])))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;再来看看各渠道的订单统计。&lt;/p&gt;
&lt;p&gt;既然是看各渠道的统计，那么这里就要用到前面定义好的label了。appkey代表的就是渠道，那么我们基于它去分一下组就可以了。&lt;/p&gt;
&lt;p&gt;就得到下面的查询。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;sum by (appkey) (ceil(increase (yyyorder_created_total[1d])))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171354341-605647121.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;同理，各渠道退单的也是一样的写法&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;sum by (appkey) (ceil(increase (yyyorder_canceled_total[1d])))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ps: 如果想把订单和退单的放在一个图里面，可以加多个查询。&lt;/p&gt;
&lt;p&gt;示例如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171410698-1711300149.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;现在有了所有渠道的总量，各个渠道独立的总量，那么我们有办法知道某个时间段的趋势吗？&lt;/p&gt;
&lt;p&gt;这个是肯定有的，且听老黄慢慢道来。&lt;/p&gt;
&lt;p&gt;有上面这个疑问，多半是经历过，某个时间段量非常多，但是有的时间段又几乎为零，玩的就是心跳呀。&lt;/p&gt;
&lt;p&gt;我们可以把这个称之为时间段内的订单增长情况。&lt;/p&gt;
&lt;p&gt;这里需要用到&lt;strong&gt;rate&lt;/strong&gt;函数，这个就是帮助我们统计增长速率的函数。&lt;/p&gt;
&lt;p&gt;它统计的是每秒的平均增长率，这个粒度有点太细，所以我们会在这个基础上乘以60，放大到一分钟。&lt;/p&gt;
&lt;p&gt;然后在看它sum的结果，最后才四舍五入。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;ceil(sum(rate(yyyorder_created_total[5m]) * 60))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200816171423610-872635277.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;p&gt;从这个结果可以看出，订单大部分时间都没有增长，只在中间那个时间段，有部分单进来。&lt;/p&gt;
&lt;p&gt;到这里主要的各个小panel已经完成了，剩下的就是在dashboard里面调整位置，大小那些了。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;这样打造出来的监控看板还是挺不错的，不过还是要注意下面几个问题的&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;prometheus是把数据存储在本地的，总是会达到上限的，这里要么是定期删，要么是写到远程存储。&lt;/li&gt;
&lt;li&gt;prometheus自己独立的查询语法可能刚开始会比较不适应，查不出自己想要的结果，这里多查查资料，多实践基本问题也不大。&lt;/li&gt;
&lt;li&gt;业务埋点这一个块，还是要尽可能的减少对现有业务代码的侵入性。&lt;/li&gt;
&lt;li&gt;业务指标一定要确定好，不然埋点痛苦，查询也痛苦。&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;这里还没有涉及到告警相关的内容，后面有时间再写一个告警相关的。&lt;/p&gt;
&lt;p&gt;文中示例代码：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/catcher1994/p/PromDemo&quot;&gt;https://github.com/catcherwong-archive/2020/tree/master/08/PromDemo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文首发于我的个人公众号 &lt;strong&gt;不才老黄&lt;/strong&gt; ，不定期发布一些内容，有兴趣的可以关注一下哟！&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2020.cnblogs.com/blog/558945/202008/558945-20200814201156451-840453165.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Mon, 17 Aug 2020 00:12:00 +0000</pubDate>
<dc:creator>Catcher8</dc:creator>
<og:description>基于Prometheus和Grafana打造业务监控看板...</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/catcher1994/p/13513184.html</dc:identifier>
</item>
<item>
<title>为什么？为什么？Java处理排序后的数组比没有排序的快？想过没有？ - 沉默王二</title>
<link>http://www.cnblogs.com/qing-gee/p/13515470.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/qing-gee/p/13515470.html</guid>
<description>&lt;blockquote readability=&quot;6.4637096774194&quot;&gt;
&lt;p&gt;先看再点赞，给自己一点思考的时间，微信搜索【&lt;strong&gt;沉默王二&lt;/strong&gt;】关注这个有颜值却假装靠才华苟且的程序员。&lt;br/&gt;本文 &lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href=&quot;https://github.com/itwanger/itwanger.github.io&quot;&gt;github.com/itwanger&lt;/a&gt; 已收录，里面还有我精心为你准备的一线大厂面试题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天周日，没什么重要的事情要做，于是我早早的就醒来了。看了一会渡边淳一的书，内心逐渐感到平静——心情不佳的时候，书好像是最好的药物。心情平静了，就需要做一些更有意义的事情——逛技术网站，学习精进。&lt;/p&gt;
&lt;p&gt;Stack Overflow 是我最喜欢逛的一个网站，它是我 Chrome 浏览器的第一个书签。里面有很多很多经典的问题，其中一些回答，剖析得深入我心。就比如说这个：“为什么处理排序后的数组比没有排序的快？”&lt;/p&gt;
&lt;p&gt;毫无疑问，直观印象里，排序后的数组处理起来就是要比没有排序的快，甚至不需要理由，就好像我们知道“夏天吃冰激凌就是爽，冬天穿羽绒服就是暖和”一样。&lt;/p&gt;
&lt;p&gt;但本着“知其然知其所以然”的态度，我们确实需要去搞清楚到底是为什么？&lt;/p&gt;
&lt;p&gt;来看一段 Java 代码：&lt;/p&gt;
&lt;pre readability=&quot;8&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;10&quot;&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;SortArrayFasterDemo&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; arraySize = &lt;span class=&quot;hljs-number&quot;&gt;32768&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; data[] = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt;[arraySize];&lt;p&gt;Random rnd = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Random(&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;);&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; c = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; c &amp;lt; arraySize; ++c) {&lt;br/&gt;data[c] = rnd.nextInt() % &lt;span class=&quot;hljs-number&quot;&gt;256&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;Arrays.sort(data);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; start = System.nanoTime();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; sum = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;; ++i)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; c = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; c &amp;lt; arraySize; ++c)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (data[c] &amp;gt;= &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) {&lt;br/&gt;sum += data[c];&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;System.out.println((System.nanoTime() - start) / &lt;span class=&quot;hljs-number&quot;&gt;1000000000.0&lt;/span&gt;);&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;sum = &quot;&lt;/span&gt; + sum);&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;这段代码非常简单，我来解释一下：&lt;/p&gt;
&lt;ul readability=&quot;2.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;声明一个指定长度（32768）的数组。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;声明一个 Random 随机数对象，种子是 0；&lt;code&gt;rnd.nextInt() % 256&lt;/code&gt; 将会产生一个余数，余数的绝对值在 0 到 256 之间，包括 0，不包括 256，可能是负数；使用余数对数组进行填充。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;使用 &lt;code&gt;Arrays.sort()&lt;/code&gt; 进行排序。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;通过 for 循环嵌套计算数组累加后的结果，并通过 &lt;code&gt;System.nanoTime()&lt;/code&gt; 计算前后的时间差，精确到纳秒级。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;我本机的环境是 Mac OS，内存 16 GB，CPU Intel Core i7，IDE 用的是 IntelliJ IDEA，排序后和未排序后的结果如下：&lt;/p&gt;
&lt;p&gt;排序后：2.811633398&lt;br/&gt;未排序：9.41434346&lt;/p&gt;
&lt;p&gt;时间差还是很明显的，对吧？未排序的时候，等待结果的时候让我有一种担心：什么时候结束啊？不会结束不了吧？&lt;/p&gt;
&lt;p&gt;读者朋友们有没有玩过火炬之光啊？一款非常经典的单机游戏，每一个场景都有一副地图，地图上有很多分支，但只有一个分支可以通往下一关；在没有刷图之前，地图是模糊的，玩家并不知道哪一条分支是正确的。&lt;/p&gt;
&lt;p&gt;如果侥幸跑的是一条正确的分支，那么很快就能到达下一关；否则就要往回跑，寻找正确的那条分支，需要花费更多的时间，但同时也会收获更多的经验和声望。&lt;/p&gt;
&lt;p&gt;作为一名玩过火炬之光很久的老玩家，几乎每一幅地图我都刷过很多次，刷的次数多了，地图差不多就刻进了我的脑袋，即便是一开始地图是模糊的，我也能凭借经验和直觉找到最正确的那条分支，就省了很多折返跑的时间。&lt;/p&gt;
&lt;p&gt;读者朋友们应该注意到了，上面的代码中有一个 if 分支——&lt;code&gt;if (data[c] &amp;gt;= 128)&lt;/code&gt;，也就是说，如果数组中的值大于等于 128，则对其进行累加，否则跳过。&lt;/p&gt;
&lt;p&gt;那这个代码中的分支就好像火炬之光中的地图分支，如果处理器能够像我一样提前预判，那累加的操作就会快很多，对吧？&lt;/p&gt;
&lt;p&gt;处理器的内部结构我是不懂的，但它应该和我的大脑是类似的，遇到 if 分支的时候也需要停下来，猜一猜，到底要不要继续，如果每次都猜对，那显然就不需要折返跑，浪费时间。&lt;/p&gt;
&lt;p&gt;这就是传说中的分支预测！&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;我需要刷很多次图才能正确地预测地图上的路线，处理器需要排序才能提高判断的准确率&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;计算机发展了这么多年，已经变得非常非常聪明，对于条件的预测通常能达到 90% 以上的命中率。但是，如果分支是不可预测的，那处理器也无能为力啊，对不对？&lt;/p&gt;
&lt;p&gt;排序后花费的时间少，未排序花费的时间多，罪魁祸首就在 if 语句上。&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java language-java hljs&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (data[c] &amp;gt;= &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) {&lt;br/&gt;sum += data[c];&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;数组中的值是均匀分布的（-255 到 255 之间），至于是怎么均匀分布的，我们暂且不管，反正由 Random 类负责。&lt;/p&gt;
&lt;p&gt;为了方便讲解，我们暂时忽略掉负数的那一部分，从 0 到 255 说起。&lt;/p&gt;
&lt;p&gt;来看经过排序后的数据：&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;code class=&quot;hljs r&quot; readability=&quot;2&quot;&gt;data[] = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;126&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;127&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;129&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;130&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;hljs-number&quot;&gt;250&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;251&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;252&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;&lt;br/&gt;branch = N  N  N  N  N  &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;   N    N    &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;    &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;    &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;  &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;    &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;    &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;  &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;&lt;p&gt;= NNNNNNNNNNNN &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt; NNNNNNNTTTTTTTTT &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt; TTTTTTTTTT&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;N 是小于 128 的，将会被 if 条件过滤掉；T 是将要累加到 sum 中的值。&lt;/p&gt;
&lt;p&gt;再来看未排序的数据：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs r&quot;&gt;data[] = &lt;span class=&quot;hljs-number&quot;&gt;226&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;185&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;125&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;158&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;198&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;144&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;217&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;79&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;202&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;118&lt;/span&gt;,  &lt;span class=&quot;hljs-number&quot;&gt;14&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;150&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;177&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;182&lt;/span&gt;, &lt;span class=&quot;hljs-number&quot;&gt;133&lt;/span&gt;, &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;&lt;br/&gt;branch =   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   N,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,  N,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   N,   N,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   &lt;span class=&quot;hljs-literal&quot;&gt;T&lt;/span&gt;,   N  &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;&lt;p&gt;= TTNTTTTNTNNTTTN &lt;span class=&quot;hljs-keyword&quot;&gt;...&lt;/span&gt;   &lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;完全没有办法预测。&lt;/p&gt;
&lt;p&gt;对比过后，就能发现，排序后的数据在遇到分支预测的时候，能够轻松地过滤掉 50% 的数据，对吧？是有规律可循的。&lt;/p&gt;
&lt;p&gt;那假如说不想排序，又想节省时间，有没有办法呢？&lt;/p&gt;
&lt;p&gt;如果你直接问我的话，我肯定毫无办法，两手一摊，一副无奈脸。不过，Stack Overflow 以上帝视角给出了答案。&lt;/p&gt;
&lt;p&gt;把：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java language-java hljs&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (data[c] &amp;gt;= &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) {&lt;br/&gt;sum += data[c];&lt;br/&gt;}&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;更换为：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;java language-java hljs&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; t = (data[c] - &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) &amp;gt;&amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;31&lt;/span&gt;;&lt;br/&gt;sum += ~t &amp;amp; data[c];&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;通过位运算消除了 if 分支（并不完全等同），但我测试了一下，计算后的 sum 结果是相同的。&lt;/p&gt;
&lt;pre readability=&quot;11.5&quot;&gt;
&lt;code class=&quot;java language-java hljs&quot; readability=&quot;17&quot;&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;SortArrayFasterDemo&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-function&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;hljs-params&quot;&gt;(String[] args)&lt;/span&gt; &lt;/span&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; arraySize = &lt;span class=&quot;hljs-number&quot;&gt;32768&lt;/span&gt;;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; data[] = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt;[arraySize];&lt;p&gt;Random rnd = &lt;span class=&quot;hljs-keyword&quot;&gt;new&lt;/span&gt; Random();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; c = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; c &amp;lt; arraySize; ++c) {&lt;br/&gt;data[c] = rnd.nextInt() % &lt;span class=&quot;hljs-number&quot;&gt;256&lt;/span&gt;;&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; start = System.nanoTime();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; sum = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;; ++i)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; c = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; c &amp;lt; arraySize; ++c)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (data[c] &amp;gt;= &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) {&lt;br/&gt;sum += data[c];&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;System.out.println((System.nanoTime() - start) / &lt;span class=&quot;hljs-number&quot;&gt;1000000000.0&lt;/span&gt;);&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;sum = &quot;&lt;/span&gt; + sum);&lt;/p&gt;&lt;p&gt;&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; start1 = System.nanoTime();&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;long&lt;/span&gt; sum1 = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;hljs-number&quot;&gt;100000&lt;/span&gt;; ++i)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; c = &lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;; c &amp;lt; arraySize; ++c)&lt;br/&gt;{&lt;br/&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;int&lt;/span&gt; t = (data[c] - &lt;span class=&quot;hljs-number&quot;&gt;128&lt;/span&gt;) &amp;gt;&amp;gt; &lt;span class=&quot;hljs-number&quot;&gt;31&lt;/span&gt;;&lt;br/&gt;sum1 += ~t &amp;amp; data[c];&lt;br/&gt;}&lt;br/&gt;}&lt;/p&gt;&lt;p&gt;System.out.println((System.nanoTime() - start1) / &lt;span class=&quot;hljs-number&quot;&gt;1000000000.0&lt;/span&gt;);&lt;br/&gt;System.out.println(&lt;span class=&quot;hljs-string&quot;&gt;&quot;sum1 = &quot;&lt;/span&gt; + sum1);&lt;br/&gt;}&lt;br/&gt;}&lt;br/&gt;&lt;/p&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;输出结果如下所示：&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs makefile&quot;&gt;8.734795196&lt;br/&gt;sum = 156871800000&lt;br/&gt;1.596423307&lt;br/&gt;sum1 = 156871800000&lt;br/&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;数组累加后的结果是相同的，但时间上仍然差得非常多，这说明时间确实耗在分支预测上——如果数组没有排序的话。&lt;/p&gt;
&lt;p&gt;最后，不得不说一句，大神级程序员不愧是大神级程序员，懂得位运算的程序员就是屌。&lt;/p&gt;
&lt;p&gt;建议还在读大学的读者朋友多读一读《计算机操作系统原理》这种涉及到底层的书，对成为一名优秀的程序员很有帮助。毕竟大学期间，学习时间充分，社会压力小，能够做到心无旁骛，加油！&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;我是沉默王二，一枚有颜值却假装靠才华苟且的程序员。&lt;strong&gt;关注即可提升学习效率，别忘了三连啊，点赞、收藏、留言，我不挑，奥利给🌹&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;注：如果文章有任何问题，欢迎毫不留情地指正。&lt;/p&gt;
&lt;p&gt;如果你觉得文章对你有些帮助，欢迎微信搜索「&lt;strong&gt;沉默王二&lt;/strong&gt;」第一时间阅读，回复关键字「&lt;strong&gt;小白&lt;/strong&gt;」可以免费获取我肝了 4 万+字的 《Java 小白从入门到放肆》2.0 版；本文 &lt;strong&gt;GitHub&lt;/strong&gt; &lt;a href=&quot;https://github.com/itwanger/itwanger.github.io&quot;&gt;github.com/itwanger&lt;/a&gt; 已收录，欢迎 star。&lt;/p&gt;
</description>
<pubDate>Sun, 16 Aug 2020 22:00:00 +0000</pubDate>
<dc:creator>沉默王二</dc:creator>
<og:description>先看再点赞，给自己一点思考的时间，微信搜索【沉默王二】关注这个有颜值却假装靠才华苟且的程序员。本文 GitHub github.com/itwanger 已收录，里面还有我精心为你准备的一线大厂面试题</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnblogs.com/qing-gee/p/13515470.html</dc:identifier>
</item>
</channel>
</rss>