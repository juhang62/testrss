<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>The Decline of Usability</title>
<link>https://datagubbe.se/decusab/</link>
<guid isPermaLink="true" >https://datagubbe.se/decusab/</guid>
<description>&lt;p&gt;In which we delve into the world of user interface design.&lt;/p&gt;
&lt;h2&gt;Our premise&lt;/h2&gt;
&lt;p&gt;There was a time (roughly between 1994 and 2012) when a reasonably computer-literate user could sit down in front of almost any operating system and quickly get to grips with the GUI, no matter what their home base was. Windows, MacOS, CDE, OpenStep, OS/2 and even outliers like Amiga, Atari and BeOS all had more in common than what set them apart. All windows had a title bar for dragging, easy identification and displaying current input focus. Clearly labeled drop-down menus following a set standard (File, Edit, View, Help etc.) made it easy for a newcomer to an application to browse program features and learn keyboard shortcuts. Buttons, input fields and other widget elements were clearly identifiable through various visual cues, such as 3D bevels.&lt;/p&gt;
&lt;p&gt;A few rogue applications didn't play by the rules, but the vast majority of software did, at least in the fundamental places that really mattered.&lt;/p&gt;
&lt;p&gt;Today, it seems we're on another track completely. Despite being endlessly fawned over by an army of professionals, Usability, or as it used to be called, &quot;User Friendliness&quot;, is steadily declining. During the last ten years or so, adhering to basic standard concepts seems to have fallen out of fashion. On comparatively new platforms, I.E. smartphones, it's inevitable: the input mechanisms and interactions with the display are so different from desktop computers that new paradigms are warranted.&lt;/p&gt;
&lt;p&gt;Worryingly, these paradigms have begun spreading to the desktop, where keyboards for fast typing and pixel-precision mice effectively render them pointless. Coupled with the flat design trend, UI elements are increasingly growing both bigger and yet somehow harder to locate and tell apart from non-interactive decorations and content.&lt;/p&gt;
&lt;p&gt;Overall, designers of desktop applications seem to have abandoned the fact that a desktop computer is capable of displaying several applications and windows at the same time and that many users are accustomed to this. Instead, we're increasingly treated to small-screen, single-app paradigms copied from smartphones. That's a turn for the worse in its own right, but perhaps more troubling and annoying is the recurring sidestepping from the tried and true UI design that is so ingrained in many users it's practically muscle memory by now.&lt;/p&gt;
&lt;h2&gt;Examples to prove a point&lt;/h2&gt;
&lt;h3&gt;Window Management&lt;/h3&gt;
&lt;p&gt;Consider these title bars of a few popular Windows 10 applications:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/windowtitles.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The image above is composed from screenshots taken on the same computer during the span of a few minutes. No settings have been changed between shots. Immediately, a plethora of UI problems become apparent.&lt;/p&gt;
&lt;p&gt;Can you even tell how many windows there are? The answer is six - although the top three and bottom two could, when ending up stacked like this, look as if they're two single applications.&lt;/p&gt;
&lt;p&gt;All of these title bars denote active windows. The top one, Outlook, looks exactly the same when inactive, as does Slack. Except for the command prompt (cmd.exe), the change between active and inactive on the remaining windows is so subtle that when aligned next to each other, it's virtually impossible to determine which one has the input focus.&lt;/p&gt;
&lt;p&gt;Almost all of the title bars contain some kind of UI widget. Some have little tool icons, some have tabs, some have drop-down menus, some have combinations thereof. There is no set behavior and, more importantly, the clickable area for traditional operations (move, focus, raise) on each title bar is now of variable width. If you're accustomed to a title bar being for handling the window and nothing else, it's very easy to misclick and activate an application feature you didn't intend to. Oh, and the little Visual Studio Code logo on the second title bar from the top? Nope, not an icon. Purely decorational.&lt;/p&gt;
&lt;p&gt;What's perhaps most jarring about this is that four of the six applications are made by Microsoft themselves, thus setting the standard for this kind of erratic design. We can already see the effects: the taking over of window title bars seems to get worse with time. Consider the latest version of Slack (click for a larger image):&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://datagubbe.se/decusab/slackhotspot.png&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/slackhotspot.png&quot; width=&quot;550&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since Windows 2 (not 2000 - I'm really talking about Windows 2), users have been able to resize windows by dragging their top border and corners. Not so with Slack, anymore. The red lines in the image denote the remaining hotspots available for resizing. The blue lines denote the remaining hotspots available for moving the window. The rest is now taken up by a non-standard concoction of widgets that most users will either soon learn keyboard shortcuts for or that could very easily be incorporated into a more traditional UI.&lt;/p&gt;
&lt;p&gt;Instead, some usability whizkid at some point decided to completely nullify the single most fundamental way of managing the window of an application mostly running on platforms where stacking, floating window management is not only the norm but pretty much the only available option.&lt;/p&gt;
&lt;h3&gt;Browsers&lt;/h3&gt;
&lt;p&gt;Microsoft and Slack aren't the only culprits. Google, for example, have gotten increasingly into some kind of A/B testing of late and their Chrome browser now features this type of tooltip when hovering on tabs:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/hovertab.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Usually, a browser tab will display a small, floating tooltip after having been hovered for a bit of time. This massive monstrosity pops up without delay and covers a large area of the underlying UI. The usefulness of browser tab tooltips can be discussed in itself, but this is no doubt both pointless and distracting.&lt;/p&gt;
&lt;p&gt;Google aren't the only ones capable of producing distracting UI:s, though. The newly released Firefox version 75 features what has become known as &quot;the megabar&quot;:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/megabar.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This new take on the URL bar pops up when you least expect it, is very hard to get rid of and, as a bonus, covers the tried, tested and highly usable bookmarks toolbar below it. Just like widgets in title bars, this breaks the behavior of a UI concept in such a major way it's hard to begin describing: text input fields are ubiquitous, ancient and their basic concept has been the same since at least the early 1980:s.&lt;/p&gt;
&lt;h3&gt;Scroll bars&lt;/h3&gt;
&lt;p&gt;Another blow against recognizability and usability is harder to take a screenshot of, namely auto-hiding scroll bars. On a smartphone, it's a great invention because you can free up real estate on a small display and you've usually got your thumb resting close to the screen, ready to do a test scroll in your full screen app to see if more content is available.&lt;/p&gt;
&lt;p&gt;On the desktop, however, a scroll bar is very useful for determining your current position in the content without having to break away from what you're presently doing and reach for the mouse. It's also useful for doing the same in windows that are currently not in focus. For example, in a tailing log file reader or command prompt with a debug stream, you might be interested in knowing if you're looking at the latest output or not. With auto-hiding scroll bars, this becomes much harder and you have to resort to other, often less apparent or more cumbersome methods.&lt;/p&gt;
&lt;p&gt;In lieu of screenshots of hidden scroll bars, let's look at how QT5 renders them by default:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/sbar.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Which part of this is the bar and which part is the tray? I know by now that the slightly brighter part is the bar, yet I frequently misclick, because the flat design makes them so hard to tell apart. Worse still is the infinitesimal contrast, so low that on older, cheaper laptop screens, it's downright impossible to tell the difference between bar and tray. New users probably don't know that with the right tools, QT5 can be configured to sport a more traditional look, so the default look should be geared towards them. Those intent on customizing the appearance of their personal desktop will usually find a way to do so anyway.&lt;/p&gt;
&lt;h3&gt;Missing Menu Bars&lt;/h3&gt;
&lt;p&gt;Another apparently unfashionable UI standard is the menu bar. It used to be a lowest common denominator between platforms and, when still present, it works basically the same on Windows, Mac and Unix-likes. For the most part, it even keeps the traditional &quot;File, Edit, View&quot; approach to things. The Gnome designers, however, have decided that such menus are apparently a bad feature and they should probably never have been used in the first place. To rectify more than three decades of such folly, they have created... something I'm not sure what to call.&lt;/p&gt;
&lt;p&gt;One of the tricks up their sleeve is the hamburger menu. On smartphones, it's a great feature, but on the desktop, it's unnecessary: If there's anything we have on today's wide screen displays, it's horizontal space. In Gnome, it seems to be a catch-all for UI operations that didn't end up somewhere else. Like in Evince:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/ev_menu3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Open, Save, Print, Close. All of them reasonable tasks, except there's no adherence to standards. In Gnome-MPV, the hamburger menu looks like this:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/gmpv_menu3.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;No Open or Close here, you silly user! What did you expect? Some kind of coherent thought? If you want to open a file, just click the little icon to the left featuring a plus sign:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/gmpv_menu2.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;There's also a button with the Gnome-MPV icon on it. One might assume this button would contain features specific to Gnome-MPV, such as the ones found the the hamburger menu, but no. Instead it looks like this, containing options for preferences and quitting:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/gmpv_menu1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In Evince, the button featuring an Evince icon produces this menu:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/ev_menu1.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Bummer! In Evince, you clearly have to look somewhere else to find in-app preferences and a quit option: things are wildly inconsistent between applications, creating confusion and frustration for users. I also can't find a way to navigate these menus using the keyboard once they're open, as opposed to normal drop-down menus and other similar hamburger menus.&lt;/p&gt;
&lt;h3&gt;More Gnome&lt;/h3&gt;
&lt;p&gt;There are so many more examples in just these two Gnome applications alone that it's bordering on parodical. For example, they are designed for Gnome's new paradigm of incorporating toolbars into the window title bar, thus institutionalizing the crimes of the Windows applications mentioned above. The difference is of course that if you're running another window manager, it just looks silly, for example leaving you with two close gadgets. It also means that to keep a reasonably large area free for moving the window (At least that's better than Slack!), widgets that could previously have been fitted into a toolbar below the title bar now needs to be opened separately, such as this search box in Evince (click for a larger image):&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://datagubbe.se/decusab/ev_search.png&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/ev_search.png&quot; width=&quot;550&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
Or this little extra toolbar for making annotations, containing a whopping total of two icons. That's one whole icon more than is used to open the toolbar itself, clearly warranting this particular design approach:
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/ev_extratoolbar.png&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;These are just a few examples of crimes against basic concepts of desktop UI design. There are plenty more and they're present on all platforms. They are also, it seems, growing increasingly common: the times of coherency and industry standards seem to be over. I hope that with this little rant, I can plant a seed to bring them back. If not, it was at least good to blow off some steam.&lt;/p&gt;
&lt;p&gt;I'm going to end by discussing some counter arguments I've come across when discussing modern UI design on various online forums:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technology is progressing! You can't stop change! Just deal with it!&lt;/strong&gt;&lt;br/&gt;These and similar truisms and platitudes are commonly used when no real argument is available. It's people like you and me who decide to change UI design, not an unstoppable force of nature. Changing things doesn't necessarily imply improving them and it's improvement we should strive for, otherwise change is pointless.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You're living in the past!&lt;/strong&gt;&lt;br/&gt;Considering the apparent anachronisms in the above screenshots, I can't argue with the fact that I am. However, that doesn't automatically mean I'm wrong. It also doesn't mean I think all modern desktop environments should look like Windows 95 or CDE. There are other roads forward and other ways to improve the look and feel of UI:s without breaking fundamental concepts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Electron apps can't follow a single platform's standard!&lt;/strong&gt;&lt;br/&gt;Multi-platform applications will of course never fully incorporate into the host enviroment the way native ones do. But because of this, it's of extra importance that they at least adhere to the paradigms that are translatable between all the common target platforms of today. Drop-down menus, clean title bars and a clear indication of window focus aren't hard to implement, even if they don't look exactly like their surroundings. In fact, a multi-platform framework should make it easy for developers to implement these concepts and hard, if not impossible, to work around them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We shouldn't complain about free software! It's free!&lt;/strong&gt;&lt;br/&gt;Yes. Yes we should. Don't get me wrong - I have a massive amount of respect and admiration for the time, skill and effort people put into FOSS projects. They are improving my personal quality of life significantly and for that I'm very grateful.&lt;/p&gt;
&lt;p&gt;It's true that Gnome and KDE are FOSS, which is a thing of wonder. But they are also large enough to, just like Microsoft and Google, have a significant impact not only on normal end users but on aspiring designers and programmers as well. We should be able to share our views and discuss what that impact might result in.&lt;/p&gt;
&lt;p&gt;In short: Anyone setting an example should also be held to a standard.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Putting things in the title bar saves screen real estate!&lt;/strong&gt;&lt;br/&gt;This is true to some extent, but screen real estate in general isn't much of a problem anymore. If this had been done in the days of 640x480 VGA, it could maybe have been a viable argument. Today, anyone working enough with computers to worry about a few pixels extra can buy a screen the size of a small TV with a 2560x1440 pixel resolution for around US$200. Even the cheapest of laptops come with at least a 1366x768 resolution, which is en par with the famed megapixel displays of yesteryear's professional workstations, coveted precisely for their generous amount of screen real estate.&lt;/p&gt;
&lt;p&gt;If anything, the problem with screen real estate comes from the current trend of UI design with so much white space between elements that what used to be a simple requester is now a full-screen application, as evident in this example (click for larger image):&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://datagubbe.se/decusab/winpara2.png&quot;&gt;&lt;img src=&quot;https://datagubbe.se/decusab/winpara2.png&quot; width=&quot;550&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For those spending all their workdays coding on a 13&quot; laptop, my tip is to stop worrying about screen real estate and start worrying about your back, neck, hands and shoulders a bit more. Trust me, RSI is a serious thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Designing UI:s is hard and application software can't please everyone all the time!&lt;/strong&gt;&lt;br/&gt;This is true and, as a software developer of more than 20 years, I have a huge amount of respect for the complexity of UI design. I also happen to know that such complexity is not a valid excuse for willingly and knowingly breaking UI concepts that have been proven and working for, in some cases, more than four decades. In fact, a lot of the examples above introduce more complexity for the user to cope with. The intricacies of each application and window decoration must be learned separately and time and energy is spent by repeatedly parsing the differences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What about Apple?&lt;/strong&gt;&lt;br/&gt;I can't comment on the current state of MacOS since the time I've spent actually using a Mac during the last 8 years or so probably totals to a few hours. Apple used to be good at this, and I hear they still do a decent job at keeping things sane, even post-Jobs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You're old and angry!&lt;/strong&gt;&lt;br/&gt;You bet! Now get off my lawn, punk.&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 18:26:49 +0000</pubDate>
<dc:creator>arexxbifs</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://datagubbe.se/decusab/</dc:identifier>
</item>
<item>
<title>Shopify CTO: our platform is now handling Black Friday level traffic every day</title>
<link>https://twitter.com/jmwind/status/1250816681024331777</link>
<guid isPermaLink="true" >https://twitter.com/jmwind/status/1250816681024331777</guid>
<description>That‚Äôs awesome &lt;a href=&quot;https://mobile.twitter.com/jmwind&quot; class=&quot;twitter-atreply dir-ltr&quot; dir=&quot;ltr&quot; data-mentioned-user-id=&quot;25218549&quot; data-screenname=&quot;jmwind&quot;&gt;@jmwind&lt;/a&gt;.
My intuition told me that &lt;a href=&quot;https://mobile.twitter.com/Shopify&quot; class=&quot;twitter-atreply dir-ltr&quot; dir=&quot;ltr&quot; data-mentioned-user-id=&quot;17136315&quot; data-screenname=&quot;Shopify&quot;&gt;@shopify&lt;/a&gt; should be seeing, both, on average strong online Buying demand from Buyers everywhere &amp;amp; a surge of new sellers who need &lt;a href=&quot;https://mobile.twitter.com/hashtag/Shopify?src=hash&quot; data-query-source=&quot;hashtag_click&quot; class=&quot;twitter-hashtag dir-ltr&quot; dir=&quot;ltr&quot;&gt;#Shopify&lt;/a&gt; to thrive/survive. Can you clarify if your choice of word ‚Äútraffic‚Äù means GMV? Not clear.</description>
<pubDate>Fri, 17 Apr 2020 17:49:30 +0000</pubDate>
<dc:creator>i_am_viet</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://mobile.twitter.com/jmwind/status/1250816681024331777</dc:identifier>
</item>
<item>
<title>NASA, SpaceX set May 27 as target date for first crew launch</title>
<link>https://spaceflightnow.com/2020/04/17/nasa-spacex-set-may-27-as-target-date-for-first-crew-launch/</link>
<guid isPermaLink="true" >https://spaceflightnow.com/2020/04/17/nasa-spacex-set-may-27-as-target-date-for-first-crew-launch/</guid>
<description>&lt;p&gt;&lt;img class=&quot;size-full wp-image-44693&quot; src=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2capsule.jpg&quot; alt=&quot;&quot; width=&quot;1200&quot; height=&quot;800&quot; srcset=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2capsule.jpg 1200w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2capsule-300x200.jpg 300w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2capsule-768x512.jpg 768w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2capsule-678x452.jpg 678w&quot; sizes=&quot;(max-width: 1200px) 100vw, 1200px&quot;/&gt;&lt;/p&gt;
The Crew Dragon spacecraft for the Demo-2 mission is at Cape Canaveral being readied for launch. Credit: SpaceX
&lt;p&gt;A decade in the making, NASA and SpaceX have set May 27 as the target launch date for the first crewed space mission to launch into orbit from U.S. soil since the retirement of the space shuttle in 2011.&lt;/p&gt;
&lt;p&gt;NASA astronauts Doug Hurley and Bob Behnken, two veterans of the shuttle program, will ride SpaceX‚Äôs Crew Dragon spacecraft into orbit on top of a Falcon 9 rocket from pad 39A at NASA‚Äôs Kennedy Space Center in Florida. That‚Äôs the same historic launch facility used by the last space shuttle flight.&lt;/p&gt;
&lt;p&gt;Liftoff time on May 27 is expected to be around 4:32 p.m. EDT (2032 GMT), when the Earth‚Äôs rotation brings the launch pad under the orbital plane of the International Space Station.&lt;/p&gt;
&lt;p&gt;‚ÄúOnce in orbit, the crew and SpaceX mission control will verify the spacecraft is performing as intended by testing the environmental control system, the displays and control system and the maneuvering thrusters, among other things,‚Äù NASA said in an update Friday.&lt;/p&gt;
&lt;p&gt;The Crew Dragon will fire its Draco thrusters to rendezvous and dock with the space station the day after launch.&lt;/p&gt;
&lt;p&gt;‚ÄúThe spacecraft is designed to do this autonomously but astronauts aboard the spacecraft and the station will be diligently monitoring approach and docking and can take control of the spacecraft if necessary,‚Äù NASA said.&lt;/p&gt;
&lt;p&gt;Assuming a launch May 27, Crew Dragon will dock with the space station at 11:29 a.m. EDT (1529 GMT) on May 28, NASA says.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;BREAKING: On May 27, &lt;a href=&quot;https://twitter.com/NASA?ref_src=twsrc%5Etfw&quot;&gt;@NASA&lt;/a&gt; will once again launch American astronauts on American rockets from American soil! With our &lt;a href=&quot;https://twitter.com/SpaceX?ref_src=twsrc%5Etfw&quot;&gt;@SpaceX&lt;/a&gt; partners, &lt;a href=&quot;https://twitter.com/Astro_Doug?ref_src=twsrc%5Etfw&quot;&gt;@Astro_Doug&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/AstroBehnken?ref_src=twsrc%5Etfw&quot;&gt;@AstroBehnken&lt;/a&gt; will launch to the &lt;a href=&quot;https://twitter.com/Space_Station?ref_src=twsrc%5Etfw&quot;&gt;@Space_Station&lt;/a&gt; on the &lt;a href=&quot;https://twitter.com/hashtag/CrewDragon?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#CrewDragon&lt;/a&gt; spacecraft atop a Falcon 9 rocket. Let‚Äôs &lt;a href=&quot;https://twitter.com/hashtag/LaunchAmerica?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#LaunchAmerica&lt;/a&gt; üá∫üá∏ &lt;a href=&quot;https://t.co/RINb3mfRWI&quot;&gt;pic.twitter.com/RINb3mfRWI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;‚Äî Jim Bridenstine (@JimBridenstine) &lt;a href=&quot;https://twitter.com/JimBridenstine/status/1251178705633841167?ref_src=twsrc%5Etfw&quot;&gt;April 17, 2020&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hurley and Behnken are training for the test flight, designated Demo-2, and are expected to live and work on the space station for several months. They will undock and return to Earth for a parachute-assisted splashdown in the Atlantic Ocean east of Florida.&lt;/p&gt;
&lt;p&gt;The Demo-2 flight with astronauts caps a decade-long effort to design, develop and qualify the Crew Dragon spacecraft for piloted missions. SpaceX successfully flew a Crew Dragon capsule without astronauts to the space station in March 2019 and returned the ship to Earth.&lt;/p&gt;
&lt;p&gt;SpaceX has also completed two major tests of the Crew Dragon‚Äôs launch escape system, which would be used to push the capsule away from its Falcon 9 rocket during fueling on the launch pad, or in flight.&lt;/p&gt;
&lt;p&gt;Last week, NASA Administrator Jim Bridenstine &lt;a href=&quot;https://spaceflightnow.com/2020/04/13/bridenstine-says-crew-dragon-could-launch-with-astronauts-at-end-of-may/&quot;&gt;said he was ‚Äúfairly confident‚Äù&lt;/a&gt; the Crew Dragon could be ready to carry astronauts to the space station at the end of May or in early June.&lt;/p&gt;
&lt;p&gt;In an interview with Spaceflight Now last week, Bridenstine cited several focus areas as SpaceX and NASA march toward launch of the Demo-2 test flight. They included completing two final parachute tests, data reviews, and ensuring the astronauts and ground teams remain healthy during the global coronavirus pandemic.&lt;/p&gt;
&lt;p&gt;One of the two remaining parachute tests was successfully completed last weekend after dropping a test rig from a C-130 cargo plane. SpaceX changed the testing plan after a helicopter drop test in March was cut short when the test rig became unstable, forcing the pilot to prematurely release the device that simulates the mass of the Crew Dragon spacecraft.&lt;/p&gt;
&lt;p&gt;The problem was not related to the parachutes themselves, and chute testing have successfully resumed, NASA officials said.&lt;/p&gt;
&lt;p&gt;One more parachute drop test from the C-130 cargo plane is scheduled for early May. That will be the final planned parachute test before the Demo-2 launch.&lt;/p&gt;
&lt;p&gt;As of Friday, SpaceX said it has completed 26 tests of the Crew Dragon‚Äôs ‚ÄúMark 3‚Äù parachute design, including 13 single parachute tests, 12 multi-chute drops, and an integrated test of the entire parachute recovery system on the Crew Dragon‚Äôs in-flight launch abort test in January.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;size-full wp-image-44697&quot; src=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2worm.jpg&quot; alt=&quot;&quot; width=&quot;900&quot; height=&quot;600&quot; srcset=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2worm.jpg 900w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2worm-300x200.jpg 300w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2worm-768x512.jpg 768w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/demo2worm-678x452.jpg 678w&quot; sizes=&quot;(max-width: 900px) 100vw, 900px&quot;/&gt;&lt;/p&gt;
The Falcon 9 rocket that will launch the Demo-2 mission is emblazoned with NASA‚Äôs ‚Äúworm‚Äù logo, which was retired from official use in 1992. ‚ÄúThe retro, modern design of the agency‚Äôs logo will help capture the excitement of a new, modern era of human spaceflight on the side of the Falcon 9 launch vehicle that will ferry astronauts to the International Space Station as part of the Demo-2 flight,‚Äù NASA said. The iconic blue NASA meatball logo will remain the agency‚Äôs primary symbol. Credit: SpaceX
&lt;p&gt;An investigation into an engine failure on the most recent launch of SpaceX‚Äôs Falcon 9 rocket ‚Äî the same design that will launch the Crew Dragon astronauts ‚Äî was also expected to be completed in short order, Bridenstine said last week.&lt;/p&gt;
&lt;p&gt;‚ÄúI think we‚Äôre really good shape,‚Äù Bridenstine told Spaceflight Now last week. ‚ÄúI‚Äôm fairly confident that we can launch at the end of May. If we do slip, it‚Äôll probably be into June. It won‚Äôt be much.‚Äù&lt;/p&gt;
&lt;p&gt;Preparations for the Demo-2 launch are continuing during the coronavirus pandemic. NASA has determined the crewed mission is essential, and the astronauts, their training teams and SpaceX‚Äôs spacecraft and rocket technicians remain at work.&lt;/p&gt;
&lt;p&gt;Other officials, such as managers and some engineers, are working remotely.&lt;/p&gt;
&lt;p&gt;SpaceX and Boeing won multibillion-dollar NASA commercial crew contracts to develop human-rated spaceships in 2014, following several years of preliminary development and testing. SpaceX is ahead of Boeing, and the crew capsule for the upcoming test flight ‚Äî designated Demo-2 ‚Äî is currently at Cape Canaveral undergoing pre-launch processing and testing.&lt;/p&gt;
&lt;p&gt;Through the agency‚Äôs Commercial Crew Program, NASA is paying SpaceX more than $3.1 billion for Crew Dragon development and six operational crew rotation flights to the space station following the Demo-2 mission.&lt;/p&gt;
&lt;p&gt;SpaceX‚Äôs Crew Dragon and Boeing‚Äôs Starliner capsule will give NASA a U.S.-built ship to ferry crews to and from the station, ending the space agency‚Äôs reliance on Russian Soyuz spacecraft for crew transportation.&lt;/p&gt;
&lt;p&gt;SpaceX is set to be the first commercial company to launch astronauts into orbit after Boeing ran into trouble on the first unpiloted test flight of its CST-100 Starliner capsule in December.&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;size-full wp-image-44373&quot; src=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/KSC-20200319-PH-SPX01_0016orig.jpg&quot; alt=&quot;&quot; width=&quot;900&quot; height=&quot;720&quot; srcset=&quot;https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/KSC-20200319-PH-SPX01_0016orig.jpg 900w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/KSC-20200319-PH-SPX01_0016orig-300x240.jpg 300w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/KSC-20200319-PH-SPX01_0016orig-768x614.jpg 768w, https://mk0spaceflightnoa02a.kinstacdn.com/wp-content/uploads/2020/04/KSC-20200319-PH-SPX01_0016orig-678x542.jpg 678w&quot; sizes=&quot;(max-width: 900px) 100vw, 900px&quot;/&gt;&lt;/p&gt;
NASA astronauts Doug Hurley (foreground) and Bob Behnken (background) participated in a two-day flight simulation in March. The astronauts are inside a SpaceX flight simulator in this photo. Credit: SpaceX
&lt;p&gt;Behnken, 49, will be the ‚Äújoint operations commander‚Äù for the Demo-2 mission, responsible for activities such as rendezvous, docking and undocking, according to NASA. He will also be responsible for mission activities while at the space station.&lt;/p&gt;
&lt;p&gt;Hurley, 53, will be the ‚Äúspacecraft commander,‚Äù responsible for launch, landing and recovery.&lt;/p&gt;
&lt;p&gt;Both were selected to join NASA‚Äôs astronaut corps in 2000, and they have each flown on two space shuttle flights. Hurley was the pilot of the space shuttle Atlantis on the shuttle program‚Äôs final mission in July 2011.&lt;/p&gt;
&lt;p&gt;In recent months, Hurley and Behnken have been training for an extended stay on the space station. The Demo-2 mission was originally slated to last just a couple of weeks, with the crew‚Äôs focus on testing the Crew Dragon spacecraft before future station crews ride it to and from the space station on operational flights.&lt;/p&gt;
&lt;p&gt;Hurley has been training to operate the space station‚Äôs Canadian-built robotic arm. Behnken has been practicing spacewalk procedures in case he is needed to go outside the station for repairs or maintenance tasks.&lt;/p&gt;
&lt;p&gt;‚ÄúWe‚Äôre trying to keep our focus on the technical matters, the operational matters, and we‚Äôre trying to get the vehicle ready to go so that when we fly it, everything goes the way it‚Äôs supposed to,‚Äù Hurley told reporters in January. ‚ÄúThe critical part is certifying the (Crew Dragon) vehicle for future crews‚Ä¶ making it a great vehicle for those guys.‚Äù&lt;/p&gt;
&lt;p&gt;‚ÄúWe‚Äôll be ready when they pick the date,‚Äù Behnken said. ‚ÄúAll the things that have to be ready, we‚Äôre just part of those things and we‚Äôll just try really hard not to be the long pole.&lt;/p&gt;
&lt;p&gt;Most of the Crew Dragon mission will be automated, but the astronauts will have the ability to command a manual launch abort with a handle on the ship‚Äôs cockpit control console. They also have the ability to manually fly the spacecraft.&lt;/p&gt;
&lt;p&gt;‚ÄúOur plan is to do some manual flying, in what we call the far-field,‚Äù Hurley said in January. ‚ÄúAnd then closer to ISS, the normal plan is to do an automated docking. Ideally, it would be nice to test that capability, but I think we can get some really good data from flying a little farther away on the docking axis with the vehicle. That is at least the plan going in, and there are some constraints with fuel and those kind of things.‚Äù&lt;/p&gt;
&lt;p&gt;Bridenstine said last week that the second Crew Dragon mission with astronauts, which will fly with a four-person crew, could launch in August or September, pending the outcome of the Demo-2 mission. On regular crew rotation flights, the Crew Dragon will be certified to remain docked at the space station as a lifeboat and return vehicle for up to 210 days.&lt;/p&gt;
&lt;p&gt;The Crew Dragon spacecraft flying on the Demo-2 mission will be able to stay at the space station for about 110 days, according to NASA. The exact duration of Hurley and Behnken‚Äôs mission will be determined based on the readiness of the next commercial crew launch, officials said.&lt;/p&gt;
&lt;p&gt;Assuming the next Crew Dragon is ready for launch in August or September, Bridenstine said the Demo-2 mission will last around two or three months.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;a href=&quot;mailto:sclark@spaceflightnow.com&quot;&gt;Email&lt;/a&gt; the author.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Follow Stephen Clark on Twitter: &lt;a href=&quot;http://www.twitter.com/stephenclark1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;@StephenClark1&lt;/a&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 17 Apr 2020 16:10:38 +0000</pubDate>
<dc:creator>whiskers</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://spaceflightnow.com/2020/04/17/nasa-spacex-set-may-27-as-target-date-for-first-crew-launch/</dc:identifier>
</item>
<item>
<title>Show HN: ZoomerBackgrounds ‚Äì community-sourced virtual video backgrounds</title>
<link>https://zoomerbackgrounds.com</link>
<guid isPermaLink="true" >https://zoomerbackgrounds.com</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://zoomerbackgrounds.com&quot;&gt;https://zoomerbackgrounds.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=22899654&quot;&gt;https://news.ycombinator.com/item?id=22899654&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 236&lt;/p&gt;
&lt;p&gt;# Comments: 64&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 15:10:05 +0000</pubDate>
<dc:creator>logane</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://zoomerbackgrounds.com/</dc:identifier>
</item>
<item>
<title>Is BGP Safe Yet?</title>
<link>https://isbgpsafeyet.com/</link>
<guid isPermaLink="true" >https://isbgpsafeyet.com/</guid>
<description>&lt;h2 id=&quot;whats-a-bgp-hijack&quot;&gt;What‚Äôs a BGP hijack?&lt;/h2&gt;
&lt;p&gt;To better understand why BGP‚Äôs lack of security is so problematic, let‚Äôs look at a simplified model of how BGP is used to route Internet packets.&lt;/p&gt;
&lt;p&gt;The Internet is not run by just &lt;em&gt;one&lt;/em&gt; company. It‚Äôs made up of thousands of autonomous systems with nodes located all around the world, connected to each other in a massive graph.&lt;/p&gt;
&lt;p&gt;In essence, the way BGP works is that each node must determine how to route packets using only what it knows from &lt;strong&gt;the nodes it connects with directly&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, in the simple network A‚ÄìB‚ÄìC‚ÄìD‚ÄìE, the node A only knows how to reach E based on information it received from B. The node B knows about the network from A and C. And so forth.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;BGP hijack&lt;/strong&gt; occurs when a malicious node deceives another node, lying about what the routes are for its neighbors. Without any security protocols, this misinformation can propagate from node to node, until a large number of nodes now know about, and attempt to use these incorrect, nonexistent, or malicious routes.&lt;/p&gt;
&lt;p&gt;Click ‚Äú&lt;strong&gt;Hijack the request&lt;/strong&gt;‚Äù to visualize how packets are re-routed:&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 15:02:00 +0000</pubDate>
<dc:creator>eastdakota</dc:creator>
<og:type>website</og:type>
<og:title>Is BGP safe yet? ¬∑ Cloudflare</og:title>
<og:description>On the Internet, network devices exchange routes via a protocol called BGP (Border Gateway Protocol). Unfortunately, issues with BGP have led to malicious actors being able to hijack and misconfigure devices leading to security problems which have the potential to cause widespread problems. BGP security can be greatly improved by using technologies such as RPKI to sign Internet routes. This page attempts to track the progress of major Internet players (ISPs, transit operators, and content providers) in their progress to adopt RPKI and other technologies.</og:description>
<og:url>https://isbgpsafeyet.com</og:url>
<og:image>https://isbgpsafeyet.com/media/open-graph.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://isbgpsafeyet.com/</dc:identifier>
</item>
<item>
<title>Learning to See in the Dark (2018)</title>
<link>https://github.com/cchen156/Learning-to-See-in-the-Dark</link>
<guid isPermaLink="true" >https://github.com/cchen156/Learning-to-See-in-the-Dark</guid>
<description>&lt;p&gt;This is a Tensorflow implementation of Learning to See in the Dark in CVPR 2018, by &lt;a href=&quot;http://cchen156.web.engr.illinois.edu/&quot; rel=&quot;nofollow&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://cqf.io/&quot; rel=&quot;nofollow&quot;&gt;Qifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://pages.cs.wisc.edu/~jiaxu/&quot; rel=&quot;nofollow&quot;&gt;Jia Xu&lt;/a&gt;, and &lt;a href=&quot;http://vladlen.info/&quot; rel=&quot;nofollow&quot;&gt;Vladlen Koltun&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://cchen156.web.engr.illinois.edu/SID.html&quot; rel=&quot;nofollow&quot;&gt;Project Website&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://cchen156.web.engr.illinois.edu/paper/18CVPR_SID.pdf&quot; rel=&quot;nofollow&quot;&gt;Paper&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/cchen156/Learning-to-See-in-the-Dark/blob/master/images/fig1.png&quot;&gt;&lt;img src=&quot;https://github.com/cchen156/Learning-to-See-in-the-Dark/raw/master/images/fig1.png&quot; alt=&quot;teaser&quot; title=&quot;Sample inpainting results on held-out images&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This code includes the default model for training and testing on the See-in-the-Dark (SID) dataset.&lt;/p&gt;
&lt;h2&gt;Demo Video&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/qWKUFK7MWvg&quot; rel=&quot;nofollow&quot;&gt;https://youtu.be/qWKUFK7MWvg&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;h3&gt;Requirement&lt;/h3&gt;
&lt;p&gt;Required python (version 2.7) libraries: Tensorflow (&amp;gt;=1.1) + Scipy + Numpy + Rawpy.&lt;/p&gt;
&lt;p&gt;Tested in Ubuntu + Intel i7 CPU + Nvidia Titan X (Pascal) with Cuda (&amp;gt;=8.0) and CuDNN (&amp;gt;=5.0). CPU mode should also work with minor changes but not tested.&lt;/p&gt;
&lt;h3&gt;Dataset&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Update Aug, 2018:&lt;/strong&gt; We found some misalignment with the ground-truth for image 10034, 10045, 10172. Please remove those images for quantitative results, but they still can be used for qualitative evaluations.&lt;/p&gt;
&lt;p&gt;You can download it directly from Google drive for the &lt;a href=&quot;https://storage.googleapis.com/isl-datasets/SID/Sony.zip&quot; rel=&quot;nofollow&quot;&gt;Sony&lt;/a&gt; (25 GB) and &lt;a href=&quot;https://storage.googleapis.com/isl-datasets/SID/Fuji.zip&quot; rel=&quot;nofollow&quot;&gt;Fuji&lt;/a&gt; (52 GB) sets.&lt;/p&gt;
&lt;p&gt;There is download limit by Google drive in a fixed period of time. If you cannot download because of this, try these links: &lt;a href=&quot;https://drive.google.com/open?id=1G6VruemZtpOyHjOC5N8Ww3ftVXOydSXx&quot; rel=&quot;nofollow&quot;&gt;Sony&lt;/a&gt; (25 GB) and &lt;a href=&quot;https://drive.google.com/open?id=1C7GeZ3Y23k1B8reRL79SqnZbRBc4uizH&quot; rel=&quot;nofollow&quot;&gt;Fuji&lt;/a&gt; (52 GB).&lt;/p&gt;
&lt;p&gt;New: we provide file parts in &lt;a href=&quot;https://pan.baidu.com/s/1fk8EibhBe_M1qG0ax9LQZA&quot; rel=&quot;nofollow&quot;&gt;Baidu Drive&lt;/a&gt; now. After you download all the parts, you can combine them together by running: &quot;cat SonyPart* &amp;gt; Sony.zip&quot; and &quot;cat FujiPart* &amp;gt; Fuji.zip&quot;.&lt;/p&gt;
&lt;p&gt;The file lists are provided. In each row, there are a short-exposed image path, the corresponding long-exposed image path, camera ISO and F number. Note that multiple short-exposed images may correspond to the same long-exposed image.&lt;/p&gt;
&lt;p&gt;The file name contains the image information. For example, in &quot;10019_00_0.033s.RAF&quot;, the first digit &quot;1&quot; means it is from the test set (&quot;0&quot; for training set and &quot;2&quot; for validation set); &quot;0019&quot; is the image ID; the following &quot;00&quot; is the number in the sequence/burst; &quot;0.033s&quot; is the exposure time 1/30 seconds.&lt;/p&gt;
&lt;h3&gt;Testing&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;Clone this repository.&lt;/li&gt;
&lt;li&gt;Download the pretrained models by running&lt;/li&gt;
&lt;/ol&gt;&lt;div class=&quot;highlight highlight-source-shell&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
python download_models.py
&lt;/pre&gt;&lt;/div&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Run &quot;python test_Sony.py&quot;. This will generate results on the Sony test set.&lt;/li&gt;
&lt;li&gt;Run &quot;python test_Fuji.py&quot;. This will generate results on the Fuji test set.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;By default, the code takes the data in the &quot;./dataset/Sony/&quot; folder and &quot;./dataset/Fuji/&quot;. If you save the dataset in other folders, please change the &quot;input_dir&quot; and &quot;gt_dir&quot; at the beginning of the code.&lt;/p&gt;
&lt;h3&gt;Training new models&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;To train the Sony model, run &quot;python train_Sony.py&quot;. The result and model will be save in &quot;result_Sony&quot; folder by default.&lt;/li&gt;
&lt;li&gt;To train the Fuji model, run &quot;python train_Fuji.py&quot;. The result and model will be save in &quot;result_Fuji&quot; folder by default.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;By default, the code takes the data in the &quot;./dataset/Sony/&quot; folder and &quot;./dataset/Fuji/&quot;. If you save the dataset in other folders, please change the &quot;input_dir&quot; and &quot;gt_dir&quot; at the beginning of the code.&lt;/p&gt;
&lt;p&gt;Loading the raw data and proccesing by Rawpy takes significant more time than the backpropagation. By default, the code will load all the groundtruth data processed by Rawpy into memory without 8-bit or 16-bit quantization. This requires at least 64 GB RAM for training the Sony model and 128 GB RAM for the Fuji model. If you need to train it on a machine with less RAM, you may need to revise the code and use the groundtruth data on the disk. We provide the 16-bit groundtruth images processed by Rawpy: &lt;a href=&quot;https://drive.google.com/file/d/1wfkWVkauAsGvXtDJWX0IFDuDl5ozz2PM/view?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;Sony&lt;/a&gt; (12 GB) and &lt;a href=&quot;https://drive.google.com/file/d/1nJM0xYVnzmOZNacBRKebiXA4mBmiTjte/view?usp=sharing&quot; rel=&quot;nofollow&quot;&gt;Fuji&lt;/a&gt; (22 GB).&lt;/p&gt;
&lt;h2&gt;Citation&lt;/h2&gt;
&lt;p&gt;If you use our code and dataset for research, please cite our paper:&lt;/p&gt;
&lt;p&gt;Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun, &quot;Learning to See in the Dark&quot;, in CVPR, 2018.&lt;/p&gt;
&lt;h3&gt;License&lt;/h3&gt;
&lt;p&gt;MIT License.&lt;/p&gt;
&lt;h2&gt;FAQ&lt;/h2&gt;
&lt;ol&gt;&lt;li&gt;Can I test my own data using the provided model?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The proposed method is designed for sensor raw data. The pretrained model probably not work for data from another camera sensor. We do not have support for other camera data. It also does not work for images after camera ISP, i.e., the JPG or PNG data.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Will this be in any product?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;This is a research project and a prototype to prove a concept.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;How can I train the model using my own raw data?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Generally, you just need to subtract the right black level and pack the data in the same way of Sony/Fuji data. If using rawpy, you need to read the black level instead of using 512 in the provided code. The data range may also differ if it is not 14 bits. You need to normalize it to [0,1] for the network input.&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;Why the results are all black?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;It is often because the pre-trained model not downloaded properly. After downloading, you should get 4 checkpoint related files for the model.&lt;/p&gt;
&lt;h2&gt;Questions&lt;/h2&gt;
&lt;p&gt;If you have additional questions after reading the FAQ, please email to &lt;a href=&quot;mailto:cchen156@illinois.edu&quot;&gt;cchen156@illinois.edu&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 14:57:19 +0000</pubDate>
<dc:creator>ksaxena</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/19864321?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>cchen156/Learning-to-See-in-the-Dark</og:title>
<og:url>https://github.com/cchen156/Learning-to-See-in-the-Dark</og:url>
<og:description>Learning to See in the Dark. CVPR 2018. Contribute to cchen156/Learning-to-See-in-the-Dark development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/cchen156/Learning-to-See-in-the-Dark</dc:identifier>
</item>
<item>
<title>Building an end-to-end Speech Recognition model in PyTorch</title>
<link>https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch</link>
<guid isPermaLink="true" >https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch</guid>
<description>&lt;p&gt;Deep Learning has changed the game in speech recognition with the introduction of end-to-end models. These models take in audio, and directly output transcriptions. Two of the most popular end-to-end models today are Deep Speech by Baidu, and Listen Attend Spell (LAS) by Google. Both Deep Speech and LAS, are recurrent neural network (RNN) based architectures with different approaches to modeling speech recognition. Deep Speech uses the Connectionist Temporal Classification (CTC) loss function to predict the speech transcript. LAS uses a sequence to sequence network architecture for its predictions.&lt;/p&gt;
&lt;p&gt;These models simplified speech recognition pipelines by taking advantage of the capacity of deep learning system to learn from large datasets. With enough data, you should, in theory, be able to build a super robust speech recognition model that can account for all the nuance in speech without having to spend a ton of time and effort hand engineering acoustic features or dealing with complex pipelines in more old-school GMM-HMM model architectures, for example.&lt;/p&gt;
&lt;p&gt;Deep learning is a fast-moving field, and Deep Speech and LAS style architectures are already quickly becoming outdated. You can read about where the industry is moving in the Latest Advancement Section below.&lt;/p&gt;
&lt;h3 class=&quot;color-1&quot;&gt;How to Build Your Own End-to-End Speech Recognition Model in PyTorch&lt;/h3&gt;
&lt;p&gt;Let's walk through how one would build their own end-to-end speech recognition model in PyTorch. The model we'll build is inspired by Deep Speech 2 (Baidu's second revision of their now-famous model) with some personal improvements to the architecture. The output of the model will be a probability matrix of characters, and we'll use that probability matrix to decode the most likely characters spoken from the audio. You can find the full code and also run the it with GPU support on &lt;a href=&quot;https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO&quot;&gt;Google Colaboratory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparing the data pipeline&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data is one of the most important aspects of speech recognition. We'll take raw audio waves and transform them into Mel Spectrograms.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/QcgsJsmohaMtFRIw.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;You can read more on the details about how that transformation looks from this excellent post &lt;a href=&quot;https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html&quot;&gt;here&lt;/a&gt;. For this post, you can just think of a Mel Spectrogram as essentially a picture of sound.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/fOPEodLdOFtLHegW.jpg&quot; alt=&quot;&quot;/&gt;&lt;p&gt;For the handling of audio data, we are going to use an extremely useful utility called &lt;code&gt;torchaudio&lt;/code&gt; which is a library built by the PyTorch team specifically for audio data. We'll be training on a subset of &lt;a href=&quot;http://www.openslr.org/12/&quot;&gt;LibriSpeech&lt;/a&gt;, which is a corpus of read English speech data derived from audiobooks, comprised of 100 hours of transcribed audio data. You can easily download this dataset using &lt;code&gt;torchaudio&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;import torchaudio

train_dataset = torchaudio.datasets.LIBRISPEECH(&quot;./&quot;, url=&quot;train-clean-100&quot;, download=True)
test_dataset = torchaudio.datasets.LIBRISPEECH(&quot;./&quot;, url=&quot;test-clean&quot;, download=True)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Each sample of the dataset contains the waveform, sample rate of audio, the utterance/label, and more metadata on the sample. You can view what each sample looks like from the source code &lt;a href=&quot;https://github.com/pytorch/audio/blob/master/torchaudio/datasets/librispeech.py#L40&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Augmentation - SpecAugment&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data augmentation is a technique used to artificially increase the diversity of your dataset in order to increase your dataset size. This strategy is especially helpful when data is scarce or if your model is overfitting. For speech recognition, you can do the standard augmentation techniques, like changing the pitch, speed, injecting noise, and adding reverb to your audio data.&lt;/p&gt;
&lt;p&gt;We found Spectrogram Augmentation (SpecAugment), to be a much simpler and more effective approach. SpecAugment, was first introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;&gt;SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition&lt;/a&gt;, in which the authors found that simply cutting out random blocks of consecutive time and frequency dimensions improved the models generalization abilities significantly!&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/YbqKqNBodFSQAVWD.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;In PyTorch, you can use the &lt;code&gt;torchaudio&lt;/code&gt; function &lt;code&gt;FrequencyMasking&lt;/code&gt; to mask out the frequency dimension, and &lt;code&gt;TimeMasking&lt;/code&gt; for the time dimension.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;torchaudio.transforms.FrequencyMasking()
torchaudio.transforms.TimeMasking()&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now that we have the data, we'll need to transform the audio into Mel Spectrograms, and map the character labels for each audio sample into integer labels:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;class TextTransform:
    &quot;&quot;&quot;Maps characters to integers and vice versa&quot;&quot;&quot;
    def __init__(self):
        char_map_str = &quot;&quot;&quot;
        ' 0
         1
        a 2
        b 3
        c 4
        d 5
        e 6
        f 7
        g 8
        h 9
        i 10
        j 11
        k 12
        l 13
        m 14
        n 15
        o 16
        p 17
        q 18
        r 19
        s 20
        t 21
        u 22
        v 23
        w 24
        x 25
        y 26
        z 27
        &quot;&quot;&quot;
        self.char_map = {}
        self.index_map = {}
        for line in char_map_str.strip().split('\n'):
            ch, index = line.split()
            self.char_map[ch] = int(index)
            self.index_map[int(index)] = ch
        self.index_map[1] = ' '

    def text_to_int(self, text):
        &quot;&quot;&quot; Use a character map and convert text to an integer sequence &quot;&quot;&quot;
        int_sequence = []
        for c in text:
            if c == ' ':
                ch = self.char_map['']
            else:
                ch = self.char_map[c]
            int_sequence.append(ch)
        return int_sequence

    def int_to_text(self, labels):
        &quot;&quot;&quot; Use a character map and convert integer labels to an text sequence &quot;&quot;&quot;
        string = []
        for i in labels:
            string.append(self.index_map[i])
        return ''.join(string).replace('', ' ')


train_audio_transforms = nn.Sequential(
    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),
    torchaudio.transforms.FrequencyMasking(freq_mask_param=15),
    torchaudio.transforms.TimeMasking(time_mask_param=35)
)

valid_audio_transforms = torchaudio.transforms.MelSpectrogram()

text_transform = TextTransform()


def data_processing(data, data_type=&quot;train&quot;):
    spectrograms = []
    labels = []
    input_lengths = []
    label_lengths = []
    for (waveform, _, utterance, _, _, _) in data:
        if data_type == 'train':
            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)
        else:
            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)
        spectrograms.append(spec)
        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))
        labels.append(label)
        input_lengths.append(spec.shape[0]//2)
        label_lengths.append(len(label))

    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)
    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)

    return spectrograms, labels, input_lengths, label_lengths&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 class=&quot;color-1&quot;&gt;&lt;strong&gt;Define the Model - Deep Speech 2 (but better)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Our model will be similar to the Deep Speech 2 architecture. The model will have two main neural network modules - N layers of Residual Convolutional Neural Networks (ResCNN) to learn the relevant audio features, and a set of Bidirectional Recurrent Neural Networks (BiRNN) to leverage the learned ResCNN audio features. The model is topped off with a fully connected layer used to classify characters per time step.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/BHOBfDVTcGCQKTtp.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;Convolutional Neural Networks (CNN) are great at extracting abstract features, and we'll apply the same feature extraction power to audio spectrograms. Instead of just vanilla CNN layers, we choose to use Residual CNN layers. Residual connections (AKA skip connections) were first introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;Deep Residual Learning for Image Recognition&lt;/a&gt;, where the author found that you can build really deep networks with good accuracy gains if you add these connections to your CNN's. Adding these Residual connections also helps the model learn faster and generalize better. The paper &lt;a href=&quot;https://arxiv.org/abs/1712.09913&quot;&gt;Visualizing the Loss Landscape of Neural Nets&lt;/a&gt; shows that networks with residual connections have a ‚Äúflatter‚Äù loss surface, making it easier for models navigate the loss landscape and find a lower and more generalizable minima.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/qbBytawOwsmKfYlI.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;Recurrent Neural Networks (RNN) are naturally great at sequence modeling problems. RNN's processes the audio features step by step, making a prediction for each frame while using context from previous frames. We use BiRNN's because we want the context of not only the frame before each step, but the frames after it as well. This can help the model make better predictions, as each frame in the audio will have more information before making a prediction. We chose to use Gated Recurrent Unit (GRU's) variant of RNN's as it needs less computational resources than LSTM's, and works just as well in some cases.&lt;/p&gt;
&lt;p&gt;The model outputs a probability matrix for characters which we'll use to feed into our decoder to extract what the model believes is the highest probability characters that were spoken.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;class CNNLayerNorm(nn.Module):
    &quot;&quot;&quot;Layer normalization built for cnns input&quot;&quot;&quot;
    def __init__(self, n_feats):
        super(CNNLayerNorm, self).__init__()
        self.layer_norm = nn.LayerNorm(n_feats)

    def forward(self, x):
        # x (batch, channel, feature, time)
        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)
        x = self.layer_norm(x)
        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) 


class ResidualCNN(nn.Module):
    &quot;&quot;&quot;Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf
        except with layer norm instead of batch norm
    &quot;&quot;&quot;
    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):
        super(ResidualCNN, self).__init__()

        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)
        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.layer_norm1 = CNNLayerNorm(n_feats)
        self.layer_norm2 = CNNLayerNorm(n_feats)

    def forward(self, x):
        residual = x  # (batch, channel, feature, time)
        x = self.layer_norm1(x)
        x = F.gelu(x)
        x = self.dropout1(x)
        x = self.cnn1(x)
        x = self.layer_norm2(x)
        x = F.gelu(x)
        x = self.dropout2(x)
        x = self.cnn2(x)
        x += residual
        return x # (batch, channel, feature, time)


class BidirectionalGRU(nn.Module):

    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):
        super(BidirectionalGRU, self).__init__()

        self.BiGRU = nn.GRU(
            input_size=rnn_dim, hidden_size=hidden_size,
            num_layers=1, batch_first=batch_first, bidirectional=True)
        self.layer_norm = nn.LayerNorm(rnn_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.layer_norm(x)
        x = F.gelu(x)
        x, _ = self.BiGRU(x)
        x = self.dropout(x)
        return x


class SpeechRecognitionModel(nn.Module):
    &quot;&quot;&quot;Speech Recognition Model Inspired by DeepSpeech 2&quot;&quot;&quot;

    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):
        super(SpeechRecognitionModel, self).__init__()
        n_feats = n_feats//2
        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features

        # n residual cnn layers with filter size of 32
        self.rescnn_layers = nn.Sequential(*[
            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) 
            for _ in range(n_cnn_layers)
        ])
        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)
        self.birnn_layers = nn.Sequential(*[
            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,
                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)
            for i in range(n_rnn_layers)
        ])
        self.classifier = nn.Sequential(
            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(rnn_dim, n_class)
        )

    def forward(self, x):
        x = self.cnn(x)
        x = self.rescnn_layers(x)
        sizes = x.size()
        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)
        x = x.transpose(1, 2) # (batch, time, feature)
        x = self.fully_connected(x)
        x = self.birnn_layers(x)
        x = self.classifier(x)
        return x&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 class=&quot;color-1&quot;&gt;Picking the Right Optimizer and Scheduler - AdamW with Super Convergence&lt;/h3&gt;
&lt;p&gt;The optimizer and learning rate schedule plays a very important role in getting our model to converge to the best point. Picking the right optimizer and scheduler can also save you compute time, and help your model generalize better to real-world use cases. For our model, we'll be using &lt;strong&gt;AdamW&lt;/strong&gt; with the &lt;strong&gt;One Cycle Learning Rate Scheduler&lt;/strong&gt;. &lt;strong&gt;Adam&lt;/strong&gt; is a widely used optimizer that helps your model converge quicker, therefore, saving compute time, but has been notorious for not generalizing as well as &lt;strong&gt;Stochastic Gradient Descent&lt;/strong&gt; AKA &lt;strong&gt;SGD&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AdamW&lt;/strong&gt; was first introduced in &lt;a href=&quot;https://arxiv.org/abs/1711.05101&quot;&gt;Decoupled Weight Decay Regularization&lt;/a&gt;, and is considered a ‚Äúfix‚Äù to &lt;strong&gt;Adam&lt;/strong&gt;. The paper pointed out that the original &lt;strong&gt;Adam&lt;/strong&gt; algorithm has a wrong implementation of weight decay, which &lt;strong&gt;AdamW&lt;/strong&gt; attempts to fix. This fix helps with &lt;strong&gt;Adam&lt;/strong&gt;'s generalization problem.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;One Cycle Learning Rate Scheduler&lt;/strong&gt; was first introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1708.07120&quot;&gt;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&lt;/a&gt;. This paper shows that you can train neural networks an order of magnitude faster, while keeping their generalizable abilities, using a simple trick. You start with a low learning rate, which warms up to a large maximum learning rate, then decays linearly to the same point of where you originally started.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/XhVKIzvgauDALinA.jpg&quot; alt=&quot;&quot;/&gt;&lt;p&gt;Because the maximum learning rate is magnitudes higher than the lowest, you also gain some regularization benefits which helps your model generalize better if you have a smaller set of data.&lt;/p&gt;
&lt;p&gt;With PyTorch, these two methods are already part of the package.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])
scheduler = optim.lr_scheduler.OneCycleLR(optimizer,
        max_lr=hparams['learning_rate'],
        steps_per_epoch=int(len(train_loader)),
        epochs=hparams['epochs'],
        anneal_strategy='linear')&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 class=&quot;color-1&quot;&gt;The CTC Loss Function - Aligning Audio to Transcript&lt;/h3&gt;
&lt;p&gt;Our model will be trained to predict the probability distribution of all characters in the alphabet for each frame (ie, timestep) in the spectrogram we feed into the model.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/mQEJyPYmfQAHalLK.png&quot; alt=&quot;&quot;/&gt;&lt;blockquote readability=&quot;3.1034482758621&quot;&gt;
&lt;p&gt;&lt;em&gt;Image taking from&lt;/em&gt; &lt;a href=&quot;https://distill.pub/2017/ctc/&quot;&gt;&lt;em&gt;distill.pub&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Traditional speech recognition models would require you to align the transcript text to the audio before training, and the model would be trained to predict specific labels at specific frames.&lt;/p&gt;
&lt;p&gt;The innovation of the CTC loss function is that it allows us to skip this step. Our model will learn to align the transcript itself during training. Key to this is the ‚Äúblank‚Äù label introduced by CTC, which gives the model the ability to say that a certain audio frame did not produce a character. You can see a more detail explanation of CTC and how it works from &lt;a href=&quot;https://distill.pub/2017/ctc/&quot;&gt;this excellent post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;CTC loss function is built into PyTorch.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;criterion = nn.CTCLoss(blank=28).to(device)&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 class=&quot;color-1&quot;&gt;Evaluating Your Speech Model&lt;/h3&gt;
&lt;p&gt;When Evaluating your speech recognition model, the industry standard is using the Word Error Rate (WER) as the metric. The Word Error Rate does exactly what it says - it takes the transcription your model outputs, and the true transcription, and measures the error between them. You can see how that's implemented &lt;a href=&quot;https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO&quot;&gt;here&lt;/a&gt;. Another useful metric is also called the Character Error Rate (CER). The CER measures the error of the characters between the model's output and the true labels. These metrics are helpful to measure how well your model performs.&lt;/p&gt;
&lt;p&gt;For this tutorial, we'll use a &quot;greedy&quot; decoding method to process our model's output into characters that can be combined to create the transcript. A &quot;greedy&quot; decoder takes in the model output, which is a softmax probability matrix of characters, and for each time step (spectrogram frame), it chooses the label with the highest probability. If the label is a blank label, we remove it from the final transcript.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):
    arg_maxes = torch.argmax(output, dim=2)
    decodes = []
    targets = []
    for i, args in enumerate(arg_maxes):
        decode = []
        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))
        for j, index in enumerate(args):
            if index != blank_label:
                if collapse_repeated and j != 0 and index == args[j -1]:
                    continue
                decode.append(index.item())
        decodes.append(text_transform.int_to_text(decode))
    return decodes, targets&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 class=&quot;color-1&quot;&gt;Training and Monitoring Your Experiments Using Comet.ml&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.comet.ml/site/&quot;&gt;Comet.ml&lt;/a&gt; provides a platform that allows deep learning researchers to track, compare, explain, and optimize their experiments and models. Comet.ml has improved our productivity at AssemblyAI and we highly recommend using this platform for teams doing any sort of data science experiments. Comet.ml is super easy to set up. And works with just a few lines of code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# initialize experiment opbject
experiment = Experiment(api_key=comet_api_key, project_name=project_name)
experiment.set_name(exp_name)

# track metrics
experiment.log_metric('loss', loss.item())&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Comet.ml provides you with a very productive dashboard where you can view and track your model's progress.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/XIvxIXiIbFxgsPCy.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;You can use Comet to track metrics, code, hyper parameters, your model's graphs, among many other things! A really handy feature that Comet provides is the ability to compare your experiment among many other experiments.&lt;/p&gt;
&lt;img src=&quot;https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/mdwMVAypIpIDBFJg.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;Comet has a rich feature set that we won't cover all here, but we highly recommended using it for a productivity and sanity boost.&lt;/p&gt;
&lt;p&gt;Here is the rest of our training script.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;class IterMeter(object):
    &quot;&quot;&quot;keeps track of total iterations&quot;&quot;&quot;
    def __init__(self):
        self.val = 0

    def step(self):
        self.val += 1

    def get(self):
        return self.val


def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment):
    model.train()
    data_len = len(train_loader.dataset)
    with experiment.train():
        for batch_idx, _data in enumerate(train_loader):
            spectrograms, labels, input_lengths, label_lengths = _data 
            spectrograms, labels = spectrograms.to(device), labels.to(device)

            optimizer.zero_grad()

            output = model(spectrograms)  # (batch, time, n_class)
            output = F.log_softmax(output, dim=2)
            output = output.transpose(0, 1) # (time, batch, n_class)

            loss = criterion(output, labels, input_lengths, label_lengths)
            loss.backward()

            experiment.log_metric('loss', loss.item(), step=iter_meter.get())
            experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())

            optimizer.step()
            scheduler.step()
            iter_meter.step()
            if batch_idx % 100 == 0 or batch_idx == data_len:
                print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch, batch_idx * len(spectrograms), data_len,
                    100. * batch_idx / len(train_loader), loss.item()))


def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):
    print('\nevaluating‚Ä¶')
    model.eval()
    test_loss = 0
    test_cer, test_wer = [], []
    with experiment.test():
        with torch.no_grad():
            for I, _data in enumerate(test_loader):
                spectrograms, labels, input_lengths, label_lengths = _data 
                spectrograms, labels = spectrograms.to(device), labels.to(device)

                output = model(spectrograms)  # (batch, time, n_class)
                output = F.log_softmax(output, dim=2)
                output = output.transpose(0, 1) # (time, batch, n_class)

                loss = criterion(output, labels, input_lengths, label_lengths)
                test_loss += loss.item() / len(test_loader)

                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)
                for j in range(len(decoded_preds)):
                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))
                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))


    avg_cer = sum(test_cer)/len(test_cer)
    avg_wer = sum(test_wer)/len(test_wer)
    experiment.log_metric('test_loss', test_loss, step=iter_meter.get())
    experiment.log_metric('cer', avg_cer, step=iter_meter.get())
    experiment.log_metric('wer', avg_wer, step=iter_meter.get())

    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\n'.format(test_loss, avg_cer, avg_wer))


def main(learning_rate=5e-4, batch_size=20, epochs=10,
        train_url=‚Äútrain-clean-100‚Äù, test_url=‚Äútest-clean‚Äù,
        experiment=Experiment(api_key='dummy_key', disabled=True)):

    hparams = {
        ‚Äún_cnn_layers‚Äù: 3,
        ‚Äún_rnn_layers‚Äù: 5,
        ‚Äúrnn_dim‚Äù: 512,
        ‚Äún_class‚Äù: 29,
        ‚Äún_feats‚Äù: 128,
        ‚Äústride‚Äù:2,
        ‚Äúdropout‚Äù: 0.1,
        ‚Äúlearning_rate‚Äù: learning_rate,
        ‚Äúbatch_size‚Äù: batch_size,
        ‚Äúepochs‚Äù: epochs
    }

    experiment.log_parameters(hparams)

    use_cuda = torch.cuda.is_available()
    torch.manual_seed(7)
    device = torch.device(‚Äúcuda‚Äù if use_cuda else ‚Äúcpu‚Äù)

    if not os.path.isdir(‚Äú./data‚Äù):
        os.makedirs(‚Äú./data‚Äù)

    train_dataset = torchaudio.datasets.LIBRISPEECH(‚Äú./data‚Äù, url=train_url, download=True)
    test_dataset = torchaudio.datasets.LIBRISPEECH(‚Äú./data‚Äù, url=test_url, download=True)

    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    train_loader = data.DataLoader(dataset=train_dataset,
                                batch_size=hparams['batch_size'],
                                shuffle=True,
                                collate_fn=lambda x: data_processing(x, 'train'),
                                **kwargs)
    test_loader = data.DataLoader(dataset=test_dataset,
                                batch_size=hparams['batch_size'],
                                shuffle=False,
                                collate_fn=lambda x: data_processing(x, 'valid'),
                                **kwargs)

    model = SpeechRecognitionModel(
        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],
        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']
        ).to(device)

    print(model)
    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))

    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])
    criterion = nn.CTCLoss(blank=28).to(device)
    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], 
                                            steps_per_epoch=int(len(train_loader)),
                                            epochs=hparams['epochs'],
                                            anneal_strategy='linear')

    iter_meter = IterMeter()
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment)
        test(model, device, test_loader, criterion, epoch, iter_meter, experiment)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;train&lt;/code&gt; function trains the model on a full epoch of data. The &lt;code&gt;test&lt;/code&gt; function evaluates the model on test data after every epoch. It gets the &lt;code&gt;test_loss&lt;/code&gt; as well as the &lt;code&gt;cer&lt;/code&gt; and &lt;code&gt;wer&lt;/code&gt; of the model. You can start running the training script right now with GPU support in the &lt;a href=&quot;https://colab.research.google.com/drive/1IPpwx4rX32rqHKpLz7dc8sOKspUa-YKO&quot;&gt;Google Colaboratory&lt;/a&gt;.&lt;/p&gt;
&lt;h3 class=&quot;color-1&quot;&gt;How to Improve Accuracy&lt;/h3&gt;
&lt;p&gt;Speech Recognition Requires a ton of data and a ton of compute resources. The example laid out is trained on a subset of LibriSpeech (100 hours of audio) and a single GPU. To get state of the art results you'll need to do distributed training on thousands of hours of data, on tens of GPU's spread out across many machines.&lt;/p&gt;
&lt;p&gt;Another way to get a big accuracy improvement is to decode the CTC probability matrix using a Language Model and the CTC beam search algorithm. CTC type models are very dependent on this decoding process to get good results. Luckily there is a handy &lt;a href=&quot;https://github.com/parlance/ctcdecode&quot;&gt;open source library&lt;/a&gt; that allows you to do that.&lt;/p&gt;
&lt;p&gt;This tutorial was made to be more accessible so it's a relatively smaller model (23 million Parameters) compared to something like BERT (340 million Parameters). It seems to be the larger you can get your network, the better it performs, although there are diminishing returns. A larger model equating to better performance is not always the case though, as proven by OpenAI's research &lt;a href=&quot;https://openai.com/assets/deep-double-descent/&quot;&gt;Deep Double Descent&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This model has 3 residual CNN layers and 5 Bidirectional GRU layers which should allow you to train a reasonable batch size on a single GPU with at least 11GB of memory. You can tweak some of the hyper parameters in the main function to reduce or increase the model size for your use case and compute availability.&lt;/p&gt;
&lt;h3 class=&quot;color-1&quot;&gt;Latest Advancements In Speech Recognition with Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep learning is a fast-moving field. It seems like you can't go by a week without some new technique getting state of the art results. Here are a few of things worth exploring int the world of speech recognition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Transformers have taken the Natural Language Processing world by storm! First Introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;, transformers have been taking and modified to beat pretty much all existing NLP task dethroning RNN's type architectures. The Transformer's ability to see the full context of sequence data is transferable to speech as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised Pre-training&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you follow deep learning closely you've probably heard of BERT, GPT, and GPT2. These Transformer models have first pertained on a language modeling task with unlabeled text data, and fine-tuned on a wide array of NLP task and get state of the art results! During pre-training, the model learns something fundamental on the statistics of language and uses that power to excel at other tasks. We believe this technique has great promises on speech data as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Word Piece Models&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our model defined above output characters. Some benefits to that are the model doesn't have to worry about out of vocabulary words when running inference on speech. So for the word &lt;code&gt;c h a t&lt;/code&gt; each character has is its own label. The downside to using characters are inefficiency and the model being prone to more errors because you're predicting one character at a time. Using the whole word as labels have been explored, to some degree of success. Using this method, the entire word &lt;code&gt;chat&lt;/code&gt; would be the label. But using whole words, you would have to keep an index of all possible vocabularies to make a prediction, which is memory inefficient with the possibility of running into out of vocabulary words during prediction. The sweet spot would be using word piece or sub-word units as labels. Instead of characters for the individual label, you can chop up the words into sub-word units, and use those as labels, i.e. &lt;code&gt;ch at&lt;/code&gt;. This solves the out of vocabulary issue, and is much more efficient, as it needs fewer steps to decode then using characters, and without the need to have an index of all possible words. Word pieces have been used successfully with many NLP models, like BERT and would work naturall with speech recognition problems as well.&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 14:11:33 +0000</pubDate>
<dc:creator>makaimc</dc:creator>
<og:image>https://landen.imgix.net/blog_KKMFzSYvUskeYQpX/assets/hsTylpFtzttnaYPj.png?w=1200&amp;h=630&amp;fit=crop</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch</dc:identifier>
</item>
<item>
<title>Show HN: A basketball hoop to maximize shots that go in [video]</title>
<link>https://youtu.be/vtN4tkvcBMA</link>
<guid isPermaLink="true" >https://youtu.be/vtN4tkvcBMA</guid>
<description>&lt;p id=&quot;eow-description&quot; class=&quot;&quot;&gt;I've wondered if it is possible to make a basketball hoop where the ball always goes in. Imagine throwing a ball and if it hits the backboard it somehow gets directed into the hoop. Thanks to physics it isn't quite possible to make ALL possible shots go in though you can make a hoop where the ball goes in a lot more often. In this video I show how I wrote a program to calculate the optical backboard then machined and fabricated it. It worked out better than I was expected so I'm really happy with it!&lt;/p&gt;&lt;p&gt;I've gotten a lot of questions about my iPad setup. I'm using the &quot;concepts&quot; app. It is a pheromonal way to explore ideas since you have an infinite canvas + good zoom + colors + undo. For some of my projects the virtual paper is the size of a dinner table with ideas that branch out in all directions. Also a lot easier to store &amp;amp; share than paper. I highly recommend it if you do a lot of brainstorming.&lt;/p&gt;&lt;p&gt;I've also gotten a lot of requests for the code. At the moment I'd categorize the code as a crime against humanity and I am too ashamed to open source it. I want to revisit this project and improve the performance in the lower corners. If I do that I'll clean up the code and share it.&lt;/p&gt;&lt;p&gt;&quot;duh-dun&quot; sound from &lt;a href=&quot;https://www.youtube.com/redirect?q=http%3A%2F%2Fwww.orangefreesounds.com%2Flaw-and-order-sound%2F&amp;amp;redir_token=iTrUbQotkjFfcIt9CTkcgeKsxjR8MTU4NzI1ODA5NEAxNTg3MTcxNjk0&amp;amp;event=video_description&amp;amp;v=vtN4tkvcBMA&quot; class=&quot;yt-uix-sessionlink&quot; data-url=&quot;/redirect?q=http%3A%2F%2Fwww.orangefreesounds.com%2Flaw-and-order-sound%2F&amp;amp;redir_token=iTrUbQotkjFfcIt9CTkcgeKsxjR8MTU4NzI1ODA5NEAxNTg3MTcxNjk0&amp;amp;event=video_description&amp;amp;v=vtN4tkvcBMA&quot; data-target-new-window=&quot;True&quot; data-sessionlink=&quot;itct=CCQQ6TgYACITCM2Koqjj8OgCFcFBTAgdl9oNXkjAifDe5Jbe6b4B&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;http://www.orangefreesounds.com/law-a...&lt;/a&gt; is licensed under CC BY-NC 4.0 &lt;a href=&quot;https://www.youtube.com/redirect?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby-nc%2F4.0%2F&amp;amp;redir_token=iTrUbQotkjFfcIt9CTkcgeKsxjR8MTU4NzI1ODA5NEAxNTg3MTcxNjk0&amp;amp;event=video_description&amp;amp;v=vtN4tkvcBMA&quot; class=&quot;yt-uix-sessionlink&quot; data-url=&quot;/redirect?q=https%3A%2F%2Fcreativecommons.org%2Flicenses%2Fby-nc%2F4.0%2F&amp;amp;redir_token=iTrUbQotkjFfcIt9CTkcgeKsxjR8MTU4NzI1ODA5NEAxNTg3MTcxNjk0&amp;amp;event=video_description&amp;amp;v=vtN4tkvcBMA&quot; data-target-new-window=&quot;True&quot; data-sessionlink=&quot;itct=CCQQ6TgYACITCM2Koqjj8OgCFcFBTAgdl9oNXkjAifDe5Jbe6b4B&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://creativecommons.org/licenses/...&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 13:12:40 +0000</pubDate>
<dc:creator>swighton</dc:creator>
<og:url>https://www.youtube.com/watch?v=vtN4tkvcBMA</og:url>
<og:title>How I made a basketball hoop that always goes in</og:title>
<og:image>https://i.ytimg.com/vi/vtN4tkvcBMA/maxresdefault.jpg</og:image>
<og:description>I've wondered if it is possible to make a basketball hoop where the ball always goes in. Imagine throwing a ball and if it hits the backboard it somehow gets...</og:description>
<og:type>video.other</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.youtube.com/watch?v=vtN4tkvcBMA&amp;feature=youtu.be</dc:identifier>
</item>
<item>
<title>Roger Penrose ‚Äì Is Mathematics Invented or Discovered? [video]</title>
<link>https://www.youtube.com/watch?v=ujvS2K06dg4</link>
<guid isPermaLink="true" >https://www.youtube.com/watch?v=ujvS2K06dg4</guid>
<description>&lt;p id=&quot;eow-description&quot; class=&quot;&quot;&gt;Free access to Closer to Truth's library of 5,000 videos: &lt;a href=&quot;https://www.youtube.com/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=http%3A%2F%2Fbit.ly%2F376lkKN&amp;amp;event=video_description&quot; class=&quot;yt-uix-sessionlink&quot; data-url=&quot;/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=http%3A%2F%2Fbit.ly%2F376lkKN&amp;amp;event=video_description&quot; data-sessionlink=&quot;itct=CC0Q6TgYACITCNrY8afj8OgCFZehxAodkEUOFUiO7Onpitv0nboB&quot; data-target-new-window=&quot;True&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;http://bit.ly/376lkKN&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Mathematics describes the real world of atoms and acorns, stars and stairs, with remarkable precision. So is mathematics invented by humans just like chisels and hammers and pieces of music? Or is mathematics discovered‚Äîalways out there, somewhere, like mysterious islands waiting to be found? Whatever mathematics is will help define reality itself.&lt;/p&gt;&lt;p&gt;Watch more interviews on the origin of mathematics: &lt;a href=&quot;https://www.youtube.com/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=https%3A%2F%2Fbit.ly%2F345xfse&amp;amp;event=video_description&quot; class=&quot;yt-uix-sessionlink&quot; data-url=&quot;/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=https%3A%2F%2Fbit.ly%2F345xfse&amp;amp;event=video_description&quot; data-sessionlink=&quot;itct=CC0Q6TgYACITCNrY8afj8OgCFZehxAodkEUOFUiO7Onpitv0nboB&quot; data-target-new-window=&quot;True&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://bit.ly/345xfse&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Sir Roger Penrose is an English mathematical physicist, recreational mathematician and philosopher. He is the Emeritus Rouse Ball Professor of Mathematics at the Mathematical Institute of the University of Oxford, as well as an Emeritus Fellow of Wadham College.&lt;/p&gt;&lt;p&gt;Register for free at CTT.com for subscriber-only exclusives: &lt;a href=&quot;https://www.youtube.com/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=http%3A%2F%2Fbit.ly%2F2GXmFsP&amp;amp;event=video_description&quot; class=&quot;yt-uix-sessionlink&quot; data-url=&quot;/redirect?redir_token=z46KzQqmpvyzQY1vZ3vL9GwgL_B8MTU4NzI1ODA5M0AxNTg3MTcxNjkz&amp;amp;v=ujvS2K06dg4&amp;amp;q=http%3A%2F%2Fbit.ly%2F2GXmFsP&amp;amp;event=video_description&quot; data-sessionlink=&quot;itct=CC0Q6TgYACITCNrY8afj8OgCFZehxAodkEUOFUiO7Onpitv0nboB&quot; data-target-new-window=&quot;True&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;http://bit.ly/2GXmFsP&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Closer to Truth presents the world‚Äôs greatest thinkers exploring humanity‚Äôs deepest questions. Discover fundamental issues of existence. Engage new and diverse ways of thinking. Appreciate intense debates. Share your own opinions. Seek your own answers.&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 06:12:45 +0000</pubDate>
<dc:creator>mmmBacon</dc:creator>
<og:url>https://www.youtube.com/watch?v=ujvS2K06dg4</og:url>
<og:title>Roger Penrose - Is Mathematics Invented or Discovered?</og:title>
<og:image>https://i.ytimg.com/vi/ujvS2K06dg4/maxresdefault.jpg</og:image>
<og:description>Free access to Closer to Truth's library of 5,000 videos: http://bit.ly/376lkKN Mathematics describes the real world of atoms and acorns, stars and stairs, w...</og:description>
<og:type>video.other</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.youtube.com/watch?v=ujvS2K06dg4</dc:identifier>
</item>
<item>
<title>How the coronavirus is driving new surveillance programs globally</title>
<link>https://onezero.medium.com/the-pandemic-is-a-trojan-horse-for-surveillance-programs-around-the-world-887fa6f12ec9</link>
<guid isPermaLink="true" >https://onezero.medium.com/the-pandemic-is-a-trojan-horse-for-surveillance-programs-around-the-world-887fa6f12ec9</guid>
<description>&lt;p id=&quot;c2e1&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Ministry of Health has built a &lt;a href=&quot;https://www.boletinoficial.gob.ar/detalleAviso/primera/227170/20200326&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;mandatory app&lt;/a&gt; for those entering the country to keep installed for 14 days, which requires users to give access to location data.&lt;/p&gt;
&lt;p id=&quot;ab52&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;It‚Äôs not clear if the government is actively tracking people with that location data, but the province of Santa Fe is allegedly forcing those who have violated quarantine to download an app that specifically &lt;a href=&quot;https://www.lacapital.com.ar/la-ciudad/controlaran-quienes-incumplieron-elaislamiento-una-app-sus-celulares-n2572740.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;tracks their whereabouts&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;541a&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Those ordered into quarantine could have government surveillance devices installed in their homes or be forced to wear electronic surveillance devices, according to a &lt;a href=&quot;https://www.theregister.co.uk/2020/04/01/west_australia_isolation/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;new law&lt;/a&gt; in the state of Western Australia.&lt;/p&gt;
&lt;p id=&quot;5c37&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;However, the Australian government has &lt;a href=&quot;https://www.abc.net.au/news/2020-04-07/coronavirus-explainer-contact-tracers-not-using-mobile-data/12117882&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;opted not to use&lt;/a&gt; cellphone-based location tracking.&lt;/p&gt;

&lt;p id=&quot;6cda&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;An Austrian telecom gave two days‚Äô worth of anonymized location data to the government in order to analyze movement in the country. The data is &lt;a href=&quot;http://privacyinternational.org/examples/3432/austria-telco-a1-gives-government-location-data-test-movement-restrictions&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;reportedly&lt;/a&gt; unable to be analyzed in groups of fewer than 20 people.&lt;/p&gt;

&lt;p id=&quot;3d61&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The small country of Bahrain has started using &lt;a href=&quot;https://www.top10vpn.com/news/surveillance/covid-19-digital-rights-tracker/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;electronic bracelets&lt;/a&gt; connected to a mobile app to track confirmed cases of the coronavirus, similar to an initiative in Hong Kong. The punishment for being caught breaking the quarantine is a potential prison sentence of at least three months, according to &lt;a href=&quot;https://www.mobihealthnews.com/news/europe/bahrain-launches-electronic-bracelets-keep-track-active-covid-19-cases&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;MobiHealthNews&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;a67f&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Three telecoms in Belgium are &lt;a href=&quot;https://www.weforum.org/agenda/2020/03/role-data-fight-coronavirus-epidemic/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;giving data&lt;/a&gt; to a company called Dalberg Data Insights, which is analyzing the information to detect widespread trends of movement in the country.&lt;/p&gt;
&lt;p id=&quot;a278&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The country is using drones to make announcements, but may also be using the devices to capture surveillance footage, according to Top10VPN‚Äôs &lt;a href=&quot;https://www.top10vpn.com/news/surveillance/covid-19-digital-rights-tracker/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Digital Rights Tracker&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;0680&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Local governments across Brazil are &lt;a href=&quot;https://www.zdnet.com/article/brazil-introduces-surveillance-tech-to-slow-the-spread-of-coronavirus/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;tracking location data&lt;/a&gt; from citizens‚Äô smartphones. The city of Recife alone is tracking 700,000 people‚Äôs locations through their personal devices, and it‚Äôs one of Brazil‚Äôs smaller metropolitan areas.&lt;/p&gt;
&lt;p id=&quot;032f&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Most of this tracking is being done by Brazilian startups working in conjunction with governments.&lt;/p&gt;
&lt;p id=&quot;9cd2&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;‚ÄúWe have visibility of certain behaviors that couldn‚Äôt be captured by other technologies. For example, if an individual leaves their house, we can detect that in a matter of seconds,‚Äù the CEO of one Brazilian firm told Brazilian news site &lt;a href=&quot;https://www.mobiletime.com.br/noticias/26/03/2020/governos-terao-tecnologia-para-detectar-se-pessoas-saem-de-casa/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Mobile Time&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;b533&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;China is using practically every surveillance system in its toolbox: Authorities are tapping publicly located cameras to run facial recognition searches, citizens are being location-tracked through their phones, and drones are being put to use in order to give directions from the government, according to &lt;a href=&quot;https://www.cnbc.com/2020/03/27/coronavirus-surveillance-used-by-governments-to-fight-pandemic-privacy-concerns.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;CNBC&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;64db&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Chinese government is also tracking individuals in more than 200 cities through a smartphone app that grades their health and assigns them a classification of green, yellow, or red, according to the &lt;a href=&quot;https://www.nytimes.com/2020/03/01/business/china-coronavirus-surveillance.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;New York Times&lt;/em&gt;&lt;/a&gt;. The app sends that data to the police and works as a hall pass for entry into certain public places. Travel to designated hot spots, contact with an infected person, or reported symptoms in the app can result in red and yellow designations, which restrict a person‚Äôs movement. How to remove that designation, as well as exactly how those decisions are made, is unclear.&lt;/p&gt;
&lt;p id=&quot;00bf&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;China is also &lt;a href=&quot;https://www.ft.com/content/760142e6-740e-11ea-95fe-fcd274e920ca&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;putting pressure on private companies&lt;/a&gt; in the country to hand over data to further contain the pandemic.&lt;/p&gt;

&lt;p id=&quot;cd80&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Cameras typically used to catch speeding motorists in Dubai will now analyze drivers‚Äô license plates and determine whether they are deemed essential workers, according to &lt;a href=&quot;https://gulfnews.com/uae/covid-19-precaution-dubai-police-using-ai-to-find-out-if-your-trip-was-essential-1.70829268&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Gulf News&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;91db&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The system allegedly tracks drivers throughout their entire trip and will know whether or not the route they take is to their job.&lt;/p&gt;

&lt;p id=&quot;5a5b&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Ecuador is tracking the location of cellphones nationwide to curb the spread of the virus and enforce the country‚Äôs 9 p.m. curfew, according to &lt;a href=&quot;https://www.ecuadortv.ec/noticias/covid-19/romo-vigilancia-epidemiologico-covid19-?&amp;amp;__cf_chl_jschl_tk__=ae0b334279ab1ff25abc3878c4cedd7e86bb68d9-1585931486-0-AdfDYEbsDvr7TlQ_T8--p43z39vu20Uvzq42Kt2Y5fMscSZJAKLMltAbrKnQSDW7EhEbPWx6BYf5fSR0tQ73qWrO6JJ3gr3jjuR6c-MoYdmqU5jNUQUrrMRvSKEGBCX6DoscYGIeY_EylMB3omJIPmENrryyLNFrsktRjeziHYj8m9QAo11920AKTk__ALj5oJweCC8nbh0zvfaxfsa31TgiWVDyq2xEvxv2ksL3JDesa0Dc0csIZfmps2nmpZ78qC19isDp4ToaXAHj_nY0E9XpXPVO_J1U9-PwmL2wIsiqp1UmVGpXE4ZHLP-EzuXi3oNZOG0-2ajDimZdjNZuXAEmH1K3A8LPvMsmA21ii1rL4M0--D3Tlu24QEfhe9hOyqysX70OtUrr797CFccSEnjJlo2Cdz-CnRtSeZByEH8Y&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;EcuadorTV&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;a502&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;German telecom Telekom is providing location data from its customers to the Robert Koch Institute, the organization coordinating the country‚Äôs national action against the coronavirus. Germany is also expected to launch a Bluetooth-based app like those used in Singapore and Indonesia to track personal movement and contact.&lt;/p&gt;
&lt;p id=&quot;9f31&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The country‚Äôs public health authority also launched a &lt;a href=&quot;https://www.nytimes.com/reuters/2020/04/07/technology/07reuters-health-coronavirus-germany-tech.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;smartwatch app&lt;/a&gt; that collects health data in an attempt to determine whether people are exhibiting signs of the coronavirus.&lt;/p&gt;

&lt;p id=&quot;e2da&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Those quarantined in Hong Kong &lt;a href=&quot;https://qz.com/1822215/hong-kong-uses-tracking-wristbands-for-coronavirus-quarantine/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;must wear electronic wristbands&lt;/a&gt; that track their locations. Wristbands are handed out at the airport and must be paired with the individual‚Äôs smartphone.&lt;/p&gt;
&lt;p id=&quot;74b8&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Once a person arrives home, they are given one minute to walk around their apartment to calibrate the wristband and the accompanying app to the space where they are confined.&lt;/p&gt;

&lt;p id=&quot;46e4&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Indian authorities have expanded tracking citizens through digital and analog means. Location data and CCTV footage are being used to track citizens in the southern Indian state of Kerala, according to &lt;a href=&quot;https://www.reuters.com/article/us-health-coronavirus-privacy/privacy-fears-as-india-hand-stamps-suspected-coronavirus-cases-idUSKBN21716U&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Reuters&lt;/a&gt;. Western states are also stamping the hands of those arriving in airports with irremovable ink, with the stamp detailing the date until which the person must quarantine.&lt;/p&gt;
&lt;p id=&quot;df47&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;In addition to personal tracking, Indian authorities are also taking passenger information from airlines and railroad companies.&lt;/p&gt;
&lt;p id=&quot;63c1&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Now that touch-based authentication like fingerprint scanners are considered risky since they require people to touch a common surface, facial recognition is getting a &lt;a class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; href=&quot;https://onezero.medium.com/facial-recognition-companies-see-the-coronavirus-as-a-business-opportunity-6c9b99d60649&quot;&gt;boost in adoption&lt;/a&gt; across India. Secureye, an Indian telecom, is also replacing 650 fingerprint-based security checkpoints in offices and hotels with facial recognition.&lt;/p&gt;

&lt;p id=&quot;d8d1&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Indonesian government has &lt;a href=&quot;https://www.thejakartapost.com/news/2020/03/30/covid-19-indonesia-develops-surveillance-app-to-bolster-contact-tracing-tracking.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;developed an app&lt;/a&gt; that tracks interactions with nearby Bluetooth devices, like other smartphones, in an attempt to track social distancing and personal interactions. It‚Äôs opt-in and offers benefits like notifying people who might have been exposed to get tested for the virus.&lt;/p&gt;

&lt;p id=&quot;2d33&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;A smartphone app developed by the Iranian government scooped up millions of users‚Äô location data alongside a short questionnaire that claimed to detect the likelihood of infection, according to &lt;a href=&quot;https://www.vice.com/en_us/article/epgkmz/iran-launched-an-app-that-claimed-to-diagnose-coronavirus-instead-it-collected-location-data-on-millions-of-people&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Vice&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;e323&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;A notice about the app was sent to tens of millions of Iranians, with the directive to take the questionnaire before going in for a coronavirus test. According to an Iranian official, at least 3.5 million people shared their location.&lt;/p&gt;

&lt;p id=&quot;c6bb&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Israeli government is using data from telecom providers to &lt;a href=&quot;https://www.nytimes.com/2020/03/16/world/middleeast/israel-coronavirus-cellphone-tracking.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;track the locations of millions of citizens&lt;/a&gt; in an attempt to find people diagnosed with the coronavirus and alert those with whom the infected person might have interacted. Those breaking quarantine are threatened with up to six months of imprisonment.&lt;/p&gt;

&lt;p id=&quot;899b&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;English telecom Vodafone is &lt;a href=&quot;https://www.vodafone.com/news-and-media/vodafone-group-releases/news/vodafone-launches-five-point-plan-to-help-counter-the-impacts-of-the-covid-19-outbreak&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;providing&lt;/a&gt; the Italian government with heatmaps of its mobile phone users‚Äô locations, with the first being from Lombardy, Italy. Officials have determined that 40% of people are moving around too much, according to the &lt;a href=&quot;https://www.nytimes.com/2020/03/23/technology/coronavirus-surveillance-tracking-privacy.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;New York Times&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p id=&quot;1659&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Kenyan government is instituting 24/7 aerial surveillance of the country‚Äôs border to detect illegal crossings of goods or people.&lt;/p&gt;

&lt;p id=&quot;6e4b&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Norwegian Institute of Public Health and Norwegian tech company Simula will &lt;a href=&quot;https://www.privacyinternational.org/examples/3534/norwegian-institute-public-health-working-app-store-location-data-30-days&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;build a voluntary app&lt;/a&gt; that tracks GPS and Bluetooth data, to be stored for 30 days.&lt;/p&gt;

&lt;p id=&quot;ccef&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Through location surveillance and mass texts, the government of Pakistan is &lt;a href=&quot;https://www.zdnet.com/article/brazil-introduces-surveillance-tech-to-slow-the-spread-of-coronavirus/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;tracking confirmed cases&lt;/a&gt; of the coronavirus and sending alerts to people found to have potentially come in contact with those suffering the disease in the past 14 days.&lt;/p&gt;

&lt;p id=&quot;1424&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;An app called Home Quarantine &lt;a href=&quot;https://www.privacyinternational.org/examples/3473/poland-app-helps-police-monitor-home-quarantine&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;requires&lt;/a&gt; Polish citizens who are quarantined to intermittently check in by sending a picture of themselves at home within 20 minutes or face a fine.&lt;/p&gt;
&lt;p id=&quot;90c6&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The app uses facial recognition to determine it‚Äôs actually the person being quarantined, and the phone‚Äôs location data is used to make sure they‚Äôre really at home.&lt;/p&gt;

&lt;p id=&quot;254b&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;With more than &lt;a href=&quot;https://www.france24.com/en/20200324-100-000-cameras-moscow-uses-facial-recognition-to-enforce-quarantine&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;100,000 cameras&lt;/a&gt; around Moscow, the Russian government is using facial recognition and &lt;a href=&quot;https://www.privacyinternational.org/examples/3516/russia-orders-system-analyse-smartphone-geolocation-data-contact-tracing-and&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;phone-based location tracking&lt;/a&gt; to monitor those under quarantine.&lt;/p&gt;
&lt;p id=&quot;4289&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Local governments have been called upon to create their own surveillance systems as well. In the Nizhny Novgorod region, citizens download an app that generates a unique, timed QR code that allows them to go out for three hours to get groceries, one hour to walk a dog, or 30 minutes to take out the trash, according to the &lt;a href=&quot;https://www.washingtonpost.com/world/europe/coronavirus-russia-surveillance-tracking/2020/04/04/0798f4dc-7519-11ea-ad9b-254ec99993bc_story.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Washington Post&lt;/em&gt;&lt;/a&gt;&lt;em class=&quot;ke&quot;&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p id=&quot;2503&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Singapore government released an app called TraceTogether, which pings nearby smartphones through Bluetooth to determine which people have come within 6.5 feet of each other for more than 30 minutes, according to the &lt;a href=&quot;https://www.latimes.com/world-nation/story/2020-03-24/coronavirus-singapore-trace-together&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Los Angeles Times&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;8998&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The data is stored for 21 days, according to the developers&lt;em class=&quot;ke&quot;&gt;,&lt;/em&gt; and does not record the users‚Äô location.&lt;/p&gt;

&lt;p id=&quot;c869&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;South African telecom companies are handing over cellphone location data in order to track the pandemic, according to &lt;a href=&quot;https://www.businessinsider.co.za/south-africa-will-be-tracking-cellphones-to-fight-covid-19-2020-3?fbclid=IwAR2SuMq5K3QiaX5UPs0XQg0pAXDWLh4j8INxDqxr3ftj1l_1lfdbPNLTMOs&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Business Insider South Africa&lt;/em&gt;&lt;/a&gt;. Reportedly &lt;a href=&quot;https://techcabal.com/2020/04/03/techcabal-daily-south-africa-okays-no-consent-tracking-of-covid-19-cases-by-cellphone-data/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;1,500 people‚Äôs data&lt;/a&gt; are being shared.&lt;/p&gt;

&lt;p id=&quot;433c&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Confirmed cases of the coronavirus are being tracked in South Korea by a fusion of credit card purchases, smartphone location tracking, and CCTV footage, presumably analyzed by facial recognition algorithms, according to &lt;a href=&quot;https://blogs.thomsonreuters.com/answerson/south-korea-covid-19-data-privacy/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Reuters&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;ff45&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;This allows the Korean government to reconstruct the past actions of those diagnosed with the virus with incredible granularity, like using the person‚Äôs location data to check nearby CCTV footage and see if they were wearing a mask, Reuters reports.&lt;/p&gt;

&lt;p id=&quot;111d&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;In an attempt to enforce social distancing, telecom company Swisscom will &lt;a href=&quot;https://www.privacyinternational.org/examples/3492/switzerland-swisscom-inform-federal-authorities-gatherings&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;alert the federal government&lt;/a&gt; when more than 20 phones are located in a 100-square-meter area.&lt;/p&gt;

&lt;p id=&quot;b8de&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Although the government &lt;a href=&quot;https://www.theguardian.com/world/2020/mar/13/how-taiwan-is-containing-coronavirus-despite-diplomatic-isolation-by-china#img-1&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;rejects the accusation&lt;/a&gt; that it‚Äôs adopting surveillance technology, Taiwan is tracking its citizens‚Äô movement by triangulating the location of their cellphone between nearby cell towers.&lt;/p&gt;

&lt;p id=&quot;40ff&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Those arriving in Thailand from high-risk areas will be &lt;a href=&quot;https://www.privacyinternational.org/examples/3452/thailand-sim-card-and-app-track-travellers&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;given a SIM card&lt;/a&gt; that lets the government track their movement for 14 days.&lt;/p&gt;

&lt;p id=&quot;8c6a&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The Turkish government is tracking the locations of coronavirus patients using their cellular data, and will automatically send warning messages if they are detected violating quarantine. All cellular companies operating in Turkey are cooperating with the government to provide this data, according to the state-run &lt;a href=&quot;https://www.aa.com.tr/en/latest-on-coronavirus-outbreak/turkey-launches-covid-19-isolation-tracking-project/1797961&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Anadolu Agency&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;ff90&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Last week, a draft law was proposed that would mandate that social media platforms like Facebook, YouTube, and WhatsApp have a legal representative in Turkey that the government could pressure to take down content or ban accounts, according to &lt;a href=&quot;https://www.hrw.org/news/2020/04/13/turkey-seeks-power-control-social-media&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Human Rights Watch&lt;/a&gt;. However, those parts of the draft were later withdrawn.&lt;/p&gt;

&lt;p id=&quot;70f4&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The U.K. is &lt;a href=&quot;https://www.businessinsider.com/explainer-coronavirus-uk-phone-tracking-2020-3&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;allegedly&lt;/a&gt; talking with telecom companies to track its citizens‚Äô location data. In the meantime, the National Health Service has &lt;a href=&quot;https://techcrunch.com/2020/04/01/palantir-coronavirus-cdc-nhs-gotham-foundry/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;partnered with Palantir&lt;/a&gt; to track the spread of the virus and its impact on the health system.&lt;/p&gt;

&lt;p id=&quot;c68f&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;To help track the movement of citizens, the mobile advertising industry is currently supplying data to local, state, and federal government organizations about the location of individuals, according to a &lt;a href=&quot;https://www.wsj.com/articles/government-tracking-how-people-move-around-in-coronavirus-pandemic-11585393202&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;report&lt;/a&gt; by the &lt;em class=&quot;ke&quot;&gt;Wall Street Journal&lt;/em&gt;. The data is granular enough to tell whether people are complying with stay-at-home directions or if parks are still in use. Foursquare, which has one of the most comprehensive repositories of personal location data, is in talks with numerous government organizations, according to the &lt;em class=&quot;ke&quot;&gt;WSJ&lt;/em&gt;. Most data being used comes from apps that have permission to log a user‚Äôs location, which is then compiled and resold.&lt;/p&gt;
&lt;p id=&quot;9a47&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;The goal of these efforts is to create a portal that could track citizen movement in up to 500 U.S. cities. Google is also contributing a trove of movement data, which it &lt;a href=&quot;https://www.wsj.com/articles/google-offers-user-location-data-to-health-officials-tackling-coronavirus-11585893602?mod=article_inline&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;collects&lt;/a&gt; for services like Google Maps‚Äô traffic function.&lt;/p&gt;
&lt;p id=&quot;db62&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;There are also troubling state and local policies. In West Virginia, those who test positive for the virus but refuse to quarantine are being outfitted with GPS ankle monitors, according to the &lt;a href=&quot;https://apnews.com/6b082395194d53b10629bb9bb3d037a2&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Associated Press&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;7b26&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Apple and Google recently announced a new set of digital tools for detecting if individuals have come in close contact with those diagnosed with the coronavirus. The software will eventually be integrated into Android and Apple phones, making the tracking all but ubiquitous. Many of the software‚Äôs privacy-guarding machinations are still unknown, but you can read more about what we know so far in &lt;a href=&quot;https://coronavirus.medium.com/apple-and-google-join-forces-to-track-spread-of-coronavirus-fc2c68fda0ea&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Medium‚Äôs Coronavirus Blog&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;90c2&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;PredPol, a controversial predictive policing company used in cities &lt;a href=&quot;https://boingboing.net/2018/10/30/el-monte-and-tacoma.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;across the United States&lt;/a&gt; to theoretically identify future crime hotspots, has advised on new ways customers can use its software for coronavirus control in a company &lt;a href=&quot;http://blog.predpol.com/using-custom-predpol-boxes-for-covid-19-patrols?utm_content=125287660&amp;amp;utm_medium=social&amp;amp;utm_source=twitter&amp;amp;hss_channel=tw-479889375http://blog.predpol.com/using-custom-predpol-boxes-for-covid-19-patrols&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;blog post&lt;/a&gt; Wednesday.&lt;/p&gt;
&lt;p id=&quot;d1ac&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;‚ÄúMost people gracefully accept these orders to protect their fellow citizens, but not everyone cooperates, so rolling a cruiser through their neighborhood periodically can remind them of their obligations,‚Äù the blog said.&lt;/p&gt;
&lt;p id=&quot;b629&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Police in New Jersey and Connecticut are &lt;a href=&quot;https://abc7ny.com/coronavirus-drones-covid-19-pandemic-nj/6102905/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;using aerial drones&lt;/a&gt; with temperature sensors and other apparatus to detect people outside who might have the coronavirus. The drones are made by a company called Draganfly, which claims the drones can detect fevers, sneezing, respiratory rate, and whether people are standing an appropriate distance from one another.&lt;/p&gt;
&lt;p id=&quot;e2a8&quot; class=&quot;jb jc do ay jd b et je ev jf jg jh ji jj jk jl jm dt&quot;&gt;Onfido, a British startup that tries to tie government documentation to your digital identity, is in early talks with U.S. and European countries about developing a ‚Äúpassport‚Äù that would prove immunity to the coronavirus, according to &lt;a href=&quot;https://www.axios.com/identity-passports-coronavirus-onfido-ce569830-85a3-4b1e-b8ad-7e61b52aa817.html&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;Axios&lt;/em&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.cityam.com/british-ai-startup-onfido-in-talks-to-provide-coronavirus-immunity-passports/&quot; class=&quot;gf hc kf kg kh ki&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;ke&quot;&gt;City A.M&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Fri, 17 Apr 2020 04:48:22 +0000</pubDate>
<dc:creator>walterbell</dc:creator>
<og:type>article</og:type>
<og:title>We Mapped How the Coronavirus Is Driving New Surveillance Programs Around the World</og:title>
<og:description>At least 30 countries are ramping up surveillance to combat the coronavirus</og:description>
<og:url>https://onezero.medium.com/the-pandemic-is-a-trojan-horse-for-surveillance-programs-around-the-world-887fa6f12ec9</og:url>
<og:image>https://miro.medium.com/focal/1200/632/49/43/1*J-731K3-hHjIwLdkRCDtvA.jpeg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://onezero.medium.com/the-pandemic-is-a-trojan-horse-for-surveillance-programs-around-the-world-887fa6f12ec9</dc:identifier>
</item>
</channel>
</rss>