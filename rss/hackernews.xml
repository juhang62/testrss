<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>The Architecture of Open Source Applications</title>
<link>http://www.aosabook.org/en/index.html</link>
<guid isPermaLink="true" >http://www.aosabook.org/en/index.html</guid>
<description>&lt;p&gt;Architects look at thousands of buildings during their training, and study critiques of those buildings written by masters. In contrast, most software developers only ever get to know a handful of large programs well—usually programs they wrote themselves—and never study the great programs of history. As a result, they repeat one another's mistakes rather than building on one another's successes.&lt;/p&gt;
&lt;p&gt;Our goal is to change that. In these two books, the authors of four dozen open source applications explain how their software is structured, and why. What are each program's major components? How do they interact? And what did their builders learn during their development? In answering these questions, the contributors to these books provide unique insights into how they think.&lt;/p&gt;
&lt;p&gt;If you are a junior developer, and want to learn how your more experienced colleagues think, these books are the place to start. If you are an intermediate or senior developer, and want to see how your peers have solved hard design problems, these books can help you too.&lt;/p&gt;
&lt;p&gt;Follow us on our blog at &lt;a href=&quot;http://aosabook.org/blog/&quot;&gt;http://aosabook.org/blog/&lt;/a&gt; or on Twitter at &lt;a href=&quot;http://twitter.com/aosabook&quot;&gt;@aosabook&lt;/a&gt; and using the &lt;a href=&quot;http://twitter.com/#!/search/%23aosa&quot;&gt;#aosa&lt;/a&gt; hashtag.&lt;/p&gt;
</description>
<pubDate>Sun, 18 Aug 2019 16:33:35 +0000</pubDate>
<dc:creator>pcr910303</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.aosabook.org/en/index.html</dc:identifier>
</item>
<item>
<title>YAML: Probably not so great after all</title>
<link>https://arp242.net/yaml-config.html</link>
<guid isPermaLink="true" >https://arp242.net/yaml-config.html</guid>
<description>&lt;p&gt;I previously wrote &lt;a href=&quot;https://arp242.net/json-config.html&quot;&gt;why using JSON for human-editable configuration files is a bad idea&lt;/a&gt;. Today we’re going to look at some general problems with the YAML format.&lt;/p&gt;
&lt;h2 id=&quot;insecure-by-default&quot;&gt;Insecure by default &lt;/h2&gt;
&lt;p&gt;YAML is insecure by default. Loading a user-provided (untrusted) YAML string needs careful consideration.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;!!python/object/apply:os.system
args: ['ls /']
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Running it with &lt;code&gt;print(yaml.load(open('a.yaml')))&lt;/code&gt; should give you something like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;bin   etc   lib    lost+found  opt   root  sbin  tmp  var sys
boot  dev   efi    home        lib64 mnt   proc  run  srv usr
0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Many other languages (including Ruby and PHP&lt;sup id=&quot;fnref:1&quot;/&gt;) are also unsafe by default. Searching for &lt;code&gt;yaml.load&lt;/code&gt; and &lt;code&gt;yaml.safe_load&lt;/code&gt; on GitHub yields &lt;a href=&quot;https://github.com/search?q=%22yaml.load%22+language%3Apython&amp;amp;type=Code&quot;&gt;215k&lt;/a&gt; and &lt;a href=&quot;https://github.com/search?q=%22yaml.safe_load%22+language%3Apython&amp;amp;type=Code&quot;&gt;53k&lt;/a&gt; results respectively. Many of those &lt;code&gt;yaml.load()&lt;/code&gt;s are fine – loading a config file with &lt;code&gt;yaml.load()&lt;/code&gt; is often okay since it’s usually (though not always!) from a ‘trusted source’, and many are from test files with static YAML. But still, one can’t help but wonder how many exploits are hidden in those 215k results (repeat for Ruby, PHP, Java, etc.)&lt;/p&gt;
&lt;p&gt;This is not a theoretical problem. In 2013 &lt;a href=&quot;https://www.sitepoint.com/anatomy-of-an-exploit-an-in-depth-look-at-the-rails-yaml-vulnerability/&quot;&gt;every Ruby on Rails application ever written was found to be vulnerable&lt;/a&gt; to remote code execution due to exactly this problem.&lt;/p&gt;
&lt;p&gt;One might argue this is not really the fault of the YAML format &lt;em&gt;as such&lt;/em&gt;, but rather the fault of the libraries implementing it wrong, but it seems to be the case that the majority of libraries are unsafe by default (especially the dynamic languages), so &lt;em&gt;de-facto&lt;/em&gt; it is a problem with YAML.&lt;/p&gt;
&lt;p&gt;One might also argue that fixing it is as easy as replacing &lt;code&gt;load()&lt;/code&gt; with &lt;code&gt;safe_load()&lt;/code&gt;, but many people are unaware of the problem, and even &lt;em&gt;if&lt;/em&gt; you’re aware of it, it’s one of those things that can be easy to forget. It’s pretty bad API design.&lt;/p&gt;
&lt;h2 id=&quot;can-be-hard-to-edit-especially-for-large-files&quot;&gt;Can be hard to edit, especially for large files &lt;/h2&gt;
&lt;p&gt;YAML files can be hard to edit, and this difficulty grows fast as the file gets larger.&lt;/p&gt;
&lt;p&gt;A good example of this are Ruby on Rails’ translation files; for example:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;en:
   formtastic:
         labels:
           title: &quot;Title&quot;  # Default global value
           article:
                 body: &quot;Article content&quot;
           post:
                 new:
                   title: &quot;Choose a title...&quot;
                   body: &quot;Write something...&quot;
                 edit:
                   title: &quot;Edit title&quot;
                   body: &quot;Edit body&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This still looks okay, right? But what if this file has 100 lines? Or 1,000 lines? It is difficult to see “where” in the file you are because it may be off the screen. You’ll need to scroll up, but then you need to keep track of the indentation, which can be pretty hard even with indentation guides, especially since 2-space indentation is the norm and &lt;a href=&quot;http://www.yaml.org/faq.html&quot;&gt;tab indentation is forbidden&lt;/a&gt;.&lt;sup id=&quot;fnref:2&quot;/&gt;&lt;/p&gt;
&lt;p&gt;And accidentally getting the indentation wrong often isn’t an error; it will often just deserialize to something you didn’t intend. Happy debugging!&lt;/p&gt;
&lt;p&gt;I’ve been happily programming Python for over a decade, so I’m used to significant whitespace, but sometimes I’m still struggling with YAML. In Python the drawbacks and loss of clarity are contained by not having functions that are several pages long, but data or configuration files have no such natural limits to their length.&lt;/p&gt;
&lt;p&gt;For small files this is not a problem; but it really doesn’t scale well to larger files, especially not if you want to edit them later on.&lt;/p&gt;
&lt;h2 id=&quot;its-pretty-complex&quot;&gt;It’s pretty complex &lt;/h2&gt;
&lt;p&gt;YAML may seem ‘simple’ and ‘obvious’ when glancing at a basic example, but turns out it’s not. The &lt;a href=&quot;http://yaml.org/spec/1.2/spec.pdf&quot;&gt;YAML spec&lt;/a&gt; is 23,449 words; for comparison, &lt;a href=&quot;https://github.com/toml-lang/toml&quot;&gt;TOML&lt;/a&gt; is 3,339 words, &lt;a href=&quot;http://www.ecma-international.org/publications/files/ECMA-ST/ECMA-404.pdf&quot;&gt;JSON&lt;/a&gt; is 1,969 words, and &lt;a href=&quot;https://www.w3.org/TR/REC-xml/&quot;&gt;XML&lt;/a&gt; is 20,603 words.&lt;/p&gt;
&lt;p&gt;Who among us have read all that? Who among us have read and &lt;em&gt;understood&lt;/em&gt; all of that? Who among of have read, &lt;em&gt;understood&lt;/em&gt;, and &lt;strong&gt;remembered&lt;/strong&gt; all of that?&lt;/p&gt;
&lt;p&gt;For example did you know there are &lt;a href=&quot;http://stackoverflow.com/a/21699210/660921&quot;&gt;&lt;em&gt;nine&lt;/em&gt; ways to write a multi-line string in YAML&lt;/a&gt; with subtly different behaviour?&lt;/p&gt;
&lt;p&gt;Yeah :-/&lt;/p&gt;
&lt;p&gt;That post gets even more interesting if you look at &lt;a href=&quot;http://stackoverflow.com/posts/21699210/revisions&quot;&gt;its revision history&lt;/a&gt;, as the author of the post discovers more and more ways to do this and more of the subtleties involved.&lt;/p&gt;
&lt;p&gt;It’s telling that the YAML spec starts with a preview, which states (emphases mine):&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;This section provides a quick glimpse into the expressive power of YAML. &lt;strong&gt;It is not expected that the first-time reader grok all of the examples&lt;/strong&gt;. Rather, these selections are used as motivation for the remainder of the specification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;surprising-behaviour&quot;&gt;Surprising behaviour &lt;/h3&gt;
&lt;p&gt;What does this parse to (examples courtesy of &lt;a href=&quot;https://github.com/crdoconnor/strictyaml/blob/master/FAQ.rst#what-is-wrong-with-implicit-typing&quot;&gt;Colm O’Connor&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;- Don Corleone: Do you have faith in my judgment?
- Clemenza: Yes
- Don Corleone: Do I have your loyalty?
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Yup!&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;[
        {'Don Corleone': 'Do you have faith in my judgment?'},
        {'Clemenza': True},
        {'Don Corleone': 'Do I have your loyalty?'}
]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Or what about:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;python: 3.5.3
postgres: 9.3
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;3.5.3&lt;/code&gt; gets recognized as as string, but &lt;code&gt;9.3&lt;/code&gt; gets recognized as a number instead of a string:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{'python': '3.5.3', 'postgres': 9.3}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Or what about:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Effenaar: Eindhoven
013: Tilburg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;013 is a popular music Venue in Tilburg, but YAML will send you the wrong way since it’s parsed as an octal number:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{11: 'Tilburg', 'Effenaar': 'Eindhoven'}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All of this – and more – is why many experienced YAMLers will often quote all strings, even when it’s not strictly required. Many people don’t use quotes, and it can be easy to forget especially if the rest of the file – possibly written by other people – doesn’t use quotes.&lt;/p&gt;
&lt;h3 id=&quot;its-not-portable&quot;&gt;It’s not portable &lt;/h3&gt;
&lt;p&gt;Because it’s so complex, its claims of portability have been greatly exaggerated. For example consider this example taken from the YAML spec:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;? - Detroit Tigers
  - Chicago cubs
:
  - 2001-07-23

? [ New York Yankees,
        Atlanta Braves ]
: [ 2001-07-02, 2001-08-12,
        2001-08-14 ]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Aside from the fact that most readers of this probably won’t even know what this does, try parsing it in Python with PyYAML:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;yaml.constructor.ConstructorError: while constructing a mapping
  in &quot;a.yaml&quot;, line 1, column 1
found unhashable key
  in &quot;a.yaml&quot;, line 1, column 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In Ruby it works:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{
        [&quot;Detroit Tigers&quot;, &quot;Chicago cubs&quot;] =&amp;gt; [
                #&amp;lt;Date: 2001-07-23 ((2452114j,0s,0n),+0s,2299161j)&amp;gt;
        ],
        [&quot;New York Yankees&quot;, &quot;Atlanta Braves&quot;] =&amp;gt; [
                #&amp;lt;Date: 2001-07-02 ((2452093j,0s,0n),+0s,2299161j)&amp;gt;,
                #&amp;lt;Date: 2001-08-12 ((2452134j,0s,0n),+0s,2299161j)&amp;gt;,
                #&amp;lt;Date: 2001-08-14 ((2452136j,0s,0n),+0s,2299161j)&amp;gt;
        ]
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The reason for this is because you can’t use a list as a dict key in Python:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; {['a']: 'zxc'}
Traceback (most recent call last):
  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;
  TypeError: unhashable type: 'list'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And this restriction is not unique to Python; common languages such as PHP, JavaScript, and Go all share this restriction.&lt;/p&gt;
&lt;p&gt;So use this in a YAML file, and you won’t be able to read it in most languages.&lt;/p&gt;
&lt;p&gt;Here’s another example again taken from the examples section of the YAML spec:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# Ranking of 1998 home runs
---
- Mark McGwire
- Sammy Sosa
- Ken Griffey

# Team ranking
---
- Chicago Cubs
- St Louis Cardinals
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Python says:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;yaml.composer.ComposerError: expected a single document in the stream
  in &quot;a.yaml&quot;, line 3, column 1
but found another document
  in &quot;a.yaml&quot;, line 8, column 1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;While Ruby outputs:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;[&quot;Mark McGwire&quot;, &quot;Sammy Sosa&quot;, &quot;Ken Griffey&quot;]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The reason for this is that there are multiple YAML documents in a single file (&lt;code&gt;---&lt;/code&gt; start the document). In Python there is the &lt;code&gt;load_all()&lt;/code&gt; function to parse all documents. Ruby’s &lt;code&gt;load()&lt;/code&gt; just loads the first document, and as near as I can tell, doesn’t have a way to load multiple documents.&lt;/p&gt;
&lt;p&gt;There are &lt;a href=&quot;https://github.com/cblp/yaml-sucks&quot;&gt;many more incompatibilities between implementations&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;goals-achieved&quot;&gt;Goals achieved? &lt;/h2&gt;
&lt;p&gt;The spec states:&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;The design goals for YAML are, in decreasing priority:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;YAML is easily readable by humans.&lt;/li&gt;
&lt;li&gt;YAML data is portable between programming languages.&lt;/li&gt;
&lt;li&gt;YAML matches the native data structures of agile languages.&lt;/li&gt;
&lt;li&gt;YAML has a consistent model to support generic tools.&lt;/li&gt;
&lt;li&gt;YAML supports one-pass processing.&lt;/li&gt;
&lt;li&gt;YAML is expressive and extensible.&lt;/li&gt;
&lt;li&gt;YAML is easy to implement and use.&lt;/li&gt;
&lt;/ol&gt;&lt;/blockquote&gt;
&lt;p&gt;So how well does it do?&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML is easily readable by humans.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;True only if you stick to a small subset. The full set is complex – much &lt;em&gt;more&lt;/em&gt; so than XML or JSON.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML data is portable between programming languages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not really true, as it’s too easy to create constructs that are not supported by common languages.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML matches the native data structures of agile languages.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;See above. Plus, why only support agile (or dynamic) languages? What about other languages?&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML has a consistent model to support generic tools.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I am not even sure what this means and I can’t find any elaboration.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML supports one-pass processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’ll take their word for it.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML is expressive and extensible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well, it is, but it’s &lt;em&gt;too&lt;/em&gt; expressive (e.g. too complex).&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;YAML is easy to implement and use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
&lt;code&gt;$ wc -l ~/go/src/github.com/go-yaml/yaml/*.go~*_test.go | tail -n1
  9566 total

$ wc -l /usr/lib/python3.7/site-packages/yaml/*.py | tail -n1
  5725 total
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion &lt;/h2&gt;
&lt;p&gt;Don’t get me wrong, it’s not like YAML is absolutely terrible – it’s probably better than &lt;a href=&quot;https://arp242.net/json-config.html&quot;&gt;using JSON&lt;/a&gt; – but it’s not exactly great either. There are some drawbacks and surprises that are not at all obvious at first, and there are a number of better alternatives such as &lt;a href=&quot;https://github.com/toml-lang/toml&quot;&gt;TOML&lt;/a&gt; and other more specialized formats. Personally I’m not likely to use it again when I’ve got a choice.&lt;/p&gt;
&lt;p&gt;One good alternative might be to just &lt;a href=&quot;https://arp242.net/flags-config.html&quot;&gt;use commandline flags&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you &lt;em&gt;must&lt;/em&gt; use YAML then I recommend you use &lt;a href=&quot;https://github.com/crdoconnor/strictyaml&quot;&gt;StrictYAML&lt;/a&gt;, which removes some (though not all) of the more hairy parts.&lt;/p&gt;
&lt;div class=&quot;postscript&quot;&gt;&lt;strong&gt;Footnotes&lt;/strong&gt;
&lt;ol readability=&quot;1&quot;&gt;&lt;li id=&quot;fn:1&quot; readability=&quot;0&quot;&gt;
&lt;p&gt;In PHP you need to modify an INI setting for the safe behaviour; you can’t just call something like &lt;code&gt;yaml_safe()&lt;/code&gt;. The PHP folks managed to make something stupid &lt;em&gt;even more stupid&lt;/em&gt;. Congratulations. &lt;/p&gt;
&lt;/li&gt;
&lt;li id=&quot;fn:2&quot; readability=&quot;2&quot;&gt;
&lt;p&gt;Don’t want to start the spaces vs. tabs debate here, but if tabs would be allowed I would be able to (temporarily) increase the tab width to a higher number to make it easier to see – this is sort of the point of tabs. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
&lt;footer class=&quot;postscript&quot; readability=&quot;17.021739130435&quot;&gt;&lt;strong&gt;Feedback&lt;/strong&gt;
&lt;p&gt;Mail me at &lt;a href=&quot;mailto:martin@arp242.net&quot;&gt;martin@arp242.net&lt;/a&gt; or &lt;a href=&quot;https://github.com/arp242/arp242.net/issues/new&quot;&gt;create a GitHub issue&lt;/a&gt; for feedback, questions, etc.&lt;/p&gt;
&lt;/footer&gt;
</description>
<pubDate>Sun, 18 Aug 2019 16:26:32 +0000</pubDate>
<dc:creator>kylequest</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arp242.net/yaml-config.html</dc:identifier>
</item>
<item>
<title>Media Can’t Stop Presenting Horrifying Stories as ‘Uplifting’ Perseverance Porn</title>
<link>https://fair.org/home/media-just-cant-stop-presenting-horrifying-stories-as-uplifting-perseverance-porn/</link>
<guid isPermaLink="true" >https://fair.org/home/media-just-cant-stop-presenting-horrifying-stories-as-uplifting-perseverance-porn/</guid>
<description>
&lt;div id=&quot;attachment_9007960&quot; class=&quot;wp-caption alignright&quot; readability=&quot;32.591715976331&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-9007960&quot; class=&quot; wp-image-9007960&quot; src=&quot;https://fair.org/wp-content/uploads/2019/07/Fox-Awesome.png&quot; alt=&quot;Fox 5 DC: THIS IS AWESOME&quot; width=&quot;350&quot; height=&quot;401&quot; srcset=&quot;https://fair.org/wp-content/uploads/2019/07/Fox-Awesome.png 635w, https://fair.org/wp-content/uploads/2019/07/Fox-Awesome-262x300.png 262w&quot; sizes=&quot;(max-width: 350px) 100vw, 350px&quot;/&gt;&lt;p id=&quot;caption-attachment-9007960&quot; class=&quot;wp-caption-text&quot;&gt;&lt;em&gt;“No… it’s not awesome at all. It’s a painful indictment of the state of healthcare in America,” reads the first comment under this tweet by &lt;strong&gt;Fox 5 DC&lt;/strong&gt; (&lt;a href=&quot;https://twitter.com/fox5dc/status/1133426102754521088?s=19&quot;&gt;5/28/19&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;“THIS IS AWESOME!” That’s how &lt;strong&gt;Fox 5 DC&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/fox5dc/status/1133426102754521088?s=19&quot;&gt;described&lt;/a&gt; its story (&lt;a href=&quot;http://www.fox5dc.com/health/home-depot-employees-build-walker-for-boy-with-hypotonia&quot;&gt;5/28/19&lt;/a&gt;) about Logan Moore of Cedartown, GA, a disabled two-year-old whose parents were unable to afford to buy him a walker, so employees at Home Depot fashioned one together themselves for him.&lt;/p&gt;
&lt;p&gt;The story closely resembles another recent &lt;strong&gt;CNN&lt;/strong&gt; report (&lt;a href=&quot;https://edition.cnn.com/2019/04/01/health/minnesota-boy-robotics-car-trnd/index.html?utm_source=twCNN&amp;amp;utm_content=2019-04-02T06%3A45%3A30&amp;amp;utm_medium=social&amp;amp;utm_term=image&quot;&gt;4/1/19&lt;/a&gt;): “A Two-Year-Old Couldn’t Walk on His Own. So a High School Robotics Team Built Him a Customized Toy Car.” That piece noted how Minnesotan toddler Cillian Jackson couldn’t walk due to a genetic condition, and how his parents couldn’t afford treatment. It described the ingenuity of the school children who built him a car, and Cillian’s new found freedom, but did not explore why a baby with a disability had been abandoned by US society.&lt;/p&gt;
&lt;p&gt;The clear implication in these stories was that those children would have been left permanently unable to move if not for the help of underpaid employees or the kindness of other children. How many disabled American children with poor parents were not so lucky? The articles did not ask. Instead, they were presented as “uplifting” human interest pieces.&lt;/p&gt;
&lt;p&gt;Cillian’s story is part of &lt;strong&gt;CNN&lt;/strong&gt;’s &lt;strong&gt;Good Stuff&lt;/strong&gt; series, which asks its readers:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;Want more inspiring, positive news? Sign up for &lt;strong&gt;The Good Stuff&lt;/strong&gt;, a newsletter for the good in life. It will brighten your inbox every Saturday morning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Unfortunately, these stories are part of a popular trend of unintentionally horrifying “uplifting” news, which we at FAIR have catalogued before (&lt;strong&gt;FAIR.org&lt;/strong&gt;, &lt;a href=&quot;https://fair.org/home/medias-grim-addiction-to-perseverance-porn/&quot;&gt;8/3/17&lt;/a&gt;; &lt;a href=&quot;https://fair.org/home/the-homeless-8-year-old-chess-champion-and-other-horrific-uplifting-stories/&quot;&gt;3/25/19&lt;/a&gt;), where out-of-touch corporate media give us supposedly charming, wholesome and positive news that actually, upon even minimal retrospection, reveals the dire conditions of late capitalism so many Americans now live under, and makes you feel worse after reading it.&lt;/p&gt;
&lt;p&gt;A lot of these stories involve mothers and the extremely difficult circumstances of raising children in the US while poor. &lt;strong&gt;CNN&lt;/strong&gt;’s “feel good” story (&lt;a href=&quot;https://edition.cnn.com/2018/08/24/us/chicago-teacher-helps-student-with-baby-trnd/index.html&quot;&gt;8/24/18&lt;/a&gt;) about a teacher sitting in a car with her student’s baby so the new mom could attend a job fair raised far more questions than it asked (which was zero). Why is there so little public childcare in the US? Should a new mother really need to immediately find a job so badly? Is this good for infants’ development?&lt;/p&gt;
&lt;div id=&quot;attachment_9007961&quot; class=&quot;wp-caption alignright&quot; readability=&quot;32.464516129032&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-9007961&quot; class=&quot; wp-image-9007961&quot; src=&quot;https://fair.org/wp-content/uploads/2019/07/GMA-Baby-Shower.png&quot; alt=&quot;GMA: Donating vacation time to new moms is a trendy co-worker baby shower gift&quot; width=&quot;350&quot; height=&quot;306&quot; srcset=&quot;https://fair.org/wp-content/uploads/2019/07/GMA-Baby-Shower.png 555w, https://fair.org/wp-content/uploads/2019/07/GMA-Baby-Shower-300x262.png 300w&quot; sizes=&quot;(max-width: 350px) 100vw, 350px&quot;/&gt;&lt;p id=&quot;caption-attachment-9007961&quot; class=&quot;wp-caption-text&quot;&gt;&lt;em&gt;Donated maternity leave is a “trendy” gift you don’t need—unless you live in the United States or Papua New Guinea (&lt;strong&gt;Good Morning America&lt;/strong&gt;, &lt;a href=&quot;https://www.goodmorningamerica.com/living/story/donating-vacation-time-moms-trendy-worker-baby-shower-55632450&quot;&gt;7/17/18&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;On a similar subject, &lt;strong&gt;Good Morning America&lt;/strong&gt; (&lt;a href=&quot;https://www.goodmorningamerica.com/living/story/donating-vacation-time-moms-trendy-worker-baby-shower-55632450&quot;&gt;7/17/18&lt;/a&gt;) describes the “trendy” new baby-shower gift of donating your pregnant co-worker your days off to give her maternity leave. &lt;a href=&quot;https://www.weforum.org/agenda/2019/03/switzerland-ranked-as-best-country-for-womens-rights-oecd/&quot;&gt;Every country in the world&lt;/a&gt; except the US and Papua New Guinea guarantees paid maternity leave, meaning the trend is unlikely to catch on abroad.&lt;/p&gt;
&lt;p&gt;Many outlets (&lt;strong&gt;CBS&lt;/strong&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=N-RZyaLm0QU&quot;&gt;5/20/16&lt;/a&gt;; &lt;strong&gt;Huffington Post&lt;/strong&gt;, &lt;a href=&quot;https://www.huffingtonpost.co.uk/entry/this-custodians-5-kids-attended-elite-college-hes-cleaned-for-years_n_5756f5a5e4b0ca5c7b502a86&quot;&gt;8/6/16&lt;/a&gt;; &lt;strong&gt;People&lt;/strong&gt;, &lt;a href=&quot;https://people.com/celebrity/dad-works-night-shift-as-janitor-to-put-all-5-of-his-kids-through-college/&quot;&gt;4/11/16&lt;/a&gt;) cheerfully reported on how one man did at least 15 years of backbreaking labor as a night shift janitor at Boston College so his children could attend for free. But none even mentioned that if he lived in nearly any country in Western Europe, this wouldn’t have been necessary, as university there is &lt;a href=&quot;http://worldpopulationreview.com/countries/countries-with-free-college/&quot;&gt;free or virtually free&lt;/a&gt; to attend.&lt;/p&gt;
&lt;p&gt;In fact, rather than discussing ballooning tuition costs, &lt;strong&gt;Yahoo!&lt;/strong&gt; (&lt;a href=&quot;https://www.yahoo.com/lifestyle/custodian-worked-night-shift-23-years-put-5-kids-college-trickle-effect-120028969.html&quot;&gt;11/15/17&lt;/a&gt;) used the story to take jabs at disloyal millennials:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;Millennials move from job to job in order to climb the ladder…. For baby boomers and other generations…loyalty and dedication to a single company or career drove, and still drives, much of their working lives.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Any of these stories could have been used to explore the pressing social and economic realities of being poor in the United States, and having to work for things considered fundamental rights in other countries. But instead they are presented as uplifting features, something only possible if we unquestionably accept the political and economic system.&lt;/p&gt;
&lt;h3&gt;Kids Do the Darndest Things&lt;/h3&gt;
&lt;p&gt;Many of what &lt;strong&gt;Think Progress&lt;/strong&gt; (&lt;a href=&quot;https://thinkprogress.org/do-the-impossible-never-complain-live-the-dream-the-dark-morals-of-todays-feel-good-stories-63b38b4a4953/&quot;&gt;8/2/18&lt;/a&gt;) labels “feel-good feel-bad stories” involve children doing things they wouldn’t have to in any reasonable society. &lt;strong&gt;CBS&lt;/strong&gt; invites us to enjoy an account of a boy selling his Xbox computer to help his (single) mom (&lt;a href=&quot;https://www.cbsnews.com/news/13-year-old-boy-sells-xbox-does-yard-work-to-buy-his-single-mom-a-car/&quot;&gt;4/2/19&lt;/a&gt;), and another repairing his town’s ravaged roads himself (&lt;a href=&quot;https://www.cbsnews.com/news/why-a-12-year-old-boy-is-on-a-mission-to-solve-his-towns-pothole-problem/&quot;&gt;4/12/19&lt;/a&gt;). &lt;strong&gt;The&lt;/strong&gt; &lt;strong&gt;Hill&lt;/strong&gt; (&lt;a href=&quot;https://thehill.com/blogs/blog-briefing-room/news/447728-9-year-old-boy-pays-off-entire-third-grade-classs-school-lunch&quot;&gt;6/10/19&lt;/a&gt;), meanwhile, describes a nine-year-old saving his pocket money to pay off his school friends’ “lunch debts.”&lt;/p&gt;
&lt;div id=&quot;attachment_9007962&quot; class=&quot;wp-caption alignright&quot; readability=&quot;34.463414634146&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-9007962&quot; class=&quot; wp-image-9007962&quot; src=&quot;https://fair.org/wp-content/uploads/2019/07/CNN-Homeless-Teen.png&quot; alt=&quot;CNN: A teen who was homeless scores more than $3 million in college scholarships&quot; width=&quot;350&quot; height=&quot;364&quot; srcset=&quot;https://fair.org/wp-content/uploads/2019/07/CNN-Homeless-Teen.png 557w, https://fair.org/wp-content/uploads/2019/07/CNN-Homeless-Teen-288x300.png 288w&quot; sizes=&quot;(max-width: 350px) 100vw, 350px&quot;/&gt;&lt;p id=&quot;caption-attachment-9007962&quot; class=&quot;wp-caption-text&quot;&gt;&lt;em&gt;“Hardships were never an excuse for Moseley,” &lt;strong&gt;CNN&lt;/strong&gt; (&lt;a href=&quot;https://edition.cnn.com/2019/05/22/us/homeless-valedictorian-scholarship-millions-trnd/index.html&quot;&gt;5/22/19&lt;/a&gt;) reports—as they are, implicitly, for homeless teens who aren’t offered millions in scholarships.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;NBC&lt;/strong&gt; (&lt;a href=&quot;https://www.nbcnews.com/news/nbcblk/tennessee-teen-graduates-high-school-valedictorian-earns-3m-scholarships-all-n1008976&quot;&gt;5/22/19&lt;/a&gt;) likewise shared the story of homeless Tennessee teen Tupac Moseley graduating high school as a valedictorian and earning many college scholarships, something that was widely reported (&lt;strong&gt;BBC&lt;/strong&gt;, &lt;a href=&quot;https://www.bbc.co.uk/news/48370530&quot;&gt;5/22/19&lt;/a&gt;; &lt;strong&gt;Newsweek&lt;/strong&gt;, &lt;a href=&quot;https://www.newsweek.com/tupac-mosley-raleigh-egypt-high-school-homeless-student-college-scholarship-1431303&quot;&gt;5/21/19&lt;/a&gt;; &lt;strong&gt;Business Insider&lt;/strong&gt;, &lt;a href=&quot;https://www.businessinsider.com/how-a-homeless-student-became-his-high-school-valedictorian-2019-5?r=US&amp;amp;IR=T&quot;&gt;5/21/19&lt;/a&gt;). &lt;strong&gt;NBC&lt;/strong&gt; matter-of-factly noted that after his father died, Moseley’s family’s home was foreclosed and they were on the streets, accepting this situation without comment. This was still among the most critical of the reports, however, as many did not even describe why a child in the richest society in history became homeless. &lt;strong&gt;CNN&lt;/strong&gt;’s report (&lt;a href=&quot;https://edition.cnn.com/2019/05/22/us/homeless-valedictorian-scholarship-millions-trnd/index.html&quot;&gt;5/22/19&lt;/a&gt;), for example, did not explain the background circumstances, let alone comment on them, and frames the story with the sentence, “Hardships were never an excuse for Moseley.”&lt;/p&gt;
&lt;p&gt;This sentence is telling: To corporate media, even the trauma of losing a parent and being forced onto the streets is merely an excuse, not a cause for poor grades. The implication is that poor housing, a lack of an adequate safety net, underfunded schools and a decimated public education system are simply excuses from bellyaching lazy people as to why they did not attend the private Boston University (at over &lt;a href=&quot;https://www.bu.edu/admissions/tuition-aid/tuition/&quot;&gt;$54,000 per year&lt;/a&gt; tuition), like the &lt;a href=&quot;https://www.linkedin.com/in/gabrielkinder/&quot;&gt;article’s author&lt;/a&gt; did.&lt;/p&gt;
&lt;p&gt;“No excuses” is a common phrase in “perseverance porn” stories. For example, &lt;strong&gt;Today&lt;/strong&gt; (&lt;a href=&quot;https://www.today.com/news/texas-man-walks-15-miles-work-every-day-no-excuses-t108388&quot;&gt;2/20/17&lt;/a&gt;) used it in the headline of a story about a Texas man who is forced to walk 15 miles to work every day. It reveals the ultimate bootstrap ideology of the media, where societal factors are irrelevant and everyone is where they are on merit.&lt;/p&gt;
&lt;p&gt;Thus Moseley’s story is effectively weaponized by &lt;strong&gt;CNN&lt;/strong&gt; against anyone who would question the system. Terrible work conditions? No excuses! Homeless? Stop complaining!&lt;/p&gt;
&lt;p&gt;In case you thought homeless children were something of an aberration in America, &lt;strong&gt;CNN&lt;/strong&gt; (&lt;a href=&quot;https://edition.cnn.com/2019/07/02/us/100-nyc-homeless-students-graduate-trnd/index.html?utm_source=twCNN&amp;amp;utm_medium=social&amp;amp;utm_content=2019-07-02T18%3A40%3A03&amp;amp;utm_term=link&quot;&gt;7/2/19&lt;/a&gt;) also recently ran a story about how over 100 homeless children graduated high school in New York City this year alone—again without comment on what this says about US society.&lt;/p&gt;
&lt;p&gt;Another reprehensible story treated as heroic by media was that of a Michigan mother who had to quit her job to look after her terminally ill son, who died of leukemia. She could not afford a headstone, so his best friend, 12-year-old Kaleb Klakulak, worked many jobs to attempt to pay for one. Many media outlets (e.g., &lt;strong&gt;Associated Press&lt;/strong&gt;, &lt;a href=&quot;https://www.apnews.com/a9c014228c234100803646ec8d2da98c&quot;&gt;12/8/18&lt;/a&gt;; &lt;strong&gt;Fox News&lt;/strong&gt;, &lt;a href=&quot;https://www.foxnews.com/health/michigan-boy-12-raising-money-for-gravestone-for-bff-who-died-from-cancer&quot;&gt;12/9/18&lt;/a&gt;; &lt;strong&gt;NBC Chicago&lt;/strong&gt;, &lt;a href=&quot;https://www.nbcchicago.com/news/local/michigan-boy-raising-money-for-friends-gravestone-gets-wish-502594311.html&quot;&gt;12/12/18&lt;/a&gt;) celebrated Kaleb’s spirit, but none asked why children are  performing hard, outdoor labor through a Michigan winter so other children can have adequate burials. Such reporting implicitly normalizes this situation, and the system that allows it to happen.&lt;/p&gt;
&lt;h3&gt;“Sweet” Stories&lt;/h3&gt;
&lt;div id=&quot;attachment_9007963&quot; class=&quot;wp-caption alignright&quot; readability=&quot;32.670391061453&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-9007963&quot; class=&quot; wp-image-9007963&quot; src=&quot;https://fair.org/wp-content/uploads/2019/07/CBS-Lemonade.png&quot; alt=&quot;CBS: Young boy takes a stand to help his baby brother and inspires a community&quot; width=&quot;350&quot; height=&quot;325&quot; srcset=&quot;https://fair.org/wp-content/uploads/2019/07/CBS-Lemonade.png 656w, https://fair.org/wp-content/uploads/2019/07/CBS-Lemonade-300x279.png 300w, https://fair.org/wp-content/uploads/2019/07/CBS-Lemonade-640x595.png 640w&quot; sizes=&quot;(max-width: 350px) 100vw, 350px&quot;/&gt;&lt;p id=&quot;caption-attachment-9007963&quot; class=&quot;wp-caption-text&quot;&gt;&lt;em&gt;Such stories (&lt;strong&gt;CBS&lt;/strong&gt;, &lt;a href=&quot;https://www.cbsnews.com/news/teamdylan-andrew-emery-dylan-emery-lemonade-stand-greenwood-south-carolina/&quot;&gt;5/29/18&lt;/a&gt;) rarely if ever ask why a baby with a life-threatening illness is forced to rely on his nine-year-old brother’s selling lemonade to pay for treatment.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A common media trope is presenting kids selling lemonade as cute,  sweet stories, no matter how horrifying or depressing the reason, including to pay off school lunch debts (&lt;strong&gt;Yahoo! News&lt;/strong&gt;, &lt;a href=&quot;https://news.yahoo.com/students-sell-lemonade-pay-off-school-lunch-debt-213417881.html?guccounter=1&amp;amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;amp;guce_referrer_sig=AQAAACsD31szcWm8EZkCCnEafCweiqcMbIunQnkYTqRr-hafy1Jw9anfNE5NiRRlQL2aqHrUzm8QH9SKGeGBL141McZEwRUsIe4TtjzD8gOb1KAlp4WT8wI3WNjaeMGK0IG94TcSr1TZ8RsxTbStE3UxgrqeGVzpz55p7inNcEH2S90g&quot;&gt;5/21/19&lt;/a&gt;; &lt;strong&gt;MSN&lt;/strong&gt;, &lt;a href=&quot;https://www.msn.com/en-us/money/news/students-sell-lemonade-to-help-pay-off-school-district-lunch-debt/vi-AABKqbe&quot;&gt;5/22/19&lt;/a&gt;), or to raise money for their baby brother’s medical treatment (&lt;strong&gt;New York Post&lt;/strong&gt;, &lt;a href=&quot;https://nypost.com/2018/05/28/9-year-old-sells-6k-worth-of-lemonade-to-help-sick-brother/&quot;&gt;5/28/18&lt;/a&gt;; &lt;strong&gt;CBS&lt;/strong&gt;, &lt;a href=&quot;https://www.cbsnews.com/news/teamdylan-andrew-emery-dylan-emery-lemonade-stand-greenwood-south-carolina/&quot;&gt;5/29/18&lt;/a&gt;) or their mother’s chemotherapy (&lt;strong&gt;KTSM El Paso&lt;/strong&gt;, &lt;a href=&quot;https://www.ktsm.com/news/local/el-paso-news/6-year-old-selling-lemonade-to-help-with-mom-s-chemotherapy/1347259442&quot;&gt;8/4/18&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Or how about the story of a New Mexico girl selling lemonade trying to fund her mother’s kidney transplant? &lt;strong&gt;People&lt;/strong&gt; magazine (&lt;a href=&quot;https://people.com/human-interest/nemiah-martinez-lemonade-stand-mom-kidney-transplant-money/&quot;&gt;5/9/18&lt;/a&gt;) applauded her resolve, and local radio &lt;a href=&quot;https://sunny981sd.radio.com/blogs/aj-sara-morning/feel-good-50s-11-year-old-girl-sells-lemonade-moms-transplant&quot;&gt;described it&lt;/a&gt; as “heartwarming” that she had raised over $1,000. The massive problem is a kidney transplant in America can cost &lt;a href=&quot;https://fortune.com/2017/09/14/organ-transplant-cost/&quot;&gt;over $400,000&lt;/a&gt;. To anyone with a heart, what this story actually represents is the desperate struggle of a child trying in vain to save her dying mother. Worse still is the fact that if she lived in Sweden, Spain or Saskatchewan, she would be given a kidney free of charge and without question.&lt;/p&gt;
&lt;p&gt;Any of the numerous other outlets (&lt;strong&gt;ABC&lt;/strong&gt;, &lt;a href=&quot;https://twitter.com/abc/status/991096943358238721?lang=en&quot;&gt;4/30/18&lt;/a&gt;; &lt;strong&gt;Good Morning America&lt;/strong&gt;, &lt;a href=&quot;https://www.goodmorningamerica.com/news/video/girl-starts-lemonade-stand-ailing-mom-54851355&quot;&gt;5/1/18&lt;/a&gt;; &lt;strong&gt;Albuquerque Journal&lt;/strong&gt;, &lt;a href=&quot;https://www.abqjournal.com/1165116/new-mexico-girl-11-selling-lemonade-for-moms-transplants.html&quot;&gt;4/30/18&lt;/a&gt;) that picked it up could have used the story to discuss the dysfunctional healthcare system that is the &lt;a href=&quot;https://www.cnbc.com/id/100840148&quot;&gt;leading cause of bankruptcy&lt;/a&gt; in the country, while producing some of the &lt;a href=&quot;https://www.healthsystemtracker.org/chart-collection/quality-u-s-healthcare-system-compare-countries/&quot;&gt;worst health outcomes&lt;/a&gt; in the developed world, or to scrutinize how corporate healthcare gouges the sickest and most vulnerable Americans, including children. Surely the most basic function of government should be to prevent its citizens from needlessly dying? Not if you wholly accept the tenets of neoliberalism, where education, housing and healthcare are not basic, inalienable human rights, but commodities to be bought and sold and bargained for on the market.&lt;/p&gt;
&lt;p&gt;To be clear, while we can admire the never-say-die attitude of those in tough conditions, this is no substitute for guaranteed public programs to help those in dire need. The problem with perseverance porn is not the brave subjects of the articles, but the lack of any journalistic scrutiny examining the failings of society that placed them in such desperate circumstances to begin with.&lt;/p&gt;
&lt;p&gt;What these articles highlight so clearly is not only the grim, inhuman and unnecessary conditions so many Americans are forced to live under, but the degree to which mainstream corporate journalists have completely internalized them as unremarkable, inevitable facts of life, rather than the consequences of decades of neoliberal policies that have robbed Americans of dignity and basic human rights. Because corporate media wholly accept and promote neoliberal, free-market doctrine, they are unable to see how what they see as “awesome” is actually a manifestation of late-capitalist dystopia.&lt;/p&gt;




&lt;div class=&quot;rp4wp-related-posts&quot;&gt;
&lt;h3&gt;Related Posts&lt;/h3&gt;
&lt;/div&gt;
</description>
<pubDate>Sun, 18 Aug 2019 14:31:54 +0000</pubDate>
<dc:creator>joeyespo</dc:creator>
<og:type>article</og:type>
<og:title>Media Just Can’t Stop Presenting Horrifying Stories as ‘Uplifting’ Perseverance Porn</og:title>
<og:description>Supposedly charming and positive &quot;uplifting&quot; news actually reveals the dire conditions of late capitalism so many Americans now live under.</og:description>
<og:url>https://fair.org/home/media-just-cant-stop-presenting-horrifying-stories-as-uplifting-perseverance-porn/</og:url>
<og:image>https://fair.org/wp-content/uploads/2019/07/Horrifying-Uplift.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://fair.org/home/media-just-cant-stop-presenting-horrifying-stories-as-uplifting-perseverance-porn/</dc:identifier>
</item>
<item>
<title>35% Faster Than The Filesystem (2017)</title>
<link>https://www.sqlite.org/fasterthanfs.html</link>
<guid isPermaLink="true" >https://www.sqlite.org/fasterthanfs.html</guid>
<description>&lt;div class=&quot;nosearch&quot; readability=&quot;0.96632996632997&quot;&gt;
&lt;p&gt;35% Faster Than The Filesystem&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;SQLite reads and writes small blobs (for example, thumbnail images) &lt;a href=&quot;https://www.sqlite.org/fasterthanfs.html#approx&quot;&gt;35% faster¹&lt;/a&gt; than the same blobs can be read from or written to individual files on disk using fread() or fwrite().&lt;/p&gt;
&lt;p&gt;Furthermore, a single SQLite database holding 10-kilobyte blobs uses about 20% less disk space than storing the blobs in individual files.&lt;/p&gt;
&lt;p&gt;The performance difference arises (we believe) because when working from an SQLite database, the open() and close() system calls are invoked only once, whereas open() and close() are invoked once for each blob when using blobs stored in individual files. It appears that the overhead of calling open() and close() is greater than the overhead of using the database. The size reduction arises from the fact that individual files are padded out to the next multiple of the filesystem block size, whereas the blobs are packed more tightly into an SQLite database.&lt;/p&gt;
&lt;p&gt;The measurements in this article were made during the week of 2017-06-05 using a version of SQLite in between 3.19.2 and 3.20.0. You may expect future versions of SQLite to perform even better.&lt;/p&gt;
&lt;h2 id=&quot;caveats&quot;&gt;&lt;span&gt;1.1.&lt;/span&gt; Caveats&lt;/h2&gt;

&lt;p&gt;¹The 35% figure above is approximate. Actual timings vary depending on hardware, operating system, and the details of the experiment, and due to random performance fluctuations on real-world hardware. See the text below for more detail. Try the experiments yourself. Report significant deviations to the &lt;a href=&quot;https://www.sqlite.org/support.html#mailinglists&quot;&gt;mailing lists&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The 35% figure is based on running tests on every machine that the author has easily at hand. Some reviewers of this article report that SQLite has higher latency than direct I/O on their systems. We do not yet understand the difference. We also see indications that SQLite does not perform as well as direct I/O when experiments are run using a cold filesystem cache.&lt;/p&gt;
&lt;p&gt;So let your take-away be this: read/write latency for SQLite is competitive with read/write latency of individual files on disk. Often SQLite is faster. Sometimes SQLite is almost as fast. Either way, this article disproves the common assumption that a relational database must be slower than direct filesystem I/O.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/people/gray/&quot;&gt;Jim Gray&lt;/a&gt; and others studied the read performance of BLOBs versus file I/O for Microsoft SQL Server and found that reading BLOBs out of the database was faster for BLOB sizes less than between 250KiB and 1MiB. (&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/to-blob-or-not-to-blob-large-object-storage-in-a-database-or-a-filesystem/&quot;&gt;Paper&lt;/a&gt;). In that study, the database still stores the filename of the content even if the content is held in a separate file. So the database is consulted for every BLOB, even if it is only to extract the filename. In this article, the key for the BLOB is the filename, so no preliminary database access is required. Because the database is never used at all when reading content from individual files in this article, the threshold at which direct file I/O becomes faster is smaller than it is in Gray's paper.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.sqlite.org/intern-v-extern-blob.html&quot;&gt;Internal Versus External BLOBs&lt;/a&gt; article on this website is an earlier investigation (circa 2011) that uses the same approach as the Jim Gray paper — storing the blob filenames as entries in the database — but for SQLite instead of SQL Server.&lt;/p&gt;

&lt;p&gt;I/O performance is measured using the &lt;a href=&quot;https://www.sqlite.org/src/file/test/kvtest.c&quot;&gt;kvtest.c&lt;/a&gt; program from the SQLite source tree. To compile this test program, first gather the kvtest.c source file into a directory with the &lt;a href=&quot;https://www.sqlite.org/amalgamation.html&quot;&gt;SQLite amalgamation&lt;/a&gt; source files &quot;sqlite3.c&quot; and &quot;sqlite3.h&quot;. Then on unix, run a command like the following:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
gcc -Os -I. -DSQLITE_DIRECT_OVERFLOW_READ \
  kvtest.c sqlite3.c -o kvtest -ldl -lpthread
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or on Windows with MSVC:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
cl -I. -DSQLITE_DIRECT_OVERFLOW_READ kvtest.c sqlite3.c
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Instructions for compiling for Android are &lt;a href=&quot;https://www.sqlite.org/fasterthanfs.html#compile-android&quot;&gt;shown below&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use the resulting &quot;kvtest&quot; program to generate a test database with 100,000 random uncompressible blobs, each with a random size between 8,000 and 12,000 bytes using a command like this:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
./kvtest init test1.db --count 100k --size 10k --variance 2k
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If desired, you can verify the new database by running this command:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot;&gt;
&lt;pre&gt;
./kvtest stat test1.db
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, make copies of all the blobs into individual files in a directory using a command like this:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
./kvtest export test1.db test1.dir
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, you can measure the amount of disk space used by the test1.db database and the space used by the test1.dir directory and all of its content. On a standard Ubuntu Linux desktop, the database file will be 1,024,512,000 bytes in size and the test1.dir directory will use 1,228,800,000 bytes of space (according to &quot;du -k&quot;), about 20% more than the database.&lt;/p&gt;
&lt;p&gt;The &quot;test1.dir&quot; directory created above puts all the blobs into a single folder. It was conjectured that some operating systems would perform poorly when a single directory contains 100,000 objects. To test this, the kvtest program can also store the blobs in a hierarchy of folders with no more than 100 files and/or subdirectories per folder. The alternative on-disk representation of the blobs can be created using the --tree command-line option to the &quot;export&quot; command, like this:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
./kvtest export test1.db test1.tree --tree
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The test1.dir directory will contain 100,000 files with names like &quot;000000&quot;, &quot;000001&quot;, &quot;000002&quot; and so forth but the test1.tree directory will contain the same files in subdirectories like &quot;00/00/00&quot;, &quot;00/00/01&quot;, and so on. The test1.dir and test1.test directories take up approximately the same amount of space, though test1.test is very slightly larger due to the extra directory entries.&lt;/p&gt;
&lt;p&gt;All of the experiments that follow operate the same with either &quot;test1.dir&quot; or &quot;test1.tree&quot;. Very little performance difference is measured in either case, regardless of operating system.&lt;/p&gt;
&lt;p&gt;Measure the performance for reading blobs from the database and from individual files using these commands:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;8&quot;&gt;
&lt;pre&gt;
./kvtest run test1.db --count 100k --blob-api
./kvtest run test1.dir --count 100k --blob-api
./kvtest run test1.tree --count 100k --blob-api
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Depending on your hardware and operating system, you should see that reads from the test1.db database file are about 35% faster than reads from individual files in the test1.dir or test1.tree folders. Results can vary significantly from one run to the next due to caching, so it is advisable to run tests multiple times and take an average or a worst case or a best case, depending on your requirements.&lt;/p&gt;
&lt;p&gt;The --blob-api option on the database read test causes kvtest to use the &lt;a href=&quot;https://www.sqlite.org/c3ref/blob_read.html&quot;&gt;sqlite3_blob_read()&lt;/a&gt; feature of SQLite to load the content of the blobs, rather than running pure SQL statements. This helps SQLite to run a little faster on read tests. You can omit that option to compare the performance of SQLite running SQL statements. In that case, the SQLite still out-performs direct reads, though by not as much as when using &lt;a href=&quot;https://www.sqlite.org/c3ref/blob_read.html&quot;&gt;sqlite3_blob_read()&lt;/a&gt;. The --blob-api option is ignored for tests that read from individual disk files.&lt;/p&gt;
&lt;p&gt;Measure write performance by adding the --update option. This causes the blobs are overwritten in place with another random blob of exactly the same size.&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;8&quot;&gt;
&lt;pre&gt;
./kvtest run test1.db --count 100k --update
./kvtest run test1.dir --count 100k --update
./kvtest run test1.tree --count 100k --update
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The writing test above is not completely fair, since SQLite is doing &lt;a href=&quot;https://www.sqlite.org/transactional.html&quot;&gt;power-safe transactions&lt;/a&gt; whereas the direct-to-disk writing is not. To put the tests on a more equal footing, add either the --nosync option to the SQLite writes to disable calling fsync() or FlushFileBuffers() to force content to disk, or using the --fsync option for the direct-to-disk tests to force them to invoke fsync() or FlushFileBuffers() when updating disk files.&lt;/p&gt;
&lt;p&gt;By default, kvtest runs the database I/O measurements all within a single transaction. Use the --multitrans option to run each blob read or write in a separate transaction. The --multitrans option makes SQLite much slower, and uncompetitive with direct disk I/O. This option proves, yet again, that to get the most performance out of SQLite, you should group as much database interaction as possible within a single transaction.&lt;/p&gt;
&lt;p&gt;There are many other testing options, which can be seen by running the command:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot;&gt;
&lt;pre&gt;
./kvtest help
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;read_performance_measurements&quot;&gt;&lt;span&gt;2.1.&lt;/span&gt; Read Performance Measurements&lt;/h2&gt;
&lt;p&gt;The chart below shows data collected using &lt;a href=&quot;https://www.sqlite.org/src/file/test/kvtest.c&quot;&gt;kvtest.c&lt;/a&gt; on five different systems:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Win7&lt;/strong&gt;: A circa-2009 Dell Inspiron laptop, Pentium dual-core at 2.30GHz, 4GiB RAM, Windows7.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Win10&lt;/strong&gt;: A 2016 Lenovo YOGA 910, Intel i7-7500 at 2.70GHz, 16GiB RAM, Windows10.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mac&lt;/strong&gt;: A 2015 MacBook Pro, 3.1GHz intel Core i7, 16GiB RAM, MacOS 10.12.5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ubuntu&lt;/strong&gt;: Desktop built from Intel i7-4770K at 3.50GHz, 32GiB RAM, Ubuntu 16.04.2 LTS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: Galaxy S3, ARMv7, 2GiB RAM&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;All machines use SSD except Win7 which has a hard-drive. The test database is 100K blobs with sizes uniformly distributed between 8K and 12K, for a total of about 1 gigabyte of content. The database page size is 4KiB. The -DSQLITE_DIRECT_OVERFLOW_READ compile-time option was used for all of these tests. Tests were run multiple times. The first run was used to warm up the cache and its timings were discarded.&lt;/p&gt;
&lt;p&gt;The chart below shows average time to read a blob directly from the filesystem versus the time needed to read the same blob from the SQLite database. The actual timings vary considerably from one system to another (the Ubuntu desktop is much faster than the Galaxy S3 phone, for example). This chart shows the ratio of the times needed to read blobs from a file divided by the time needed to from the database. The left-most column in the chart is the normalized time to read from the database, for reference.&lt;/p&gt;
&lt;p&gt;In this chart, an SQL statement (&quot;SELECT v FROM kv WHERE k=?1&quot;) is prepared once. Then for each blob, the blob key value is bound to the ?1 parameter and the statement is evaluated to extract the blob content.&lt;/p&gt;
&lt;p&gt;The chart shows that on Windows10, content can be read from the SQLite database about 5 times faster than it can be read directly from disk. On Android, SQLite is only about 35% faster than reading from disk.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&quot;'imgcontainer'&quot;&gt;&lt;img src=&quot;https://www.sqlite.org/images/faster-read-sql.jpg&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;Chart 1: SQLite read latency relative to direct filesystem reads.&lt;br /&gt;100K blobs, avg 10KB each, random order using SQL&lt;/center&gt;
&lt;p&gt;The performance can be improved slightly by bypassing the SQL layer and reading the blob content directly using the &lt;a href=&quot;https://www.sqlite.org/c3ref/blob_read.html&quot;&gt;sqlite3_blob_read()&lt;/a&gt; interface, as shown in the next chart:&lt;/p&gt;
&lt;center&gt;
&lt;div class=&quot;'imgcontainer'&quot;&gt;&lt;img src=&quot;https://www.sqlite.org/images/faster-read-blobapi.jpg&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;Chart 2: SQLite read latency relative to direct filesystem reads.&lt;br /&gt;100K blobs, avg size 10KB, random order&lt;br /&gt;using sqlite3_blob_read().&lt;/center&gt;
&lt;p&gt;Further performance improves can be made by using the &lt;a href=&quot;https://www.sqlite.org/mmap.html&quot;&gt;memory-mapped I/O&lt;/a&gt; feature of SQLite. In the next chart, the entire 1GB database file is memory mapped and blobs are read (in random order) using the &lt;a href=&quot;https://www.sqlite.org/c3ref/blob_read.html&quot;&gt;sqlite3_blob_read()&lt;/a&gt; interface. With these optimizations, SQLite is twice as fast as Android or MacOS-X and over 10 times faster than Windows.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&quot;'imgcontainer'&quot;&gt;&lt;img src=&quot;https://www.sqlite.org/images/faster-read-mmap.jpg&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;Chart 3: SQLite read latency relative to direct filesystem reads.&lt;br /&gt;100K blobs, avg size 10KB, random order&lt;br /&gt;using sqlite3_blob_read() from a memory-mapped database.&lt;/center&gt;
&lt;p&gt;The third chart shows that reading blob content out of SQLite can be twice as fast as reading from individual files on disk for Mac and Android, and an amazing ten times faster for Windows.&lt;/p&gt;
&lt;h2 id=&quot;write_performance_measurements&quot;&gt;&lt;span&gt;2.2.&lt;/span&gt; Write Performance Measurements&lt;/h2&gt;
&lt;p&gt;Writes are slower. On all systems, using both direct I/O and SQLite, write performance is between 5 and 15 times slower than reads.&lt;/p&gt;
&lt;p&gt;Write performance measurements were made by replacing (overwriting) an entire blob with a different blob. All of the blobs in these experiment are random and incompressible. Because writes are so much slower than reads, only 10,000 of the 100,000 blobs in the database are replaced. The blobs to be replaced are selected at random and are in no particular order.&lt;/p&gt;
&lt;p&gt;The direct-to-disk writes are accomplished using fopen()/fwrite()/fclose(). By default, and in all the results shown below, the OS filesystem buffers are never flushed to persistent storage using fsync() or FlushFileBuffers(). In other words, there is no attempt to make the direct-to-disk writes transactional or power-safe. We found that invoking fsync() or FlushFileBuffers() on each file written causes direct-to-disk storage to be about 10 times or more slower than writes to SQLite.&lt;/p&gt;
&lt;p&gt;The next chart compares SQLite database updates in &lt;a href=&quot;https://www.sqlite.org/wal.html&quot;&gt;WAL mode&lt;/a&gt; against raw direct-to-disk overwrites of separate files on disk. The &lt;a href=&quot;https://www.sqlite.org/pragma.html#pragma_synchronous&quot;&gt;PRAGMA synchronous&lt;/a&gt; setting is NORMAL. All database writes are in a single transaction. The timer for the database writes is stopped after the transaction commits, but before a &lt;a href=&quot;https://www.sqlite.org/wal.html#ckpt&quot;&gt;checkpoint&lt;/a&gt; is run. Note that the SQLite writes, unlike the direct-to-disk writes, are &lt;a href=&quot;https://www.sqlite.org/transactional.html&quot;&gt;transactional&lt;/a&gt; and &lt;a href=&quot;https://www.sqlite.org/transactional.html&quot;&gt;power-safe&lt;/a&gt;, though because the synchronous setting is NORMAL instead of FULL, the transactions are not durable.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&quot;'imgcontainer'&quot;&gt;&lt;img src=&quot;https://www.sqlite.org/images/faster-write-safe.jpg&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;Chart 4: SQLite write latency relative to direct filesystem writes.&lt;br /&gt;10K blobs, avg size 10KB, random order,&lt;br /&gt;WAL mode with synchronous NORMAL,&lt;br /&gt;exclusive of checkpoint time&lt;/center&gt;
&lt;p&gt;The android performance numbers for the write experiments are omitted because the performance tests on the Galaxy S3 are so random. Two consecutive runs of the exact same experiment would give wildly different times. And, to be fair, the performance of SQLite on android is slightly slower than writing directly to disk.&lt;/p&gt;
&lt;p&gt;The next chart shows the performance of SQLite versus direct-to-disk when transactions are disabled (&lt;a href=&quot;https://www.sqlite.org/pragma.html#pragma_journal_mode&quot;&gt;PRAGMA journal_mode=OFF&lt;/a&gt;) and &lt;a href=&quot;https://www.sqlite.org/pragma.html#pragma_synchronous&quot;&gt;PRAGMA synchronous&lt;/a&gt; is set to OFF. These settings put SQLite on an equal footing with direct-to-disk writes, which is to say they make the data prone to corruption due to system crashes and power failures.&lt;/p&gt;
&lt;center&gt;
&lt;div class=&quot;'imgcontainer'&quot;&gt;&lt;img src=&quot;https://www.sqlite.org/images/faster-write-unsafe.jpg&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;Chart 5: SQLite write latency relative to direct filesystem writes.&lt;br /&gt;10K blobs, avg size 10KB, random order,&lt;br /&gt;journaling disabled, synchronous OFF.&lt;/center&gt;
&lt;p&gt;In all of the write tests, it is important to disable anti-virus software prior to running the direct-to-disk performance tests. We found that anti-virus software slows down direct-to-disk by an order of magnitude whereas it impacts SQLite writes very little. This is probably due to the fact that direct-to-disk changes thousands of separate files which all need to be checked by anti-virus, whereas SQLite writes only changes the single database file.&lt;/p&gt;
&lt;h2 id=&quot;variations&quot;&gt;&lt;span&gt;2.3.&lt;/span&gt; Variations&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.sqlite.org/compile.html#direct_overflow_read&quot;&gt;-DSQLITE_DIRECT_OVERFLOW_READ&lt;/a&gt; compile-time option causes SQLite to bypass its page cache when reading content from overflow pages. This helps database reads of 10K blobs run a little faster, but not all that much faster. SQLite still holds a speed advantage over direct filesystem reads without the SQLITE_DIRECT_OVERFLOW_READ compile-time option.&lt;/p&gt;
&lt;p&gt;Other compile-time options such as using -O3 instead of -Os or using &lt;a href=&quot;https://www.sqlite.org/compile.html#threadsafe&quot;&gt;-DSQLITE_THREADSAFE=0&lt;/a&gt; and/or some of the other &lt;a href=&quot;https://www.sqlite.org/compile.html#rcmd&quot;&gt;recommended compile-time options&lt;/a&gt; might help SQLite to run even faster relative to direct filesystem reads.&lt;/p&gt;
&lt;p&gt;The size of the blobs in the test data affects performance. The filesystem will generally be faster for larger blobs, since the overhead of open() and close() is amortized over more bytes of I/O, whereas the database will be more efficient in both speed and space as the average blob size decreases.&lt;/p&gt;

&lt;ol type=&quot;A&quot; readability=&quot;9.2264875239923&quot;&gt;&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;SQLite is competitive with, and usually faster than, blobs stored in separate files on disk, for both reading and writing.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;SQLite is much faster than direct writes to disk on Windows when anti-virus protection is turned on. Since anti-virus software is and should be on by default in Windows, that means that SQLite is generally much faster than direct disk writes on Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Reading is about an order of magnitude faster than writing, for all systems and for both SQLite and direct-to-disk I/O.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;I/O performance varies widely depending on operating system and hardware. Make your own measurements before drawing conclusions.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3.7115384615385&quot;&gt;
&lt;p&gt;Some other SQL database engines advise developers to store blobs in separate files and then store the filename in the database. In that case, where the database must first be consulted to find the filename before opening and reading the file, simply storing the entire blob in the database gives much faster read and write performance with SQLite. See the &lt;a href=&quot;https://www.sqlite.org/intern-v-extern-blob.html&quot;&gt;Internal Versus External BLOBs&lt;/a&gt; article for more information.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;compiling_and_testing_on_android&quot;&gt;&lt;span&gt;4.1.&lt;/span&gt; Compiling And Testing on Android&lt;/h2&gt;
&lt;p&gt;The kvtest program is compiled and run on Android as follows. First install the Android SDK and NDK. Then prepare a script named &quot;android-gcc&quot; that looks approximately like this:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;9&quot;&gt;
&lt;pre&gt;
#!/bin/sh
#
NDK=/home/drh/Android/Sdk/ndk-bundle
SYSROOT=$NDK/platforms/android-16/arch-arm
ABIN=$NDK/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin
GCC=$ABIN/arm-linux-androideabi-gcc
$GCC --sysroot=$SYSROOT -fPIC -pie $*
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Make that script executable and put it on your $PATH. Then compile the kvtest program as follows:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
android-gcc -Os -I. kvtest.c sqlite3.c -o kvtest-android
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next, move the resulting kvtest-android executable to the Android device:&lt;/p&gt;
&lt;div class=&quot;codeblock&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
adb push kvtest-android /data/local/tmp
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally use &quot;adb shell&quot; to get a shell prompt on the Android device, cd into the /data/local/tmp directory, and begin running the tests as with any other unix host.&lt;/p&gt;
</description>
<pubDate>Sun, 18 Aug 2019 13:08:03 +0000</pubDate>
<dc:creator>Tomte</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.sqlite.org/fasterthanfs.html</dc:identifier>
</item>
<item>
<title>Ask HN: What book to read to get a footing in CS theory?</title>
<link>https://news.ycombinator.com/item?id=20729252</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=20729252</guid>
<description>&lt;tr readability=&quot;0.58823529411765&quot;&gt;&lt;td bgcolor=&quot;#FF6600&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr id=&quot;pagespace&quot; title=&quot;Ask HN: What book to read to get a footing in CS theory?&quot;&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5.8009259259259&quot;&gt;&lt;td&gt;
&lt;table class=&quot;fatitem&quot; border=&quot;0&quot; readability=&quot;4.5578703703704&quot;&gt;&lt;tr class=&quot;athing&quot; id=&quot;20729252&quot; readability=&quot;0&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;title&quot;/&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;title&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=20729252&quot; class=&quot;storylink&quot;&gt;Ask HN: What book to read to get a footing in CS theory?&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.74074074074074&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td class=&quot;subtext&quot;&gt;&lt;span class=&quot;score&quot; id=&quot;score_20729252&quot;&gt;228 points&lt;/span&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=bjackman&quot; class=&quot;hnuser&quot;&gt;bjackman&lt;/a&gt; &lt;span class=&quot;age&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=20729252&quot;&gt;15 hours ago&lt;/a&gt;&lt;/span&gt; &lt;span id=&quot;unv_20729252&quot;/&gt; | &lt;a href=&quot;https://news.ycombinator.com/hide?id=20729252&amp;amp;goto=item%3Fid%3D20729252&quot;&gt;hide&lt;/a&gt; | &lt;a href=&quot;https://hn.algolia.com/?query=Ask%20HN%3A%20What%20book%20to%20read%20to%20get%20a%20footing%20in%20CS%20theory%3F&amp;amp;sort=byDate&amp;amp;dateRange=all&amp;amp;type=story&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0&quot; class=&quot;hnpast&quot;&gt;past&lt;/a&gt; | &lt;a href=&quot;https://www.google.com/search?q=Ask%20HN%3A%20What%20book%20to%20read%20to%20get%20a%20footing%20in%20CS%20theory%3F&quot;&gt;web&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/fave?id=20729252&amp;amp;auth=c180b6b5905b30a71c70d8cdfdf9b970e91ea93d&quot;&gt;favorite&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/item?id=20729252&quot;&gt;62 comments&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;10.5&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td readability=&quot;10&quot;&gt;A friend of mine has just taught herself the basics of JavaScript then done a code boot camp. She's pretty comfortable writing code now, but hasn't had a chance to get to grips with stuff like complexity analysis yet.
&lt;p&gt;It seems to me like she's a member of a large and growing target audience for a book that gets you started with all the stuff you would learn in a computer science degree, with the assumption that you already know how to express an algorithm as code.&lt;/p&gt;
&lt;p&gt;Can anyone recommend such a book?&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;1&quot;&gt;&lt;td&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;10&quot; width=&quot;0&quot;/&gt;&lt;br/&gt;&lt;center&gt;&lt;span class=&quot;yclinks&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/newsguidelines.html&quot;&gt;Guidelines&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/newsfaq.html&quot;&gt;FAQ&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Support&lt;/a&gt; | &lt;a href=&quot;https://github.com/HackerNews/API&quot;&gt;API&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/security.html&quot;&gt;Security&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/lists&quot;&gt;Lists&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/bookmarklet.html&quot; rel=&quot;nofollow&quot;&gt;Bookmarklet&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/legal/&quot;&gt;Legal&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/apply/&quot;&gt;Apply to YC&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Contact&lt;/a&gt;&lt;/span&gt;
&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Sun, 18 Aug 2019 09:47:04 +0000</pubDate>
<dc:creator>bjackman</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=20729252</dc:identifier>
</item>
<item>
<title> Building ages in the Netherlands</title>
<link>https://parallel.co.uk/netherlands/#13.8/52.365/4.9/0/40</link>
<guid isPermaLink="true" >https://parallel.co.uk/netherlands/#13.8/52.365/4.9/0/40</guid>
<description>&lt;p&gt;&lt;strong&gt;Building ages in the Netherlands&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All 10 million or so buildings in the Netherlands. Building heights and date of construction from 3D BAG (Basisregistratie Adressen en Gebouwen) data.&lt;/p&gt;
&lt;p&gt;2020&lt;/p&gt;
&lt;p&gt;2015&lt;/p&gt;
&lt;p&gt;2000&lt;/p&gt;
&lt;p&gt;1975&lt;/p&gt;
&lt;p&gt;1950&lt;/p&gt;
&lt;p&gt;1925&lt;/p&gt;
&lt;p&gt;1900&lt;/p&gt;
&lt;p&gt;1850&lt;/p&gt;
&lt;p&gt;1800&lt;/p&gt;
&lt;p&gt;1750&lt;/p&gt;
&lt;p&gt;1700 or earlier&lt;/p&gt;
&lt;p&gt;Not known&lt;/p&gt;

&lt;p&gt;Data: 3D BAG by 3D Geoinformation Group, TU Delft, July 2019: &lt;a href=&quot;http://3dbag.bk.tudelft.nl/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;3dbag.bk.tudelft.nl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks to inspiration from: &lt;a href=&quot;http://code.waag.org/buildings/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code.waag.org/buildings&lt;/a&gt; and &lt;a href=&quot;https://nieneb.nl/online-projects/gebouwen/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;nieneb.nl/online-projects/gebouwen&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 18 Aug 2019 07:59:14 +0000</pubDate>
<dc:creator>Schiphol</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://parallel.co.uk/netherlands/</dc:identifier>
</item>
<item>
<title>In the US, it&amp;#039;s cheaper to build and operate wind farms than buy fossil fuels</title>
<link>https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/</link>
<guid isPermaLink="true" >https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/</guid>
<description>&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/wind-power-reliability-research-hero-800x445.jpg&quot; alt=&quot;Image of wind turbines on a ridge&quot;/&gt;&lt;aside id=&quot;social-left&quot; aria-label=&quot;Read the comments or share this article&quot;&gt;
&lt;h4 class=&quot;comment-count-before&quot;&gt;&lt;a title=&quot;114 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/?comments=1&quot;&gt;reader comments&lt;/a&gt;&lt;/h4&gt;
&lt;a title=&quot;114 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/?comments=1&quot;&gt;&lt;span class=&quot;comment-count-number&quot;&gt;352&lt;/span&gt; &lt;span class=&quot;visually-hidden&quot;&gt;with 114 posters participating&lt;/span&gt;&lt;/a&gt;
&lt;div class=&quot;share-links&quot;&gt;
&lt;h4&gt;Share this story&lt;/h4&gt;
&lt;/div&gt;
&lt;/aside&gt;&lt;p&gt;This week, the US Department of Energy &lt;a href=&quot;https://emp.lbl.gov/sites/default/files/wtmr_final_for_posting_8-9-19.pdf&quot;&gt;released a report&lt;/a&gt; that looks back on the state of wind power in the US by running the numbers on 2018. The analysis shows that wind hardware prices are dropping, even as new turbine designs are increasing the typical power generated by each turbine. As a result, recent wind farms have gotten so cheap that you can build and operate them for less than the expected cost of buying fuel for an equivalent natural gas plant.&lt;/p&gt;
&lt;p&gt;Wind is even cheaper at the moment because of a tax credit given to renewable energy generation. But that credit is in the process of fading out, leading to long term uncertainty in a power market where demand is generally stable or dropping.&lt;/p&gt;
&lt;h2&gt;A lot of GigaWatts&lt;/h2&gt;
&lt;p&gt;2018 saw about 7.6 GigaWatts of new wind capacity added to the grid, accounting for just over 20 percent of the US' capacity additions. This puts it in third place behind natural gas and solar power. That's less impressive than it might sound, however, given that things like coal and nuclear are essentially at a standstill. Because the best winds aren't evenly distributed in the US, there are areas, like parts of the Great Plains, where wind installations were more than half of the new power capacity installed.&lt;/p&gt;
&lt;p&gt;Overall, that brings the US' installed capacity up to nearly 100GW. That leaves only China ahead of the US, although the gap is substantial with China having more than double the US' installed capacity. It still leaves wind supplying only 6.5 percent of the US' total electricity in 2018, though, which places it behind a dozen other countries. Four of them—Denmark, Germany, Ireland, and Portugal—get over 20 percent of their total electric needs supplied by wind, with Denmark at over 40 percent.&lt;/p&gt;
&lt;p&gt;That figure is notable, as having over 30 percent of your power supplied by an intermittent source is a challenge for many existing grids. But there are a number of states that have now cleared the 30 percent threshold: Kansas, Iowa, and Oklahoma, with the two Dakotas not far behind. The Southwest Power Pool, which serves two of those states plus wind giant Texas, is currently getting a quarter of its electricity from wind. (Texas leads the US with 25GW of installed wind capacity.)&lt;/p&gt;
&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.45.56-PM.png&quot; class=&quot;enlarge&quot; data-height=&quot;652&quot; data-width=&quot;1096&quot; alt=&quot;Despite having a lot of wind installed, the US uses far more power from other sources.&quot;&gt;&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.45.56-PM-640x381.png&quot; width=&quot;640&quot; height=&quot;381&quot; srcset=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.45.56-PM.png 2x&quot; alt=&quot;Despite having a lot of wind installed, the US uses far more power from other sources.&quot;/&gt;&lt;/a&gt;
&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.45.56-PM.png&quot; class=&quot;enlarge-link&quot; data-height=&quot;652&quot; data-width=&quot;1096&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; Despite having a lot of wind installed, the US uses far more power from other sources.&lt;/div&gt;

&lt;p&gt;So while wind remains a small factor in the total electricity market in the US, there are parts of the country where it's a major factor in the generating mix. And, given the prices, those parts are likely to expand.&lt;/p&gt;
&lt;h2&gt;Plummeting prices&lt;/h2&gt;
&lt;p&gt;In the US, the prices for wind power had risen up until 2009, when power purchase agreements for wind-generated electricity peaked at about $70 per MegaWatt-hour. Since then, there's been a very steady decline, and 2018 saw the national average fall below $20/MW-hr for the first time. Again, there's regional variation with the Great Plains seeing the lowest prices, in some cases reaching the mid-teens.&lt;/p&gt;
&lt;p&gt;That puts wind in an incredibly competitive position. The report uses an estimate of future natural gas prices that show an extremely gradual rise of about $10/MW-hr out to 2050. But natural gas—on its own, without considering the cost of a plant to burn it for electricity—is already over $20/MW-hr. That means wind sited in the center of the US is already cheaper than fueling a natural gas plant, and wind sited elsewhere is roughly equal.&lt;/p&gt;
&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.50.17-PM.png&quot; class=&quot;enlarge&quot; data-height=&quot;687&quot; data-width=&quot;1111&quot; alt=&quot;Those black bars are the price of gas. Blue circles are wind, while yellow are solar.&quot;&gt;&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.50.17-PM-640x396.png&quot; width=&quot;640&quot; height=&quot;396&quot; srcset=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.50.17-PM.png 2x&quot; alt=&quot;Those black bars are the price of gas. Blue circles are wind, while yellow are solar.&quot;/&gt;&lt;/a&gt;
&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.50.17-PM.png&quot; class=&quot;enlarge-link&quot; data-height=&quot;687&quot; data-width=&quot;1111&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; Those black bars are the price of gas. Blue circles are wind, while yellow are solar.&lt;/div&gt;

&lt;p&gt;The report notes that photovoltaics have reached prices that are roughly equivalent to wind, but those got there from a starting point of about $150/MW-hr in 2009. Thus, unless natural gas prices reverse the expected trend and get cheaper, wind and solar will remain the cheapest sources of new electricity in the US.&lt;/p&gt;
&lt;p&gt;The levelized cost of electricity, which eliminates the impact of incentives and subsidies on the final prices, places wind below $40/MW-hr in 2018. The cheapest form of natural gas generation was roughly $10 more per MegaWatt-hour. Note that, as recently as 2015, the US' Energy Information Agency &lt;a href=&quot;https://en.wikipedia.org/wiki/Cost_of_electricity_by_source#United_States&quot;&gt;was predicting&lt;/a&gt; that wind's levelized cost in 2020 would be $74/MW-hr.&lt;/p&gt;
&lt;h2&gt;Built on better tech&lt;/h2&gt;
&lt;p&gt;Why has wind gotten much cheaper than expected? Part of it is in improved technology. The report notes that in 2008, there were no turbines installed in the US with rotors above 100 meters in diameter. In 2018, 99 percent of them were over 100m, and the average size was 116m. In general, the turbine's generator grew in parallel. The average capacity for 2018 installs was 2.4MW, which is up five percent from the year previous.&lt;/p&gt;
&lt;p&gt;The area swept by the blades goes up with the square of their length. Thus, even though blade length and rated generating capacity are going up in parallel, the actual potential energy input from the blades is growing much faster. This has the effect of lowering what's called the specific power of the wind turbine. These lower specific power turbines work better in areas where the wind isn't as strong or consistent. On the truly windy days, they'll saturate the ability of the generator to extract power, while on a more typical day when the winds are lighter or erratic, they'll get more out of them.&lt;/p&gt;
&lt;p&gt;So even though more turbines are being built at sites without the best wind resources, we're generating more power per turbine. The capacity factor—the amount of power generated relative to the size of the generator—for projects built in the previous four years has now hit 42 percent, a figure that would once have required offshore wind. That's dragged the capacity factor of the entire US wind industry up to over 35 percent for the first time last year.&lt;/p&gt;
&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.49.10-PM.png&quot; class=&quot;enlarge&quot; data-height=&quot;701&quot; data-width=&quot;1112&quot; alt=&quot;Each year, the capacity factor of newly installed projects is typically higher than that of the years prior.&quot;&gt;&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.49.10-PM-640x403.png&quot; width=&quot;640&quot; height=&quot;403&quot; srcset=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.49.10-PM.png 2x&quot; alt=&quot;Each year, the capacity factor of newly installed projects is typically higher than that of the years prior.&quot;/&gt;&lt;/a&gt;
&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2019/08/Screen-Shot-2019-08-16-at-3.49.10-PM.png&quot; class=&quot;enlarge-link&quot; data-height=&quot;701&quot; data-width=&quot;1112&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; Each year, the capacity factor of newly installed projects is typically higher than that of the years prior.&lt;/div&gt;

&lt;p&gt;The economics of these low-wind designs are so good that 23 existing sites were &quot;repowered,&quot; with new, larger rotors replacing older hardware on existing towers. One thing that may be encouraging this is that older plants (those a decade old or more) seem to see a small dip in capacity factor over time. But the reason for this isn't clear at this point, so it's something that will have to be tracked in the future.&lt;/p&gt;
&lt;p&gt;Better grid management also helped the economics of wind. At times, strong winds can cause wind farms to produce an excess of power relative to demand, causing a farm's output to be reduced. This process, called curtailment, remained a small factor, with only two percent of the potential generation lost this way. Put differently, if the curtailed electricity had been used, it would have only raised the average capacity factor by 0.7 percentage points.&lt;/p&gt;
&lt;p&gt;Overall, given these economics, it's clear that the economic case for wind energy will remain solid as the tax credits for the construction of renewable energy fade out over the next few years. But the vanishing credits are causing lots of developers to start projects sooner rather than later, so we may see a bubble in construction for the next couple of years, followed by a dramatic drop off.&lt;/p&gt;

</description>
<pubDate>Sun, 18 Aug 2019 05:35:39 +0000</pubDate>
<dc:creator>anandaverma18</dc:creator>
<og:url>https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/</og:url>
<og:title>Wind power prices now lower than the cost of natural gas</og:title>
<og:image>https://cdn.arstechnica.net/wp-content/uploads/2019/08/wind-power-reliability-research-hero-760x380.jpg</og:image>
<og:description>In the US, it's cheaper to build and operate wind farms than buy fossil fuels.</og:description>
<og:type>article</og:type>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arstechnica.com/science/2019/08/wind-power-prices-now-lower-than-the-cost-of-natural-gas/</dc:identifier>
</item>
<item>
<title>Apple will soon treat online web tracking the same as a security vulnerability</title>
<link>https://thenextweb.com/privacy/2019/08/16/apple-will-soon-treat-online-web-tracking-the-same-as-a-security-vulnerability/</link>
<guid isPermaLink="true" >https://thenextweb.com/privacy/2019/08/16/apple-will-soon-treat-online-web-tracking-the-same-as-a-security-vulnerability/</guid>
<description>&lt;p&gt;Apple is taking a hard stance on online privacy with a new anti-tracking policy in Safari.&lt;/p&gt;
&lt;p&gt;The iPhone maker has published a “&lt;a href=&quot;https://webkit.org/tracking-prevention-policy/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;WebKit Tracking Prevention Policy&lt;/a&gt;” that goes into specifics about the types of anti-tracking methods it has developed, the practices it believes are harmful to users, and the unintended consequences of those preventive countermeasures.&lt;/p&gt;


&lt;p&gt;The open-source WebKit browser rendering engine is the basis for Safari, in addition to the browsers bundled with Amazon Kindle ebook reader and Samsung Tizen OS.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.chromium.org/blink&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Blink&lt;/a&gt; — the rendering engine that powers Google Chrome — is also a fork of WebKit. But on iOS, Chrome and other third-party browsers rely on WebKit due to restrictions imposed by Apple’s &lt;a href=&quot;https://developer.apple.com/app-store/review/guidelines/#software-requirements&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;App Store Review Guidelines&lt;/a&gt; (Section 2.5.6).&lt;/p&gt;
&lt;p&gt;The policy enforcement comes as use of &lt;a href=&quot;https://blogs.harvard.edu/doc/2019/03/23/2billion/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;ad-blockers is at its peak&lt;/a&gt;, with privacy-focused web browser &lt;a href=&quot;https://decrypt.co/8297/number-of-publishers-on-brave-increased-1200-since-mid-2018&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Brave registering a 1,200 percent increase in verified publishers using its Brave Rewards program&lt;/a&gt; since July last year.&lt;/p&gt;
&lt;p&gt;Brave blocks ads by by default, but &lt;a href=&quot;https://brave.com/brave-rewards/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;allows its users to earn BAT tokens&lt;/a&gt; if they choose to accept ads that are provided by the company.&lt;/p&gt;
&lt;h3&gt;Intelligent Tracking Protection&lt;/h3&gt;
&lt;p&gt;Publishers and companies rely heavily on online tracking — i.e. collecting (anonymized) data about a user’s activity on the web — to keep tabs on your every move as you hop from one site to the other.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; readability=&quot;8.8&quot;&gt;
&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Thanks everyone who attended my talk on web privacy at &lt;a href=&quot;https://twitter.com/hashtag/usesec19?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;#usesec19&lt;/a&gt;. My demos worked – yay!&lt;/p&gt;
&lt;p&gt;By the way, we *just* announced the WebKit Tracking Prevention Policy: &lt;a href=&quot;https://t.co/jo5MPkNAAs&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://t.co/jo5MPkNAAs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— John Wilander (@johnwilander) &lt;a href=&quot;https://twitter.com/johnwilander/status/1161786363806568448?ref_src=twsrc%5Etfw&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;August 14, 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;While this is typically used for targeted advertising, the implications go beyond just serving relevant ads in that it allows marketers to create detailed dossiers about your interests — resulting in significant loss of privacy.&lt;/p&gt;
&lt;p&gt;This involves the use of cookies, tracking pixels, &lt;a href=&quot;https://amiunique.org/faq&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;browser and device fingerprinting&lt;/a&gt;, and other adtech-based navigational tracking methods intended to amass browsing activities and build elaborate profiles of web users.&lt;/p&gt;
&lt;p&gt;Apple, for its part, began to crack down on web-tracking two years ago with &lt;a href=&quot;https://webkit.org/blog/8828/intelligent-tracking-prevention-2-2/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Intelligent Tracking Protection&lt;/a&gt; (ITP). The technology aims to limit advertisers’ cross-site tracking on iOS and macOS Safari browsers, at the same time, &lt;a href=&quot;https://thenextweb.com/business/2019/05/23/google-and-facebook-are-gonna-hate-apples-new-privacy-preserving-online-ads/&quot;&gt;measure the effectiveness of their ad campaigns on the web without compromising on your privacy&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The anti-tracking policy&lt;/h3&gt;
&lt;p&gt;Viewed in that light, the new policy is an extension of this &lt;a href=&quot;https://en.wikipedia.org/wiki/Privacy_by_design&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;privacy-by-design paradigm&lt;/a&gt;. It seeks to prevent all forms of covert tracking methods outlined above, failing which it will ask for user’s informed consent before allowing tracking.&lt;/p&gt;
&lt;p&gt;Apple warns that parties trying to circumvent its anti-tracking tech in Safari will be treated “with the same seriousness as exploitation of security vulnerabilities,” and that it “may add additional restrictions without prior notice.”&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; readability=&quot;7.3603238866397&quot;&gt;
&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;This is a big deal — &lt;a href=&quot;https://twitter.com/webkit?ref_src=twsrc%5Etfw&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;@webkit&lt;/a&gt; releases a Tracking Prevention Policy outlining the types of tracking practices that are (or will be) blocked in WebKit (and thus Safari). &lt;a href=&quot;https://t.co/E0SKOx6rqp&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://t.co/E0SKOx6rqp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Steven Englehardt (@s_englehardt) &lt;a href=&quot;https://twitter.com/s_englehardt/status/1161820610781044736?ref_src=twsrc%5Etfw&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;August 15, 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At the same time, the Cupertino-based tech giant acknowledged it will “try to limit unintended impact” of its anti-tracking methods, particularly on practices that could be affected because “they rely on techniques that can also be used for tracking,” such as “Like” buttons, third-party sign-on, and bot detection.&lt;/p&gt;
&lt;p&gt;It is, however, not immediately clear if paywall detection methods employed by publishers will be considered forbidden in this context.&lt;/p&gt;
&lt;h3&gt;Privacy trumps all&lt;/h3&gt;
&lt;p&gt;“When faced with a tradeoff, we will typically prioritize user benefits over preserving current website practices,” the WebKit engineering team said.&lt;/p&gt;
&lt;p&gt;Apple is not the first company to go after abusive tracking methods — the company notes its anti-tracking methods were inspired by &lt;a href=&quot;https://wiki.mozilla.org/Security/Anti_tracking_policy&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Mozilla’s anti-tracking policy&lt;/a&gt;. But with Safari being the default browser on all Apple devices, the development could tip the scales in its favor.&lt;/p&gt;
&lt;p&gt;By equating circumvention of anti-tracking measures with a security vulnerability, &lt;a href=&quot;https://thenextweb.com/privacy/2019/08/07/apple-will-limit-voip-background-data-collection-from-whatsapp-snapchat-et-al-in-ios-13/&quot;&gt;Apple has taken its efforts to guarantee user privacy&lt;/a&gt; up a notch.&lt;/p&gt;
&lt;p&gt;While a lot of it will depend how the policy is enforced, you can bet it will force advertisers and other browser makers — including &lt;a href=&quot;https://gs.statcounter.com/browser-market-share&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Google’s widely used Chrome&lt;/a&gt; — to rethink their approach. And that can only be a win for privacy.&lt;/p&gt;
&lt;p class=&quot;post-article-read-next&quot;&gt;&lt;strong&gt;Read next:&lt;/strong&gt; &lt;a class=&quot;gtm-article-read-next&quot; data-event-category=&quot;Article&quot; data-event-action=&quot;Next post&quot; data-event-label=&quot;&quot; data-event-non-interaction=&quot;true&quot; href=&quot;https://thenextweb.com/hardfork/2019/08/16/tech-journalist-holding-cryptocurrency-loses-300000/&quot;&gt;Tech journalist breaks first rule of holding cryptocurrency, loses $30,000&lt;/a&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 18 Aug 2019 05:33:00 +0000</pubDate>
<dc:creator>praveenscience</dc:creator>
<og:type>article</og:type>
<og:title>Apple will soon treat online web tracking the same as a security vulnerability</og:title>
<og:description>Apple ups its privacy batte with a new &quot;WebKit Tracking Prevention Policy&quot; that aims to prevent all forms covert tracking online.</og:description>
<og:url>https://thenextweb.com/privacy/2019/08/16/apple-will-soon-treat-online-web-tracking-the-same-as-a-security-vulnerability/</og:url>
<og:image>https://img-cdn.tnwcdn.com/image/tnw?filter_last=1&amp;fit=1280%2C640&amp;url=https%3A%2F%2Fcdn0.tnwcdn.com%2Fwp-content%2Fblogs.dir%2F1%2Ffiles%2F2018%2F10%2FApple-Event-Oct.jpg&amp;signature=75c48527b47d360dc8713d88aa6b8ba1</og:image>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://thenextweb.com/privacy/2019/08/16/apple-will-soon-treat-online-web-tracking-the-same-as-a-security-vulnerability/</dc:identifier>
</item>
<item>
<title>Python vs. Rust for Neural Networks</title>
<link>https://ngoldbaum.github.io/posts/python-vs-rust-nn/</link>
<guid isPermaLink="true" >https://ngoldbaum.github.io/posts/python-vs-rust-nn/</guid>
<description>&lt;p&gt;In &lt;a href=&quot;https://ngoldbaum.github.io/posts/loading-mnist-data-in-rust/&quot;&gt;a previous post&lt;/a&gt; I introduced the MNIST dataset and the problem of classifying handwritten digits. In this post I’ll be using the code I wrote in that post to port a simple neural network implementation to rust. My goal is to explore performance and ergonomics for data science workflows in rust.&lt;/p&gt;
&lt;h2 id=&quot;the-python-implementation&quot;&gt;The Python Implementation&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;Chapter 1&lt;/a&gt; of the book describes a very simple single-layer Neural Network that can classify handwritten digits from the &lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST dataset&lt;/a&gt; using a learning algorithm based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;stochastic gradient descent&lt;/a&gt;. This sounds complicated — and it kind of is, this stuff was state-of-the-art in the mid 1980s — but really it all comes down to about &lt;a href=&quot;https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py&quot;&gt;150 lines of heavily commented Python code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’m going to assume that you already know the content of that chapter so stop here and go read that if you want to brush up on neural network basics. Or don’t and just pay attention to the code, it’s not super important to understand the details of exactly why the code works the way it does to see the differences between the Python approach and the Rust approach.&lt;/p&gt;
&lt;p&gt;The fundamental data container in this code is a &lt;code&gt;Network&lt;/code&gt; class that represents a neural network with a user-controllable number of layers and number of neurons per layer. The data for the &lt;code&gt;Network&lt;/code&gt; class are represented internally as lists of 2D NumPy arrays. Each layer of the network is represented as a 2D array of weights and 1D array of biases, contained in attributes of the &lt;code&gt;Network&lt;/code&gt; class named &lt;code&gt;biases&lt;/code&gt; and &lt;code&gt;weights&lt;/code&gt;. These are both lists of 2D arrays. The biases are column vectors but are still stored as 2D arrays by making use of a &lt;a href=&quot;https://stackoverflow.com/questions/17428621/python-differentiating-between-row-and-column-vectors&quot;&gt;dummy dimension&lt;/a&gt;. The initializer for the &lt;code&gt;Network&lt;/code&gt; class looks like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;25&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;class&lt;/span&gt; &lt;span&gt;Network&lt;/span&gt;(object):

    &lt;span&gt;def&lt;/span&gt; __init__(self, sizes):
        &lt;span&gt;&quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the
&lt;/span&gt;&lt;span&gt;        respective layers of the network.  For example, if the list
&lt;/span&gt;&lt;span&gt;        was [2, 3, 1] then it would be a three-layer network, with the
&lt;/span&gt;&lt;span&gt;        first layer containing 2 neurons, the second layer 3 neurons,
&lt;/span&gt;&lt;span&gt;        and the third layer 1 neuron.  The biases and weights for the
&lt;/span&gt;&lt;span&gt;        network are initialized randomly, using a Gaussian
&lt;/span&gt;&lt;span&gt;        distribution with mean 0, and variance 1.  Note that the first
&lt;/span&gt;&lt;span&gt;        layer is assumed to be an input layer, and by convention we
&lt;/span&gt;&lt;span&gt;        won't set any biases for those neurons, since biases are only
&lt;/span&gt;&lt;span&gt;        ever used in computing the outputs from later layers.&quot;&quot;&quot;&lt;/span&gt;
        self&lt;span&gt;.&lt;/span&gt;num_layers &lt;span&gt;=&lt;/span&gt; len(sizes)
        self&lt;span&gt;.&lt;/span&gt;sizes &lt;span&gt;=&lt;/span&gt; sizes
        self&lt;span&gt;.&lt;/span&gt;biases &lt;span&gt;=&lt;/span&gt; [np&lt;span&gt;.&lt;/span&gt;random&lt;span&gt;.&lt;/span&gt;randn(y, &lt;span&gt;1&lt;/span&gt;) &lt;span&gt;for&lt;/span&gt; y &lt;span&gt;in&lt;/span&gt; sizes[&lt;span&gt;1&lt;/span&gt;:]]
        self&lt;span&gt;.&lt;/span&gt;weights &lt;span&gt;=&lt;/span&gt; [np&lt;span&gt;.&lt;/span&gt;random&lt;span&gt;.&lt;/span&gt;randn(y, x)
                        &lt;span&gt;for&lt;/span&gt; x, y &lt;span&gt;in&lt;/span&gt; zip(sizes[:&lt;span&gt;-&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;], sizes[&lt;span&gt;1&lt;/span&gt;:])]&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In this simple implementation the weights and biases are initialized by drawing from the standard normal distribution — a normal distribution with a mean of zero, standard deviation of 1. We can also see how the biases are explicitly initialized as column vectors.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Network&lt;/code&gt; class exposes two methods that users would call directly. First, the &lt;code&gt;evaluate&lt;/code&gt; method, which asks the network to try to identify the digits in a set of test images and then scores the result based on the &lt;em&gt;a priori&lt;/em&gt; known correct answer. Second, the &lt;code&gt;SGD&lt;/code&gt; method runs a stochastic gradient descent learning procedure by iterating over a set of images, breaking up the full set of images into small mini-batches, updating the network’s state based on each mini-batch of images and a user-specifiable learning rate, &lt;code&gt;eta&lt;/code&gt;, and then re-running the training procedure for a new randomly selected set of mini-batches for a user-specifiable number of &lt;em&gt;epochs&lt;/em&gt;. The core of the algorithm, where each mini-batch and the state of the neural network gets updated, looks like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;25&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;def&lt;/span&gt; &lt;span&gt;update_mini_batch&lt;/span&gt;(self, mini_batch, eta):
    &lt;span&gt;&quot;&quot;&quot;Update the network's weights and biases by applying
&lt;/span&gt;&lt;span&gt;    gradient descent using backpropagation to a single mini batch.
&lt;/span&gt;&lt;span&gt;    The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
&lt;/span&gt;&lt;span&gt;    is the learning rate.&quot;&quot;&quot;&lt;/span&gt;
    nabla_b &lt;span&gt;=&lt;/span&gt; [np&lt;span&gt;.&lt;/span&gt;zeros(b&lt;span&gt;.&lt;/span&gt;shape) &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt; self&lt;span&gt;.&lt;/span&gt;biases]
    nabla_w &lt;span&gt;=&lt;/span&gt; [np&lt;span&gt;.&lt;/span&gt;zeros(w&lt;span&gt;.&lt;/span&gt;shape) &lt;span&gt;for&lt;/span&gt; w &lt;span&gt;in&lt;/span&gt; self&lt;span&gt;.&lt;/span&gt;weights]
    &lt;span&gt;for&lt;/span&gt; x, y &lt;span&gt;in&lt;/span&gt; mini_batch:
        delta_nabla_b, delta_nabla_w &lt;span&gt;=&lt;/span&gt; self&lt;span&gt;.&lt;/span&gt;backprop(x, y)
        nabla_b &lt;span&gt;=&lt;/span&gt; [nb&lt;span&gt;+&lt;/span&gt;dnb &lt;span&gt;for&lt;/span&gt; nb, dnb &lt;span&gt;in&lt;/span&gt; zip(nabla_b, delta_nabla_b)]
        nabla_w &lt;span&gt;=&lt;/span&gt; [nw&lt;span&gt;+&lt;/span&gt;dnw &lt;span&gt;for&lt;/span&gt; nw, dnw &lt;span&gt;in&lt;/span&gt; zip(nabla_w, delta_nabla_w)]
    self&lt;span&gt;.&lt;/span&gt;weights &lt;span&gt;=&lt;/span&gt; [w&lt;span&gt;-&lt;/span&gt;(eta&lt;span&gt;/&lt;/span&gt;len(mini_batch))&lt;span&gt;*&lt;/span&gt;nw
                    &lt;span&gt;for&lt;/span&gt; w, nw &lt;span&gt;in&lt;/span&gt; zip(self&lt;span&gt;.&lt;/span&gt;weights, nabla_w)]
    self&lt;span&gt;.&lt;/span&gt;biases &lt;span&gt;=&lt;/span&gt; [b&lt;span&gt;-&lt;/span&gt;(eta&lt;span&gt;/&lt;/span&gt;len(mini_batch))&lt;span&gt;*&lt;/span&gt;nb
                   &lt;span&gt;for&lt;/span&gt; b, nb &lt;span&gt;in&lt;/span&gt; zip(self&lt;span&gt;.&lt;/span&gt;biases, nabla_b)]&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For each training image in the mini-batch, we accumulate estimates for the gradient of the cost function via backpropagation (implemented in the &lt;code&gt;backprop&lt;/code&gt; function). Once we exhaust the mini-batch, we adjust the weights and biases according to the estimated gradients. The update includes &lt;code&gt;len(mini_batch)&lt;/code&gt; in the denominator because we want the average gradient over all the estimates in the mini-batch. We can also control how fast the weights and biases get updated by adjusting the learning rate, &lt;code&gt;eta&lt;/code&gt;, which globally modulates how big the updates from each mini-batch can be.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;backprop&lt;/code&gt; function calculates the cost gradient for the neural network by starting with the expected output of the network given the input image and then working backward through the network to propagate the error in the network through the layers. This requires a substantial amount of data munging, and its where I spent most of my time porting the code to rust but I think it’s a little too long to dive into in depth here, take a look at &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap2.html&quot;&gt;chapter 2&lt;/a&gt; of the book if you want more detail.&lt;/p&gt;
&lt;h2 id=&quot;the-rust-implementation&quot;&gt;The Rust Implementation&lt;/h2&gt;
&lt;p&gt;The first step here was to figure out how to load the data. That ended up being fiddly enough that I decided to break that off into its &lt;a href=&quot;https://ngoldbaum.github.io/posts/loading-mnist-data-in-rust/&quot;&gt;own post&lt;/a&gt;. With that sorted I then had to figure out how to represent the Python &lt;code&gt;Network&lt;/code&gt; class in rust. I ended up deciding to use a struct:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;use&lt;/span&gt; ndarray::Array2;

&lt;span&gt;#[derive(Debug)]&lt;/span&gt;
&lt;span&gt;struct&lt;/span&gt; &lt;span&gt;Network&lt;/span&gt; {
    num_layers: &lt;span&gt;usize&lt;/span&gt;,
    sizes: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;usize&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;,
    biases: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt;,
    weights: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt;,
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The struct gets initialized with the number of neurons in each layer in much the same way as the Python implementation:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;19&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;use&lt;/span&gt; rand::distributions::StandardNormal;
&lt;span&gt;use&lt;/span&gt; ndarray::{Array, Array2};
&lt;span&gt;use&lt;/span&gt; ndarray_rand::RandomExt;

&lt;span&gt;impl&lt;/span&gt; Network {
       &lt;span&gt;fn&lt;/span&gt; &lt;span&gt;new&lt;/span&gt;(sizes: &lt;span&gt;&amp;amp;&lt;/span&gt;[&lt;span&gt;usize&lt;/span&gt;]) -&amp;gt; &lt;span&gt;Network&lt;/span&gt; {
        &lt;span&gt;let&lt;/span&gt; num_layers &lt;span&gt;=&lt;/span&gt; sizes.len();
        &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; biases: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; Vec::new();
        &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; weights: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; Vec::new();
        &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;..num_layers {
            biases.push(Array::random((sizes[i], &lt;span&gt;1&lt;/span&gt;), StandardNormal));
            weights.push(Array::random((sizes[i], sizes[i &lt;span&gt;-&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;]), StandardNormal));
        }
        Network {
            num_layers: &lt;span&gt;num_layers&lt;/span&gt;,
            sizes: &lt;span&gt;sizes&lt;/span&gt;.to_owned(),
            biases: &lt;span&gt;biases&lt;/span&gt;,
            weights: &lt;span&gt;weights&lt;/span&gt;,
        }
    } 
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One difference is that in Python we used &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html&quot;&gt;&lt;code&gt;numpy.random.randn&lt;/code&gt;&lt;/a&gt; to initialize the biases and weights while in rust we use the &lt;code&gt;ndarray::Array::random&lt;/code&gt; function which accepts a &lt;code&gt;rand::distribution::Distribution&lt;/code&gt;as a parameter, allowing the choice of an arbitrary distribution. In this case we used the &lt;code&gt;rand::distributions::StandardNormal&lt;/code&gt; distribution. It’s worth noting that this uses an interface defined in three different crates, two of which — &lt;code&gt;ndarray&lt;/code&gt; itself and &lt;code&gt;ndarray-rand&lt;/code&gt; — are maintained by the &lt;code&gt;ndarray&lt;/code&gt; authors, and another — &lt;code&gt;rand&lt;/code&gt; — maintained by a different set of developers.&lt;/p&gt;
&lt;h3 id=&quot;the-merits-of-monolithic-packages&quot;&gt;The merits of monolithic packages&lt;/h3&gt;
&lt;p&gt;In principle it’s nice that random number generation is not isolated inside the &lt;code&gt;ndarray&lt;/code&gt; codebase and if new random number distributions or capabilities are added to &lt;code&gt;rand&lt;/code&gt;, &lt;code&gt;ndarray&lt;/code&gt; and all other crates in the rust ecosystem that need random numbers can benefit equally. On the other hand it does add some cognitive overhead to need to refer between the documentation for the various crates instead of having a single centralized place to look. In my particular case I also got a little unlucky and happened to do this project right after &lt;code&gt;rand&lt;/code&gt; made a release that changed its public API. This led to an incompatibility between &lt;code&gt;ndarray-rand&lt;/code&gt;, which depended on version 0.6 of &lt;code&gt;rand&lt;/code&gt;, and my project which declared a dependency on version 0.7.&lt;/p&gt;
&lt;p&gt;I’d heard that &lt;code&gt;cargo&lt;/code&gt; and rust’s build system handle this sort of problem really well but at least in this case I was presented with a confusing error message about how the random number distribution I was passing in didn’t satisfy the &lt;code&gt;Distribution&lt;/code&gt; trait. While this is true — it satisfied the &lt;code&gt;Distribution&lt;/code&gt; trait from &lt;code&gt;rand 0.7&lt;/code&gt; but not the one from &lt;code&gt;rand 0.6&lt;/code&gt; that &lt;code&gt;ndarray-rand&lt;/code&gt; expected — it is extremely confusing because the version numbers of the various crates don’t show up in the error message. I ended up reporting this as &lt;a href=&quot;https://github.com/rust-ndarray/ndarray/issues/658&quot;&gt;an issue&lt;/a&gt;. I discovered there that these confusing error messages from crates with incompatible APIs is &lt;a href=&quot;https://github.com/rust-lang/rust/issues/22750&quot;&gt;a long-standing issue&lt;/a&gt; for the rust language. Hopefully in the future rust can grow more helpful error messages.&lt;/p&gt;
&lt;p&gt;In the end this separation of concerns caused a lot of friction for me as a new user. In Python I could have simply done &lt;code&gt;import numpy&lt;/code&gt; and be done. I do think that NumPy probably went a bit too far in the direction of being completely monolithic — it was originally written at a time when packaging and distributing Python code with C extensions was much harder than it is today — I do think that going too far in the other extreme can make a language or ecosystem of tools harder to learn.&lt;/p&gt;
&lt;h3 id=&quot;types-and-ownership&quot;&gt;Types and ownership&lt;/h3&gt;
&lt;p&gt;The next bit I’ll show in detail is the rust version of &lt;code&gt;update_mini_batch&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;19&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;impl&lt;/span&gt; Network {
    &lt;span&gt;fn&lt;/span&gt; &lt;span&gt;update_mini_batch&lt;/span&gt;(
        &lt;span&gt;&amp;amp;&lt;/span&gt;&lt;span&gt;mut&lt;/span&gt; self,
        training_data: &lt;span&gt;&amp;amp;&lt;/span&gt;[MnistImage],
        mini_batch_indices: &lt;span&gt;&amp;amp;&lt;/span&gt;[&lt;span&gt;usize&lt;/span&gt;],
        eta: &lt;span&gt;f64&lt;/span&gt;,
    ) {
        &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; nabla_b: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; zero_vec_like(&lt;span&gt;&amp;amp;&lt;/span&gt;self.biases);
        &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; nabla_w: Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;=&lt;/span&gt; zero_vec_like(&lt;span&gt;&amp;amp;&lt;/span&gt;self.weights);
        &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; mini_batch_indices {
            &lt;span&gt;let&lt;/span&gt; (delta_nabla_b, delta_nabla_w) &lt;span&gt;=&lt;/span&gt; self.backprop(&lt;span&gt;&amp;amp;&lt;/span&gt;training_data[&lt;span&gt;*&lt;/span&gt;i]);
            &lt;span&gt;for&lt;/span&gt; (nb, dnb) &lt;span&gt;in&lt;/span&gt; nabla_b.iter_mut().zip(delta_nabla_b.iter()) {
                &lt;span&gt;*&lt;/span&gt;nb &lt;span&gt;+=&lt;/span&gt; dnb;
            }
            &lt;span&gt;for&lt;/span&gt; (nw, dnw) &lt;span&gt;in&lt;/span&gt; nabla_w.iter_mut().zip(delta_nabla_w.iter()) {
                &lt;span&gt;*&lt;/span&gt;nw &lt;span&gt;+=&lt;/span&gt; dnw;
            }
        }
        &lt;span&gt;let&lt;/span&gt; nbatch &lt;span&gt;=&lt;/span&gt; mini_batch_indices.len() &lt;span&gt;as&lt;/span&gt; &lt;span&gt;f64&lt;/span&gt;;
        &lt;span&gt;for&lt;/span&gt; (w, nw) &lt;span&gt;in&lt;/span&gt; self.weights.iter_mut().zip(nabla_w.iter()) {
            &lt;span&gt;*&lt;/span&gt;w &lt;span&gt;-=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;nw.mapv(&lt;span&gt;|&lt;/span&gt;x&lt;span&gt;|&lt;/span&gt; x &lt;span&gt;*&lt;/span&gt; eta &lt;span&gt;/&lt;/span&gt; nbatch);
        }
        &lt;span&gt;for&lt;/span&gt; (b, nb) &lt;span&gt;in&lt;/span&gt; self.biases.iter_mut().zip(nabla_b.iter()) {
            &lt;span&gt;*&lt;/span&gt;b &lt;span&gt;-=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;nb.mapv(&lt;span&gt;|&lt;/span&gt;x&lt;span&gt;|&lt;/span&gt; x &lt;span&gt;*&lt;/span&gt; eta &lt;span&gt;/&lt;/span&gt; nbatch);
        }
    }
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The function makes use of two short helper functions I defined that makes this a little less verbose:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;fn&lt;/span&gt; &lt;span&gt;to_tuple&lt;/span&gt;(inp: &lt;span&gt;&amp;amp;&lt;/span&gt;[&lt;span&gt;usize&lt;/span&gt;]) -&amp;gt; (&lt;span&gt;usize&lt;/span&gt;, &lt;span&gt;usize&lt;/span&gt;) {
    &lt;span&gt;match&lt;/span&gt; inp {
        [a, b] &lt;span&gt;=&amp;gt;&lt;/span&gt; (&lt;span&gt;*&lt;/span&gt;a, &lt;span&gt;*&lt;/span&gt;b),
        _ &lt;span&gt;=&amp;gt;&lt;/span&gt; panic&lt;span&gt;!&lt;/span&gt;(),
    }
}

&lt;span&gt;fn&lt;/span&gt; &lt;span&gt;zero_vec_like&lt;/span&gt;(inp: &lt;span&gt;&amp;amp;&lt;/span&gt;[Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;]) -&amp;gt; Vec&lt;span&gt;&amp;lt;&lt;/span&gt;Array2&lt;span&gt;&amp;lt;&lt;/span&gt;&lt;span&gt;f64&lt;/span&gt;&lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; {
    inp.iter()
        .map(&lt;span&gt;|&lt;/span&gt;x&lt;span&gt;|&lt;/span&gt; Array2::zeros(to_tuple(x.shape())))
        .collect()
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Comparing with the Python implementation the interface for calling &lt;code&gt;update_mini_batch&lt;/code&gt; is a little different. Rather than passing in a list of objects directly, instead of I pass in a reference to the full set of training data and a slice of indices to consider within that full set. This ended up being a little easier to reason about without triggering the borrow checker.&lt;/p&gt;
&lt;p&gt;Creating &lt;code&gt;nabla_b&lt;/code&gt; and &lt;code&gt;nabla_w&lt;/code&gt; in &lt;code&gt;zero_vec_like&lt;/code&gt; is very similar to the list comprehension we used in Python. There is one wrinkle that caused me some frustration which is that if I try to create a zero-filled array with &lt;code&gt;Array2::zeros&lt;/code&gt; and pass it a slice or &lt;code&gt;Vec&lt;/code&gt; for the shape, I get back an &lt;code&gt;ArrayD&lt;/code&gt; instance. To get an &lt;code&gt;Array2&lt;/code&gt; — that is explicitly a 2D array and not a generic D-dimensional array — I need to pass a tuple to &lt;code&gt;Array::zeros&lt;/code&gt;. However, since &lt;code&gt;ndarray::shape&lt;/code&gt; returns a slice, I need to convert the slice to a tuple manually using the &lt;code&gt;to_tuple&lt;/code&gt; function. This sort of thing can be glossed over in Python but in rust the difference between a tuple and slice can be very important, as in this API.&lt;/p&gt;
&lt;p&gt;The code to estimate the updates for the weights and biases via backpropagation has a very similar structure to the python implementation. We train each example image in the mini-batch and obtain estimates for the gradient of the quadratic cost as a function of the biases and weights:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;let&lt;/span&gt; (delta_nabla_b, delta_nabla_w) &lt;span&gt;=&lt;/span&gt; self.backprop(&lt;span&gt;&amp;amp;&lt;/span&gt;training_data[&lt;span&gt;*&lt;/span&gt;i]);
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;and then accumulate these estimates:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;for&lt;/span&gt; (nb, dnb) &lt;span&gt;in&lt;/span&gt; nabla_b.iter_mut().zip(delta_nabla_b.iter()) {
    &lt;span&gt;*&lt;/span&gt;nb &lt;span&gt;+=&lt;/span&gt; dnb;
}
&lt;span&gt;for&lt;/span&gt; (nw, dnw) &lt;span&gt;in&lt;/span&gt; nabla_w.iter_mut().zip(delta_nabla_w.iter()) {
    &lt;span&gt;*&lt;/span&gt;nw &lt;span&gt;+=&lt;/span&gt; dnw;
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once we’ve finished processing the mini-batch, we update the weights and biases, modulated by the learning rate:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;let&lt;/span&gt; nbatch &lt;span&gt;=&lt;/span&gt; mini_batch_indices.len() &lt;span&gt;as&lt;/span&gt; &lt;span&gt;f64&lt;/span&gt;;
&lt;span&gt;for&lt;/span&gt; (w, nw) &lt;span&gt;in&lt;/span&gt; self.weights.iter_mut().zip(nabla_w.iter()) {
    &lt;span&gt;*&lt;/span&gt;w &lt;span&gt;-=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;nw.mapv(&lt;span&gt;|&lt;/span&gt;x&lt;span&gt;|&lt;/span&gt; x &lt;span&gt;*&lt;/span&gt; eta &lt;span&gt;/&lt;/span&gt; nbatch);
}
&lt;span&gt;for&lt;/span&gt; (b, nb) &lt;span&gt;in&lt;/span&gt; self.biases.iter_mut().zip(nabla_b.iter()) {
    &lt;span&gt;*&lt;/span&gt;b &lt;span&gt;-=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;nb.mapv(&lt;span&gt;|&lt;/span&gt;x&lt;span&gt;|&lt;/span&gt; x &lt;span&gt;*&lt;/span&gt; eta &lt;span&gt;/&lt;/span&gt; nbatch);
}
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This example illustrates how the ergonomics of working with array data is very different in Rust compared with Python. First, rather than multiplying the array by the float &lt;code&gt;eta / nbatch&lt;/code&gt;, we instead use &lt;code&gt;Array::mapv&lt;/code&gt; and define a closure in-line to map in a vectorized manner over the full array. This sort of thing would not be very fast in Python because function calls are very slow. In rust it doesn’t make much difference. We also need to borrow the return value of &lt;code&gt;mapv&lt;/code&gt; with &lt;code&gt;&amp;amp;&lt;/code&gt; when we subtract, lest we consume the array data while we iterate over it. Needing to think carefully about whether functions consume data or take references makes it much more conceptually demanding to write code like this in Rust than in Python. On the other hand I do have much higher confidence that my code is correct when it compiles. I’m not sure whether the fact that this code was so demanding for me to write is due to Rust really being harder to write or the disparity between my experience in Rust and Python.&lt;/p&gt;
&lt;h2 id=&quot;rewrite-it-in-rust-and-everything-will-be-better&quot;&gt;Rewrite it in rust and everything will be better&lt;/h2&gt;
&lt;p&gt;At this point I was left with something that was faster than the unoptimized Python version I had started with. However, instead of a 10x or better speedup that one might expect moving from a dynamic, interpreted language like Python to a compiled performance-oriented language like rust, I only observed about a 2x improvement. To understand why I decided to measure the performance of the rust code. Luckily there is a very nice project that makes it easy to generate flame graphs for rust projects: &lt;a href=&quot;https://github.com/ferrous-systems/flamegraph&quot;&gt;flamegraph&lt;/a&gt;. This adds a &lt;code&gt;flamegraph&lt;/code&gt; subcommand to &lt;code&gt;cargo&lt;/code&gt;, so one needs only to do &lt;code&gt;cargo flamegraph&lt;/code&gt; in a crate, it will run the code, and then write a flamegraph &lt;code&gt;svg&lt;/code&gt; file one can inspect with a web browser.&lt;/p&gt;
Flame Graph Reset ZoomSearch nda.._$L..nda..cbl..dge..dge..nndl..core..&amp;gt;::apply_core&amp;gt;::apply_cor..co..ndarray::impl_methods::&amp;gt;::apply_core&amp;gt;::apply_cor..ndarray::impl_ops::assign_ops::try__rust_maybe_catch_panicdo_call{{closure}}std::rt::lang_start::_$u7b$$u7b$closure$u7d$$u7d$::h780d4f1c15ceddd2nndl_rust::mainc..dgemm_beta_HASW..dgemm_kernel_HASWELLnndl-rust
&lt;p&gt;If you’ve never looked at a flamegraph before the idea is that the proportion of a program’s runtime that occurs in a routine is proportional to the width of the bar for that routine. The main function is at the bottom of the graph and functions called by main are stacked on top. This gives you a simple view into what functions take up the most time in a program - anything that is very “wide” in the graph is where most of the time is spent and any stack of functions that is very tall and wide is spending a lot of time in code very deep in a call stack. Looking at the flamegraph above we can see that about half of the time is spent in functions with names like &lt;code&gt;dgemm_kernel_HASWELL&lt;/code&gt; — these are functions in the OpenBLAS linear alebra library. The rest of the time is spent doing addition between arrays in `update_mini_batch and allocating arrays — all other parts of my program make a negligible contribution to the runtime.&lt;/p&gt;
&lt;p&gt;If we made an analogous flamegraph for the python code, we would see a similar pattern — most time is spent doing linear algebra (in the places where &lt;code&gt;np.dot&lt;/code&gt; is called inside the backpropagation routine). So since most of the time in either Rust or Python is spent inside a numerical linear algebra library, we can never hope for a 10x speedup.&lt;/p&gt;
&lt;p&gt;In fact it’s worse than that. One of the exercises in the book is to rewrite the Python code to use vectorized matrix multiplication. In this approach the backpropagation for all of the samples in each mini-batch happens in a single set of vectorized matrix multiplication operations. This requires the ability to matrix multiplication between 3D and 2D arrays. Since each matrix multiplication operation happens using a larger amount of data than the non-vectorized case, OpenBLAS is able to more efficiently occupy CPU caches and registers, ultimately better using the available CPU resources on my laptop. The rewritten Python version ends up faster than the Rust version, again by a factor of two or so.&lt;/p&gt;
&lt;p&gt;In principle it’s possible to apply the same optimization to the Rust code, however the &lt;code&gt;ndarray&lt;/code&gt; crate does &lt;a href=&quot;https://github.com/rust-ndarray/ndarray/issues/16&quot;&gt;not yet support&lt;/a&gt; matrix multiplication for dimensionalities higher than 2. It might also be possible to use thread parallelization on the mini-batch updates using a library like &lt;a href=&quot;https://docs.rs/rayon/1.1.0/rayon/&quot;&gt;rayon&lt;/a&gt;. I tried this on my laptop and did not see any speedups but might have on a beefier machine with more CPU threads. I could also have tried using a different low-level linear algebra implementation, for example there are rust bindings &lt;a href=&quot;https://github.com/tensorflow/rust&quot;&gt;for tensorflow&lt;/a&gt; and &lt;a href=&quot;https://github.com/LaurentMazare/tch-rs&quot;&gt;torch&lt;/a&gt;, however at that point I feel like I might as well be using the Python bindings for those libraries.&lt;/p&gt;
&lt;h2 id=&quot;is-rust-suitable-for-data-science-workflows&quot;&gt;Is rust suitable for data science workflows?&lt;/h2&gt;
&lt;p&gt;Right now I have to say that the answer is “not yet”. I’ll definitely reach for rust in the future when I need to write optimized low-level code with minimal dependencies. However using it as a full replacement for python or C++ will require a more stabilized and well-developed ecosystem of packages.&lt;/p&gt;
</description>
<pubDate>Sun, 18 Aug 2019 04:46:29 +0000</pubDate>
<dc:creator>rencire</dc:creator>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://ngoldbaum.github.io/posts/python-vs-rust-nn/</dc:identifier>
</item>
<item>
<title>Tech Interview Handbook</title>
<link>https://yangshun.github.io/tech-interview-handbook/</link>
<guid isPermaLink="true" >https://yangshun.github.io/tech-interview-handbook/</guid>
<description>&lt;blockquote readability=&quot;20&quot;&gt;
&lt;p&gt;&quot;The Tech Interview Handbook played a crucial role in the success of my previous job search. The contents are carefully curated and well organized. It served as an excellent roadmap for my interview prep.&lt;/p&gt;&lt;p&gt;In addition to the thorough Data Structures and Algorithms section, the handbook also provides a lot of resources on other aspects of the application process that helped me see the tech interviews in a more holistic way. My favorite non-technical part was &quot;Questions To Ask&quot;! I used quite a few insightful questions from there to challenge and impress my interviewers. The results were great!&lt;/p&gt;&lt;p&gt;With the help of Tech Interview Handbook, I was able to land offers from Google, Amazon, Uber and several other great companies. Really appreciate Yangshun and other contributors for putting out such quality content for the community. I'd wholeheartedly recommend this handbook to anyone!&quot;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
<pubDate>Sat, 17 Aug 2019 23:34:41 +0000</pubDate>
<dc:creator>yangshun</dc:creator>
<og:title>Tech Interview Handbook</og:title>
<og:description>Carefully curated content to help you ace your next technical interview</og:description>
<dc:language>&quot;en&quot;</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://yangshun.github.io/tech-interview-handbook/</dc:identifier>
</item>
</channel>
</rss>