<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Making our new homepage fast and performant</title>
<link>https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/</link>
<guid isPermaLink="true" >https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/</guid>
<description>&lt;p&gt;This post is the third installment of our five-part series on building GitHub’s new homepage:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://github.blog/2020-12-21-how-we-built-the-github-globe/&quot;&gt;How our globe is built&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.blog/2020-12-21-visualizing-githubs-global-community/&quot;&gt;How we collect and use the data behind the globe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How we made the page fast and performant&lt;/li&gt;
&lt;li&gt;How our illustrators work with designers and engineers&lt;/li&gt;
&lt;li&gt;How we designed the homepage and wrote the narrative/&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Creating a page full of product shots, animations, and videos that still loads fast and performs well can be tricky. Throughout the process of building &lt;a href=&quot;https://github.com/home&quot;&gt;GitHub’s new homepage&lt;/a&gt;, we’ve used the &lt;a href=&quot;https://web.dev/vitals/&quot;&gt;Core Web Vitals&lt;/a&gt; as one of our North Stars and measuring sticks. There are many &lt;a href=&quot;https://web.dev/optimize-lcp/&quot;&gt;different&lt;/a&gt; &lt;a href=&quot;https://web.dev/optimize-fid/&quot;&gt;ways&lt;/a&gt; of &lt;a href=&quot;https://web.dev/optimize-cls/&quot;&gt;optimizing&lt;/a&gt; for these metrics, and we’ve already written about &lt;a href=&quot;https://github.blog/2020-12-21-how-we-built-the-github-globe/#creative-constraints-from-performance-optimizations&quot;&gt;how we optimized our WebGL globe&lt;/a&gt;. We’re going to take a deep-dive here into two of the strategies that produced the overall biggest performance impact for us: crafting high performance animations and serving the perfect image.&lt;/p&gt;
&lt;h2 id=&quot;high-performance-animation-and-interactivity&quot;&gt;High performance animation and interactivity&lt;/h2&gt;
&lt;p&gt;As you scroll down the GitHub homepage, we animate in certain elements to bring your attention to them:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.blog/wp-content/uploads/2021/01/106009518-b6ad6d00-60b8-11eb-980d-061d5f414a37.gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Traditionally, a typical way of building this relied on listening to the scroll event, calculating the visibility of all elements that you’re tracking, and triggering animations depending on the elements’ position in the viewport:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;javascript&quot;&gt;// Old-school scroll event listening (avoid)
window.addEventListener('scroll', () =&amp;gt; checkForVisibility)
window.addEventListener('resize', () =&amp;gt; checkForVisibility)

function checkForVisibility() {
  animatedElements.map(element =&amp;gt; {
    const distTop = element.getBoundingClientRect().top
    const distBottom = element.getBoundingClientRect().bottom
    const distPercentTop = Math.round((distTop / window.innerHeight) * 100)
    const distPercentBottom = Math.round((distBottom / window.innerHeight) * 100)
    // Based on this position, animate element accordingly
  }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There’s at least one big problem with an approach like this: calls to getBoundingClientRect() will &lt;a href=&quot;https://gist.github.com/paulirish/5d52fb081b3570c81e3a&quot;&gt;trigger reflows&lt;/a&gt;, and utilizing this technique might quickly create a performance bottleneck.&lt;/p&gt;
&lt;p&gt;Luckily, &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API&quot;&gt;&lt;code&gt;IntersectionObservers&lt;/code&gt;&lt;/a&gt; are &lt;a href=&quot;https://caniuse.com/intersectionobserver&quot;&gt;supported in all modern browsers&lt;/a&gt;, and they can be set up to notify you of an element’s position in the viewport, without ever listening to scroll events, or without calling getBoundingClientRect.  An IntersectionObserver can be set up in just a few lines of code to track if an element is shown in the viewport, and trigger animations depending on its state, using each entry’s isIntersecting method:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;javascript&quot;&gt;// Create an intersection observer with default options, that 
// triggers a class on/off depending on an element’s visibility 
// in the viewport
const animationObserver = new IntersectionObserver((entries, observer) =&amp;gt; {
  for (const entry of entries) {
    entry.target.classList.toggle('build-in-animate', entry.isIntersecting)
  }
});

// Use that IntersectionObserver to observe the visibility
// of some elements
for (const element of querySelectorAll('.js-build-in')) {
  animationObserver.observe(element);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;avoiding-animation-pollution&quot;&gt;Avoiding animation pollution&lt;/h3&gt;
&lt;p&gt;As we moved over to &lt;code&gt;IntersectionObservers&lt;/code&gt; for our animations, we also went through all of our animations and doubled down on one of the core tenets of optimizing animations: only animate the transform and opacity properties, since these properties are &lt;a href=&quot;https://www.html5rocks.com/en/tutorials/speed/high-performance-animations/&quot;&gt;easier for browsers to animate&lt;/a&gt; (generally computationally less expensive). We thought we did a fairly good job of following this principle already, but we discovered that in some circumstances we did not, because unexpected properties were bleeding into our transitions and polluting them as elements changed state.&lt;/p&gt;
&lt;p&gt;One might think a reasonable implementation of the “only animate transform and opacity” principle might be to define a transition in CSS like so:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;javascript&quot;&gt;// Don’t do this
.animated {
  opacity: 0;
  transform: translateY(10px);
  transition: * 0.6s ease;
}

.animated:hover {
  opacity: 0;
  transform: translateY(0);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In other words, we’re only explicitly changing opacity and transform, but we’re defining the transition to animate all changed properties. These transitions can lead to poor performance since other property changes can pollute the transition (you may have a global style that changes the text color on hover, for example), which can cause unnecessary style and layout calculations. To avoid this kind of animation pollution, we moved to always explicitly defining only opacity and transform as animatable:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;javascript&quot;&gt;// Be explicit about what can animate (and not)
.animated {
  opacity: 0;
  transform: translateY(10px);
  transition: opacity 0.6s ease, transform 0.6s ease;
}

.animated:hover {
  opacity: 0;
  transform: translateY(0);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As we rebuilt all of our animations to be triggered through IntersectionObservers and to explicitly specify only opacity and transform as animatable, we saw a drastic decrease in CPU usage and style recalculations, helping to improve our Cumulative Layout Shift score:&lt;/p&gt;
&lt;p&gt;&lt;img loading=&quot;lazy&quot; class=&quot;aligncenter size-full wp-image-55970&quot; src=&quot;https://github.blog/wp-content/uploads/2021/01/106009495-af865f00-60b8-11eb-84de-b28f3281f5de.png?resize=1024%2C654&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;654&quot; srcset=&quot;https://github.blog/wp-content/uploads/2021/01/106009495-af865f00-60b8-11eb-84de-b28f3281f5de.png?resize=1024%2C654?w=1280 1280w, https://github.blog/wp-content/uploads/2021/01/106009495-af865f00-60b8-11eb-84de-b28f3281f5de.png?resize=1024%2C654?w=300 300w, https://github.blog/wp-content/uploads/2021/01/106009495-af865f00-60b8-11eb-84de-b28f3281f5de.png?resize=1024%2C654?w=768 768w, https://github.blog/wp-content/uploads/2021/01/106009495-af865f00-60b8-11eb-84de-b28f3281f5de.png?resize=1024%2C654?w=1024 1024w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;lazy-loading-videos-with-intersectionobservers&quot;&gt;Lazy-loading videos with &lt;code&gt;IntersectionObservers&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;If you’re powering any animations through video elements, you likely want to do two things: only play the video while it’s visible in the viewport, and lazy-load the video when it’s needed. Sadly, &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/Performance/Lazy_loading#images_and_iframes&quot;&gt;the lazy load attribute&lt;/a&gt; doesn’t work on videos, but if we use &lt;code&gt;IntersectionObservers&lt;/code&gt; to play videos as they appear in the viewport, we can get both of these features in one go:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;javascript&quot;&gt;&amp;lt;!-- HTML: A video that plays inline, muted, w/o autoplay &amp;amp; preload --&amp;gt;
&amp;lt;video loop muted playsinline preload=&quot;none&quot; class=&quot;js-viewport-aware-video&quot; poster=&quot;video-first-frame.jpg&quot;&amp;gt;
  &amp;lt;source type=&quot;video/mp4&quot; src=&quot;video.h264.mp4&quot;&amp;gt;
&amp;lt;/video&amp;gt;

// JS: Play videos while they are visible in the viewport
const videoObserver = new IntersectionObserver((entries, observer) =&amp;gt; {
  for (const entry of entries) entry.isIntersecting ? video.play() : video.pause();
});

for (const element of querySelectorAll('.js-viewport-aware-video')) {
  videoObserver.observe(element);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Together with setting &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element/video&quot;&gt;preload&lt;/a&gt; to none, this simple observer setup saves us several megabytes on each page load.&lt;/p&gt;
&lt;h2 id=&quot;serving-the-perfect-image&quot;&gt;Serving the perfect image&lt;/h2&gt;
&lt;p&gt;We visit web pages with a myriad of different devices, screens and browsers, and something simple as displaying an image is becoming increasingly complex if you want to cover all bases. Our particular illustration style also happens to fall between all of the classic JPG, PNG or SVG formats. Take this illustration, for example, that we use to transition from the main narrative to the footer:&lt;/p&gt;
&lt;p&gt;&lt;img loading=&quot;lazy&quot; class=&quot;aligncenter size-full wp-image-55971&quot; src=&quot;https://github.blog/wp-content/uploads/2021/01/106011272-99799e00-60ba-11eb-8c47-b636c1619353.jpg?resize=1024%2C561&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;561&quot; srcset=&quot;https://github.blog/wp-content/uploads/2021/01/106011272-99799e00-60ba-11eb-8c47-b636c1619353.jpg?resize=1024%2C561?w=1280 1280w, https://github.blog/wp-content/uploads/2021/01/106011272-99799e00-60ba-11eb-8c47-b636c1619353.jpg?resize=1024%2C561?w=300 300w, https://github.blog/wp-content/uploads/2021/01/106011272-99799e00-60ba-11eb-8c47-b636c1619353.jpg?resize=1024%2C561?w=768 768w, https://github.blog/wp-content/uploads/2021/01/106011272-99799e00-60ba-11eb-8c47-b636c1619353.jpg?resize=1024%2C561?w=1024 1024w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/p&gt;
&lt;p&gt;To render this illustration, we would ideally need the transparency from PNGs but combine it with the compression from JPGs, as saving an illustration like this as a PNG would weigh in at several megabytes. Luckily, &lt;a href=&quot;https://developers.google.com/speed/webp&quot;&gt;WebP&lt;/a&gt; is, as of iOS 14 and macOS Big Sur, supported in Safari on both desktops and phones, which brings &lt;a href=&quot;https://caniuse.com/webp&quot;&gt;browser support up to a solid +90%&lt;/a&gt;. WebP does in fact give us the best of both worlds: we can create compressed, lossy images with transparency. What about support for older browsers? Even a new Mac running the latest version of Safari on macOS Catalina can’t render WebP images, so we have to do &lt;em&gt;something&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This challenge eventually led us to develop a somewhat obscure solution: two JPGs embedded in an SVG (one for the image data and one for the mask), embedded as base64 data—essentially creating a transparent JPG with one single HTTP request. Take a look at &lt;a href=&quot;https://github.githubassets.com/images/modules/site/home/footer-illustration.svg&quot;&gt;this image&lt;/a&gt;. Download it, open it up, and inspect it. Yes, it’s a JPG with transparency, encoded in base64, wrapped in an SVG.&lt;/p&gt;
&lt;p&gt;Part of the SVG specification is the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/SVG/Element/mask&quot;&gt;mask element&lt;/a&gt;. With it, you can mask out parts of an SVG. If we embed an SVG in a document, we can use the mask element in tandem with the image element to render an image with transparency:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;html&quot;&gt;&amp;lt;svg viewBox=&quot;0 0 300 300&quot;&amp;gt;
  &amp;lt;defs&amp;gt;
    &amp;lt;mask id=&quot;mask&quot;&amp;gt;
      &amp;lt;image width=&quot;300&quot; height=&quot;300&quot; href=&quot;mask.jpg&quot;&amp;gt;&amp;lt;/image&amp;gt;
    &amp;lt;/mask&amp;gt;
  &amp;lt;/defs&amp;gt;
  &amp;lt;image mask=&quot;url(#mask)&quot; width=&quot;300&quot; height=&quot;300&quot; href=&quot;image.jpg&quot;&amp;gt;&amp;lt;/image&amp;gt;
&amp;lt;/svg&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is great, but it won’t work as a fallback for WebP. Since the paths for these images are dynamic (see href in the example above), the SVG needs to be embedded inside the document. If we instead save this SVG in a file and set it as the src of a regular img, the images won’t be loaded, and we’ll see nothing.&lt;/p&gt;
&lt;p&gt;We can work around this limitation by embedding the image data inside the SVG as &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Glossary/Base64&quot;&gt;base64&lt;/a&gt;. There are services online where you can convert an image to base64, but if you’re on a Mac, base64 is available by default in your Terminal, and you can use it like so:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;base64 -i &amp;lt;in-file&amp;gt; -o &amp;lt;outfile&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Where the in-file is your image of choice, the outfile is a text file where you’ll save the base64 data. With this technique, we can embed the images inside of the SVG and use the SVG as a src on a regular image.&lt;/p&gt;
&lt;p&gt;These are the two images that we’re using to construct the footer illustration—one for the image data and one for the mask (black is completely transparent and white is fully opaque):&lt;/p&gt;
&lt;p&gt;&lt;img loading=&quot;lazy&quot; class=&quot;aligncenter size-full wp-image-55972&quot; src=&quot;https://github.blog/wp-content/uploads/2021/01/106009473-ab5a4180-60b8-11eb-8da9-dd432a5c832b.jpg?resize=1024%2C606&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;606&quot; srcset=&quot;https://github.blog/wp-content/uploads/2021/01/106009473-ab5a4180-60b8-11eb-8da9-dd432a5c832b.jpg?resize=1024%2C606?w=1280 1280w, https://github.blog/wp-content/uploads/2021/01/106009473-ab5a4180-60b8-11eb-8da9-dd432a5c832b.jpg?resize=1024%2C606?w=300 300w, https://github.blog/wp-content/uploads/2021/01/106009473-ab5a4180-60b8-11eb-8da9-dd432a5c832b.jpg?resize=1024%2C606?w=768 768w, https://github.blog/wp-content/uploads/2021/01/106009473-ab5a4180-60b8-11eb-8da9-dd432a5c832b.jpg?resize=1024%2C606?w=1024 1024w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/p&gt;
&lt;p&gt;We convert the mask and the image to base64 using the Terminal command and then paste the data into the SVG:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;plain&quot;&gt;&amp;lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; viewBox=&quot;0 0 2900 1494&quot;&amp;gt;
  &amp;lt;defs&amp;gt;
    &amp;lt;mask id=&quot;mask&quot;&amp;gt;
      &amp;lt;image width=&quot;300&quot; height=&quot;300&quot; href=&quot;data:image/png;base64,/* your image in base64 */”&amp;gt;&amp;lt;/image&amp;gt;
    &amp;lt;/mask&amp;gt;
  &amp;lt;/defs&amp;gt;
  &amp;lt;image mask=&quot;url(#mask)&quot; width=&quot;300&quot; height=&quot;300&quot; href=&quot;data:image/jpeg;base64,/* your image in base64 */”&amp;gt;&amp;lt;/image&amp;gt;
&amp;lt;/svg&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You can save that SVG and use it like any regular image. We can then safely use WebP with lazy loading and a solid fallback that works in all browsers:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;html&quot;&gt;&amp;lt;picture&amp;gt;
  &amp;lt;source srcset=&quot;compressed-transparent-image.webp&quot; type=&quot;image/webp&quot;&amp;gt;
  &amp;lt;img src=&quot;compressed-transparent-image.svg&quot; loading=&quot;lazy&quot;&amp;gt;
&amp;lt;/picture&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This somewhat obscure SVG hack saves us hundreds of kilobytes on each page load, and it enables us to utilize the latest technologies for the browsers and operating systems that support them.&lt;/p&gt;
&lt;h2 id=&quot;towards-a-faster-web&quot;&gt;Towards a faster web&lt;/h2&gt;
&lt;p&gt;We’re working throughout the company to create a faster and more reliable GitHub, and these are some of the techniques that we’re utilizing. We still have a long way to go, and if you’d like to be part of that journey, &lt;a href=&quot;https://github.com/about/careers&quot;&gt;check out our careers page&lt;/a&gt;.&lt;/p&gt;


</description>
<pubDate>Fri, 29 Jan 2021 17:11:14 +0000</pubDate>
<dc:creator>todsacerdoti</dc:creator>
<og:type>article</og:type>
<og:title>Making GitHub’s new homepage fast and performant - The GitHub Blog</og:title>
<og:description>This post is the third installment of our five-part series on building GitHub’s new homepage: How our globe is built How we collect and use the data behind the globe How we made the page</og:description>
<og:url>https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/</og:url>
<og:image>https://github.blog/wp-content/uploads/2021/01/102393310-07478b80-3f8d-11eb-84eb-392d555ebd29.png?fit=1200%2C630</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/</dc:identifier>
</item>
<item>
<title>Robinhood denies claims that it sold GameStop shares out from under its traders</title>
<link>https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</link>
<guid isPermaLink="true" >https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</guid>
<description>&lt;p id=&quot;HrSPad&quot;&gt;No, Robinhood tells &lt;em&gt;The Verge&lt;/em&gt;, it didn’t sell off full shares of &lt;a href=&quot;https://www.theverge.com/22251427/reddit-gamestop-stock-short-wallstreetbets-robinhood-wall-street&quot;&gt;GameStop, AMC, and other buzzy stocks&lt;/a&gt; without permission from its traders.&lt;/p&gt;
&lt;p id=&quot;f9qz0P&quot;&gt;That contradicts the stories of twelve people who spoke with &lt;em&gt;The Verge,&lt;/em&gt; saying that the app unexpectedly sold off their holdings in some of these companies. Quite a number of Robinhood users expressed their surprise on social media today that the app was selling off their stakes, and we tracked down a dozen of them. These traders didn’t believe they had prompted the sales, and they said they weren’t aware of anything on their account that would have automatically triggered them.&lt;/p&gt;
&lt;p id=&quot;PZTzgT&quot;&gt;“I didn’t have any triggers to sell the stock whether it went up or down. I certainly wouldn’t have put it at $197 when it had just been almost $500,” Jett Flores, who said he was holding stock in GameStop and AMC through Robinhood, told &lt;em&gt;The Verge&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;
&lt;aside id=&quot;oPcjb5&quot;&gt;&lt;q&gt;The small traders planned to hold onto their stock, rather than sell&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id=&quot;IoLdvA&quot;&gt;A spokesperson for Robinhood said these small sellers are wrong about how their shares were sold. “I can confirm that claims that Robinhood proactively sold customers’ shares outside of our standard margin-related sellouts or options assignment procedures are false,” the spokesperson told &lt;em&gt;The Verge&lt;/em&gt;.&lt;/p&gt;
&lt;p id=&quot;laWYcl&quot;&gt;On Wednesday, Robinhood warned some investors with options in GameStop and AMC that it may automatically sell off their stakes to reduce risk, the spokesperson said. But these investors told &lt;em&gt;The Verge&lt;/em&gt; they didn’t have options in GameStop or AMC and hadn’t purchased the stocks on margin. They had purchased the shares outright, they said, and were planning to hold onto them.&lt;/p&gt;
&lt;p id=&quot;kr6KcG&quot;&gt;Margin orders occur when an investor borrows money from the broker (in this case Robinhood) to complete a sale, and brokers can call in those shares if they’re worried the investor can’t pay up. According to Robinhood, most of its actions have been calling in options to purchase shares — a more aggressive move, but not unprecedented. But if users fully owned their shares, as these traders claim they did, selling the holdings would be far more unusual.&lt;/p&gt;
&lt;p id=&quot;x0bJ13&quot;&gt;&lt;em&gt;The Verge&lt;/em&gt; saw screenshots from six traders indicating that their purchase of GameStop or AMC stock had been filled within Robinhood. Six traders sent screenshots showing that their stock in these companies had been sold, with four clearly indicating that they had been sold today. Another trader sent screenshots showing a purchase of Naked Brand stock being filled and then sold within the app. The screenshots don’t indicate how the purchases were funded or how the sales were initiated, but in several of them the app displays a message saying, “We’ve received your order to sell [#] shares of [stock] at the best available price.”&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;
&lt;aside id=&quot;qykRDd&quot;&gt;&lt;q&gt;“I certainly wouldn’t have put it at $197 when it had just been almost $500.”&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id=&quot;s1AHtZ&quot;&gt;Traders who spoke with &lt;em&gt;The Verge&lt;/em&gt; said they were disappointed to lose their stake in these companies. The traders had been planning to keep the stock for longer, and several said they certainly wouldn’t have sold it at the point that they believe Robinhood pushed through the transaction, as GameStop’s stock was faltering from a nearly $500 high.&lt;/p&gt;
&lt;p id=&quot;0Mp7Fl&quot;&gt;“It’s extremely dishonest trade on their part and unacceptable,” Ian Q., who said Robinhood unexpectedly sold his shares in GameStop this morning, told &lt;em&gt;The Verge&lt;/em&gt;.&lt;/p&gt;
&lt;p id=&quot;x2J8hJ&quot;&gt;The surprise selloff isn’t happening to everyone — plenty of people on the r/WallStreetBets subreddit (and one person I know, who told me they purchased GameStop stock days ago) say they weren’t impacted. And though traders may be outraged by the surprise, Robinhood’s terms of service grant it permission to close a trader’s position under a number of circumstances.&lt;/p&gt;
&lt;p id=&quot;sEn2Ij&quot;&gt;While r/WallStreetBets has been at the center of the frenzy around GameStop and AMC stocks, Robinhood has been the tool of choice for many of the small-time and brand new traders jumping in to take part. But this morning, &lt;a href=&quot;https://www.theverge.com/2021/1/28/22254102/robinhood-gamestop-bloc-stock-purchase-amc-reddit-wsb&quot;&gt;Robinhood blocked new purchases&lt;/a&gt; of stock from GameStop, AMC, BlackBerry, Nokia, and others that were spiking in large part because of purchases coming through the app. The company is &lt;a href=&quot;https://www.theverge.com/2021/1/28/22254168/robinhood-gamestop-ban-amc-nokia-public-webull-ameritrade-stock-buy-trade&quot;&gt;now facing widespread backlash&lt;/a&gt; from users, celebrities, and &lt;a href=&quot;https://www.theverge.com/2021/1/28/22254909/senate-banking-stocks-sherrod-brown-aoc-gamestop-reddit-hearing&quot;&gt;politicians&lt;/a&gt;, and it’s announced plans to &lt;a href=&quot;https://www.theverge.com/2021/1/28/22255031/robinhood-gamestop-stop-purchases-reddit-wsb-stock-market&quot;&gt;re-open purchases on a “limited” basis&lt;/a&gt; on Friday.&lt;/p&gt;
&lt;p id=&quot;3YKLEt&quot;&gt;It’s still not clear what happened to cause these users’ stakes to be sold off today. But at the very least, it means Robinhood has even more unhappy customers.&lt;/p&gt;
</description>
<pubDate>Fri, 29 Jan 2021 15:59:38 +0000</pubDate>
<dc:creator>Alupis</dc:creator>
<og:description>Users say they didn’t trigger the transactions</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/hgc6ZugqBqCQcY9TlxnxHcYsyVM=/0x622:5472x3487/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22267038/1291817081.jpg</og:image>
<og:title>Robinhood denies claims that it sold GameStop shares out from under its traders</og:title>
<og:type>article</og:type>
<og:url>https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</dc:identifier>
</item>
<item>
<title>Die Shots of the Raspberry Pi RP2040</title>
<link>https://twitter.com/johndmcmaster/status/1355092011829719046</link>
<guid isPermaLink="true" >https://twitter.com/johndmcmaster/status/1355092011829719046</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://twitter.com/johndmcmaster/status/1355092011829719046&quot;&gt;https://twitter.com/johndmcmaster/status/1355092011829719046&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=25958138&quot;&gt;https://news.ycombinator.com/item?id=25958138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 224&lt;/p&gt;
&lt;p&gt;# Comments: 64&lt;/p&gt;
</description>
<pubDate>Fri, 29 Jan 2021 15:23:41 +0000</pubDate>
<dc:creator>mmastrac</dc:creator>
<og:description>Users say they didn’t trigger the transactions</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/hgc6ZugqBqCQcY9TlxnxHcYsyVM=/0x622:5472x3487/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22267038/1291817081.jpg</og:image>
<og:title>Robinhood denies claims that it sold GameStop shares out from under its traders</og:title>
<og:type>article</og:type>
<og:url>https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theverge.com/2021/1/28/22254857/robinhood-gamestop-amc-shares-sold-surprised-users</dc:identifier>
</item>
<item>
<title>u/DeepFuckingValue and the GameStop Reddit mania</title>
<link>https://www.wsj.com/articles/keith-gill-drove-the-gamestop-reddit-mania-he-talked-to-the-journal-11611931696</link>
<guid isPermaLink="true" >https://www.wsj.com/articles/keith-gill-drove-the-gamestop-reddit-mania-he-talked-to-the-journal-11611931696</guid>
<description>&lt;p&gt;BOSTON—The investor who helped &lt;a href=&quot;https://www.wsj.com/articles/gamestop-mania-reveals-power-shift-on-wall-streetand-the-pros-are-reeling-11611774663?mod=article_inline&quot; target=&quot;_blank&quot; class=&quot;icon none&quot;&gt;direct the world’s attention to GameStop&lt;/a&gt;, leading a horde of online followers in a bizarre market rally that made and lost fortunes from one day to the next, says he’s just a normal guy.&lt;/p&gt;&lt;p&gt;“I didn’t expect this,” said Keith Gill, 34 years old, known as “DeepF—ingValue” by fans on &lt;a href=&quot;https://www.wsj.com/articles/tiktok-and-discord-are-the-new-wall-street-trading-desks-11610361004?mod=searchresults_pos5&amp;amp;page=1&amp;amp;mod=article_inline&quot; target=&quot;_blank&quot; class=&quot;icon none&quot;&gt;Reddit’s WallStreetBets forum&lt;/a&gt; and “Dada” by his 2-year-old daughter. He said he didn’t set out to draw the attention of Congress, the Federal Reserve, hedge funds, the media, trading platforms and hundreds of thousands of investors.&lt;/p&gt;&lt;div readability=&quot;256.67354228056&quot;&gt;
&lt;p&gt;“This story is so much bigger than me,” Mr. Gill told The Wall Street Journal in his first interview since the &lt;a href=&quot;https://www.wsj.com/articles/gamestop-stock-short-squeeze-ugly-side-11611750250?mod=article_inline&quot; target=&quot;_blank&quot; class=&quot;icon none&quot;&gt;unboxing this week of a volatile new stock-market game.&lt;/a&gt; “I support these retail investors, their ability to make a statement.”&lt;/p&gt;
&lt;p&gt;To many of them, Mr. Gill—who until recently worked in marketing for Massachusetts Mutual Life Insurance Co.—is the force behind the quadruple-digit gains in shares of the videogame retailer GameStop, up more than 1600% this year through Friday. On Wednesday, the stock jumped 135% to $347.51, a record, before plunging to $194 a share Thursday and then sharply rebounding to end the week. At the start of the year, GameStop shares went for around $18.&lt;/p&gt;
&lt;p&gt;Many online investors say his advocacy helped turn them into a force powerful enough to cause big losses for established hedge funds and, for the moment, turn the investing world upside down.&lt;/p&gt;
&lt;p&gt;Mr. Gill posted a screenshot of his brokerage account Wednesday, showing a roughly $20 million daily gain on GameStop shares and options. “Your steady hand convinced many of us to not only buy, but hold. Your example has literally changed the lives of thousands of ordinary normal people. Seriously thank you. You deserve every penny,” replied one Reddit user, reality_czech.&lt;/p&gt;
&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetMediaIllustration inline scope-web|mobileapps article__inset article__inset--type-InsetMediaIllustration article__inset--inline&quot;&gt;
&lt;div data-mobile-ratio=&quot;56.25%&quot; data-layout-ratio=&quot;56.25%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container responsive-media article__inset__image__image&quot;&gt;&lt;img srcset=&quot;https://images.wsj.net/im-292197?width=140&amp;amp;size=custom_2560x1440 140w, https://images.wsj.net/im-292197?width=540&amp;amp;size=custom_2560x1440 540w, https://images.wsj.net/im-292197?width=620&amp;amp;size=custom_2560x1440 620w, https://images.wsj.net/im-292197?width=700&amp;amp;size=custom_2560x1440 700w, https://images.wsj.net/im-292197?width=860&amp;amp;size=custom_2560x1440 860w, https://images.wsj.net/im-292197?width=1260&amp;amp;size=custom_2560x1440 1260w&quot; sizes=&quot;(max-width: 140px) 100px, (max-width: 540px) 500px, (max-width: 620px) 580px, (max-width: 700px) 660px, (max-width: 860px) 820px, 1260px&quot; src=&quot;https://images.wsj.net/im-292197?width=620&amp;amp;size=custom_2560x1440&quot; data-enlarge=&quot;https://images.wsj.net/im-292197?width=1260&amp;amp;size=custom_2560x1440&quot; alt=&quot;&quot; title=&quot;Online traders credit Mr. Gill with helping power the GameStop frenzy this week.&quot;/&gt;&lt;/div&gt;
&lt;h4 class=&quot;wsj-article-caption-content&quot;&gt;Online traders credit Mr. Gill with helping power the GameStop frenzy this week.&lt;/h4&gt;
&lt;/div&gt;
&lt;p&gt;The next day, Mr. Gill posted another screenshot—showing about a $15 million loss. After Thursday’s market close, his E*Trade brokerage account, viewed by the Journal, held around $33 million, including GameStop stock, options and millions in cash.&lt;/p&gt;
&lt;p&gt;“He always liked money,” said Elaine Gill, his mother. As a child, she said, “he would get money from those scratch tickets that people didn’t know they’d won. People would throw them on the ground….A lot of times there was still money on them.”&lt;/p&gt;
&lt;p&gt;Mr. Gill’s online persona—he goes by “Roaring Kitty” on YouTube—has drawn tens of thousands of fans and copycats who share screenshots of their own brokerage accounts. As the GameStop frenzy peaked this week, hundreds of thousands of new investors downloaded applications like Robinhood to join the action, according to Apptopia Inc.&lt;/p&gt;
&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetMediaVideo inline scope-web|mobileapps article__inset article__inset--type-InsetMediaVideo article__inset--inline&quot; readability=&quot;10&quot;&gt;


Wall Street is in an uproar over GameStop shares this week, after members of Reddit’s popular WallStreetBets forum encouraged bets on the video game retailer. WSJ explains how options trading is driving the action and what’s at stake.&lt;/div&gt;
&lt;p&gt;Mr. Gill said he wasn’t a rabble-rouser out to take on the establishment, just someone who believes investors can find value in unloved stocks. He never expected to have a legion of fans debating his identity online, or millions of dollars in his trading account, he said. He was just a dad with an online hobby and a plastic kiddie slide on the front lawn of a Boston suburb.&lt;/p&gt;
&lt;p&gt;Mr. Gill began investing in GameStop around June 2019, he said, when it was hovering around $5 a share. Earlier that year, the game retailer was &lt;a href=&quot;https://www.wsj.com/articles/as-videogame-market-shifts-gamestop-struggles-to-boost-sales-11546561467?ru=yahoo?mod=yahoo_itp&amp;amp;yptr=yahoo&amp;amp;mod=article_inline&quot; target=&quot;_blank&quot; class=&quot;icon none&quot;&gt;hunting for its fifth chief executive&lt;/a&gt; in a little over 12 months. Mr. Gill kept buying. Although he never played much besides Super Mario or Donkey Kong, he saw potential for the struggling retailer to reinvigorate itself by attracting new customers with the latest videogame consoles.&lt;/p&gt;


&lt;p&gt;“People were doing a quick take, saying GameStop was the next &lt;a href=&quot;https://www.wsj.com/market-data/quotes/BLIBQ&quot;&gt;Blockbuster&lt;/a&gt;&lt;span class=&quot;company-name-type&quot;&gt;,&lt;/span&gt; ” he said, a chain caught in a retail decline. “It appeared many folks just weren’t digging in deeper. It was a gross misclassification of the opportunity.”&lt;/p&gt;
&lt;p&gt;Mr. Gill, tall with shoulder-length hair, opened a YouTube channel last summer, and he worked in the basement of the home he rents in Wilmington, Mass., to avoid disturbing his daughter after bedtime, he said. On his channel, he touted GameStop and Belgian beers. His favorite is Delirium Tremens.&lt;/p&gt;
&lt;p&gt;On a recent YouTube live-stream, he wore a red headband and aviator sunglasses while fielding questions on stocks. He poured himself Prosecco then switched to beer as he celebrated big gains and gave shout-outs to legions of viewers and traders in a seven-hour-plus extravaganza. The stream has tallied more than 200,000 views.&lt;/p&gt;
&lt;p&gt;Mr. Gill’s obscene username on Reddit’s WallStreetBets forum is supposed to reflect a belief in value investing—buying shares of companies that are inexpensive relative to the underlying business.&lt;/p&gt;
&lt;p&gt;Among his many Reddit fans, Mr. Gill “will go down as the greatest legend in the history of WallStreetBets,” said Jon Hagedorn, a 34-year-old training supervisor based in Ronkonkoma, N.Y. “He’s the original OG.”&lt;/p&gt;
&lt;p&gt;The stock’s wild ride, seemingly divorced from standard measures of corporate value, has spurred complaints that investors banding together to provoke this kind of frenzy amounts to market manipulation.&lt;/p&gt;
&lt;p&gt;The Securities and Exchange Commission said Friday it would “act to protect retail investors when the facts demonstrate abusive or manipulative trading activity.” Mr. Gill said he hasn’t heard from the SEC.&lt;/p&gt;
&lt;h6&gt;Fast times&lt;/h6&gt;
&lt;p&gt;“The first thing that I had asked him when this craziness started was: is this illegal or anything dishonest? He said, ‘No mom, it’s not,’ ” recalled Ms. Gill, who lives in Brockton, Mass., where she and Steve Gill raised their son.&lt;/p&gt;
&lt;p&gt;In high school, Mr. Gill was a distance runner, and he earned national honors on the team at nearby Stonehill College, where he graduated in 2009 with an accounting major. He ran a four-minute mile until sidelined by an Achilles injury.&lt;/p&gt;
&lt;p&gt;Mr. Gill moved to New Hampshire for a few years and found a mentor, an investor and software developer his aunt introduced him to&lt;strong&gt;.&lt;/strong&gt; He holds a designation as a Chartered Financial Analyst and said he was drawn by the complexity and challenge of stock picking, which became an outlet for the energy he once put into running. He started working at MassMutual in 2019.&lt;/p&gt;

&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetMediaIllustration inline scope-mobileapps article__inset article__inset--type-InsetMediaIllustration article__inset--inline&quot;&gt;
&lt;div data-mobile-ratio=&quot;100%&quot; data-layout-ratio=&quot;100%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container responsive-media article__inset__image__image&quot;&gt;&lt;img srcset=&quot;https://images.wsj.net/im-292297?width=140&amp;amp;size=1 140w, https://images.wsj.net/im-292297?width=540&amp;amp;size=1 540w, https://images.wsj.net/im-292297?width=620&amp;amp;size=1 620w, https://images.wsj.net/im-292297?width=700&amp;amp;size=1 700w, https://images.wsj.net/im-292297?width=860&amp;amp;size=1 860w, https://images.wsj.net/im-292297?width=1260&amp;amp;size=1 1260w&quot; sizes=&quot;(max-width: 140px) 100px, (max-width: 540px) 500px, (max-width: 620px) 580px, (max-width: 700px) 660px, (max-width: 860px) 820px, 1260px&quot; src=&quot;https://images.wsj.net/im-292297?width=620&amp;amp;size=1&quot; data-enlarge=&quot;https://images.wsj.net/im-292297?width=1260&amp;amp;size=1&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetMediaIllustration inline scope-mobileapps article__inset article__inset--type-InsetMediaIllustration article__inset--inline&quot;&gt;
&lt;div data-mobile-ratio=&quot;100%&quot; data-layout-ratio=&quot;100%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container responsive-media article__inset__image__image&quot;&gt;&lt;img srcset=&quot;https://images.wsj.net/im-292296?width=140&amp;amp;size=1 140w, https://images.wsj.net/im-292296?width=540&amp;amp;size=1 540w, https://images.wsj.net/im-292296?width=620&amp;amp;size=1 620w, https://images.wsj.net/im-292296?width=700&amp;amp;size=1 700w, https://images.wsj.net/im-292296?width=860&amp;amp;size=1 860w, https://images.wsj.net/im-292296?width=1260&amp;amp;size=1 1260w&quot; sizes=&quot;(max-width: 140px) 100px, (max-width: 540px) 500px, (max-width: 620px) 580px, (max-width: 700px) 660px, (max-width: 860px) 820px, 1260px&quot; src=&quot;https://images.wsj.net/im-292296?width=620&amp;amp;size=1&quot; data-enlarge=&quot;https://images.wsj.net/im-292296?width=1260&amp;amp;size=1&quot; alt=&quot;&quot; title=&quot;A notebook with notes on stocks and an ‘8 Ball’ toy that Keith Gill uses...&quot;/&gt;&lt;/div&gt;
&lt;h4 class=&quot;wsj-article-caption-content&quot;&gt;A notebook with notes on stocks and an ‘8 Ball’ toy that Keith Gill uses in his video streams. At the end of his YouTube stream, Mr. Gill and his viewers ask questions of the ‘8 Ball’ about how a stock will perform.&lt;/h4&gt;
&lt;/div&gt;
&lt;p&gt;In the summer of 2019, he started building his position in GameStop and would post screenshots of his E*Trade account’s options positions on WallStreetBets forum. “Holy s— bro, what made you drop 53K on GameStop?” one trader posted about one of Mr. Gill’s screenshots in September 2019.&lt;/p&gt;
&lt;p&gt;In the months that followed, he posted regularly, putting up a “GME YOLO update,” a reference to GameStop’s ticker and the mantra “you only live once.” He showed off gains in the five- and six-digits, and times when his investments plunged.&lt;/p&gt;
&lt;p&gt;Mr. Gill stuck with GameStop, and his wagers became day-trader lore.&lt;/p&gt;
&lt;p&gt;To fans, he tapped into the desire by millions of amateur investors around the U.S. to try their hand at stock trading. Trading fees have fallen to zero, and apps allow investors to buy and sell on their phones. The easy market access is augmented by an online community swelled with eager helpers.&lt;/p&gt;
&lt;div data-layout=&quot;wrap&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetRichText wrap scope-web article__inset article__inset--type-InsetRichText article__inset--wrap&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;media-object-rich-text&quot; readability=&quot;7&quot;&gt;
&lt;h4&gt;Share Your Thoughts&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Did you join in the trading frenzy this week?&lt;/em&gt;Join the conversation below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Many first-time investors stuck at home in the pandemic said they found solace in chatting with others online about trading stocks or options, as well as hearing from those making profitable bets.&lt;/p&gt;
&lt;p&gt;The discourse isn’t always positive. An off-Reddit chat room associated with WallStreetBets is filled with obscenity, racism and antigay screeds. Many on the platforms lash out against Wall Street power players, and some express a desire to see the financial pros reel from losses.&lt;/p&gt;
&lt;p&gt;“I’m not out for anybody,” Mr. Gill said. “Roaring Kitty was an educational channel where I was showcasing my investment philosophy.”&lt;/p&gt;
&lt;h6&gt;Bear bust&lt;/h6&gt;
&lt;p&gt;Many on Wall Street disagreed with Mr. Gill’s bullish view on GameStop and have taken a big hit as a result. Hedge funds and other investment professionals piled into wagers that the shares would tumble.&lt;/p&gt;

&lt;p&gt;To bet against a stock, hedge funds borrow shares and sell them, hoping to buy them back later at a lower price and return them. That allows them to pocket the difference between the prices. But when a shorted stock stages such a dramatic rally, it turns painful, often forcing them to exit from the positions by purchasing shares at a loss. In turn, that can inspire sharp gains in stocks, known as a “short squeeze.”&lt;/p&gt;
&lt;p&gt;The bearish positioning of hedge funds was part of what drew many small GameStop investors, anticipating a short squeeze. Mr. Gill said his investing strategy didn’t entirely depend on a short squeeze, but he knew others were potentially betting on it.&lt;/p&gt;
&lt;p&gt;So far, the professionals have been wrong, giving a win to Mr. Gill and other individual investors who bet big on GameStop. Hedge funds like Melvin Capital Management and Maplelane Capital were the ones burned, as well as jeered by boastful Reddit investors.&lt;/p&gt;
&lt;p&gt;Many others have piled into GameStop, trying to ride the rally “to the moon,” as many Reddit investors say. Individual investors have also piled into shares of companies like &lt;a href=&quot;https://www.wsj.com/market-data/quotes/AMC&quot;&gt;AMC Entertainment Holdings&lt;/a&gt; &lt;span class=&quot;company-name-type&quot;&gt;Inc.&lt;/span&gt; in the hopes of catching similar momentum and making a quick buck.&lt;/p&gt;
&lt;p&gt;GameStop has garnered hundreds of thousands of posts over the past month across Reddit, &lt;a href=&quot;https://www.wsj.com/market-data/quotes/TWTR&quot;&gt;Twitter&lt;/a&gt; and &lt;a href=&quot;https://www.wsj.com/market-data/quotes/FB&quot;&gt;Facebook&lt;/a&gt;&lt;span class=&quot;company-name-type&quot;&gt;,&lt;/span&gt; according to data this week from &lt;a href=&quot;https://www.wsj.com/market-data/quotes/NO/XOSL/MWTR&quot;&gt;Meltwater&lt;/a&gt;&lt;span class=&quot;company-name-type&quot;&gt;,&lt;/span&gt; a global media intelligence company. As the stock has vaulted higher, its shares have traded in a frenzy, making it one of the most popular bets in the U.S. market in recent days, according to Dow Jones Market Data.&lt;/p&gt;
&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetMediaIllustration inline scope-web|mobileapps article__inset article__inset--type-InsetMediaIllustration article__inset--inline&quot;&gt;
&lt;div data-mobile-ratio=&quot;66.66666666666666%&quot; data-layout-ratio=&quot;66.66666666666666%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container responsive-media article__inset__image__image&quot;&gt;&lt;img srcset=&quot;https://images.wsj.net/im-292294?width=140&amp;amp;size=1.5 140w, https://images.wsj.net/im-292294?width=540&amp;amp;size=1.5 540w, https://images.wsj.net/im-292294?width=620&amp;amp;size=1.5 620w, https://images.wsj.net/im-292294?width=700&amp;amp;size=1.5 700w, https://images.wsj.net/im-292294?width=860&amp;amp;size=1.5 860w, https://images.wsj.net/im-292294?width=1260&amp;amp;size=1.5 1260w&quot; sizes=&quot;(max-width: 140px) 100px, (max-width: 540px) 500px, (max-width: 620px) 580px, (max-width: 700px) 660px, (max-width: 860px) 820px, 1260px&quot; src=&quot;https://images.wsj.net/im-292294?width=620&amp;amp;size=1.5&quot; data-enlarge=&quot;https://images.wsj.net/im-292294?width=1260&amp;amp;size=1.5&quot; alt=&quot;&quot; title=&quot;Keith Gill at his basement work station in Wilmington, Mass. &quot;/&gt;&lt;/div&gt;
&lt;h4 class=&quot;wsj-article-caption-content&quot;&gt;Keith Gill at his basement work station in Wilmington, Mass.&lt;/h4&gt;
&lt;/div&gt;
&lt;p&gt;Seasoned traders are starting to take into account the behavior of influential investors like Mr. Gill and others.&lt;/p&gt;
&lt;p&gt;Mark Sebastian, founder of Chicago-based Option Pit and an options trader for around 20 years, has developed a screener analyzing reams of stocks to spot those with heavy activity from individual investors. He buys or sells options based on which stocks are gaining momentum, trying to ride the wave higher or lower. Recently, this included AMC, though he said he wasn’t a fan.&lt;/p&gt;
&lt;p&gt;“We’re trying to get on these names before they completely take off,” Mr. Sebastian said, calling one recent trade “free money.”&lt;/p&gt;
&lt;p&gt;Mr. Gill said his life has changed overnight and hasn’t set his future plans. He would like to continue the “Roaring Kitty” YouTube channel, maybe buy a house. “I thought this trade would be successful,” he said, “but I never expected what happened over the past week.”&lt;/p&gt;
&lt;p&gt;He has one dream in mind. “I always wanted to build an indoor track facility or a field house in Brockton,” he said of his hometown. “And now, it looks like I actually could do that.”&lt;/p&gt;
&lt;p class=&quot;articleTagLine&quot;&gt;—Elisa Cho, Jim Oberman and Caitlin McCabe contributed to this article.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Write to&lt;/strong&gt; Julia-Ambra Verlaine at &lt;a href=&quot;mailto:Julia.Verlaine@wsj.com&quot; target=&quot;_blank&quot; class=&quot;icon&quot;&gt;Julia.Verlaine@wsj.com&lt;/a&gt; and Gunjan Banerji at &lt;a href=&quot;mailto:Gunjan.Banerji@wsj.com&quot; target=&quot;_blank&quot; class=&quot;icon&quot;&gt;Gunjan.Banerji@wsj.com&lt;/a&gt;&lt;/p&gt;
&lt;div data-layout=&quot;inline&quot; data-layout-mobile=&quot;&quot; class=&quot; media-object type-InsetDynamic inline scope-web|mobileapps article__inset article__inset--type-InsetDynamic article__inset--inline&quot;&gt;
&lt;div class=&quot;dynamic-inset-container article__inset__dynamic&quot;&gt;
&lt;div id=&quot;series-nav-MBhBQInr&quot; class=&quot;sc-AxmLO gmtmqV series-nav__inset-container&quot;&gt;
&lt;div class=&quot;series-nav__inset-container-inner&quot;&gt;
&lt;p&gt;
&lt;h3 class=&quot;sc-AxjAm bcMPWx&quot;&gt;Retail Trading Mania&lt;/h3&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Copyright ©2020 Dow Jones &amp;amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8&lt;/p&gt;</description>
<pubDate>Fri, 29 Jan 2021 15:05:28 +0000</pubDate>
<dc:creator>gangwolf</dc:creator>
<og:title>WSJ News Exclusive | Keith Gill Drove the GameStop Reddit Mania. He Talked to the Journal.</og:title>
<og:description>The trader known as DeepF—ingValue on the WallStreetBets forum and “Roaring Kitty” on YouTube helped turn the investing world upside down. “I didn’t expect this.”</og:description>
<og:url>https://www.wsj.com/articles/keith-gill-drove-the-gamestop-reddit-mania-he-talked-to-the-journal-11611931696</og:url>
<og:image>https://images.wsj.net/im-292292/social</og:image>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wsj.com/articles/keith-gill-drove-the-gamestop-reddit-mania-he-talked-to-the-journal-11611931696</dc:identifier>
</item>
<item>
<title>Statement of SEC Regarding Recent Market Volatility</title>
<link>https://www.sec.gov/news/public-statement/joint-statement-market-volatility-2021-01-29</link>
<guid isPermaLink="true" >https://www.sec.gov/news/public-statement/joint-statement-market-volatility-2021-01-29</guid>
<description>&lt;div readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;field_speaker_name_and_title&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;Acting Chair Allison Herren Lee&lt;br/&gt;Commissioner Hester M. Peirce&lt;br/&gt;Commissioner Elad L. Roisman&lt;br/&gt;Commissioner Caroline A. Crenshaw&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Jan. 29, 2021&lt;/p&gt;&lt;div readability=&quot;60.325986673501&quot;&gt;
&lt;p&gt;The Commission is closely monitoring and evaluating the extreme price volatility of certain stocks’ trading prices over the past several days. Our core market infrastructure has proven resilient under the weight of this week’s extraordinary trading volumes. Nevertheless, extreme stock price volatility has the potential to expose investors to rapid and severe losses and undermine market confidence. &lt;/p&gt;
&lt;p&gt;As always, the Commission will work to protect investors, to maintain fair, orderly, and efficient markets, and to facilitate capital formation. The Commission is working closely with our regulatory partners, both across the government and at FINRA and other self-regulatory organizations, including the stock exchanges, to ensure that regulated entities uphold their obligations to protect investors and to identify and pursue potential wrongdoing. The Commission will closely review actions taken by regulated entities that may disadvantage investors or otherwise unduly inhibit their ability to trade certain securities.&lt;/p&gt;
&lt;p&gt;In addition, we will act to protect retail investors when the facts demonstrate abusive or manipulative trading activity that is prohibited by the federal securities laws. Market participants should be careful to avoid such activity. Likewise, issuers must ensure compliance with the federal securities laws for any contemplated offers or sales of their own securities.&lt;/p&gt;
&lt;p&gt;The Commission will continue our work on behalf of investors and the markets. In this regard, we hope to facilitate a robust public dialogue among market participants and investors on the structure and operation of our securities markets. Members of the public can submit tips or complaints through the Commission’s website &lt;a href=&quot;https://www.sec.gov/tcr&quot;&gt;using this online form&lt;/a&gt;. Members of the public with questions should contact the &lt;a href=&quot;https://www.sec.gov/page/oieasectionlanding&quot;&gt;Commission’s Office of Investor Education and Advocacy&lt;/a&gt; at 1-800-732-0330, ask a question &lt;a href=&quot;https://www.sec.gov/oiea/QuestionsAndComments.html&quot;&gt;using this online form&lt;/a&gt;, or email us at &lt;a href=&quot;mailto:Help@SEC.gov&quot;&gt;Help@SEC.gov&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Fri, 29 Jan 2021 14:47:39 +0000</pubDate>
<dc:creator>TeMPOraL</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.sec.gov/news/public-statement/joint-statement-market-volatility-2021-01-29</dc:identifier>
</item>
<item>
<title>Pony – High-Performance Safe Actor Programming</title>
<link>https://www.ponylang.io/discover/</link>
<guid isPermaLink="true" >https://www.ponylang.io/discover/</guid>
<description>&lt;h2 id=&quot;what-is-pony&quot;&gt;What is Pony?&lt;/h2&gt;
&lt;p&gt;Pony is an open-source, object-oriented, &lt;a href=&quot;https://en.wikipedia.org/wiki/Actor_model&quot;&gt;actor-model&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Capability-based_security&quot;&gt;capabilities-secure&lt;/a&gt;, high-performance programming language.&lt;/p&gt;
&lt;p&gt;If you are looking to jump in and get started with Pony &lt;em&gt;right now&lt;/em&gt;, you can try it in your browser using the &lt;a href=&quot;http://playground.ponylang.io&quot;&gt;Pony Playground&lt;/a&gt;. Keep reading if you are interested in what makes Pony different and why you should consider using it.&lt;/p&gt;
&lt;p&gt;If you are interested in the early history of Pony and how it came into existence, you’re in luck: &lt;a href=&quot;https://www.ponylang.io/blog/2017/05/an-early-history-of-pony/&quot;&gt;“An Early History of Pony”&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;what-makes-pony-different&quot;&gt;What makes Pony different?&lt;/h2&gt;
&lt;h3 id=&quot;pony-is-type-safe&quot;&gt;Pony is type safe&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Really type safe&lt;/em&gt;. There’s a mathematical &lt;a href=&quot;https://www.ponylang.io/media/papers/fast-cheap-with-proof.pdf&quot;&gt;proof&lt;/a&gt; and everything.&lt;/p&gt;
&lt;h3 id=&quot;pony-is-memory-safe&quot;&gt;Pony is memory safe&lt;/h3&gt;
&lt;p&gt;There are no dangling pointers and no buffer overruns. The language doesn’t even have the concept of null!&lt;/p&gt;
&lt;h3 id=&quot;exception-safe&quot;&gt;Exception-Safe&lt;/h3&gt;
&lt;p&gt;There are no runtime exceptions. All exceptions have defined semantics, and they are &lt;em&gt;always&lt;/em&gt; caught.&lt;/p&gt;
&lt;h3 id=&quot;data-race-free&quot;&gt;Data-race Free&lt;/h3&gt;
&lt;p&gt;Pony doesn’t have locks nor atomic operations or anything like that. Instead, the type system ensures at compile time that your concurrent program can never have data races. So you can write highly concurrent code and never get it wrong.&lt;/p&gt;
&lt;h3 id=&quot;deadlock-free&quot;&gt;Deadlock-Free&lt;/h3&gt;
&lt;p&gt;This one is easy because Pony has no locks at all! So they definitely don’t deadlock, because they don’t exist!&lt;/p&gt;
&lt;h3 id=&quot;native-code&quot;&gt;Native Code&lt;/h3&gt;
&lt;p&gt;Pony is an ahead-of-time (AOT) compiled language. There is no interpreter nor virtual machine.&lt;/p&gt;
&lt;h3 id=&quot;compatible-with-c&quot;&gt;Compatible with C&lt;/h3&gt;
&lt;p&gt;Pony programs can natively call C libraries. Our compiler is able to generate a C-header file for Pony libraries. Consequently, C/C++ programs can natively call Pony programs!&lt;/p&gt;
&lt;h2 id=&quot;why-pony&quot;&gt;Why Pony?&lt;/h2&gt;
&lt;p&gt;There’s plenty to love about Pony, but more than anything else, what we love most is that Pony makes it easy to write fast, safe, efficient, highly concurrent programs. How? The Pony type system introduces a novel concept: “reference capabilities”. &lt;a href=&quot;https://tutorial.ponylang.io/capabilities/reference-capabilities.html&quot;&gt;Reference capabilities&lt;/a&gt; allow you to label different bits of data based on how that data can be shared. The Pony compiler will then verify that you are in fact correctly using the data based on the labels you provide. Reference capabilities combined with Pony’s actor model of concurrency makes for a powerful pairing. Let’s dig in and take a quick look:&lt;/p&gt;
&lt;h3 id=&quot;mutable-state-is-hard&quot;&gt;Mutable state is hard&lt;/h3&gt;
&lt;p&gt;The problem with concurrency is shared mutable data. If two different threads have access to the same piece of data then they might try to update it at the same time. At best this can lead to those two threads having different versions of the data. At worst the updates can interact badly resulting in the data being overwritten with garbage. The standard way to avoid these problems is to use locks to prevent data updates from happening at the same time. This causes big performance hits and is very difficult to get right, so it causes lots of bugs.&lt;/p&gt;
&lt;h3 id=&quot;immutable-data-can-be-safely-shared&quot;&gt;Immutable data can be safely shared&lt;/h3&gt;
&lt;p&gt;Any data that is immutable (i.e. it cannot be changed) is safe to use concurrently. Since it is immutable it is never updated and it’s the updates that cause concurrency problems.&lt;/p&gt;
&lt;h3 id=&quot;isolated-data-is-safe&quot;&gt;Isolated data is safe&lt;/h3&gt;
&lt;p&gt;If a block of data has only one reference to it then we call it &lt;em&gt;isolated&lt;/em&gt;. Since there is only one reference to it, isolated data cannot be &lt;em&gt;shared&lt;/em&gt; by multiple threads, so there are no concurrency problems. Isolated data can be passed between multiple threads. As long as only one of them has a reference to it at a time then the data is still safe from concurrency problems.&lt;/p&gt;
&lt;h3 id=&quot;every-actor-is-single-threaded&quot;&gt;Every actor is single threaded&lt;/h3&gt;
&lt;p&gt;The code within a single actor is never run concurrently. This means that, within a single actor, data updates cannot cause problems. It’s only when we want to share data between actors that we have problems.&lt;/p&gt;
&lt;h3 id=&quot;reference-capabilities-enforce-safe-data-handling&quot;&gt;Reference capabilities enforce safe data handling&lt;/h3&gt;
&lt;p&gt;By sharing only immutable data and exchanging only isolated data we can have safe concurrent programs without locks. The problem is that it’s very difficult to do that correctly. If you accidentally hang on to a reference to some isolated data you’ve handed over or change something you’ve shared as immutable then everything goes wrong. What you need is for the compiler to force you to live up to your promises. Pony reference capabilities allow the compiler to do just that.&lt;/p&gt;
&lt;p&gt;If you ask us, that’s pretty damn cool and a hell of a reason to give Pony a try.&lt;/p&gt;
&lt;h2 id=&quot;why-not-pony&quot;&gt;Why not Pony?&lt;/h2&gt;
&lt;p&gt;There are many valid reasons to not use Pony. Amongst these are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Lack of API stability&lt;/li&gt;
&lt;li&gt;Lack of high-quality 3rd party libraries&lt;/li&gt;
&lt;li&gt;Limited native tooling&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;api-stability&quot;&gt;API stability&lt;/h3&gt;
&lt;p&gt;Pony is pre-1.0. We regularly have releases that involve breaking changes. This lack of stability is plenty of reason for many projects to avoid using Pony.&lt;/p&gt;
&lt;h3 id=&quot;batteries-required&quot;&gt;Batteries required&lt;/h3&gt;
&lt;p&gt;If your project is going to succeed or fail based on the size of community around the tools you are using, Pony is not a good choice for you. While it’s possible to write stable, high-performance applications using Pony, you will have to do a decent amount of work. The pool of open source, ready to use Pony libraries is very small. If it’s not in the standard library then odds are you are going to have to add it yourself, either by writing it from scratch in Pony or by wrapping an existing C library using Pony’s excellent &lt;a href=&quot;https://tutorial.ponylang.io/c-ffi/&quot;&gt;C-FFI&lt;/a&gt; functionality.&lt;/p&gt;
&lt;h3 id=&quot;tooling&quot;&gt;Tooling&lt;/h3&gt;
&lt;p&gt;There’s a wide swath of tooling that some people have come to expect that isn’t currently available for Pony. We don’t have an IDE. You can use standard debuggers like GDB or LLDB but the experience still has some rough edges. If you are comfortable working with a basic text editor and using LLDB, VTune and other tools, you’ll probably be ok. Just don’t expect a full, robust ecosystem. We aren’t there yet.&lt;/p&gt;
&lt;p&gt;If your project isn’t going to get a great deal of benefit from any of Pony’s strengths, then you shouldn’t use Pony. If you are writing a single threaded application without any overriding performance concerns, and you need access to a large community and wealth of libraries then you’re much better off selecting another language. However, we hope that you see enough potential in Pony to start playing around with it even if it isn’t right for your current project.&lt;/p&gt;
&lt;h2 id=&quot;the-pony-philosophy&quot;&gt;The Pony Philosophy&lt;/h2&gt;
&lt;p&gt;In the spirit of &lt;a href=&quot;http://www.jwz.org/doc/worse-is-better.html&quot;&gt;Richard Gabriel&lt;/a&gt;, the Pony philosophy is neither “the-right-thing” nor “worse-is-better”. It is “get-stuff-done”.&lt;/p&gt;
&lt;h3 id=&quot;correctness&quot;&gt;Correctness&lt;/h3&gt;
&lt;p&gt;Incorrectness is simply not allowed. &lt;em&gt;It’s pointless to try to get stuff done if you can’t guarantee the result is correct.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Runtime speed is more important than everything except correctness. If performance must be sacrificed for correctness, try to come up with a new way to do things. &lt;em&gt;The faster the program can get stuff done, the better. This is more important than anything except a correct result.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;simplicity&quot;&gt;Simplicity&lt;/h3&gt;
&lt;p&gt;Simplicity can be sacrificed for performance. It is more important for the interface to be simple than the implementation. &lt;em&gt;The faster the programmer can get stuff done, the better. It’s ok to make things a bit harder on the programmer to improve performance, but it’s more important to make things easier on the programmer than it is to make things easier on the language/runtime.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;consistency&quot;&gt;Consistency&lt;/h3&gt;
&lt;p&gt;Consistency can be sacrificed for simplicity or performance. &lt;em&gt;Don’t let excessive consistency get in the way of getting stuff done.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;completeness&quot;&gt;Completeness&lt;/h3&gt;
&lt;p&gt;It’s nice to cover as many things as possible, but completeness can be sacrificed for anything else. &lt;em&gt;It’s better to get some stuff done now than wait until everything can get done later.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The “get-stuff-done” approach has the same attitude towards correctness and simplicity as “the-right-thing”, but the same attitude towards consistency and completeness as “worse-is-better”. It also adds performance as a new principle, treating it as the second most important thing (after correctness).&lt;/p&gt;
&lt;h2 id=&quot;guiding-principles&quot;&gt;Guiding Principles&lt;/h2&gt;
&lt;p&gt;Throughout the design and development of the language, the following principles should be adhered to.&lt;/p&gt;
&lt;ul readability=&quot;31.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Use the get-stuff-done approach.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Simple grammar. Language must be trivial to parse for both humans and computers.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;No loadable code. Everything is known to the compiler.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Fully type safe. There is no “trust me, I know what I’m doing” coercion.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Fully memory safe. There is no “this random number is really a pointer, honest.”&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;No crashes. A program that compiles should never crash (although it may hang or do something unintended).&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Sensible error messages. Where possible use simple error messages for specific error cases. It is fine to assume the programmer knows the definitions of words in our lexicon, but avoid compiler or other computer science jargon.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Inherent build system. No separate applications required to configure or build.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Aim to reduce common programming bugs through the use of restrictive syntax.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Provide a single, clean and clear way to do things rather than catering to every programmer’s preferred prejudices.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Make upgrades clean. Do not try to merge new features with the ones they are replacing, if something is broken remove it and replace it in one go. Where possible provide rewrite utilities to upgrade source between language versions.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Reasonable build time. Keeping down build time is important, but less important than runtime performance and correctness.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;Allowing the programmer to omit some things from the code (default arguments, type inference, etc) is fine, but fully specifying should always be allowed.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;No ambiguity. The programmer should never have to guess what the compiler will do, or vice-versa.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Document required complexity. Not all language features have to be trivial to understand, but complex features must have full explanations in the docs to be allowed in the language.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Language features should be minimally intrusive when not used.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Fully defined semantics. The semantics of all language features must be available in the standard language docs. It is not acceptable to leave behavior undefined or “implementation dependent”.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Efficient hardware access must be available, but this does not have to pervade the whole language.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;The standard library should be implemented in Pony.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Interoperability. Must be interoperable with other languages, but this may require a shim layer if non-primitive types are used.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Avoid library pain. Use of 3rd party Pony libraries should be as easy as possible, with no surprises. This includes writing and distributing libraries and using multiple versions of a library in a single program.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;aside class=&quot;copyright&quot; role=&quot;note&quot;&gt;© 2021 Pony Developers&lt;/aside&gt;</description>
<pubDate>Fri, 29 Jan 2021 14:03:51 +0000</pubDate>
<dc:creator>ibraheemdev</dc:creator>
<og:url>/discover/</og:url>
<og:title>Discover - Pony</og:title>
<og:image>/images/logo.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.ponylang.io/discover/</dc:identifier>
</item>
<item>
<title>Achieving 11M IOPS and 66 GB/S IO on a Single ThreadRipper Workstation</title>
<link>https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/</link>
<guid isPermaLink="true" >https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;/&gt;&lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;180x180&quot; href=&quot;/apple-touch-icon.png&quot;/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;32x32&quot; href=&quot;/favicon-32x32.png&quot;/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;16x16&quot; href=&quot;/favicon-16x16.png&quot;/&gt;&lt;link rel=&quot;manifest&quot; href=&quot;/site.webmanifest&quot;/&gt;&lt;link rel=&quot;mask-icon&quot; href=&quot;/safari-pinned-tab.svg&quot; color=&quot;#5BBAD5&quot;/&gt;&lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#da532c&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;#ffffff&quot;/&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;Achieving 11M IOPS &amp;amp; 66 GB/s IO on a Single ThreadRipper Workstation - Tanel Poder Consulting&quot;/&gt;&lt;meta property=&quot;og:title&quot; content=&quot;Achieving 11M IOPS &amp;amp; 66 GB/s IO on a Single ThreadRipper Workstation - Tanel Poder Consulting&quot;/&gt;&lt;meta name=&quot;description&quot; content=&quot;TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right! Introduction In this post I&amp;amp;rsquo;ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve 11M IOPS with 4kB random reads and 66 GiB/s throughput with larger IOs - and what bottlenecks &amp;amp;amp; issues I fixed to get there. - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right! Introduction In this post I&amp;amp;rsquo;ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve 11M IOPS with 4kB random reads and 66 GiB/s throughput with larger IOs - and what bottlenecks &amp;amp;amp; issues I fixed to get there. - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta itemprop=&quot;description&quot; content=&quot;TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right! Introduction In this post I&amp;amp;rsquo;ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve 11M IOPS with 4kB random reads and 66 GiB/s throughput with larger IOs - and what bottlenecks &amp;amp;amp; issues I fixed to get there. - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta property=&quot;og:description&quot; content=&quot;TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right! Introduction In this post I&amp;amp;rsquo;ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve 11M IOPS with 4kB random reads and 66 GiB/s throughput with larger IOs - and what bottlenecks &amp;amp;amp; issues I fixed to get there. - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta content=&quot;&quot; name=&quot;keywords&quot;/&gt;&lt;title&gt;Achieving 11M IOPS &amp;amp; 66 GB/s IO on a Single ThreadRipper Workstation | Tanel Poder Consulting&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;782.00793373187&quot;&gt;
&lt;nav&gt;&lt;hr/&gt;&lt;/nav&gt;&lt;p&gt;

&lt;h2 class=&quot;date&quot;&gt;2021-01-29&lt;/h2&gt;
&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right!&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;In this post I’ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve &lt;strong&gt;11M IOPS&lt;/strong&gt; with 4kB random reads and &lt;strong&gt;66 GiB/s&lt;/strong&gt; throughput with larger IOs - and what bottlenecks &amp;amp; issues I fixed to get there. We’ll look into Linux block I/O internals and their interaction with modern hardware. We’ll use tools &amp;amp; techniques, old and new, for measuring bottlenecks - and other adventures in the kernel I/O stack.&lt;/p&gt;
&lt;p&gt;When running &lt;code&gt;fio&lt;/code&gt; on Linux with &lt;code&gt;io_uring&lt;/code&gt;, the end result is this:&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;dstat -pcmrd&lt;/strong&gt;

---procs--- ----total-usage---- ------memory-usage----- &lt;strong&gt;--io/total- -dsk/total-&lt;/strong&gt;
run blk new|usr sys idl wai stl| used  free  buf   cach| read  writ| read  writ
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 33   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 33   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.0M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.0M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 28  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  72   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
&lt;/pre&gt;
&lt;p&gt;With 4kB blocksize random reads, I get over 11 million IOPS, at 42 GiB/ (~45 GB/s) throughput. I will go through technical details and larger blocksizes below, but I’ll start by answering the &lt;em&gt;“why”&lt;/em&gt; question first.&lt;/p&gt;
&lt;h3 id=&quot;index&quot;&gt;Index&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#why&quot;&gt;Why?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#hardware&quot;&gt;Hardware&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#storage&quot;&gt;Storage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#single-disk-test&quot;&gt;Single-disk test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#io-configuration---direct-io&quot;&gt;I/O configuration - direct I/O&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#io-configuration---io-scheduler&quot;&gt;I/O configuration - I/O scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#multi-disk-test&quot;&gt;Multi-disk test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#bios-settings&quot;&gt;BIOS settings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#pcie-root-complex-bottleneck&quot;&gt;PCIe root complex bottleneck (before)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#pcie-root-complex-bottleneck-after&quot;&gt;PCIe root complex bottleneck (after)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#final-results&quot;&gt;Final results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#further-reading&quot;&gt;Further reading&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3 id=&quot;why&quot;&gt;Why?&lt;/h3&gt;
&lt;p&gt;Why would you even need such IO throughput in a single machine? Shouldn’t I be building a 50-node cluster in the cloud “for scalability”? This is exactly the point of my experiment - do you really want to have all the complexity of clusters or performance implications of remote storage &lt;em&gt;if&lt;/em&gt; you can run your I/O heavy workload on just one server with local NVMe storage? How many databases out there need to sustain even “only” 1M disk IOPS? Or if you really &lt;em&gt;do&lt;/em&gt; need that sweet &lt;strong&gt;1 TB/s&lt;/strong&gt; data scanning speed, you could do this with 10-20 well-configured cluster nodes instead of 200. Modern hardware is powerful, if used right!&lt;/p&gt;
&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;I’m well aware of various enterprise data management functional requirements, such as availability, remote replication, data sharing or just huge data volumes that may direct you away from using local NVMe SSDs as your primary storage and I am not arguing against that. Some of these requirements can be addressed in software, some not. The scope of this article is to show the &lt;em&gt;raw performance&lt;/em&gt; even cheap commodity hardware gives you nowadays, so if your company is paying 10x more for 100x lower throughput, it’s good to be aware of other options.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My future plans for this article series include running I/O heavy performance tests with various database engines, other ideas are welcome too!&lt;/p&gt;
&lt;h3 id=&quot;hardware&quot;&gt;Hardware&lt;/h3&gt;
&lt;h4 id=&quot;machine&quot;&gt;Machine&lt;/h4&gt;
&lt;h4 id=&quot;cpu&quot;&gt;CPU&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.amd.com/en/products/cpu/amd-ryzen-threadripper-pro-3955wx&quot;&gt;AMD Ryzen Threadripper PRO 3955WX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zen 2 microarchitecture (7nm)&lt;/li&gt;
&lt;li&gt;Max number of CPU cores is 64 (128 threads), but I bought a 16-core version because I’m cheap&lt;/li&gt;
&lt;li&gt;All 16 cores can run at 3.9 GHz sustained base frequency, with 4.3 GHz max “boost” frequency&lt;/li&gt;
&lt;li&gt;This single CPU has &lt;strong&gt;8 memory channels&lt;/strong&gt; supporting 8 x DDR4 3200 ECC RDIMMs&lt;/li&gt;
&lt;li&gt;This CPU supports 128 &lt;strong&gt;PCIe 4.0&lt;/strong&gt; lanes&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;So, the excellent throughput of this workstation does not come only from CPU processing speeds, but from the additional bandwidth 8 memory channels and 128 PCIe 4.0 lanes offer! A &lt;em&gt;single&lt;/em&gt; PCIe 4.0 lane gives you about 1.969 GB/s bandwidth in each direction (PCIe is switched, point-to-point full duplex). In theory, 128 lanes should mean that the CPU can handle ~250 GB/s (2 terabit/s!) PCIe traffic in each direction, if all lanes were fully used.&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;Note that on modern CPUs, most PCIe lanes are connected directly to the CPU (as are memory channels) and do not go through some external “southbridge” controller or “front-side bus”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This also means that a single PCIe 4.0 x4 card can achieve close to 8 GB/s data transfer speed, assuming that the devices can handle it and you do not hit other bottlenecks first. This leads us to the next section, suitable SSDs.&lt;/p&gt;
&lt;h4 id=&quot;storage&quot;&gt;Storage&lt;/h4&gt;
&lt;p&gt;The Samsung 980 Pro is a true PCIe 4.0 SSD, with specs claiming 7000 MB/s read and 5000 MB/s write throughput (with caveats). Its internal controller is capable of true PCIe 4.0 transfer speeds and is not an old PCIe 3.0 chip that just presents it as a PCIe 4.0 compatible one with low GT/s. I understand that some other “PCIe4” SSDs out there will max out at the PCIe3 speeds (~3.5 GB/s) as they don’t have new generation controllers in them.&lt;/p&gt;
&lt;p&gt;I ended up buying 8 x 1 TB SSDs, eventually used for data volumes and 2 x 500 GB ones for boot disks and software. I also have a 380 GB Intel Optane 905P SSD for low latency writes (like transaction logs), but more about that in a future post.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-10ssds.jpg&quot; alt=&quot;10 Samsung 980 Pro SSDs&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The NVMe disks show up like this (8x1 TB and 2x500 GB):&lt;/p&gt;
&lt;pre&gt;
$ sudo nvme list
Node             SN                   Model                      
---------------- -------------------- ---------------------------
/dev/nvme0n1     S5P2NG0N902798J      Samsung SSD 980 PRO 1TB    
/dev/nvme1n1     S5P2NG0NA02399T      Samsung SSD 980 PRO 1TB    
/dev/nvme2n1     S5P2NG0NA04362H      Samsung SSD 980 PRO 1TB    
/dev/nvme3n1     S5P2NG0N902802P      Samsung SSD 980 PRO 1TB    
/dev/nvme4n1     S5P2NG0NA00551P      Samsung SSD 980 PRO 1TB    
/dev/nvme5n1     S5P2NG0NA03266N      Samsung SSD 980 PRO 1TB    
/dev/nvme6n1     S5P2NG0NA01498X      Samsung SSD 980 PRO 1TB    
/dev/nvme7n1     S5P2NG0NA04358V      Samsung SSD 980 PRO 1TB    
/dev/nvme8n1     S5NYNG0N906374T      Samsung SSD 980 PRO 500GB  
/dev/nvme9n1     S5NYNG0N906379K      Samsung SSD 980 PRO 500GB  
&lt;/pre&gt;
&lt;p&gt;Samsung’s specs pages (&lt;a href=&quot;https://www.samsung.com/semiconductor/minisite/ssd/product/consumer/980pro/&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;https://s3.ap-northeast-2.amazonaws.com/global.semi.static/Samsung_NVMe_SSD_980_PRO_Data_Sheet_Rev.1.2.pdf&quot;&gt;2&lt;/a&gt;) state that the max sequential read &amp;amp; write speeds as 7000 MB/s &amp;amp; 5000 MB/s for the 1 TB drives using PCIe 4.0. The IO pattern doesn’t really have to be sequential, just with big enough I/O sizes. When these cards are plugged in to PCIe 3.0 slots, you’d get only ~3500 MB/s read and write speeds due to the PCIe 3.0 x4 throughput limitations.&lt;/p&gt;
&lt;blockquote readability=&quot;17.874125874126&quot;&gt;
&lt;p&gt;Take the write speeds with a grain of salt, as TLC &amp;amp; QLC cards have slower multi-bit writes into the main NAND area, but may have some DIMM memory for buffering writes and/or a “TurboWrite buffer” (as Samsung calls it) that uses part of the SSDs NAND as faster SLC storage. It’s done by issuing single-bit “SLC-like” writes into TLC area. So, once you’ve filled up the “SLC” TurboWrite buffer at 5000 MB/s, you’ll be bottlenecked by the TLC “main area” at 2000 MB/s (on the 1 TB disks).&lt;/p&gt;
&lt;p&gt;Apparently on the 980’s the TurboWrite buffer defaults to ~6 GB on the 1 TB SSD, but it’s dynamic and can grow up to 108 GB if there’s high write demand &lt;em&gt;and&lt;/em&gt; enough unused NAND space. Samsung also has their own &lt;em&gt;designed-in-house&lt;/em&gt; disk controller (Elpis) that is built for achieving PCIe 4.0 speeds. It can handle 128 I/O queues (with 64k command sets per queue!). This &lt;a href=&quot;https://pcper.com/2020/09/samsung-980-pro-pci-express-4-0-ssd-review/&quot;&gt;article&lt;/a&gt; is a good reference about this disk - and an overview of complexity (and potential bottlenecks) of modern SSDs!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As I want to focus on just the max &lt;strong&gt;raw block I/O&lt;/strong&gt; performance for this article, I will run my tests directly against the NVMe block devices (without a filesystem or LVM on the I/O path). NVMe block devices have per-CPU multi-queues (MQ) enabled by default and device interrupts are “striped” across all CPUs. I’ll present some LVM, multiqueue and file system tests in future articles.&lt;/p&gt;
&lt;blockquote readability=&quot;3.3018867924528&quot;&gt;
&lt;p&gt;You can &lt;a href=&quot;https://tanelpoder.com/contact&quot;&gt;subscribe&lt;/a&gt; to email updates or &lt;a href=&quot;https://tanelpoder.com/contact/&quot;&gt;follow me&lt;/a&gt; here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;single-disk-test&quot;&gt;Single-disk test&lt;/h4&gt;
&lt;p&gt;I used Ubuntu 20.10 with Ubuntu-provided Linux kernel 5.8.0-29-generic for these tests. I briefly tested the (currently) latest Linux kernel 5.11-rc4 too, it has some &lt;code&gt;io_uring&lt;/code&gt; enhancements, but got lower throughput out of it. There are some big AMD ThreadRipper (power-aware scheduler) updates in it, apparently with some unsolved performance regressions.&lt;/p&gt;
&lt;p&gt;I first ran a single-disk test to avoid hitting any system-wide throughput bottlenecks when accessing all 10 SSDs simultaneously. This test aimed to give me a theoretical max throughput of a single disk. I used &lt;code&gt;fio&lt;/code&gt; with &lt;code&gt;--io_uring&lt;/code&gt; &lt;em&gt;asynchronous&lt;/em&gt; I/O option. As expected, it was more efficient than &lt;code&gt;libaio&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I ended up using the following &lt;code&gt;fio&lt;/code&gt; command for my narrowly focused synthetic benchmark. I’ll explain the reasoning for some of the command line options later in this post.&lt;/p&gt;
&lt;p&gt;Here’s the &lt;code&gt;onessd.sh&lt;/code&gt; script that I used for single-disk testing:&lt;/p&gt;
&lt;pre&gt;
#!/bin/bash

[ $# -ne 3 ] &amp;amp;&amp;amp; echo Usage $0 numjobs /dev/DEVICENAME BLOCKSIZE &amp;amp;&amp;amp; exit 1

fio --readonly --name=onessd \
    --filename=$2 \
    --filesize=100g --rw=randread --bs=$3 &lt;strong&gt;--direct=1&lt;/strong&gt; --overwrite=0 \
    --numjobs=$1 &lt;strong&gt;--iodepth=32&lt;/strong&gt; --time_based=1 --runtime=3600 \
    --ioengine=&lt;strong&gt;io_uring&lt;/strong&gt; \
    --registerfiles --fixedbufs \
    --gtod_reduce=1 --group_reporting
&lt;/pre&gt;
&lt;p&gt;Ok, let’s first run &lt;code&gt;fio&lt;/code&gt; with 3 concurrent workers doing 4kB reads against a single disk only (with queue_depth=32 per worker), to see the maximum theoretical throughput of such a disk in my machine:&lt;/p&gt;
&lt;pre&gt;
$ sudo &lt;strong&gt;./onessd.sh 3 /dev/nvme0n1 4k&lt;/strong&gt;
fio-3.25-26-gb213
Starting 3 processes
Jobs: 3 (f=3): [r(3)][0.3%][r=4489MiB/s][r=&lt;mark&gt;1149k IOPS&lt;/mark&gt;][eta 59m:50s]
&lt;/pre&gt;
&lt;p&gt;Looks like we got even more IOPS out of the disk than Samsung promised - their specs said “only” 1,000,000 IOPS per disk. If you wonder why I’m running the single-disk test with 3 concurrent worker processes - it turns out that a single process maxes out a single CPU at about 450k IOPS on my setup. So, a single process is physically incapable of submitting (and reaping) more than 450k I/Os per second of CPU time (being 100% on CPU). Assuming that its CPU core was running at around 3.9 GHz (and didn’t have much else to do), it translates to about 3,900,000,000 / 450,000 = 8,666 &lt;em&gt;CPU cycles&lt;/em&gt; used for submitting + issuing + completing + reaping each I/O.&lt;/p&gt;
&lt;p&gt;If you look into the metrics below, you see that my 3 worker processes used about 9-10% of CPU capacity of my machine with 32 (logical) processors:&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;dstat -pcrmd&lt;/strong&gt;

---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total-
&lt;strong&gt;run&lt;/strong&gt; blk new|&lt;strong&gt;usr&lt;/strong&gt; &lt;strong&gt;sys&lt;/strong&gt; idl wai stl| used  free  buf   cach| &lt;strong&gt;read&lt;/strong&gt;  writ| read  writ
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  &lt;mark&gt;2&lt;/mark&gt;&lt;mark&gt;8&lt;/mark&gt;  91   0   0|1792M  249G   41M  406M|&lt;mark&gt;1145k&lt;/mark&gt;    0 |4472M    0 
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  &lt;mark&gt;2&lt;/mark&gt;&lt;mark&gt;7&lt;/mark&gt;  91   0   0|1793M  249G   41M  406M|&lt;mark&gt;1150k&lt;/mark&gt;    0 |4493M    0 
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  &lt;mark&gt;2&lt;/mark&gt;&lt;mark&gt;8&lt;/mark&gt;  91   0   0|1793M  249G   41M  406M|&lt;mark&gt;1152k&lt;/mark&gt;    0 |4499M    0 
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  &lt;mark&gt;1&lt;/mark&gt;&lt;mark&gt;8&lt;/mark&gt;  91   0   0|1793M  249G   41M  406M|&lt;mark&gt;1151k&lt;/mark&gt;    0 |4498M    0 
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Sidenote: It looks like &lt;code&gt;dstat&lt;/code&gt; on Ubuntu 20.10 has some CPU utilization rounding &amp;amp; reporting errors. I used other tools to verify that my CPU numbers in this article are similar.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;How about large I/Os? Let’s try 1 MB sized reads, something that a database engine would be using for scanning large tables:&lt;/p&gt;
&lt;pre&gt;
$ sudo ./onessd.sh 3 /dev/nvme0n1 &lt;strong&gt;1m&lt;/strong&gt;
fio-3.25-26-gb213
Starting 3 processes
Jobs: 3 (f=3): [r(3)][0.2%][r=&lt;mark&gt;6811MiB/s&lt;/mark&gt;][r=6811 IOPS][eta 59m:53s]
&lt;/pre&gt;
&lt;p&gt;We are scanning data at over &lt;strong&gt;6 GiB/s&lt;/strong&gt; just using one disk!&lt;/p&gt;
&lt;p&gt;Seems like we couldn’t actually do 1 MB sized reads, as we are issuing 13.8k read operations for ~6800 MB below. Either &lt;code&gt;fio&lt;/code&gt; wasn’t able to issue 1 MB-sized IOs or the larger requests got split into ~512kB chunks somewhere in the kernel:&lt;/p&gt;
&lt;pre&gt;
----total-usage---- ---procs--- ------memory-usage----- --io/total- -dsk/total-
usr sys idl wai stl|run blk new| used  free  buf   cach| read  writ| read  writ
  0   1  99   0   0|  0   0   0|1990M  249G   54M  441M|&lt;mark&gt;13.8k&lt;/mark&gt;    0 |&lt;mark&gt;6807M&lt;/mark&gt;    0 
  0   1  99   0   0|  0   0   0|1990M  249G   54M  441M|&lt;mark&gt;13.7k&lt;/mark&gt;    0 |&lt;mark&gt;6799M&lt;/mark&gt;    0 
  0   1  99   0   0|  0   0   0|1990M  249G   54M  441M|&lt;mark&gt;13.8k&lt;/mark&gt;    0 |&lt;mark&gt;6805M&lt;/mark&gt;    0 
  0   1  99   0   0|  0   0   0|1990M  249G   54M  441M|&lt;mark&gt;13.7k&lt;/mark&gt;    0 |&lt;mark&gt;6803M&lt;/mark&gt;    0 
&lt;/pre&gt;
&lt;p&gt;Thanks to the big I/O sizes, every request takes a lot of time as the disk’s DMA controller copies the request’s data to relevant memory location in RAM. PCIe 4.0 x4 max theoretical transfer rate is about 7.877 GB/s, even the PCIe transfer from the disk controller memory to CPU will take over 120 &lt;em&gt;us&lt;/em&gt; per MB, in addition to the flash reading and SSD controller’s latency.&lt;/p&gt;
&lt;p&gt;So, as you see from the CPU usage numbers above, my CPUs were 99% idle, despite running 3 concurrent &lt;code&gt;fio&lt;/code&gt; workers and &lt;code&gt;top&lt;/code&gt; confirmed that. What were the worker processes doing then? They were mostly sleeping, waiting for events in io_uring completion queue, with WCHAN &lt;code&gt;io_cqring_wait&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
$ sudo &lt;strong&gt;psn -G syscall,wchan -a -p ^fio&lt;/strong&gt;

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, wchan, stat for 5 seconds... finished.


=== Active Threads =============================================================================

 samples | avg_threads | comm  | state                  | syscall         | wchan               
------------------------------------------------------------------------------------------------
     203 |        &lt;mark&gt;2.03 | (fio) | Sleep (Interruptible)  | io_uring_enter  | io_cqring_wait&lt;/mark&gt;
     100 |        1.00 | (fio) | Sleep (Interruptible)  | select          | do_select           
      85 |        0.85 | (fio) | Sleep (Interruptible)  | clock_nanosleep | hrtimer_nanosleep   
      14 |        0.14 | (fio) | Disk (Uninterruptible) | openat          | __blkdev_get        
      12 |        0.12 | (fio) | Running (ON CPU)       | io_uring_enter  | 0                   
      12 |        0.12 | (fio) | Sleep (Interruptible)  | [running]       | io_cqring_wait      
       9 |        0.09 | (fio) | Running (ON CPU)       | io_uring_enter  | io_cqring_wait      
       2 |        0.02 | (fio) | Running (ON CPU)       | [running]       | 0                   
       2 |        0.02 | (fio) | Sleep (Interruptible)  | [running]       | 0                   
       1 |        0.01 | (fio) | Running (ON CPU)       | futex           | futex_wait_queue_me 


samples: 100 (expected: 100)
total processes: 4, threads: 5
runtime: 5.00, measure time: 0.18
&lt;/pre&gt;
&lt;blockquote readability=&quot;4.0983606557377&quot;&gt;
&lt;p&gt;&lt;em&gt;You can download the open source &lt;a href=&quot;https://tanelpoder.com/psnapper/&quot;&gt;psn&lt;/a&gt; tool from &lt;a href=&quot;https://0x.tools&quot;&gt;0x.tools&lt;/a&gt; site.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;io-configuration---direct-io&quot;&gt;I/O configuration - direct I/O&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;In the following two sections I show how I tried different OS level I/O configuration options (direct vs cached I/O and using an I/O scheduler). These sections go pretty deep into Linux kernel troubleshooting topics, if you want to skip this and read about the hardware configuration challenges, jump to the &lt;a href=&quot;https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/#multi-disk-test&quot;&gt;Multi-disk test&lt;/a&gt; section.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I used &lt;code&gt;--direct=1&lt;/code&gt; option that forces files to be opened with &lt;code&gt;O_DIRECT&lt;/code&gt; flag - bypassing the OS pagecache. When running millions of IOPS, you want to minimize the CPU overhead of every operation and copying, searching &amp;amp; replacing pages in the OS pagecache will radically increase your CPU usage and memory traffic. Most mature database engines have a built-in cache anyway, so why duplicate work (and memory usage).&lt;/p&gt;
&lt;p&gt;But for fun, I ran the same test with &lt;code&gt;--direct=0&lt;/code&gt; anyway and the results are below:&lt;/p&gt;
&lt;pre&gt;
$ dstat -pcmrd

---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total-
&lt;strong&gt;run&lt;/strong&gt; blk new|usr &lt;strong&gt;sys&lt;/strong&gt; idl wai stl| used  free  buf   cach| read  writ| read  writ
 &lt;mark&gt;75&lt;/mark&gt;   0   0|  0 &lt;mark&gt;100&lt;/mark&gt;   0   0   0|1997M 1864M  244G  243M|5947     0 |2953M    0
 &lt;mark&gt;60&lt;/mark&gt; 3.0   0|  0 &lt;mark&gt;100&lt;/mark&gt;   0   0   0|2005M 1193M  245G  243M|7188  11.0 |3593M  244k
 &lt;mark&gt;56&lt;/mark&gt; 2.0   0|  0  &lt;mark&gt;99&lt;/mark&gt;   0   1   0|1993M 1363M  245G  243M|6290     0 |3143M    0
 &lt;mark&gt;56&lt;/mark&gt; 1.0   0|  0  &lt;mark&gt;99&lt;/mark&gt;   0   1   0|1992M 1316M  245G  240M|6545     0 |3266M    0
 &lt;mark&gt;38&lt;/mark&gt;  19   0|  0  &lt;mark&gt;99&lt;/mark&gt;   0   1   0|1984M 1271M  245G  237M|6493     0 |3239M    0 ^C
&lt;/pre&gt;
&lt;p&gt;Wait, what? My &lt;code&gt;fio&lt;/code&gt; test with 3 workers somehow keeps all the CPUs 100% busy in &lt;em&gt;kernel mode&lt;/em&gt;? The runnable threads (&lt;em&gt;run&lt;/em&gt; column) alternates between 38 and 75? The read throughput has dropped from over 6 GiB/s to 3 GiB/s. But why do we have so much CPU activity? Is that just how it is?&lt;/p&gt;
&lt;p&gt;Let’s not guess or give up, but measure! As the kernel CPU usage is very high, we have a couple of options for drilling down.&lt;/p&gt;
&lt;p&gt;pSnapper can show which threads (and in which state) are most active:&lt;/p&gt;
&lt;pre&gt;
$ sudo psn -G syscall,wchan

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, wchan, stat for 5 seconds... finished.


=== Active Threads ================================================================================================

 samples | avg_threads | comm                 | state                  | syscall         | wchan                   
-------------------------------------------------------------------------------------------------------------------
    4218 |       &lt;mark&gt;59.41 | (io_wqe_worker-*)    | Disk (Uninterruptible) | [kernel_thread] | wait_on_page_bit_common&lt;/mark&gt;
    1698 |       &lt;mark&gt;23.92 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0&lt;/mark&gt;                       
      54 |        0.76 | (fio)                | Running (ON CPU)       | [running]       | 0                       
      18 |        0.25 | (kswapd*)            | Running (ON CPU)       | [running]       | 0                       
       8 |        0.11 | (io_wqe_worker-*)    | Running (ON CPU)       | [kernel_thread] | 0                       
       5 |        0.07 | (io_wqe_worker-*)    | Disk (Uninterruptible) | [running]       | 0                       
       5 |        0.07 | (io_wqe_worker-*)    | Disk (Uninterruptible) | [running]       | wait_on_page_bit_common 
       5 |        0.07 | (kworker/*:*-events) | Running (ON CPU)       | [running]       | 0                       
       4 |        0.06 | (fio)                | Running (ON CPU)       | io_uring_enter  | io_cqring_wait          
       4 |        0.06 | (io_wqe_worker-*)    | Running (ON CPU)       | [kernel_thread] | wait_on_page_bit_common 
       1 |        0.01 | (fio)                | Running (ON CPU)       | io_uring_enter  | 0                       
       1 |        0.01 | (io_wqe_worker-*)    | Running (ON CPU)       | [kernel_thread] | io_wqe_worker           
       1 |        0.01 | (rcu_sched)          | Running (ON CPU)       | [running]       | 0                       
&lt;/pre&gt;
&lt;p&gt;So, it’s not my 3 &lt;code&gt;fio&lt;/code&gt; processes that magically eat all the CPU time, but there’s a lot of &lt;code&gt;io_wge_worker-N&lt;/code&gt; kernel threads that are doing something! It starting to look like &lt;em&gt;“it’s not me, it’s you - Linux kernel”&lt;/em&gt;. When you scroll the above output right, you see that the top entry reports these threads waiting in thread state &lt;strong&gt;D - uninterruptible (usually) disk I/O&lt;/strong&gt; sleep and the kernel function (&lt;strong&gt;wchan&lt;/strong&gt;) that requested the sleep is &lt;strong&gt;wait_on_page_bit_common&lt;/strong&gt;. That’s the common WHAN that shows up whenever you’re waiting in cached I/O (pagecache) codepath. We still don’t know whether this is just a “waiting for slow I/O via pagecache to complete” scenario or some kernel issue. Remember that there’s a significant amount of these kernel worker threads also burning CPU in kernel mode, not waiting for anything (the 2nd highlighted line with “Running (ON CPU)” above).&lt;/p&gt;
&lt;p&gt;The good news is that we can easily drill down further! pSnapper allows you to sample &lt;code&gt;/proc/PID/stack&lt;/code&gt; too to get a rough idea of in which kernel locations any of your threads are. You will have to scroll all the way to the right, until you see the highlighted functions:&lt;/p&gt;
&lt;pre&gt;
$ sudo psn -G syscall,wchan,&lt;strong&gt;kstack&lt;/strong&gt;

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stack, wchan, stat for 5 seconds... finished.


=== Active Threads =======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================

 samples | avg_threads | comm                 | state                  | syscall         | wchan                   | kstack                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
     686 |       19.06 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
     418 |       11.61 | (io_wqe_worker-*)    | Disk (Uninterruptible) | [kernel_thread] | wait_on_page_bit_common | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;wait_on_page_bit_common()                                                                                                                                                                                                                                                                                                           
     384 |       10.67 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;wait_on_page_bit_common()                                                                                                                                                                                                                                                                                                           
     284 |        7.89 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;&lt;mark&gt;page_cache_sync_readahead()-&amp;gt;force_page_cache_readahead()-&amp;gt;page_cache_readahead_unbounded()-&amp;gt;__page_cache_alloc()-&amp;gt;alloc_pages_current()-&amp;gt;__alloc_pages_nodemask()-&amp;gt;__alloc_pages_slowpath.constprop.0()-&amp;gt;try_to_free_pages()-&amp;gt;do_try_to_free_pages()-&amp;gt;shrink_zones()-&amp;gt;shrink_node()-&amp;gt;shrink_node_memcgs()-&amp;gt;shrink_lruvec()-&amp;gt;shrink&lt;/mark&gt; 
     233 |        6.47 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;&lt;mark&gt;page_cache_sync_readahead()-&amp;gt;force_page_cache_readahead()-&amp;gt;page_cache_readahead_unbounded()-&amp;gt;__page_cache_alloc()-&amp;gt;alloc_pages_current()-&amp;gt;__alloc_pages_nodemask()-&amp;gt;__alloc_pages_slowpath.constprop.0()-&amp;gt;try_to_free_pages()-&amp;gt;do_try_to_free_pages()-&amp;gt;shrink_zones()-&amp;gt;shrink_node()-&amp;gt;shrink_node_memcgs()-&amp;gt;shrink_lruvec()-&amp;gt;shrink&lt;/mark&gt; 
      47 |        1.31 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;&lt;mark&gt;page_cache_sync_readahead()-&amp;gt;force_page_cache_readahead()-&amp;gt;page_cache_readahead_unbounded()-&amp;gt;add_to_page_cache_lru()-&amp;gt;__add_to_page_cache_locked()-&amp;gt;mem_cgroup_charge()-&amp;gt;mem_cgroup_charge_statistics.constprop.0()                                                                                                                &lt;/mark&gt; 
      34 |        0.94 | (fio)                | Running (ON CPU)       | [running]       | 0                       | __x64_sys_io_uring_enter()-&amp;gt;__do_sys_io_uring_enter()-&amp;gt;io_cqring_wait()                                                                                                                                                                                                                                                                                                                                                                                                                                              
      34 |        0.94 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;page_cache_sync_readahead()-&amp;gt;force_page_cache_readahead()-&amp;gt;page_cache_readahead_unbounded()-&amp;gt;__page_cache_alloc()-&amp;gt;alloc_pages_current()-&amp;gt;__alloc_pages_nodemask()                                                                                                                                                                  
      22 |        0.61 | (kswapd*)            | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;kswapd()-&amp;gt;balance_pgdat()-&amp;gt;shrink_node()-&amp;gt;shrink_node_memcgs()-&amp;gt;shrink_lruvec()-&amp;gt;shrink_inactive_list()-&amp;gt;shrink_page_list()-&amp;gt;__remove_mapping()-&amp;gt;native_queued_spin_lock_slowpath()                                                                                                                                                                                                                                                                                                                       
      11 |        0.31 | (io_wqe_worker-*)    | Running (ON CPU)       | [kernel_thread] | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;wait_on_page_bit_common()                                                                                                                                                                                                                                                                                                           
      11 |        0.31 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()-&amp;gt;io_wqe_worker()-&amp;gt;io_worker_handle_work()-&amp;gt;io_wq_submit_work()-&amp;gt;io_issue_sqe()-&amp;gt;io_read()-&amp;gt;blkdev_read_iter()-&amp;gt;generic_file_read_iter()-&amp;gt;generic_file_buffered_read()-&amp;gt;page_cache_sync_readahead()-&amp;gt;force_page_cache_readahead()-&amp;gt;page_cache_readahead_unbounded()-&amp;gt;__page_cache_alloc()-&amp;gt;alloc_pages_current()-&amp;gt;__alloc_pages_nodemask()-&amp;gt;__alloc_pages_slowpath.constprop.0()                                                                                                                            
&lt;/pre&gt;
&lt;p&gt;Or, if you don’t like to read ultra-wide outputs, you can just use a &lt;code&gt;sed&lt;/code&gt; trick to replace the &lt;code&gt;-&amp;gt;&lt;/code&gt; with &lt;code&gt;\n&lt;/code&gt; in pSnapper stack prints. I usually use my terminal to highlight the normal output lines of pSnapper, so I can easily spot where in the output a stack trace ends and the next thread entry starts:&lt;/p&gt;
&lt;pre&gt;
$ sudo psn -G syscall,wchan,kstack &lt;strong&gt;| sed 's/-&amp;gt;/\n/g'&lt;/strong&gt;

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stack, wchan, stat for 5 seconds... finished.


=== Active Threads ===============================================================================================================

 samples | avg_threads | comm                 | state                  | syscall         | wchan                   | kstack       
----------------------------------------------------------------------------------------------------------------------------------
&lt;mark&gt;     726 |       18.15 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | -&lt;/mark&gt;&lt;mark&gt;     516 |       12.90 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()&lt;/mark&gt;
io_wqe_worker()
io_worker_handle_work()
io_wq_submit_work()
io_issue_sqe()
io_read()
blkdev_read_iter()
generic_file_read_iter()
generic_file_buffered_read()
wait_on_page_bit_common()                                                                                                                                                                                                                                                                                                           
&lt;mark&gt;     375 |        9.38 | (io_wqe_worker-*)    | Running (ON CPU)       | [running]       | 0                       | kthread()&lt;/mark&gt;
io_wqe_worker()
io_worker_handle_work()
io_wq_submit_work()
io_issue_sqe()
io_read()
blkdev_read_iter()
generic_file_read_iter()
generic_file_buffered_read()
&lt;strong&gt;page_cache_sync_readahead()&lt;/strong&gt;
force_page_cache_readahead()
page_cache_readahead_unbounded()
__page_cache_alloc()
alloc_pages_current()
__alloc_pages_nodemask()
__alloc_pages_slowpath.constprop.0()
&lt;strong&gt;try_to_free_pages()&lt;/strong&gt;
do_try_to_free_pages()
shrink_zones()
shrink_node()
shrink_node_memcgs()
shrink_lruvec()
shrink 
&lt;/pre&gt;
&lt;p&gt;I showed the output of just 3 top pSnapper entries, but you see there’s a bunch of page-cache read-ahead happening! And worker threads are searching for free pages for this read-ahead. And to find enough free pages, they’ve had to resort to shrinking - throwing away existing pages using the pagecache LRU. And judging from the earlier CPU utilization and “runnable tasks” numbers, there are tends of threads, trying to do the same things at the same time.&lt;/p&gt;
&lt;p&gt;Could the high CPU usage be explained by some sort of contention in the Linux kernel?&lt;/p&gt;
&lt;p&gt;Let’s not guess - let’s measure!&lt;/p&gt;
&lt;p&gt;I have &lt;a href=&quot;https://0x.tools&quot;&gt;0x.tools&lt;/a&gt; enabled in my machines and among other things, &lt;code&gt;run_xcpu.sh&lt;/code&gt; runs &lt;code&gt;perf record&lt;/code&gt; at 1 Hz frequency. It’s &lt;em&gt;always on&lt;/em&gt; and has unnoticeable overhead as it samples on-CPU thread stacks infrequently (I built 0x.tools for &lt;em&gt;always-on low frequency profiling of production systems&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;The CPU stack capture tool creates a separate output file for every minute, so I’ll pick the latest:&lt;/p&gt;
&lt;pre&gt;
$ ls -lt xcap/xcpu* | head
-rw------- 1 root root  522984 Jan 27 12:26 xcap/xcpu
-rw------- 1 root root 1069888 Jan 27 12:26 xcap/xcpu.&lt;strong&gt;202101271226&lt;/strong&gt;5851
-rw------- 1 root root 1502960 Jan 27 12:25 xcap/xcpu.2021012712255829
-rw------- 1 root root  536472 Jan 27 01:13 xcap/xcpu.2021012701133387
-rw------- 1 root root  537600 Jan 27 01:13 xcap/xcpu.2021012701131716
-rw------- 1 root root  702856 Jan 27 01:12 xcap/xcpu.2021012701121705
-rw------- 1 root root 1034184 Jan 27 01:11 xcap/xcpu.2021012701111678
-rw------- 1 root root 1030944 Jan 27 01:10 xcap/xcpu.2021012701101654
-rw------- 1 root root 1030960 Jan 27 01:09 xcap/xcpu.2021012701091640
-rw------- 1 root root 1033800 Jan 27 01:08 xcap/xcpu.2021012701081614

$ sudo &lt;strong&gt;perf report -i xcap/xcpu.2021012712265851&lt;/strong&gt;
&lt;/pre&gt;
&lt;p&gt;Here’s the output:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-spinlock-contention-readahead.png&quot;/&gt; Linux kernel spinlock contention due to concurrent readaheads&lt;/p&gt;
&lt;p&gt;What’s the best solution for reducing spinlock contention? &lt;strong&gt;Use it less!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since I don’t want to do less I/O, on the contrary, I want to push the system to do more of it - let’s just disable the readahead and see if it makes anything better. &lt;code&gt;fio&lt;/code&gt; has a &lt;code&gt;--fadvise_hint=random&lt;/code&gt; hint to tell the OS it expects to be doing random I/O. This didn’t seem to make a difference in my brief tests, so I just moved on and used the other option - I disabled the readahead at block device level:&lt;/p&gt;
&lt;pre&gt;
$ sudo &lt;strong&gt;hdparm /dev/nvme0n1&lt;/strong&gt;

/dev/nvme0n1:
 HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
 readonly      =  0 (off)
 &lt;mark&gt;readahead     = 256 (on)&lt;/mark&gt;
 HDIO_DRIVE_CMD(identify) failed: Inappropriate ioctl for device
 geometry      = 953869/64/32, sectors = 1953525168, start = 0
&lt;/pre&gt;
&lt;p&gt;Ok, readahead is enabled, let’s turn it off for this device:&lt;/p&gt;
&lt;pre&gt;
$ sudo &lt;strong&gt;hdparm -a 0 /dev/nvme0n1&lt;/strong&gt;

/dev/nvme0n1:
 setting fs readahead to 0
 &lt;mark&gt;readahead     =  0 (off)&lt;/mark&gt;&lt;/pre&gt;
&lt;p&gt;And now let’s run the &lt;code&gt;onessd.sh&lt;/code&gt; benchmark again, with the same command as earlier:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ sudo ./onessd.sh 3 /dev/nvme0n1 1m
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At first, it looked like a small success, without readahead we were doing &lt;strong&gt;4000+ MiB/s&lt;/strong&gt; instead of ~3200 MiB/s and so.&lt;/p&gt;
&lt;p&gt;But…&lt;/p&gt;
&lt;pre&gt;
---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total-
run blk new|usr &lt;strong&gt;sys&lt;/strong&gt; idl wai stl| used  free  buf   cach| read  writ| &lt;strong&gt;read&lt;/strong&gt;  writ
 18  80   0|  0  &lt;mark&gt;21&lt;/mark&gt;   0  69   0|2025M   47G  197G  249M|1027k    0 |&lt;mark&gt;4011M&lt;/mark&gt;    0 
 13  82   0|  0  &lt;mark&gt;20&lt;/mark&gt;   0  69   0|2024M   43G  201G  248M|1026k    0 |&lt;mark&gt;4009M&lt;/mark&gt;    0 
 14  83   0|  0  &lt;mark&gt;21&lt;/mark&gt;   0  69   0|2024M   39G  205G  249M|1028k 54.0 |&lt;mark&gt;4016M&lt;/mark&gt;  252k
 15  79   0|  0  &lt;mark&gt;21&lt;/mark&gt;   0  69   0|2024M   35G  209G  249M|1027k    0 |&lt;mark&gt;4010M&lt;/mark&gt;    0 
 15  81   0|  0  &lt;mark&gt;20&lt;/mark&gt;   0  69   0|2025M   31G  213G  248M|1027k    0 |&lt;mark&gt;4011M&lt;/mark&gt;    0 

&lt;em&gt;... about 60 seconds of similar output removed ...&lt;/em&gt;

 40  51   0|  0  &lt;mark&gt;69&lt;/mark&gt;   0  27   0|2002M 1341M  241G  248M| 766k 14.0 |&lt;mark&gt;2993M&lt;/mark&gt;  692k
 65  22   0|  0  &lt;mark&gt;75&lt;/mark&gt;   0  21   0|2021M 1100M  241G  245M| 689k    0 |&lt;mark&gt;2692M&lt;/mark&gt;    0 
 19  67   0|  0  &lt;mark&gt;92&lt;/mark&gt;   0   7   0|2002M 1168M  241G  244M| 653k    0 |&lt;mark&gt;2550M&lt;/mark&gt;    0 
 21  76   0|  0  &lt;mark&gt;88&lt;/mark&gt;   0  10   0|2001M 1140M  241G  243M| 663k 2.00 |&lt;mark&gt;2591M&lt;/mark&gt;   12k
 44  39   0|  0  &lt;mark&gt;86&lt;/mark&gt;   0  11   0|2008M 1273M  241G  246M| 639k    0 |&lt;mark&gt;2496M&lt;/mark&gt;    0 
&lt;/pre&gt;
&lt;p&gt;… after running for ~30-35 seconds, the throughput dropped to &lt;strong&gt;3000+ MiB/s&lt;/strong&gt; again and kernel-mode CPU usage increased too. The perf CPU profile was following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-spinlock-contention-no-readahead.png&quot;/&gt; Linux kernel spinlock contention due to concurrent readaheads&lt;/p&gt;
&lt;p&gt;I highlighted a few items in the above screenshot with “talk bubbles”. If you compare the CPU time consumed by &lt;em&gt;_raw_spinlock_irq&lt;/em&gt; function vs. their parents/grandparents handling cached reads or LRU operations, you’ll see that the majority of time is spent in spinlock spinning and not the actual LRU/cached read function code. One question to ask would be, why are there so many worker threads, trying to concurrently work on my 16 core/32 thread machine and ending up with a spin storm. I have only 3 &lt;code&gt;fio&lt;/code&gt; processes accessing just one disk. With less concurrency, we’d get less spinlock contention on these pagecache LRU structures (hint: when bypassing pagecache using direct I/O, these threads won’t show up at all).&lt;/p&gt;
&lt;p&gt;Even though the previously seen &lt;em&gt;page_cache_readahead&lt;/em&gt; functions do not show up in kernel stack traces, we still see similar pagecache LRU replacement activity and spinlock contention. It makes sense as even without read-ahead, &lt;code&gt;fio&lt;/code&gt; is still issuing IOs as fast as it can. My hypothesis that the readahead functions (that clearly showed up in the perf profile) had something to do with this problem, was wrong. In fact, I went back to my previous test (with read-ahead enabled) and indeed, at first I got 6800+ MiB/s scan rates - they only dropped to 3000-something after a while!&lt;/p&gt;
&lt;p&gt;I had missed this detail at first. Even with buffered I/O mode, I got the full throughput out of the single disk (at a fairly high CPU overhead), but after a while the throughput dropped 2x, CPU usage jumped up 3-4x - and the page replacement related spinlock contention showed up in the stack. Incidentally, I have 256 GB of RAM in this machine that is not used for anything else and reading a disk for ~35 seconds at 6800 MiB gets close to 256 GB (of my RAM). And similarly, with readahead disabled, it took about 55-60 seconds at 4000 MiB/s (close to 256 GB of data read), until the pagecache LRU spinlock problem kicked in. Maybe a coincidence, but it was consistently reproducible - more research needed.&lt;/p&gt;
&lt;blockquote readability=&quot;15&quot;&gt;
&lt;p&gt;This is a good example of sudden spinlock contention showing up due to a behavior change somewhere higher in the code-path or state change in some underlying data structure, that in turn causes a behavior change. It is a feasible hypothesis that as long as there are enough unused pages in VM freelists or in the inactive area of the pagecache, kernel does not have to scan and discard pages from the LRU list, thus only allocating new pages require using spinlocks. But once you’ve exhausted the available memory (and inactive “cold” pages in the LRU), then every thread has to first search, discard pages from the LRU list, before adding them back as needed. This will suddenly cause a huge kernel CPU usage spike, even though “nothing has changed”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are various tracing options and &lt;code&gt;/proc/vmstat&lt;/code&gt; metrics that would help us to drill down further, but I won’t do it here for keeping this article “short”. Remember, all this CPU usage and spinning problem happens only when using reads via OS pagecache (&lt;code&gt;fio --direct=0&lt;/code&gt;) and we’ll go back to using direct I/O in next steps.&lt;/p&gt;
&lt;p&gt;If you want to &lt;strong&gt;avoid disk I/O&lt;/strong&gt;, use the built-in cache in your database engines and applications. They don’t have purpose-built caches, then use OS pagecache, if you expect it to be able to hold your “working set” of hot, frequently accessed data. But if you want to do &lt;strong&gt;efficient disk I/O&lt;/strong&gt;, use direct I/O via whatever application configuration settings that use the &lt;code&gt;O_DIRECT&lt;/code&gt; flag on file open.&lt;/p&gt;
&lt;h4 id=&quot;io-configuration---io-scheduler&quot;&gt;IO configuration - IO scheduler&lt;/h4&gt;
&lt;p&gt;Here’s another example of many things that may reduce your performance when doing millions of IOPS (or even only 100k) - I/O scheduler. This feature trades CPU time for hopefully better contained I/O latency for spinning disks. Every I/O request submission and handling will have extra CPU overhead, requests get inserted into sorted queues, timestamps are checked, hashes are computed, etc. When doing thousands of IOPS, you might not even notice any CPU overhead and on “dumb” spinning disks the investment will likely pay off.&lt;/p&gt;
&lt;p&gt;For NVME SSDs, the I/O scheduler is disabled by default - good! If you have uniform access latency regardless of data location (like SSDs provide) &lt;em&gt;or&lt;/em&gt; a storage array/controller with large enough queue depth, so that I/Os can be rearranged &amp;amp; scheduled at the storage layer, you won’t need I/O scheduling at the host level anyway. And at high IOPS rates, the CPU overhead will be noticeable, with no I/O benefit.&lt;/p&gt;
&lt;p&gt;Let’s test this anyway! The I/O scheduler is currently &lt;code&gt;none&lt;/code&gt; for all my SSDs:&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;grep . /sys/class/block/nvme*n1/queue/scheduler&lt;/strong&gt;
/sys/class/block/nvme0n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme1n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme2n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme3n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme4n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme5n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme6n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme7n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme8n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
/sys/class/block/nvme9n1/queue/scheduler:[&lt;mark&gt;none&lt;/mark&gt;] mq-deadline 
&lt;/pre&gt;
&lt;p&gt;Let’s enable the &lt;code&gt;mq-deadline&lt;/code&gt; (Multi-Queue deadline) scheduling option for my test disk:&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;echo mq-deadline | sudo tee -a /sys/class/block/nvme0n1/queue/scheduler&lt;/strong&gt;
mq-deadline

$ &lt;strong&gt;grep . /sys/class/block/nvme*n1/queue/scheduler&lt;/strong&gt;
/sys/class/block/nvme0n1/queue/scheduler:[&lt;mark&gt;mq-deadline&lt;/mark&gt;] none
/sys/class/block/nvme1n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme2n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme3n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme4n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme5n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme6n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme7n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme8n1/queue/scheduler:[none] mq-deadline 
/sys/class/block/nvme9n1/queue/scheduler:[none] mq-deadline 
&lt;/pre&gt;
&lt;p&gt;Rerun the previous &lt;code&gt;onessd.sh&lt;/code&gt; command with direct I/O enabled again and with 4kB I/O size, we get this:&lt;/p&gt;
&lt;pre&gt;
---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total-
&lt;strong&gt;run&lt;/strong&gt; blk new|usr sys idl wai stl| used  free  buf   cach| &lt;strong&gt;read&lt;/strong&gt;  writ| read  writ
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  4   8  90   0   0|2157M  248G   46M  730M| &lt;mark&gt;525k&lt;/mark&gt;    0 |2051M    0 
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  3   8  90   0   0|2157M  248G   46M  730M| &lt;mark&gt;523k&lt;/mark&gt;    0 |2041M    0 
&lt;mark&gt;4.0&lt;/mark&gt;   0   0|  4   8  91   0   0|2157M  248G   46M  730M| &lt;mark&gt;524k&lt;/mark&gt; 7.00 |2046M   52k
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  3   8  90   0   0|2157M  248G   46M  730M| &lt;mark&gt;532k&lt;/mark&gt; 21.0 |2079M   84k
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  3   8  90   0   0|2157M  248G   46M  730M| &lt;mark&gt;527k&lt;/mark&gt;    0 |2060M    0 
&lt;mark&gt;3.0&lt;/mark&gt;   0   0|  3   8  90   0   0|2157M  248G   46M  730M| &lt;mark&gt;526k&lt;/mark&gt;    0 |2056M    0 ^C
&lt;/pre&gt;
&lt;p&gt;The “run” column shows just 3 threads running on CPU most of the time, there’s no need for hundreds of kernel threads doing concurrent I/O for pagecache. pSnapper confirms that, too. Note that you don’t have to run pSnapper with sudo if you’re not reporting the “advanced” columns like syscall and kstack.&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;psn&lt;/strong&gt;

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/stat for 5 seconds... finished.


=== Active Threads ====================================================

 samples | avg_threads | comm                 | state                  
-----------------------------------------------------------------------
     300 |        &lt;mark&gt;3.00 | (fio)                | Running (ON CPU)&lt;/mark&gt;
       3 |        0.03 | (kworker/*:*-events) | Running (ON CPU)       
       1 |        0.01 | (jbd*/dm-*-*)        | Disk (Uninterruptible) 
&lt;/pre&gt;
&lt;p&gt;However, we only get ~525k IOPS out of these 3 processes running 100% on CPU, instead of 1150k IOPS seen earlier in my initial tests. So, with an I/O scheduler enabled, we get 2x less throughput with similar amount of CPU usage. In other, words - every I/O request seems to be using 2x more CPU now. This is not surprising, as the additional I/O scheduling magic needs to use CPU too.&lt;/p&gt;
&lt;p&gt;As usual, you can run perf or look into a &lt;a href=&quot;https://0x.tools&quot;&gt;0x.tools&lt;/a&gt; &lt;em&gt;always-on CPU profiling&lt;/em&gt; output file from around the right time:&lt;/p&gt;
&lt;pre&gt;
$ sudo perf report -i xcpu.&lt;strong&gt;202101272309&lt;/strong&gt;1293
&lt;/pre&gt;
&lt;p&gt;Partial output is below, I highlighted the interesting parts:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-io-scheduler.png&quot;/&gt; CPU overhead of delaying I/O request dispatch by I/O scheduler&lt;/p&gt;
&lt;p&gt;We are doing direct I/O, like the &lt;code&gt;blkdev_direct_IO&lt;/code&gt; function in the stack shows, but in the bottom of the output, you see a lot of “timer” and “delay” functions show up under &lt;code&gt;blk_mq_do_dispatch_sched&lt;/code&gt;. So, we will do extra CPU work for deciding whether an I/O request (that’s already submitted to the block device’s I/O queue in the OS) should be &lt;em&gt;dispatched&lt;/em&gt; to the actual hardware device driver that’s handling this disk.&lt;/p&gt;
&lt;p&gt;Ok, enough of this software stuff for now, as I mentioned I will write about performance of LVM, filesystems and databases in future posts, let’s move on.&lt;/p&gt;
&lt;h3 id=&quot;multi-disk-test&quot;&gt;Multi-disk test&lt;/h3&gt;
&lt;p&gt;We have been looking into just one disk’s performance so far! The hope is that now we have a better idea of theoretical max throughput of a single disk, when plenty of extra PCIe, inter-CPU-core and memory channel bandwidth was available. Now let’s put all 10 disks into use at the same time - and make sure that we have set them up right! I will show the command I used in the end of the post, let’s start with hardware setup &amp;amp; configuration.&lt;/p&gt;
&lt;h4 id=&quot;pcie-40-quad-ssd-adapter---asus-hyper-m2-x16-gen4-cardhttpswwwasuscomusmotherboard-accessorieshyper-m-2-x16-gen-4-card&quot;&gt;PCIe 4.0 Quad-SSD adapter - &lt;a href=&quot;https://www.asus.com/us/Motherboard-Accessories/HYPER-M-2-X16-GEN-4-CARD/&quot;&gt;Asus Hyper M.2 X16 Gen4 Card&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The workstation had only 2 existing M.2 NVMe slots - and I deliberately stayed away from buying SATA SSDs as the SATA interface is completely different (and wayyy slower) than using direct PCIe-NVMe protocol. Lenovo sells quad M.2 SSD adapters that go into PCIe x16 slots, but the one they currently have supports only PCIe3.0 as far as I know.&lt;/p&gt;
&lt;p&gt;I was happy to find that ASUS was already selling a PCIe4.0 capable adapter card that fits into a PCIe x16 slot and holds 4 x M.2 SSDs. All this for just $70. I had to buy two of these. 2 x 4 SSDs went inside these ASUS adapter cards and the remaining 2 SSDs went directly into the existing M.2 slots built in to the Lenovo machine.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-pci-quad-ssds.jpg&quot; alt=&quot;Quad SSD PCIe Adapter Cards&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Should you want to buy similar adapters, make sure you buy the “Gen 4” version from ASUS if you need PCIe 4.0 compatibility. Or when buying a different brand, you don’t want to get an adapter that has a built-in PCIe &lt;em&gt;switch&lt;/em&gt; in it, but a card (like ASUS) that supports the motherboard’s PCIe lane bifurcation directly (see below). Otherwise, you may end up with more latency and more potential bottlenecks, if the switch doesn’t support full PCIe 4.0 speeds.&lt;/p&gt;
&lt;h4 id=&quot;bios-settings&quot;&gt;BIOS Settings&lt;/h4&gt;
&lt;p&gt;I had to make two changes in BIOS. First, I had to manually configure the &lt;strong&gt;PCIe Port Bifurcation&lt;/strong&gt; from Auto to &lt;strong&gt;x4x4x4x4&lt;/strong&gt; (other options were x16 and x8x8), as the BIOS didn’t automatically realize that I had 4 separate PCIe x4 devices plugged into that physical x16 slot (no disks showed up at all). Perhaps the auto-detection didn’t work as I wasn’t using the Lenovo’s official Quad SSD adapter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-pci-link-speed.jpg&quot; alt=&quot;BIOS PCI port bifurcation and link speed&quot;/&gt;&lt;/p&gt;
&lt;p&gt;I also had to change the &lt;strong&gt;Link Speed&lt;/strong&gt; from Auto to &lt;strong&gt;Gen 4 (16 GT/s)&lt;/strong&gt;, as otherwise BIOS defaulted the speed to PCIe 3.0 / Gen 3 and I got only half of the throughput out of the SSDs behind these Quad SSD adapters.&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;lspci -v&lt;/strong&gt;
...
64:00.0 Non-Volatile memory controller: Samsung Electronics Co Ltd Device a80a (prog-if 02 [NVM Express])
        Subsystem: Samsung Electronics Co Ltd Device a801
        Flags: bus master, fast devsel, latency 0, IRQ 69, NUMA node 0, IOMMU group 16
        Memory at cc000000 (64-bit, non-prefetchable) [size=16K]
        Capabilities: [40] Power Management version 3
        Capabilities: [50] MSI: Enable- Count=1/32 Maskable- 64bit+
        Capabilities: [70] Express Endpoint, MSI 00
        Capabilities: [b0] MSI-X: Enable+ Count=130 Masked-
        Capabilities: [100] Advanced Error Reporting
        Capabilities: [168] Alternative Routing-ID Interpretation (ARI)
        Capabilities: [178] Secondary PCI Express
        Capabilities: [198] &lt;mark&gt;Physical Layer 16.0 GT/s ?&amp;gt;
&lt;/mark&gt;
        Capabilities: [1bc] Lane Margining at the Receiver ?&amp;gt;

        Capabilities: [214] Latency Tolerance Reporting
        Capabilities: [21c] L1 PM Substates
        Capabilities: [3a0] Data Link Feature ?&amp;gt;

        Kernel driver in use: nvme
        Kernel modules: nvme
...
&lt;/pre&gt;
&lt;p&gt;After enabling 16 GT/s from BIOS, I was able to get 6800 MiB/s read throughput from a single disk, yay! But…&lt;/p&gt;
&lt;h4 id=&quot;pcie-root-complex-bottleneck&quot;&gt;PCIe Root Complex Bottleneck&lt;/h4&gt;
&lt;p&gt;I had physically installed the Quad SSD adapters like this (the adapter cards have a silver/aluminum casing):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-pci-before.jpg&quot; alt=&quot;PCI card placement before&quot;/&gt;&lt;/p&gt;
&lt;p&gt;However, when running a throughput test with all disks and 1 MB read sizes, I got only about 50 GiB/s aggregate throughput. Pretty good, but 10 SSDs should have given up to 68 GiB/s, judging from their specs and individual single-disk tests. There must have been some bottleneck between the disks and my CPU.&lt;/p&gt;
&lt;p&gt;Since multiple PCIe slots may be sharing a PCIe &lt;a href=&quot;https://en.wikipedia.org/wiki/Root_complex&quot;&gt;root complex&lt;/a&gt; and no component in the CPU-IO network topology has infinite bandwidth, I was wondering whether I had accidentally installed the SSD adapter cards under the same PCIe root complex. Modern CPUs can have separate sets of PCIe lanes connected to different “edges” of the CPU, similar to memory channels. So, if you place your PCIe cards suboptimally, you may end up saturating some PCIe lanes (going into the same CPU), while others have pretty low traffic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-pcie-topology.svg&quot;/&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Root_complex&quot;&gt;https://en.wikipedia.org/wiki/Root_complex&lt;/a&gt; [&lt;a href=&quot;https://creativecommons.org/licenses/by-sa/4.0/&quot;&gt;CC BY-SA 4.0&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;Let’s see how many PCIe root complexes (PCIe lane sets connecting to the CPU) my machine has:&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;lspci | grep Root&lt;/strong&gt;
00:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex
20:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex
40:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex
60:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex 
&lt;/pre&gt;
&lt;p&gt;So, 4 PCIe root complexes, each having 32 lanes (I’m assuming equal distribution, the system has 128 lanes in total).&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;lspci&lt;/code&gt; command shows the PCIe network topology (click on the triangle below to expand). I have removed some unused device entries from the list. Also, I made the root complex nodes &lt;strong&gt;bold&lt;/strong&gt; and the SSD devices &lt;mark&gt;yellow&lt;/mark&gt;.&lt;/p&gt;
&lt;details readability=&quot;22.5&quot;&gt;View lspci output (initial)
&lt;div readability=&quot;50&quot;&gt;
&lt;pre&gt;
$ lspci -tv
&lt;strong&gt;-+-[0000:60]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-03.1-[61]--+-00.0  NVIDIA Corporation GP107GL [Quadro P620]
 |           |            \-00.1  NVIDIA Corporation GP107GL High Definition Audio Controller
 |           +-07.1-[62]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[63]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
&lt;strong&gt; +-[0000:40]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           &lt;mark&gt;+-01.1-[41]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-01.2-[42]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-01.3-[43]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-01.4-[44]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           &lt;mark&gt;+-03.1-[45]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.2-[46]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.3-[47]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.4-[48]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-07.1-[49]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[4a]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
&lt;strong&gt; +-[0000:20]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           &lt;mark&gt;+-01.1-[21]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-01.2-[22]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-07.1-[23]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[24]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
 |                        +-00.1  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Cryptographic Coprocessor PSPCPP
 |                        \-00.3  Advanced Micro Devices, Inc. [AMD] Starship USB 3.0 Host Controller
&lt;strong&gt; \-[0000:00]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
             +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
             +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-01.1-[01]----00.0  Aquantia Corp. AQC107 NBase-T/IEEE 802.3bz Ethernet Controller [AQtion]
             +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-03.1-[02-06]----00.0-[03-06]--+-08.0-[04]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
             |                               |            +-00.1  Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller
             |                               |            \-00.3  Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller
             |                               +-09.0-[05]----00.0  Advanced Micro Devices, Inc. [AMD] FCH SATA Controller [AHCI mode]
             |                               \-0a.0-[06]----00.0  Advanced Micro Devices, Inc. [AMD] FCH SATA Controller [AHCI mode]
             +-07.1-[07]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
             +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-08.1-[08]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
             |            \-00.3  Advanced Micro Devices, Inc. [AMD] Starship USB 3.0 Host Controller
             +-14.0  Advanced Micro Devices, Inc. [AMD] FCH SMBus Controller
             +-14.3  Advanced Micro Devices, Inc. [AMD] FCH LPC Bridge
             \-18.7  Advanced Micro Devices, Inc. [AMD] Starship Device 24; Function 7
&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;&lt;p&gt;Indeed, I had accidentally placed both quad SSD cards into two PCIe slots that were connected to the same PCIe root complex (and connection to the CPU). When setting things up, I didn’t bother diving deep into this workstation’s documentation and got unlucky. The two additional Samsung devices under &lt;strong&gt;0000:20&lt;/strong&gt; are the SSDs that I plugged into the M.2 slots already built in to the workstation.&lt;/p&gt;
&lt;h4 id=&quot;pcie-root-complex-bottleneck-after&quot;&gt;PCIe Root Complex Bottleneck (after)&lt;/h4&gt;
&lt;p&gt;From the &lt;code&gt;lspci&lt;/code&gt; output above you can see that my NVIDIA GPU is under the PCIe address &lt;strong&gt;0000:60&lt;/strong&gt; and there are no other I/O hungry cards there. Since I’m currently not using the GPU on that machine, I just swapped one of the Quad SSD adapters with the “mostly idle” video card. Now each of the quad SSD adapters can feed their data through separate PCIe root complexes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/iops-pci-after.jpg&quot; alt=&quot;PCI card placement after&quot;/&gt;&lt;/p&gt;
&lt;details readability=&quot;23&quot;&gt;View lspci output (after moving cards around)
&lt;div readability=&quot;51&quot;&gt;
&lt;pre&gt;
$ sudo lspci -tv
&lt;strong&gt;-+-[0000:60]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           &lt;mark&gt;+-03.1-[61]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.2-[62]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.3-[63]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.4-[64]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-07.1-[65]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[66]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
&lt;strong&gt; +-[0000:40]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-01.1-[41]--+-00.0  NVIDIA Corporation GP107GL [Quadro P620]
 |           |            \-00.1  NVIDIA Corporation GP107GL High Definition Audio Controller
 |           +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           &lt;mark&gt;+-03.1-[42]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.2-[43]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.3-[44]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-03.4-[45]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-07.1-[46]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[47]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
&lt;strong&gt; +-[0000:20]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
 |           +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
 |           +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           &lt;mark&gt;+-01.1-[21]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           &lt;mark&gt;+-01.2-[22]----00.0  Samsung Electronics Co Ltd Device a80a&lt;/mark&gt;
 |           +-07.1-[23]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
 |           +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
 |           \-08.1-[24]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
 |                        +-00.1  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Cryptographic Coprocessor PSPCPP
 |                        \-00.3  Advanced Micro Devices, Inc. [AMD] Starship USB 3.0 Host Controller
&lt;strong&gt; \-[0000:00]-+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Root Complex&lt;/strong&gt;
             +-00.2  Advanced Micro Devices, Inc. [AMD] Starship/Matisse IOMMU
             +-01.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-01.1-[01]----00.0  Aquantia Corp. AQC107 NBase-T/IEEE 802.3bz Ethernet Controller [AQtion]
             +-02.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-03.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-03.1-[02-06]----00.0-[03-06]--+-08.0-[04]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
             |                               |            +-00.1  Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller
             |                               |            \-00.3  Advanced Micro Devices, Inc. [AMD] Matisse USB 3.0 Host Controller
             |                               +-09.0-[05]----00.0  Advanced Micro Devices, Inc. [AMD] FCH SATA Controller [AHCI mode]
             |                               \-0a.0-[06]----00.0  Advanced Micro Devices, Inc. [AMD] FCH SATA Controller [AHCI mode]
             +-07.1-[07]----00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Function
             +-08.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse PCIe Dummy Host Bridge
             +-08.1-[08]--+-00.0  Advanced Micro Devices, Inc. [AMD] Starship/Matisse Reserved SPP
             |            \-00.3  Advanced Micro Devices, Inc. [AMD] Starship USB 3.0 Host Controller
             +-14.0  Advanced Micro Devices, Inc. [AMD] FCH SMBus Controller
             +-14.3  Advanced Micro Devices, Inc. [AMD] FCH LPC Bridge
             \-18.7  Advanced Micro Devices, Inc. [AMD] Starship Device 24; Function 7
&lt;/pre&gt;&lt;/div&gt;
&lt;/details&gt;&lt;p&gt;So, now both of the heavy-hitting quad SSD cards have their own PCIe root complex to communicate through. The NVIDIA Quadro P620 video card shares the same PCIe root complex with one quad SSD adapter, but since I’m not using the GPU in this machine for anything, I don’t expect it to eat significant bandwidth.&lt;/p&gt;
&lt;h3 id=&quot;final-results&quot;&gt;Final results&lt;/h3&gt;
&lt;p&gt;So, here are the promised results! Let’s execute 10 &lt;code&gt;fio&lt;/code&gt; commands concurrently (the script is listed below):&lt;/p&gt;
&lt;pre&gt;
$ sudo &lt;strong&gt;./allmulti.sh&lt;/strong&gt; 4k

[10] + Running                   
[9] - Running                    
[8]   Running                    
[7]   Running                    
[6]   Running                    
[5]   Running                    
[4]   Running                    
[3]   Running                    
[2]   Running                    
[1]   Running                    
&lt;/pre&gt;
&lt;p&gt;We get up to &lt;strong&gt;11 million IOPS&lt;/strong&gt; with 4k reads (while CPUs are 100% busy, mostly in kernel mode):&lt;/p&gt;
&lt;pre&gt;
$ &lt;strong&gt;dstat -pcmrd&lt;/strong&gt;

---procs--- ----total-usage---- ------memory-usage----- &lt;strong&gt;--io/total- -dsk/total-&lt;/strong&gt;
run blk new|usr sys idl wai stl| used  free  buf   cach| read  writ| read  writ
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 33   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 33   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.0M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.0M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 28  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  72   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
 32   0   0| 27  73   0   0   0|2232M  249G   61M  568M|&lt;mark&gt;11.1M&lt;/mark&gt;    0 |  &lt;mark&gt;42G&lt;/mark&gt;    0
&lt;/pre&gt;
&lt;p&gt;Since issuing so many block I/Os on my 16-core CPU keeps it 100% busy, I had to find other tricks to reduce the CPU usage of &lt;code&gt;fio&lt;/code&gt; and I/O handling layer in general. This is why I’m running 10 separate &lt;code&gt;fio&lt;/code&gt; commands (one for each disk) as when using one single &lt;code&gt;fio&lt;/code&gt; instance accessing all 10 disks, I couldn’t get beyond some measly 8M IOPS while using 100% CPU. Possibly there’s some internal coordination overhead in &lt;code&gt;fio&lt;/code&gt;, but I didn’t drill down into that, running separate &lt;code&gt;fio&lt;/code&gt; instances was an acceptable workaround:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;allmulti.sh&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;
./onessd.sh 3 /dev/nvme0n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme1n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme2n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme3n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme4n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme5n1 $1 &amp;amp; 
./onessd.sh 3 /dev/nvme6n1 $1 &amp;amp;
./onessd.sh 3 /dev/nvme7n1 $1 &amp;amp;
./onessd.sh 4 /dev/nvme8n1 $1 &amp;amp;
./onessd.sh 4 /dev/nvme9n1 $1 &amp;amp;
jobs
wait
&lt;/pre&gt;
&lt;p&gt;At 4k I/O size, I also set &lt;code&gt;fio --hipri&lt;/code&gt; option as this does not rely on interrupts to get notified about I/O completions, instead if polls for completion in a busy-loop on CPU. Since my CPUs were 100% busy under this load anyway, then not having to handle interrupts and go “straight to the source”, polling I/O states from NVMe queues via direct device-mapped memory reads. This kind of busy-loop polling &lt;em&gt;on all CPU cores&lt;/em&gt; is probably not practical for real-life workloads, but for real-life workloads I wouldn’t be so cheap and would go buy a bigger CPU.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;onessd.sh&lt;/code&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;pre&gt;
$ cat onessd.sh 
#!/bin/bash

[ $# -ne 3 ] &amp;amp;&amp;amp; echo Usage $0 numjobs /dev/DEVICENAME BLOCKSIZE &amp;amp;&amp;amp; exit 1

fio --readonly --name=onessd \
    --filename=$2 \
    --filesize=900g --rw=randread --bs=$3 --direct=1 --overwrite=0 \
    --numjobs=$1 --iodepth=32 --time_based=1 --runtime=3600 \
    --ioengine=io_uring \
    --registerfiles --fixedbufs &lt;mark&gt;--hipri&lt;/mark&gt; \
    &lt;mark&gt;--gtod_reduce=1&lt;/mark&gt; --group_reporting --minimal
&lt;/pre&gt;
&lt;p&gt;For final tests, I even disabled the frequent &lt;code&gt;gettimeofday&lt;/code&gt; system calls that are used for I/O latency measurement. &lt;em&gt;Less CPU usage = More IOPS&lt;/em&gt; when you’re bottlenecked by CPU capacity.&lt;/p&gt;
&lt;p&gt;I removed the &lt;code&gt;--fixedpri&lt;/code&gt; option in following tests, as with larger I/O sizes, the bottleneck is in waiting for the NVMe device’s DMA engine that in turn waits for the disk -&amp;gt; RAM memory copy to finish (over the PCIe network to memory channels to memory DIMMs). I’d rather let my application threads to go sleep and be waken up later by OS scheduler who receives an interrupt, so some CPU time would be available for application processing too!&lt;/p&gt;
&lt;p&gt;In my earlier tests, 1M I/Os were split to 512k apparently (or perhaps &lt;code&gt;fio&lt;/code&gt; didn’t request higher I/O sizes itself), I’ll run a 512k I/O size test:&lt;/p&gt;
&lt;pre&gt;
$ sudo ./allmulti.sh &lt;strong&gt;512k&lt;/strong&gt;
...

$ sudo dstat -pmrd --bw
---procs--- ------memory-usage----- --io/total- -dsk/total-
run blk new| used  free  buf   cach| read  writ| read  writ
2.0   0   0|2402M  249G   47M  427M| &lt;mark&gt;138k&lt;/mark&gt;    0 |  &lt;mark&gt;66G&lt;/mark&gt;    0 
1.0   0   0|2402M  249G   47M  427M| &lt;mark&gt;138k&lt;/mark&gt;    0 |  &lt;mark&gt;66G&lt;/mark&gt;    0 
1.0   0   0|2402M  249G   47M  427M| &lt;mark&gt;138k&lt;/mark&gt;    0 |  &lt;mark&gt;66G&lt;/mark&gt;    0 
1.0   0   0|2402M  249G   47M  427M| &lt;mark&gt;138k&lt;/mark&gt;    0 |  &lt;mark&gt;66G&lt;/mark&gt;    0 
3.0   0   0|2402M  249G   47M  427M| &lt;mark&gt;138k&lt;/mark&gt; 3.00 |  &lt;mark&gt;66G&lt;/mark&gt;   20k
&lt;/pre&gt;
&lt;p&gt;We are now doing 138k IOPS with 512kB block size, this translates to &lt;strong&gt;66 GiB/s&lt;/strong&gt; throughput (71 GB/s when reporting the units correctly). And look at the CPU usage - my total CPU utilization is only around &lt;strong&gt;7.5%&lt;/strong&gt;!&lt;/p&gt;
&lt;pre&gt;
$ sudo mpstat 1
Linux 5.8.0-29-generic (linux02)        01/28/2021      _x86_64_        (32 CPU)

01:31:25 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft   %idle
01:31:26 PM  all    0.64    0.00    6.68    0.00    0.00    0.13   &lt;mark&gt;92.55&lt;/mark&gt;  
01:31:27 PM  all    0.74    0.00    6.85    0.00    0.00    0.19   &lt;mark&gt;92.21&lt;/mark&gt;
01:31:28 PM  all    0.55    0.00    6.65    0.00    0.00    0.23   &lt;mark&gt;92.57&lt;/mark&gt;
01:31:29 PM  all    0.58    0.00    6.71    0.00    0.00    0.19   &lt;mark&gt;92.52&lt;/mark&gt;
01:31:30 PM  all    0.58    0.00    6.97    0.00    0.00    0.03   &lt;mark&gt;92.42&lt;/mark&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Note that I used &lt;code&gt;mpstat&lt;/code&gt; for CPU reporting in this case, I hit some sort of &lt;code&gt;dstat&lt;/code&gt; bug that didn’t report CPU correctly in this case. I possibly happens due to this Ubuntu kernel not being configured to break out interrupt reporting separately (I will write about this in the future).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The full &lt;code&gt;iostat&lt;/code&gt; output is here too (it’s pretty wide).&lt;/p&gt;
&lt;p&gt;This is from the 4k &lt;em&gt;maximize IOPS&lt;/em&gt; test:&lt;/p&gt;
&lt;pre&gt;
Device            r/s     rMB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wMB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dMB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1       &lt;mark&gt;1090190.40&lt;/mark&gt;   4258.56     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   19.40 100.16
nvme1n1       &lt;mark&gt;1062752.40&lt;/mark&gt;   4151.38     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   17.47 100.00
nvme2n1       &lt;mark&gt;1103848.80&lt;/mark&gt;   4311.91     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   21.18 100.00
nvme3n1       &lt;mark&gt;1111700.20&lt;/mark&gt;   4342.58     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   25.17 100.16
nvme4n1       &lt;mark&gt;1101897.20&lt;/mark&gt;   4304.29     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   22.81 100.48
nvme5n1       &lt;mark&gt;1105498.00&lt;/mark&gt;   4318.35     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   24.45 100.00
nvme6n1       &lt;mark&gt;1046649.00&lt;/mark&gt;   4088.47     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   16.97 100.48
nvme7n1       &lt;mark&gt;1061635.00&lt;/mark&gt;   4147.01     0.00   0.00    0.02     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00   17.83 100.00
nvme8n1       &lt;mark&gt;1134761.40&lt;/mark&gt;   4432.69     6.00   0.00    0.35     4.00   20.20      0.11     1.00   4.72    0.03     5.82    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  400.48 100.64
nvme9n1       &lt;mark&gt;1128532.40&lt;/mark&gt;   4408.33     0.00   0.00    0.37     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  415.12 100.32
&lt;/pre&gt;
&lt;p&gt;Probably all disks above would have been able to deliver over 1.1M IOPS each, but my CPUs just couldn’t issue &amp;amp; handle I/O requests fast enough!&lt;/p&gt;
&lt;p&gt;This is from a test where I used 64kB/s I/O sizes (8 x lower IO size compared to 4k, thus 8 x less IOPS). This test was able to saturate the 66 GiB/s max disk throughput, yet do 1M IOPS in aggregate &lt;em&gt;and&lt;/em&gt; have 75% of CPU time free for application work!&lt;/p&gt;
&lt;pre&gt;
Device            r/s     rMB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wMB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dMB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1       108722.00   &lt;mark&gt;6795.12&lt;/mark&gt;     0.00   0.00    3.08    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  335.35 100.00
nvme1n1       108723.20   &lt;mark&gt;6795.20&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  336.12 100.00
nvme2n1       108711.40   &lt;mark&gt;6794.46&lt;/mark&gt;     0.00   0.00    3.08    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  335.34 100.00
nvme3n1       108719.00   &lt;mark&gt;6794.94&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  335.49 100.00
nvme4n1       108720.40   &lt;mark&gt;6795.02&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  335.48 100.00
nvme5n1       108721.00   &lt;mark&gt;6795.06&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  336.16 100.00
nvme6n1       108722.60   &lt;mark&gt;6795.16&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  336.41 100.00
nvme7n1       108722.00   &lt;mark&gt;6795.12&lt;/mark&gt;     0.00   0.00    3.09    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  336.17 100.00
nvme8n1       108621.40   &lt;mark&gt;6788.84&lt;/mark&gt;     0.00   0.00    4.14    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  450.22 100.00
nvme9n1       108738.80   &lt;mark&gt;6796.17&lt;/mark&gt;     0.00   0.00    4.14    64.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00  450.09 100.00
&lt;/pre&gt;
&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Modern machines are ridiculously fast!&lt;/em&gt;&lt;/strong&gt; You just have to build, configure and use them right. Modern SSDs are super-fast, the PCIe 4.0/CPU/RAM network has very high throughput if you have enough PCIe lanes, memory channels (with enough memory banks populated with DIMMs) &lt;em&gt;and&lt;/em&gt; when there’s no CPU- or front-side-bus-imposed data transfer bottleneck (in some IO hub in your old-school architecture).&lt;/p&gt;
&lt;p&gt;As I mentioned in the beginning, having 128 x PCIe 4.0 lanes is what makes this ridiculous I/O throughput (for a desktop workstation!) possible and the 8 memory channels should make a difference when I actually start using the blocks of data just read from disk.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Computers are networks.&lt;/li&gt;
&lt;li&gt;PCIe, memory channel networks are just one part of the story.&lt;/li&gt;
&lt;li&gt;Modern CPUs have networks &lt;em&gt;within them&lt;/em&gt;, and their layout &amp;amp; capabilities differ by vendor and generation!&lt;/li&gt;
&lt;li&gt;No network has infinite bandwidth and zero latency.&lt;/li&gt;
&lt;li&gt;Disk I/O used to be the slow part of computing …&lt;/li&gt;
&lt;li&gt;Today the bottleneck is CPU and &lt;em&gt;feeding that CPU&lt;/em&gt; via multiple levels of networks inside your computer.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Thanks for reading all the way to the end! There will be more posts where I put the same machine into use using various database workloads, including heavy writes, different filesystems - and maybe even throw a GPU in the mix!&lt;/p&gt;
&lt;h3 id=&quot;more-stuff&quot;&gt;More stuff!&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;You can &lt;a href=&quot;https://tanelpoder.com/contact&quot;&gt;subscribe&lt;/a&gt; to email updates or &lt;a href=&quot;https://tanelpoder.com/contact/&quot;&gt;follow me&lt;/a&gt; here.&lt;/li&gt;
&lt;li&gt;You can &lt;a href=&quot;https://attendee.gotowebinar.com/register/3772989956528795149&quot;&gt;sign up&lt;/a&gt; for a “Hacking Session” webinar on this same topic (Thursday, 4. February 12pm-13:30pm ET)&lt;/li&gt;
&lt;li&gt;HackerNews &lt;a href=&quot;https://news.ycombinator.com/item?id=25956670&quot;&gt;discussion&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;further-reading&quot;&gt;Further reading&lt;/h3&gt;
&lt;hr/&gt;&lt;div&gt;&lt;span&gt;&lt;a href=&quot;https://tanelpoder.com/about/&quot;&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/tanelpoder_small.png&quot;/&gt;&lt;/a&gt;&lt;/span&gt;

&lt;/div&gt;
&lt;hr/&gt;&lt;footer&gt;&lt;hr/&gt;
© &lt;a href=&quot;https://tanelpoder.com/&quot;&gt;Tanel Põder&lt;/a&gt; 2007-2021&lt;/footer&gt;&lt;/body&gt;</description>
<pubDate>Fri, 29 Jan 2021 12:45:35 +0000</pubDate>
<dc:creator>tanelpoder</dc:creator>
<og:title>Achieving 11M IOPS &amp; 66 GB/s IO on a Single ThreadRipper Workstation - Tanel Poder Consulting</og:title>
<og:description>TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right! Introduction In this post I&amp;rsquo;ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve 11M IOPS with 4kB random reads and 66 GiB/s throughput with larger IOs - and what bottlenecks &amp;amp; issues I fixed to get there. - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp; training.</og:description>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/</dc:identifier>
</item>
<item>
<title>Factorio 1.1 stable</title>
<link>https://factorio.com/blog/post/fff-364</link>
<guid isPermaLink="true" >https://factorio.com/blog/post/fff-364</guid>
<description>&lt;h3&gt;1.1 stable &lt;span&gt;kovarex&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Hello, we have a stable version!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-1-1-stable-postcard.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-1-1-stable-postcard.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When we were releasing the &lt;a href=&quot;https://www.factorio.com/blog/post/fff-360&quot;&gt;1.0 FFF-360&lt;/a&gt;, we actually stated that there were &lt;em&gt;&quot;around 150 bugs on the forums and around 80 internal tasks to be solved&quot;&lt;/em&gt;. These were obviously minor issues, things hard to reproduce or very rare problems. In other words, it was quite reasonably stable, which normally goes without saying when it comes to Factorio stable versions. But it proved to be a mistake wording it this way, since some media picked up on it and presented it as a &quot;fairly bugged release&quot;.&lt;/p&gt;
&lt;p&gt;So I'm pretty thrilled to finally get to the point, where we actually have 0 known issues and 0 active bug reports on the forums. Its like cleaning the kitchen properly, so you can start cooking something fresh. More about that next week!&lt;/p&gt;
&lt;p&gt;For now, we want to go over some of the features of the 1.1 that you might have missed until now if you've been sticking with stable 1.0.&lt;/p&gt;
&lt;hr /&gt;&lt;h3&gt;Blueprint flip &lt;span&gt;Klonan&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;It was requested a lot over the years, and we always said no. The reason being, well not everything can flip, things like oil recipes, rail signals, pumpjacks etc. Due to these asymmetries, if you flipped a blueprint with these entities, the result would be non-functional in the best case, and cause complete chaos in others.&lt;/p&gt;
&lt;p&gt;But then kovarex was playing and &lt;em&gt;really really&lt;/em&gt; wanted to just copy and flip his train unloading setup (just your typical Inserters, chests, and belts). So he decided to just add it, and prevented the flip problems by just disallowing flipping blueprints that wouldn't flip properly.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Mp4 playback not supported on your device.&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;&lt;h3&gt;Spidertron control &lt;span&gt;Klonan&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;We added a few last minute nice to have features to the Spiders for 1.1. This is in addition to the things we demonstrated in &lt;a href=&quot;https://factorio.com/blog/post/fff-362&quot;&gt;FFF-362&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Spider waypoints&lt;/h4&gt;
&lt;p&gt;It was super frustrating trying to navigate our quite watery playtesting map with the spiders. They would always get caught on the edge of some lake. Queuing move commands seemed like a pretty obvious fix, and it wasn't much work in the end.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Mp4 playback not supported on your device.&lt;/p&gt;
&lt;/div&gt;
&lt;h4&gt;Spider follow command &lt;span&gt;Klonan&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Another frustration was trying to control multiple spiders in combat. The remote works fine for a handful of spiders, but once you get into the double digits, managing the spider remotes and spiders was just a hassle.&lt;/p&gt;
&lt;p&gt;So alongside the waypoints, kovarex added the feature of letting spiders follow entities... and even other spiders...&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Mp4 playback not supported on your device.&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;&lt;h3&gt;Smart belt building &lt;span&gt;Klonan&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;With the Lane locked belt building added in 1.1.0, there was some wonky/undefined behavior related to rotating the belt while building.&lt;/p&gt;
&lt;p&gt;So we fixed the bug by extending the feature set of belt building, with the new 'Smart belt building'.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Mp4 playback not supported on your device.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And with a great feature, we give ourselves the oppourtunity to do a little feature creep. So we added the anti-frustration feature of the automatic underground belt traversal.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Mp4 playback not supported on your device.&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;&lt;h3&gt;Multithreaded belts &lt;span&gt;boskid&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;In the constant battle with the optimizations to let players build bigger bases, quite often we do small changes that give performance gains in the range of 1-2%. Recently I started looking into the possibility of multithreading the transport belt logic. This is one of the more significant performance drains and it is still being done single threaded, resulting in a huge potential performance gain.&lt;/p&gt;
&lt;p&gt;Transport belts were already quite optimized (&lt;a href=&quot;https://www.factorio.com/blog/post/fff-176&quot;&gt;FFF-176&lt;/a&gt;). One could simply say &quot;make it multithreaded&quot; but there are a lot of technical challenges to solve before this is even possible in order to not cause any issues when running in multiplayer. Basically, to ensure it will still be deterministic.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot;&gt;
&lt;div class=&quot;panel-inset&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-mtb-1-update-order-matters.png&quot; /&gt;&lt;p&gt;Fig. 1&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Trying to do naive multi-threading with all the transport lines being updated at the same time doesn't work. In &lt;em&gt;Fig. 1&lt;/em&gt;: depending on which transport line would be updated first, the copper plate could land before or after the iron plate. This could be fixed by using all sorts of synchronization primitives, but using them would increase the complexity of the code to the point where nobody would want to maintain it, and mainly it would not guarantee the code would be faster or would not desync.&lt;/p&gt;
&lt;h4&gt;Transport line groups&lt;/h4&gt;
&lt;p&gt;After some observations I noticed a trivial fact: not all transport lines can interact with each other.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot;&gt;
&lt;div class=&quot;panel-inset&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-mtb-2-independent-groups.png&quot; /&gt;&lt;p&gt;Fig. 2&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;em&gt;Fig. 2&lt;/em&gt;, the left transport line on the horizontal belts can never receive items from the right line of the horizontal belts or any items from the side-loading. This gave me a simple nice idea. We can group transport lines in a way, that lines from one group don't interact with any line from any other group. This means, we can avoid all the synchronization logic between transport lines. When one thread updates one group, other groups can be updated by other threads independently at the same time.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;panel-inset&quot; readability=&quot;7&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-belt-lines.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-belt-lines.png&quot; /&gt;&lt;/a&gt;
&lt;p&gt;Each color is a different group of transport lines. Each group is updated in parallel.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Maintaining those groups in itself was a challenge: when a player builds or rotates a transport belt, underground belt, or splitter, transport lines will change their connections and some groups may now be connected, and they need to be merged. The opposite is also true: the player is able to remove some connections making the group contain multiple components which can be extracted to separate groups. While groups merging has to be done immediately (having multiple groups being able to interact with each other would create race conditions), splitting does not.&lt;/p&gt;
&lt;h4&gt;Wake-up lists&lt;/h4&gt;
&lt;p&gt;By far the largest issue we faced was that the transport lines aren't entirely independent. We have a sleep/wakeup mechanism in the game since version 0.9 and it goes like this: Inserters taking from belts may become inactive when there is nothing happening on both transport lines to save UPS. An inactive inserter would be stuck on its own in that position if there would not be any way to wake it up. In this case, the Inserter registers on the transport lines so that when a new item comes to it, it can be woken up to check if it needs to pick up any items.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-mtb-5-wakeup-lists.gif&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Since the 2 transport lines going through a single belt can belong to different groups, those groups are not independent because both could try to wakeup/put to sleep the same inserter. In that case, the inserter becomes the shared state for both groups and has to be properly dealt with. Transport lines cannot simply wakeup that inserter, because it could be woken up by another thread, and activation order is important as it also defines the order in which Inserters will be updated. If Inserters would activate in a different order, desyncs would happen.&lt;/p&gt;
&lt;p&gt;To solve this, when working, the threads don't wake the entities immediately, but instead add the wakeup requests to a list to be processed later. After all threads are done, the main thread collects and merges all those requests, and wakes up the entities in a deterministic way based on the group update order number.&lt;/p&gt;
&lt;p&gt;With the wakeup lists and other similar cases handled properly, I started comparing some belt based megabases which are capable of producing 10k science per minute and I noticed that transport belt update times dropped from 4ms to 1.6ms which in total update time gives between 20 to 40% overall performance gain.&lt;/p&gt;
&lt;hr /&gt;&lt;h3&gt;New Train Overview GUI &lt;span&gt;Klonan&lt;/span&gt;&lt;/h3&gt;
&lt;h4&gt;The beginning&lt;/h4&gt;
&lt;p&gt;The original Trains GUI was added in 0.13, and only had cosmetic changes all the way to 1.1.0.&lt;/p&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;panel-inset&quot; readability=&quot;7&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-013.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-013.png&quot; /&gt;&lt;/a&gt;
&lt;p&gt;Trains GUI in 0.13 (First release).&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;div class=&quot;panel-inset p8&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-017.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-017.png&quot; /&gt;&lt;/a&gt;
&lt;p&gt;0.17&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;panel-inset p8&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-110.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-old-110.png&quot; /&gt;&lt;/a&gt;
&lt;p&gt;1.1.0&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The basic design is clear:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;It is a list of all Trains.&lt;/li&gt;
&lt;li&gt;You can see the schedule.&lt;/li&gt;
&lt;li&gt;You can click the button to open the Train.&lt;/li&gt;
&lt;li&gt;You can search by schedule.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;At the time it was a big step forward, as there was no other good way of interacting with trains. For instance you couldn't open them from the map view, there wasn't even zoom to world, and the Locomotive GUI was very limited.&lt;/p&gt;
&lt;p&gt;During the old days, the mindset was, that the players would have just a few trains or 10-20 max in an extreme case, so we didn't really see a need to categorize them much.&lt;/p&gt;
&lt;h4&gt;The problem&lt;/h4&gt;
&lt;div class=&quot;flex-space-evenly&quot; readability=&quot;15&quot;&gt;
&lt;div class=&quot;w50p&quot; readability=&quot;23&quot;&gt;
&lt;p&gt;With time, the game evolved, factories were growing, and subsequently the number of train kept going up. The insufficiency of the GUI became very clear during our internal 1.1 playtesting. We played for a whole week and extensively used the new train limits. We had over 100 trains and over 200 train stops.&lt;/p&gt;
&lt;p&gt;Once we finished I asked the rest of the team &quot;Did anybody opened the Trains GUI even once?&quot;. The answer was &quot;No&quot;.&lt;/p&gt;
&lt;p&gt;While playtesting I also became highly familiar with what features I &lt;em&gt;wished&lt;/em&gt; the Trains GUI had, and what questions it should answer:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;How many trains do we have?&lt;/li&gt;
&lt;li&gt;How many stops do we have?&lt;/li&gt;
&lt;li&gt;What is the cumulative train limit of all the stops of a certain name?&lt;/li&gt;
&lt;li&gt;Am I keeping up with the demand for the given item?&lt;/li&gt;
&lt;li&gt;And mainly, do I have enough trains for this route?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;With all this in mind, I got to work on the new Trains GUI.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;panel-inset m8 p8 w50p&quot; readability=&quot;7&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-1.1-playtest.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-1.1-playtest.png&quot; /&gt;&lt;/a&gt;
&lt;p&gt;Our playtesting map after 40 hours.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4&gt;The new Trains GUI&lt;/h4&gt;
&lt;p&gt;I started by making a mockup mod in Lua. It allowed me to quickly iterate on the design and content on the GUI much faster than C++ would have allowed. After 2 days I basically had a finalised design ready. After we reviewed the mockups, we agreed it was a massive improvement, and decided to make it properly. I took it as 'Christmas homework' to write the new Train GUI into the engine in C++ (even though programming isn't much my area of expertise). After the New year, it was ready, so we did the usual QA and released it in 1.1.8.&lt;/p&gt;
&lt;p&gt;There are 2 tabs of the new Trains GUI, and each follows a simple but meaningful design principle:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Trains tab&lt;/strong&gt;: A list of Trains categorised by their Schedule.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stations tab&lt;/strong&gt;: A list of Train stops categories by their name.&lt;/li&gt;
&lt;/ul&gt;
...and honestly from there it just kinda all fell into place. But let me explain some of the details...
&lt;h4&gt;Trains tab&lt;/h4&gt;
&lt;div class=&quot;flex-space-evenly&quot;&gt;
&lt;div class=&quot;panel-inset m8 p8&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-new-1.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-new-1.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;I would say the most controversial part here is removing the Schedule display under each Train map, and replacing it with the 'State description button'. With the schedule displayed on the side, it is almost completely redundant information. The state button provides much more precise and relevant information, you can see which stop it is going to, how far away it is, and also click on the state button to open the map at the specific train stop it is going to.&lt;/p&gt;
&lt;p&gt;The nice thing is, that whenever you have some wrong or inconsistent schedule, you can easily notice it in this list.&lt;/p&gt;
&lt;h4&gt;Stations tab&lt;/h4&gt;
&lt;div class=&quot;flex-space-evenly&quot;&gt;
&lt;div class=&quot;panel-inset m8 p8&quot;&gt;&lt;a href=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-new-2.png&quot;&gt;&lt;img src=&quot;https://cdn.factorio.com/assets/img/blog/fff-364-train-gui-new-2.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Since the stations tab is completely new, there are no changes that people can disagree with it. The aim with this tab was to try to present the information about the train limit of each station in some easy to digest way.&lt;/p&gt;
&lt;p&gt;The remark in the form of &lt;em&gt;&amp;lt;Reservations&amp;gt;&lt;/em&gt;/&lt;em&gt;&amp;lt;Train limit&amp;gt;&lt;/em&gt; describes the station state. When I look at the Dropoff+Pickup station of a single product, I can easily identify 3 basic states:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Not enough trains&lt;/strong&gt;: when the sum of &lt;em&gt;&amp;lt;Reservations&amp;gt;&lt;/em&gt; is too low compared to the sum of the &lt;em&gt;&amp;lt;Train limits&amp;gt;&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Not enough input&lt;/strong&gt;: when almost all of the trains are in the Pickup waiting to be loaded.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Backed up&lt;/strong&gt;: when &lt;em&gt;&amp;lt;Reservation&amp;gt;&lt;/em&gt;=&lt;em&gt;&amp;lt;Train limit&amp;gt;&lt;/em&gt; in the Dropoff where the trains are waiting to be unloaded.&lt;/li&gt;
&lt;/ul&gt;&lt;hr /&gt;&lt;h3&gt;Save game speed &lt;span&gt;Rseding&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;There are many things in Factorio I've optimized over the years; runtime performance, startup time, saving, loading, even quit speed. It gets more and more difficult to find anything to improve on without changing game features – which mostly I try to avoid. Autosave time has always been one of those &quot;I wish it could be faster&quot; but never finding anything that stuck out or gave any measurable improvements.&lt;/p&gt;
&lt;p&gt;Several years ago I parallelized the iterate-the-game and the compress-and-save-to-disk (&lt;a href=&quot;https://www.factorio.com/blog/post/fff-201&quot;&gt;FFF-201&lt;/a&gt;) however there hasn't been any major improvements since then. The issue I would always run into was: I could make one of those two processes faster but the other always bottlenecked and so the overall speed didn’t change. Every few months I would go back to it and try to think of new ways to make it faster but always ran into that same issue.&lt;/p&gt;
&lt;h4&gt;Try it and see&lt;/h4&gt;
&lt;p&gt;The other week I was doing that again and I wondered just how much faster the save process would be if I could just 'pretend' the compress and write to disk didn't happen. It turned out; a lot. The larger the save got the more time got spent waiting for the compress and write to disk. After thinking for a while I decided to instead of save the map information in one big compressed file, to split it into multiple compressed files. It's not a new concept but we hadn't done it before in Factorio. A quick 30 minute implementation later showed that without a lot of changes – it just worked.&lt;/p&gt;
&lt;p&gt;The end result meant the compress-and-write-to-disk parallel processing never blocked the main iterate-the-game logic. That alone made the entire save process around 20% faster and as a side result any improvements I made to the main logic gave direct further improvements. In the testing I performed the larger saves saw upwards of 2x improvements to save times.&lt;/p&gt;
&lt;hr /&gt;&lt;p&gt;Thank you for tuning in, let us know what you think at the usual places.&lt;/p&gt;

</description>
<pubDate>Fri, 29 Jan 2021 12:28:35 +0000</pubDate>
<dc:creator>OrderlyTiamat</dc:creator>
<og:title>Friday Facts #364 - 1.1 stable | Factorio</og:title>
<og:type>website</og:type>
<og:image>https://cdn.factorio.com/assets/img/blog/fff-364-1-1-stable-postcard-no-text.png</og:image>
<og:description>Hello, we have a stable version! When we were releasing the 1.0 FFF-360, we actually stated that there were &quot;around 150 bugs on the forums and around 80 internal tasks to be solved&quot;. These were obviously minor issues, things hard to reproduce or very rare problems. In other words, it was quite reasonably stable, which normally goes without saying when it comes to Factorio stable versions. But it proved to be a mistake wording it this way, since some media picked up on it and presented it as a &quot;fairly bugged release&quot;. So I'm pretty thrilled to finally get to the point, where we actually have 0 known issues and 0 active bug reports on the forums. Its like cleaning the kitchen properly, so you can start cooking something fresh....</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>https://factorio.com/blog/post/fff-364</dc:identifier>
</item>
<item>
<title>PyO3: Rust Bindings for the Python Interpreter</title>
<link>https://github.com/PyO3/pyo3</link>
<guid isPermaLink="true" >https://github.com/PyO3/pyo3</guid>
<description>&lt;p&gt;&lt;a href=&quot;https://github.com/PyO3/pyo3/actions&quot;&gt;&lt;img src=&quot;https://github.com/PyO3/pyo3/workflows/Test/badge.svg&quot; alt=&quot;Actions Status&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/PyO3/pyo3&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2002182d46ccd1606ce3ed2688d61fc01a3f041d121d594609c1042f168f1307/68747470733a2f2f636f6465636f762e696f2f67682f50794f332f70796f332f6272616e63682f6d61737465722f67726170682f62616467652e737667&quot; alt=&quot;codecov&quot; data-canonical-src=&quot;https://codecov.io/gh/PyO3/pyo3/branch/master/graph/badge.svg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://crates.io/crates/pyo3&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2954c308fdbd96ffc43499e4f0aea49a259d0b06a18693858e9773a20491d33f/687474703a2f2f6d6572697462616467652e6865726f6b756170702e636f6d2f70796f33&quot; alt=&quot;crates.io&quot; data-canonical-src=&quot;http://meritbadge.herokuapp.com/pyo3&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://rust-lang.github.io/rfcs/2495-min-rust-version.html&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/23278b7d0792927befa17f187cac9b1e8106ff0bf3da03ceae41f6de504a3006/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72757374632d312e34352b2d626c75652e737667&quot; alt=&quot;minimum rustc 1.45&quot; data-canonical-src=&quot;https://img.shields.io/badge/rustc-1.45+-blue.svg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/PyO3/Lobby&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/9847ff4337174c20e5fe5d125928134dca305514e5c0b554013c3839992c9cf2/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6e776a732f6e772e6a732e737667&quot; alt=&quot;Join the dev chat&quot; data-canonical-src=&quot;https://img.shields.io/gitter/room/nwjs/nw.js.svg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.rust-lang.org/&quot; rel=&quot;nofollow&quot;&gt;Rust&lt;/a&gt; bindings for &lt;a href=&quot;https://www.python.org/&quot; rel=&quot;nofollow&quot;&gt;Python&lt;/a&gt;. This includes running and interacting with Python code from a Rust binary, as well as writing native Python modules.&lt;/p&gt;
&lt;p&gt;A comparison with rust-cpython can be found &lt;a href=&quot;https://pyo3.rs/master/rust_cpython.html&quot; rel=&quot;nofollow&quot;&gt;in the guide&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;PyO3 supports Python 3.6 and up. The minimum required Rust version is 1.45.0.&lt;/p&gt;
&lt;p&gt;Building with PyPy is also possible (via cpyext) for Python 3.6, targeted PyPy version is 7.3+. Please refer to the &lt;a href=&quot;https://pyo3.rs/master/pypy.html&quot; rel=&quot;nofollow&quot;&gt;pypy section in the guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can either write a native Python module in Rust, or use Python from a Rust binary.&lt;/p&gt;
&lt;p&gt;However, on some OSs, you need some additional packages. E.g. if you are on &lt;em&gt;Ubuntu 18.04&lt;/em&gt;, please run&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
sudo apt install python3-dev python-dev
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Using Rust from Python&lt;/h2&gt;
&lt;p&gt;PyO3 can be used to generate a native Python module.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Cargo.toml&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-toml&quot; readability=&quot;14&quot;&gt;
&lt;pre&gt;
[&lt;span class=&quot;pl-en&quot;&gt;package&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;name&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;string-sum&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-smi&quot;&gt;version&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;0.1.0&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-smi&quot;&gt;edition&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;2018&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;

[&lt;span class=&quot;pl-en&quot;&gt;lib&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;name&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;string_sum&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; &quot;cdylib&quot; is necessary to produce a shared library for Python to import from.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Downstream Rust code (including code in `bin/`, `examples/`, and `tests/`) will not be able&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; to `use string_sum;` unless the &quot;rlib&quot; or &quot;lib&quot; crate type is also included, e.g.:&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; crate-type = [&quot;cdylib&quot;, &quot;rlib&quot;]&lt;/span&gt;
&lt;span class=&quot;pl-smi&quot;&gt;crate-type&lt;/span&gt; = [&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;cdylib&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;]

[&lt;span class=&quot;pl-en&quot;&gt;dependencies&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;pyo3&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;version&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;0.13.1&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-smi&quot;&gt;features&lt;/span&gt; = [&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;extension-module&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;src/lib.rs&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-rust&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;use&lt;/span&gt; pyo3&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;prelude&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;*&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;use&lt;/span&gt; pyo3&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;wrap_pyfunction;

&lt;span class=&quot;pl-c&quot;&gt;/// Formats the sum of two numbers as string.&lt;/span&gt;
#[pyfunction]
&lt;span class=&quot;pl-k&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;sum_as_string&lt;/span&gt;(a: &lt;span class=&quot;pl-k&quot;&gt;usize&lt;/span&gt;, b: &lt;span class=&quot;pl-k&quot;&gt;usize&lt;/span&gt;) -&amp;gt; PyResult&amp;lt;&lt;span class=&quot;pl-k&quot;&gt;String&lt;/span&gt;&amp;gt; {
    &lt;span class=&quot;pl-c1&quot;&gt;Ok&lt;/span&gt;((a &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; b).&lt;span class=&quot;pl-en&quot;&gt;to_string&lt;/span&gt;())
}

&lt;span class=&quot;pl-c&quot;&gt;/// A Python module implemented in Rust.&lt;/span&gt;
#[pymodule]
&lt;span class=&quot;pl-k&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;string_sum&lt;/span&gt;(py: Python, m: &lt;span class=&quot;pl-k&quot;&gt;&amp;amp;&lt;/span&gt;PyModule) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    m.&lt;span class=&quot;pl-en&quot;&gt;add_function&lt;/span&gt;(&lt;span class=&quot;pl-en&quot;&gt;wrap_pyfunction!&lt;/span&gt;(sum_as_string, m)?)?;

    &lt;span class=&quot;pl-c1&quot;&gt;Ok&lt;/span&gt;(())
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;On Windows and Linux, you can build normally with &lt;code&gt;cargo build --release&lt;/code&gt;. On macOS, you need to set additional linker arguments. One option is to compile with &lt;code&gt;cargo rustc --release -- -C link-arg=-undefined -C link-arg=dynamic_lookup&lt;/code&gt;, the other is to create a &lt;code&gt;.cargo/config&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-toml&quot; readability=&quot;17&quot;&gt;
&lt;pre&gt;
[&lt;span class=&quot;pl-en&quot;&gt;target&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;x86_64-apple-darwin&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;rustflags&lt;/span&gt; = [
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;-C&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;link-arg=-undefined&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;-C&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;link-arg=dynamic_lookup&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
]

[&lt;span class=&quot;pl-en&quot;&gt;target&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;aarch64-apple-darwin&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;rustflags&lt;/span&gt; = [
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;-C&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;link-arg=-undefined&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;-C&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;link-arg=dynamic_lookup&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While developing, you can symlink (or copy) and rename the shared library from the target folder: On MacOS, rename &lt;code&gt;libstring_sum.dylib&lt;/code&gt; to &lt;code&gt;string_sum.so&lt;/code&gt;, on Windows &lt;code&gt;libstring_sum.dll&lt;/code&gt; to &lt;code&gt;string_sum.pyd&lt;/code&gt;, and on Linux &lt;code&gt;libstring_sum.so&lt;/code&gt; to &lt;code&gt;string_sum.so&lt;/code&gt;. Then open a Python shell in the same folder and you'll be able to &lt;code&gt;import string_sum&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To build, test and publish your crate as a Python module, you can use &lt;a href=&quot;https://github.com/PyO3/maturin&quot;&gt;maturin&lt;/a&gt; or &lt;a href=&quot;https://github.com/PyO3/setuptools-rust&quot;&gt;setuptools-rust&lt;/a&gt;. You can find an example for setuptools-rust in &lt;a href=&quot;https://github.com/PyO3/pyo3/tree/master/examples/word-count&quot;&gt;examples/word-count&lt;/a&gt;, while maturin should work on your crate without any configuration.&lt;/p&gt;
&lt;h2&gt;Using Python from Rust&lt;/h2&gt;
&lt;p&gt;If you want your Rust application to create a Python interpreter internally and use it to run Python code, add &lt;code&gt;pyo3&lt;/code&gt; to your &lt;code&gt;Cargo.toml&lt;/code&gt; like this:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-toml&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
[&lt;span class=&quot;pl-en&quot;&gt;dependencies&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;pyo3&lt;/span&gt;]
&lt;span class=&quot;pl-smi&quot;&gt;version&lt;/span&gt; = &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;0.13.1&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-smi&quot;&gt;features&lt;/span&gt; = [&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;auto-initialize&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Example program displaying the value of &lt;code&gt;sys.version&lt;/code&gt; and the current user name:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-rust&quot; readability=&quot;18&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;use&lt;/span&gt; pyo3&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;prelude&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;*&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;use&lt;/span&gt; pyo3&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;types&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;IntoPyDict;

&lt;span class=&quot;pl-k&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() -&amp;gt; &lt;span class=&quot;pl-k&quot;&gt;Result&lt;/span&gt;&amp;lt;(), ()&amp;gt; {
    Python&lt;span class=&quot;pl-k&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;with_gil&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt;py&lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt; {
        &lt;span class=&quot;pl-en&quot;&gt;main_&lt;/span&gt;(py).&lt;span class=&quot;pl-en&quot;&gt;map_err&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt;e&lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt; {
          &lt;span class=&quot;pl-c&quot;&gt;// We can't display Python exceptions via std::fmt::Display,&lt;/span&gt;
          &lt;span class=&quot;pl-c&quot;&gt;// so print the error here manually.&lt;/span&gt;
          e.&lt;span class=&quot;pl-en&quot;&gt;print_and_set_sys_last_vars&lt;/span&gt;(py);
        })
    })
}

&lt;span class=&quot;pl-k&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main_&lt;/span&gt;(py: Python) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    &lt;span class=&quot;pl-k&quot;&gt;let&lt;/span&gt; sys &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; py.&lt;span class=&quot;pl-en&quot;&gt;import&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&quot;sys&quot;&lt;/span&gt;)?;
    &lt;span class=&quot;pl-k&quot;&gt;let&lt;/span&gt; version: &lt;span class=&quot;pl-k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; sys.&lt;span class=&quot;pl-en&quot;&gt;get&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&quot;version&quot;&lt;/span&gt;)?.&lt;span class=&quot;pl-en&quot;&gt;extract&lt;/span&gt;()?;
    &lt;span class=&quot;pl-k&quot;&gt;let&lt;/span&gt; locals &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; [(&lt;span class=&quot;pl-s&quot;&gt;&quot;os&quot;&lt;/span&gt;, py.&lt;span class=&quot;pl-en&quot;&gt;import&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&quot;os&quot;&lt;/span&gt;)?)].&lt;span class=&quot;pl-en&quot;&gt;into_py_dict&lt;/span&gt;(py);
    &lt;span class=&quot;pl-k&quot;&gt;let&lt;/span&gt; code &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&quot;os.getenv('USER') or os.getenv('USERNAME') or 'Unknown'&quot;&lt;/span&gt;;
    &lt;span class=&quot;pl-k&quot;&gt;let&lt;/span&gt; user: &lt;span class=&quot;pl-k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; py.&lt;span class=&quot;pl-en&quot;&gt;eval&lt;/span&gt;(code, &lt;span class=&quot;pl-c1&quot;&gt;None&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;Some&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;&amp;amp;&lt;/span&gt;locals))?.&lt;span class=&quot;pl-en&quot;&gt;extract&lt;/span&gt;()?;
    &lt;span class=&quot;pl-c1&quot;&gt;println!&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&quot;Hello {}, I'm Python {}&quot;&lt;/span&gt;, user, version);
    &lt;span class=&quot;pl-c1&quot;&gt;Ok&lt;/span&gt;(())
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Our guide has &lt;a href=&quot;https://pyo3.rs/master/python_from_rust.html&quot; rel=&quot;nofollow&quot;&gt;a section&lt;/a&gt; with lots of examples about this topic.&lt;/p&gt;
&lt;h2&gt;Tools and libraries&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/PyO3/maturin&quot;&gt;maturin&lt;/a&gt; &lt;em&gt;Zero configuration build tool for Rust-made Python extensions&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/PyO3/setuptools-rust&quot;&gt;setuptools-rust&lt;/a&gt; &lt;em&gt;Setuptools plugin for Rust support&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/PyO3/pyo3-built&quot;&gt;pyo3-built&lt;/a&gt; &lt;em&gt;Simple macro to expose metadata obtained with the &lt;a href=&quot;https://crates.io/crates/built&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;built&lt;/code&gt;&lt;/a&gt; crate as a &lt;a href=&quot;https://docs.rs/pyo3/0.12.0/pyo3/types/struct.PyDict.html&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;PyDict&lt;/code&gt;&lt;/a&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/PyO3/rust-numpy&quot;&gt;rust-numpy&lt;/a&gt; &lt;em&gt;Rust binding of NumPy C-API&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/gperinazzo/dict-derive&quot;&gt;dict-derive&lt;/a&gt; &lt;em&gt;Derive FromPyObject to automatically transform Python dicts into Rust structs&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/vorner/pyo3-log&quot;&gt;pyo3-log&lt;/a&gt; &lt;em&gt;Bridge from Rust to Python logging&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/davidhewitt/pythonize&quot;&gt;pythonize&lt;/a&gt; &lt;em&gt;Serde serializer for converting Rust objects to JSON-compatible Python objects&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Examples&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/mre/hyperjson&quot;&gt;hyperjson&lt;/a&gt; &lt;em&gt;A hyper-fast Python module for reading/writing JSON data using Rust's serde-json&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/PyO3/setuptools-rust/tree/master/html-py-ever&quot;&gt;html-py-ever&lt;/a&gt; &lt;em&gt;Using &lt;a href=&quot;https://github.com/servo/html5ever&quot;&gt;html5ever&lt;/a&gt; through &lt;a href=&quot;https://github.com/kuchiki-rs/kuchiki&quot;&gt;kuchiki&lt;/a&gt; to speed up html parsing and css-selecting.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ManifoldFR/point-process-rust/tree/master/pylib&quot;&gt;point-process&lt;/a&gt; &lt;em&gt;High level API for pointprocesses as a Python library&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/autopilot-rs/autopy&quot;&gt;autopy&lt;/a&gt; &lt;em&gt;A simple, cross-platform GUI automation library for Python and Rust.&lt;/em&gt;
&lt;ul&gt;&lt;li&gt;Contains an example of building wheels on TravisCI and appveyor using &lt;a href=&quot;https://github.com/joerick/cibuildwheel&quot;&gt;cibuildwheel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ijl/orjson&quot;&gt;orjson&lt;/a&gt; &lt;em&gt;Fast Python JSON library&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/dronesforwork/inline-python&quot;&gt;inline-python&lt;/a&gt; &lt;em&gt;Inline Python code directly in your Rust code&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kngwyu/rogue-gym&quot;&gt;Rogue-Gym&lt;/a&gt; &lt;em&gt;Customizable rogue-like game for AI experiments&lt;/em&gt;
&lt;ul&gt;&lt;li&gt;Contains an example of building wheels on Azure Pipelines&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/thedrow/fastuuid/&quot;&gt;fastuuid&lt;/a&gt; &lt;em&gt;Python bindings to Rust's UUID library&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/wasmerio/wasmer-python&quot;&gt;wasmer-python&lt;/a&gt; &lt;em&gt;Python library to run WebAssembly binaries&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/cds-astro/mocpy&quot;&gt;mocpy&lt;/a&gt; &lt;em&gt;Astronomical Python library offering data structures for describing any arbitrary coverage regions on the unit sphere&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/tokenizers/tree/master/bindings/python&quot;&gt;tokenizers&lt;/a&gt; &lt;em&gt;Python bindings to the Hugging Face tokenizers (NLP) written in Rust&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Project-Dream-Weaver/Pyre&quot;&gt;pyre&lt;/a&gt; &lt;em&gt;Fast Python HTTP server written in Rust&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Stranger6667/jsonschema-rs/tree/master/python&quot;&gt;jsonschema-rs&lt;/a&gt; &lt;em&gt;Fast JSON Schema validation library&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Stranger6667/css-inline/tree/master/python&quot;&gt;css-inline&lt;/a&gt; &lt;em&gt;CSS inlining for Python implemented in Rust&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;PyO3 is licensed under the &lt;a href=&quot;http://opensource.org/licenses/APACHE-2.0&quot; rel=&quot;nofollow&quot;&gt;Apache-2.0 license&lt;/a&gt;. Python is licensed under the &lt;a href=&quot;https://docs.python.org/2/license.html&quot; rel=&quot;nofollow&quot;&gt;Python License&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Fri, 29 Jan 2021 12:17:11 +0000</pubDate>
<dc:creator>batterylow</dc:creator>
<og:image>https://avatars.githubusercontent.com/u/28156855?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>PyO3/pyo3</og:title>
<og:url>https://github.com/PyO3/pyo3</og:url>
<og:description>Rust bindings for the Python interpreter. Contribute to PyO3/pyo3 development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/PyO3/pyo3</dc:identifier>
</item>
<item>
<title>Businesses should try to be the best, not the biggest</title>
<link>https://joanwestenberg.medium.com/dont-grow-d7f87faeaf56</link>
<guid isPermaLink="true" >https://joanwestenberg.medium.com/dont-grow-d7f87faeaf56</guid>
<description>&lt;p id=&quot;ef5d&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;When you walk down any street with a touch of history, you’ll see storefronts marked with dates. Est. 1856. Est. 1912.&lt;/p&gt;
&lt;p id=&quot;9b0e&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;There’s a sobering clash of the modern way of viewing business success — immediate scale and ultimately acquisition — and the older idea of generational dynastic family businesses, carrying with them the histories and the DNA of their founders, employees and customers, dating back hundreds of years.&lt;/p&gt;
&lt;p id=&quot;0ed9&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Storied companies that hold a lo&lt;span id=&quot;rmm&quot;&gt;n&lt;/span&gt;g, rich past at their core and remain true to their original values and traditions are remarkable, not only for their ability to remain in business over staggering periods of time, but also for their ability to adapt and change while remaining true to themselves and their purpose.&lt;/p&gt;
&lt;p id=&quot;19ca&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Founded in 1141 and run by the 55th generation of the Sudo family, Sudo Honke is one of these businesses. For 879 years, the oldest Sake brewery in Japan has thrived and survived through massive social, technological and economic change. Most recently it has weathered the threat of radioactive contamination from the Fukushima Nuclear Power Plant, with its products remaining pure to their original recipes.&lt;/p&gt;
&lt;p id=&quot;c5fc&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Sudo Honke’s president, Mr. Gen-uemon Sudo, is conscious of the solidity and strength of their family’s commitment.&lt;/p&gt;
&lt;blockquote class=&quot;jp jq jr&quot; readability=&quot;8&quot;&gt;
&lt;p id=&quot;def5&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;“Fashion changes all the time, but it does not last. For us, we will continue to make our traditional products which have appealed to many throughout these years.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;ebac&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Sudo Honke have created something that is deeply rooted in their own culture, their local produce and their identity; and in doing so, they have developed a product that is loved and celebrated throughout the world. They have stayed true to who they are and what they do, eschewing the growth, scale and commoditization that has destroyed many young startups. There is a lesson here about the lasting opportunities of companies that are not designed to burn bright and burn out overnight.&lt;/p&gt;

&lt;p id=&quot;bc4e&quot; class=&quot;ir is fp it b iu kp iw ix iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo dh gl&quot;&gt;When we talk about successful founders now, we’re rarely talking about the people who have built long term businesses, and made the successful transition from being a startup, to being an established, healthy and thriving company. For the most part, we’re talking about pump and dump high growth, media and VC darlings. The ideal is no longer the driven creator, whose dedication to her craft enabled her to design and sell a product. It’s the founder as a personality who has sold their product, their company, and ultimately themselves.&lt;/p&gt;
&lt;p id=&quot;7ca7&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;We have developed a “growth as a good” mentality. There is nothing, in our modern viewpoint, that is noble, beautiful or memorable about things that are built to stand the test of time; if they’re found to be in the way of constant growth, they should be destroyed, whether they’re traditions, values or heritage sites. It’s a way of thinking that is both ancient and entirely new. Paul Jarvis puts it quite succinctly;&lt;/p&gt;
&lt;blockquote class=&quot;jp jq jr&quot; readability=&quot;11&quot;&gt;
&lt;p id=&quot;5a3d&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;“From an evolutionary point of view it is explainable why we wanted to gather more and more: with more food, more water, more protection against predators, we may be less likely to die. But today, growth feeds our ego and social standing.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;a3de&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Growth, where it no longer serves a purpose beyond the accumulation of more growth — in the form of investors’ returns, company coffers and the personal wealth of founders — is worshipped. And the cost is astronomical. The endless growth goals of fossil fuel companies has for all intents and purposes doomed our entire planet. The endless growth of software and platform giants like Google and Facebook has destroyed lives and damaged democracy itself. On a smaller scale, growth as a god-value has turned promising founders into burned out shells, chasing impossible goals and sacrificing their time and their lives and their health in the process.&lt;/p&gt;
&lt;p id=&quot;b44f&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;Jurgen Appelo is the author of Managing for Happiness. His view is that growth unchecked becomes not only healthy, it becomes destructive.&lt;/p&gt;
&lt;blockquote class=&quot;jp jq jr&quot; readability=&quot;8&quot;&gt;
&lt;p id=&quot;57c8&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;“Why do many business leaders want bigger organizations? If the purpose of a company is merely growth, it is similar to cancer. Your business should try to be the best, not the biggest.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;adf7&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;There is a world outside of this paradigm. It is possible to build a million dollar company with just a handful of people, or even one single founder. There are companies with growth scale and exits are not the measures of success. They’re companies with the founders’ passion, values and ideas are kept at the heart of every decision, and every action.&lt;/p&gt;

&lt;p id=&quot;402d&quot; class=&quot;ir is fp it b iu kp iw ix iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo dh gl&quot;&gt;Basecamp, the creators of the leading productivity suite, multiple bestselling books and my preferred email client Hey, are an example of this approach to business success.&lt;/p&gt;
&lt;blockquote class=&quot;jp jq jr&quot; readability=&quot;69&quot;&gt;
&lt;p id=&quot;fd32&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;“It all started back in 2003. Back then we were a web design firm. Companies hired us to re-design and simplify their website. Business was great and we were busy. But we were disorganized. With so many concurrent projects, things began to slip through the cracks.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;0796&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Projects dragged on too long. We dropped the ball on key deliverables. We had some major miscommunication (“Wait, who said that? We did? When? Where?”).&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;da74&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Back then, we relied on email for everything. Email’s great for many things. But it’s not great for long-running projects. Things get lost, people get left out of conversations, there’s nowhere to go to see what’s left to do. Know what I mean?&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;a94f&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;So we started looking for a project management tool. We needed something to help us communicate ideas, organize the work to be done, and present work to stakeholders. Simple as that.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;28dc&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;We tried a few tools, but they were complicated and too hard to use. So we slowly slipped back to using our old standby — email. Our problems continued.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;758b&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Frustrated, we decided to build our own simple project management app. A few months later we had something ready. We started using this tool with our existing clients.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;f7ed&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Immediately projects ran better! We regained the sense of order and calmness we’d been craving. And clients noticed — they really appreciated the improved communication and organization.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;f3eb&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Then our clients started asking us what software we were using to run these projects. Turns out they wanted to use it for their own in-house projects!&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;4bee&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Hey, maybe we’ve got a product here! So we polished it up, priced it fairly, and put it on the market. On February 5th, 2004, Basecamp was born.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;dfa7&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;We announced it on our blog. Within a month, we had about a hundred paying customers. Hundreds more followed. Within a year, Basecamp was generating more income for us than our web design business. We had a hit! So we stopped designing web sites and went all-in on our software business.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;07b7&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Soon we started hearing stories of Basecamp being used in schools, governments, churches, consulting firms, publishers, and just about every other industry on earth. The stories keep coming and we keep growing.&lt;/em&gt;&lt;/p&gt;
&lt;p id=&quot;6784&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;Today, 22 years after Basecamp first hit the market, over 20,000,000 people have worked on a project with Basecamp! And every week, thousands of companies sign up to use Basecamp.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;461a&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;The company has been built, grown and run at its founders’ own pace. They have never IPO’d. They have never expressed any interest in doing so. They remain a profitable company with their sights set firmly on culture, capacity and purpose. The potential to become a billion dollar company has always been there; but it’s never become Basecamp’s goal, and as far as they’re concerned, it never will.&lt;/p&gt;
&lt;p id=&quot;c96c&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;In doing so, they have an incredibly low turnover rate, low overheads, high revenues, high profits and an ongoing pursuit of excellence that sets them apart from their competitors and peers, by sheer weight of their dedicated customer and employee focus. Growth happens. But growth is not the god-value. Growth happens, but growth does not require blood sacrifice.&lt;/p&gt;
&lt;p id=&quot;48de&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;As an angel investor, the prospect of companies avoiding hyper growth doesn’t scare me. I’m interested in teams, products and platforms that seek out revenue and profitability, and I’m interested in companies that can provide returns and gains organically, over an extended period of time, and through their longevity. I want to build relationships with founders that have a solid foundation — relationships that are built to last, without conflicts of interest, while treating them as human beings.&lt;/p&gt;
&lt;p id=&quot;bba2&quot; class=&quot;ir is fp it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;I’m reminded of this quote from Karma Ura, the director of the Center for Bhutan Studies, in the New York Times.&lt;/p&gt;
&lt;blockquote class=&quot;jp jq jr&quot; readability=&quot;12&quot;&gt;
&lt;p id=&quot;049e&quot; class=&quot;ir is js it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo dh gl&quot;&gt;&lt;em class=&quot;fp&quot;&gt;“Growth is not a goal by itself, everyone would agree. The real goal is happiness and well-being and all those things that they represent like vitality and health, socially empathetic relations, pure and vibrant nature, and meaning and freedom. There is no necessary causal or correlational link between growth and most of these goals. So it is surprising that growth as a topic has had such a hold on leaders all around the world. The “short-termism” of growth neither encourages us to look beyond our lifetimes to the distant future generations nor to pay our respect to the tremendous works of past generations on whose inheritance we live.”&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
<pubDate>Fri, 29 Jan 2021 11:54:50 +0000</pubDate>
<dc:creator>joanwestenberg</dc:creator>
<og:type>article</og:type>
<og:title>Don’t grow.</og:title>
<og:description>When you walk down any street with a touch of history, you’ll see storefronts marked with dates. Est. 1856. Est. 1912.</og:description>
<og:url>https://joanwestenberg.medium.com/dont-grow-d7f87faeaf56</og:url>
<og:image>https://miro.medium.com/max/1200/0*UjwSApra5H7pC8JF</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://joanwestenberg.medium.com/dont-grow-d7f87faeaf56</dc:identifier>
</item>
</channel>
</rss>