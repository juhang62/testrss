<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Facebook&amp;#039;s Email-Harvesting Practice Is Under Investigation in N.Y.</title>
<link>https://www.bloomberg.com/news/articles/2019-04-25/n-y-opens-investigation-into-facebook-s-email-harvesting</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2019-04-25/n-y-opens-investigation-into-facebook-s-email-harvesting</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2019-04-25/n-y-opens-investigation-into-facebook-s-email-harvesting&quot;&gt;https://www.bloomberg.com/news/articles/2019-04-25/n-y-opens-investigation-into-facebook-s-email-harvesting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19753805&quot;&gt;https://news.ycombinator.com/item?id=19753805&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 231&lt;/p&gt;
&lt;p&gt;# Comments: 74&lt;/p&gt;
</description>
<pubDate>Fri, 26 Apr 2019 00:27:55 +0000</pubDate>
<dc:creator>kerng</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=7dcd8790-67f9-11e9-ab76-b197ea9ee299&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOS0wNC0yNS9uLXktb3BlbnMtaW52ZXN0aWdhdGlvbi1pbnRvLWZhY2Vib29rLXMtZW1haWwtaGFydmVzdGluZw==</dc:identifier>
</item>
<item>
<title>Crypto Market Roiled by New Allegations Against Tether, Bitfinex</title>
<link>https://www.bloomberg.com/news/articles/2019-04-25/bitfinex-operator-accused-by-new-york-of-850-million-coverup?utm_source=google&amp;utm_medium=bd&amp;cmpId=google</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2019-04-25/bitfinex-operator-accused-by-new-york-of-850-million-coverup?utm_source=google&amp;utm_medium=bd&amp;cmpId=google</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2019-04-25/bitfinex-operator-accused-by-new-york-of-850-million-coverup?utm_source=google&amp;utm_medium=bd&amp;cmpId=google&quot;&gt;https://www.bloomberg.com/news/articles/2019-04-25/bitfinex-operator-accused-by-new-york-of-850-million-coverup?utm_source=google&amp;utm_medium=bd&amp;cmpId=google&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19752699&quot;&gt;https://news.ycombinator.com/item?id=19752699&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 412&lt;/p&gt;
&lt;p&gt;# Comments: 235&lt;/p&gt;
</description>
<pubDate>Thu, 25 Apr 2019 21:30:55 +0000</pubDate>
<dc:creator>seibelj</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=7dcd1260-67f9-11e9-8e50-1fe1c809be38&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOS0wNC0yNS9iaXRmaW5leC1vcGVyYXRvci1hY2N1c2VkLWJ5LW5ldy15b3JrLW9mLTg1MC1taWxsaW9uLWNvdmVydXA/dXRtX3NvdXJjZT1nb29nbGUmdXRtX21lZGl1bT1iZCZjbXBJZD1nb29nbGU=</dc:identifier>
</item>
<item>
<title>Amazon Has Gone from Neutral Platform to Cutthroat Competitor</title>
<link>https://onezero.medium.com/open-source-betrayed-industry-leaders-accuse-amazon-of-playing-a-rigged-game-with-aws-67177bc748b7?gi=356d74c0b36e</link>
<guid isPermaLink="true" >https://onezero.medium.com/open-source-betrayed-industry-leaders-accuse-amazon-of-playing-a-rigged-game-with-aws-67177bc748b7?gi=356d74c0b36e</guid>
<description>&lt;h2 name=&quot;99f6&quot; id=&quot;99f6&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;Community leaders say AWS increasingly poses an existential threat&lt;/h2&gt;
&lt;div class=&quot;uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup&quot;&gt;
&lt;div class=&quot;u-flex0&quot;&gt;
&lt;div class=&quot;u-relative u-inlineBlock u-flex0&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/fit/c/100/100/1*GsLF_pXXLQw7DyW8hb1fwQ.jpeg&quot; class=&quot;avatar-image u-size50x50&quot; alt=&quot;Go to the profile of Andrew Leonard&quot;/&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;p name=&quot;1377&quot; id=&quot;1377&quot; class=&quot;graf graf--p graf--hasDropCapModel graf--hasDropCap graf--hasDropCapImage graf-after--h4&quot;&gt;&lt;span class=&quot;graf-dropCap&quot;&gt;&lt;span class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;&lt;img class=&quot;graf-dropCapImage&quot; data-image-id=&quot;1*HucWxq9Uj8AfT5xvutQt_w.png&quot; data-width=&quot;500&quot; data-height=&quot;500&quot; src=&quot;https://cdn-images-1.medium.com/max/800/1*HucWxq9Uj8AfT5xvutQt_w.png&quot;/&gt;&lt;/span&gt;&lt;span class=&quot;graf-dropCapText&quot;&gt;On&lt;/span&gt;&lt;/span&gt; March 11, a Vice President at Amazon Web Services, Amazon’s cloud computing behemoth, &lt;a href=&quot;https://aws.amazon.com/blogs/opensource/keeping-open-source-open-open-distro-for-elasticsearch/&quot; data-href=&quot;https://aws.amazon.com/blogs/opensource/keeping-open-source-open-open-distro-for-elasticsearch/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;published a blog post&lt;/a&gt; announcing the release of its own version of Elasticsearch, a powerful open-source software search engine tool.&lt;/p&gt;
&lt;p name=&quot;de7b&quot; id=&quot;de7b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Elastic is a public company founded in 2012 that is currently worth over $5 billion; the vast majority of its revenue is generated by selling subscription access to Elastic’s search capabilities via the cloud. It’s based in Amsterdam and employs more than 1,200 people.&lt;/p&gt;
&lt;p name=&quot;4fa5&quot; id=&quot;4fa5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In the blog post, Adrian Cockcroft, VP of cloud architecture strategy at Amazon Web Services (AWS), explained that the company felt forced to take action because &lt;a href=&quot;https://www.elastic.co/&quot; data-href=&quot;https://www.elastic.co/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Elastic&lt;/a&gt; was “changing the rules” on how its software code could be shared. Those changes, made in the run-up to Elastic’s 2018 IPO, started mixing intellectual property into Elastic’s overall line of software products.&lt;/p&gt;
&lt;p name=&quot;927e&quot; id=&quot;927e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Open-source software is defined as code that can be freely shared and modified by anyone. But now Elastic was telling customers that certain elements in its product mix could not be accessed without payment and that the code could not be freely shared.&lt;/p&gt;
&lt;p name=&quot;4794&quot; id=&quot;4794&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Elastic did not explain its strategic shift at the time. But industry observers interpreted the changes as a response to increasing competition from AWS, which had incorporated Elasticsearch’s code and search functionality into its own suite of computing services.&lt;/p&gt;
&lt;p name=&quot;dae3&quot; id=&quot;dae3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Elastic isn’t the only open source cloud tool company currently looking over its shoulder at AWS. In 2018 alone, at least eight firms have made similar “rule changes” designed to ward off what they see as unfair competition from a company intent on cannibalizing their services.&lt;/p&gt;
&lt;p name=&quot;227a&quot; id=&quot;227a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In his blog post, Cockcroft argued that by making part of its product suite proprietary, Elastic was betraying the core principles of the open source community. “Customers must be able to trust that open source projects stay open,” Cockcroft wrote. “When important open source projects that AWS and our customers depend on begin restricting access, changing licensing terms, or intermingling open source and proprietary software, we will invest to sustain the open source project and community.”&lt;/p&gt;
&lt;p name=&quot;1e14&quot; id=&quot;1e14&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;AWS’s announcement did not attract the immediate attention of the Democratic &lt;a href=&quot;https://medium.com/@teamwarren/heres-how-we-can-break-up-big-tech-9ad9e0da324c&quot; data-href=&quot;https://medium.com/@teamwarren/heres-how-we-can-break-up-big-tech-9ad9e0da324c&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;presidential candidates&lt;/a&gt; or the growing cadre of antitrust activists who have recently set their sights on Amazon. But in the world of open source and free software, where picayune changes in arcane language can spark the internet equivalent of the Hundred Years War, the release of AWS’s Open Distro for Elasticsearch launched a heated debate.&lt;/p&gt;
&lt;p name=&quot;4405&quot; id=&quot;4405&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Open source software has been one of the biggest success stories of the software industry. In &lt;a href=&quot;https://blog.timescale.com/open-source-demise-of-proprietary-software-a49f73f54165/&quot; data-href=&quot;https://blog.timescale.com/open-source-demise-of-proprietary-software-a49f73f54165/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;2018&lt;/a&gt; alone, Microsoft’s purchase of the open source software development platform GitHub for $7.5 billion, Salesforce’s purchase of the open source company Mulesoft for $6.5 billion, and IBM’s blockbuster $34 billion purchase of the Linux vendor Red Hat proved that open source is a crucial part of the larger software industry. And there is &lt;a href=&quot;https://www.forbes.com/sites/forbestechcouncil/2018/07/16/how-open-source-became-the-default-business-model-for-software/#67895d514e72&quot; data-href=&quot;https://www.forbes.com/sites/forbestechcouncil/2018/07/16/how-open-source-became-the-default-business-model-for-software/#67895d514e72&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;growing acceptance&lt;/a&gt; that the collaborative model of developing open source software is a winning strategy to meet the tech industry’s need for constant innovation. So, when the likes of Amazon start accusing companies of not playing fair, people notice.&lt;/p&gt;
&lt;blockquote name=&quot;d645&quot; id=&quot;d645&quot; class=&quot;graf graf--pullquote graf-after--p&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;AWS is striking at the Achilles’ heel of open source: lifting the work of others, and renting access to it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;8666&quot; id=&quot;8666&quot; class=&quot;graf graf--p graf-after--pullquote&quot;&gt;&lt;a href=&quot;https://medium.com/@shar1z&quot; data-href=&quot;https://medium.com/@shar1z&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;Sharone Zitzman&lt;/a&gt;, a respected &lt;a href=&quot;https://thenewstack.io/what-the-fork-amazon/&quot; data-href=&quot;https://thenewstack.io/what-the-fork-amazon/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;commentator&lt;/a&gt; on open source software and the head of developer relations at AppsFlyer, an app development company, called Amazon’s move a “&lt;a href=&quot;https://twitter.com/shar1z/status/1106422552405200896&quot; data-href=&quot;https://twitter.com/shar1z/status/1106422552405200896&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;hostile takeover&lt;/a&gt;” of Elastic’s business. Steven O’Grady, co-founder of the software industry analyst firm RedMonk, cited it as an example of the “&lt;a href=&quot;https://redmonk.com/sogrady/2019/03/15/cloud-open-source-powder-keg/&quot; data-href=&quot;https://redmonk.com/sogrady/2019/03/15/cloud-open-source-powder-keg/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;existential threat&lt;/a&gt;” that open source companies like Elastic believe a handful of cloud computing giants could pose. Shay Banon, founder and CEO of Elastic, &lt;a href=&quot;https://www.elastic.co/blog/on-open-distros-open-source-and-building-a-company&quot; data-href=&quot;https://www.elastic.co/blog/on-open-distros-open-source-and-building-a-company&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;carefully defended&lt;/a&gt; Elastic’s new licensing practices, while at the same time making his unhappiness with Amazon crystal clear.&lt;/p&gt;
&lt;p name=&quot;7088&quot; id=&quot;7088&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Elastic’s products, Banon wrote, have been “redistributed and rebundled so many times I lost count… There was always a ‘reason’, at times masked with fake altruism or benevolence. None of these have lasted. [Amazon and other vendors] were built to serve their own needs, drive confusion, and splinter the community.”&lt;/p&gt;
&lt;p name=&quot;a793&quot; id=&quot;a793&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;(AWS declined to comment on the record for this story, and open source companies on the front lines of the confrontation refused to speak in detail about their relationship with Amazon, either providing generic statements or declining interviews.)&lt;/p&gt;
&lt;p name=&quot;bb99&quot; id=&quot;bb99&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The reaction to Amazon’s move wasn’t all negative. Some veterans of the open source community &lt;a href=&quot;https://www.infoworld.com/article/3233485/the-critics-are-wrong-about-awss-open-source-approach.html&quot; data-href=&quot;https://www.infoworld.com/article/3233485/the-critics-are-wrong-about-awss-open-source-approach.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;praised&lt;/a&gt; Amazon’s defense of open source values, while pointing out the &lt;a href=&quot;https://medium.com/sustainable-free-and-open-source-communities/free-software-is-the-only-winner-in-elastic-nv-vs-aws-9416f2a0a7f5&quot; data-href=&quot;https://medium.com/sustainable-free-and-open-source-communities/free-software-is-the-only-winner-in-elastic-nv-vs-aws-9416f2a0a7f5&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;fundamentally messy contradictions&lt;/a&gt; of Elastic mixing commercial priorities with open source principles. And fundamentally, adopting open source code is entirely legal.&lt;/p&gt;
&lt;p name=&quot;f734&quot; id=&quot;f734&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;But the notion that Amazon was presenting itself as an altruistic defender of the digital public commons rankled community veterans like Zitzman, who says that Amazon has a poor reputation for working with the community. (GitHub &lt;a href=&quot;https://resources.whitesourcesoftware.com/blog-whitesource/git-much-the-top-10-companies-contributing-to-open-source&quot; data-href=&quot;https://resources.whitesourcesoftware.com/blog-whitesource/git-much-the-top-10-companies-contributing-to-open-source&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;data&lt;/a&gt; shows that Amazon has far fewer employees than Microsoft, Google, or IBM contributing code to open source projects.)&lt;/p&gt;
&lt;p name=&quot;00ff&quot; id=&quot;00ff&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;These critics see Amazon’s decision to recreate Elasticsearch as opportunistic . behavior. Amazon, they say, is leveraging its dominant power in cloud computing in order to unfairly reap intellectual property. In doing so, AWS is striking at the Achilles’ heel of open source: lifting the work of others, and renting access to it.&lt;/p&gt;
&lt;p name=&quot;6f8b&quot; id=&quot;6f8b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;What happened to Elastic, Zitzman says, fits into a “long-standing trend of AWS rolling out managed services of popular open source technology, or replicating such technologies… This move is a text-book commoditization move — providing Elastic’s premium services for free.” Or as Salil Deshpande, a managing director at Bain Capital Ventures and an investor in multiple open source companies, puts it: “It is clear that AWS is using its market power to be anti-competitive.”&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*82tYB64jHgDUDl_vJVp2Vg.jpeg&quot; data-width=&quot;2400&quot; data-height=&quot;2400&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*82tYB64jHgDUDl_vJVp2Vg.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*82tYB64jHgDUDl_vJVp2Vg.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;4b32&quot; id=&quot;4b32&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;On the campaign trail, Senator Elizabeth Warren recently &lt;a href=&quot;https://twitter.com/ewarren/status/1110565340105400322&quot; data-href=&quot;https://twitter.com/ewarren/status/1110565340105400322&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;called for the breakup of Amazon&lt;/a&gt;, declaring that “you can be an umpire, or you can own a team, but you can’t do both at the same time.” She was referring to Amazon’s role as both an e-commerce platform and a vendor — a scheme that lets the company observe market trends and undercut sellers with in-house products at opportune moments. But Warren’s words might also describe Amazon’s behavior in the open source economy.&lt;/p&gt;
&lt;p name=&quot;aeee&quot; id=&quot;aeee&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;If Amazon uses the same ostensibly anticompetitive tactics in the cloud that have helped it establish a commanding position in e-commerce, regulators might want to start paying closer attention. A single company enjoying a dominant position in the cloud could, some critics suggest, result in less overall innovation in the most important part of our digital infrastructure. And that means that the future of antitrust may be up in the cloud.&lt;/p&gt;
&lt;p name=&quot;5189&quot; id=&quot;5189&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--p&quot;&gt;“What’s happening to open source providers illustrates the raw power that Amazon has,” says Stacy Mitchell, a longtime &lt;a href=&quot;https://www.thenation.com/article/amazon-doesnt-just-want-to-dominate-the-market-it-wants-to-become-the-market/&quot; data-href=&quot;https://www.thenation.com/article/amazon-doesnt-just-want-to-dominate-the-market-it-wants-to-become-the-market/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;critic of Amazon&lt;/a&gt; and co-director of the &lt;a href=&quot;https://ilsr.org/&quot; data-href=&quot;https://ilsr.org/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Institute for Local Self-Reliance&lt;/a&gt;, a nonprofit organization that specializes in &lt;a href=&quot;https://ilsr.org/about-the-institute-for-local-self-reliance/&quot; data-href=&quot;https://ilsr.org/about-the-institute-for-local-self-reliance/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;challengin&lt;/a&gt;g “concentrated economic and political power.”&lt;/p&gt;
&lt;p name=&quot;52fe&quot; id=&quot;52fe&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--p graf--trailing&quot;&gt;“Amazon’s control of the core infrastructure for exchanging goods and data means they have the ability to set the rules for how everyone else operates,” Mitchell says. “We should recognize that as a kind of governing power.”&lt;/p&gt;
</description>
<pubDate>Thu, 25 Apr 2019 19:15:13 +0000</pubDate>
<dc:creator>howard941</dc:creator>
<og:title>Amazon Has Gone From Neutral Platform to Cutthroat Competitor, Say Open Source Developers</og:title>
<og:url>https://onezero.medium.com/open-source-betrayed-industry-leaders-accuse-amazon-of-playing-a-rigged-game-with-aws-67177bc748b7</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*zNRhSEpe1PKv3GfGjXl6yw.jpeg</og:image>
<og:description>For open-source developers, AWS has gone from a neutral platform to a cutthroat competitor</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://onezero.medium.com/open-source-betrayed-industry-leaders-accuse-amazon-of-playing-a-rigged-game-with-aws-67177bc748b7?gi=84c3bff9967b</dc:identifier>
</item>
<item>
<title>People Who Claim to Work 75-Hour Weeks Usually Only Work About 50 Hours</title>
<link>http://nymag.com/intelligencer/2019/04/people-who-claim-to-work-75-hour-weeks-are-lying.html</link>
<guid isPermaLink="true" >http://nymag.com/intelligencer/2019/04/people-who-claim-to-work-75-hour-weeks-are-lying.html</guid>
<description>&lt;div class=&quot;lede-image-wrapper inline horizontal&quot;&gt;&lt;img src=&quot;https://pixel.nymag.com/imgs/daily/intelligencer/2019/04/25/24-working-overtime.w700.h700.jpg&quot; class=&quot;lede-image&quot; data-src=&quot;https://pixel.nymag.com/imgs/daily/intelligencer/2019/04/25/24-working-overtime.w700.h700.jpg&quot; data-content-img=&quot;&quot; alt=&quot;&quot;/&gt;&lt;div class=&quot;lede-image-data&quot;&gt;
&lt;div class=&quot;attribution&quot;&gt;&lt;span class=&quot;credit&quot;&gt;Photo: Nick Dolding/Getty Images&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwpm2ne00sbsay60er24mj5@published&quot; data-word-count=&quot;44&quot;&gt;I want to thank &lt;a href=&quot;https://www.motherjones.com/kevin-drum/2019/04/most-of-us-dont-work-more-than-40-hours-a-week/&quot;&gt;Kevin Drum from &lt;em&gt;Mother Jones&lt;/em&gt;&lt;/a&gt; for surfacing a &lt;a href=&quot;https://www.bls.gov/opub/mlr/2011/06/art3full.pdf&quot;&gt;2011 Bureau of Labor Statistics study&lt;/a&gt; that confirms something I’ve long suspected: Virtually anyone you know who claims to be working more than 60 hours a week is not telling the truth.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwpogye001c3h63ss49kpkn@published&quot; data-word-count=&quot;50&quot;&gt;Bureau of Labor Statistics researchers reached this conclusion by comparing regular survey data to diary data from the American Time Use Survey, a Census project that asks Americans to track, diary style, how their weekly time is divided among 163 different activity categories, from sleeping to shopping to pet care.&lt;/p&gt;
&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s6say63gjmlafe@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-0-480&quot; data-name=&quot;/4088/Intelligencer_Mobile&quot; data-sizes=&quot;300x250,300x252,320x100&quot; data-label=&quot;inArticleMobile&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-UGNa7g1D&quot;/&gt;&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s8say6jq5f5toi@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-480-768&quot; data-name=&quot;/4088/Intelligencer_Mobile&quot; data-sizes=&quot;468x60,480x60,300x250&quot; data-label=&quot;inArticleMobile&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-EBzeF51a&quot;/&gt;&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwpvwbh001j3h63s95oy9rf@published&quot; data-word-count=&quot;152&quot;&gt;The ATUS data has a couple of advantages over simpler survey data where you just ask people how many hours a week they typically spend working for pay, or taking care of the house, or attending church. One is that people are bad at estimating: When you give people a long list of activity categories and ask them how many hours a week they spend on each, they tend to give an answer that sums to more than 168 hours. The ATUS diary structure has the advantage of limiting respondents to 24 hours of time when reporting a day’s activity. Another issue is that questions about one specific activity are more subject to social desirability bias: If you ask people to tell you everything they did on Sunday, rather than asking specifically how much time they spent at church, they may be less likely to fib in a way that projects virtue.&lt;/p&gt;
&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s7say6i1dcxcdw@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-768-1180&quot; data-name=&quot;/4088/Intelligencer&quot; data-sizes=&quot;300x250&quot; data-label=&quot;inArticleTablet&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-BDTXV3RK&quot;/&gt;&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwpyjap001q3h63vks4swg7@published&quot; data-word-count=&quot;95&quot;&gt;The BLS study found respondents in the ATUS tend to give an estimate of typical working time that is 5 to 10 percent higher than what shows up in their diaries. But the divergence was not uniform across the population. The largest overestimates came from the people providing the highest estimates: People who said they typically worked 75 or more hours per week tended to provide diaries reflecting 25 hours’ less work per week than they estimated. People claiming to typically work between 65 and 74 hours weekly tended to be overestimating by 18 hours.&lt;/p&gt;
&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s9say64x8hro4e@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-0-768&quot; data-name=&quot;/4088/Intelligencer_Mobile&quot; data-sizes=&quot;528x379&quot; data-label=&quot;outStreamMobile&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-1xug3wWR&quot;/&gt;&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00sasay6fi7e9ezv@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-1180-plus&quot; data-name=&quot;/4088/Intelligencer&quot; data-sizes=&quot;528x379&quot; data-label=&quot;outStreamDesktop&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-JoIFUK7p&quot;/&gt;&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwq71d9001x3h63g3mkadb0@published&quot; data-word-count=&quot;67&quot;&gt;Again, this sort of misreporting is not limited to work hours. People overestimate how often they do all sorts of things they “ought” to be doing, often by even larger margins than with work for pay. One study from the 1980s found swim and tennis club members provided estimates of their frequency of use of recreation facilities that was &lt;a href=&quot;https://www.cabdirect.org/cabdirect/abstract/19831800363&quot;&gt;double what was reflected in their clubs’ logbooks&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwqgx8x002b3h63lumsvctf@published&quot; data-word-count=&quot;88&quot;&gt;As George Costanza says, it’s not a lie if you believe it. Do the people who give inaccurate survey answers about their time use know they’re not telling the truth? Like Drum, I lean toward the view that the deception here is mostly self-deception, especially in the case of gyms and other recreational facilities. Their business models often rely on members overestimating how often they will attend, so they will be willing to pay monthly fees that &lt;a href=&quot;https://www.nytimes.com/2015/01/11/upshot/how-to-make-yourself-go-to-the-gym.html&quot;&gt;in some cases exceed what it would cost to pay per-visit&lt;/a&gt;.&lt;/p&gt;
&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s6say63gjmlafe@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-0-480&quot; data-name=&quot;/4088/Intelligencer_Mobile&quot; data-sizes=&quot;300x250,300x252,320x100&quot; data-label=&quot;inArticleMobile&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-rzOdregQ&quot;/&gt;&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/ad/instances/cjuwpm2ne00s8say6jq5f5toi@published&quot; data-placeholder=&quot;settings&quot; class=&quot;ad vp-480-768&quot; data-name=&quot;/4088/Intelligencer_Mobile&quot; data-sizes=&quot;468x60,480x60,300x250&quot; data-label=&quot;inArticleMobile&quot; data-site=&quot;Intelligencer&quot; id=&quot;ad-cid-VdPZVfrc&quot;/&gt;&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/intelligencer/_components/clay-paragraph/instances/cjuwqf9rn00243h63k6n73nzx@published&quot; data-word-count=&quot;42&quot;&gt;So you can think of your friend who claims to work 75 hours a week at the bank similarly to your friend who claims to go to the gym six days a week: He’s probably lying to you, and, possibly, to himself.&lt;/p&gt;
&lt;aside data-uri=&quot;nymag.com/intelligencer/_components/newsletter-flex-text/instances/cjuwpm2ne00scsay6sefeli1s@published&quot; class=&quot;newsletter-flex-text initially-hidden opacity-zero&quot; data-track-id=&quot;intelligencer&quot; data-track-type=&quot;newsletter-signup&quot;&gt;&lt;div class=&quot;wrapper-style&quot;&gt;
&lt;div data-editable=&quot;settings&quot;&gt;
&lt;div class=&quot;text-form-wrapper&quot;&gt;
&lt;div class=&quot;text&quot;&gt;
&lt;h2 class=&quot;title&quot;&gt;Sign Up for the Intelligencer Newsletter&lt;/h2&gt;
&lt;div class=&quot;description&quot;&gt;Daily news about the politics, business, and technology shaping our world.&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;terms-and-policy-wrapper&quot;&gt;&lt;button class=&quot;terms-button&quot; role=&quot;button&quot;&gt;Terms &amp;amp; Privacy Notice&lt;/button&gt; &lt;span class=&quot;expanded-terms&quot; aria-hidden=&quot;true&quot;&gt;By submitting your email, you agree to our &lt;a href=&quot;http://nymag.com/newyork/terms/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Terms&lt;/a&gt; and &lt;a href=&quot;http://nymag.com/newyork/privacy/&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Privacy Notice&lt;/a&gt; and to receive email correspondence from us.&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/aside&gt;</description>
<pubDate>Thu, 25 Apr 2019 18:25:26 +0000</pubDate>
<dc:creator>ekovarski</dc:creator>
<og:title>People Who Claim to Work 75-Hour Weeks Usually Only Work About 50 Hours</og:title>
<og:url>http://nymag.com/intelligencer/2019/04/people-who-claim-to-work-75-hour-weeks-are-lying.html</og:url>
<og:description>They’re lying to you — but also maybe to themselves.</og:description>
<og:image>https://pixel.nymag.com/imgs/daily/intelligencer/2019/04/25/24-working-overtime.w1200.h630.jpg</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://nymag.com/intelligencer/2019/04/people-who-claim-to-work-75-hour-weeks-are-lying.html</dc:identifier>
</item>
<item>
<title>MuseNet</title>
<link>https://openai.com/blog/musenet/</link>
<guid isPermaLink="true" >https://openai.com/blog/musenet/</guid>
<description>&lt;div class=&quot;js-excerpt&quot; readability=&quot;16.524475524476&quot;&gt;
&lt;p&gt;We've created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT-2&lt;/a&gt;, a large-scale &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;transformer&lt;/a&gt; model trained to predict the next token in a sequence, whether audio or text.&lt;/p&gt;
&lt;/div&gt;&lt;div class=&quot;bg-fg-5 color-fg-80 p-0.5 mt-n0.5 mb-2 rounded small-copy&quot; readability=&quot;9.5205479452055&quot;&gt;Through May 12th, we're inviting everyone to &lt;a href=&quot;https://openai.com/blog/musenet/#try&quot;&gt;try out&lt;/a&gt; our prototype MuseNet-powered music generation tool! See the Try MuseNet section of the post.&lt;/div&gt;
&lt;h2 id=&quot;samples&quot;&gt;Samples&lt;/h2&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;p&gt;Since MuseNet knows many different styles, we can blend generations in novel ways. Here the model is given the first 6 notes of a Chopin Nocturne, but is asked to generate a piece in a pop style with piano, drums, bass, and guitar. The model manages to blend the two styles convincingly, with the full band joining in at around the 30 second mark:&lt;/p&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;h2 id=&quot;try&quot;&gt;Try MuseNet&lt;/h2&gt;
&lt;p&gt;Our prototype of a MuseNet-powered co-composer is included below, and will be available through May 12th (at which point we'll chart a next step based on feedback). It allows only a subset of MuseNet's options. We’re excited to see how musicians and non-musicians alike will use MuseNet to create new compositions!&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;simple mode&lt;/em&gt; (shown by default), you'll hear random uncurated samples that we've pre-generated. Choose a composer or style, an optional start of a famous piece, and start generating. This lets you explore the variety of musical styles the model can create. In &lt;em&gt;advanced mode&lt;/em&gt; you can interact with the model directly. The completions will take longer, but you'll be creating an entirely new piece.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;The tool is currently a prototype, so please be patient! MuseNet's limitations include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The instruments you ask for are strong suggestions, not requirements. MuseNet generates each note by calculating the probabilities across all possible notes and instruments. The model shifts to make your instrument choices more likely, but there's always a chance it will choose something else.&lt;/li&gt;
&lt;li&gt;MuseNet has a more difficult time with odd pairings of styles and instruments (such as Chopin with bass and drums). Generations will be more natural if you pick instruments closest to the composer or band’s usual style.&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;composerandinstrumentationtokens&quot;&gt;Composer and instrumentation tokens&lt;/h2&gt;
&lt;p&gt;We created composer and instrumentation tokens to give more control over the kinds of samples MuseNet generates. During training time, these composer and instrumentation tokens were prepended to each sample, so the model would learn to use this information in making note predictions. At generation time, we can then condition the model to create samples in a chosen style by starting with a prompt such as a Rachmaninoff piano start:&lt;/p&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;p&gt;Or prompted with the band Journey, with piano, bass, guitar, and drums:&lt;/p&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;p&gt;We can visualize the embeddings from MuseNet to gain insight into what the model has learned. Here we use &lt;a href=&quot;https://lvdmaaten.github.io/tsne/&quot;&gt;t-SNE&lt;/a&gt; to create a 2-D map of the cosine similarity of various musical composer and style embeddings.&lt;/p&gt;
&lt;div class=&quot;full pt-1 pt-md-1.5 pb-1 pb-lg-2 my-2.5 small-copy scrim bg-cover&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;container&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Hover over a specific composer or style to see how it relates to others.&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;h2 id=&quot;longtermstructure&quot;&gt;Long-term structure&lt;/h2&gt;
&lt;p&gt;MuseNet uses the recompute and optimized kernels of &lt;a href=&quot;https://openai.com/blog/sparse-transformer/&quot;&gt;Sparse Transformer&lt;/a&gt; to train a 72-layer network with 24 attention heads—with full attention over a context of 4096 tokens. This long context may be one reason why it is able to remember long term structure in a piece, like in the following sample imitating Chopin:&lt;/p&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;p&gt;It can also create musical melodic structures, as in this sample imitating Mozart:&lt;/p&gt;
&lt;section&gt;&lt;hr class=&quot;my-0.5&quot;/&gt;
&lt;hr class=&quot;my-0.5&quot;/&gt;&lt;/section&gt;&lt;p&gt;Music generation is a useful domain for testing the Sparse Transformer as it sits on a middle ground between text and images. It has the fluid token structure of text (in images you can look back N tokens and find the row above, whereas in music there’s not a fixed number for looking back to the previous measure). Yet we can easily hear whether the model is capturing long term structure on the order of hundreds to thousands of tokens. It’s much more obvious if a music model messes up structure by changing the rhythm, in a way that it’s less clear if a text model goes on a brief tangent.&lt;/p&gt;
&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;
&lt;p&gt;We collected training data for MuseNet from many different sources. &lt;a href=&quot;https://www.classicalarchives.com/&quot;&gt;ClassicalArchives&lt;/a&gt; and &lt;a href=&quot;https://bitmidi.com/&quot;&gt;BitMidi&lt;/a&gt; donated their large collections of MIDI files for this project, and we also found several collections online, including jazz, pop, African, Indian, and Arabic styles. Additionally, we used the &lt;a href=&quot;https://arxiv.org/abs/1810.12247&quot;&gt;MAESTRO dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The transformer is trained on sequential data: given a set of notes, we ask it to predict the upcoming note. We experimented with several different ways to encode the MIDI files into tokens suitable for this task. First, a chordwise approach that considered every combination of notes sounding at one time as an individual &quot;chord&quot;, and assigned a token to each chord. Second, we tried condensing the musical patterns by only focusing on the starts of notes, and tried further compressing that using a byte pair encoding scheme.&lt;/p&gt;
&lt;p&gt;We also tried two different methods of marking the passage of time: either tokens that were scaled according to the piece’s tempo (so that the tokens represented a musical beat or fraction of a beat), or tokens that marked absolute time in seconds. We landed on an encoding that combines expressivity with conciseness: combining the pitch, volume, and instrument information into a single token.&lt;/p&gt;
&lt;pre class=&quot;language-python&quot;&gt;
&lt;code class=&quot;language-python&quot;&gt;bach piano_strings start tempo90 piano:v72:G1 piano:v72:G2 piano:v72:B4 piano:v72:D4 violin:v80:G4 piano:v72:G4 piano:v72:B5 piano:v72:D5 wait:12 piano:v0:B5 wait:5 piano:v72:D5 wait:12 piano:v0:D5 wait:4 piano:v0:G1 piano:v0:G2 piano:v0:B4 piano:v0:D4 violin:v0:G4 piano:v0:G4 wait:1 piano:v72:G5 wait:12 piano:v0:G5 wait:5 piano:v72:D5 wait:12 piano:v0:D5 wait:5 piano:v72:B5 wait:12&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Sample encoding&lt;/p&gt;
&lt;p&gt;During training, we:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Transpose the notes by raising and lowering the pitches (later in training, we reduce the amount of transposition so that generations stay within the individual instrument ranges).&lt;/li&gt;
&lt;li&gt;Augment the volumes, turning up or turning down the overall volumes of the various samples.&lt;/li&gt;
&lt;li&gt;Augment timing (when using the absolute time in seconds encoding), effectively slightly slowing or speeding up the pieces.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup&lt;/a&gt; on the token embedding space&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;We also create an inner critic: the model is asked during training time to predict whether a given sample is truly from the dataset or if it is one of the model's own past generations. This score is used to select samples at generation time.&lt;/p&gt;
&lt;h2 id=&quot;embeddings&quot;&gt;Embeddings&lt;/h2&gt;
&lt;p&gt;We added several different kinds of embeddings to give the model more structural context. In addition to the standard positional embeddings, we added a learned embedding that tracks the passage of time in a given sample. This way, all of the notes that sound at the same time are given the same timing embedding. We then add an embedding for each note in a chord (this mimics relative attention, since it will be easier for the model to learn that note 4 needs to look back at note 3, or else at note 4 of the previous chord). Finally, we add two structural embeddings which tell the model where a given musical sample is within the larger musical piece. One embedding divides the larger piece into 128 parts, while the second encoding is a countdown from 127 to 0 as the model approaches the (end) token.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;We’re excited to see (or rather, hear) what people create! If you create a piece you like, you can upload it to a free service like &lt;a href=&quot;https://instaud.io/&quot;&gt;Instaudio&lt;/a&gt; and then tweet us the link (the co-composer has a tweet button to help with this).&lt;/p&gt;
&lt;p&gt;If you’re interested in learning more about OpenAI’s music work, consider &lt;a href=&quot;https://openai.com/jobs/&quot;&gt;applying&lt;/a&gt; to join our team. Please feel free to &lt;a href=&quot;mailto:musenet@openai.com&quot;&gt;email us&lt;/a&gt; with suggestions for the co-composer interface. We'd also love to hear from you if you're interested in composing with MuseNet in more depth, or if you have MIDI files you'd like to add to the training set.&lt;/p&gt;
&lt;section class=&quot;mt-1.5 rounded&quot;&gt;&lt;iframe src=&quot;https://player.twitch.tv/?autoplay=false&amp;amp;video=v416276005&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;true&quot; scrolling=&quot;no&quot; height=&quot;378&quot; width=&quot;620&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/section&gt;&lt;div class=&quot;caption&quot; readability=&quot;10.975609756098&quot;&gt;MuseNet played an experimental concert on April 25th, 2019, livestreamed on OpenAI’s &lt;a href=&quot;https://www.twitch.tv/openai&quot;&gt;Twitch channel&lt;/a&gt;, in which no human (including us) had heard the pieces before.&lt;/div&gt;
</description>
<pubDate>Thu, 25 Apr 2019 16:29:53 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:type>article</og:type>
<og:title>MuseNet</og:title>
<og:description>We’ve created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments and can combine styles from country to Mozart to the Beatles.</og:description>
<og:url>https://openai.com/blog/musenet/</og:url>
<og:image>https://openai.com/content/images/2019/04/Screen-Shot-2019-04-25-at-7.37.40-AM-1.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://openai.com/blog/musenet/</dc:identifier>
</item>
<item>
<title>A Recipe for Training Neural Networks</title>
<link>https://karpathy.github.io/2019/04/25/recipe/</link>
<guid isPermaLink="true" >https://karpathy.github.io/2019/04/25/recipe/</guid>
<description>&lt;p&gt;Some few weeks ago I &lt;a href=&quot;https://twitter.com/karpathy/status/1013244313327681536?lang=en&quot;&gt;posted&lt;/a&gt; a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a &lt;a href=&quot;https://www.bigmarker.com/missinglink-ai/PyTorch-Code-to-Unpack-Andrej-Karpathy-s-6-Most-Common-NN-Mistakes&quot;&gt;webinar&lt;/a&gt; :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.&lt;/p&gt;&lt;p&gt;So I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.&lt;/p&gt;
&lt;h4 id=&quot;1-neural-net-training-is-a-leaky-abstraction&quot;&gt;1) Neural net training is a leaky abstraction&lt;/h4&gt;
&lt;p&gt;It is allegedly easy to get started with training neural nets. Numerous libraries and frameworks take pride in displaying 30-line miracle snippets that solve your data problems, giving the (false) impression that this stuff is plug and play. It’s common see things like:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;your_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# plug your awesome dataset here&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SuperCrossValidator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SuperDuper&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;your_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResNet50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGDOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# conquer world here&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;These libraries and examples activate the part of our brain that is familiar with standard software - a place where clean APIs and abstractions are often attainable. &lt;a href=&quot;http://docs.python-requests.org/en/master/&quot;&gt;Requests&lt;/a&gt; library to demonstrate:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://api.github.com/user'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'user'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'pass'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;That’s cool! A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are familiar with and expect. Unfortunately, neural nets are nothing like that. They are not “off-the-shelf” technology the second you deviate slightly from training an ImageNet classifier. I’ve tried to make this point in my post &lt;a href=&quot;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&quot;&gt;“Yes you should understand backprop”&lt;/a&gt; by picking on backpropagation and calling it a “leaky abstraction”, but the situation is unfortunately much more dire. Backprop + SGD does not magically make your network work. Batch norm does not magically make it converge faster. RNNs don’t magically let you “plug in” text. And just because you can formulate your problem as RL doesn’t mean you should. If you insist on using the technology without understanding how it works you are likely to fail. Which brings me to…&lt;/p&gt;
&lt;h4 id=&quot;2-neural-net-training-fails-silently&quot;&gt;2) Neural net training fails silently&lt;/h4&gt;
&lt;p&gt;When you break or misconfigure code you will often get some kind of an exception. You plugged in an integer where something expected a string. The function only expected 3 arguments. This import failed. That key does not exist. The number of elements in the two lists isn’t equal. In addition, it’s often possible to create unit tests for a certain functionality.&lt;/p&gt;
&lt;p&gt;This is just a start when it comes to training neural nets. Everything could be correct syntactically, but the whole thing isn’t arranged properly, and it’s really hard to tell. The “possible error surface” is large, logical (as opposed to syntactic), and very tricky to unit test. For example, perhaps you forgot to flip your labels when you left-right flipped the image during data augmentation. Your net can still (shockingly) work pretty well because your network can internally learn to detect flipped images and then it left-right flips its predictions. Or maybe your autoregressive model accidentally takes the thing it’s trying to predict as an input due to an off-by-one bug. Or you tried to clip your gradients but instead clipped the loss, causing the outlier examples to be ignored during training. Or you initialized your weights from a pretrained checkpoint but didn’t use the original mean. Or you just screwed up the settings for regularization strengths, learning rate, its decay rate, model size, etc. Therefore, your misconfigured neural net will throw exceptions only if you’re lucky; Most of the time it will train but silently work a bit worse.&lt;/p&gt;
&lt;p&gt;As a result, (and this is reeaally difficult to over-emphasize) &lt;strong&gt;a “fast and furious” approach to training neural networks does not work&lt;/strong&gt; and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.&lt;/p&gt;
&lt;h2 id=&quot;the-recipe&quot;&gt;The recipe&lt;/h2&gt;
&lt;p&gt;In light of the above two facts, I have developed a specific process for myself that I follow when applying a neural net to a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.&lt;/p&gt;
&lt;h4 id=&quot;1-become-one-with-the-data&quot;&gt;1. Become one with the data&lt;/h4&gt;
&lt;p&gt;The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. Luckily, your brain is pretty good at this. One time I discovered that the data contained duplicate examples. Another time I found corrupted images / labels. I look for data imbalances and biases. I will typically also pay attention to my own process for classifying the data, which hints at the kinds of architectures we’ll eventually explore. As an example - are very local features enough or do we need global context? How much variation is there and what form does it take? What variation is spurious and could be preprocessed out? Does spatial position matter or do we want to average pool it out? How much does detail matter and how far could we afford to downsample the images? How noisy are the labels?&lt;/p&gt;
&lt;p&gt;In addition, since the neural net is effectively a compressed/compiled version of your dataset, you’ll be able to look at your network (mis)predictions and understand where they might be coming from. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.&lt;/p&gt;
&lt;p&gt;Once you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.&lt;/p&gt;
&lt;h4 id=&quot;2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines&quot;&gt;2. Set up the end-to-end training/evaluation skeleton + get dumb baselines&lt;/h4&gt;
&lt;p&gt;Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.&lt;/p&gt;
&lt;p&gt;Tips &amp;amp; tricks for this stage:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;fix random seed&lt;/strong&gt;. Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome. This removes a factor of variation and will help keep you sane.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;simplify&lt;/strong&gt;. Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage. Data augmentation is a regularization strategy that we may incorporate later, but for now it is just another opportunity to introduce some dumb bug.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;add significant digits to your eval&lt;/strong&gt;. When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;verify loss @ init&lt;/strong&gt;. Verify that your loss starts at the correct loss value. E.g. if you initialize your final layer correctly you should measure &lt;code class=&quot;highlighter-rouge&quot;&gt;-log(1/n_classes)&lt;/code&gt; on a softmax at initialization. The same default values can be derived for L2 regression, Huber losses, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;init well&lt;/strong&gt;. Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up convergence and eliminate “hockey stick” loss curves where in the first few iteration your network is basically just learning the bias.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;human baseline&lt;/strong&gt;. Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it. Alternatively, annotate the test data twice and for each example treat one annotation as prediction and the second as ground truth.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;input-indepent baseline&lt;/strong&gt;. Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;overfit one batch&lt;/strong&gt;. Overfit a single batch of only a few examples (e.g. as little as two). To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). I also like to visualize in the same plot both the label and the prediction and ensure that they end up aligning perfectly once we reach the minimum loss. If they do not, there is a bug somewhere and we cannot continue to the next stage.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;verify decreasing training loss&lt;/strong&gt;. At this stage you will hopefully be underfitting on your dataset because you’re working with a toy model. Try to increase its capacity just a bit. Did your training loss go down as it should?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualize just before the net&lt;/strong&gt;. The unambiguously correct place to visualize your data is immediately before your &lt;code class=&quot;highlighter-rouge&quot;&gt;y_hat = model(x)&lt;/code&gt; (or &lt;code class=&quot;highlighter-rouge&quot;&gt;sess.run&lt;/code&gt; in tf). That is - you want to visualize &lt;em&gt;exactly&lt;/em&gt; what goes into your network, decoding that raw tensor of data and labels into visualizations. This is the only “source of truth”. I can’t count the number of times this has saved me and revealed problems in data preprocessing and augmentation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualize prediction dynamics&lt;/strong&gt;. I like to visualize model predictions on a fixed test batch during the course of training. The “dynamics” of how these predictions move will give you incredibly good intuition for how the training progresses. Many times it is possible to feel the network “struggle” to fit your data if it wiggles too much in some way, revealing instabilities. Very low or very high learning rates are also easily noticeable in the amount of jitter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;use backprop to chart dependencies&lt;/strong&gt;. Your deep learning code will often contain complicated, vectorized, and broadcasted operations. A relatively common bug I’ve come across a few times is that people get this wrong (e.g. they use &lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;transpose/permute&lt;/code&gt; somewhere) and inadvertently mix information across the batch dimension. It is a depressing fact that your network will typically still train okay because it will learn to ignore data from the other examples. One way to debug this (and other related problems) is to set the loss for some example &lt;strong&gt;i&lt;/strong&gt; to be 1.0, run the backward pass all the way to the input, and ensure that you get a non-zero gradient only on the &lt;strong&gt;i-th&lt;/strong&gt; example. More generally, gradients give you information about what depends on what in your network, which can be useful for debugging.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;generalize a special case&lt;/strong&gt;. This is a bit more of a general coding tip but I’ve often seen people create bugs when they bite off more than they can chew, writing a relatively general functionality from scratch. I like to write a very specific function to what I’m doing right now, get that to work, and then generalize it later making sure that I get the same result. Often this applies to vectorizing code, where I almost always write out the fully loopy version first and only then transform it to vectorized code one loop at a time.&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;3-overfit&quot;&gt;3. Overfit&lt;/h4&gt;
&lt;p&gt;At this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these), and we have a rough sense of the performance of a human (we hope to reach this). The stage is now set for iterating on a good model.&lt;/p&gt;
&lt;p&gt;The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.&lt;/p&gt;
&lt;p&gt;A few tips &amp;amp; tricks for this stage:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;picking the model&lt;/strong&gt;. To reach a good training loss you’ll want to choose an appropriate architecture for the data. When it comes to choosing this my #1 advice is: &lt;strong&gt;Don’t be a hero&lt;/strong&gt;. I’ve seen a lot of people who are eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to them. Resist this temptation strongly in the early stages of your project. I always advise people to simply find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. if you are classifying images don’t be a hero and just copy paste a ResNet-50 for your first run. You’re allowed to do something more custom later and beat this.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;adam is safe&lt;/strong&gt;. In the early stages of setting baselines I like to use Adam with a learning rate of &lt;a href=&quot;https://twitter.com/karpathy/status/801621764144971776?lang=en&quot;&gt;3e-4&lt;/a&gt;. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If you are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, don’t be a hero and follow whatever the most related papers do.)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;complexify only one at a time&lt;/strong&gt;. If you have multiple signals to plug into your classifier I would advise that you plug them in one by one and every time ensure that you get a performance boost you’d expect. Don’t throw the kitchen sink at your model at the start. There are other ways of building up complexity - e.g. you can try to plug in smaller images first and make them bigger later, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;do not trust learning rate decay defaults&lt;/strong&gt;. If you are re-purposing code from some other domain always be very careful with learning rate decay. Not only would you want to use different decay schedules for different problems, but - even worse - in a typical implementation the schedule will be based current epoch number, which can vary widely simply depending on the size of your dataset. E.g. ImageNet would decay by 10 on epoch 30. If you’re not training ImageNet then you almost certainly do not want this. If you’re not careful your code could secretely be driving your learning rate to zero too early, not allowing your model to converge. In my own work I always disable learning rate decays entirely (I use a constant LR) and tune this all the way at the very end.&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;4-regularize&quot;&gt;4. Regularize&lt;/h4&gt;
&lt;p&gt;Ideally, we are now at a place where we have a large model that is fitting at least the training set. Now it is time to regularize it and gain some validation accuracy by giving up some of the training accuracy. Some tips &amp;amp; tricks:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;get more data&lt;/strong&gt;. First, the by far best and preferred way to regularize a model in any practical setting is to add more real training data. It is a very common mistake to spend a lot engineering cycles trying to squeeze juice out of a small dataset when you could instead be collecting more data. As far as I’m aware adding more data is pretty much the only guaranteed way to monotonically improve the performance of a well-configured neural network almost indefinitely. The other would be ensembles (if you can afford them), but that tops out after ~5 models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;data augment&lt;/strong&gt;. The next best thing to real data is half-fake data - try out more aggressive data augmentation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;creative augmentation&lt;/strong&gt;. If half-fake data doesn’t do it, fake data may also do something. People are finding creative ways of expanding datasets; For example, &lt;a href=&quot;https://openai.com/blog/learning-dexterity/&quot;&gt;domain randomization&lt;/a&gt;, use of &lt;a href=&quot;http://vladlen.info/publications/playing-data-ground-truth-computer-games/&quot;&gt;simulation&lt;/a&gt;, clever &lt;a href=&quot;https://arxiv.org/abs/1708.01642&quot;&gt;hybrids&lt;/a&gt; such as inserting (potentially simulated) data into scenes, or even GANs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pretrain&lt;/strong&gt;. It rarely ever hurts to use a pretrained network if you can, even if you have enough data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;stick with supervised learning&lt;/strong&gt;. Do not get over-excited about unsupervised pretraining. Unlike what that blog post from 2008 tells you, as far as I know, no version of it has reported strong results in modern computer vision (though NLP seems to be doing pretty well with BERT and friends these days, quite likely owing to the more deliberate nature of text, and a higher signal to noise ratio).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;smaller input dimensionality&lt;/strong&gt;. Remove features that may contain spurious signal. Any added spurious input is just another opportunity to overfit if your dataset is small. Similarly, if low-level details don’t matter much try to input a smaller image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;smaller model size&lt;/strong&gt;. In many cases you can use domain knowledge constraints on the network to decrease its size. As an example, it used to be trendy to use Fully Connected layers at the top of backbones for ImageNet but these have since been replaced with simple average pooling, eliminating a ton of parameters in the process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;decrease the batch size&lt;/strong&gt;. Due to the normalization inside batch norm smaller batch sizes somewhat correspond to stronger regularization. This is because the batch empirical mean/std are more approximate versions of the full mean/std so the scale &amp;amp; offset “wiggles” your batch around more.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;drop&lt;/strong&gt;. Add dropout. Use dropout2d (spatial dropout) for ConvNets. Use this sparingly/carefully because dropout &lt;a href=&quot;https://arxiv.org/abs/1801.05134&quot;&gt;does not seem to play nice&lt;/a&gt; with batch normalization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;weight decay&lt;/strong&gt;. Increase the weight decay penalty.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;early stopping&lt;/strong&gt;. Stop training based on your measured validation loss to catch your model just as it’s about to overfit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;try a larger model&lt;/strong&gt;. I mention this last and only after early stopping but I’ve found a few times in the past that larger models will of course overfit much more eventually, but their “early stopped” performance can often be much better than that of smaller models.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Finally, to gain additional confidence that your network is a reasonable classifier, I like to visualize the network’s first-layer weights and ensure you get nice edges that make sense. If your first layer filters look like noise then something could be off. Similarly, activations inside the net can sometimes display odd artifacts and hint at problems.&lt;/p&gt;
&lt;h4 id=&quot;5-tune&quot;&gt;5. Tune&lt;/h4&gt;
&lt;p&gt;You should now be “in the loop” with your dataset exploring a wide model space for architectures that achieve low validation loss. A few tips and tricks for this step:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;random over grid search&lt;/strong&gt;. For simultaneously tuning multiple hyperparameters it may sound tempting to use grid search to ensure coverage of all settings, but keep in mind that it is &lt;a href=&quot;http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf&quot;&gt;best to use random search instead&lt;/a&gt;. Intuitively, this is because neural nets are often much more sensitive to some parameters than others. In the limit, if a parameter &lt;strong&gt;a&lt;/strong&gt; matters but changing &lt;strong&gt;b&lt;/strong&gt; has no effect then you’d rather sample &lt;strong&gt;a&lt;/strong&gt; more throughly than at a few fixed points multiple times.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;hyper-parameter optimization&lt;/strong&gt;. There is a large number of fancy bayesian hyper-parameter optimization toolboxes around and a few of my friends have also reported success with them, but my personal experience is that the state of the art approach to exploring a nice and wide space of models and hyperparameters is to use an intern :). Just kidding.&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;6-squeeze-out-the-juice&quot;&gt;6. Squeeze out the juice&lt;/h4&gt;
&lt;p&gt;Once you find the best types of architectures and hyper-parameters you can still use a few more tricks to squeeze out the last pieces of juice out of the system:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;ensembles&lt;/strong&gt;. Model ensembles are a pretty much guaranteed way to gain 2% of accuracy on anything. If you can’t afford the computation at test time look into distilling your ensemble into a network using &lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;dark knowledge&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;leave it training&lt;/strong&gt;. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”).&lt;/li&gt;
&lt;/ul&gt;&lt;h4 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;Once you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!&lt;/p&gt;
</description>
<pubDate>Thu, 25 Apr 2019 16:24:12 +0000</pubDate>
<dc:creator>yigitdemirag</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://karpathy.github.io/2019/04/25/recipe/</dc:identifier>
</item>
<item>
<title>Bullshitters: Who Are They and What Do We Know about Their Lives? [pdf]</title>
<link>http://ftp.iza.org/dp12282.pdf</link>
<guid isPermaLink="true" >http://ftp.iza.org/dp12282.pdf</guid>
<description>&lt;a href=&quot;http://ftp.iza.org/dp12282.pdf&quot;&gt;Download PDF&lt;/a&gt;</description>
<pubDate>Thu, 25 Apr 2019 15:26:26 +0000</pubDate>
<dc:creator>etxm</dc:creator>
<dc:format>application/pdf</dc:format>
<dc:identifier>http://ftp.iza.org/dp12282.pdf</dc:identifier>
</item>
<item>
<title>Flutter desktop shells</title>
<link>https://github.com/flutter/flutter/wiki/Desktop-shells</link>
<guid isPermaLink="true" >https://github.com/flutter/flutter/wiki/Desktop-shells</guid>
<description>&lt;div class=&quot;markdown-body&quot;&gt;
&lt;p&gt;Work is ongoing to extend Flutter to support desktop as a target environment, allowing developers to create macOS, Windows, and Linux applications with Flutter. On the long run, this effort will create lead to a fully integrated solution where &lt;code&gt;flutter create&lt;/code&gt;, &lt;code&gt;flutter run&lt;/code&gt;, and &lt;code&gt;flutter build&lt;/code&gt; work for desktop platforms as they do for mobile platforms, but currently this effort is still under way.&lt;/p&gt;
&lt;h2&gt;Current Status&lt;/h2&gt;
&lt;p&gt;A high-level overview of the status of each platform is provided below. For details, see &lt;a href=&quot;https://github.com/flutter/engine/tree/master/shell/platform/&quot;&gt;the source&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; The Flutter desktop APIs are still in early stages of development, and are &lt;strong&gt;subject to change without warning&lt;/strong&gt;. No backwards compatibility, either API or ABI, will be provided. Expect any code using these libraries to need to be updated and recompiled after any Flutter update.&lt;/p&gt;
&lt;h3&gt;macOS&lt;/h3&gt;
&lt;p&gt;This is the most mature of the desktop platforms (for various reasons, including that it's quite close to iOS, which we already support).&lt;/p&gt;
&lt;p&gt;Classes starting with &lt;code&gt;Flutter&lt;/code&gt; are shared with iOS, and should be essentially stable. Classes starting with &lt;code&gt;FLE&lt;/code&gt; are still in early stages.&lt;/p&gt;
&lt;h3&gt;Windows&lt;/h3&gt;
&lt;p&gt;The current Windows shell is a GLFW placeholder, to allow early experimentation. It will be replaced in the future with a Win32 or UWP shell that allows view-level embedding of Flutter within an application.&lt;/p&gt;
&lt;p&gt;Expect the APIs for the final shell to be radically different from the current implementation.&lt;/p&gt;
&lt;h3&gt;Linux&lt;/h3&gt;
&lt;p&gt;The current Linux shell is a GLFW placeholder, to allow early experimentation. We would like to create a library that lets you embed Flutter regardless of whether you're using GTK+, Qt, wxWidgets, Motif, or another arbitrary toolkit for other parts of your application, but have not yet determined a good way to do that.&lt;/p&gt;
&lt;p&gt;Expect the APIs for the final shell to be radically different from the current implementation.&lt;/p&gt;
&lt;h3&gt;Plugins&lt;/h3&gt;
&lt;p&gt;Writing plugins is supported on all platforms, however, there are currently very few plugins that actually have desktop support (&lt;a href=&quot;https://github.com/google/flutter-desktop-embedding/tree/master/plugins&quot;&gt;such as these plugins from the flutter-desktop-embedding project&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;For Windows and Linux, only the JSON codec is supported at the moment (which means the Dart side of most existing plugins would need to be changed to implement desktop support). The standard codec should be coming soon.&lt;/p&gt;
&lt;h3&gt;Tooling&lt;/h3&gt;
&lt;p&gt;Support for desktop in the &lt;code&gt;flutter&lt;/code&gt; tool is a work in progress. To use any of the support (such the host machine being listed by &lt;code&gt;flutter devices&lt;/code&gt;), two things must currently be true:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You must not be on the &lt;code&gt;stable&lt;/code&gt; &lt;a href=&quot;https://github.com/flutter/flutter/wiki/Flutter-build-release-channels&quot;&gt;Flutter channel&lt;/a&gt;. This is to make it clear that desktop support is not yet considered stable and production-ready.&lt;/li&gt;
&lt;li&gt;You must set the &lt;code&gt;ENABLE_FLUTTER_DESKTOP&lt;/code&gt; environment variable to &lt;code&gt;true&lt;/code&gt;. This is to avoid interfering with existing mobile development workflows while the long-term solution is being worked out (see &lt;a href=&quot;https://github.com/flutter/flutter/issues/30724&quot;&gt;#30724&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Prebuilt Shell Libraries&lt;/h2&gt;
&lt;p&gt;The desktop libraries are not currently downloaded by default, but can be downloaded to Flutter's engine artifact cache by running &lt;code&gt;flutter precache&lt;/code&gt; with the &lt;code&gt;--linux&lt;/code&gt;, &lt;code&gt;--macos&lt;/code&gt;, or &lt;code&gt;--windows&lt;/code&gt; flag, depending on your platform.&lt;/p&gt;
&lt;h3&gt;C++ Wrapper&lt;/h3&gt;
&lt;p&gt;The Windows and Linux libraries provide a C API. To make it easier to use them, there is a C++ wrapper available which you can build into your application to provide a higher-level API surface. The &lt;code&gt;precache&lt;/code&gt; command above will download the source for this wrapper into a &lt;code&gt;cpp_client_wrapper&lt;/code&gt; folder next to the library.&lt;/p&gt;
&lt;h2&gt;Using the Shells&lt;/h2&gt;
&lt;p&gt;Since there is currently no tooling support for desktop shells, you will need to write a runner application yourself, and link in the library, as well as any plugins you are using. This will require some familiarity with doing native development on your platform(s); if you don't already have experience with that, you'll probably want to check back later, when &lt;code&gt;flutter&lt;/code&gt; tool support for desktop is available.&lt;/p&gt;
&lt;p&gt;See the headers that come with the library for your platform for information on using them. More documentation will be available in the future; for now it may be helpful to look at &lt;a href=&quot;https://github.com/google/flutter-desktop-embedding/tree/master/example&quot;&gt;the flutter-desktop-embedding example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to the Flutter library, your application will need to bundle your Flutter assets (as created by &lt;code&gt;flutter build bundle&lt;/code&gt;). On Windows and Linux you will also nee the ICU data from the Flutter engine (look for &lt;code&gt;icudtl.dat&lt;/code&gt; under the &lt;code&gt;bin/cache/artifacts/engine&lt;/code&gt; directory in your Flutter tree).&lt;/p&gt;
&lt;h4&gt;macOS Note&lt;/h4&gt;
&lt;p&gt;Currently you must set up your FLEView in a XIB, rather than in code (this will change in the future). To do so:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Drag in an OpenGL View.&lt;/li&gt;
&lt;li&gt;Change the class to FLEView.&lt;/li&gt;
&lt;li&gt;Check the Double Buffer option. If your view doesn't draw, you have likely forgotten this step.&lt;/li&gt;
&lt;li&gt;Check the Supports Hi-Res Backing option. If you only see a portion of your application when running on a high-DPI monitor, you have likely forgotten this step.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Plugins&lt;/h3&gt;
&lt;h4&gt;macOS&lt;/h4&gt;
&lt;p&gt;When you set up your FLEViewController, before calling &lt;code&gt;launchEngine...&lt;/code&gt;, call &lt;code&gt;-registerWithRegistrar:&lt;/code&gt; on each plugin you want to use. For instance:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-objc&quot;&gt;
&lt;pre&gt;
  [XYZMyAwesomePlugin &lt;span class=&quot;pl-c1&quot;&gt;registerWithRegistrar:&lt;/span&gt;
      [myFlutterViewController &lt;span class=&quot;pl-c1&quot;&gt;registrarForPlugin:&lt;/span&gt;&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;XYZMyAwesomePlugin&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;]];
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Windows/Linux&lt;/h4&gt;
&lt;p&gt;After creating your Flutter window controller, call your plugin's registrar function. For instance:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c++&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-en&quot;&gt;MyAwesomePluginRegisterWithRegistrar&lt;/span&gt;(
      flutter_controller.GetRegistrarForPlugin(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;MyAwesomePlugin&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;));
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Flutter Application Requirements&lt;/h2&gt;
&lt;p&gt;Because desktop platforms are not yet fully supported by the Flutter framework, existing Flutter applications are likely to require slight modifications to run.&lt;/p&gt;
&lt;h3&gt;Target Platform Override&lt;/h3&gt;
&lt;p&gt;Most applications will need to override the target platform for the application to one of the supported values in order to avoid 'Unknown platform' exceptions. This should be done as early as possible.&lt;/p&gt;
&lt;p&gt;In the simplest case, where the code will only run on desktop and the behavior should be consistent on all platforms, you can hard-code a single target:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-dart&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;'package:flutter/foundation.dart'&lt;/span&gt;
    &lt;span class=&quot;pl-k&quot;&gt;show&lt;/span&gt; debugDefaultTargetPlatformOverride;
[...]

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() {
  debugDefaultTargetPlatformOverride &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;TargetPlatform&lt;/span&gt;.fuchsia;
  [...]
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If the code needs to run on both mobile and desktop, or you want different behavior on different desktop platforms, you can conditionalize on &lt;code&gt;Platform&lt;/code&gt;. For example, the line in &lt;code&gt;main()&lt;/code&gt; above could be replaced with a call to:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-dart&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;///&lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt; If the current platform is desktop, override the default platform to&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;///&lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt; a supported platform (iOS for macOS, Android for Linux and Windows).&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;///&lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt; Otherwise, do nothing.&lt;/span&gt;
&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;_setTargetPlatformForDesktop&lt;/span&gt;() {
  &lt;span class=&quot;pl-c1&quot;&gt;TargetPlatform&lt;/span&gt; targetPlatform;
  &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;Platform&lt;/span&gt;.isMacOS) {
    targetPlatform &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;TargetPlatform&lt;/span&gt;.iOS;
  } &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;Platform&lt;/span&gt;.isLinux &lt;span class=&quot;pl-k&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;Platform&lt;/span&gt;.isWindows) {
    targetPlatform &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;TargetPlatform&lt;/span&gt;.android;
  }
  &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (targetPlatform &lt;span class=&quot;pl-k&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;null&lt;/span&gt;) {
    debugDefaultTargetPlatformOverride &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; targetPlatform;
  }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that the target platform you use will affect not only the behavior and appearance of the widgets, but also the expectations Flutter will have for what is available on the platform, such as fonts.&lt;/p&gt;
&lt;h3&gt;Fonts&lt;/h3&gt;
&lt;p&gt;Flutter applications may default to fonts that are standard for the target platform, but unavailable on desktop. For instance, if the target platform is &lt;code&gt;TargetPlatform.iOS&lt;/code&gt; the Material library will default to San Francisco, which is available on macOS but not Linux or Windows.&lt;/p&gt;
&lt;p&gt;Most applications will need to set the font (e.g., via &lt;code&gt;ThemeData&lt;/code&gt;) based on the host platform, or set a specific font that is bundled with the application.&lt;/p&gt;
&lt;p&gt;Symptoms of missing fonts include text failing to display and console logging about failure to load fonts.&lt;/p&gt;
&lt;h3&gt;Plugins&lt;/h3&gt;
&lt;p&gt;If your project uses any plugins (unless they have desktop support), they won't work, as the native side will be missing. Depending on how the Dart side of the plugin is written, they may fail gracefully, or may throw errors.&lt;/p&gt;
&lt;/div&gt;

</description>
<pubDate>Thu, 25 Apr 2019 13:21:02 +0000</pubDate>
<dc:creator>Memosyne</dc:creator>
<og:image>https://avatars3.githubusercontent.com/u/14101776?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>flutter/flutter</og:title>
<og:url>https://github.com/flutter/flutter</og:url>
<og:description>Flutter makes it easy and fast to build beautiful mobile apps. - flutter/flutter</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/flutter/flutter/wiki/Desktop-shells</dc:identifier>
</item>
<item>
<title>How to hide from the AI surveillance state with a color printout</title>
<link>https://www.technologyreview.com/f/613409/how-to-hide-from-the-ai-surveillance-state-with-a-color-printout/</link>
<guid isPermaLink="true" >https://www.technologyreview.com/f/613409/how-to-hide-from-the-ai-surveillance-state-with-a-color-printout/</guid>
<description>&lt;p class=&quot;jsx-1279085685&quot;&gt;AI-powered video technology is becoming ubiquitous, tracking our faces and bodies through stores, offices, and public spaces. In some countries the technology constitutes a powerful new layer of policing and government surveillance.&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;Fortunately, as some researchers from the Belgian university KU Leuven  &lt;span class=&quot;jsx-2473920458 storyLink&quot;&gt;&lt;a class=&quot;&quot; href=&quot;https://arxiv.org/abs/1904.08653&quot;&gt;have just shown&lt;/a&gt;&lt;/span&gt;, you can often hide from an AI video system with the aid of a simple color printout.&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;&lt;strong&gt;Who said that?&lt;/strong&gt; The researchers showed that the image they designed can hide a whole person from an AI-powered computer-vision system. They demonstrated it on a popular open-source object recognition system called &lt;span class=&quot;jsx-2473920458 storyLink&quot;&gt;&lt;a class=&quot;&quot; href=&quot;https://pjreddie.com/darknet/yolov2/&quot;&gt;YoLo(v2)&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;&lt;strong&gt;Hide and seek:&lt;/strong&gt; The trick could conceivably let crooks hide from security cameras, or offer dissidents a way to dodge government scrutiny. “What our work proves is that it is possible to bypass camera surveillance systems using adversarial patches,” says &lt;span class=&quot;jsx-2473920458 storyLink&quot;&gt;&lt;a class=&quot;&quot; href=&quot;https://www.kuleuven.be/wieiswie/en/person/00093819&quot;&gt;Wiebe Van Ranst&lt;/a&gt;&lt;/span&gt;, one of the authors.&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;&lt;strong&gt;Get lost:&lt;/strong&gt; Van Ranst says it shouldn’t be too hard to adapt the approach to off-the-shelf video surveillance systems. “At the moment we also need to know which detector is in use. What we’d like to do in the future is generate a patch that works on multiple detectors at the same time,” he told MIT Technology Review. “If this works, chances are high that the patch will also work on the detector that is in use in the surveillance system.”&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;&lt;strong&gt;Fool’s errand:&lt;/strong&gt; The deception demonstrated by the Belgian team exploits what’s known as adversarial machine learning. Most computer vision relies on training a (convolutional) neural network to recognize different things by feeding it examples and tweaking its parameters until it classifies objects correctly. By feeding examples into a trained deep neural net and monitoring the output, it is possible to infer what types of images confuse or fool the system.&lt;/p&gt;
&lt;p class=&quot;jsx-1279085685&quot;&gt;&lt;strong&gt;Eyes everywhere:&lt;/strong&gt; The work is significant because AI is increasingly found in everyday surveillance cameras and software. It’s even being used to obviate the need for a checkout line in some experimental stores, including &lt;span class=&quot;jsx-2473920458 storyLink&quot;&gt;&lt;a class=&quot;&quot; href=&quot;https://www.technologyreview.com/s/610006/amazons-checkout-free-grocery-store-is-opening-to-the-public/&quot;&gt;ones operated by Amazon&lt;/a&gt;&lt;/span&gt;. And in China the technology is emerging as a powerful new means of catching criminals as well as, more troublingly, &lt;span class=&quot;jsx-2473920458 storyLink&quot;&gt;&lt;a class=&quot;&quot; href=&quot;https://www.nytimes.com/2019/04/14/technology/china-surveillance-artificial-intelligence-racial-profiling.html&quot;&gt;tracking certain ethnic groups&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;



</description>
<pubDate>Thu, 25 Apr 2019 13:15:33 +0000</pubDate>
<dc:creator>hsnewman</dc:creator>
<og:title>How to hide from the AI surveillance state with a color printout</og:title>
<og:description>AI-powered video technology is becoming ubiquitous, tracking our faces and bodies through stores, offices, and public spaces.</og:description>
<og:type>article</og:type>
<og:url>https://www.technologyreview.com/f/613409/how-to-hide-from-the-ai-surveillance-state-with-a-color-printout/</og:url>
<og:image>https://cdn.technologyreview.com/i/images/acr13282673947520124191772.jpg?sw=1200&amp;cx=0&amp;cy=0&amp;cw=1834&amp;ch=1032</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.technologyreview.com/f/613409/how-to-hide-from-the-ai-surveillance-state-with-a-color-printout/</dc:identifier>
</item>
<item>
<title>Tesla Posts Big Quarterly Loss as Its Electric-Car Sales Lag</title>
<link>https://www.nytimes.com/2019/04/24/business/tesla-earnings-elon-musk.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2019/04/24/business/tesla-earnings-elon-musk.html</guid>
<description>&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla’s surprisingly weak &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2019/04/03/business/tesla-sales-elon-musk.html?rref=collection%2Fbyline%2Fneal-e.-boudette&amp;amp;action=click&amp;amp;contentCollection=undefined&amp;amp;region=stream&amp;amp;module=inline&amp;amp;version=latest&amp;amp;contentPlacement=6&amp;amp;pgtype=collection&quot; title=&quot;&quot;&gt;electric-car deliveries&lt;/a&gt; in the first quarter took a heavy toll on its bottom line.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The company &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://ir.tesla.com/static-files/b2218d34-fbee-4f1f-ac95-050eb29dd42f&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;said on Wednesday&lt;/a&gt; that it lost $702 million in the first quarter, a sharp reversal from the profits it made in the second half of last year. The loss, equivalent to $4.10 per share, was far greater than the $1.81 per share that Wall Street analysts, surveyed by FactSet, had forecast. The quarter’s revenue of $4.54 billion fell well short of expectations.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla had $2.2 billion of cash at the end of the first quarter, a 40 percent decline from the figure at the end of last year. The company spent $920 million paying off a bond in March. Tesla’s operations consumed $640 million of cash in the first quarter.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Investors had been bracing for the red ink after Tesla said it sold 31 percent fewer vehicles in the first quarter than in the fourth quarter of 2018. The company said logistical challenges had hindered deliveries of the Model 3 sedan to Europe and China. A reduction in a federal tax benefit for Tesla’s buyers may have weighed on Model 3 sales in the United States.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Another weak spot for the company was its solar business, where sales dropped by more than 35 percent in the quarter.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“We expected weak results, but Tesla’s revenue and profit misses were stunning,” Vicki Bryan, chief executive of Bond Angle, a research firm, said in an email. “Revenue was soft, but spiking costs for obvious strategy missteps really drew blood at the bottom line.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The big question for Tesla is whether the drop in car sales in the first quarter was a temporary phenomenon or something more serious. Sales of Model 3 sedans could recover as the company delivers cars overseas. But demand for the higher-priced Model S and Model X vehicles plunged 56 percent in the first quarter from the fourth, even though the company cut the price of the cars at the end of February.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla said on Wednesday that the reduction in the tax credit might have weakened demand for the Model X and the Model S. The company added that after the price cut, the increase in orders for high-end versions of those models exceeded the available supply. Tesla is also hoping that buyers flock to new versions of the S and X that can travel farther on a full charge.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;In an investor call after the results were announced, a stock analyst asked why Tesla was cutting prices if demand was strong for its products. Tesla’s chief executive, Elon Musk, responded that the goal was to make its cars “as affordable as possible.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Analysts had slashed their earnings estimates in recent weeks, and after the anemic delivery numbers, Tesla again faces doubts that it can achieve its goals and meet Wall Street’s financial targets. Its stock was little changed in extended trading after the earnings announcement, but it is down over 30 percent from its most recent high.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla said Wednesday that it expected to deliver 90,000 to 100,000 cars in the second quarter, up from 63,000 in the first three months of the year. It said it would lose money again in the second quarter, though less than in the first quarter, and would turn a profit in the third quarter.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla’s supporters are hoping that Model 3 sales will surge and enable the company to meet its target of delivering 360,000 to 400,000 vehicles this year. Tesla affirmed that goal on Wednesday, but said its vehicle production would be “significantly higher than deliveries,” because of the time it takes to transport cars from California to other countries.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The company is now offering a $35,000 version of Model 3. The lower price may stir up demand for the car, but charging less could make it harder for Tesla to make a profit on the vehicle.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Tesla’s cash position is crucial to the company’s future. Mr. Musk intends to produce new vehicles in volume, including a large truck called the Semi, but setting up the new production facilities would consume large amounts of cash. Many analysts expect that Tesla will have to issue new shares to raise money.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Mr. Musk, who had previously said the company did not need more capital, indicated that he had changed his mind because Tesla was now in a position to use capital more efficiently. “There is merit to the idea of raising capital at this point,” he told analysts.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Wed, 24 Apr 2019 23:19:33 +0000</pubDate>
<dc:creator>pseudolus</dc:creator>
<og:url>https://www.nytimes.com/2019/04/24/business/tesla-earnings-elon-musk.html</og:url>
<og:type>article</og:type>
<og:title>Tesla Posts Big Quarterly Loss as Its Electric-Car Sales Lag</og:title>
<og:image>https://static01.nyt.com/images/2019/04/24/business/24tesla/24tesla-facebookJumbo.jpg</og:image>
<og:description>The $702 million loss exceeded expectations, but the company reaffirmed its guidance on the year’s deliveries. It said it was open to raising capital.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2019/04/24/business/tesla-earnings-elon-musk.html</dc:identifier>
</item>
</channel>
</rss>