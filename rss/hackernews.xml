<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Justice Department to Open Broad, New Antitrust Review of Big Tech Companies</title>
<link>https://www.wsj.com/articles/justice-department-to-open-broad-new-antitrust-review-of-big-tech-companies-11563914235?mod=rsswn</link>
<guid isPermaLink="true" >https://www.wsj.com/articles/justice-department-to-open-broad-new-antitrust-review-of-big-tech-companies-11563914235?mod=rsswn</guid>
<description>&lt;div class=&quot;is-lead-inset&quot;&gt;
      &lt;div data-layout=&quot;header&amp;#10;              &quot; data-layout-mobile=&quot;&quot; class=&quot;&amp;#10;        media-object&amp;#10;        type-InsetMediaIllustration&amp;#10;          header&amp;#10;  article__inset&amp;#10;        article__inset--type-InsetMediaIllustration&amp;#10;          article__inset--lead&amp;#10;  &quot;&gt;
      
      
          
    &lt;figure class=&quot;media-object-image enlarge-image renoImageFormat- img-header article__inset__image&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;&lt;div data-mobile-ratio=&quot;66.66666666666666%&quot; data-layout-ratio=&quot;66.66666666666666%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container  responsive-media article__inset__image__image&quot;&gt;
    
        &lt;img srcset=&quot;https://images.wsj.net/im-92176?width=140&amp;amp;size=1.5 140w,&amp;#10;https://images.wsj.net/im-92176?width=540&amp;amp;size=1.5 540w,&amp;#10;https://images.wsj.net/im-92176?width=620&amp;amp;size=1.5 620w,&amp;#10;https://images.wsj.net/im-92176?width=700&amp;amp;size=1.5 700w,&amp;#10;https://images.wsj.net/im-92176?width=860&amp;amp;size=1.5 860w,&amp;#10;https://images.wsj.net/im-92176?width=1260&amp;amp;size=1.5 1260w&quot; sizes=&quot;(max-width: 140px) 100px,&amp;#10;(max-width: 540px) 500px,&amp;#10;(max-width: 620px) 580px,&amp;#10;(max-width: 700px) 660px,&amp;#10;(max-width: 860px) 820px,&amp;#10;1260px&quot; src=&quot;https://images.wsj.net/im-92176?width=620&amp;amp;size=1.5&quot; data-enlarge=&quot;https://images.wsj.net/im-92176?width=1260&amp;amp;size=1.5&quot; alt=&quot;&quot; title=&quot;Attorney General William Barr has asked how some Big Tech companies took shape ‘under the...&quot;/&gt;&lt;/div&gt;
    
      &lt;figcaption class=&quot;wsj-article-caption article__inset__image__caption&quot; itemprop=&quot;caption&quot;&gt;&lt;span class=&quot;wsj-article-caption-content&quot;&gt;Attorney General William Barr has asked how some Big Tech companies took shape ‘under the nose of the antitrust enforcers.’ &lt;/span&gt;
          &lt;span class=&quot;wsj-article-credit article__inset__image__caption__credit&quot; itemprop=&quot;creator&quot;&gt;
            &lt;span class=&quot;wsj-article-credit-tag&quot;&gt;
              Photo: 
            &lt;/span&gt;
        Alex Wong/Getty Images
          &lt;/span&gt;
  &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt; 
    &lt;/div&gt;

  &lt;div class=&quot;clearfix byline-wrap&quot;&gt;


      
      &lt;div class=&quot;byline article__byline&quot;&gt;
      
      
          &lt;span&gt;By&lt;/span&gt;
              &lt;div class=&quot;author mobile-scrim hasMenu&quot; data-scrim=&quot;{&amp;quot;type&amp;quot;:&amp;quot;author&amp;quot;,&amp;quot;header&amp;quot;:&amp;quot;Brent Kendall&amp;quot;,&amp;quot;subhead&amp;quot;:&amp;quot;The Wall Street Journal&amp;quot;,&amp;quot;list&amp;quot;:[{&amp;quot;type&amp;quot;:&amp;quot;link&amp;quot;,&amp;quot;icon&amp;quot;:&amp;quot;bio&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https://www.wsj.com/news/author/7658&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Biography&amp;quot;},{&amp;quot;type&amp;quot;:&amp;quot;link&amp;quot;,&amp;quot;icon&amp;quot;:&amp;quot;twitter&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;https://twitter.com/brkend&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;@brkend&amp;quot;},{&amp;quot;type&amp;quot;:&amp;quot;link&amp;quot;,&amp;quot;icon&amp;quot;:&amp;quot;email&amp;quot;,&amp;quot;url&amp;quot;:&amp;quot;mailto:Brent.Kendall@wsj.com&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;Brent.Kendall@wsj.com&amp;quot;}]}&quot;&gt;
                  &lt;span class=&quot;name&quot; itemprop=&quot;name&quot;&gt;Brent Kendall&lt;/span&gt;
                  &lt;/div&gt;

      &lt;/div&gt;
      
        &lt;time class=&quot;timestamp article__timestamp flexbox__flex--1&quot;&gt;
          Updated July 23, 2019 5:34 pm ET
        &lt;/time&gt;&lt;/div&gt;

  

  &lt;div class=&quot;article-content &quot;&gt;

      &lt;p&gt;WASHINGTON—The Justice Department is opening a broad antitrust review into whether dominant technology firms are unlawfully stifling competition, adding a new Washington threat for companies such as






            &lt;a href=&quot;https://quotes.wsj.com/FB&quot;&gt;Facebook&lt;/a&gt;
            &lt;span class=&quot;company-name-type&quot;&gt; Inc.,&lt;/span&gt;


      Google,






            &lt;a href=&quot;https://quotes.wsj.com/AMZN&quot;&gt;Amazon.com&lt;/a&gt;
            &lt;span class=&quot;company-name-type&quot;&gt; Inc.&lt;/span&gt;


      and






            &lt;a href=&quot;https://quotes.wsj.com/AAPL&quot;&gt;Apple&lt;/a&gt;
            &lt;span class=&quot;company-name-type&quot;&gt; Inc.&lt;/span&gt;


      &lt;/p&gt;




      &lt;div class=&quot;paywall&quot;&gt;&lt;p&gt;The review is geared toward examining the practices of online platforms that dominate internet search, social media and retail services, the department said, confirming the review shortly after The Wall Street Journal reported it.&lt;/p&gt;




      &lt;p&gt;The new antitrust inquiry under Attorney General William Barr could ratchet up the already considerable regulatory pressures facing the top U.S. tech firms. The review is designed to go above and beyond recent plans for scrutinizing the tech sector that were crafted by the department and the Federal Trade Commission.&lt;/p&gt;




      &lt;p&gt;The two agencies, which share antitrust enforcement authority, in recent months worked out which one of them would take the lead on exploring different issues involving the big four tech giants. Those&lt;a href=&quot;https://www.wsj.com/articles/ftc-to-examine-how-facebook-s-practices-affect-digital-competition-11559576731?mod=article_inline&quot; class=&quot;icon none&quot;&gt; turf agreements caused a stir in the tech industry&lt;/a&gt; and rattled investors. Now, the new Justice Department review could amplify the risk, because some of those companies could face antitrust claims from both the Justice Department and the FTC.&lt;/p&gt;




      &lt;p&gt;The &lt;a href=&quot;https://www.wsj.com/articles/ftc-aims-new-task-force-at-big-tech-11551209556?mod=article_inline&quot; class=&quot;icon none&quot;&gt;FTC in February created its own task force &lt;/a&gt;to monitor competition in the tech sector; that team’s work is ongoing.&lt;/p&gt;






       




      &lt;p&gt;The Justice Department will examine issues including how the most dominant tech firms have grown in size and might—and expanded their reach into additional businesses. &lt;/p&gt;




      &lt;p&gt;The Justice Department also is interested in how Big Tech has leveraged the powers that come with having very large networks of users, the department said.&lt;/p&gt;




      &lt;p&gt;There is no defined end-goal yet for the Big Tech review other than to understand whether there are antitrust problems that need addressing, but a range of options are on the table, the officials said. The inquiry could eventually lead to more focused investigations of specific company conduct, they said.&lt;/p&gt;




      &lt;p&gt;The review also presents risks for the companies beyond whether antitrust issues are identified. The department won’t ignore other company practices that may raise concerns about compliance with other laws, officials said.&lt;/p&gt;




      &lt;p&gt;“Without the discipline of meaningful market-based competition, digital platforms may act in ways that are not responsive to consumer demands,” Justice Department antitrust chief Makan Delrahim said in a statement. “The department’s antitrust review will explore these important issues.”&lt;/p&gt;






      &lt;div data-layout=&quot;wrap&amp;#10;              &quot; data-layout-mobile=&quot;&quot; class=&quot;&amp;#10;        media-object&amp;#10;        type-InsetMediaIllustration&amp;#10;          wrap&amp;#10;    scope-web|mobileapps&amp;#10;  article__inset&amp;#10;        article__inset--type-InsetMediaIllustration&amp;#10;            article__inset--wrap&amp;#10;  &quot;&gt;
      
      
          
    &lt;figure class=&quot;media-object-image enlarge-image renoImageFormat- img-wrap article__inset__image&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;&lt;div data-mobile-ratio=&quot;66.66666666666666%&quot; data-layout-ratio=&quot;66.66666666666666%&quot; data-subtype=&quot;photo&quot; class=&quot;image-container  responsive-media article__inset__image__image&quot;&gt;
    
        &lt;img srcset=&quot;https://images.wsj.net/im-92190?width=140&amp;amp;size=1.5 140w,&amp;#10;https://images.wsj.net/im-92190?width=540&amp;amp;size=1.5 540w,&amp;#10;https://images.wsj.net/im-92190?width=620&amp;amp;size=1.5 620w,&amp;#10;https://images.wsj.net/im-92190?width=700&amp;amp;size=1.5 700w,&amp;#10;https://images.wsj.net/im-92190?width=860&amp;amp;size=1.5 860w,&amp;#10;https://images.wsj.net/im-92190?width=1260&amp;amp;size=1.5 1260w&quot; sizes=&quot;(max-width: 140px) 100px,&amp;#10;(max-width: 540px) 500px,&amp;#10;(max-width: 620px) 580px,&amp;#10;(max-width: 700px) 660px,&amp;#10;(max-width: 860px) 820px,&amp;#10;1260px&quot; src=&quot;https://images.wsj.net/im-92190?width=620&amp;amp;size=1.5&quot; data-enlarge=&quot;https://images.wsj.net/im-92190?width=1260&amp;amp;size=1.5&quot; alt=&quot;&quot; title=&quot;The Justice Department has been getting ready to investigate whether Google engages in illegal monopolization...&quot;/&gt;&lt;/div&gt;
    
      &lt;figcaption class=&quot;wsj-article-caption article__inset__image__caption&quot; itemprop=&quot;caption&quot;&gt;&lt;span class=&quot;wsj-article-caption-content&quot;&gt;The Justice Department has been getting ready to investigate whether Google engages in illegal monopolization practices. &lt;/span&gt;
          &lt;span class=&quot;wsj-article-credit article__inset__image__caption__credit&quot; itemprop=&quot;creator&quot;&gt;
            &lt;span class=&quot;wsj-article-credit-tag&quot;&gt;
              Photo: 
            &lt;/span&gt;
        Drew Angerer/Getty Images
          &lt;/span&gt;
  &lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt; 




      &lt;p&gt;Representatives for Facebook, Alphabet, Amazon and Apple didn’t respond to requests to comment. &lt;/p&gt;




      &lt;p&gt;In after-hours trading Tuesday, shares for those four companies were down. Apple was down by about 0.4%, Amazon by about 1.13%, Alphabet by about 0.96%, and Facebook by about 1.65%. &lt;/p&gt;




      &lt;p&gt;The Big Tech companies have said they are highly innovative firms that create jobs and provide products and services that consumers love. They have said they have rightly won their places atop the tech pyramid and have to compete fiercely to stay there.&lt;/p&gt;




      &lt;p&gt;The Justice Department already has been preparing to probe whether






            &lt;a href=&quot;https://quotes.wsj.com/GOOG&quot;&gt;Alphabet&lt;/a&gt;
            &lt;span class=&quot;company-name-type&quot;&gt; Inc.&lt;/span&gt;


      ’s Google is engaging in unlawful monopolization practices. The Journal earlier reported &lt;a href=&quot;https://www.wsj.com/articles/justice-department-is-preparing-antitrust-investigation-of-google-11559348795?mod=article_inline&quot; class=&quot;icon none&quot;&gt;the department’s plans&lt;/a&gt; for that investigation, whose existence hasn’t been confirmed by the agency.&lt;/p&gt;




      




      &lt;p&gt;The department’s antitrust division will conduct both reviews; it is unknown if and when the two efforts will intersect. On the broader tech review, the division will work in close coordination with Deputy Attorney General Jeffrey Rosen,  the officials said.&lt;/p&gt;






      &lt;div data-layout=&quot;wrap&amp;#10;              &quot; data-layout-mobile=&quot;&quot; class=&quot;&amp;#10;        media-object&amp;#10;        type-InsetRichText&amp;#10;          wrap&amp;#10;    scope-web&amp;#10;  article__inset&amp;#10;        article__inset--type-InsetRichText&amp;#10;            article__inset--wrap&amp;#10;  &quot;&gt;
      
      
      
      
      
      
      
      
      
          &lt;div class=&quot;media-object-rich-text&quot;&gt;
    	

&lt;h4&gt;Share Your Thoughts&lt;/h4&gt; &lt;p&gt; &lt;em&gt;What do you think the Justice Department’s new antitrust inquiry should focus on? Join the conversation below&lt;/em&gt;.&lt;/p&gt;
    &lt;/div&gt;

      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      &lt;/div&gt; 




      &lt;p&gt;The department appears to be hitting the ground running. It recently hosted a private presentation where officials heard from critics of Facebook, including academics, who walked through their concerns about the social-media giant and advocated for its breakup, according to people familiar with that meeting. Tech and antitrust observers believed issues related to Facebook’s dominance were to be handled by the FTC.&lt;/p&gt;




      &lt;p&gt;Both the FTC and the Justice Department have made clear that they view tech-sector competition issues as a priority. Under agreements brokered in recent months between Mr. Delrahim and FTC Chairman Joseph Simons, the Justice Department obtained clearance to proceed with a probe of whether Google has engaged in illegal monopolization tactics, as well as jurisdiction over Apple for similar issues. The FTC, meanwhile, won for itself the right to explore monopolization questions involving Facebook and Amazon. (The commission already has undertaken a lengthy consumer-protection investigation of Facebook’s privacy practices, and the company has agreed to &lt;a href=&quot;https://www.wsj.com/articles/ftc-approves-roughly-5-billion-facebook-settlement-11562960538?mod=article_inline&quot; class=&quot;icon none&quot;&gt;a $5 billion fine&lt;/a&gt;.)&lt;/p&gt;




      &lt;p&gt;Justice Department officials said those agreements weren’t meant to be open-ended or all-encompassing. But in any case the department isn’t trying to pre-empt the FTC’s work, they said, and suggested the two agencies might explore different tech practices by the same company, as well as different legal theories for possible cases.&lt;/p&gt;




      &lt;p&gt;The two agencies have been in regular contact at both the leadership and staff levels to coordinate their efforts, according to a person familiar with the discussions.&lt;/p&gt;




      &lt;p&gt;While the top tech firms were once the darlings of the public, attitudes have shifted as some consumers, and politicians on both the left and the right, have grown uncomfortable with how much power and influence they wield in the economy and society. &lt;/p&gt;




      &lt;p&gt;Some Democratic presidential candidates have called for the breakup of companies like Google and Facebook, while lawmakers of both parties have sounded alarm bells, though at times for different reasons. Some Republicans have voiced concerns about whether tech companies disfavor conservative voices, claims that industry leaders have denied.&lt;/p&gt;




      &lt;p&gt;President Trump has escalated his criticisms of Big Tech recently, openly suggesting the U.S. ought to sue Google and Facebook, comments that could hang over the Justice Department’s new efforts.&lt;/p&gt;




      &lt;p&gt;Aside from Justice Department and FTC scrutiny, a House antitrust subcommittee also is taking a broad look at potential anticompetitive conduct in the tech sector. Executives from Facebook, Google, Apple and Amazon all &lt;a href=&quot;https://www.wsj.com/articles/congress-puts-big-tech-in-crosshairs-11563311754?mod=article_inline&quot; class=&quot;icon none&quot;&gt;testified before the panel&lt;/a&gt; last week.&lt;/p&gt;




      &lt;p&gt;Seeds for the new Justice Department review were planted in January at Mr. Barr’s confirmation hearing, when he said that he believed antitrust issues in the tech sector were important.&lt;/p&gt;




      &lt;p&gt;“I don’t think big is necessarily bad, but I think a lot of people wonder how such huge behemoths that now exist in Silicon Valley have taken shape under the nose of the antitrust enforcers,” Mr. Barr told senators. “You can win that place in the marketplace without violating the antitrust laws, but I want to find out more about that dynamic.”&lt;/p&gt;




      &lt;p&gt;Justice Department officials said they would use the new antitrust review to seek extensive input and information from industry participants, and eventually from the dominant tech firms themselves. It isn’t yet known whether much of the information-gathering will be done on a voluntary basis or if companies eventually could be compelled by the government to turn over materials. &lt;/p&gt;




      &lt;p class=&quot;articleTagLine&quot;&gt;—Ryan Tracy contributed to this article.&lt;/p&gt;




      &lt;p&gt; &lt;strong&gt;Write to &lt;/strong&gt;Brent Kendall at &lt;a href=&quot;mailto:brent.kendall@wsj.com&quot; target=&quot;_blank&quot; class=&quot;icon &quot;&gt;brent.kendall@wsj.com&lt;/a&gt;&lt;/p&gt;




      &lt;/div&gt;



  
  &lt;p&gt;Copyright ©2019 Dow Jones &amp;amp; Company, Inc. All Rights Reserved. 87990cbe856818d5eddac44c7b1cdeb8&lt;/p&gt;
  
  &lt;/div&gt;


    &lt;p class=&quot;printheadline&quot;&gt;
      Appeared in the July 24, 2019, print edition as 'Barr Starts New Antitrust Inquiry Into Tech Giants.'
    &lt;/p&gt;




</description>
<pubDate>Tue, 23 Jul 2019 20:43:40 +0000</pubDate>
<dc:creator>mudil</dc:creator>
<og:title>Justice Department to Open Broad, New Antitrust Review of Big Tech Companies</og:title>
<og:description>The Justice Department is opening a broad antitrust review into whether dominant technology firms are unlawfully stifling competition, according to department officials.</og:description>
<og:url>https://www.wsj.com/articles/justice-department-to-open-broad-new-antitrust-review-of-big-tech-companies-11563914235</og:url>
<og:image>https://images.wsj.net/im-92176/social</og:image>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wsj.com/articles/justice-department-to-open-broad-new-antitrust-review-of-big-tech-companies-11563914235?mod=rsswn</dc:identifier>
</item>
<item>
<title>Mitchell Feigenbaum (1944–2019), 4.66920160910299067185320382…</title>
<link>https://blog.stephenwolfram.com/2019/07/mitchell-feigenbaum-1944-2019-4-66920160910299067185320382/</link>
<guid isPermaLink="true" >https://blog.stephenwolfram.com/2019/07/mitchell-feigenbaum-1944-2019-4-66920160910299067185320382/</guid>
<description>&lt;h2 id=&quot;behind-the-feigenbaum-constant&quot;&gt;Behind the Feigenbaum Constant&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;It’s called the &lt;a href=&quot;http://mathworld.wolfram.com/FeigenbaumConstant.html&quot;&gt;Feigenbaum constant&lt;/a&gt;, and it’s about 4.6692016. And it shows up, quite universally, in certain kinds of mathematical—and physical—systems that can exhibit chaotic behavior.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;&lt;a href=&quot;https://www.wolframalpha.com/input/?i=mitchell+feigenbaum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mitchell Feigenbaum&lt;/a&gt;, who died on June 30 at the age of 74, was the person who discovered it—back in 1975, by doing &lt;a href=&quot;https://blog.stephenwolfram.com/2017/03/two-hours-of-experimental-mathematics/&quot;&gt;experimental mathematics&lt;/a&gt; on a pocket calculator.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;It became a defining discovery in the history of chaos theory. But when it was first discovered, it was a surprising, almost bizarre result, that didn’t really connect with anything that had been studied before. Somehow, though, it’s fitting that it should have been Mitchell Feigenbaum—who I knew for nearly 40 years—who would discover it.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Trained in theoretical physics, and a connoisseur of its mathematical traditions, Mitchell always seemed to see himself as an outsider. He looked a bit like Beethoven—and projected a certain stylish sense of intellectual mystery. He would often make strong assertions, usually with a conspiratorial air, a twinkle in his eye, and a glass of wine or a cigarette in his hand.&lt;span id=&quot;more-21041&quot;/&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;He would talk in long, flowing sentences which exuded a certain erudite intelligence. But ideas would jump around. Sometimes detailed and technical. Sometimes leaps of intuition that I, for one, could not follow. He was always calculating, staying up until 5 or 6 am, filling yellow pads with formulas and stressing &lt;a href=&quot;https://www.wolfram.com/mathematica/&quot;&gt;Mathematica&lt;/a&gt; with elaborate algebraic computations that might run for hours.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;He published very little, and what he did publish he was often disappointed wasn’t widely understood. When he died, he had been working for years on the optics of perception, and on questions like why the Moon appears larger when it’s close to the horizon. But he never got to the point of publishing anything on any of this.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;For more than 30 years, Mitchell’s official position (obtained essentially on the basis of his Feigenbaum constant result) was as a professor at the &lt;a href=&quot;https://www.rockefeller.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Rockefeller University&lt;/a&gt; in New York City. (To fit with Rockefeller’s biological research mission, he was themed as the Head of the “Laboratory of Mathematical Physics”.) But he dabbled elsewhere, lending his name to a financial computation startup, and becoming deeply involved in inventing new cartographic methods for the &lt;em&gt;&lt;a href=&quot;https://www.amazon.com/Hammond-New-Century-World-Atlas/dp/0843711965/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hammond World Atlas&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;what-mitchell-discovered&quot;&gt;What Mitchell Discovered&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;The basic idea is &lt;a href=&quot;https://www.wolframscience.com/nks/p149--iterated-maps-and-the-chaos-phenomenon/&quot;&gt;quite simple&lt;/a&gt;. Take a value &lt;em&gt;x&lt;/em&gt; between 0 and 1. Then iteratively replace &lt;em&gt;x&lt;/em&gt; by &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a x&lt;/em&gt; (1 – &lt;em&gt;x&lt;/em&gt;)&lt;/span&gt;. Let’s say one starts from &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; = &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math3.png&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;&lt;/span&gt;, and takes &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt; = 3.2&lt;/span&gt;. Then here’s what one gets for the successive values of &lt;em&gt;x&lt;/em&gt;:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;5&quot;&gt;&lt;tr readability=&quot;10&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;8&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/successive-values.png&quot; alt=&quot;Successive values&quot; title=&quot;Successive values&quot; width=&quot;571&quot; height=&quot;219&quot; class=&quot;alignnone size-full wp-image-21175&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;15&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;1_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;1_out_text&quot;&gt;
ListLinePlot[NestList[Compile[x, 3.2 x (1 - x)], N[1/3], 50], 
 Mesh -&amp;gt; All, PlotRange -&amp;gt; {0, 1}, Frame -&amp;gt; True]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;After a little transient, the values of &lt;em&gt;x&lt;/em&gt; are periodic, with period 2. But what happens with other values of &lt;em&gt;a&lt;/em&gt;? Here are a few results for this so-called “logistic map”:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;10&quot;&gt;&lt;tr readability=&quot;20&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;13&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/grid_orange.png&quot; alt=&quot;Logistic map&quot; title=&quot;Logistic map&quot; width=&quot;614&quot; height=&quot;300&quot; class=&quot;alignnone size-full wp-image-21166&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;25&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;2_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;2_out_text&quot;&gt;
GraphicsGrid[
 Partition[
  Table[Labeled[
    ListLinePlot[NestList[Compile[x, a x (1 - x)], N[1/3], 50], 
     Sequence[
     Mesh -&amp;gt; All, PlotRange -&amp;gt; {0, 1}, Frame -&amp;gt; True, 
      FrameTicks -&amp;gt; None]], StringTemplate[&quot;a = ``&quot;][a]], {a, 2.75, 
    4, .25}], 3], Spacings -&amp;gt; {.1, -.1}]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;For small &lt;em&gt;a&lt;/em&gt;, the values of &lt;em&gt;x&lt;/em&gt; quickly go to a fixed point. For larger &lt;em&gt;a&lt;/em&gt; they become periodic, first with period 2, then 4. And finally, for larger &lt;em&gt;a&lt;/em&gt;, the values start bouncing around seemingly randomly.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;One can summarize this by plotting the values of &lt;em&gt;x&lt;/em&gt; (here, 300, after dropping the first 50 to avoid transients) reached as a function of the value of &lt;em&gt;a&lt;/em&gt;:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;8&quot;&gt;&lt;tr readability=&quot;16&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;11&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/period-doublings.png&quot; alt=&quot;Period doublings&quot; title=&quot;Period doublings&quot; width=&quot;570&quot; height=&quot;327&quot; class=&quot;alignnone size-full wp-image-21172&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;21&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;3_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;3_out_text&quot;&gt;
ListPlot[Flatten[
  Table[{a, #} &amp;amp; /@ 
    Drop[NestList[Compile[x, a x (1 - x)], N[1/3], 300], 50], {a, 0, 
    4, .01}], 1], Frame -&amp;gt; True, FrameLabel -&amp;gt; {&quot;a&quot;, &quot;x&quot;}]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;As &lt;em&gt;a&lt;/em&gt; increases, one sees a cascade of “period doublings”. In this case, they’re at &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt; = 3&lt;/span&gt;, &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 3.449&lt;/span&gt;, &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 3.544090&lt;/span&gt;, &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 3.5644072&lt;/span&gt;. What Mitchell noticed is that these successive values approach a limit (here &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;span class=&quot;special-character Infinity&quot;&gt;∞&lt;/span&gt;&lt;/sub&gt;&lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 3.569946&lt;/span&gt;) in a geometric sequence, with &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;span class=&quot;special-character Infinity&quot;&gt;∞&lt;/span&gt;&lt;/sub&gt; – &lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt; ~ &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt;&lt;sup&gt;-&lt;em&gt;n&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span class=&quot;traditional-math&quot;&gt;&lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 4.669&lt;/span&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;That’s a nice little result. But here’s what makes it much more significant: it isn’t just true about the specific iterated map &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;em&gt;a x&lt;/em&gt; (1 – &lt;em&gt;x&lt;/em&gt;)&lt;/span&gt;; it’s true about any map like that. Here, for example, is the “bifurcation diagram” for &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;em&gt;a&lt;/em&gt; sin(&lt;span class=&quot;special-character Pi&quot;&gt;π&lt;/span&gt; &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math7.png&quot; width=&quot;22&quot; height=&quot;18&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;):&lt;/span&gt;&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;8&quot;&gt;&lt;tr readability=&quot;16&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;11&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/bifurcation-diagram.png&quot; alt=&quot;Bifucation diagram&quot; title=&quot;Bifucation diagram&quot; width=&quot;620&quot; height=&quot;329&quot; class=&quot;alignnone size-full wp-image-21161&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;21&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;4_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;4_out_text&quot;&gt;
ListPlot[Flatten[
  Table[{a, #} &amp;amp; /@ 
    Drop[NestList[Compile[x, a Sin[Pi Sqrt@x]], N[1/3], 300], 50], {a,
     0, 1, .002}], 1], Frame -&amp;gt; True, FrameLabel -&amp;gt; {&quot;a&quot;, &quot;x&quot;}]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;The details are different. But what Mitchell noticed is that the positions of the period doublings again form a geometric sequence, with the exact same base: &lt;span class=&quot;traditional-math&quot;&gt;&lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 4.669&lt;/span&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;It’s not just that different iterated maps give qualitatively similar results; when one measures the convergence rate this turns out be exactly and quantitatively the same—always &lt;span class=&quot;traditional-math&quot;&gt;&lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 4.669&lt;/span&gt;. And this was Mitchell’s big discovery: a quantitatively universal feature of the approach to chaos in a class of systems.&lt;/p&gt;
&lt;h2 id=&quot;the-scientific-backstory&quot;&gt;The Scientific Backstory&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;The basic idea behind iterated maps has a &lt;a href=&quot;https://www.wolframscience.com/nks/notes-4-7--history-of-iterated-maps/&quot;&gt;long history&lt;/a&gt;, stretching all the way back to antiquity. Early versions arose in connection with finding successive approximations, say to square roots. For example, using Newton’s method from the late 1600s, &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/sqrt-2-hr2.png&quot; width=&quot;22&quot; height=&quot;22&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt; can be obtained by iterating &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math8.png&quot; width=&quot;38&quot; height=&quot;31&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;&lt;/span&gt; (here starting from &lt;em&gt;x&lt;/em&gt; = 1):&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;3&quot;&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;6&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/img81.png&quot; alt=&quot;Starting from x = 1&quot; title=&quot;Starting from x = 1&quot; width=&quot;529&quot; height=&quot;18&quot; class=&quot;alignnone size-full wp-image-21180&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;11&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;5_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;5_out_text&quot;&gt;
NestList[Function[x, 1/x + x/2], N[1, 8], 6]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;The notion of iterating an arbitrary function seems to have first been formalized in an &lt;a href=&quot;https://link.springer.com/article/10.1007%2FBF01443992&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1870 paper&lt;/a&gt; by &lt;a href=&quot;http://www-history.mcs.st-and.ac.uk/Biographies/Schroder.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ernst Schröder&lt;/a&gt; (who was notable for his work in formalizing things from powers to &lt;a href=&quot;https://www.wolframscience.com/nks/notes-12-9--basic-logic-and-axioms/&quot;&gt;Boolean algebra&lt;/a&gt;), although most of the discussion that arose was around solving functional equations, not actually doing iterations. (An exception was the investigation of &lt;a href=&quot;https://www.wolframscience.com/nks/notes-10-12--memory-analogs-with-numerical-data/&quot;&gt;regions of convergence for Newton’s approximation&lt;/a&gt; by &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Arthur+Cayley&quot;&gt;Arthur Cayley&lt;/a&gt; in 1879.) In 1918 &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Gaston+Julia&quot;&gt;Gaston Julia&lt;/a&gt; made a fairly extensive study of iterated rational functions in the complex plane—inventing, if not drawing, Julia sets. But until fractals in the late 1970s (which soon led to the &lt;a href=&quot;https://reference.wolfram.com/language/ref/MandelbrotSetPlot.html&quot;&gt;Mandelbrot set&lt;/a&gt;), this area of mathematics basically languished.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But quite independent of any pure mathematical developments, iterated maps with forms similar to &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;em&gt;a x&lt;/em&gt; (1 – &lt;em&gt;x&lt;/em&gt;)&lt;/span&gt; started appearing in the 1930s as possible practical models in fields like population biology and business cycle theory—usually arising as discrete annualized versions of continuous equations like the &lt;a href=&quot;https://www.wolframalpha.com/input/?i=logistic+differential+equation&quot;&gt;Verhulst logistic differential equation&lt;/a&gt; from the mid-1800s. Oscillatory behavior was often seen—and in 1954 &lt;a href=&quot;https://en.wikipedia.org/wiki/Bill_Ricker&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;William Ricker&lt;/a&gt; (one of the founders of fisheries science) also found more complex behavior when he iterated some empirical fish reproduction curves.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Back in pure mathematics, versions of iterated maps had also shown up from time to time in number theory. In 1799 &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Carl+Friedrich+Gauss&quot;&gt;Carl Friedrich Gauss&lt;/a&gt; effectively &lt;a href=&quot;https://www.wolframscience.com/nks/notes-4-7--history-of-iterated-maps/&quot;&gt;studied&lt;/a&gt; the map &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; &lt;span class=&quot;special-character RightArrow&quot;&gt;→&lt;/span&gt; &lt;tt&gt;&lt;a href=&quot;http://reference.wolfram.com/language/ref/FractionalPart.html&quot;&gt;FractionalPart&lt;/a&gt;&lt;/tt&gt;[&lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math17.png&quot; height=&quot;25&quot; width=&quot;9&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;]&lt;/span&gt; in connection with continued fractions. And starting in the late 1800s there was interest in studying maps like &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;tt&gt;&lt;a href=&quot;http://reference.wolfram.com/language/ref/FractionalPart.html&quot;&gt;FractionalPart&lt;/a&gt;&lt;/tt&gt;[&lt;em&gt;a x&lt;/em&gt;]&lt;/span&gt; and their connections to the properties of the number &lt;em&gt;a&lt;/em&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Particularly following &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Henri+Poincare&quot;&gt;Henri Poincaré&lt;/a&gt;’s work on celestial mechanics around 1900, the idea of sensitive dependence on initial conditions arose, and it was eventually noted that iterated maps could effectively “&lt;a href=&quot;https://www.wolframscience.com/nks/p304--chaos-theory-and-randomness-from-initial-conditions/&quot;&gt;excavate digits&lt;/a&gt;” in their initial conditions. For example, iterating &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;tt&gt;&lt;a href=&quot;http://reference.wolfram.com/language/ref/FractionalPart.html&quot;&gt;FractionalPart&lt;/a&gt;&lt;/tt&gt;[10 &lt;em&gt;x&lt;/em&gt;]&lt;/span&gt;, starting with the digits of &lt;span class=&quot;special-character Pi&quot;&gt;π&lt;/span&gt;, gives (effectively just shifting the sequence of digits one place to the left at each step):&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;3.5&quot;&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;6.5&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/img91.png&quot; alt=&quot;Starting with the digits of pi...&quot; title=&quot;Starting with the digits of pi...&quot; width=&quot;594&quot; height=&quot;18&quot; class=&quot;alignnone size-full wp-image-21181&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;12&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;6_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;6_out_text&quot;&gt;
N[NestList[Function[x, FractionalPart[10 x]], N[Pi, 100], 5], 10]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;4.5&quot;&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;7&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/fractional-part-pi.png&quot; alt=&quot;FractionalPart&quot; title=&quot;FractionalPart&quot; width=&quot;537&quot; height=&quot;179&quot; class=&quot;alignnone size-full wp-image-21165&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;13&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;7_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;7_out_text&quot;&gt;
ListLinePlot[
 Rest@N[NestList[Function[x, FractionalPart[10 x]], N[Pi, 100], 50], 
   40], Mesh -&amp;gt; All]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;(Confusingly enough, with typical “machine precision” computer arithmetic, &lt;a href=&quot;https://www.wolframscience.com/nks/notes-4-7--problems-with-computer-experiments-in-chaos-theory/&quot;&gt;this doesn’t work correctly&lt;/a&gt;, because even though one “runs out of precision”, the IEEE Floating Point standard says to keep on delivering digits, even though they are completely wrong. &lt;a href=&quot;https://reference.wolfram.com/language/tutorial/ArbitraryPrecisionNumbers.html&quot;&gt;Arbitrary precision&lt;/a&gt; in the &lt;a href=&quot;https://www.wolfram.com/language/&quot;&gt;Wolfram Language&lt;/a&gt; gets it right.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Maps like &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ &lt;em&gt;a x&lt;/em&gt;(1 – &lt;em&gt;x&lt;/em&gt;)&lt;/span&gt; show similar kinds of “digit excavation” behavior (for example, replacing &lt;em&gt;x&lt;/em&gt; by &lt;span class=&quot;traditional-math&quot;&gt;sin[&lt;span class=&quot;special-character Pi&quot;&gt;π&lt;/span&gt; &lt;em&gt;u&lt;/em&gt;]&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;, &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ 4 &lt;em&gt;x&lt;/em&gt;(1 – &lt;em&gt;x&lt;/em&gt;)&lt;/span&gt; becomes exactly &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;u&lt;/em&gt; ⟶ &lt;tt&gt;&lt;a href=&quot;http://reference.wolfram.com/language/ref/FractionalPart.html&quot;&gt;FractionalPart&lt;/a&gt;&lt;/tt&gt;[&lt;em&gt;u&lt;/em&gt;, 2]&lt;/span&gt;—and this was already known by the 1940s, and, for example, commented on by &lt;a href=&quot;https://www.wolframalpha.com/input/?i=John+von+Neumann&quot;&gt;John von Neumann&lt;/a&gt; in connection with his 1949 iterative &lt;a href=&quot;https://www.wolframscience.com/nks/notes-7-5--random-number-generators/&quot;&gt;“middle-square” method&lt;/a&gt; for generating pseudorandom numbers by computer.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But what about doing experimental math on iterated maps? There wasn’t too much experimental math at all on early digital computers (after all, most computer time was expensive). But in the aftermath of the Manhattan Project, Los Alamos had &lt;a href=&quot;https://library.lanl.gov/cgi-bin/getfile?00326886.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;built its own computer (named MANIAC)&lt;/a&gt;, that ended up being used for a whole series of experimental math studies. And in 1964 Paul Stein and &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Stan+Ulam&quot;&gt;Stan Ulam&lt;/a&gt; wrote a report entitled “&lt;a href=&quot;https://eudml.org/doc/268371&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Non-linear Transformation Studies on Electronic Computers&lt;/a&gt;” that included photographs of oscilloscope-like MANIAC screens displaying output from some fairly elaborate iterated maps. In 1971, another “just out of curiosity” report from Los Alamos (this time by &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Nick+Metropolis&quot;&gt;Nick Metropolis&lt;/a&gt; [leader of the MANIAC project, and developer of the &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Monte+Carlo+method&quot;&gt;Monte Carlo method&lt;/a&gt;], Paul Stein and his brother Myron Stein) started to give more specific computer results for the behavior logistic maps, and noted the basic phenomenon of period doubling (which they called the “U-sequence”), as well as its qualitative robustness under changes in the underlying map.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But quite separately from all of this, there were other developments in physics and mathematics. In 1964 &lt;a href=&quot;http://lorenz.mit.edu/ed-lorenz/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ed Lorenz&lt;/a&gt; (a meteorologist at MIT) introduced and simulated his “naturally occurring” &lt;a href=&quot;https://demonstrations.wolfram.com/LorenzAttractor/&quot;&gt;Lorenz differential equations&lt;/a&gt;, that showed sensitive dependence on initial conditions. Starting in the 1940s (but following on from Poincaré’s work around 1900) there’d been a steady stream of developments in mathematics in so-called dynamical systems theory—particularly investigating global properties of the solutions to differential equations. Usually there’d be simple fixed points observed; sometimes “limit cycles”. But by the 1970s, particularly after the arrival of early computer simulations (like Lorenz’s), it was clear that for nonlinear equations something else could happen: a so-called “strange attractor”. And in studying so-called “return maps” for strange attractors, iterated maps like the logistic map again appeared.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But it was in 1975 that various threads of development around iterated maps somehow converged. On the mathematical side, dynamical systems theorist &lt;a href=&quot;https://en.wikipedia.org/wiki/James_A._Yorke&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jim Yorke&lt;/a&gt; and his student &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Tien-Yien+Li&quot;&gt;Tien-Yien Li&lt;/a&gt; at the University of Maryland published their paper “&lt;a href=&quot;https://pdfs.semanticscholar.org/a08f/3e13af255b5173a5cb70ba37768db9671366.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Period Three Implies Chaos&lt;/a&gt;”, showing that in an iterated map with a particular parameter value, if there’s ever an initial condition that leads to a cycle of length 3, there must be other initial conditions that don’t lead to cycles at all—or, as they put it, show chaos. (As it turned out, &lt;a href=&quot;https://www.wolframscience.com/nks/notes-6-6--sarkovskiis-theorem/&quot;&gt;Aleksandr Sarkovskii&lt;/a&gt;—who was part of a Ukrainian school of dynamical systems research—had already in 1962 proved the &lt;a href=&quot;https://www.wolframscience.com/nks/notes-6-6--sarkovskiis-theorem/&quot;&gt;slightly weaker result&lt;/a&gt; that a cycle of period 3 implies cycles of all periods.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But meanwhile there had also been growing interest in things like the logistic maps among mathematically oriented population biologists, leading to the rather readable review (published in mid-1976) entitled “&lt;a href=&quot;https://pdfs.semanticscholar.org/752e/0468e5e2e6a1c175b401eddd74bbbdcb9d5c.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Simple Mathematical Models with Very Complicated Dynamics&lt;/a&gt;” by physics-trained Australian &lt;a href=&quot;https://en.wikipedia.org/wiki/Robert_May,_Baron_May_of_Oxford&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Robert May&lt;/a&gt;, who was then a biology professor at Princeton (and would subsequently become science advisor to the UK government, and is now “Baron May of Oxford”).&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But even though things like sketches of bifurcation diagrams existed, the discovery of their quantitatively universal properties had to await Mitchell Feigenbaum and his discovery.&lt;/p&gt;
&lt;h2 id=&quot;mitchells-journey&quot;&gt;Mitchell’s Journey&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell Feigenbaum grew up in Brooklyn, New York. His father was an analytical chemist, and his mother was a public-school teacher. Mitchell was unenthusiastic about school, though did well on math and science tests, and managed to teach himself calculus and piano. In 1960, at age 16, as something of a prodigy, he enrolled in the &lt;a href=&quot;https://www.ccny.cuny.edu/&quot; target=&quot;_blank&quot;&gt;City College of New York&lt;/a&gt;, officially studying electrical engineering, but also taking physics and math classes. After graduating in 1964, he went to &lt;a href=&quot;https://mit.edu&quot; target=&quot;_blank&quot;&gt;MIT&lt;/a&gt;. Initially he was going to do a PhD in electrical engineering, but he quickly switched to physics.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But although he was enamored of classic mathematical physics (as represented, for example, in the books of &lt;a href=&quot;https://en.wikipedia.org/wiki/Course_of_Theoretical_Physics&quot; target=&quot;_blank&quot;&gt;Landau and Lifshiftz&lt;/a&gt;), he ended up writing his thesis on a topic set by his advisor about particle physics, and specifically about evaluating a class of Feynman diagrams for the scattering of photons by scalar particles (with lots of integrals, if not special functions). It wasn’t a terribly exciting thesis, but in 1970 he was duly dispatched to Cornell for a postdoc position.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell struggled with motivation, preferring to hang out in coffee shops doing the &lt;em&gt;New York Times&lt;/em&gt; crossword (at which he was apparently very fast) to doing physics. But at Cornell, Mitchell made several friends who were to be important to him. One was &lt;a href=&quot;https://en.wikipedia.org/wiki/Predrag_Cvitanovi%C4%87&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Predrag Cvitanović&lt;/a&gt;, a star graduate student from what is now Croatia, who was studying &lt;a href=&quot;https://www.wolframscience.com/nks/notes-9-16--feynman-diagrams/&quot;&gt;quantum electrodynamics&lt;/a&gt;, and with whom he shared an interest in German literature. Another was a young poet named Kathleen Doorish (later, Kathy Hammond), who was a friend of Predrag’s. And another was a rising-star physics professor named &lt;a href=&quot;https://en.wikipedia.org/wiki/Peter_A._Carruthers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Pete Carruthers&lt;/a&gt;, with whom he shared an interest in classical music.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;In the early 1970s quantum field theory was entering a golden age. But despite the topic of his thesis, Mitchell didn’t get involved, and in the end, during his two years at Cornell, he produced no visible output at all. Still, he had managed to impress &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Hans+Bethe&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hans Bethe&lt;/a&gt; enough to be dispatched for another postdoc position, though now at a place lower in the pecking order of physics, &lt;a href=&quot;https://vt.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Virginia Polytechnic Institute&lt;/a&gt;, in &lt;a href=&quot;https://www.wolframalpha.com/input/?i=blacksburg,+va&quot;&gt;rural Virginia&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;At Virginia Tech, Mitchell did even less well than at Cornell. He didn’t interact much with people, and he produced only one three-page paper: “&lt;a href=&quot;https://vtechworks.lib.vt.edu/bitstream/handle/10919/47077/1.522952.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Relationship between the Normalization Coefficient and Dispersion Function for the Multigroup Transport Equation&lt;/a&gt;”. As its title might suggest, the paper was quite technical and quite unexciting.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;As Mitchell’s two years at Virginia Tech drew to a close it wasn’t clear what was going to happen. But luck intervened. Mitchell’s friend from Cornell, Pete Carruthers, had just been hired to build up the &lt;a href=&quot;https://www.lanl.gov/org/ddste/aldsc/theoretical/index.php&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;theory division&lt;/a&gt; (“T Division”) at Los Alamos, and given &lt;em&gt;carte blanche&lt;/em&gt; to hire several bright young physicists. Pete would later tell me with pride (as part of his advice to me about general scientific management) that he had a gut feeling that Mitchell could do something great, and that despite other people’s input—and the evidence—he decided to bet on Mitchell.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Having brought Mitchell to Los Alamos, Pete set about suggesting projects for him. At first, it was following up on some of Pete’s own work, and trying to compute bulk collective (“transport”) properties of quantum field theories as a way to understand high-energy particle collisions—a kind of foreshadowing of investigations of quark-gluon plasma.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But soon Pete suggested that Mitchell try looking at fluid turbulence, and in particular on seeing whether renormalization group methods might help in understanding it.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Whenever a fluid—like water—flows sufficiently rapidly it forms lots of little eddies and behaves in a complex and seemingly random way. But even though this qualitative phenomenon had been discussed for centuries (with, for example, &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Leonardo+da+Vinci&quot;&gt;Leonardo da Vinci&lt;/a&gt; making nice pictures of it), physics had had remarkably little to say about it—though in the 1940s Andrei Kolmogorov had given a simple argument that the eddies should form a cascade with a &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;k&lt;/em&gt;&lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math13.png&quot; height=&quot;30&quot; width=&quot;15&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;&lt;/span&gt; distribution of energies. At Los Alamos, though, with its focus on nuclear weapons development (inevitably involving violent fluid phenomena), turbulence was a very important thing to understand—even if it wasn’t obvious how to approach it.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But in 1974, there was news that &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Kenneth+G.+Wilson&quot;&gt;Ken Wilson&lt;/a&gt; from Cornell had just “solved the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kondo_effect&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kondo problem&lt;/a&gt;” using a technique called the &lt;a href=&quot;https://www.wolframscience.com/nks/notes-6-6--renormalization-group/&quot;&gt;renormalization group&lt;/a&gt;. And Pete Carruthers suggested that Mitchell should try to apply this technique to turbulence.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;The renormalization group is about seeing how changes of scale (or other parameters) affect descriptions (and behavior) of systems. And as it happened, it was Mitchell’s thesis advisor at MIT, &lt;a href=&quot;https://en.wikipedia.org/wiki/Francis_E._Low&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Francis Low&lt;/a&gt;, who, along with &lt;a href=&quot;https://blog.stephenwolfram.com/2019/05/remembering-murray-gell-mann-1929-2019-inventor-of-quarks/&quot;&gt;Murray Gell-Mann&lt;/a&gt;, had introduced it back in 1954 in the context of quantum electrodynamics. The idea had lain dormant for many years, but in the early 1970s it came back to life with dramatic—though quite different—applications in both particle physics (specifically, QCD) and condensed matter physics.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;In a piece of iron at room temperature, you can basically get all electron spins associated with each atom lined up, so the iron is magnetized. But if you heat the iron up, there start to be fluctuations, and suddenly—above the so-called Curie temperature (&lt;a href=&quot;https://www.wolframalpha.com/input/?i=iron+curie+temperature&quot;&gt;770°C&lt;/a&gt; for iron)—there’s effectively so much randomness that the magnetization disappears. And in fact there are lots of situations (think, for example, melting or boiling—or, for that matter, the formation of traffic jams) where this kind of sudden so-called &lt;a href=&quot;https://www.wolframscience.com/nks/notes-7-7--phase-transitions/&quot;&gt;phase transition&lt;/a&gt; occurs.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But what is actually going on in a phase transition? I think the clearest way to see this is by looking at an &lt;a href=&quot;https://www.wolframscience.com/nks/p339--origins-of-discreteness/&quot;&gt;analog in cellular automata&lt;/a&gt;. With the particular rule shown below, if there aren’t very many initial black cells, the whole system will soon be white. But if you increase the number of initial black cells (as a kind of analog of increasing the temperature in a magnetic system), then suddenly, in this case at 50% black, there’s a sharp transition, and now the whole system eventually becomes black. (For phase transition experts: yes, this is a &lt;a href=&quot;https://www.wolframscience.com/nks/notes-7-7--general-features-of-phase-transitions/&quot;&gt;phase transition in a 1D system&lt;/a&gt;; one only needs 2D if the system is required to be microscopically reversible.)&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;10.5&quot;&gt;&lt;tr readability=&quot;21&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;13.5&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/phase-transition.png&quot; alt=&quot;&quot; title=&quot;Phase transition&quot; width=&quot;600&quot; height=&quot;140&quot; class=&quot;aligncenter size-full wp-image-21173&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;26&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;8_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;8_out_text&quot;&gt;
GraphicsRow[SeedRandom[234316];
 Table[ArrayPlot[
   CellularAutomaton[&amp;lt;|
     &quot;RuleNumber&quot; -&amp;gt; 294869764523995749814890097794812493824, 
     &quot;Colors&quot; -&amp;gt; 4|&amp;gt;, 
    3 Boole[Thread[RandomReal[{0, 1}, 2000] &amp;lt; rho]], {500, {-300, 
      300}}], FrameLabel -&amp;gt; {None, 
Row[{
Round[100 rho], &quot;% black&quot;}]}], {rho, {0.4, 0.45, 0.55, 0.6}}], -30]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;But what does the system do near 50% black? In effect, it can’t decide whether to finally become black or white. And so it ends up showing a whole hierarchy of “fluctuations” from the smallest scales to the largest. And what became clear by the 1960s is that the “critical exponents” characterizing the power laws describing these fluctuations are universal across many different systems.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But how can one compute these critical exponents? In a few toy cases, analytical methods were known. But mostly, something else was needed. And in the late 1960s &lt;a href=&quot;https://en.wikipedia.org/wiki/Kenneth_G._Wilson&quot; target=&quot;_blank&quot;&gt;Ken Wilson&lt;/a&gt; realized that one could use the renormalization group, and computers. One might have a model for how individual spins interact. But the renormalization group gives a procedure for “scaling up” to the interactions of larger and larger blocks of spins. And by studying that on a computer, Ken Wilson was able to start computing critical exponents.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;At first, the physics world didn’t pay much attention, not least because they &lt;a href=&quot;https://www.stephenwolfram.com/publications/academic/computing-new-tool-fundamental-physics.pdf&quot;&gt;weren’t used to computers being so intimately in the loop&lt;/a&gt; in theoretical physics. But then there was the Kondo problem (and, yes, so far as I know, it has no relation to &lt;a href=&quot;https://en.wikipedia.org/wiki/Marie_Kondo&quot; target=&quot;_blank&quot;&gt;modern Kondoing&lt;/a&gt;—though it does relate to modern quantum dot cellular automata). In most materials, electrical resistivity decreases as the temperature decreases (going to zero for superconductors even above absolute zero). But back in the 1930s, measurements on gold had shown instead an increase of resistivity at low temperatures. By the 1960s, it was believed that this was due to the scattering of electrons from magnetic impurities—but calculations ran into trouble, generating infinite results.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But then, in 1975, Ken Wilson applied his renormalization group methods—and correctly managed to compute the effect. There was still a certain mystery about the whole thing (and it probably didn’t help that—at least when I knew him in the 1980s and beyond—I often found Ken Wilson’s explanations quite hard to understand). But the idea that the renormalization group could be important was established.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;So how might it apply to fluid turbulence? Kolmogorov’s power law seemed suggestive. But could one take the &lt;a href=&quot;https://www.wolframscience.com/nks/notes-8-4--navier-stokes-equations/&quot;&gt;Navier–Stokes equations&lt;/a&gt; which govern idealized fluid flow and actually derive something like this? This was the project on which Mitchell Feigenbaum embarked.&lt;/p&gt;
&lt;h2 id=&quot;the-big-discovery&quot;&gt;The Big Discovery&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;The &lt;a href=&quot;https://www.wolframscience.com/nks/notes-8-4--navier-stokes-equations/&quot;&gt;Navier–Stokes equations&lt;/a&gt; are very hard to work with. In fact, to this day it’s still not clear how even the most obvious feature of turbulence—its apparent randomness—arises from these equations. (It could be that the equations aren’t a full or consistent mathematical description, and one’s actually seeing amplified microscopic molecular motions. It could be that—as in chaos theory and the Lorenz equations—it’s due to amplification of randomness in the initial conditions. But my own belief, based on &lt;a href=&quot;https://www.wolframscience.com/nks/p376--fluid-flow/&quot;&gt;work I did&lt;/a&gt; in the 1980s, is that it’s actually an &lt;a href=&quot;https://www.wolframscience.com/nks/p315--the-intrinsic-generation-of-randomness/&quot;&gt;intrinsic computational phenomenon&lt;/a&gt;—analogous to the randomness one sees in my &lt;a href=&quot;https://www.wolframscience.com/nks/p27--how-do-simple-programs-behave/&quot;&gt;rule 30 cellular automaton&lt;/a&gt;.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;So how did Mitchell approach the problem? He tried simplifying it—first by going from equations depending on both space and time to ones depending only on time, and then by effectively making time discrete, and looking at iterated maps. Through Paul Stein, Mitchell knew about the (not widely known) previous work at Los Alamos on iterated maps. But Mitchell didn’t quite know where to go with it, though having just got a swank new HP-65 programmable calculator, he decided to program iterated maps on it.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Then in July 1975, Mitchell went (as I also did a few times in the early 1980s) to the summer &lt;a href=&quot;https://en.wikipedia.org/wiki/Aspen_Center_for_Physics&quot; target=&quot;_blank&quot;&gt;physics hang-out-together event in Aspen, CO&lt;/a&gt;. There he ran into &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Steve+Smale&quot;&gt;Steve Smale&lt;/a&gt;—a well-known mathematician who’d been studying dynamical systems—and was surprised to find Smale talking about iterated maps. Smale mentioned that someone had asked him if the limit of the period-doubling cascade &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;span class=&quot;special-character Infinity&quot;&gt;∞&lt;/span&gt;&lt;/sub&gt;&lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 3.56995&lt;/span&gt; could be expressed in terms of standard constants like &lt;span class=&quot;special-character Pi&quot;&gt;π&lt;/span&gt; and &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/sqrt-2-hr2.png&quot; width=&quot;22&quot; height=&quot;22&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;. Smale related that he’d said he didn’t know. But Mitchell’s interest was piqued, and he set about trying to figure it out.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;He didn’t have his HP-65 with him, but he dove into the problem using the standard tools of a well-educated mathematical physicist, and had soon turned it into something about poles of functions in the complex plane—about which he couldn’t really say anything. Back at Los Alamos in August, though, he had his HP-65, and he set about programming it to find the bifurcation points &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;The iterative procedure ran pretty fast for small &lt;em&gt;n&lt;/em&gt;. But by &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;n&lt;/em&gt; = 5&lt;/span&gt; it was taking 30 seconds. And for &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;n&lt;/em&gt; = 6&lt;/span&gt; it took minutes. While it was computing, however, Mitchell decided to look at the &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; values he had so far—and noticed something: they seemed to be converging geometrically to a final value.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;At first, he just used this fact to estimate &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;span class=&quot;special-character Infinity&quot;&gt;∞&lt;/span&gt;&lt;/sub&gt;&lt;/span&gt;, which he tried—unsuccessfully—to express in terms of standard constants. But soon he began to think that actually the convergence exponent &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; was more significant than &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;span class=&quot;special-character Infinity&quot;&gt;∞&lt;/span&gt;&lt;/sub&gt;&lt;/span&gt;—since its value stayed the same under simple changes of variables in the map. For perhaps a month Mitchell tried to express &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; in terms of standard constants.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But then, in early October 1975, he remembered that Paul Stein had said period doubling seemed to look the same not just for logistic maps but for any iterated map with a single hump. Reunited with his HP-65 after a trip to Caltech, Mitchell immediately tried the map &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;x&lt;/em&gt; ⟶ sin(&lt;em&gt;x&lt;/em&gt;)&lt;/span&gt;—and discovered that, at least to 3-digit precision, the exponent &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; was exactly the same.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;He was immediately convinced that he’d discovered something great. But Stein told him he needed more digits to really conclude much. Los Alamos had plenty of powerful computers—so the next day Mitchell got someone to show him how to write a program in FORTRAN on one of them to go further—and by the end of the day he had managed to compute that in both cases &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; was about 4.6692.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;The computer he used was a typical workhorse US scientific computer of the day: a &lt;a href=&quot;https://en.wikipedia.org/wiki/CDC_6000_series&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CDC 6000 series&lt;/a&gt; machine (of the same type I used when I first moved to the US in 1978). It had been designed by &lt;a href=&quot;https://www.wolframalpha.com/input/?i=seymour+cray&quot;&gt;Seymour Cray&lt;/a&gt;, and by default it used 60-bit floating-point numbers. But at this precision (about 14 decimal digits), 4.6692 was as far as Mitchell could compute. Fortunately, however, Pete’s wife Lucy Carruthers was a programmer at Los Alamos, and she showed Mitchell how to use double precision—with the result that he was able to compute &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; to 11-digit precision, and determine that the values for his two different iterated maps agreed.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Within a few weeks, Mitchell had found that &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; seemed to be universal whenever the iterated map had a single quadratic maximum. But he didn’t know why this was, or have any particular framework for thinking about it. But still, finally, at the age of 30, Mitchell had discovered something that he thought was really interesting.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;On Mitchell’s birthday, December 19, he saw his friend Predrag, and told him about his result. But at the time, Predrag was working hard on mainstream particle physics, and didn’t pay too much attention.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell continued working, and within a few months he was convinced that not only was the exponent &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; universal—the appropriately scaled, limiting, infinitely wiggly, actual iteration of the map was too. In April 1976 Mitchell wrote a report announcing his results. Then on May 2, 1976, he gave a talk about them at the &lt;a href=&quot;https://www.ias.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Institute for Advanced Study&lt;/a&gt; in Princeton. Predrag was there, and now he got interested in what Mitchell was doing.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;As so often, however, it was hard to understand just what Mitchell was talking about. But by the next day, Predrag had successfully simplified things, and come up with a single, explicit, functional equation for the limiting form of the scaled iterated map: &lt;span class=&quot;traditional-math&quot;&gt;&lt;em&gt;g&lt;/em&gt;(&lt;em&gt;g&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;)) = &lt;span class=&quot;InlineFormula&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/trad-math16.png&quot; width=&quot;40&quot; height=&quot;26&quot; align=&quot;absmiddle&quot;/&gt;&lt;/span&gt;&lt;/span&gt;, with &lt;span class=&quot;traditional-math&quot;&gt;&lt;span class=&quot;special-character Alpha&quot;&gt;α&lt;/span&gt; &lt;span class=&quot;special-character TildeEqual&quot;&gt;≃&lt;/span&gt; 2.50290&lt;/span&gt;—implying that for any iterated map of the appropriate type, the limiting form would always look like an even wigglier version of:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;9&quot;&gt;&lt;tr readability=&quot;18&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;12&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/feigenbaum-function.png&quot; alt=&quot;FeigenbaumFunction plot&quot; title=&quot;FeigenbaumFunction plot&quot; width=&quot;450&quot; height=&quot;263&quot; class=&quot;alignnone size-full wp-image-21320&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;23&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;9_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;9_out_text&quot;&gt;
fUD[z_] = 
  1. - 1.5276329970363323 z^2 + 0.1048151947874277 z^4 + 
   0.026705670524930787 z^6 - 0.003527409660464297 z^8 + 
   0.00008160096594827505 z^10 + 0.000025285084886512315 z^12 - 
   2.5563177536625283*^-6 z^14 - 9.65122702290271*^-8 z^16 + 
   2.8193175723520713*^-8 z^18 - 2.771441260107602*^-10 z^20 - 
   3.0292086423142963*^-10 z^22 + 2.6739057855563045*^-11 z^24 + 
   9.838888060875235*^-13 z^26 - 3.5838769501333333*^-13 z^28 + 
   2.063994985307743*^-14 z^30;
   fCF = Compile[{z}, 
    Module[{\[Alpha] = -2.5029078750959130867, n, \[Zeta]},
     n = If[Abs[z] &amp;lt;= 1., 0, Ceiling[Log[-\[Alpha], Abs[z]]]];
     \[Zeta] = z/\[Alpha]^n;
     Do[\[Zeta] = #, {2^n}];
     \[Alpha]^n \[Zeta]]] &amp;amp;[fUD[\[Zeta]]];
     Plot[fCF[x], {x, -100, 100}, MaxRecursion -&amp;gt; 5, PlotRange -&amp;gt; All]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;how-it-developed&quot;&gt;How It Developed&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;The whole area of iterated maps got a boost on June 10, 1976, with the publication in &lt;em&gt;Nature&lt;/em&gt; of Robert May’s survey about them, written independent of Mitchell and (of course) not mentioning his results. But in the months that followed, Mitchell traveled around and gave talks about his results. The reactions were mixed. Physicists wondered how the results related to physics. Mathematicians wondered about their status, given that they came from experimental mathematics, without any formal mathematical proof. And—as always—people found Mitchell’s explanations hard to understand.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;In the fall of 1976, Predrag went as a postdoc to Oxford—and on the very first day that I showed up as 17-year-old &lt;a href=&quot;https://www.stephenwolfram.com/publications/academic/?cat=all&quot;&gt;particle-physics-paper-writing&lt;/a&gt; undergraduate, I ran into him. We talked mostly about his elegant “bird tracks” method for doing group theory (about which he &lt;a href=&quot;http://www.birdtracks.eu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;finally published a book&lt;/a&gt; 32 years later). But he also tried to explain iterated maps. And I still remember him talking about an idealized model for fish populations in the Adriatic Sea (only years later did I make the connection that Predrag was from what is now Croatia).&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;At the time I didn’t pay much attention, but somehow the idea of iterated maps lodged in my consciousness, soon mixed together with the notion of fractals that I learned from &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Benoit+Mandelbrot&quot;&gt;Benoit Mandelbrot&lt;/a&gt;’s &lt;a href=&quot;https://www.stephenwolfram.com/publications/the-father-of-fractals/&quot;&gt;book&lt;/a&gt;. And when I began to concentrate on issues of complexity a couple of years later, these ideas helped guide me towards systems like cellular automata.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But back in 1976, Mitchell (who I wouldn’t meet for several more years) was off giving lots of talks about his results. He also submitted a paper to the prestigious academic journal &lt;em&gt;Advances in Mathematics&lt;/em&gt;. For 6 months he heard nothing. But eventually the paper was rejected. He tried again with another paper, now sending it to the &lt;em&gt;SIAM Journal of Applied Mathematics&lt;/em&gt;. Same result.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I have to say I’m not surprised this happened. In my own experience of academic publishing (now long in the past), if one was reporting progress within an established area it wasn’t too hard to get a paper published. But anything genuinely new or original one could pretty much count on getting rejected by the peer review process, either through intellectual shortsightedness or through academic corruption. And for Mitchell there was the additional problem that his explanations weren’t easy to understand.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But finally, in late 1977, &lt;a href=&quot;https://en.wikipedia.org/wiki/Joel_Lebowitz&quot; target=&quot;_blank&quot;&gt;Joel Lebowitz&lt;/a&gt;, editor of the &lt;em&gt;Journal of Statistical Physics&lt;/em&gt;, agreed to publish Mitchell’s paper—essentially on the basis of knowing Mitchell, even though he admitted he didn’t really understand the paper. And so it was that early in 1978 “&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.418.9339&amp;amp;rep=rep1&amp;amp;type=pdf&quot; target=&quot;_blank&quot;&gt;Quantitative Universality for a Class of Nonlinear Transformations&lt;/a&gt;”—reporting Mitchell’s big result—officially appeared. (For purposes of academic priority, Mitchell would sometimes quote a summary of a talk he gave on August 26, 1976, that was &lt;a href=&quot;http://chaosbook.org/extras/mjf/LA-6816-PR.pdf&quot; target=&quot;_blank&quot;&gt;published in the Los Alamos Theoretical Division Annual Report 1975–1976&lt;/a&gt;. Mitchell was quite affected by the rejection of his papers, and for years kept the rejection letters in his desk drawer.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell continued to travel the world talking about his results. There was interest, but also confusion. But in the summer of 1979, something exciting happened: &lt;a href=&quot;https://en.wikipedia.org/wiki/Albert_J._Libchaber&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Albert Libchaber&lt;/a&gt; in Paris reported results on a physical experiment on the transition to turbulence in convection in liquid helium—where he saw period doubling, with exactly the exponent &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; that Mitchell had calculated. Mitchell’s &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt; apparently wasn’t just universal to a class of mathematical systems—it also showed up in real, physical systems.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Pretty much immediately, Mitchell was famous. Connections to the renormalization group had been made, and his work was becoming fashionable among both physicists and mathematicians. Mitchell himself was still traveling around, but now he was regularly hobnobbing with the top physicists and mathematicians.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I remember him coming to Caltech, perhaps in the fall of 1979. There was a certain rock-star character to the whole thing. Mitchell showed up, gave a stylish but somewhat mysterious talk, and was then whisked away to talk privately with &lt;a href=&quot;https://www.stephenwolfram.com/publications/short-talk-about-richard-feynman/&quot;&gt;Richard Feynman&lt;/a&gt; and &lt;a href=&quot;https://blog.stephenwolfram.com/2019/05/remembering-murray-gell-mann-1929-2019-inventor-of-quarks/&quot;&gt;Murray Gell-Mann&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Soon Mitchell was being offered all sorts of high-level jobs, and in 1982 he triumphantly returned to Cornell as a full professor of physics. There was an air of Nobel Prize–worthiness, and by June 1984 he was appearing in the &lt;em&gt;New York Times&lt;/em&gt; magazine, in full Beethoven mode, in front of a Cornell waterfall:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/JamesGleick/status/1146413458667245568&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/nyt-mag.jpg&quot; alt=&quot;Mitchell in New York Times Magazine&quot; title=&quot;Mitchell in New York Times Magazine&quot; width=&quot;350&quot; height=&quot;365&quot; class=&quot;aligncenter size-full wp-image-21171&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Still, the mathematicians weren’t satisfied. As with &lt;a href=&quot;https://www.stephenwolfram.com/publications/the-father-of-fractals/&quot;&gt;Benoit Mandelbrot’s work&lt;/a&gt;, they tended to see Mitchell’s results as mere “numerical conjectures”, not proven and not always even quite worth citing. But top mathematicians (who Mitchell had befriended) were soon working on the problem, and results began to appear—though it took a decade for there to be a full, final proof of the universality of &lt;span class=&quot;special-character Delta&quot;&gt;δ&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;where-the-science-went&quot;&gt;Where the Science Went&lt;/h2&gt;
&lt;p class=&quot;Text&quot;&gt;So what happened to Mitchell’s big discovery? It was famous, for sure. And, yes, period-doubling cascades with his universal features were seen in a whole sequence of systems—in fluids, optics and more. But how general was it, really? And could it, for example, be extended to the full problem of fluid turbulence?&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell and others studied systems other than iterated maps, and found some related phenomena. But none were quite as striking as Mitchell’s original discovery.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;In a sense, &lt;a href=&quot;https://www.wolframscience.com/nks/&quot;&gt;my own efforts&lt;/a&gt; on cellular automata and the behavior of simple programs, &lt;a href=&quot;https://www.wolframscience.com/nks/p17--the-personal-story-of-the-science-in-this-book/&quot;&gt;beginning around 1981&lt;/a&gt;, have tried to address some of the same bigger questions as Mitchell’s work might have led to. But the methods and results have been very different. Mitchell always tried to stay close to the kinds of things that traditional mathematical physics can address, while I unabashedly struck out into the computational universe, investigating the phenomena that occur there.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I tried to see how Mitchell’s work might relate to mine—and even in my very first paper on cellular automata in 1981 I noted for example that the average density of black cells on successive steps of a cellular automaton’s evolution &lt;a href=&quot;https://www.wolframscience.com/nks/notes-6-5--probabilistic-estimates-of-cellular-automaton-properties/&quot;&gt;can be approximated (in “mean field theory”)&lt;/a&gt; by an iterated map.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I also noted that mathematically the whole evolution of a cellular automaton can be viewed as an iterated map—&lt;a href=&quot;https://www.wolframscience.com/nks/notes-2-1--mathematical-interpretation-of-cellular-automata/&quot;&gt;though on the Cantor set&lt;/a&gt;, rather than on ordinary real numbers. In &lt;a href=&quot;https://www.stephenwolfram.com/publications/academic/statistical-mechanics-cellular-automata.pdf&quot;&gt;my first paper&lt;/a&gt;, I even plotted the analog of Mitchell’s smooth mappings, but now they were &lt;a href=&quot;https://www.wolframscience.com/nks/notes-2-1--mathematical-interpretation-of-cellular-automata/&quot;&gt;wild and discontinuous&lt;/a&gt;:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;9&quot;&gt;&lt;tr readability=&quot;18&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;12&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/rules-plot.png&quot; alt=&quot;Rules plot&quot; title=&quot;Rules plot&quot; width=&quot;620&quot; height=&quot;180&quot; class=&quot;aligncenter size-full wp-image-21174&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;23&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;10_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;10_out_text&quot;&gt;
GraphicsRow[
 Labeled[ListPlot[
     Table[FromDigits[CellularAutomaton[#, IntegerDigits[n, 2, 12]], 
       2], {n, 0, 2^12 - 1}], Sequence[
     AspectRatio -&amp;gt; 1, Frame -&amp;gt; True, FrameTicks -&amp;gt; None]], 
    Text[StringTemplate[&quot;rule ``&quot;][#]]] &amp;amp; /@ {22, 42, 90, 110}]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;Text&quot;&gt;But try as I might, I could never find any strong connection with Mitchell’s work. I looked for analogs of things like period doubling, and Sarkovskii’s theorem, but &lt;a href=&quot;https://www.wolframscience.com/nks/p266--special-initial-conditions/&quot;&gt;didn’t find much&lt;/a&gt;. In my computational framework, even thinking about real numbers, with their infinite sequence of digits, &lt;a href=&quot;https://www.wolframscience.com/nks/notes-12-4--continuous-computation/&quot;&gt;was a bit unnatural&lt;/a&gt;. Years later, in &lt;em&gt;&lt;a href=&quot;https://www.wolframscience.com/nks/&quot;&gt;A New Kind of Science&lt;/a&gt;&lt;/em&gt;, I had a note entitled “&lt;a href=&quot;https://www.wolframscience.com/nks/notes-4-7--smooth-iterated-maps/&quot;&gt;Smooth iterated maps&lt;/a&gt;”. I showed their digit sequences, and observed, rather undramatically, that Mitchell’s discovery implied an unusual nested structure at the beginning of the sequences:&lt;/p&gt;
&lt;table class=&quot;InCell&quot;&gt;&lt;tbody readability=&quot;10.5&quot;&gt;&lt;tr readability=&quot;21&quot;&gt;&lt;td colspan=&quot;1&quot; rowspan=&quot;1&quot; class=&quot;Input&quot; readability=&quot;13.5&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/nested.png&quot; alt=&quot;Nested&quot; title=&quot;Nested&quot; width=&quot;620&quot; height=&quot;250&quot; class=&quot;aligncenter size-full wp-image-21170&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;IFL&quot; readability=&quot;26&quot;&gt;&lt;span class=&quot;close&quot; id=&quot;11_out&quot;&gt;&amp;amp;#10005&lt;/span&gt;
&lt;pre class=&quot;text&quot; id=&quot;11_out_text&quot;&gt;
FractionalDigits[x_, digs_Integer] := 
 NestList[{Mod[2 First[#], 1], Floor[2 First[#]]} &amp;amp;, {x, 0}, digs][[
  2 ;;, -1]];
  GraphicsRow[
 Function[a, 
   ArrayPlot[
    FractionalDigits[#, 40] &amp;amp; /@ 
     NestList[a # (1 - #) &amp;amp;, N[1/8, 80], 80]]] /@ {2.5, 3.3, 3.4, 3.5,
    3.6, 4}]
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;the-rest-of-the-story&quot;&gt;The Rest of the Story&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/mitchell-portrait.jpg&quot; alt=&quot;Portrait of Mitchell&quot; title=&quot;Portrait of Mitchell&quot; width=&quot;320&quot; height=&quot;375&quot; class=&quot;alignnone size-full wp-image-21169&quot;/&gt;&lt;br/&gt;&lt;small&gt;(&lt;em&gt;Photograph by Predrag Cvitanović&lt;/em&gt;)&lt;/small&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;So what became of Mitchell? After four years at Cornell, he moved to the Rockefeller University in New York, and for the next 30 years settled into a somewhat Bohemian existence, spending most of his time at his apartment on the Upper East Side of Manhattan.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;While he was still at Los Alamos, Mitchell had married a woman from Germany named Cornelia, who was the sister of the wife of physicist (and longtime friend of mine) &lt;a href=&quot;https://en.wikipedia.org/wiki/David_Kelly_Campbell&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;David Campbell&lt;/a&gt;, who had started the &lt;a href=&quot;https://cnls.lanl.gov/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Center for Nonlinear Studies&lt;/a&gt; at Los Alamos, and would later go on to be provost at Boston University. But after not too long, Cornelia left Mitchell, taking up instead with none other than Pete Carruthers. (Pete—who struggled with alcoholism and other issues—later reunited with Lucy, but died in 1997 at the age of 61.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;When he was back at Cornell, Mitchell met a woman named &lt;a href=&quot;http://gunillafeigenbaum.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Gunilla&lt;/a&gt;, who had run away from her life as a pastor’s daughter in a small town in northern Sweden at the age of 14, had ended up as a model for &lt;a href=&quot;https://www.wolframalpha.com/input/?i=Salvador+Dali&quot;&gt;Salvador Dalí&lt;/a&gt;, and then in 1966 had been brought to New York as a fashion model. Gunilla had been a journalist, video maker, playwright and painter. Mitchell and she married in 1986, and remained married for 26 years, during which time Gunilla developed quite a career as a &lt;a href=&quot;http://gunillafeigenbaum.com/artist.asp?ArtistID=23226&amp;amp;AKey=NQFHP8B4&amp;amp;tid=2634&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;figurative painter&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell’s last solo academic paper was published in 1987. He did publish a handful of other papers with various collaborators, though none were terribly remarkable. Most were extensions of his earlier work, or attempts to apply traditional methods of mathematical physics to various complex fluid-like phenomena.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell liked interacting with the upper echelons of academia. He received all sorts of honors and recognition (though never a Nobel Prize). But to the end he viewed himself as something of an outsider—a Renaissance man who happened to have focused on physics, but didn’t really buy into all its institutions or practices.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;From the early 1980s on, I used to see Mitchell fairly regularly, in New York or elsewhere. He became a daily user of Mathematica, singing its praises and often telling me about elaborate calculations he had done with it. Like many mathematical physicists, Mitchell was a connoisseur of &lt;a href=&quot;https://reference.wolfram.com/language/guide/SpecialFunctions.html&quot;&gt;special functions&lt;/a&gt;, and would regularly talk to me about &lt;a href=&quot;https://www.stephenwolfram.com/publications/history-future-special-functions/index.html&quot;&gt;more and more exotic functions&lt;/a&gt; he thought we should add.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Mitchell had two major excursions outside of academia. By the mid-1980s, the young poetess—now named Kathy Hammond—that Mitchell had known at Cornell had been an advertising manager for the &lt;em&gt;New York Times&lt;/em&gt; and had then married into the family that owned the &lt;em&gt;Hammond World Atlas&lt;/em&gt;. And through this connection, Mitchell was pulled into a completely new field for him: cartography.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I talked to him about it many times. He was very proud of figuring out how to use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_mapping_theorem&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Riemann mapping theorem&lt;/a&gt; to produce custom local projections for maps. He described (though I never fully understood it) a very physics-based algorithm for placing labels on maps. And he was very pleased when finally an entirely new edition of the &lt;em&gt;Hammond World Atlas&lt;/em&gt; (that he would refer to as “my atlas”) came out.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Starting in the 1980s, there’d been an increasing trend for physics ideas to be applied to quantitative finance, and for physicists to become Wall Street quants. And with people in finance continually looking for a unique edge, there was always an interest in new methods. I was certainly contacted a lot about this—but with the success of &lt;a href=&quot;https://www.wolframalpha.com/input/?i=james+gleick&quot;&gt;James Gleick&lt;/a&gt;’s 1987 book &lt;em&gt;Chaos&lt;/em&gt; (for which I did a long interview, though was only mentioned, misspelled, in a list of scientists who’d been helpful), there was a whole new set of people looking to see how “chaos” could help them in finance.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;One of those was a certain &lt;a href=&quot;https://en.wikipedia.org/wiki/Michael_Goodkin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Michael Goodkin&lt;/a&gt;. When he was in college back in the early 1960s, Goodkin had started a company that marketed the legal research services of law students. A few years later, he enlisted several Nobel Prize–winning economists and started what may have been the first hedge fund to do computerized arbitrage trading. Goodkin had always been a high-rolling, globetrotting gambler and backgammon player, and he made and lost a lot of money. And, down on his luck, he was looking for the next big thing—and found chaos theory, and Mitchell Feigenbaum.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;For a few years he cultivated various physicists, then in 1995 he found a team to start a company called &lt;a href=&quot;https://www.numerix.com/&quot; target=&quot;_blank&quot;&gt;Numerix&lt;/a&gt; to commercialize the use of physics-like methods in computations for increasingly exotic financial instruments. Mitchell Feigenbaum was the marquee name, though the heavy lifting was mostly done by my longtime friend &lt;a href=&quot;http://guava.physics.uiuc.edu/~nigel/&quot; target=&quot;_blank&quot;&gt;Nigel Goldenfeld&lt;/a&gt;, and a younger colleague of his named Sasha Sokol.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;At the beginning there was lots of mathematical-physics-like work, and Mitchell was quite involved. (He was an enthusiast of &lt;a href=&quot;https://reference.wolfram.com/language/guide/StochasticDifferentialEquationProcesses.html&quot;&gt;Itô calculus&lt;/a&gt;, gave &lt;a href=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/Notes-on-Ito-Calculus.pdf&quot; target=&quot;_blank&quot;&gt;lectures about it&lt;/a&gt;, and was proud of having found 1000&lt;span class=&quot;special-character Cross&quot;&gt;&lt;/span&gt; speed-ups of stochastic integrations.) But what the company actually did was to write C++ libraries for banks to integrate into their systems. It wasn’t something Mitchell wanted to do long term. And after a number of years, Mitchell’s active involvement in the company declined.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;(I’d met Michael Goodkin back in 1998, and 14 years later—having recently written his autobiography &lt;a href=&quot;https://www.amazon.com/Wrong-Answer-Faster-Machine-Trillions/dp/1118133404&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;The Wrong Answer Faster: The Inside Story of Making the Machine That Trades Trillions&lt;/em&gt;&lt;/a&gt;—he suddenly contacted me again, pitching my involvement in a rather undefined new venture. Mitchell still spoke highly of Michael, though when the discussion rather bizarrely pivoted to me basically starting and CEOing a new company, I quickly dropped it.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;I had many interactions with Mitchell over the years, though they’re not as well archived as they might be, because they tended to be verbal rather than written, since, as Mitchell told me (in email): “I dislike corresponding by email. I still prefer to hear an actual voice and interact…”&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;There are fragments in my archive, though. There’s correspondence, for example, about Mitchell’s 2004 60th-birthday event, that I couldn’t attend because it conflicted with a significant birthday for one of my children. In lieu of attending, I commissioned the creation of a “Feigenbaum–Cvitanović Crystal”—a 3D rendering in glass of the limiting function &lt;em&gt;g&lt;/em&gt;(&lt;em&gt;z&lt;/em&gt;) in the complex plane.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;It was a little complex to solve the functional equation, and the laser manufacturing method initially shattered a few blocks of glass, but eventually the object was duly made, and sent—and I was pleased many years later to see it nicely displayed in Mitchell’s apartment:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/feigenbaum-cvitanovic-crystal.jpg&quot; alt=&quot;Feigenbaum–Cvitanović crystal&quot; title=&quot;Feigenbaum–Cvitanović crystal&quot; width=&quot;450&quot; height=&quot;250&quot; class=&quot;alignnone size-full wp-image-21369&quot;/&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Sometimes my archives record mentions of Mitchell by others, usually Predrag. In 2007, Predrag reported (with characteristic wit):&lt;/p&gt;
&lt;p&gt;“Other news: just saw Mitchell, he is dating Odyssey.&lt;/p&gt;
&lt;p&gt;No, no, it’s not a high-level Washington type escort service—he is dating Homer’s Odyssey, by computing the positions of low stars as function of the 26000 year precession—says Hiparcus [sic] had it all figured out, but Catholic church succeeded in destroying every single copy of his tables.”&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;Living up to the Renaissance man tradition, Mitchell always had a serious interest in history. In 2013, responding to a &lt;a href=&quot;https://blog.stephenwolfram.com/2013/05/dropping-in-on-gottfried-leibniz/&quot;&gt;piece of mine about Leibniz&lt;/a&gt;, Mitchell said he’d been a Leibniz enthusiast since he was a teenager, then explained:&lt;/p&gt;
&lt;p&gt;“The Newton hagiographer (literally) Voltaire had no idea of the substance of the Monadology, so could only spoof ‘the best of all possible worlds’. Long ago I’ve published this as a verbal means of explaining 2^n universality.&lt;/p&gt;
&lt;p&gt;Leibniz’s second published paper at age 19, ‘On the Method of Inverse Tangents’, or something like that, is actually the invention of the method of isoclines to solve ODEs, quite contrary to the extant scholarly commentary. Both Leibniz and Newton start with differential equations, already having received the diff. calculus. This is quite an intriguing story.”&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;But the mainstay of Mitchell’s intellectual life was always mathematical physics, though done more as a personal matter than as part of institutional academic work. At some point he was asked by his then-young goddaughter (he never had children of his own) why the Moon looks larger when it’s close to the horizon. He wrote back an explanation (a bit in the style of Euler’s &lt;a href=&quot;https://archive.org/details/letterseulertoa00eulegoog/&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;Letters to a German Princess&lt;/em&gt;&lt;/a&gt;), then realized he wasn’t sure of the answer, and got launched into many years of investigation of optics and image formation. (He’d actually been generally interested in the retina since he was at MIT, influenced by &lt;a href=&quot;https://en.wikipedia.org/wiki/Jerome_Lettvin&quot; target=&quot;_blank&quot;&gt;Jerry Lettvin&lt;/a&gt; of “&lt;a href=&quot;https://hearingbrain.org/docs/letvin_ieee_1959.pdf&quot; target=&quot;_blank&quot;&gt;What the Frog’s Eye Tells the Frog’s Brain&lt;/a&gt;” fame.)&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;He would tell me about it, explaining that the usual theory of image formation was wrong, and he had a better one. He always used the size of the Moon as an example, but I was never quite clear whether the issue was one of optics or perception. He never published anything about what he did, though with luck his manuscripts (rumored to have the makings of a book) will eventually see the light of day—assuming others can understand them.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;When I would visit Mitchell (and Gunilla), their apartment had a distinctly Bohemian feel, with books, papers, paintings and various devices strewn around. And then there was The Bird. It was a cockatoo, and it was loud. I’m not sure who got it or why. But it was a handful. Mitchell and Gunilla nearly got ejected from their apartment because of noise complaints from neighbors, and they ended up having to take The Bird to therapy. (As I learned in a slightly bizarre—and never executed—plan to make videogames for “&lt;a href=&quot;https://blog.stephenwolfram.com/2018/01/showing-off-to-the-universe-beacons-for-the-afterlife-of-our-civilization/&quot;&gt;they-are-alien-intelligences-right-here-on-this-planet&lt;/a&gt;” pets, cockatoos are social and, as pets, arguably really need a “Twitter for Cockatoos”.)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/cockatoo.png&quot; alt=&quot;The Bird&quot; title=&quot;The Bird&quot; width=&quot;350&quot; height=&quot;306&quot; class=&quot;alignnone size-full wp-image-21203&quot;/&gt;&lt;br/&gt;&lt;small&gt;&lt;em&gt;(Photograph by &lt;a href=&quot;https://www.flickr.com/photos/birdtracks/3300192151/in/album-72157614039068642&quot; target=&quot;_blank&quot;&gt;Predrag Cvitanović&lt;/a&gt;)&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;In the end, though, it was Gunilla who left, with the rumor being that she’d been driven away by The Bird.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;The last time I saw Mitchell in person was a few years ago. My son Christopher and I visited him at his apartment—and he was in full Mitchell form, with eyes bright, talking rapidly and just a little conspiratorially about the mathematical physics of image formation. “Bird eyes are overrated”, he said, even as his cockatoo squawked in the next room. “Eagles have very small foveas, you know. Their eyes are like telescopes.”&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;“Fish have the best eyes”, he said, explaining that all eyes evolved underwater—and that the architecture hadn’t really changed since. “Fish keep their entire field of view in focus, not like us”, he said. It was charming, eccentric, and very Mitchell.&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;For years, we had talked from time to time on the phone, usually late at night. I saw Predrag a few months ago, saying that I was surprised not to have heard from Mitchell. He explained that Mitchell was sick, but was being very private about it. Then, a few weeks ago, just after midnight, Predrag sent me an email with the subject line “Mitchell is dead”, explaining that Mitchell had died at around 8 pm, and attaching a quintessential Mitchell-in-New-York picture:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/mitchell-new-york.jpg&quot; alt=&quot;Mitchell in New York&quot; title=&quot;Mitchell in New York&quot; width=&quot;320&quot; height=&quot;448&quot; class=&quot;alignnone size-full wp-image-21168&quot;/&gt;&lt;br/&gt;&lt;small&gt;(&lt;em&gt;Photograph by Predrag Cvitanović&lt;/em&gt;)&lt;/small&gt;&lt;/p&gt;
&lt;p class=&quot;Text&quot;&gt;It’s kind of a ritual I’ve developed when I hear that someone I know has died: I immediately search my archives. And this time I was surprised to find that a few years ago Mitchell had successfully reached voicemail I didn’t know I had. So now we can give Mitchell the last word:&lt;br/&gt;&lt;/p&gt;
&lt;div class=&quot;WriAudio&quot;&gt;
&lt;p&gt;Play Audio&lt;/p&gt;

&lt;img src=&quot;https://blog.stephenwolfram.com/data/uploads/2019/07/player.png&quot; alt=&quot;Mitchell's voicemail&quot; title=&quot;Mitchell's voicemail&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p class=&quot;Text&quot;&gt;And, of course, the last number too: 4.66920160910299067185320382…&lt;/p&gt;
</description>
<pubDate>Tue, 23 Jul 2019 19:22:03 +0000</pubDate>
<dc:creator>techgipper</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.stephenwolfram.com/2019/07/mitchell-feigenbaum-1944-2019-4-66920160910299067185320382/</dc:identifier>
</item>
<item>
<title>Want to hire the best programmers? Offer growth</title>
<link>https://triplebyte.com/blog/want-hire-best-programmers-offer-growth</link>
<guid isPermaLink="true" >https://triplebyte.com/blog/want-hire-best-programmers-offer-growth</guid>
<description>&lt;p&gt;Each week, Triplebyte matches hundreds of programmers with 450+ tech companies. We're somewhat like a dating site for software engineers and the companies that want to hire them. So, much like a dating site would, we ask every engineer on our platform what they would be most excited about in their next job in order to best match them with companies. Among other things, we have candidates select from workplace attributes that we refer to as motivators. These include options like “opportunities for professional growth”, “mentorship”, and “inclusive workplace”.&lt;/p&gt;
&lt;p&gt;Motivator data helps us optimize our matches for a good candidate/company fit, but it also gives us useful insight into the values and desires of software engineers. We recently analyzed this data to identify patterns and to better understand how factors like experience level, gender identity, and skill level correlate with these preferences.&lt;/p&gt;
&lt;p&gt;It's not hard to imagine how this data might be useful. Many companies, for example, tell us they're interested in increasing the representation of women in their engineering departments. This can be a bit of a guessing game on their part, especially when (almost by definition) the people guessing are often a currently-male-dominated engineering department. Thanks to our very large data set, we don't have to guess: we can say what women engineers in particular are looking for with high statistical confidence. Similarly, some companies are trying to attract competent engineers and some are trying to attract the very best, and we can ask whether those two groups are motivated differently. As we'll see later, it turns out that they are. &lt;a href=&quot;https://triplebyte.com/blog/outcompete-faang-hiring-engineers&quot;&gt;Outcompeting tech giants is hard&lt;/a&gt;, and knowing more about what candidates are looking for can give you a competitive edge.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;br/&gt;Let’s start with a high-level look at the data. We took a look at every candidate that has passed Triplebyte’s screening process, scrubbed out a few for whom data was incomplete (typically candidates from earlier in Triplebyte’s history, prior to us tracking all of the relevant data), and counted up the selections for every remaining candidate on our platform. Note that because candidates typically select three motivator choices, the percentages in this and the following graphs do not add up to 100%.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;“Opportunities for professional growth” is the clear winner, appearing 13 percentage points above even “salary”. As we'll see, this turns out to be a universal interest: in almost every cohort we looked at, professional growth was the most selected motivator. Other motivators move up or down in the rankings between men and women, between junior and senior candidates, and so on, but professional growth is always a priority.&lt;/p&gt;
&lt;p&gt;This is probably rational. Software engineering is a fast-moving field, and to stay still is to be left behind. &lt;a href=&quot;https://triplebyte.com/blog/who-y-combinator-companies-want&quot;&gt;We've written before&lt;/a&gt; about how things like an engineer's choice of language can affect their job prospects, and popular choices change quickly over time. In 2002, a very good COBOL or Fortran engineer was probably in a good place as far as their job search was concerned, but today they're unlikely to get much attention from companies that have moved on to more modern languages. Had that engineer focused on growth to keep up with changes in the industry, they might have learned Python or JavaScript and been able to keep up with the market.&lt;/p&gt;
&lt;p&gt;It's not uncommon for us to see skilled programmers left on the sidelines for the sin of sticking with PHP or Perl for a little too long. They're stuck in a catch-22, unable to move on to more modern technologies because the companies using them don't want someone rooted in old ways of doing things. For all that software engineering is a lucrative and comfortable career in some respects, a smart and career-aware engineer knows that their long-term employability is more precarious than it might look at any given time.&lt;/p&gt;
&lt;p&gt;Still, “professional growth” is a broad term, and it would be dangerous to assume too much about candidates' motivation for selecting it. Perhaps they interpreted “growth” to mean something like “promotion into management”, rather than growth of their technical skills. So we listened to a number of phone calls with candidates and reached out to a few candidates directly to dig deeper about what “professional growth” meant to them. Learning was the common theme in their responses:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;“A job that helps me gain professional growth would include opportunities for me to learn new things and learn how to improve my skills in various technical areas.”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“[It's important to] feel like you're constantly learning.”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“I want to be the worst person on a team so I can learn.”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;“I think professional growth means room for improvement in terms of both technical skills and interpersonal skills. I chose this because I want to continue to improve my skills during my work at my workplace instead of just utilizing skills that I already have and sticking to what I am already comfortable with. So a job that helps me gain professional growth would include opportunities for me to learn new things and learn how to improve my skills in various technical areas.”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In short, &lt;strong&gt;software engineers of all stripes want, more than anything else, to develop their abilities as engineers&lt;/strong&gt;. They want it more than pay, more than work/life balance, and more than autonomy. This is probably, at least in part, because they know that every other priority they have will eventually depend on keeping up with the industry.&lt;/p&gt;
&lt;p&gt;This remains true even for very experienced candidates. Triplebyte's background-blind process often attracts non-traditional junior candidates who might have trouble getting their foot in the door in a traditional recruitment process. As a result, one might wonder whether this interest in professional growth turns out to be an artifact of having non-traditional junior engineers. This turns out not to be the case.&lt;/p&gt;
&lt;p&gt;In fact, preferences do vary between engineers of different seniorities, but the focus on growth does not. Here's the same data shown above, this time broken down by different tiers of seniority: junior (defined here as &amp;lt;1 year of experience), mid-level (1-5 years), and senior (&amp;gt;5 years). All three levels of seniority overwhelmingly prioritize professional growth relative to other options.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/52bba46695c0791cfc1e18377f793eaf4f817232.png?1563315985&quot; alt=&quot;Blog-Motivators2-BySeniority (1).png&quot;/&gt;&lt;br/&gt;Senior engineers actually &lt;em&gt;are&lt;/em&gt; marginally less likely to select “opportunities for professional growth”, but the difference is small, and growth remains their most-selected option.&lt;/p&gt;
&lt;p&gt;Other motivators, however, do show significant shifts. “Mentorship”, unsurprisingly, plummets as engineers gain more experience. “Autonomy” and &lt;q&gt;flexible work arrangements&lt;/q&gt; rise. This is probably in part because more experienced engineers tend to be more skilled, though not by nearly as much as is generally believed (one of many reasons our interviews are background-blind). As we’ll see later, these are motivators that are more common among more capable engineers.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;br/&gt;Going into this analysis, we felt fairly sure that “inclusive workplace” would turn out to be a strong differentiator between male and female engineers. We weren't sure whether or not other motivators would show significant differences, but one of the valuable things about data analysis is that you sometimes find things you're not looking for.&lt;/p&gt;
&lt;p&gt;Our first step was to simply break the data down by gender.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Both here and throughout this article, 'gender' and related terms refer to a candidate's self-reported choice of preferred pronouns. Although a few candidates chose gender-neutral pronouns or did not select a pronoun at all, the number who did so is small enough that we can't draw any meaningful statistical conclusions about them. For the purposes of this article, we'll focus on candidates who chose binary pronouns.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/0deb00add518206150cc1bae4074282491012ba4.png?1563316002&quot; alt=&quot;Blog-Motivators3-ByGender.png&quot;/&gt;&lt;br/&gt;“Opportunities for professional growth” remains at the top. In fact, it’s even more of a priority for women engineers than it is for engineers in general.&lt;/p&gt;
&lt;p&gt;As expected, “inclusive workplace” shows the largest relative difference between men and women engineers. Female-identified candidates chose it three times as often, a far larger relative difference than any of the other common motivators. Although some of the reasoning is probably obvious, we reached out to a few women to ask why they prioritized inclusivity:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;q&gt;As a female software engineer with an international worker status, I recognize that I may be minorities in multiple different dimensions. So I definitely wanted to be in a workplace that would at least put some emphasis on creating inclusive workspace so that I can feel comfortable in the environment.&lt;/q&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;One response was especially striking:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;q&gt;I picked “inclusive workplace” and I didn't want to put “mentorship” down, even though it is important to me. There's a sense that women are less independent, and I feel like saying you want “mentorship” gets you told that “we don't do hand-holding here”.&lt;/q&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Some of the other apparent differences are more surprising. Women in our data set were much more likely to choose “mentorship” and much less likely to choose “flexible work arrangements” than men were, for example. But here, we have to be careful with a naive analysis, because our samples of men and of women are not directly comparable. As gender-imbalanced as tech is today, it was even more so in the past. As a result, women in our data set tended to have fewer years of experience than the men in our dataset did, and thus motivators more common to junior engineers would (spuriously) appear to be more common among women. As an example, only about a third of our sample of women had more than five years of experience, while around three-fifths of our sample of men did. Since “flexible work arrangements” is chosen much more often by senior engineers than by junior engineers, this could easily introduce an apparent difference between men and women engineers that turns out to be a masked difference in seniority rather than a difference in their actual priorities.&lt;/p&gt;
&lt;p&gt;To correct for this, we scaled our sample of women engineers to a seniority breakdown that matched our male sample in order to make apples-to-apples comparisons. In other words, we split our data set by both gender and seniority, then weighted the seniority tiers in our sample of women to match the proportions of the corresponding tier among men.&lt;/p&gt;
&lt;p&gt;The resulting data looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/da38800544662f80a00b6a7a255a7d68c4681a55.png?1563316018&quot; alt=&quot;Blog-Motivators4-ByGenderAdjusted.png&quot;/&gt;&lt;br/&gt;Some of the differences disappear. “Mentorship”, for example, no longer shows any difference between male- and female-identified candidates (19% of men and an adjusted 20% of women chose “mentorship” as a motivator).&lt;/p&gt;
&lt;p&gt;'Opportunities for professional growth' remains the dominant motivator and, in fact, becomes even more dominant among women. After adjusting for seniority, a whopping 62% of women chose professional growth as a motivator.&lt;/p&gt;
&lt;p&gt;“Work-life balance” is a more interesting case. Among men, “work-life balance” doesn't vary much by seniority, so one might reasonably expect that adjusting our sample of women for seniority wouldn't affect its prevalence very much. However, senior women engineers pick it so often (almost 50% of the time) that it rises into second place, selected almost half again as often by our adjusted sample of women (43%) as by men (31%).&lt;/p&gt;
&lt;p&gt;Most of these differences are highly statistically significant, but this is a case where we might be more interested in effect size than in statistical power. Here’s the same seniority-adjusted data, this time sorted by the relative differences between men’s and women’s selections. For example, “open communication” is selected 75% more often by men. The highlighted entries are statistically significant.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/00ff01ee9ea9a0b64f0786bc68c927ea5ce110d1.png?1563316034&quot; alt=&quot;Blog-Motivators5-GenderDifferences (1).png&quot;/&gt;&lt;br/&gt;Unsurprisingly, women selected “inclusive workplace” 171% more often than men did. There’s no contrarian conclusion here: &lt;strong&gt;inclusivity matters to women engineers.&lt;/strong&gt; That being said, even this very large relative difference doesn’t override the fact that &lt;strong&gt;women engineers, like engineers in general, still prioritize professional growth over all else.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;More interesting is the fact that many other motivators without obvious links to gender or to gendered experience show differences. In addition to inclusivity, women were more likely to select “high quality codebase”, “work-life balance”, and (much more weakly) “culture of transparency”. These motivators, for the most part, fit a “comfort cluster” of environmental motivators weakly correlated with one another. In other words, &lt;strong&gt;after growth and inclusivity, women in our data set tended to disproportionately value comfort in both their workplace and their codebase.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Men, on the other hand, were statistically more likely to select “autonomy”, “salary”, “fast-paced environment”, “flat organization”, “product-driven”, and “open communication”. Some of these fit into traditional gender norms in ways that probably don't need much further explanation. Some candidates may believe in these roles themselves; others may feel social pressure to conform to them. In either case they apparently prioritize them in their job search.&lt;/p&gt;
&lt;p&gt;Obvious norms don't account for all the differences, though. “Flat organization” in particular stands out - not a single woman in our data set selected it! (&lt;q&gt;Flat organization&lt;/q&gt; is a rare selection in general, but the difference is highly statistically significant to the point that it is unlikely to be noise.) It's harder to get a signal on why a candidate might &lt;em&gt;avoid&lt;/em&gt; selecting one of these motivators, but one speculative possibility is that women engineers may see ostensibly flat organizations as fertile ground for implicit biases, and thus find them less comfortable than their male counterparts.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;br/&gt;One advantage to Triplebyte's data set is that we can look at our data not just for demographic traits, but also for the skill displayed on our technical screen. It's reasonable to imagine that particularly good engineers might differ from the population as a whole, so we sliced up our motivator data further by how candidates scored on our screen.&lt;/p&gt;
&lt;p&gt;For the purposes of this article, we’re arbitrarily defining “great” candidates as those scoring between 95th and 98th percentile on our technical interview, and “the best” candidates as those scoring at 98th percentile or above. We pass a little over 30% of candidates who take our interview (the exact number fluctuates depending on incoming candidate quality), so these two groups together make up a little under a sixth of our passing candidates that actually end up on our platform visible to companies that hire through us. Only passing candidates select motivators, so all of the data in previous sections of this article has been on this passing 30+% percent of candidates.&lt;/p&gt;
&lt;p&gt;As it turns out, highly skilled 'great' engineers do indeed differ from the rest of our candidate pool:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/408882d429f788720d05134debef4e31580272e0.png?1563316056&quot; alt=&quot;Blog-Motivators6-AllvsGreat.png&quot;/&gt;&lt;br/&gt;Great engineers answer “mentorship” much less, which makes sense - in most cases, they're already quite a bit better than the person who would be mentoring them. “Opportunities for professional growth” also drops, probably because at this skill level job prospects are much rosier. Much more surprising is the massive drop-off in “open communication”: again, it's hard to tell why a candidate would avoid a selection, but one possibility that these are the sorts of engineers who like to be handed a problem and then left alone.&lt;/p&gt;
&lt;p&gt;On the other hand, great engineers answer “salary”, “impressive team members”, “autonomy”, “fast-paced environment” and “equity” much more. “Autonomy” suggests that these engineers know that they're skilled and can be trusted to work with minimal supervision, and “salary”, “fast-paced environment”, and “impressive team members” reflect career ambition appropriate to that skill level.&lt;/p&gt;
&lt;p&gt;In short, &lt;strong&gt;great engineers want to be paid well and allowed to exercise their skills in challenging environments&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This isn’t very surprising. We expected that these differences would persist, or be magnified, when we looked at the very best engineers; after all, it seems logical that more of whatever traits make someone a good engineer would make someone an even better one.&lt;/p&gt;
&lt;p&gt;Our expectations turned out to be quite wrong!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d25hn4jiqx5f7l.cloudfront.net/file_attachments/files/original/1b7fec856271bb8fb9f425489d22b11bccabf592.png?1563316068&quot; alt=&quot;Blog-Motivators7-AllvsGreatvsBest (1).png&quot;/&gt;&lt;br/&gt;The best engineers have completely different priorities from great engineers. In many cases, the trends from the previous graph actually reverse when looking at the best engineers. For example, great engineers selected “work/life balance” less often than the population as a whole, but the best engineers rank it as their top priority by a gigantic margin, making it the most selected option in any cohort at 65%. “Impressive team members”, on the other hand, was selected more among great engineers but plummets to near the bottom among the best.&lt;/p&gt;
&lt;p&gt;The best engineers notably &lt;em&gt;de&lt;/em&gt;-emphasize ambition. They’re so good that their employment is almost certainly secure at least in the short term, and so they can work where they want - this group includes an electrical engineer, a professor at a small college in New England, a remote developer for an education platform, and a musician. With concerns about their professional future out of the way, they can seek meaning (“product-driven”) and comfort (“flexible work arrangements” and “work/life balance”).&lt;/p&gt;
&lt;p&gt;In other words, &lt;strong&gt;the priorities of the very best engineers are very different from those of merely great engineers&lt;/strong&gt;. The very best engineers want comfortable and meaningful work and the ability to choose their work environment and timing.&lt;/p&gt;
&lt;p&gt;This has very interesting implications for hiring. &lt;strong&gt;Most companies do not need the very best engineers,&lt;/strong&gt; and trying to appeal to them might discourage great - but slightly less superlative - engineers who would be able to meet their needs just fine. In other words, companies should consider whether they need good engineers or the very best engineers, and choose how they market to candidates accordingly.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;br/&gt;What can we take away from all of this?&lt;/p&gt;
&lt;p&gt;First, &lt;strong&gt;engineers, regardless of demographic, want the chance to learn and to advance their technical skills&lt;/strong&gt;. This is especially great news for new startups, since playing up the opportunity for engineers to learn new skills and technologies is a great way to set themselves apart. Relative to typical Bay Area software engineer salaries in the $150,000/year range, devoting (say) $1,000 or even $5,000 to classes or other forms of professional growth is not particularly expensive, but it can have a disproportionate effect on recruiting.&lt;/p&gt;
&lt;p&gt;Companies willing to put this investment into their engineers also have a competitive advantage. They can hire strong senior engineers who have fallen behind the latest technologies and thus source candidates who are much less competitive than other candidates of a similar skill level. Offering even moderate investment in training opens up a candidate pool that is currently underrated by the job market.&lt;/p&gt;
&lt;p&gt;Second, &lt;strong&gt;women engineers prioritize growth to an even greater extent than men, and they place particular secondary emphasis on inclusivity and comfort with their work and environment&lt;/strong&gt;. Putting time aside to work on technical debt and taking the time to build proper infrastructure are often discussed in terms of their business benefits, but it's rare to think of them as a recruiting tool to improve gender diversity in an engineering department.&lt;/p&gt;
&lt;p&gt;Explicit inclusivity is still important, however, and so is the growth that engineers as a whole want. In some cases, a desire for inclusivity is tied into a desire for growth: for example, a woman engineer who wants to learn through mentorship may not feel comfortable asking for it if she feels her environment is not supportive. It's important to note that an inclusive environment will help with more general recruiting, too: the top male engineers are also slightly more likely to mention inclusivity as an important value.&lt;/p&gt;
&lt;p&gt;Finally, &lt;strong&gt;it's critical to decide whether your company needs good engineers or the very best engineers&lt;/strong&gt;. It turns out to be quite difficult to target both, because the desires of a 95th percentile engineer and a 99th percentile engineer are different and often mutually exclusive. Most companies do not need the very very best engineers available. If your company doesn't - and it probably doesn't - then you can gain some competitive advantage by targeting less than the best engineers. They aren't in as much demand, they have a very different set of priorities, and they are likely just as capable for your purposes as the best engineers would be.&lt;/p&gt;
&lt;p&gt;If your company wants great engineers, emphasize pay, autonomy, and challenge. If your company wants the very best engineers, let them work from home, keep your expectations around hours worked reasonable, and go out of your way to emphasize meaningful product.&lt;/p&gt;
</description>
<pubDate>Tue, 23 Jul 2019 17:24:33 +0000</pubDate>
<dc:creator>ammon</dc:creator>
<og:type>article</og:type>
<og:title>Want to hire the best programmers? Offer growth.</og:title>
<og:description>We've interviewed thousands of engineers for jobs at small startups and tech giants. Here's what they've told us they want in their next job, broken down by experience level, gender, and skill.</og:description>
<og:url>https://triplebyte.com/blog/want-hire-best-programmers-offer-growth</og:url>
<og:image>https://d25hn4jiqx5f7l.cloudfront.net/blog-posts/og_images/og/78_1563317937.png?1563317937</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://triplebyte.com/blog/want-hire-best-programmers-offer-growth</dc:identifier>
</item>
<item>
<title>Facebook&amp;#039;s fake account problem</title>
<link>http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/</link>
<guid isPermaLink="true" >http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/</guid>
<description>&lt;div id=&quot;&quot;&gt;

&lt;p class=&quot;large&quot;&gt;Mark Zuckerberg's Ponzi Scheme&lt;br/&gt;&lt;span&gt;Congress and the FTC brought a knife to a gun fight.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;July 23, 2019&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;It's already a campaign issue for the next presidential election: should we, or should we not, break up the big tech companies? Elizabeth Warren says yes. Beto O'Rourke wants &quot;stronger regulations.&quot; Kamala Harris would rather talk about privacy. Everyone else—even Donald Trump—generally agrees that something needs to be done. But what?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;There are plenty of law professors, think tanks and political consultants eager to share their ideas, but none of them are asking the right questions. In the case of Facebook, distractions are understandable when the company arguably has the worst track record of any major technology company in history (and will soon pay a record, if toothless, $5 billion fine). Yet the unspoken issue at the center of it all remains: although Wall Street, Congress and the Federal Trade Commission haven't figured it out, Mark is running a Ponzi scheme.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;At this point it should come as no surprise to anyone paying attention that Mark is a bad-faith actor. He has no appreciation for the rule of law, or the role of a free press, and he has a dangerous tendency to view himself as infallible. After discovering a gaping security flaw in his product that revealed bulk information about friends of friends, exactly like Cambridge Analytica, I warned Mark in writing about the way his sloppy code would inevitably lead him to cross paths with the FTC and cause massive privacy and security concerns—in &lt;a href=&quot;http://www.thinkpress.com/authoritas/timeline.pdf&quot; target=&quot;_new&quot;&gt;April 2005&lt;/a&gt;. His response: problems with the &quot;Mark Zuckerberg production&quot; were actually someone else's responsibility and &quot;not worth arguing about.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;Clearly, Mark can no longer argue that his decisions as Facebook's CEO are immaterial (though he has &lt;a href=&quot;https://www.vox.com/2016/11/11/13596792/facebook-fake-news-mark-zuckerberg-donald-trump&quot; target=&quot;_new&quot;&gt;tried&lt;/a&gt;). Many have already lost their lives, whether through avoidable suicides or avoidable genocidal acts in Myanmar, due to his string of increasingly tone-deaf and spectacularly dishonest decisions. Now, fifteen years and approximately as many false apologizes after my classmate started a grand social experiment that first captivated the media, then locked it in a profitless box, and then played a major supporting role in bringing fascism to America, the general consensus is that the best way to handle Mark and his tech brethren is through the Sherman Anti-Trust Act. But the consensus is wrong, based on a mountain of misapprehensions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;In a nutshell, the argument in favor of anti-trust action is that in the midst of the longest economic expansion in U.S. history, it's the Progressive Era all over again. A recent New York Times op-ed penned by Mark's former roommate and co-founder, Chris Hughes, made essentially this point, relying heavily on input from the Roosevelt Institute. The Open Markets Institute agrees. In a &lt;a href=&quot;https://www.youtube.com/watch?v=xM9GMGDsKUU&amp;amp;t=24m40s&quot; target=&quot;_new&quot;&gt;talk at Harvard Law School&lt;/a&gt;, Matt Stoller argued that Facebook, Google and Amazon were &quot;born as monopolists.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;It's a compelling story, so long as one is willing to ignore the reality on the ground. For one thing, software products are not railroads, which require significant physical capital and labor to establish. Were he determined to do so, it would take Mark a few weeks to re-build Instagram and WhatsApp, and there really isn't any way the government could stop him. For another, I know that on this particular issue, Stoller is incorrect, because I was there when The Facebook was born on my hard drive on September 19, 2003, in Lowell House. It hardly resembled a monopoly. Monopolies are what happen as the result of prolonged neglect by law enforcement. They're not born; they're nourished by years and years of perverse incentives.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;The biggest problem with treating Facebook as a monopoly, as opposed to the byproduct of what Jesse Eisenger calls &quot;The Chickenshit Club,&quot; is that it wrongly affirms Mark's infallibility and fails to see through him and his scheme, let alone the reality that he's not even in control anymore because no one is. On October 26, 2012, Mark's friend and lieutenant, Sam Lessin, wrote, &quot;we are running out of humans (and have run-out of valuable humans from an advertiser perspective).&quot; At the time, it was far from clear that Facebook even had a viable business model, and according to Frontline, Sheryl Sandberg was panicking due to the company's poor revenue numbers.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;How times have changed; now there's a different source of panic. Facebook now has a market capitalization approaching $600 billion, making it nominally one of the most valuable companies on earth. It's a true business miracle: a company that was out of users in 2012 managed to find a wellspring of nearly infinite and sustained growth that has lasted it, so far, half of the way through 2019. So what is that magical ingredient, that secret sauce, that &quot;genius&quot; trade secret, that turned an over-funded money-losing startup into one of America's greatest business success stories? It's one that Bernie Madoff would recognize instantly: fraud, in the form of fake accounts.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;Old money goes out, and new money comes in to replace it. That's how a traditional Ponzi scheme works. Madoff kept his going for decades, managing to attain the rank of Chairman of the NASDAQ while he was at it. Zuckerberg's version is slightly different, but only slightly: old users leave after getting bored, disgusted and distrustful, and new users come in to replace them. Except that as Sam Lessin told us, the &quot;new users&quot; part of the equation was already getting to be a problem in 2012. To balance it out and keep &quot;growth&quot; on the rise, all Facebook had to do was turn a blind eye. And did it ever.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;In &lt;a href=&quot;https://www.plainsite.org/dockets/3bvv82ier/california-northern-district-court/singer-v-facebook-inc/&quot; target=&quot;_new&quot;&gt;&lt;em&gt;Singer v. Facebook, Inc.&lt;/em&gt;&lt;/a&gt;—a lawsuit filed in the Northern District of California alleging that Facebook has been telling advertisers that it can &quot;reach&quot; more people than actually exist in basically every major metropolitan area—the plaintiffs quote former Facebook employees, understandably identified only as Confidential Witnesses, as stating that Facebook's &quot;Potential Reach&quot; statistic was a &quot;made-up PR number&quot; and &quot;fluff.&quot; Also, that &quot;those who were responsible for ensuring the accuracy ‘did not give a shit.'&quot; Another individual, &quot;a former Operations Contractor with Facebook, stated that Facebook was not concerned with stopping duplicate or fake accounts.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;That's probably because according to its &lt;a href=&quot;https://s21.q4cdn.com/399680738/files/doc_financials/2019/Q1/Q1-2019-Earnings-Presentation.pdf&quot; target=&quot;_new&quot;&gt;last investor slide deck&lt;/a&gt; and basic subtraction, Facebook is not growing anymore in the United States, with zero million new accounts in Q1 2019, and only four million new accounts since Q1 2017. That leaves the rest of the world, where Facebook is growing fastest &quot;in India, Indonesia, and the Philippines,&quot; according to Facebook CFO David Wehner. Wehner didn't mention the fine print on page 18 of the slide deck, which highlights the Philippines, Indonesia and Vietnam as countries where there are &quot;meaningfully higher&quot; percentages of, and &quot;episodic spikes&quot; in, fake accounts. In other words, Facebook is growing the fastest in the locations worldwide where one finds the most fraud. In other other words, Facebook isn't growing anymore at all—it's shrinking. Even India, Indonesia and the Philippines don't register as many searches for Facebook as they used to. Many of the &quot;new&quot; users on Instagram are actually old users from the core platform looking to escape the deluge of fakery.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;The last time Mark suggested that Facebook's growth heyday might be behind it, in July 2018, the stock took a nosedive that ended up being the single largest one-day fall of any company's stock in the history of the United States. In about an hour, it &lt;a href=&quot;https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25&quot; target=&quot;_new&quot;&gt;plunged 20%&lt;/a&gt; from around $220 per share to about $165. Needless to day, the loss of about $120 billion in market capitalization in an hour provided a sufficient disincentive for Mark to avoid a repeat performance.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;Having narrowly escaped the ire of Wall Street, Mark knows he cannot get off the growth treadmill he set in motion years ago. The only solution: lying to investors about growth in an attempt to convince them that everything is fine. Yet signs that Mark's fake account problem is no different than Madoff's fake account statement problem are everywhere. Google Trends shows worldwide &quot;Facebook&quot; queries down 80% from their November 2012 peak. (Instagram doesn't even come close to making up for the loss.) Mobile metrics measuring use of the Facebook mobile app are down. And the company's own disclosures about fake accounts stand out mostly for their internal inconsistency—one set of numbers, measured in percentages, is disclosed to the SEC, while another, with absolute figures, appears on its &quot;transparency portal.&quot; While they reveal a problem escalating at an alarming rate and are constantly being revised upward—Facebook claims that false accounts are at 5% and duplicate accounts at 11%, up from 1% and 6% respectively in Q2 2017—they don't measure quite the same things, and are &lt;a href=&quot;https://www.plainsite.org/realitycheck/facebook.html&quot; target=&quot;_new&quot;&gt;impossible to reconcile&lt;/a&gt;. At the end of 2017, Facebook decided to stop releasing those percentages on a quarterly basis, opting for an annual basis instead. Out of sight, out of mind.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;One could argue that SEC disclosures are subject to strict regulations under the Securities Exchange Act and that Facebook would never be so bold as to lie to investors in black and white. That's true: it qualifies its fake account disclosures with the quizzical legal phrase &quot;significant judgment&quot; and it chose the color orange instead of black (insert Netflix joke here) for its transparency portal graph disclaimers that read, &quot;These metrics are in development.&quot; And one could further argue that the transparency portal metrics are reviewed by a team of academics, known as the Data Transparency Advisory Group (DTAG), who are supposed to vouch for their validity. But the DTAG academics—not one of whom is a statistician, despite Facebook's direct claim to the contrary, now erased—fully admit that they are paid by Facebook, and even after months of hard work, their &lt;a href=&quot;https://law.yale.edu/system/files/area/center/justice/document/dtag_report_5.22.2019.pdf&quot; target=&quot;_new&quot;&gt;final report&lt;/a&gt; released in April mentioned fake accounts only three times, and all three were in passing. On the accuracy or validity of Facebook's fake account numbers, the DTAG oddly had absolutely nothing to say.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;What Facebook does say is &lt;a href=&quot;https://newsroom.fb.com/news/2019/05/fake-accounts/&quot; target=&quot;_new&quot;&gt;this&lt;/a&gt;: its measurements, the ones subject to &quot;significant judgment,&quot; are taken from an undisclosed &quot;limited sample of accounts.&quot; How limited? That doesn't matter, because &quot;[w]e believe fake accounts are measured correctly within the limitations to our measurement systems&quot; and &quot;reporting fake accounts…may be a bad way to look at things.&quot; And how many fake accounts did Facebook report being created in Q2 2019? Only 2.2 billion, with a &quot;B,&quot; which is approximately the same as the number of active users Facebook would like us to believe that it has. A comprehensive look back at Facebook's disclosures suggests that of the company's 12 billion total accounts ever created, about 10 billion are fake. And &lt;a href=&quot;https://www.plainsite.org/realitycheck/facebook.html&quot; target=&quot;_new&quot;&gt;as many as 1 billion are probably active&lt;/a&gt;, if not more. (Facebook says that this estimate is &quot;not based on any facts,&quot; but much like the &lt;a href=&quot;http://nymag.com/intelligencer/2018/12/how-much-of-the-internet-is-fake.html&quot; target=&quot;_new&quot;&gt;false statistics&lt;/a&gt; it provided to advertisers on video viewership, that too is a lie.)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;So, fake accounts may be a bad way to look at things, as Facebook suggests—or they may be the key to the largest corporate fraud in history. Advertisers pay Facebook on the assumption that the people viewing and clicking their ads are real. But that's often not the case, Facebook has absolutely no incentive to solve the problem, it's already in court over it, and its former employees are talking. From Mark's vantage point, it's raining free money. All he has to do to get advertisers to spend is convince the world that Facebook is huge and it's only getting huger. No one in the media, let alone Congress, dares to ask potentially embarrassing questions, and few understand the minutiae of real-time pricing auctions, cookies and user disambiguation anyway. Everyone would rather talk about the company's dedication to &quot;innovation&quot; and the laughably remote chance that Libra, a needlessly complex pseudo-cryptocurrency system will disrupt central banking. In fact, Libra is best described as Facebook's Business Model Plan C (Plan A having been &quot;no privacy at all&quot; and Plan B being &quot;encrypt everything&quot;), which may actually be necessary as the scheme is starting to unravel.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;Mark is smart, but he's never been smart enough to listen to those with experience. Instead, he has prioritized growth at any cost, pulling all of the control rods out of the reactor to achieve it, and now that those costs have caught up with him—namely, genocide, a role in putting a fascist, white supremacist in the White House, and severe reputational damage—he literally has no idea what to do. His usual go-to acronyms—VR? AI?—aren't quite cutting it, and much like Chernobyl, the resulting fallout is everywhere, impossible to clean up, and there are dead bodies on the ground. Even his co-founder can't fully support him anymore, though Chris did still obsequiously refer to Mark as a &quot;good, kind person&quot; engaged in &quot;nothing more nefarious than the virtuous hustle of a talented entrepreneur.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;That's obviously false. Mark is not a good, kind person, as I have &lt;a href=&quot;https://www.huffpost.com/entry/the-legend-of-mark-zucker_b_732625&quot; target=&quot;_new&quot;&gt;written&lt;/a&gt; &lt;a href=&quot;https://www.huffpost.com/entry/what-it-was-like-to-be-ma_b_8586752&quot; target=&quot;_new&quot;&gt;for&lt;/a&gt; &lt;a href=&quot;http://www.aarongreenspan.com/writing/20170917/open-to-attack-and-connected-to-the-kremlin/&quot; target=&quot;_new&quot;&gt;years&lt;/a&gt;. The only hustle he is engaged in is the usual kind: the fraudulent kind. And if I'm wrong about any or all of this, and Facebook releases the data and methodology it is using to reach the conclusions that it has about the strength of its platform, then I will gladly admit that I'm wrong.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;But I'm not wrong. Facebook is a real product, but like Enron, it's also a scam, now the largest corporate scandal ever. It won't release its data about the 2016 election, about fake accounts, or about anything material—and because he knows it's a scam, Mark won't agree to testify before the British parliament in a way that could require him to actually answer any substantive questions, as &lt;a href=&quot;https://parliamentlive.tv/Event/Index/d434d37f-c020-44b4-bda8-11bbad29ac58&quot; target=&quot;_new&quot;&gt;I did in June&lt;/a&gt;. And because Facebook is also a component of the S&amp;amp;P 500, countless people have an incentive to maintain the status quo.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;gray legible&quot;&gt;So should we break up the tech companies and Facebook in particular? Would it have helped to separate Madoff Securities LLC into one company per floor, or split up Enron by division? Probably not, but talking about it is Facebook's dream come true. Because the question we should really be discussing is &quot;how many years should Mark Zuckerberg and Sheryl Sandberg ultimately serve in prison?&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;21,602 Views&lt;/p&gt;

&lt;table width=&quot;100%&quot; border=&quot;0&quot; cellpadding=&quot;8&quot; cellspacing=&quot;0&quot; readability=&quot;28.731562348375&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td/&gt;
&lt;td class=&quot;medium&quot;&gt;&lt;strong&gt;Add your comment in the box below.&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;Name*&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;name&quot; size=&quot;50&quot; value=&quot;&quot; class=&quot;textbox&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;E-Mail*&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;email&quot; size=&quot;50&quot; value=&quot;&quot; class=&quot;textbox&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;Web Site&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;website&quot; size=&quot;50&quot; value=&quot;&quot; class=&quot;textbox&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;Comment*&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot;&gt;
&lt;textarea name=&quot;comment&quot; rows=&quot;10&quot; cols=&quot;80&quot; class=&quot;textbox&quot; wrap=&quot;virtual&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td/&gt;
&lt;td class=&quot;medium&quot;&gt;&lt;strong&gt;What is the total when you put 4 and 5 together and add one hundred?&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt; *&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;input type=&quot;text&quot; name=&quot;checksum&quot; size=&quot;3&quot; value=&quot;&quot; class=&quot;textbox&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; colspan=&quot;2&quot;&gt;&lt;input type=&quot;image&quot; src=&quot;/shared/buttons/post.gif&quot; name=&quot;post&quot; align=&quot;bottom&quot; border=&quot;0&quot; width=&quot;80&quot; height=&quot;20&quot; alt=&quot;Post&quot; title=&quot;Post&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;13&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;1&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;&lt;strong&gt;Daniel&lt;/strong&gt;&lt;br/&gt;Today at 1:32 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&quot;Many have already lost their lives, whether through avoidable suicides or avoidable genocidal acts in Myanmar, due to his string of increasingly tone-deaf and spectacularly dishonest decisions.&quot;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;You can't be for fucking real... I dislike Zuckerberg as much as the next person, but that is just horseshit and an absolutely deranged and slanderous insinuation. Idiots like you really do allow yourselves some outrageous freedoms in what you accuse others for.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;14.519640852974&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;2&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;19.636670416198&quot;&gt;
&lt;p&gt;&lt;strong&gt;Freddy Bobs&lt;/strong&gt;&lt;br/&gt;Today at 3:09 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;'You can't be for fucking real... I dislike Zuckerberg as much as the next person, but that is just horseshit and an absolutely deranged and slanderous insinuation. You can't be for fucking real... '&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Why? The issues around depression and Facebook usage are well covered in the press and the literature. Facebook has played a significant (perhaps very) effect in terms of elections. There's plenty from reputable sites about facebook and myanmar...&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html&quot; target=&quot;_new&quot;&gt;https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;And on suicide...&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Social_media_and_suicide&quot; target=&quot;_new&quot;&gt;https://en.wikipedia.org/wiki/Social_media_and_suicide&lt;/a&gt;&lt;/p&gt;&lt;p&gt;So if you are trying to claim that facebook doesn't have real world serious effects - all the way through to *death*, then you'll need to back that up as to how and why.&lt;/p&gt;&lt;p&gt;In terms of the ponzi aspect - it's believable in so far as facebook operates largely a black box.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;26.5&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;3&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;22&quot;&gt;
&lt;p&gt;&lt;strong&gt;timothy holmes&lt;/strong&gt;&lt;br/&gt;Today at 3:24 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;WHAT_the_Fck!!@#$%&amp;gt;*&amp;amp;&amp;amp;^% This &quot;a role in putting a fascist, white supremacist in the White House,&quot; is an old argument of blaming fire when it burns your hand, and praising it when it cooks your meal. The Huffington Post became competitive to the NY TIMES, because there was no need for huge sums of money for the infrastructure of newspapers---printing presses, deliver systems, etc. Some of the critical software used to build Facebook, like PHP and Mysql are freely available to anyone, to monetize in any way they want. But the issue is not really about software. We in the West have been working on a theory of mind that is computational and representational for well over 300 years. The internet that emerged from this is really a minor issue. The real issue is what this theory of mind implies for human cognition. Our world is truly confused by all this; they literally do not know how to respond. But all current proposals are rooted in the past, which is not taking into account that first and foremost we are in the midst of a cognitive revolution, not primarily an industrial one. And this is the real danger here, because regulation could lead to censorship of mind and thought, not what machines are doing.&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;6&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;4&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;strong&gt;Mory&lt;/strong&gt;&lt;br/&gt;Today at 4:14 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Make that case, man. Sounds like you're well-placed to do it.&lt;br/&gt;I reckon a key part would be to be able to prove the % of fake accounts.&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;16.885714285714&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;5&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;16.032951289398&quot;&gt;
&lt;p&gt;&lt;strong&gt;A&lt;/strong&gt; (&lt;a href=&quot;http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/www.elonsai.space&quot; target=&quot;_new&quot;&gt;www.elonsai.space&lt;/a&gt;)&lt;br/&gt;Today at 4:26 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Great stuff, well argued. I think this message needs to go out on multiple channels, saturating the bandwidth is the only chance of effectively screaming into the void, it's hard to make a man understand something when his continued employment depends on him not understanding it, or something like that. I'm an artist working on some similar ideas involving Late Capitalism and Surveillance Capitalism, the future is already here - it's just not evenly distributed :D or in terms of what's happening in Xinjiang (1), :(&lt;br/&gt;I'd love to talk about it Aaron, hit me up&lt;/span&gt;&lt;/p&gt;&lt;p&gt;(1) &lt;a href=&quot;https://www.nytimes.com/2018/12/16/world/asia/xinjiang-china-forced-labor-camps-uighurs.html&quot; target=&quot;_new&quot;&gt;https://www.nytimes.com/2018/12/16/world/asia/xinjiang-china-forced-labor-camps-uighurs.html&lt;/a&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;4.5&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;6&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;strong&gt;FL&lt;/strong&gt;&lt;br/&gt;Today at 5:21 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;My guess is that at least 50% of the accounts on Facebook are fake. And it could be as high as 75%. I personally created 10 accounts when I was building a FB app.&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr bgcolor=&quot;#FFFFFF&quot; readability=&quot;9.5&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;medium&quot;&gt;&lt;strong&gt;&lt;span&gt;7&lt;/span&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;medium&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;&lt;strong&gt;Matthew Graybosch&lt;/strong&gt;&lt;br/&gt;Today at 7:13 PM&lt;/p&gt;
&lt;p&gt;&lt;span&gt;How many years should Zuckerberg and Sandberg serve? None. Why should we pay to give these two three hots and a cot at some white collar federal prison. Instead, they should be stripped bare under civil forfeiture and condemned to spend the rest of their lives in exile.&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;p align=&quot;center&quot;&gt;&lt;span&gt;Copyright © 2001-2017 Aaron Greenspan. All Rights Reserved.&lt;/span&gt;&lt;br/&gt;&lt;img src=&quot;http://www.aarongreenspan.com/shared/titles/hebrew.png&quot; align=&quot;bottom&quot; border=&quot;0&quot; width=&quot;320&quot; height=&quot;32&quot;/&gt;&lt;/p&gt;</description>
<pubDate>Tue, 23 Jul 2019 15:00:32 +0000</pubDate>
<dc:creator>thinkcomp</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.aarongreenspan.com/writing/20190723/mark-zuckerbergs-ponzi-scheme/</dc:identifier>
</item>
<item>
<title>No CS Degree – Interviews with self-taught developers</title>
<link>http://www.nocsdegree.com</link>
<guid isPermaLink="true" >http://www.nocsdegree.com</guid>
<description>&lt;header class=&quot;post-card-header&quot;&gt;&lt;h2 class=&quot;post-card-title&quot;&gt;This 22 year old self-taught PHP developer earns $15k a month and lives in an Austrian farmhouse&lt;/h2&gt;
&lt;/header&gt;&lt;section class=&quot;post-card-excerpt&quot; readability=&quot;30&quot;&gt;&lt;p&gt;I had a chat with Richard 'Pretzel Hands' Blechinger about earning a high income at a young age, advice he has for people with no CS degree and how he learned coding. It was great fun and I hope you enjoy the interview.&lt;/p&gt;
&lt;/section&gt;&lt;footer class=&quot;post-card-meta&quot;&gt;&lt;ul class=&quot;author-list&quot;&gt;&lt;li class=&quot;author-list-item&quot;&gt;
&lt;p&gt;Pete Codes&lt;/p&gt;
&lt;a href=&quot;https://www.nocsdegree.com/author/pete/&quot; class=&quot;static-avatar&quot;&gt;&lt;img class=&quot;author-profile-image&quot; src=&quot;https://www.nocsdegree.com/content/images/size/w100/2019/07/me-and-puppy.jpg&quot; alt=&quot;Pete Codes&quot;/&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;span class=&quot;reading-time&quot;&gt;7 min read&lt;/span&gt;&lt;/footer&gt;</description>
<pubDate>Tue, 23 Jul 2019 14:24:13 +0000</pubDate>
<dc:creator>Pete-Codes</dc:creator>
<og:type>website</og:type>
<og:title>No CS Degree</og:title>
<og:description>Inspiring interviews with successful self-taught and bootcamp developers</og:description>
<og:url>https://www.nocsdegree.com/</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nocsdegree.com/</dc:identifier>
</item>
<item>
<title>New York City to Consider Banning Sale of Cellphone Location Data</title>
<link>https://www.nytimes.com/2019/07/23/nyregion/cellphone-tracking-location-data.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2019/07/23/nyregion/cellphone-tracking-location-data.html</guid>
<description>&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Telecommunications firms and mobile-based apps make billions of dollars per year by selling customer location data to marketers and other businesses, offering a vast window into the whereabouts of cellphone and app users, often without their knowledge.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;That practice, which has come under increasing scrutiny and criticism in recent years, is now the subject of a proposed ban in New York. If the legislation is approved, it is believed that the city would become the first to forbid the sale of geolocation data to third parties.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;&lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://t.co/5ioCafwepy?amp=1&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The bill&lt;/a&gt;, which was introduced on Tuesday, would make it illegal for cellphone companies and mobile app developers to share location data gathered while a customer’s mobile device is within the five boroughs.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Cellphone companies and mobile apps collect detailed geolocation data of their users and then sell that information to legitimate companies such as digital marketers, roadside emergency assistance services, retail advertisers, hedge funds or — in the &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.vice.com/en_us/article/43j99g/eff-hits-atandt-with-class-action-lawsuit-for-selling-customers-location-to-bounty-hunters&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;case of a class-action lawsuit filed against AT&amp;amp;T&lt;/a&gt; — bounty hunters.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“The average person has no idea they are vulnerable to this,” said Councilman Justin L. Brannan, a Brooklyn Democrat who introduced the bill. “We are concerned by the fact that someone can sign up for cell service and their data can wind up in the hands of five different companies.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-17y7ayl epkadsg3&quot;&gt;
&lt;div class=&quot;css-15g2oxy epkadsg2&quot;&gt;
&lt;div class=&quot;css-4q4vfu e16ij5yr6&quot;&gt;
&lt;div class=&quot;css-i9gxme e16ij5yr4&quot;&gt;
&lt;div class=&quot;css-11jd139 e16ij5yr2&quot;&gt;The Unlikely Activists Who Took On Silicon Valley — and Won&lt;/div&gt;
&lt;time class=&quot;css-rqb9bm e16638kd0&quot; datetime=&quot;2018-08-14&quot;&gt;Aug 14, 2018&lt;/time&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;In the absence of a federal law specifically protecting consumers’ location data, cities and states have stepped up to enforce privacy regulations and location data rules. In San Francisco, voters approved a ballot measure in 2018 that would &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://news.bloomberglaw.com/privacy-and-data-security/san-francisco-voters-ok-privacy-first-policy&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;require companies to disclose their data practices&lt;/a&gt; and secure customer data to win city contracts.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;In January, the City of Los Angeles &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2019/01/03/technology/weather-channel-app-lawsuit.html?module=inline&quot; title=&quot;&quot;&gt;filed a lawsuit against the developers of the Weather Channel app&lt;/a&gt;, accusing them of deceptively collecting location data from millions of American users. The app tricked users into turning on location data by telling them that it would be used only to localize weather reports. Instead, the data was used for commercial purposes, like analysis for hedge funds, the suit alleged.&lt;/p&gt;
&lt;div class=&quot;css-79elbk&quot; data-testid=&quot;photoviewer-wrapper&quot;&gt;

&lt;div data-testid=&quot;photoviewer-children&quot; class=&quot;css-1a48zt4 ehw59r15&quot;&gt;
&lt;div class=&quot;css-zgakxe erfvjey0&quot;&gt;&lt;span class=&quot;css-1ly73wi e1tej78p0&quot;&gt;Image&lt;/span&gt;&lt;img alt=&quot;“The average person has no idea they are vulnerable to this,” said Councilman Justin L. Brannan, a Brooklyn Democrat who is introducing the bill.&quot; class=&quot;css-1m50asq&quot; src=&quot;https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy2/23nyprivacy2-articleLarge.jpg?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot; srcset=&quot;https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy2/23nyprivacy2-articleLarge.jpg?quality=90&amp;amp;auto=webp 600w,https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy2/23nyprivacy2-jumbo.jpg?quality=90&amp;amp;auto=webp 791w,https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy2/23nyprivacy2-superJumbo.jpg?quality=90&amp;amp;auto=webp 1582w&quot; sizes=&quot;50vw&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy2/23nyprivacy2-articleLarge.jpg?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot;/&gt;&lt;/div&gt;
&lt;span aria-hidden=&quot;true&quot; class=&quot;css-8i9d0s e13ogyst0&quot;&gt;“The average person has no idea they are vulnerable to this,” said Councilman Justin L. Brannan, a Brooklyn Democrat who is introducing the bill.&lt;/span&gt;&lt;span itemprop=&quot;copyrightHolder&quot; class=&quot;css-vuqh7u e1z0qqy90&quot;&gt;&lt;span class=&quot;css-1ly73wi e1tej78p0&quot;&gt;Credit&lt;/span&gt;&lt;span&gt;Gabriela Bhaskar for The New York Times&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;The proposed legislation in New York will almost certainly be opposed by the ad tech industry, which has a strong presence in the city. Mr. Brannan said he did not consult industry leaders for input.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;The bill would restrict cellphone companies and mobile apps from sharing location data to situations where they were “providing a service explicitly requested” by the customer. The language is designed to challenge the vague agreements customers click on when signing up for an app or a cellular service. The legislation would also exclude the collection of location data in “exchange for products or services.”&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Marc Rotenberg, president of the Electronic Privacy Information Center, said the collection of location data was now the “primary technique to track people in physical space” and “raises far-reaching privacy concerns.”&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“New York City is joining a growing list of cities that are not waiting for Washington to pass privacy legislation,” he added.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;The bill provides for steep fines, ranging from $1,000 per violation to $10,000 per day per user for multiple violations, while giving customers who have had their location data shared without their explicit permission the right to sue.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;David LeDuc, vice president of public policy for the Network Advertising Initiative, said he was concerned about allowing consumers to sue. The civil penalties under the law are “substantial” enough to force compliance, he said.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“Tacking on a private right of action is merely an invitation for unscrupulous trial attorneys to sue companies out of business, in many cases likely for merely making a compliance error where consumers have not suffered any injury,” Mr. LeDuc said.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Dave Grimaldi, executive vice president for public policy for the Interactive Advertising Bureau, a national trade association, said that a better approach would involve “a uniform national data privacy law that protects consumers regardless of where they reside.”&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Kathryn Wylde, president and chief executive of the Partnership for New York City, echoed Mr. Grimaldi’s remarks, suggesting that New York’s proposal would be an unwelcome addition to the “wide-ranging policy standards” in different states that companies must navigate.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“This is not an efficient way to solve this problem,” Ms. Wylde said.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;It is not yet clear whether the bill will pass the full Council, but the Council speaker, Corey Johnson, said he was open to exploring solutions to the privacy issues addressed in Mr. Brannan’s bill.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“We live in an age where people’s private information is being sold to the highest bidder, and it is important for us to think of 21st-century fixes,” Mr. Johnson said.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;A spokeswoman for Mayor Bill de Blasio, Freddi Goldstein, said that the mayor’s office will review the bill. If passed, the city’s Department of Information Technology and Telecommunications would enforce the law; a department spokeswoman, Robin Levine, said the agency agreed “wholeheartedly that New Yorkers’ personal information should not be surreptitiously collected and exploited.”&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Exceptions to the proposed ban include information legally supplied to law enforcement agencies; data provided to emergency responders; data required under federal, state or city law; and situations where customers knowingly provide their location data. The legislation does not require that law enforcement agencies be given location data or provide them with an entitlement to collect it.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Before the bill can come to a vote, it would probably first be heard by the Council’s Committee on Technology. Robert F. Holden, the committee chairman, called the bill a “priority.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“Privacy is a serious concern, especially in a day and age where more and more of it is stripped from us as technology progresses,” said Mr. Holden, who added that he was interested in seeing “how municipal government can do better in protecting user data.”&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;AT&amp;amp;T, T-Mobile and Verizon have &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2018/06/19/technology/verizon-att-cellphone-tracking.html?module=inline&quot; title=&quot;&quot;&gt;pledged to stop selling location data to brokers&lt;/a&gt; who may sell the information on the black market. But data on users’ whereabouts is lucrative: Location-targeted advertising sales &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/interactive/2018/12/10/business/location-data-privacy-apps.html?module=inline&quot; title=&quot;&quot;&gt;brought in an estimated $21 billion&lt;/a&gt; last year.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;

&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Senator Kamala Harris of California, a Democrat seeking her party’s nomination for president, has called on Congress to pass laws ending the sale of location data.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;Mr. Brannan said the failure of cellular companies to police themselves was all the more reason for cities to step up.&lt;/p&gt;
&lt;p class=&quot;css-exrw3m evys1bk0&quot;&gt;“Why should we trust them?” he said. “Once municipalities know they have this tool in the arsenal, hopefully it will have a chain reaction.”&lt;/p&gt;
&lt;p class=&quot;css-jwz2nf etfikam0&quot;&gt;Natasha Singer and Jennifer Valentino-DeVries contributed reporting.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Tue, 23 Jul 2019 13:40:08 +0000</pubDate>
<dc:creator>johnny313</dc:creator>
<og:url>https://www.nytimes.com/2019/07/23/nyregion/cellphone-tracking-location-data.html</og:url>
<og:type>article</og:type>
<og:title>New York City to Consider Banning Sale of Cellphone Location Data</og:title>
<og:image>https://static01.nyt.com/images/2019/07/23/nyregion/23nyprivacy1/23nyprivacy1-facebookJumbo.jpg</og:image>
<og:description>A bill would make it illegal for cellphone companies and mobile apps to share user location information collected in the city without a customer’s explicit permission.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2019/07/23/nyregion/cellphone-tracking-location-data.html</dc:identifier>
</item>
<item>
<title>I Took Apple To Court And Won, Twice</title>
<link>https://www.reddit.com/r/apple/comments/cgi7ro/i_took_apple_to_court_and_won_twice_water_damaged/</link>
<guid isPermaLink="true" >https://www.reddit.com/r/apple/comments/cgi7ro/i_took_apple_to_court_and_won_twice_water_damaged/</guid>
<description>&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;&lt;strong class=&quot;_12FoOEddL7j_RgMQN0SNeU&quot;&gt;First Court Case: Water 'Damaged' MacBook Pro&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;So I've just read this article on Reddit (&lt;a href=&quot;https://www.reddit.com/r/apple/comments/aiknf4/i_fought_apple_and_won/&quot; class=&quot;_3t5uN8xUmg0TOwRCOGQEcU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;I Fought Apple and Won&lt;/a&gt;) and thought I'd share my victory as well. It was the end of 2017 when my 13&quot; Macbook Pro started to act strange. With just 1 month remaining of the default 1-year warranty, the screen began to peel off. It was in the middle of my PhD and needed to do a lot of reading and writing and my eyes where getting tired very fast while trying to distinguish between L, l, i, I, o, O, and 0, and so on.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Went online, did a quick search and discovered that's a common issue with specific models. Also, it seemed that Apple was aware of the issue and had a free replacement program in place. A program that covered all devices, in or out of warranty. Happy to see that I've booked an appt with Apple and they had a next day slot at Apple Milton Keynes, UK.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;At the store, I've explained to the genius tech that while the peeling off is affecting the aesthetic of the device, it only bothers me as it prevents me from using it, especially now when I need to read a lot. The agent visually inspected the device, took notes, checked the SN, and finally agreed that it is an issue that Apple has to fix for free, as part of their own recall program.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Anyway, he said, &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;'your device was under warranty, covered by our program, and even covered by consumer law here in the UK'.&lt;/em&gt; So he happily booked it in for a free screen replacement. All seemed too good to be true, given my past repair experience with Apple, but little I knew what was going to come next.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Two hours later, an email arrived from Apple MK store. It was detailing the repair and the overall total cost. I initially ignored the email thinking that the cost shown there must be for their own record. At the end of the day, I concluded, 'my screen replacement was already checked and approved; free, under warranty; as cover by their own program'.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;However, in the back of my mind, I was still bothered by that email...&lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;'Why was there a mention of a logic board replacement?'.&lt;/em&gt; So I called the store.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;The guy on the phone read back a part of the engineer's notes to me: &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;'the sensors show extensive water damage'.&lt;/em&gt; Therefore, he said, &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;'the logic board must be replaced before they exchange the screen'&lt;/em&gt;. I've politely refused the 'offer' to pay for a new logic board replacement, explaining that it must be a mistake somewhere there:&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;a) To start with, my MacBook was working fine when I dropped it off to the repair store. There was no issue with the logic board, only the screen. It was peeling off. I do not need or want a logic board replacement, just the screen cleaned, fixed, replaced, whatever they have to do so I can see better. His answer was that no matter the reason I've brought it in for, they have to replace the logic board.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;b) Second, I said, my Macbook Pro has never been in contact with water. I take great care of my items, and my MacBook has always been in a protective enclosure, in a bag, so where does the water come from? Sadly, he couldn't tell what is damaged (as the Macbook was still working fine) or where it could have possibly come from, knowing I've never got it in contact with water.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;c) Third, I asked about the applicability of Consumer Law here in the UK. His reply was that &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;&lt;strong class=&quot;_12FoOEddL7j_RgMQN0SNeU&quot;&gt;''WATER DAMAGE' invalidates anything: Remaining warranty, Consumer Laws, etc.'&lt;/strong&gt;&lt;/em&gt; Therefore, they won't touch the screen unless I pay to have the logic board replaced as well.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;I refused and arranged to collect my device, and from this point on, things couldn't get stranger. After I've collected my Macbook Pro, out of curiosity, I went online trying to find more about how Apple devices get water damaged without ever being in contact with water.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;I've discovered that what Apple calls sensor is in fact, a piece of paper (officially called Liquid Contact Indicators) that changes colour from white to red in the presence of humidity. I put emphasis on that: in the presence of moisture.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;There's no need for &lt;strong class=&quot;_12FoOEddL7j_RgMQN0SNeU&quot;&gt;LIQUID CONTACT, or WATER DAMAGE&lt;/strong&gt; to trigger the 'sensor' but just humidity, caused for example, by taking the laptop from a well-heated room outside, in the winter.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;So took it to a local repair centre to get a quote for the screen repair, and a full diagnostic that would help me understand more about that water damage. At best, I could learn from this mistake and make sure I don't destroy any MacBooks I might buy in the future.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Two days later, I get a call from the repair centre with a quote and a full report. The report reads: &lt;strong class=&quot;_12FoOEddL7j_RgMQN0SNeU&quot;&gt;&quot;In perfect working order...impeccable outside/inside...one humidity sensor triggered...no water damage...&quot;&lt;/strong&gt;&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Advised by the engineer from the independent repair centre, I decided to leave the device untouched and take Apple to court. Called the Milton Keynes Apple store, explained the findings of the independent assessor and my intention to take them to court, only to be told &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;&quot;good luck'&lt;/em&gt; by the store manager.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;And so I did. Took Apple Milton Keynes store to court (&lt;a href=&quot;https://www.gov.uk/make-money-claim&quot; class=&quot;_3t5uN8xUmg0TOwRCOGQEcU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.gov.uk/make-money-claim&lt;/a&gt;) and won. To cut the story short, it took a year and a half, lots of court fees (received them all back after I won the case - keep all receipts). There were endless email exchanges containing lots of subtle threatenings from Apple's lawyers. There were lots of dirty tactics such as switching court locations forth and back, creating and cancelling mediation appointments, missed phone calls, etc. They've tried everything you can imagine.&lt;/p&gt;

&lt;ul class=&quot;_33MEMislY0GAlB78wL1_CR&quot; readability=&quot;-0.5&quot;&gt;&lt;li class=&quot;_3gqTEjt4x9UIIpWiro7YXz&quot; readability=&quot;2&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Finally, there are some things you have to understand - Apple might/will refuse your repair under these two main excuses: out of warranty, or water damaged.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;_33MEMislY0GAlB78wL1_CR&quot; readability=&quot;0.5&quot;&gt;&lt;li class=&quot;_3gqTEjt4x9UIIpWiro7YXz&quot; readability=&quot;4&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;However, irrespective of what they say, here's what you have to do, step by step, to win against Apple in court:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;ol class=&quot;_1eJr7K139jnMstd4HajqYP&quot; readability=&quot;2.5&quot;&gt;&lt;li class=&quot;_3gqTEjt4x9UIIpWiro7YXz&quot; readability=&quot;1&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Get an independent third party report. If the report findings are different from what Apple's engineers are telling you, proceeded to step two.&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;_3gqTEjt4x9UIIpWiro7YXz&quot; readability=&quot;2&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Contact Apple, talk about the independent diagnostic and ask them to compare reports. Record the calls/emails. If they refuse, proceed to step 3.&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;_3gqTEjt4x9UIIpWiro7YXz&quot; readability=&quot;-1&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Go online and open a small claim court.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Here: &lt;a href=&quot;https://www.gov.uk/make-money-claim&quot; class=&quot;_3t5uN8xUmg0TOwRCOGQEcU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;https://www.gov.uk/make-money-claim&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;It is very straight forward; describe the issue, decided on the compensation, pay the court fee, and follow the steps.&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;4. Always reply to all court and Apple demands. Accept and attend all mediations.&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;5. Stick with your guns. Go to court on the date with all proof/pics/call records you have. Apple diagnostic, pics, third party diagnostics, emails, receipts, even parking receipts.&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;6. Share your story as it unfolds here, for others to learn and get better.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;I said, &quot;I Took Apple To Court And Won &lt;strong class=&quot;_12FoOEddL7j_RgMQN0SNeU&quot;&gt;(Twice)&lt;/strong&gt; 'Water Damaged' &amp;amp; Out of Warranty Mackbook Pro&quot; in my title as I am in my second court case with Apple over another MacBook Pro 2017. It is the device I've purchased as a replacement of the one above, with the peeling off screen - as decided not to touch it until the court reaches a verdict. I know, I know...why did you buy another Apple?!! Let's not get there...&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;So this new MacBook Pro 2017 refused to turn on about a month or so ago. As usual, I took it to the Apple store in MK. The diagnostic was a dead logic board, replacement £500. As I've learned from my past experience with Apple, I took my MacBook out of the store, and immediately to an independent assessor.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Their findings were very different. A faulty 'Daughter Board Flex Cable', which was replaced for £15 and the device works again now. However, the engineer from the company that did the assessment and replaced the cable has kindly explained that the problem with my MacBook is serious: Apparently, a large number of Apple Macbook Pro A1708 13 models suffer from a factory fault (logic board-SSD), and Apple has in place a new recall program, and it seems my device qualifies for that particular recall.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;I've called Apple again, explained the situation, that my device works now without having to replace the logic board, and mentioned the recall program. This time I got in touch with a senior advisor from the US, in their attempt to avoid another court case as last time Apple lost a lot of money. I guess we all agree. Wouldn't be easier, faster, and much cheaper to replace a faulty part, especially knowing that it is correct to do so?&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Back to the in-store manager and the US supervisor. While both sounded overly empathetic towards the issue, their answers were quite similar:&lt;/p&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;&lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;&quot;Sorry, we can't help. You can invoke Consumer Law, but it does not affect us as it applies only to vendors. We are a manufacturer, so it does not apply to us&quot;&lt;/em&gt;. Or, &lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;&quot;the recall program does not apply as your device is no longer under warranty&quot;&lt;/em&gt;. And finally, &quot;&lt;em class=&quot;_7s4syPYtk5hfUIjySXcRE&quot;&gt;if you feel you are right, do take us to court.&quot;&lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Interestingly enough, &lt;a href=&quot;https://support.apple.com/13-inch-macbook-pro-solid-state-drive-service&quot; class=&quot;_3t5uN8xUmg0TOwRCOGQEcU&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;the recall states clearly that it applies to all devices:&lt;/a&gt; in or out of warranty, all devices are covered. I don't know if it is Apple's latest policy to mislead customers or the engineers are no longer motivated to care. Don't know what it is, but if you, Apple, don't care, why should we? See you in court, Apple, once again.&lt;/p&gt;

&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Any of you interested to know more, feel free to contact me directly. I'll share with you court files, pics, tips, how much was the payout, and much much more.&lt;/p&gt;
</description>
<pubDate>Tue, 23 Jul 2019 11:22:31 +0000</pubDate>
<dc:creator>jasoneckert</dc:creator>
<og:title>r/apple - I Took Apple To Court And Won (Twice) 'Water Damaged' &amp; Out of Warranty Mackbook Pro</og:title>
<og:type>website</og:type>
<og:url>https://www.reddit.com/r/apple/comments/cgi7ro/i_took_apple_to_court_and_won_twice_water_damaged/</og:url>
<og:description>986 votes and 192 comments so far on Reddit</og:description>
<og:image>https://external-preview.redd.it/owIdxmX3vdXXEXyLV7srIXxYHA-9T_XTIG2ca85purE.jpg?auto=webp&amp;s=2b4a4d597122e57106b1980e2210624e090a7d39</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reddit.com/r/apple/comments/cgi7ro/i_took_apple_to_court_and_won_twice_water_damaged/</dc:identifier>
</item>
<item>
<title>I&amp;#039;m writing a book about algorithms and Lisp</title>
<link>https://lisp-univ-etc.blogspot.com/2019/07/programming-algorithms-book.html</link>
<guid isPermaLink="true" >https://lisp-univ-etc.blogspot.com/2019/07/programming-algorithms-book.html</guid>
<description>&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-xmwP2e5QeSA/XTXJQd0BTFI/AAAAAAAACG4/FAlIVeffx1M524S7ANdlbwu7Pg4DxK74QCLcBGAs/s1600/cover3.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;1440&quot; data-original-width=&quot;960&quot; height=&quot;320&quot; src=&quot;https://2.bp.blogspot.com/-xmwP2e5QeSA/XTXJQd0BTFI/AAAAAAAACG4/FAlIVeffx1M524S7ANdlbwu7Pg4DxK74QCLcBGAs/s320/cover3.png&quot; width=&quot;213&quot;/&gt;&lt;/a&gt;&lt;br/&gt;Drago — a nice example of a real-world binary tree&lt;/div&gt;
&lt;p&gt;I'm writing a book about algorithms and Lisp. It, actually, started several years ago, but as I experience a constant shortage of quality time to devote to such side activities, short periods of writing alternated with long pauses. Now, I'm, finally, at the stage when I can start publishing it. But I intend to do that, first, gradually in this blog and then put the final version — hopefully, improved and polished thanks to the comments of the first readers — on Leanpub. The book will be freely available with a &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot;&gt;CC BY-NC-ND license&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The book will have 16 chapters grouped into 3 parts: essential data structures, derivative ones, and advanced algorithms. I plan to publish each one approximately once a week. To finish the process by the end of the year.&lt;/p&gt;
&lt;p&gt;I hope the book turns out to be an enlightening read for those who start their career in programming or want to level up in it. At least, I tried to accumulate inside all my experience from production algorithmic development, teaching of these topics, and Lisp programming, over the last 10+ years. Below is a short preface and an introductory chapter about Complexity.&lt;/p&gt;
&lt;h2&gt;Why Algorithms Matter&lt;/h2&gt;
&lt;p&gt;In our industry, currently, there seems to prevail a certain misunderstanding of the importance of algorithms for the working programmer. There's often a disconnect between the algorithmic questions posed at the job interviews and the everyday essence of the same job. That's why &lt;a href=&quot;http://nathanmarz.com/blog/the-limited-value-of-a-computer-science-education.html&quot;&gt;opinions&lt;/a&gt; are &lt;a href=&quot;https://news.ycombinator.com/item?id=9695102&quot;&gt;voiced&lt;/a&gt; that you, actually, don't have to know CS to be successful in the software developer's job. That's true, you don't, but you'd better do if you want to be in the notorious top 10% programmers. For several reasons. One is that, actually, you can find room for algorithms almost at every corner of your work — provided you are aware of their existence. To put it simply, the fact that you don't know a more efficient or elegant solution to a particular programming problem doesn't make your code less crappy. The current trend in software development is that, although the hardware becomes more performant, the software becomes slower faster. There are two reasons for that, in my humble opinion:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Most of the application programmers don't know the inner workings of the underlying platforms. And the number of platform layers keeps increasing.&lt;/li&gt;
&lt;li&gt;Most of the programmers also don't know enough algorithms and algorithmic development technics to squeeze the most from their code. And often this means a loss of one or more orders of magnitude of performance.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In the book, I'll address, primarily, the second issue but will also try to touch on the first whenever possible.&lt;/p&gt;
&lt;p&gt;Besides, learning the art of solving difficult algorithmic problems trains the brain and makes it more apt to solving various other problems, in the course of your day-to-day work.&lt;/p&gt;
&lt;p&gt;Finally, you will be speaking the same lingua franca as other advanced programmers — the tongue that transcends the mundane differences of particular programming languages. And you'll gain a more detached view of those differences, freeing your mind from the dictate of a particular set of choices exhibiting in any one of them.&lt;/p&gt;
&lt;p&gt;One of the reasons for this gap of understanding of the value of algorithms, probably, originates from how they are usually presented in the computer science curriculum. First, it is often done in a rather theoretical or &quot;mathematical&quot; way with rigorous proofs and lack of connection to the real world™. Second, the audience is usually freshmen or sophomores who don't have a lot of practical programming experience and thus can't appreciate and relate how this knowledge may be applied to their own programming challenges (because they didn't have those yet) — rather, most of them are still at the level of struggling to learn well their first programming language and, in their understanding of computing, are very much tied to its choices and idiosyncrasies.&lt;/p&gt;
&lt;p&gt;In this book, the emphasis is made on the demonstration of the use of the described data structures and algorithms in various areas of computer programming. Moreover, I anticipate that the self-selected audience will comprise programmers with some experience in the field. This makes a significant difference in the set of topics that are relevant and how they can be conveyed. Another thing that helps a lot is when the programmer has a good command of more than one programming language, especially, if the languages are from different paradigms: static and dynamic, object-oriented and functional. These factors allow bridging the gap between &quot;theoretical&quot; algorithms and practical coding, making the topic accessible, interesting, and inspiring.&lt;/p&gt;
&lt;p&gt;This is one answer to a possible question: why write another book on algorithms? Indeed, there are several good textbooks and online courses on the topic, of which I'd recommend the most Steven Skienna's &lt;a href=&quot;http://www.algorist.com/&quot;&gt;The Algorithm Design Manual&lt;/a&gt;. Yet, as I said, this book is not at all academic in presentation of the material, which is a norm for other textbooks. Except for simple arithmetic, it contains almost no &quot;math&quot; or proofs. And, although proper attention is devoted to algorithm complexity, it doesn't deal with theories of complexity or computation and similar scientific topics. Besides, all the algorithms and data structures come with some example practical use cases. Last, but not least, there's no book on algorithms in Lisp, and, in my opinion, it's a great topic to introduce the language. The next chapter will provide a crash course to grasp the basic ideas, and then we'll discuss various Lisp programming approaches alongside the algorithms they will be used to implement.&lt;/p&gt;
&lt;p&gt;This is an introductory book, not a bible of algorithms. It will draw a comprehensive picture and cover all topics necessary for further advancement of your algorithms knowledge. However, it won't go too deep into the advanced topics, such as persistent or probabilistic data structures, advanced tree, graph, and optimization algorithms, as well as algorithms for particular fields, such as Machine Learning, Cryptography or Computational Geometry. All of those fields require (and usually have) separate books of their own.&lt;/p&gt;
&lt;h2&gt;A Few Words about Lisp&lt;/h2&gt;
For a long time, I've been contemplating writing an introductory book on Lisp, but something didn't add up, I couldn't see the coherent picture, in my mind. And then I got a chance to teach algorithms with Lisp. From my point of view, it's a perfect fit for demonstrating data structures and algorithms (with a caveat that students should be willing to learn it), while discussing the practical aspects of those algorithms allows to explain the language naturally. At the same time, this topic requires almost no endeavor into the adjacent areas of programming, such as architecture and program design, integration with other systems, user interface, and use of advanced language features, such as types or macros. And that is great because those topics are overkill for an introductory text and they are also addressed nicely and in great detail elsewhere (see &lt;a href=&quot;http://www.gigamonkeys.com/book/&quot;&gt;Practical Common Lisp&lt;/a&gt; and &lt;a href=&quot;http://www.paulgraham.com/acl.html&quot;&gt;ANSI Common Lisp&lt;/a&gt;).
&lt;p&gt;Why Lisp is great for algorithmic programs? One reason is that the language was created with such use case in mind. It has support for all the proper basic data structures, such as arrays, hash-tables, linked lists, strings, and tuples. It also has a numeric tower, which means no overflow errors and, so, a much saner math. Next, it's created for the interactive development style, so the experimentation cycle is very short, there's no compile-wait-run-revise red tape, and there are no unnecessary constraints, like the need for additional annotations (a.k.a. types), prohibition of variable mutation or other stuff like that. You just write a function in the REPL, run it and see the results. In my experience, Lisp programs look almost like pseudocode. Compared to other languages, they may be slightly more verbose at times but are much more clear, simple, and directly compatible with the algorithm's logical representation.&lt;/p&gt;
&lt;p&gt;But why not choose a popular programming language? The short answer is that it wouldn't have been optimal. There are 4 potential mainstream languages that could be considered for this book: C++, Java, Python, and JavaScript. (Surely, there's already enough material on algorithms that uses them). The first two are statically-typed, which is, in itself, a big obstacle to using them as teaching languages. Java is also too verbose, while C++ — too low-level. These qualities don't prevent them from being used in the majority of production algorithm code, in the wild, and you'll, probably, end up dealing with such code sooner than later if not already. Besides, their standard libraries provide great examples of practical algorithm implementation. But, I believe that gaining good conceptual understanding will allow to easily adapt to one of these languages if necessary while learning them in parallel with diving into algorithms creates unnecessary complexity. Python and JS are, in many ways, the opposite choices: they are dynamic and provide some level of an interactive experience (albeit inferior compared to Lisp), but those languages are in many ways anti-algorithmic. Trying to be simple and accessible, they hide too much from the programmer and don't give enough control of the concrete data. Teaching algorithms, using their standard libraries, seems like cheating to me as their basic data structures often are not what they claim to be. Lisp is in the middle: it is both highly interactive and gives enough control of the environment, while not being too verbose and demanding. And the price to pay — the unfamiliar syntax — is really small, in my humble opinion.&lt;/p&gt;
&lt;p&gt;Yet, there's another language that is rapidly gaining popularity and is considered by many to be a good choice for algorithmic development — Rust. It's also a static language, although not so ceremonial as Java or C++. However, neither am I an expert in Rust, nor intend to become one. Moreover, I think the same general considerations regarding static languages apply to it.&lt;/p&gt;
&lt;h2&gt;Algorithmic Complexity&lt;/h2&gt;
&lt;p&gt;Complexity is a point that will be mentioned literally on every page of this book; the discussion of any algorithm or data structure can't avoid this topic. After correctness, it is the second most important quality of every algorithm — moreover, often correctness alone doesn't matter if complexity is neglected, while the opposite is possible: to compromise correctness somewhat in order to get significantly better complexity. By and large, algorithm theory differs from other subjects of CS in that it concerns not about presenting a working (correct) way to solve some problem but about finding an efficient way to do it. Where efficiency is understood as the minimal (or admissible) number of operations performed and occupied memory space.&lt;/p&gt;
&lt;p&gt;In principle, the complexity of an algorithm is the dependence of the number of operations that will be performed on the size of the input. It is crucial to the computer system's scalability: it may be easy to solve the programming problem for a particular set of inputs, but how will the solution behave if the input is doubled, increased tenfold or million-fold? This is not a theoretical question, and an analysis of any general-purpose algorithm should have a clear answer to it.&lt;/p&gt;
&lt;p&gt;Complexity is a substantial research topic: a whole separate branch of CS — Complexity Theory — exists to study it. Yet, throughout the book, we'll try to utilize the end results of such research without delving deep into rigorous proofs or complex math, especially since, in most of the cases, measuring complexity is a matter of simple counting. Let's look at the following illustrative example:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;(defun mat-max (mat)
  (let (max)
    (dotimes (i (array-dimension mat 0))
      (dotimes (j (array-dimension mat 1))
        (when (or (null max)
                  (&amp;gt; (aref mat i j) max))
          (:= max (aref mat i j)))))
    max))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This function finds the maximum element of a two-dimensional array (matrix):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;CL-USER&amp;gt; (mat-max #2A((1 2 3) (4 5 6)))
6
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What's its complexity? To answer, we can just count the number of operations performed: at each iteration of the inner loop, there are 2 comparisons involving 1 array access, and, sometimes, if the planets align we perform another access for assignment. The inner loop is executed &lt;code&gt;(array-dimension mat 1)&lt;/code&gt; times (let's call it &lt;code&gt;m&lt;/code&gt; where &lt;code&gt;m=3&lt;/code&gt;), and the outer one — &lt;code&gt;(array-dimension mat 0)&lt;/code&gt; (&lt;code&gt;n=2&lt;/code&gt;, in the example). If we sum this all up we'll get: &lt;code&gt;n * m * 4&lt;/code&gt; as an upper limit, for the worst case when each sequent array element is larger then the previous. As a rule of thumb, each loop adds multiplication to the formula, and each sequential block adds a plus sign.&lt;/p&gt;
&lt;p&gt;In this calculation, there are two variables (array dimensions &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;m&lt;/code&gt;) and one constant (the number of operations performed for each array element). There exists a special notation — &lt;strong&gt;Big-O&lt;/strong&gt; — used to simplify the representation of end results of such complexity arithmetic. In it, all constants are reduced to 1, and thus &lt;code&gt;m * 1&lt;/code&gt; becomes just &lt;code&gt;m&lt;/code&gt;, and also since we don't care about individual array dimension differences we can just put &lt;code&gt;n * n&lt;/code&gt; instead of &lt;code&gt;n * m&lt;/code&gt;. With such simplification, we can write down the final complexity result for this function: &lt;code&gt;O(n^2)&lt;/code&gt;. In other words, our algorithm has quadratic complexity (which happens to be a variant of a broader class called &quot;polynomial complexity&quot;) in array dimensions. It means that by increasing the dimensions of our matrix ten times, we'll increase the number of operations of the algorithm 100 times. In this case, however, it may be more natural to be concerned with the dependence of the number of operations on the number of &lt;strong&gt;elements&lt;/strong&gt; of the matrix, not its dimensions. We can observe that &lt;code&gt;n^2&lt;/code&gt; is the actual number of elements, so it can also be written as just &lt;code&gt;n&lt;/code&gt; — if by `n` we mean the number of elements, and then the complexity is linear in the number of elements (&lt;code&gt;O(n)&lt;/code&gt;). As you see, it is crucial to understand what `n` we are talking about!&lt;/p&gt;
&lt;p&gt;There are just a few more things to know about Big-O complexity before we can start using it to analyze our algorithms.&lt;/p&gt;
&lt;p&gt;1. There are 6 major complexity classes of algorithms:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;constant-time (&lt;code&gt;O(1)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;sublinear (usually, logarithmic — &lt;code&gt;O(log n)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;linear (&lt;code&gt;O(n)&lt;/code&gt;) and superlinear (&lt;code&gt;O(n * log n)&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;higher-order polynomial (&lt;code&gt;O(n^c)&lt;/code&gt;, where &lt;code&gt;c&lt;/code&gt; is some constant greater than 1)&lt;/li&gt;
&lt;li&gt;exponential (&lt;code&gt;O(с^n)&lt;/code&gt;, where &lt;code&gt;с&lt;/code&gt; is usually 2 but, at least, greater than 1)&lt;/li&gt;
&lt;li&gt;and just plain lunatic complex (&lt;code&gt;O(n!)&lt;/code&gt; and so forth) — I call them &lt;code&gt;O(mg)&lt;/code&gt;, jokingly&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Each class is a step-function change in performance, especially, at scale. We'll talk about each one of them as we'll be discussing the particular examples of algorithms falling into it.&lt;/p&gt;
&lt;p&gt;2. Worst-case vs. average-case behavior. In this example, we saw that there may be two counts of operations: for the average case, we can assume that approximately half of the iterations will require assignment (which results in 3,5 operations in each inner loop), and, for the worst case, the number will be exactly 4. As Big-O reduces all numbers to 1, for this example, the difference is irrelevant, but there may be others, for which it is much more drastic and can't be discarded. Usually, for such algorithms, both complexities should be mentioned (alongside with ways to avoid worst-case scenarios): a good example is quicksort algorithm described in the subsequent chapter.&lt;/p&gt;
&lt;p&gt;3. We have also seen the so-called &quot;constant factors hidden by the Big-O notation&quot;. I.e., from the point of view of algorithm complexity, it doesn't matter if we need to perform 3 operations in the inner loop or 30. Yet, it is quite important in practice, and we'll also discuss it below when examining binary search. Even more, some algorithms with better theoretical complexity may be worse in many practical applications due to these hidden factors (for example, until the dataset reaches a certain size).&lt;/p&gt;
&lt;p&gt;4. Finally, besides execution time complexity, there's also space complexity, which instead of the number of operations measures the amount of storage space used proportional to the size of the input. In general, similar approaches are applied to its estimation.&lt;/p&gt;

</description>
<pubDate>Tue, 23 Jul 2019 09:01:52 +0000</pubDate>
<dc:creator>ska80</dc:creator>
<og:url>http://lisp-univ-etc.blogspot.com/2019/07/programming-algorithms-book.html</og:url>
<og:title>&quot;Programming Algorithms&quot; Book</og:title>
<og:description>Drago — a nice example of a real-world binary tree I'm writing a book about algorithms and Lisp. It, actually, started several years ago,...</og:description>
<og:image>https://2.bp.blogspot.com/-xmwP2e5QeSA/XTXJQd0BTFI/AAAAAAAACG4/FAlIVeffx1M524S7ANdlbwu7Pg4DxK74QCLcBGAs/w1200-h630-p-k-no-nu/cover3.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://lisp-univ-etc.blogspot.com/2019/07/programming-algorithms-book.html</dc:identifier>
</item>
<item>
<title>Prototype: Puppeteer for Firefox</title>
<link>https://github.com/GoogleChrome/puppeteer/tree/master/experimental/puppeteer-firefox</link>
<guid isPermaLink="true" >https://github.com/GoogleChrome/puppeteer/tree/master/experimental/puppeteer-firefox</guid>
<description>&lt;div class=&quot;Box-body&quot;&gt;
&lt;article class=&quot;markdown-body entry-content p-5&quot; itemprop=&quot;text&quot;&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://user-images.githubusercontent.com/39191/49555713-a07b3c00-f8b5-11e8-8aba-f2d03cd83da5.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/39191/49555713-a07b3c00-f8b5-11e8-8aba-f2d03cd83da5.png&quot; height=&quot;200&quot; align=&quot;right&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Use Puppeteer's API with Firefox&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;⚠️ BEWARE&lt;/strong&gt;: Experimental. Just for preview. Installation and usage will change.&lt;/p&gt;
&lt;p&gt;This project is a feasibility prototype to guide the work of implementing Puppeteer endpoints into Firefox's code base. Mozilla's &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1545057&quot; rel=&quot;nofollow&quot;&gt;bug 1545057&lt;/a&gt; tracks the initial milestone, which will be based on a CDP-based &lt;a href=&quot;https://wiki.mozilla.org/Remote&quot; rel=&quot;nofollow&quot;&gt;remote protocol&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;h3&gt;Installation&lt;/h3&gt;
&lt;p&gt;To try out Puppeteer with Firefox in your project, run:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
npm i puppeteer-firefox
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; or &quot;yarn add puppeteer-firefox&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note: When you install puppeteer-firefox, it downloads a &lt;a href=&quot;https://github.com/puppeteer/juggler&quot;&gt;custom-built Firefox&lt;/a&gt; (Firefox/63.0.4) that is guaranteed to work with the API.&lt;/p&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; - navigating to &lt;a href=&quot;https://example.com&quot; rel=&quot;nofollow&quot;&gt;https://example.com&lt;/a&gt; and saving a screenshot as &lt;em&gt;example.png&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;Save file as &lt;strong&gt;example.js&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-js&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;pptrFirefox&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;require&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;puppeteer-firefox&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;);

(&lt;span class=&quot;pl-k&quot;&gt;async&lt;/span&gt; () &lt;span class=&quot;pl-k&quot;&gt;=&amp;gt;&lt;/span&gt; {
  &lt;span class=&quot;pl-k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;browser&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;pl-smi&quot;&gt;pptrFirefox&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;launch&lt;/span&gt;();
  &lt;span class=&quot;pl-k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;page&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;pl-smi&quot;&gt;browser&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;newPage&lt;/span&gt;();
  &lt;span class=&quot;pl-k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;pl-smi&quot;&gt;page&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;goto&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;https://example.com&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;);
  &lt;span class=&quot;pl-k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;pl-smi&quot;&gt;page&lt;/span&gt;.&lt;span class=&quot;pl-en&quot;&gt;screenshot&lt;/span&gt;({path&lt;span class=&quot;pl-k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;example.png&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;});
  &lt;span class=&quot;pl-k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;pl-smi&quot;&gt;browser&lt;/span&gt;.&lt;span class=&quot;pl-c1&quot;&gt;close&lt;/span&gt;();
})();
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Execute script on the command line&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
node example.js
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;API Status&lt;/h3&gt;
&lt;p&gt;Current tip-of-tree status of Puppeteer-Firefox is availabe at &lt;a href=&quot;https://aslushnikov.github.io/ispuppeteerfirefoxready/&quot; rel=&quot;nofollow&quot;&gt;isPuppeteerFirefoxReady?&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Credits&lt;/h3&gt;
&lt;p&gt;Special thanks to &lt;a href=&quot;https://bitbucket.org/aminerop/&quot; rel=&quot;nofollow&quot;&gt;Amine Bouhlali&lt;/a&gt; who volunteered the &lt;a href=&quot;https://www.npmjs.com/package/puppeteer-firefox&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;puppeteer-firefox&lt;/code&gt;&lt;/a&gt; NPM package.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Tue, 23 Jul 2019 06:52:05 +0000</pubDate>
<dc:creator>amjd</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/1778935?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>GoogleChrome/puppeteer</og:title>
<og:url>https://github.com/GoogleChrome/puppeteer</og:url>
<og:description>Headless Chrome Node API. Contribute to GoogleChrome/puppeteer development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/GoogleChrome/puppeteer/tree/master/experimental/puppeteer-firefox</dc:identifier>
</item>
<item>
<title>Files Are Fraught with Peril</title>
<link>https://danluu.com/deconstruct-files/</link>
<guid isPermaLink="true" >https://danluu.com/deconstruct-files/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;title&gt;Files are fraught with peril&lt;/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot;/&gt;&lt;link rel=&quot;icon&quot; href=&quot;data:;base64,=&quot;/&gt;&lt;link rel=&quot;prefetch&quot; href=&quot;//danluu.com&quot;/&gt;&lt;link rel=&quot;prefetch&quot; href=&quot;//danluu.com/fsyncgate/&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;868.27213947279&quot;&gt;
&lt;p&gt;&lt;strong&gt;Files are fraught with peril&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;small&gt;&lt;em&gt;This is a psuedo-transcript for a talk given at Deconstruct 2019. &lt;a href=&quot;https://danluu.com/web-bloat/&quot;&gt;To make this accessible for people on slow connections&lt;/a&gt; as well as people using screen readers, the slides have been replaced by in-line text (the talk has ~120 slides; at an average of 20 kB per slide, that's 2.4 MB. If you think that's trivial, consider that &lt;a href=&quot;https://blogs.microsoft.com/on-the-issues/2019/04/08/its-time-for-a-new-approach-for-mapping-broadband-data-to-better-serve-americans/&quot;&gt;half of Americans still aren't on broadband&lt;/a&gt; and the situation is much worse in developing countries.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Let's talk about files! Most developers seem to think that files are easy. Just for example, let's take a look at the top reddit r/programming comments from when Dropbox announced that they were only going to support ext4 on Linux (the most widely used Linux filesystem). For people not familiar with reddit r/programming, I suspect r/programming is the most widely read English langauge programming forum in the world.&lt;/p&gt;
&lt;p&gt;The top comment reads:&lt;/p&gt;
&lt;blockquote readability=&quot;12&quot;&gt;
&lt;p&gt;I'm a bit confused, why do these applications have to support these file systems directly? Doesn't the kernel itself abstract away from having to know the lower level details of how the files themselves are stored?&lt;/p&gt;
&lt;p&gt;The only differences I could possibly see between different file systems are file size limitations and permissions, but aren't most modern file systems about on par with each other?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The #2 comment (and the top replies going two levels down) are:&lt;/p&gt;
&lt;blockquote readability=&quot;27&quot;&gt;
&lt;p&gt;#2: Why does an application care what the filesystem is?&lt;/p&gt;
&lt;p&gt;#2: Shouldn't that be abstracted as far as &quot;normal apps&quot; are concerned by the OS?&lt;/p&gt;
&lt;p&gt;Reply: It's a leaky abstraction. I'm willing to bet each different FS has its own bugs and its own FS specific fixes in the dropbox codebase. More FS's means more testing to make sure everything works right . . .&lt;/p&gt;
&lt;p&gt;2nd level reply: What are you talking about? This is a dropbox, what the hell does it need from the FS? There are dozenz of fssync tools, data transfer tools, distributed storage software, and everything works fine with inotify. What the hell does not work for dropbox exactly?&lt;/p&gt;
&lt;p&gt;another 2nd level reply: Sure, but any bugs resulting from should be fixed in the respective abstraction layer, not by re-implementing the whole stack yourself. You shouldn't re-implement unless you don't get the data you need from the abstraction. . . . DropBox implementing FS-specific workarounds and quirks is way overkill. That's like vim providing keyboard-specific workarounds to avoid faulty keypresses. All abstractions are leaky - but if no one those abstractions, nothing will ever get done (and we'd have billions of &quot;operating systems&quot;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this talk, we're going to look at how file systems differ from each other and other issues we might encounter when writing to files. We're going to look at the file &quot;stack&quot; starting at the top with the file API, which we'll see is nearly impossible to use correctly and that supporting multiple filesystems without corrupting data is much harder than supporting a single filesystem; move down to the filesystem, which we'll see has serious bugs that cause data loss and data corruption; and then we'll look at disks and see that disks can easily corrupt data at a rate five million times greater than claimed in vendor datasheets.&lt;/p&gt;
&lt;h3 id=&quot;file-api&quot;&gt;File API&lt;/h3&gt;
&lt;h4 id=&quot;writing-one-file&quot;&gt;Writing one file&lt;/h4&gt;
&lt;p&gt;Let's say we want to write a file safely, so that we don't want to get data corruption. For the purposes of this talk, this means we'd like our write to be &quot;atomic&quot; -- our write should either fully complete, or we should be able to undo the write and end up back where we started. Let's look at an example from Pillai et al., OSDI’14.&lt;/p&gt;
&lt;p&gt;We have a file that contains the text &lt;code&gt;a foo&lt;/code&gt; and we want to overwrite &lt;code&gt;foo&lt;/code&gt; with &lt;code&gt;bar&lt;/code&gt; so we end up with &lt;code&gt;a bar&lt;/code&gt;. We're going to make a number of simplifications. For example, you should probably think of each character we're writing as a sector on disk (or, if you prefer, you can imagine we're using a hypothetical advanced NVM drive). Don't worry if you don't know what that means, I'm just pointing this out to note that this talk is going to contain many simplifications, which I'm not going to call out because we only have twenty-five minutes and the unsimplified version of this talk would probably take about three hours.&lt;/p&gt;
&lt;p&gt;To write, we might use the &lt;code&gt;pwrite&lt;/code&gt; syscall. This is a function provided by the operating system to let us interact with the filesystem. Our invocation of this syscall looks like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;pwrite(
  [file], 
  “bar”, // data to write
  3,     // write 3 bytes
  2)     // at offset 2
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pwrite&lt;/code&gt; takes the file we're going to write, the data we want to write, &lt;code&gt;bar&lt;/code&gt;, the number of bytes we want to write, &lt;code&gt;3&lt;/code&gt;, and the offset where we're going to start writing, &lt;code&gt;2&lt;/code&gt;. If you're used to using a high-level language, like Python, you might be used to an interface that looks different, but underneath the hood, when you write to a file, it's eventually going to result in a syscall like this one, which is what will actually write the data into a file.&lt;/p&gt;
&lt;p&gt;If we just call &lt;code&gt;pwrite&lt;/code&gt; like this, we might succeed and get &lt;code&gt;a bar&lt;/code&gt; in the output, or we might end up doing nothing and getting &lt;code&gt;a foo&lt;/code&gt;, or we might end up with something in between, like &lt;code&gt;a boo&lt;/code&gt;, &lt;code&gt;a bor&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;What's happening here is that we might crash or lose power when we write. Since &lt;code&gt;pwrite&lt;/code&gt; isn't gauranteed to be atomic, if we crash, we can end up with some fraction of the write completing, causing data corruption. One way to avoid this problem is to store an &quot;undo log&quot; that will let us restore corrupted data. Before we're modify the file, we'll make a copy of the data that's going to be modified (into the undo log), then we'll modify the file as normal, and if nothing goes wrong, we'll delete the undo log.&lt;/p&gt;
&lt;p&gt;If we crash while we're writing the undo log, that's fine -- we'll see that the undo log isn't complete and we know that we won't have to restore because we won't have started modifying the file yet. If we crash while we're modifying the file, that's also ok. When we try to restore from the crash, we'll see that the undo log is complete and we can use it to recover from data corruption:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;creat(/d/log) // Create undo log
write(/d/log, &quot;2,3,foo&quot;, 7) // To undo, at offset 2, write 3 bytes, &quot;foo&quot;
pwrite(/d/orig, “bar&quot;, 3, 2) // Modify original file as before
unlink(/d/log) // Delete log file
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If we're using &lt;code&gt;ext3&lt;/code&gt; or &lt;code&gt;ext4&lt;/code&gt;, widely used Linux filesystems, and we're using the mode &lt;code&gt;data=journal&lt;/code&gt; (we'll talk about what these modes mean later), here are some possible outcomes we could get:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;d/log: &quot;2,3,f&quot;
d/orig: &quot;a foo&quot;

d/log: &quot;&quot;
d/orig: &quot;a foo&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It's possible we'll crash while the log file write is in progress and we'll have an incomplete log file. In the first case above, we know that the log file isn't complete because the file says we should start at offset &lt;code&gt;2&lt;/code&gt; and write &lt;code&gt;3&lt;/code&gt; bytes, but only one byte, &lt;code&gt;f&lt;/code&gt;, is specified, so the log file must be incomplete. In the second case above, we can tell the log file is incomplete because the undo log format should start with an offset and a length, but we have neither. Either way, since we know that the log file isn't complete, we know that we don't need to restore.&lt;/p&gt;
&lt;p&gt;Another possible outcome is something like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;d/log: &quot;2,3,foo&quot;
d/orig: &quot;a boo&quot;

d/log: &quot;2,3,foo&quot;
d/orig: &quot;a bar&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the first case, the log file is complete we crashed while writing the file. This is fine, since the log file tells us how to restore to a known good state. In the second case, the write completed, but since the log file hasn't been deleted yet, we'll restore from the log file.&lt;/p&gt;
&lt;p&gt;If we're using &lt;code&gt;ext3&lt;/code&gt; or &lt;code&gt;ext4&lt;/code&gt; with &lt;code&gt;data=ordered&lt;/code&gt;, we might see something like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;d/log: &quot;2,3,fo&quot;
d/orig: &quot;a boo&quot;

d/log: &quot;&quot;
d/orig: &quot;a bor&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With &lt;code&gt;data=ordered&lt;/code&gt;, there's no guarantee that the &lt;code&gt;write&lt;/code&gt; to the log file and the &lt;code&gt;pwrite&lt;/code&gt; that modifies the original file will execute in program order. Instesad, we could get&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;creat(/d/log) // Create undo log
pwrite(/d/orig, “bar&quot;, 3, 2) // Modify file before writing undo log!
write(/d/log, &quot;2,3,foo&quot;, 7) // Write undo log
unlink(/d/log) // Delete log file
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To prevent this re-ordering, we can use another syscall, &lt;code&gt;fsync&lt;/code&gt;. &lt;code&gt;fsync&lt;/code&gt; is a barrier (prevents re-ordering) and it flushes caches (which we'll talk about later).&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;creat(/d/log)
write(/d/log, “2,3,foo”, 7)
fsync(/d/log) // Add fsync to prevent re-ordering
pwrite(/d/orig, “bar”, 3, 2)
fsync(/d/orig) // Add fsync to prevent re-ordering
unlink(/d/log)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This works with &lt;code&gt;ext3&lt;/code&gt; or &lt;code&gt;ext4&lt;/code&gt;, &lt;code&gt;data=ordered&lt;/code&gt;, but if we use &lt;code&gt;data=writeback&lt;/code&gt;, we might see something like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;d/log: &quot;2,3,WAT&quot;
d/orig: &quot;a boo&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Unfortunately, with &lt;code&gt;data=writeback&lt;/code&gt;, the &lt;code&gt;write&lt;/code&gt; to the log file isn't guaranteed to be atomic and the filesystem metadata that tracks the file length can get updated before we've finished writing the log file, which will make it look like the log file contains whatever bits happened to be on disk where the log file was created. Since the log file exists, when we try to restore after a crash, we may end up &quot;restoring&quot; random garbage into the original file. To prevent this, we can add a checksum (a way of making sure the file is actually valid) to the log file.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;creat(/d/log)
write(/d/log,“…[✓∑],foo”,7) // Add checksum to log file to detect incomplete log file
fsync(/d/log)
pwrite(/d/orig, “bar”, 3, 2)
fsync(/d/orig)
unlink(/d/log)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This should work with &lt;code&gt;data=writeback&lt;/code&gt;, but we could still see the following:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;d/orig: &quot;a boo&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There's no log file! Although we created a file, wrote to it, and then fsync'd it. Unfortunately, there's no guarantee that the directory will actually store the location of the file if we crash. In order to make sure we can easily find the file when we restore from a crash, we need to fsync the parent of the newly created log.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;creat(/d/log)
write(/d/log,“…[✓∑],foo”,7)
fsync(/d/log)
fsync(/d) /// fsync parent directory
pwrite(/d/orig, “bar”, 3, 2)
fsync(/d/orig)
unlink(/d/log)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There are a couple more things we should do. We shoud also fsync after we're done (not shown), and we also need to check for errors. These syscalls can return errors and those errors need to be handled appropriately. There's at least one filesystem issue that makes this very difficult, but since that's not an API usage thing per se, we'll look at this again in the &lt;strong&gt;Filesystems&lt;/strong&gt; section.&lt;/p&gt;
&lt;p&gt;We've now seen what we have to do to write a file safely. It might be more complicated than we like, but it seems doable -- if someone asks you to write a file in a self-contained way, like an interview question, and you know the appropriate rules, you can probably do it correctly. But what happens if we have to do this as a day-to-day part of our job, where we'd like to write to files safely every time to write to files in a large codebase.&lt;/p&gt;
&lt;h4 id=&quot;api-in-practice&quot;&gt;API in practice&lt;/h4&gt;
&lt;p&gt;Pillai et al., OSDI’14 looked at a bunch of software that writes to files, including things we'd hope write to files safely, like datbases and version control systems: Leveldb, LMDB, GDBM, HSQLDB, Sqlite, PostgreSQL, Git, Mercurial, HDFS, Zookeeper. They then wrote a static analysis tool that can find incorrect usage of the file API, things like incorrectly assuming that operations that aren't atomic are actually atomic, incorrectly assuming that operations that can be re-ordered will execute in program order, etc.&lt;/p&gt;
&lt;p&gt;When they did this, they found that every single piece of software they tested except for SQLite in one particular mode had at least one bug. This isn't a knock on the developers of this software or the software -- the programmers who work on things like Leveldb, LBDM, etc., know more about filesystems than the vast majority programmers and the software has more rigorous tests than most software. But they still can't use files safely every time! A natural follow-up to this is the question: why the file API so hard to use that even experts make mistakes?&lt;/p&gt;
&lt;h5 id=&quot;concurrent-programming-is-hard&quot;&gt;Concurrent programming is hard&lt;/h5&gt;
&lt;p&gt;There are a number of reasons for this. If you ask people &quot;what are hard problems in programming?&quot;, you'll get answers like distributed systems, concurrent programming, security, aligning things with CSS, dates, etc.&lt;/p&gt;
&lt;p&gt;And if we look at what mistakes cause bugs when people do concurrent programming, we see bugs come from things like &quot;incorrectly assuming operations are atomic&quot; and &quot;incorrectly assuming operations will execute in program order&quot;. These things that make concurrent programming hard also make writing files safely hard -- we saw examples of both of these kinds of bugs in our first example. More generally, many of the same things that make concurrent programming hard are the same things that make writing to files safely hard, so of course we should expect that writing to files is hard!&lt;/p&gt;
&lt;p&gt;Another property writing to files safely shares with concurrent programming is that it's easy to write code that has infrequent, non-deterministc failures. With respect to files, people will sometimes say this makes things easier (&quot;I've never noticed data corruption&quot;, &quot;your data is still mostly there most of the time&quot;, etc.), but if you want to write files safely because you're working on software that shouldn't corrupt data, this makes things more difficult by making it more difficult to tell if your code is really correct.&lt;/p&gt;
&lt;h5 id=&quot;api-inconsistent&quot;&gt;API inconsistent&lt;/h5&gt;
&lt;p&gt;As we saw in our first example, even when using one filesystem, different modes may have significantly different behavior. Large parts of the file API look like this, where behavior varies across filesystems or across different modes of the same filesystem. For example, if we look at mainstream filesystems, appends are atomic, except when using &lt;code&gt;ext3&lt;/code&gt; or &lt;code&gt;ext4&lt;/code&gt; with &lt;code&gt;data=writeback&lt;/code&gt;, or &lt;code&gt;ext2&lt;/code&gt; in any mode and directory operations can't be re-ordered w.r.t. any other operations, except on &lt;code&gt;btrfs&lt;/code&gt;. In theory, we should all read the POSIX spec carefully and make sure all our code is valid according to POSIX, but if they check filesystem behavior at all, people tend to code to what their filesystem does and not some abtract spec.&lt;/p&gt;
&lt;p&gt;If we look at one particular mode of one filesystem (&lt;code&gt;ext4&lt;/code&gt; with &lt;code&gt;data=journal&lt;/code&gt;), that seems relatively possible to handle safely, but when writing for a variety of filesystems, especially when handling filesystems that are very different from &lt;code&gt;ext3&lt;/code&gt; and &lt;code&gt;ext4&lt;/code&gt;, like &lt;code&gt;btrfs&lt;/code&gt;, it becomes very difficult for people to write correct code.&lt;/p&gt;
&lt;h4 id=&quot;docs-unclear&quot;&gt;Docs unclear&lt;/h4&gt;
&lt;p&gt;In our first example, we saw that we can get different behavior from using different &lt;code&gt;data=&lt;/code&gt; modes. If we look at the manpage (manual) on what these modes mean in &lt;code&gt;ext3&lt;/code&gt; or &lt;code&gt;ext4&lt;/code&gt;, we get:&lt;/p&gt;
&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;journal: All data is committed into the journal prior to being written into the main filesystem.&lt;/p&gt;
&lt;p&gt;ordered: This is the default mode. All data is forced directly out to the main file system prior to its metadata being committed to the journal.&lt;/p&gt;
&lt;p&gt;writeback: Data ordering is not preserved – data may be written into the main filesystem after its metadata has been committed to the journal. &lt;strong&gt;This is rumoured to be&lt;/strong&gt; the highest-throughput option. It guarantees internal filesystem integrity, however it can allow old data to appear in files after a crash and journal recovery.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you want to know how to use your filesystem safely, and you don't already know what a journaling filesystem is, this definitely isn't going to help you. If you know what a journaling filesystem is, this will give you some hints but it's still not sufficient. It's theoretically possible to figure everything out from reading the source code, but this is pretty impractical for most people who don't already know how the filesystem works.&lt;/p&gt;
&lt;p&gt;For English-language documentation, there's lwn.net and the Linux kernel mailing list (LKML). LWN is great, but they can't keep up with everything, so you LKML is the place to go if you want something comprehensive. Here's an example of an exchange on LKML about filesystems:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dev 1&lt;/strong&gt;: Personally, I care about metadata consistency, and ext3 documentation suggests that journal protects its integrity. Except that it does not on broken storage devices, and you still need to run fsck there.&lt;br/&gt;&lt;strong&gt;Dev 2&lt;/strong&gt;: as the ext3 authors have stated many times over the years, you still need to run fsck periodically anyway.&lt;br/&gt;&lt;strong&gt;Dev 1&lt;/strong&gt;: Where is that documented?&lt;br/&gt;&lt;strong&gt;Dev 2&lt;/strong&gt;: linux-kernel mailing list archives.&lt;br/&gt;&lt;strong&gt;FS dev&lt;/strong&gt;: Probably from some 6-8 years ago, in e-mail postings that I made.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;While the filesystem developers tend to be helpful and they write up informative responses, most people probably don't keep up with the past 6-8 years of LKML.&lt;/p&gt;
&lt;h4 id=&quot;performance-correctness-conflict&quot;&gt;Performance / correctness conflict&lt;/h4&gt;
&lt;p&gt;Another issue is that the file API has an inherent conflict between performance and correctness. We noted before that &lt;code&gt;fsync&lt;/code&gt; is a barrier (which we can use to enforce ordering) and that it flushes caches. If you've ever worked on the design of a high-performance cache, like a microprocessor cache, you'll probably find the bundling of these two things into a single primitive to be unusual. A reason this is unusual is that flushing caches has a significant performance cost and there are many cases where we want to enforce ordering without paying this performance cost. Bundling these two things into a single primitive forces us to pay the cache flush cost when we only care about ordering.&lt;/p&gt;
&lt;p&gt;Chidambaram et al., SOSP’13 looked at the performance cost of this by modifying &lt;code&gt;ext4&lt;/code&gt; to add a barrier mechanism that doesn't flush caches and they found that, if they modified software appropriately and used their barrier operation where a full &lt;code&gt;fsync&lt;/code&gt; wasn't necessary, they were able to achieve performance roughly equivalent to &lt;code&gt;ext4&lt;/code&gt; with cache flushing entirely disabled (which is unsafe and can lead to data corruption) without sacrificing safety. However, making your own filesystem and getting it adopted is impractical for most people writing user-level software. Some databases will bypass the filesystem entirely or almost entirely, but this is also impractical for most software.&lt;/p&gt;
&lt;p&gt;That's the file API. Now that we've seen that it's extraordinarily difficult to use, let's look at filesystems.&lt;/p&gt;
&lt;h3 id=&quot;filesystem&quot;&gt;Filesystem&lt;/h3&gt;
&lt;p&gt;If we want to make sure that filessytems work, one of the most basic tests we could do is to inject errors are the layer below the filesystem to see if the filesystem handles them properly. For example, on a write, we could have the disk fail to write the data and return the appropriate error. If the filesystem drops this error or doesn't handle ths properly, that means we have data loss or data corruption. This is analogous to the kinds of distributed systems faults Kyle Kingsbury talked about in his distributed systems testing talk yesterday (although these kinds of errors are much more straightforward to test).&lt;/p&gt;
&lt;p&gt;Prabhakaran et al., SOSP’05 did this and found that, for most filesystems tested, almost all write errors were dropped. The major exception to this was on ReiserFS, which did a pretty good job with all types of errors tested, but ReiserFS isn't really used today for reasons beyond the scope of this talk.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://danluu.com/filesystem-errors/&quot;&gt;We (Wesley Aptekar-Cassels and I) looked at this again in 2017&lt;/a&gt; and found that things had improved significantly. Most filesystems (other than JFS) could pass these very basic tests on error handling.&lt;/p&gt;
&lt;p&gt;Another way to look for errors is to look at filesystems code to see if it handles internal errors correctly. Gunawai et al., FAST’08 did this and found that internal errors were dropped a significant percentage of the time. The technique they used made it difficult to tell if functions that could return many different errors were correctly handling each error, so they also looked at calls to functions that can only return a single error. In those cases, depending on the function, errors were dropped rouhgly 2/3 to 3/4 of the time, depending on the function.&lt;/p&gt;
&lt;p&gt;Wesley and I also looked at this again in 2017 and found significant improvement -- errors for the same functions Gunawi et al. looked at were &quot;only&quot; ignored 1/3 to 2/3 of the time, depending on the function.&lt;/p&gt;
&lt;p&gt;Gunawai et al. also looked at comments near these dropped errors and found comments like &quot;Just ignore errors at this point. There is nothing we can do except to try to keep going.&quot; (XFS) and &quot;Error, skip block and hope for the best.&quot; (ext3).&lt;/p&gt;
&lt;p&gt;Now we've seen that while filesystems used to drop even the most basic errors, they now handle then correctly, but there are some code paths where errors can get dropped. For a concrete example of a case where this happens, let's look back at our first example. If we get an &lt;a href=&quot;https://lwn.net/Articles/752063/&quot;&gt;error on &lt;code&gt;fsync&lt;/code&gt;&lt;/a&gt;, unless we have a pretty recent Linux kernel (Q2 2018-ish), there's a pretty good chance that the error will be dropped and it may even get reported to the wrong process!&lt;/p&gt;
&lt;p&gt;On recent Linux kernels, there's a good chance the error will be reported (to the correct process, even). Wilcox, PGCon’18 notes that an error on &lt;code&gt;fsync&lt;/code&gt; is basically unrecoverable. The details for depending on filesystem -- on &lt;code&gt;XFS&lt;/code&gt; and &lt;code&gt;btrfs&lt;/code&gt;, modified data that's in the filesystem will get thrown away and there's no way to recover. On &lt;code&gt;ext4&lt;/code&gt;, the data isn't thrown away, but it's marked as unmodified, so the filesystem won't try to write it back to disk later, and if there's memory pressure, the data can be thrown out at any time. If you're feeling adventerous, you can try to recover the data before it gets thrown out with various tricks (e.g., by forcing the filesystem to mark it as modified again, or by writing it out to another device, which will force the filesystem to write the data out even though it's marked as unmodified), but there's no guarantee you'll be able to recover the data before it's thrown out. On Linux &lt;code&gt;ZFS&lt;/code&gt;, it appears that there's a code path designed to do the right thing, but CPU usage spikes and the system may hang or become unusable.&lt;/p&gt;
&lt;p&gt;In general, there isn't a good way to recover from this on Linux. Postgres, MySQL, and MongoDB (widely used databases) will crash themselves and the user is expected to restore from the last checkpoint. Most software will probably just silently lose or corrupt data. And &lt;code&gt;fsync&lt;/code&gt; is a relatively good case -- for example, &lt;code&gt;syncfs&lt;/code&gt; simply doesn't return errors on Linux at all, leading to silent data loss and data corruption.&lt;/p&gt;
&lt;p&gt;BTW, when Craig Ringer first proposed that Postgres should crash on &lt;code&gt;fsync&lt;/code&gt; error, the &lt;a href=&quot;https://danluu.com/fsyncgate/&quot;&gt;first response on the Postgres dev mailing list&lt;/a&gt; was:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;Surely you jest . . . If [current behavior of fsync] is actually the case, we need to push back on this kernel brain damage&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But after talking through the details, everyone agreed that crashing was the only good option. One of the many unfortunate things is that most disk errors are transient. Since the filesystem discards critical information that's necessary to proceed without data corruption on any error, transient errors that could be retried instead force software to take drastic measures.&lt;/p&gt;
&lt;p&gt;And while we've talked about Linux, this isn't unique to Linux. Fsync error handling (and error handling in general) is broken on many different operating systems. At the time Postgres &quot;discovered&quot; the behavior of fsync on Linux, FreeBSD had arguably correct behavior, but OpenBSD and NetBSD behaved the same as Linux (true error status dropped, retrying causes success response, data lost). This has been fixed on OpenBSD and probably some other BSDs, but Linux still basically has the same behavior and you don't have good guarantees that this will work on any random UNIX-like OS.&lt;/p&gt;
&lt;p&gt;Now that we've seen that, for many years, filesystems failed to handle errors in some of the most straightforward and simple cases and that there are cases that still aren't handled correctly today, let's look at disks.&lt;/p&gt;
&lt;h3 id=&quot;disk&quot;&gt;Disk&lt;/h3&gt;
&lt;h4 id=&quot;flushing&quot;&gt;Flushing&lt;/h4&gt;
&lt;p&gt;We've seen that it's easy to not realize we have to call &lt;code&gt;fsync&lt;/code&gt; when we have to call &lt;code&gt;fsync&lt;/code&gt;, and that even if we call &lt;code&gt;fsync&lt;/code&gt; appropriately, bugs may prevent &lt;code&gt;fsync&lt;/code&gt; from actually working. Rajimwale et al., DSN’11 into whether or not disks actually flush when you ask them to flush, assuming everything above the disk works correctly (their paper is actually mostly about something else, they just discuss this briefly at the beginning). Someone from Microsoft anonymously told them &quot;[Some disks] do not allow the file system to force writes to disk properly&quot; and someone from Seagate, a disk manufacturer, told them &quot;[Some disks (though none from us)] do not allow the file system to force writes to disk properly&quot;. Bairavasundaram et al., FAST’07 also found the same thing when they looked into disk reliability.&lt;/p&gt;
&lt;h4 id=&quot;error-rates&quot;&gt;Error rates&lt;/h4&gt;
&lt;p&gt;We've seen that filessytems sometimes don't handle disk errors correctly. If we want to know how serious this issue is, we should look at the rate at which disks emit errors. Disk datasheets will usually an uncorrectable bit error rate of 1e-14 for consuemr HDDs (often called spinning metal or spinning rust disks), 1e-15 for enterprise HDDs, 1e-15 for consumer SSDs, and 1e-16 for enterprise SSDs. This means that, on average, we expect to see one unrecoverable data error every 1e14 bits we read on an HDD.&lt;/p&gt;
&lt;p&gt;To get an intuition for what this means in practice, 1TB is now a pretty normal disk size. If we read a full drive once, that's 1e12 bytes, or almost 1e13 bits (technically 8e12 bits), which means we should expect to see one unrecoverable error every if we buy a 1TB HDD and read the entire disk ten-ish times. Nowadays, we can buy 10TB HDDs, in which case we'd expect to see an error (technically, 8/10th errors) on every read of an entire consumer HDD.&lt;/p&gt;
&lt;p&gt;In practice, observed data rates are are significantly higher. Narayanan et al., SYSTOR’16 (Microsoft) observed SSD error rates from 1e-11 to 6e-14, depending on the drive model. Meza et al., SIGMETRICS’15 (FB) observed even worse SSD error rates, 2e-9 to 6e-11 depending on the model of drive. Depending on the type of drive, 2e-9 is 2 gigabits, or 250 MB, 500 thousand to 5 million times worse than stated on datasheets depending on the class of drive.&lt;/p&gt;
&lt;p&gt;Bit error rate is arguably a bad metric for disk drives, but this is the metric disk vendors claim, so that's what we have to compare against if we want an apples-to-apples comparison. See Bairavasundaram et al., SIGMETRICS'07, Schroeder et al., FAST'16, and others for other kinds of error rates.&lt;/p&gt;
&lt;p&gt;One thing to note is that it's often claimed that SSDs don't have problems with corruption because they use error correcting codes (ECC), which can fix data corruption issues. &quot;Flash banishes the specter of the unrecoverable data error&quot;, etc. The thing this misses is that modern high-density flash devices are very unreliable and need ECC to be usable at all. Grupp et al., FAST’12 looked at error rates of the kind of flash the underlies SSDs and found errors rates from 1e-1 to 1e-8. 1e-1 is one error every ten bits, 1e-8 is one error every 100 megabits.&lt;/p&gt;
&lt;h4 id=&quot;power-loss&quot;&gt;Power loss&lt;/h4&gt;
&lt;p&gt;Another claim you'll hear is that SSDs are safe against power loss and some types of crashes because they now have &quot;power loss protectection&quot; -- there's some mechanism in the SSDs that can hold power for long enough during an outage that the internal SSD cache can be written out safely.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://lkcl.net/reports/ssd_analysis.html&quot;&gt;Luke Leighton tested this&lt;/a&gt; by buying 6 SSDs that claim to have power loss protection and found that four out of the six models of drive he tested failed (every drive that wasn't an Intel drive). If we look at the details of the tests, when drives fail, it appears to be because they were used in a way that the implementor of power loss protection didn't expect (writing &quot;too fast&quot;, although well under the rate at which the drive is capable of writing, or writing &quot;too many&quot; files in parallel). When a drive advertises that it has power loss protection, this appears to mean that someone spent some amount of effort implementing something that will, under some circumstances, prevent data loss or data corruption under power loss. But, as we saw in Kyle's talk yesterday on distributed systems, if you want to make sure that the mechanism actually works, you can't rely on the vendor to do rigorous or perhaps even any semi-serious testing and you have to test it yourself.&lt;/p&gt;
&lt;h4 id=&quot;retention&quot;&gt;Retention&lt;/h4&gt;
&lt;p&gt;If we look at SSD datasheets, a young-ish drive (one with 90% of its write cycles remaining) will usually be specced to hold data for about ten years after a write. If we look at a worn out drive, one very close to end-of-life, it's specced to retain data for one year to three months, depending on the class of drive. I think people are often surprised to find that it's within spec for a drive to lose data three months after the data is written.&lt;/p&gt;
&lt;p&gt;These numbers all come from datasheets and specs, as we've seen, datasheets can be a bit optimistic. On many early SSDs, using up most or all of a drives write cycles would cause the drive to brick itself, so you wouldn't even get the spec'd three month data retention.&lt;/p&gt;
&lt;h3 id=&quot;corollaries&quot;&gt;Corollaries&lt;/h3&gt;
&lt;p&gt;Now that we've seen that there are significant problems at every level of the file stack, let's look at a couple things that follow from this.&lt;/p&gt;
&lt;h4 id=&quot;what-to-do&quot;&gt;What to do?&lt;/h4&gt;
&lt;p&gt;What we should do about this is a big topic, in the time we have left, one thing we can do instead of writing to files is to use databases. If you want something lightweight and simple that you can use in most places you'd use a file, SQLite is pretty good. I'm not saying you should never use files. There is a tradeoff here. But if you have an application where you'd like to reduce the rate of data corruption, considering using a database to store data instead of using files.&lt;/p&gt;
&lt;h4 id=&quot;fs-support&quot;&gt;FS support&lt;/h4&gt;
&lt;p&gt;At the start of this talk, we looked at this Dropbox example, where most people thought that there was no reason to remove support for most Linux filesystems because filesystems are all the same. I believe their hand was forced by the way they want to store/use data, which they can only do with &lt;code&gt;ext&lt;/code&gt; given how they're doing things (which is arguably a mis-feature), but even if that wasn't the case, perhaps you can see why software that's attempting to sync data to disk reliably and with decent performance might not want to support every single filesystem in the universe for an OS that, for their product, is relatively niche. Maybe it's worth supporting every filesystem for PR reasons and then going through the contortions necessary to avoid data corruption on a per-filesystem basis (you can try coding straight to your reading of the POSIX spec, but as we've seen, that won't save you on Linux), but the PR problem is caused by a misunderstanding.&lt;/p&gt;
&lt;p&gt;The other comment we looked at on reddit, and also a common sentiment, is that it's not a program's job to work around bugs in libraries or the OS. But user data gets corrupted regardless of who's &quot;fault&quot; the bug is, and as we've seen, bugs can perist in the filesystem layer for many years. In the case of &lt;code&gt;Linux&lt;/code&gt;, most filesystems other than &lt;code&gt;ZFS&lt;/code&gt; seem to have decided it's correct behavior to throw away data on fsync error and also not report that the data can't be written (as opposed to &lt;code&gt;FreeBSD&lt;/code&gt; or &lt;code&gt;OpenBSD&lt;/code&gt;, where most filesystems will at least report an error on subsequent &lt;code&gt;fsync&lt;/code&gt;s if the error isn't resolved). This is arguably a bug and also arguably correct behavior, but either way, if your software doesn't take this into account, you're going to lose or corrupt data. If you want to take the stance that it's not your fault that the filesystem is corrupting data, your users are going to pay the cost for that.&lt;/p&gt;
&lt;h3 id=&quot;faq&quot;&gt;FAQ&lt;/h3&gt;
&lt;p&gt;While putting this talk to together, I read a bunch of different online discussions about how to write to files safely. For discussions outside of specialized communities (e.g., LKML, the Postgres mailing list, etc.), many people will drop by to say something like &quot;why is everyone making this so complicated? You can do this very easily and completely safely with this one weird trick&quot;. Let's look at the most common &quot;one weird trick&quot;s from two thousand internet comments on how to write to disk safely.&lt;/p&gt;
&lt;h4 id=&quot;rename&quot;&gt;Rename&lt;/h4&gt;
&lt;p&gt;The most frequently mentioned trick is to rename instead of overwriting. If you remember our single-file write example, we made a copy of the data that we wanted to overwrite before modifying the file. The trick here is to do the opposite:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Make a copy of the entire file&lt;/li&gt;
&lt;li&gt;Modify the copy&lt;/li&gt;
&lt;li&gt;Rename the copy on top of the original file&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;This trick doesn't work. People seem to think that this is safe becaus the POSIX spec says that &lt;code&gt;rename&lt;/code&gt; is atomic, but that only means &lt;code&gt;rename&lt;/code&gt; is atomic with respect to normal operation, that doesn't mean it's atomic on crash. This isn't just a theoretical problem; if we look at mainstream Linux filesystems, most have at least one mode where rename isn't atomic on crash. Rename also isn't gauranteed to execute in program order, as people sometimes expect.&lt;/p&gt;
&lt;p&gt;The most mainstream exception where rename is atomic on crash is probably &lt;code&gt;btrfs&lt;/code&gt;, but even there, it's a bit subtle -- as noted in Bornholt et al., ASPLOS’16, &lt;code&gt;rename&lt;/code&gt; is only atomic on crash when renaming to replace an existing file, not when renaming to create a new file. Also, Mohan et al., OSDI’18 found numerous rename atomicity bugs on &lt;code&gt;btrfs&lt;/code&gt;, some quite old and some introduced the same year as the paper, so you want not want to rely on this without extensive testing, even if you're writing &lt;code&gt;btrfs&lt;/code&gt; specific code.&lt;/p&gt;
&lt;p&gt;And even if this worked, the performance of this technique is quite poor.&lt;/p&gt;
&lt;h4 id=&quot;append&quot;&gt;Append&lt;/h4&gt;
&lt;p&gt;The second most frequently mentioned trick is to only ever append (instead of sometimes overwriting). This also doesn't work. As noted in Pillai et al., OSDI’14 and Bornholt et al., ASPLOS’16, appends don't gaurantee ordering or atomicity and believing that appends are safe is the cause of some bugs.&lt;/p&gt;
&lt;h4 id=&quot;one-weird-tricks&quot;&gt;One weird tricks&lt;/h4&gt;
&lt;p&gt;We've seen that the most commonly cited simple tricks don't work. Something I find interesting is that, in these discussions, people will drop into a discussion where it's already been explained, often in great detail, why writing to files is harder than someone might naively think, ignore all warnings and explanations and still proceed with their explanation for why it's, in fact, really easy. Even when warned that files are harder than people think, people still think they're easy!&lt;/p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In conclusion, computers don't work (but you probably already know this if you're here at Gary-conf). This talk happened to be about files, but there are many areas we could've looked into where we would've seen similar things.&lt;/p&gt;
&lt;p&gt;One thing I'd like to note before we finish is that, IMO, the underlying problem isn't technical. If you look at what huge tech companies do (companies like FB, Amazon, MS, Google, etc.), they often handle writes to disk pretty safely. They'll make sure that they have disks where power loss protection actually work, they'll have patches into the OS and/or other instrumentation to make sure that errors get reported correctly, there will be large distributed storage groups to make sure data is replicated safely, etc. We know how to make this stuff pretty reliable. It's hard, and it takes a lot of time and effort, i.e., a lot of money, but it can be done.&lt;/p&gt;
&lt;p&gt;If you ask someone who works on that kind of thing why they spend mind boggling sums of money to enure (or really, increase the probability of) correctness, you'll often get an answer like &quot;we have a zillion machines and if you do the math on the rate of data corruption, if we didn't do all of this, we'd have data corruption every minute of every day. It would be totally untenable&quot;. A huge tech company might have, what, order of ten million machines? The funny thing is, if you do the math for how many consumer machines there are out there and much consumer software runs on unreliable disks, the math is similar. There are many more consumer machines; they're typically operated at much lighter load, but there are enough of them that, if you own a widely used piece of desktop/laptop/workstation software, the math on data corruption is pretty similar. Without &quot;extreme&quot; protections, we should expect to see data corruption all the time.&lt;/p&gt;
&lt;p&gt;But if we look at how consumer software works, it's usually quite unsafe with respect to handling data. IMO, the key difference here is that when a huge tech company loses data, whether that's data on who's likely to click on which ads or user emails, the company pays the cost, directly or indirectly and the cost is large enough that it's obviously correct to spend a lot of effort to avoid data loss. But when consumers have data corruption on their own machines, they're mostly not sophisticated enough to know who's at fault, so the company can avoid taking the brunt of the blame. If we have a global optimization function, the math is the same -- of course we should put more effort into protecting data on consumer machines. But if we're a company that's locally optimizing for our own benefit, the math works out differently and maybe it's not worth it to spend a lot of effort on avoiding data corruption.&lt;/p&gt;
&lt;p&gt;Yesterday, Ramsey Nasser gave a talk where he made a very compelling case that something was a serious problem, which was followed up by a comment that his proposed solution will have a hard time getting adoption. I agree with both parts -- he discussed an important problem, and it's not clear how solving that problem will make anyone a lot of money, so the problem is likely to go unsolved.&lt;/p&gt;
&lt;p&gt;With GDPR, we've seen that regulation can force tech companies to protect people's privacy in a way they're not naturally inclined to do, but regulation is a very big hammer and the unintended consequences can often negate or more than negative the benefits of regulation. When we look at the history of regulations that are designed to force companies to do the right thing, we can see that it's often many years, sometimes decades, before the full impact of the regulation is understood. Designing good regulations is hard, much harder than any of the technical problems we've discussed today.&lt;/p&gt;
&lt;h3 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thanks to Leah Hanson, Gary Bernhardt, Kamal Marhubi, Rebecca Isaacs, Jesse Luehrs, Tom Crayford, Wesley Aptekar-Cassels, Rose Ames, and Benjamin Gilbert for their help with this talk!&lt;/p&gt;
&lt;p&gt;Sorry we went so fast. If there's anything you missed you can catch it in the pseudo-transcript at danluu.com/deconstruct-files.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;&lt;em&gt;This &quot;transcript&quot; is pretty rough since I wrote it up very quickly this morning before the talk. I'll try to clean it within a few weeks, which will include adding material that was missed, inserting links, fixing typos, adding references that were missed, etc.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks to Anatole Shaw and Jernej Simoncic for comments/corrections/discussion on this transcript.&lt;/em&gt;&lt;/p&gt;


&lt;/body&gt;</description>
<pubDate>Tue, 23 Jul 2019 05:56:30 +0000</pubDate>
<dc:creator>tambourine_man</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://danluu.com/deconstruct-files/</dc:identifier>
</item>
</channel>
</rss>