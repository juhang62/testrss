<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>&quot;Pwned Passwords&quot; V2 With Half a Billion Passwords</title>
<link>https://www.troyhunt.com/ive-just-launched-pwned-passwords-version-2/</link>
<guid isPermaLink="true" >https://www.troyhunt.com/ive-just-launched-pwned-passwords-version-2/</guid>
<description>&lt;p&gt;Last August, I launched a little feature within &lt;a href=&quot;https://haveibeenpwned.com/&quot;&gt;Have I Been Pwned&lt;/a&gt; (HIBP) I called &lt;a href=&quot;https://www.troyhunt.com/introducing-306-million-freely-downloadable-pwned-passwords/&quot;&gt;Pwned Passwords&lt;/a&gt;. This was a list of 320 million passwords from a range of different data breaches which organisations could use to better protect their own systems. How? &lt;a href=&quot;https://pages.nist.gov/800-63-3/sp800-63b.html&quot;&gt;NIST explains&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;When processing requests to establish and change memorized secrets, verifiers SHALL compare the prospective secrets against a list that contains values known to be commonly-used, expected, or compromised.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They then go on to recommend that passwords &quot;obtained from previous breach corpuses&quot; should be disallowed and that the service should &quot;advise the subscriber that they need to select a different secret&quot;. This makes a lot of sense when you think about it: if someone is signing up to a service with a password that has previously appeared in a data breach, either it's the same person reusing their passwords (bad) or two different people who through mere coincidence, have chosen &lt;em&gt;exactly&lt;/em&gt; the same password. In reality, this means they probably both have dogs with the same name or some other personal attribute they're naming their passwords after (also bad).&lt;/p&gt;
&lt;p&gt;Now all of this was great advice from NIST, but they stopped short of providing the one thing organisations really need to make all this work: the passwords themselves. That's why I created Pwned Passwords - because there was a gap that needed filling - and let's face it, I do have access to rather a lot of them courtesy of running HIBP. So 6 months ago I launched the service and today, I'm pleased to launch version 2 with more passwords, more features and something I'm particularly excited about - more privacy. Here's what it's all about:&lt;/p&gt;
&lt;h2 id=&quot;theresnow501636842pwnedpasswords&quot;&gt;There's Now 501,636,842 Pwned Passwords&lt;/h2&gt;
&lt;p&gt;Back at the V1 launch, I explained how the original data set was comprised of sources such as &lt;a href=&quot;https://www.troyhunt.com/password-reuse-credential-stuffing-and-another-1-billion-records-in-have-i-been-pwned/&quot;&gt;the Anti Public and Exploit.in combo lists&lt;/a&gt; as well as &quot;a variety of other data sources&quot;. In V2, I've expanded that to include a bunch of data sources along with 2 major ones:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The 711 million record &lt;a href=&quot;https://www.troyhunt.com/inside-the-massive-711-million-record-onliner-spambot-dump/&quot;&gt;Onliner Spambot dump&lt;/a&gt;. This was a &lt;em&gt;lot&lt;/em&gt; of work to parse varying data formats and if you read the comments on that blog post, you'll get a sense of how much people wanted this (and why it was problematic).&lt;/li&gt;
&lt;li&gt;The 1.4B &lt;a href=&quot;https://www.troyhunt.com/making-light-of-the-dark-web-and-debunking-the-fud/&quot;&gt;clear text credentials from the &quot;dark web&quot;&lt;/a&gt;. This data resulted in many totally overblown news stories (and contributed to my &quot;dark web&quot; FUD blog post last week), but it did serve as a useful reference for V2. This data also had a bunch of integrity problems which meant the actual number was somewhat less. For example, the exact same username and password pairs appearing with different delimiters:&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2017/12/1.4B-Data-Integrity.png&quot; alt=&quot;1.4B-Data-Integrity&quot;/&gt;&lt;/p&gt;
&lt;p&gt;There's also &lt;em&gt;a heap&lt;/em&gt; of other separate sources there where passwords were available in plain text. As with V1, I'm not going to name them here, suffice to say it's a broad collection from many more breaches than I used in the original version. It's taken a heap of effort to parse through these but it's helped build that list up to beyond the half billion mark which is a &lt;em&gt;significant&lt;/em&gt; amount of data. From a defensive standpoint, this is good - more data means more ability to block risky passwords.&lt;/p&gt;
&lt;p&gt;But I haven't just &lt;em&gt;added&lt;/em&gt; data, I've also removed some. Let me explain why and to begin with, let's do a quick recap on the rationale for hashing them.&lt;/p&gt;
&lt;h2 id=&quot;theyrestillsha1hashedbutwithsomejunkremoved&quot;&gt;They're Still SHA-1 Hashed, But with Some Junk Removed&lt;/h2&gt;
&lt;p&gt;When I launched V1, I explained why I SHA-1 hashed them:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;It doesn't matter that SHA1 is a fast algorithm unsuitable for storing your customers' passwords with because that's not what we're doing here, it's simply about ensuring the source passwords are not immediately visible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's still 100% true as of today. There are certainly those that don't agree with this approach; they claim that either the data is easily discoverable enough online anyway or conversely, that SHA-1 is an insufficiently robust algorithm for password storage. They're right, too - on both points - but that's not what this is about. The entire point is to ensure that any personal info in the source data is obfuscated such that it requires a concerted effort to remove the protection, but that the data is still usable for its intended purposes. SHA-1 has done that in V1 and I'm still confident enough in the model to use the same approach in V2.&lt;/p&gt;
&lt;p&gt;One of the things that did surprise me a little in V1 was the effort some folks went to in order to crack the passwords. I was surprised primarily because the vast majority of those passwords were already available in the clear via the 2 combo lists I mentioned earlier anyway, so why bother? Just download the (easily discoverable) lists! The penny that later dropped was that it presented a challenge - and people like challenges!&lt;/p&gt;
&lt;p&gt;One upside from people cracking the passwords for fun was that &lt;a href=&quot;https://twitter.com/cynoprime&quot;&gt;CynoSure Prime&lt;/a&gt; managed to identify a bunch of junk. Due to the integrity of the source data being a bit patchy in places, there were entries such as the following.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;$HEX[e3eeeb]&lt;/li&gt;
&lt;li&gt;6dcc978317511fd8&lt;/li&gt;
&lt;li&gt;&amp;lt;div align=\\\'center\\\' style=\\\'font:bold 11px Verdana; width:310px\\\'&amp;gt;&amp;lt;a style=\\\'background-color:#eeeeee;display:block;width:310px;border:solid 2px black; padding:5px\\\' href=\\\'http://...&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Of course, it's &lt;em&gt;possible&lt;/em&gt; people actually used these strings as passwords but applying a bit of &lt;a href=&quot;https://en.wikipedia.org/wiki/Occam%27s_razor&quot;&gt;Occam's Razor&lt;/a&gt; suggests that it's simply parsing issues upstream of this data set. In total, CynoSure Prime identified 3,472,226 junk records which I've removed in V2. (Incidentally, these are the same guys that &lt;a href=&quot;https://cynosureprime.blogspot.com.au/2015/09/how-we-cracked-millions-of-ashley.html&quot;&gt;found the shortcomings in Ashley Madison's password storage approach&lt;/a&gt; back in 2015 - they do quality work!)&lt;/p&gt;
&lt;p&gt;Frankly though, there's little point in removing a few million junk strings. It reduced the overall data size of V2 by 0.69% and other than the tiny fraction of extra bytes added to the set, it makes no practical difference to how the data is used. On that point and in terms of extraneous records, I want to be really clear about the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;This list is not perfect - it's not &lt;em&gt;meant&lt;/em&gt; to be perfect - and there will be some junk due to input data quality and some missing passwords because they weren't in the source data sets. It's simply meant to be a list of strings that pose an elevated risk if used for passwords and for that purpose, it's enormously effective.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Whilst the total number of records included in V2 is significant, it also doesn't tell the whole story and indeed the feedback from V1 was that the 320M passwords needed something more: an indicator of just how bad each one really was.&lt;/p&gt;
&lt;h2 id=&quot;eachpasswordnowhasacountnexttoit&quot;&gt;Each Password Now Has a Count Next to It&lt;/h2&gt;
&lt;p&gt;Is the password &quot;abc123&quot; worse than &quot;acl567&quot;? Most password strength meters would consider them equivalent because mathematically, they are. But as I've said before, &lt;a href=&quot;https://www.troyhunt.com/password-strength-indicators-help-people-make-dumb-choices/&quot;&gt;password strength indicators help people make ill-informed choices&lt;/a&gt; and this is a perfect example of that. They're both terrible passwords - don't get me wrong - but a predictable keyboard pattern makes the former much worse and that's now reflected in the Pwned Passwords data.&lt;/p&gt;
&lt;p&gt;Now on the one hand, you could argue that once a password has appeared breached even just once, it's unfit for future use. It'll go into password dictionaries, be tested against the username it was next to and forever more be a weak choice regardless of where it appears in the future. However, I got a lot of feedback from V1 along the lines of &quot;simply blocking 320M passwords is a usability nightmare&quot;. Blocking half a billion, even more so.&lt;/p&gt;
&lt;p&gt;In V2, every single password has a count next to it. What this means is that next to &quot;abc123&quot; you'll see 2,670,319 - that's how many times it appeared in my data sources. Obviously with a number that high, it appeared many times over in the same sources because many people chose the same password. The password &quot;acl567&quot;, on the other hand, only appeared once. Having visibility to the prevalence means, for example, you might outright block every password that's appeared 100 times or more and force the user to choose another one (there are 1,858,690 of those in the data set), strongly recommend they choose a different password where it's appeared between 20 and 99 times (there's a further 9,985,150 of those), and merely flag the record if it's in the source data less than 20 times. Of course, the password &quot;acl567&quot; may well be deemed too weak by the requirements of the site even without Pwned Passwords so this is by no means the only test a site should apply.&lt;/p&gt;
&lt;p&gt;In total, there were 3,033,858,815 occurrences of those 501,636,842 unique passwords. In other words, on average, each password appeared 6 times across various data breaches. In some cases, the same password appeared many times in the one incident - &lt;em&gt;often thousands of times&lt;/em&gt; - because that's how many people chose the same damn password!&lt;/p&gt;
&lt;p&gt;Now, having said all that, in the lead-up to the launch of V2 I've had people argue vehemently that they all should be blocked or that none of them should be blocked or any combination in between. That's not up to me, that's up to whoever uses this data, my job is simply to give people enough information to be able to make informed decisions. My own subjective view on this is that &quot;it depends&quot;; different risk levels, different audiences and different mitigating controls should all factor into this decision.&lt;/p&gt;
&lt;h2 id=&quot;ihaventincludedpasswordlength&quot;&gt;I &lt;em&gt;Haven't&lt;/em&gt; Included Password Length&lt;/h2&gt;
&lt;p&gt;One request that came up a few times was to include a length attribute on each password hash. This way, those using the data could exclude passwords from the original data set that fall beneath their minimum password length requirements. The thinking there being that it would reduce the data size they're searching through thus realising some performance (and possibly financial) gains. But there are many reasons why this ultimately didn't make sense:&lt;/p&gt;
&lt;p&gt;The first is that from the perspective of protecting the source data (remember, it contains PII in places), explicitly specifying the length greatly reduces the effort required to crack the passwords. Yes, I know I said earlier that the hashing approach wasn't meant to be highly resilient, but providing a length would be significantly detrimental to the protection that SHA-1 &lt;em&gt;does&lt;/em&gt; provide.&lt;/p&gt;
&lt;p&gt;Then, I actually got a bit scientific about it and looked at what minimum length password websites required. In fact, that's why I wrote the piece on &lt;a href=&quot;https://www.troyhunt.com/how-long-is-long-enough-minimum-password-lengths-by-the-worlds-top-sites/&quot;&gt;minimum length by the world's top sites&lt;/a&gt; a couple of weeks back; I wanted to put hard numbers on it. 11 of the 15 sites I referred to had a minimum length of 6 chars or less. When I then went and looked at the data set I was using, excluding passwords of less than 6 chars would have only reduced the set by less than 1% Excluding anything under 8 chars would have reduced it by just under 16%. They're very small numbers.&lt;/p&gt;
&lt;p&gt;Then there's the overhead required to host and search this data, that is the overhead those organisations who use it will incur. It should be &lt;em&gt;very&lt;/em&gt; close to nothing with the whole half billion data set. Chuck it in a storage construct like Azure Table Storage and you're looking at single digit dollars per month with single digit millisecond lookup times. There's no need for this to be any more complex than that.&lt;/p&gt;
&lt;p&gt;So in short, it put the protection of the hashing at greater risk, there was very little value gained and it's easy to implement this in a way that's fast and cheap anyway. Some people will disagree, but a lot of thought went into this and I'm confident that the conclusion was the right one.&lt;/p&gt;
&lt;h2 id=&quot;downloadingthedata&quot;&gt;Downloading the Data&lt;/h2&gt;
&lt;p&gt;And now to the pointy bit - downloading the data. As with V1, there's one big 7z archive you can &lt;a href=&quot;https://haveibeenpwned.com/Passwords&quot;&gt;go and pull down immediately from the Pwned Passwords page on HIBP&lt;/a&gt;. Also as before, it's available via direct download from the site or via torrent. I want to &lt;em&gt;strongly&lt;/em&gt; encourage you to take it via the torrent, let me explain why:&lt;/p&gt;
&lt;p&gt;The underlying storage construct for this data is Azure Blob storage. If I was to serve the file directly from there, I'd cop a &lt;em&gt;very&lt;/em&gt; hefty data bill. Cloudflare came to rescue in V1 and gave me a free plan that enabled a file of that size to be cached as their edge nodes. The impact of that on my bill was &lt;em&gt;massive:&lt;/em&gt;&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; readability=&quot;4.755980861244&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Another massive thanks to &lt;a href=&quot;https://twitter.com/Cloudflare?ref_src=twsrc%5Etfw&quot;&gt;@Cloudflare&lt;/a&gt; for supporting the &lt;a href=&quot;https://twitter.com/haveibeenpwned?ref_src=twsrc%5Etfw&quot;&gt;@haveibeenpwned&lt;/a&gt; Pwned Passwords, just did the maths on how much it saved me - whoa! &lt;a href=&quot;https://t.co/70kki5Uw7o&quot;&gt;pic.twitter.com/70kki5Uw7o&lt;/a&gt;&lt;/p&gt;
‚Äî Troy Hunt (@troyhunt) &lt;a href=&quot;https://twitter.com/troyhunt/status/897571703202603008?ref_src=twsrc%5Etfw&quot;&gt;August 15, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Imagine the discussion I'd be having with my wife if it wasn't for Cloudflare's support! And that was before another 6 months' worth of downloads too. Cloudflare might have given me the service for free, but they still have to pay for bandwidth so I'd like to ask for your support in pulling the data down via torrents rather than from the direct download link. To that effect, the UI actively encourages you to grab the torrent:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Download-Pwned-Passwords.png&quot; alt=&quot;Download Pwned Passwords&quot;/&gt;&lt;/p&gt;
&lt;p&gt;If you can't grab the torrent (and I'm conscious there are, for example, corporate environments where torrents are blocked), then download it direct but do your bit to help me out by supporting the folks supporting me where you can. As with V1, the torrent file is served directly from HIBP's Blob Storage and you'll find a SHA-1 hash of the Pwned Passwords file next to it so you can check integrity if you're so inclined.&lt;/p&gt;
&lt;p&gt;So that's the download - go forth and do good things with it! Now for something else cool and that's the online search.&lt;/p&gt;
&lt;h2 id=&quot;queryingthedataonline&quot;&gt;Querying the Data Online&lt;/h2&gt;
&lt;p&gt;In V1, I stood up &lt;a href=&quot;https://haveibeenpwned.com/Passwords&quot;&gt;an online search feature&lt;/a&gt; where you could plug in a password and see if it appeared in the data set. That sat on top of an API which I also made available for independent consumption should people wish to use it. And many people did use it. In fact, some of the entrants to &lt;a href=&quot;https://www.troyhunt.com/introducing-306-million-freely-downloadable-pwned-passwords/&quot;&gt;my competition to win a Lenovo laptop&lt;/a&gt; leveraged that particular endpoint including the winner of the competition, 16,year-old F√©lix Giffard. He created &lt;a href=&quot;https://passwordsecurity.info/&quot;&gt;PasswordSecurity.info&lt;/a&gt; which directly consumes the Pwned Passwords API via the client side:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/PasswordSecurity.info.png&quot; alt=&quot;PasswordSecurity.info&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Getting back to the online search, being conscious of not wanting to send the wrong message to people, immediately before the search box I put a very clear, very &lt;strong&gt;bold&lt;/strong&gt; message: &quot;Do not send any password you actively use to a third-party service - even this one!&quot;&lt;/p&gt;
&lt;p&gt;But people don't always read these things. The service got &lt;em&gt;a heap&lt;/em&gt; of press and millions of people descended on the site to check their passwords. At least I &lt;em&gt;assume&lt;/em&gt; it was their passwords, I certainly don't log those searches but based on the news articles and social media commentary, yeah, it would have been a heap of real passwords. And I'm actually ok with that - let me explain:&lt;/p&gt;
&lt;p&gt;As much as I don't want to encourage people to plug their real password(s) into random third-party sites, I can &lt;em&gt;guarantee&lt;/em&gt; that a sizable number of people got a positive hit and then changed their security hygiene as a result. One of the biggest things that's resonated with me in running HIBP is how much impact it's had on changing user behaviour. Seeing either your email address or your password pwned has a way of making people reconsider some of their security decisions.&lt;/p&gt;
&lt;p&gt;The online search works &lt;em&gt;almost&lt;/em&gt; identically to V1 albeit with the count of the password now represented too:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Searching-for-a-Pwned-Password.jpg&quot; alt=&quot;Pwned Search&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Pretty simple stuff and for the most part, also pretty familiar. But there's one &lt;em&gt;really&lt;/em&gt; important - and &lt;em&gt;really&lt;/em&gt; cool - difference. Let me explain:&lt;/p&gt;
&lt;h2 id=&quot;cloudflareprivacyandkanonymity&quot;&gt;Cloudflare, Privacy and k-Anonymity&lt;/h2&gt;
&lt;p&gt;In what proved to be very fortuitous timing, &lt;a href=&quot;https://twitter.com/icyapril&quot;&gt;Junade Ali&lt;/a&gt; from Cloudflare reached out to me last month with an idea. They wanted to build a tool to search through Pwned Passwords V1 but to do so in a way that allowed external parties to use it &lt;em&gt;and&lt;/em&gt; maintain anonymity. You see, the problem with my existing implementation was that whilst you could pass just a SHA-1 hash of the password, if it returned a hit and I was to take that and reverse it back to the clear (which I could easily do because I created the hashes in the first place!) I'd know the password. That made the service hard to justify sending &lt;em&gt;real&lt;/em&gt; passwords to.&lt;/p&gt;
&lt;p&gt;Junade's idea was different though; he proposed using a mathematical property called &lt;a href=&quot;https://en.wikipedia.org/wiki/K-anonymity&quot;&gt;&lt;em&gt;k&lt;/em&gt;-anonymity&lt;/a&gt; and within the scope of Pwned Passwords, it works like this: imagine if you wanted to check whether the password &quot;P@ssw0rd&quot; exists in the data set. (Incidentally, the hackers have worked out people do stuff like this. I know, it sucks. They're onto us.) The SHA-1 hash of that string is &quot;21BD12DC183F740EE76F27B78EB39C8AD972A757&quot; so what we're going to do is take &lt;em&gt;just the first 5 characters&lt;/em&gt;, in this case that means &quot;21BD1&quot;. That gets sent to the Pwned Passwords API and it responds with 475 hash &lt;em&gt;suffixes&lt;/em&gt; (that is everything after &quot;21BD1&quot;) and a count of how many times the original password has been seen. For example:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;(21BD1) &lt;strong&gt;0018A45C4D1DEF81644B54AB7F969B88D65:1&lt;/strong&gt; (password &quot;lauragpe&quot;)&lt;/li&gt;
&lt;li&gt;(21BD1) &lt;strong&gt;00D4F6E8FA6EECAD2A3AA415EEC418D38EC:2&lt;/strong&gt; (password &quot;alexguo029&quot;)&lt;/li&gt;
&lt;li&gt;(21BD1) &lt;strong&gt;011053FD0102E94D6AE2F8B83D76FAF94F6:1&lt;/strong&gt; (password &quot;BDnd9102&quot;)&lt;/li&gt;
&lt;li&gt;(21BD1) &lt;strong&gt;012A7CA357541F0AC487871FEEC1891C49C:2&lt;/strong&gt; (password &quot;melobie&quot;)&lt;/li&gt;
&lt;li&gt;(21BD1) &lt;strong&gt;0136E006E24E7D152139815FB0FC6A50B15:2&lt;/strong&gt; (password &quot;quvekyny&quot;)&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I added the prefix in brackets beforehand and the source passwords in brackets afterwards simply to illustrate what we're doing here; they're all just different strings that hash down to values with the same first 5 characters. In other words, they're all within the same &quot;range&quot; and you'll see that term referenced more later on. Using this model, someone searching the data set just gets back the hash suffixes and counts (everything in bold after the first 5 chars) and they can then see if everything after the first 5 chars of &lt;em&gt;their&lt;/em&gt; hash matches any of the returned strings. Now keep in mind that as far as I'm concerned, the partial hash I was sent could be any one of 475 different possible values. Or it could be something totally different, I simply don't know and therein lies the anonymity value.&lt;/p&gt;
&lt;p&gt;For the sake of perspective, here are some stats on what this means for the data within Pwned Passwords:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Every hash prefix from 00000 to FFFFF is populated with data (16^5 combinations)&lt;/li&gt;
&lt;li&gt;The average number of hashes returned is 478&lt;/li&gt;
&lt;li&gt;The smallest is 381 (hash prefixes &quot;E0812&quot; and &quot;E613D&quot;)&lt;/li&gt;
&lt;li&gt;The largest is 584 (hash prefixes &quot;00000&quot; and &quot;4A4E8&quot;)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Junade has written a great piece that's just gone live on Cloudflare's blog titled &lt;a href=&quot;https://blog.cloudflare.com/validating-leaked-passwords-with-k-anonymity/&quot;&gt;Validating Leaked Passwords with k-Anonymity&lt;/a&gt; and he goes into more depth in that piece. As he explains, there are other cryptographic approaches which could also address the desire for anonymity (for example, private set intersections), but not with the ease and level of simplicity Junade proposed. I loved it so much that I offered to build and run it as a service out of HIBP. Junade (and Cloudflare) thought that was a great idea so they offered to point folks over to the HIBP version rather than build out something totally separate. That's a partnership I'm &lt;em&gt;enormously&lt;/em&gt; happy with I appreciate their confidence in my running it.&lt;/p&gt;
&lt;p&gt;This model of anonymity is what now sits behind the online search feature. You can see it in action by trying a search for &quot;P@ssw0rd&quot; which will return the screen in the previous image. If we drop down and take a look at the dev tools, here's the actual request that's been made:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Searching-for-a-range.png&quot; alt=&quot;Searching for a range&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The password has been hashed client side and just the first 5 characters passed to the API (I'll talk more about the mechanics of that shortly). Here's what then comes back in the response:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Hit-found-in-password-range.png&quot; alt=&quot;Hit found in password range&quot;/&gt;&lt;/p&gt;
&lt;p&gt;As mentioned earlier, there are 475 hashes beginning with &quot;21BD1&quot;, but only 1 which matches the remainder of the hash for &quot;P@ssw0rd&quot; and that record indicates that the password has previously been seen 47,205 times. And that's it - that's what I've done with Cloudflare's support and that's what we've done together to protect anonymity and make the service available to everyone. Let me now talk about how you can use the API.&lt;/p&gt;
&lt;h2 id=&quot;consumingtheapiandthemechanicsbehindtherangesearch&quot;&gt;Consuming the API (and the Mechanics Behind the Range Search)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://haveibeenpwned.com/API/v2#PwnedPasswords&quot;&gt;The existing API documentation on HIBP&lt;/a&gt; has been updated so you can go there for all the implementation details. There are a few things in particular I want to call out though:&lt;/p&gt;
&lt;p&gt;Firstly, you'll notice that I'm serving this API from a different domain to the other HIBP APIs and indeed from V1 of the Pwned Passwords service. For V2, I've stood up an Azure Function on the api.pwnedpasswords.com domain which gets the API out of the HIBP website and running on serverless infrastructure instead. &lt;a href=&quot;https://www.troyhunt.com/azure-functions-in-practice/&quot;&gt;I've written about Azure Functions in the past&lt;/a&gt; and they're an awesome way of building a highly scalable, resilient &quot;code as a service&quot; architecture. It ensures that load comes off the HIBP website and that I can scale the Pwned Passwords service infinitely, albeit with a line directly to my wallet! It's also given me the flexibility to do things like trim off a bunch of excessive headers such as the content security policy HIBP uses (that's of no use to a lone API endpoint).&lt;/p&gt;
&lt;p&gt;Secondly, the existing API (that many people have created dependencies on!) still works just fine. It also points to the storage repository for V2 of the password set so it's now searching through the full half billion records. I'll leave this running for the foreseeable future, but if you are using it then I'd &lt;em&gt;prefer&lt;/em&gt; you roll over to the endpoint on api.pwnedpasswords.com for the reasons mentioned above, and for these other reasons:&lt;/p&gt;
&lt;p&gt;If you were using the original API via HTTP GET, rolling over to the new one changes &lt;em&gt;absolutely nothing&lt;/em&gt; in your implementation other than the URL which will look like this:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;GET https://api.pwnedpasswords.com/pwnedpassword/{password}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It'll still return HTTP 200 when a password is found and 404 when it's not. The only difference (and this shouldn't break any existing usages), is that the 200 response now also contains a count in the body by way of a single integer:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Password-Search.png&quot; alt=&quot;Password Search&quot;/&gt;&lt;/p&gt;
&lt;p&gt;And as before, you can always pass a hash if preferred:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Password-Search-by-Hash.png&quot; alt=&quot;Password Search by Hash&quot;/&gt;&lt;/p&gt;
&lt;p&gt;But, of course, we've just had the anonymity chat and you would have seen the path for calling that endpoint earlier on. Just to point it out again here, you can pass the first 5 chars of the hash to this address:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;https://api.pwnedpasswords.com/range/{hashPrefix}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Which returns a result like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Range-Search-Results.png&quot; alt=&quot;Range Search Results&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Remember, these are all hash &lt;em&gt;suffixes&lt;/em&gt; (followed by a count) so the full value of the first hash, for example, is &quot;21BD10018A45C4D1DEF81644B54AB7F969B88D65&quot;. Incidentally, input to the API is not case sensitive so &quot;21bd1&quot; works just as well as &quot;21BD1&quot;. All hash suffixes returned (and indeed those provided in the downloadable data) are uppercase simply because that's the default output from &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/t-sql/functions/hashbytes-transact-sql&quot;&gt;SQL Server's HASHBYTES function&lt;/a&gt; (I processed the source data in a local RDBMS instance).&lt;/p&gt;
&lt;p&gt;Unlike the original version, there's no rate-limiting. That was a construct I needed primarily to protect personal data in the breached account search (i.e. when you search for your email address amongst data breaches), but I extended it to Pwned Passwords as well to help protect the infrastructure. Now running on serverless Azure Functions, I don't have that concern so I've dropped it altogether. I'd also dropped version numbers, I'll deal with that when I need them which may not be for a long time (if ever).&lt;/p&gt;
&lt;p&gt;Now, a few more things around some design decisions I've made: I'm &lt;em&gt;very&lt;/em&gt; wary of the potential impact on my wallet of running the service this way. It's one thing to stand up V1 that only returned an HTTP response code, was rate-limited and really wasn't designed to be called in bulk by a single consumer (considering the privacy implications), it's quite another to do what I've done with V2, especially when each search of the range API returns hundreds of records. That &quot;P@ssw0rd&quot; search, for example, returns 9,730 bytes when gzipped (that's a pretty average size) and I'm paying for egress bandwidth out of Azure, the execution of the function and the call to the underlying storage. Tiny amounts each time, mind you, but I've had to reduce that impact on me as far as possible through a range of measures.&lt;/p&gt;
&lt;p&gt;For example, the result of that range query is not a neatly formatted piece of JSON, it's just colon delimited rows. That impacts my ability to add attributes at a later date and pretty much locks in the current version to today's behaviour, but it saves on the response size. Yes, I know some curly braces and quotes wouldn't add a lot of size, but every byte counts when volumes get large.&lt;/p&gt;
&lt;p&gt;You'll also notice there's a long max-age on the cache-control header:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Cache-Settings.png&quot; alt=&quot;Cache Settings&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This is 31 days' worth of cache and the subsequent Cloudflare cache status header explains why: by routing through their infrastructure, they can aggressively cache these results which ensures not only is the response &lt;em&gt;lightning&lt;/em&gt; fast (remember, they presently have &lt;a href=&quot;https://www.cloudflare.com/&quot;&gt;121 edge nodes around the world&lt;/a&gt; so there's one near you), but that I don't wear the financial hit of people hammering my origin. Especially when you consider the extent to which multiple people use the same password, when we're talking about the range search where many different passwords have identical hash prefixes, there's some significant benefits to be had from caching. As mentioned earlier, there are 16^5 different hash prefixes (1,048,576) within the range of 00000 to FFFFF so you can see how extensive usage would benefit greatly from caching across many millions of searches. The performance difference alone when comparing a cached result with a non-cached one makes a compelling argument:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Cached-versus-non-cached-V2-queries.png&quot; alt=&quot;Cached versus non-cached V2 queries&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This means that even though the response is significantly larger than in V1, if I can serve a request to the new API from cache there's actually a &lt;em&gt;massive&lt;/em&gt; improvement. Here's a series of hits to V1 where every single time, the request had to go all the way to the origin server, hit the API and then query 320M records:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/Query-times-of-V1-API.png&quot; alt=&quot;Query times of V1 API&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In order to make aggressive caching feasible, I'm also &lt;em&gt;only&lt;/em&gt; supporting HTTP GET. Now, some people will lose their minds over this because they'll say &quot;that means it goes into logs and you'll track the passwords being searched for&quot;. If you're worried about me tracking anything, don't use the service. That's not intended to be a flippant statement, rather a simple acknowledgment that you need to trust the operator of the service if you're going to be sending passwords in any shape or form. Offsetting that is the whole k-Anonymity situation; even if you &lt;em&gt;don't&lt;/em&gt; trust the service or you think logs may be leaked and abused (and incidentally, &lt;em&gt;nothing&lt;/em&gt; is explicitly logged, they're transient system logs at most), the range search goes a very long way to protecting the source. If you &lt;em&gt;still&lt;/em&gt; don't trust it, then just download the hashes and host them yourself. No really, that's the whole point of making them available and in all honesty, if it was me building on top of these hashes then I'd definitely be querying my own repository of them.&lt;/p&gt;
&lt;p&gt;In summary, if you're using the range search then you get protection of the source password &lt;em&gt;well&lt;/em&gt; in excess of what I was able to do in V1 plus it's &lt;em&gt;massively&lt;/em&gt; faster if anyone else has done a search for any password that hashes down to the same first 5 characters of SHA-1. Plus, it helps me out an awful lot in terms of keeping the costs down!&lt;/p&gt;
&lt;h2 id=&quot;pwnedpasswordsinaction&quot;&gt;Pwned Passwords in Action&lt;/h2&gt;
&lt;p&gt;Lastly, I want to call out a number of examples of the first generation of Pwned Passwords in action. My hope is that they inspire others to build on top of this data set and ultimately, make a positive difference to web security for everyone.&lt;/p&gt;
&lt;p&gt;For example, Workbooks.com (they make CRM software, among other things) &lt;a href=&quot;https://www.workbooks.com/node/1798&quot;&gt;explains to customers that a Pwned Password is weak or has previously appeared in a data breach&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then there's Colloq (they help you discover conferences) who've &lt;a href=&quot;https://colloq.io/blog/how-our-password-check-works&quot;&gt;written up a great piece with loads of performance stats&lt;/a&gt; about their implementation of the data.&lt;/p&gt;
&lt;p&gt;Or &lt;a href=&quot;https://toepoke.co.uk/user.aspx/create&quot;&gt;try creating an account on Toepoke&lt;/a&gt; with a password of &quot;P@ssw0rd&quot; and see how that goes for you:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.troyhunt.com/content/images/2018/02/toepoke.co.uk-password-check.png&quot; alt=&quot;toepoke.co.uk password check&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://safepass.me/&quot;&gt;safepass.me&lt;/a&gt; also picked up the data and wrapped it into an offline commercial Active Directory filter (plus a free home version).&lt;/p&gt;
&lt;p&gt;On the mobile front, there's &lt;a href=&quot;https://play.google.com/store/apps/details?id=pwnedpasswords.pwnedpasswords&quot;&gt;Pwned Pass in the Google Play store&lt;/a&gt; which sits on top of the existing API.&lt;/p&gt;
&lt;h2 id=&quot;thisisallstillfreeandistilllikebeer&quot;&gt;This is All Still Free (and I Still Like Beer!)&lt;/h2&gt;
&lt;p&gt;Nothing gains traction like free things! Keeping HIBP free to search your address (or your entire domain) was the best thing I ever did in terms of making it stick. A few months after I launched the service, I stood up &lt;a href=&quot;https://haveibeenpwned.com/Donate&quot;&gt;a donations page&lt;/a&gt; where you could buy me some beers (or coffee or other things). It only went up after people specifically asked for it (&quot;hey awesome service, can I get you a coffee?&quot;) and I've been really happy with the responses to it. As I say on the page, it's more the &lt;em&gt;time&lt;/em&gt; commitment that really costs me (I'm independent so while I'm building something like Pwned Passwords, I'm not doing something else), but there are also costs that may surprise you:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; readability=&quot;7.5090252707581&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Just burned through $100 of mobile data so that I could finish processing Pwned Passwords this weekend. 110kbps on unlimited broadband plan or 8,286kbps on 4G at $10/GB. It was going to be hard to get it live next week otherwise üôÅ&lt;/p&gt;
‚Äî Troy Hunt (@troyhunt) &lt;a href=&quot;https://twitter.com/troyhunt/status/964785654847627265?ref_src=twsrc%5Etfw&quot;&gt;February 17, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;This is one of those true &quot;Australianisms&quot; courtesy of the fact my &lt;em&gt;up-speed&lt;/em&gt; maxes out at about 1.5Mbps (and is then shared across all the things in my house that send data out). Down-speed is about 114 but getting anything up is a nightmare. (And for Aussie friends, no, there's no NBN available in my area of the Gold Coast yet, but apparently it's not far off.) And no, this is not a solvable problem by doing everything in the cloud and there are many reasons why that wouldn't have worked (I'll blog them at a later date).&lt;/p&gt;
&lt;p&gt;If you want to help kick in for these costs and &lt;a href=&quot;https://haveibeenpwned.com/Donate&quot;&gt;shout me a sympathy coffee or beer(s)&lt;/a&gt;, it's still very much appreciated!&lt;/p&gt;
&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;
&lt;p&gt;Pwned Passwords V2 is now live! Everything you need to use them is over on &lt;a href=&quot;https://haveibeenpwned.com/Passwords&quot;&gt;the Pwned Passwords page of HIBP&lt;/a&gt; where you can check them online, learn about the API or just download the whole lot. All those models are free, unrestricted and don't even require attribution if you don't want to provide it, just take what's there and go do good things with it üòÄ&lt;/p&gt;
</description>
<pubDate>Wed, 21 Feb 2018 19:46:52 +0000</pubDate>
<dc:creator>explodingcamera</dc:creator>
<og:type>article</og:type>
<og:title>I've Just Launched &quot;Pwned Passwords&quot; V2 With Half a Billion Passwords for Download</og:title>
<og:description>Last August, I launched a little feature within Have I Been Pwned (HIBP) I called Pwned Passwords. This was a list of 320 million passwords from a range of different data breaches which organisations could use to better protect their own systems. How? NIST explains: When processing requests to establish</og:description>
<og:url>https://www.troyhunt.com/ive-just-launched-pwned-passwords-version-2/</og:url>
<og:image>https://www.troyhunt.com/content/images/2018/02/Searching-for-a-Pwned-Password.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.troyhunt.com/ive-just-launched-pwned-passwords-version-2/</dc:identifier>
</item>
<item>
<title>Signal Foundation</title>
<link>https://signal.org/blog/signal-foundation/</link>
<guid isPermaLink="true" >https://signal.org/blog/signal-foundation/</guid>
<description>&lt;p&gt;Long before we knew that it would be called Signal, we knew what we wanted it to be. Instead of teaching the rest of the world cryptography, we wanted to see if we could develop cryptography that worked for the rest of the world. At the time, the industry consensus was largely that encryption and cryptography would remain unusable, but we started Signal with the idea that private communication could be simple.&lt;/p&gt;
&lt;p&gt;Since then, we‚Äôve made some progress. We‚Äôve built a service used by millions, and software used by billions. The stories that make it back to us and keep us going are the stories of people discovering each other in moments where they found they could speak freely over Signal, of people falling in love over Signal, of people organizing ambitious plans over Signal. When we ask friends who at their workplace is on Signal and they respond ‚Äúevery C-level executive, and the kitchen staff.‚Äù When we receive a subpoena for user data and have nothing to send back but a blank sheet of paper. When we catch that glimpse of ‚ÄúSignal blue‚Äù on a metro commuter‚Äôs phone and smile.&lt;/p&gt;
&lt;p&gt;However, we‚Äôve always wanted to do much more, and our limitations have often been challenging. Over the lifetime of the project, there have only been an average of 2.3 full-time software developers, and the entire Signal team has never been more than 7 people. With three client platforms to develop, a service to build and run, a growing list of integrations to assist with, and millions of users to support, that has often left us wanting.&lt;/p&gt;
&lt;p&gt;Even so, Signal has never taken VC funding or sought investment, because we felt that putting profit first would be incompatible with building a sustainable project that put users first. As a consequence, Signal has sometimes suffered from our lack of resources or capacity in the short term, but we‚Äôve always felt those values would lead to the best possible experience in the long term.&lt;/p&gt;
&lt;p&gt;We‚Äôre glad those are the choices we‚Äôve made. Today, we are launching the &lt;a href=&quot;https://signalfoundation.org&quot;&gt;Signal Foundation&lt;/a&gt;, an emerging 501(c)(3) nonprofit created and made possible by Brian Acton, the co-founder of WhatsApp, to support, accelerate, and broaden Signal‚Äôs mission of making private communication accessible and ubiquitous. In case you missed it, Brian left WhatsApp and Facebook last year, and has been thinking about how to best focus his future time and energy on building nonprofit technology for public good.&lt;/p&gt;
&lt;p&gt;Starting with an initial $50,000,000 in funding, we can now increase the size of our team, our capacity, and our ambitions. This means reduced uncertainty on the path to sustainability, and the strengthening of our long-term goals and values. Perhaps most significantly, the addition of Brian brings an incredibly talented engineer and visionary with decades of experience building successful products to our team.&lt;/p&gt;
&lt;p&gt;The Signal Foundation completes our vision of Signal operating as part of a full 501(c)(3). Up until now, we‚Äôve never been able to take on the 501(c)(3) management overhead ourselves, and we‚Äôve relied on the generosity of the Freedom Of The Press Foundation as our fiscal sponsor. Without their support, Signal would not be where it is today, and they have graciously agreed to continue accepting donations on our behalf while our status is pending.&lt;/p&gt;
&lt;p&gt;Looking back at the progress we‚Äôve made with such a small group, it‚Äôs exciting to imagine what an expanded team and the new foundation will help us accomplish in the future. We still know what we want Signal to be, and my personal commitment to Signal is as strong as ever.&lt;/p&gt;
&lt;h3 id=&quot;a-message-from-brian&quot;&gt;&lt;em&gt;A message from Brian&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;Hi everyone, Brian here. I am incredibly excited to be launching the Signal Foundation with Moxie. The Signal Foundation‚Äôs mission is to develop open source privacy technology that protects free expression and enables secure global communication.&lt;/p&gt;
&lt;p&gt;As more and more of our lives happen online, data protection and privacy are critical. This isn‚Äôt just important for select people in select countries. It‚Äôs important for people from all walks of life in every part of the world. Everyone deserves to be protected. We created the Signal Foundation in response to this global need. Our plan is to pioneer a new model of technology nonprofit focused on privacy and data protection for everyone, everywhere.&lt;/p&gt;
&lt;p&gt;Moxie and I share a belief that the best way to continue to ensure the universal availability of high-security and low-cost communications services like Signal is to do so through a foundation structure that is free of the inherent limitations of a for-profit company. Ultimately, our goal is to make the Signal Foundation financially self-sustaining. We believe there is an opportunity to act in the public interest and make a meaningful contribution to society by building sustainable technology that respects users and does not rely on the commoditization of personal data. Signal has always been a collaborative project with a strong community, and we will continue to learn from our users and experiment together.&lt;/p&gt;
&lt;p&gt;Moxie and his team have built something very special in Signal Messenger and I am thrilled to join their effort to provide the most trusted communications experience on the planet. I first met Moxie in 2013 when I was at WhatsApp and we were working on a joint effort to add end-to-end encryption to the app. I was blown away by his technical ability and admired his passion and absolute commitment to data protection and personal privacy. Moxie will continue to serve as CEO of the newly created Signal Messenger nonprofit organization, and I will serve as Executive Chairman of the Signal Foundation where I will take an active, daily role in operations and product development. After over 20 years of working for some of the largest technology companies in the world, I couldn‚Äôt be more excited for this opportunity to build an organization at the intersection of technology and the nonprofit world.&lt;/p&gt;
&lt;p&gt;Of course, this is just the beginning. There is a lot of work to be done to make our dream a reality and we will continually be asking our peers, our community, and ourselves if there are more effective ways to serve the public good. In the immediate future we are focused on adding to our talented-but-small team and improving Signal Messenger. Our long-term vision is for the Signal Foundation to provide multiple offerings that align with our core mission. That will come in time. For now, I invite you to sign up for Signal Messenger and join us in the experiment.&lt;/p&gt;
&lt;p&gt;Brian Acton&lt;/p&gt;

</description>
<pubDate>Wed, 21 Feb 2018 18:42:37 +0000</pubDate>
<dc:creator>conroy</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://signal.org/blog/signal-foundation/</dc:identifier>
</item>
<item>
<title>Why It‚Äôs So Hard to Actually Work in Shared Offices</title>
<link>https://thewalrus.ca/why-its-so-hard-to-actually-work-in-shared-offices/</link>
<guid isPermaLink="true" >https://thewalrus.ca/why-its-so-hard-to-actually-work-in-shared-offices/</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://s3.amazonaws.com/walrus-assets/img/WEB_Hune-Brown_WeWork_art.jpg&quot; alt=&quot;Illustration by Josh Holinaty&quot;/&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;O&lt;/span&gt;&lt;span class=&quot;smallcaps&quot;&gt;ne afternoon last&lt;/span&gt; May, on the fourth floor of a massive renovation site in downtown Toronto, a lanky twentysomething in a hard hat asked me to envision the future. Jarred was from WeWork, a company that was in the midst of building a six-storey communal workplace where self-employed strivers could rent desks, mingle, and share ideas around craft-beer taps. The space, he assured me, was going to be &lt;em&gt;funky&lt;/em&gt;. ‚ÄúThere‚Äôll be exposed brick and sockets to give it that modern look,‚Äù he said, gesturing at the dusty expanse.&lt;/p&gt;
&lt;p&gt;Jarred was trying to sell me on more than just aesthetics‚Äî he was offering a utopian vision of community. My future co-workers, he said, would be fascinating. They were startup founders and young creative types. A tequila company had rented office space and wanted to host tequila Tuesdays. He opened the WeWork app on his phone, and I watched as a cascade of posts from my soon-to-be colleagues and collaborators flew past. ‚ÄúI‚Äôve heard from people who have tried other co-working spaces and‚Ä¶the other ones aren‚Äôt &lt;em&gt;bad&lt;/em&gt;,‚Äù Jarred said with an exaggerated pause. WeWork was just that much better. ‚ÄúWe know your name, we remember your birthday, we remember your dog‚Äôs birthday,‚Äù he continued. I don‚Äôt have a dog, but I appreciated the sentiment. I signed up on the spot.&lt;/p&gt;
&lt;p&gt;WeWork was founded by Adam Neumann and Miguel McKelvey in 2010, and it started with a single office in New York City. Today, the company has 274 offices in fifty-nine cities, from Bogot√° to Tel Aviv. It is the fourth-largest startup in America, and it is reportedly valued at more than $20 billion (US), which puts it below only Uber, Airbnb, and SpaceX. WeWork leases buildings, renovates them to a millennial-approved sheen, and then rents them out desk by desk and office by office. There are now five locations in Canada, and at the inaugural Toronto office, a ‚Äúhot desk‚Äù‚Äî a spot at a communal table or couch‚Äî starts at $500 per month, a permanent desk at $700, and a private office at $1,000. The company is now trying to become the leader in a crowded market where dozens of hubs all promise a variation on the same thing: an inspirational environment among like-minded members of the creative class, plus coffee.&lt;/p&gt;
&lt;p&gt;According to a survey by Upwork and the Freelancers Union, more than one-third of workers in the United States were freelancers in 2016‚Äî some 55 million and counting. A study by accounting-software provider QuickBooks predicts that 45 percent of the Canadian workforce will be self-employed by 2020. WeWork, with its enormous pocketbook and hipster-capitalist aesthetic, is determined to become the default home for a new generation of white-collar workers. It believes that these budding entrepreneurs have no interest in the grey cubicles of the past. They want an office that matches their personality. And apparently, that means beer on tap and lots of it.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;smallcaps&quot;&gt;hree months after&lt;/span&gt; my tour, my co-workers and I began our new life together. The office looked like the lobby of a hipster hotel. There was a graffiti mural in the foyer, and the prayer hands emoji that Drake has tattooed on his arm was rendered in neon on the sixth floor. Fresh pastries were laid out, to go with the bottomless citrus- and cucumber-infused water and micro-roasted coffee. Rows of simple wooden tables ran across the common area, and there were booths for private phone calls, couches for conversations, and an open kitchen for contemplative snacking. On the upper floors, startups and established companies occupied small offices separated by glass walls (&lt;span class=&quot;smallcaps&quot;&gt;RBC&lt;/span&gt;, in an attempt to find new inspiration and new customers, had rented nearly an entire floor). The place had a first-day-of-school air, with freelancers holding their phones in front of their faces as they entered their selfies into the WeWork app‚Äî the virtual community that would complement our physical community.&lt;/p&gt;
&lt;p&gt;Over the next few weeks, we showed up each day and tapped away on MacBook Airs to the sounds of Portuguese house music and old-school hip hop piped in through speakers. (‚ÄúRap is urban, and so is WeWork,‚Äù the company explains online. ‚ÄúBut more profoundly, the common themes of rap are in tune with the company‚Äôs mission.‚Äù) While we created, cleaning crews in WeWork T-shirts quietly restocked the citrus water and wiped up our spilled drinks.&lt;/p&gt;
&lt;p&gt;Fotini Iconomopoulos, a negotiation consultant, perched on one of the many couches. She had spent years working on the road and from caf√©s before trying out a co-working space. ‚ÄúI like the idea of seeing people that I can connect with on a regular basis, seeing familiar faces,‚Äù she explained. Dane Jensen, a high-school classmate of mine who is now the head of a performance-coaching company, sat at a nearby desk. ‚ÄúThis place must have more Apple AirPods per capita than anywhere in the world,‚Äù he said one afternoon, warily looking around. Even so, Jensen said, the building‚Äôs vibe could feel invigorating at times. ‚ÄúIf I‚Äôve been working at home for too many days in a row and I‚Äôm feeling sluggish, it‚Äôs nice to go somewhere where there‚Äôs a lot of busy people being productive.‚Äù&lt;/p&gt;
&lt;p&gt;Still, the future of work looked very much like, well, work. A hip hop soundtrack does not change the tedium of sending emails and updating spreadsheets. The most distinctive feature of the co-working life‚Äî imported from our social networks on Instagram and Facebook‚Äî was the pervasive sense that everyone was hustling, killing it, and eagerly selling themselves. The building was home to endless happy hours, meditation groups, and marketing seminars‚Äî elaborate PR affairs disguised as community events. On the WeWork app, a whir of requests whizzed past each day: a company offering ‚Äúnon-traditional lifestyle swag‚Äù was looking to barter its featured deals for business expertise; a food-ordering app that offers steep discounts on old food that‚Äôs destined for the dumpster (‚Äúthink of it as the happy hour for food!‚Äù) was asking for beta users. The WeWork experience was recognizing that you were, at any given moment, both a product that needed selling and the target market for a noisy community of smiling, desperate salespeople.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;smallcaps&quot;&gt;hen the WeWork&lt;/span&gt; founders talk about their company, they use the grandiose terms typical of Silicon Valley. Their stated mission is ‚Äúto create a world where people work to make a life, not just a living.‚Äù A mere office-rental company, after all, could hardly justify a $20 billion valuation. As the &lt;em&gt;Wall Street Journal&lt;/em&gt; noted, the office-leasing company &lt;span class=&quot;smallcaps&quot;&gt;IWG&lt;/span&gt; manages five times the square footage of WeWork but has one-eighth the market valuation. WeWork has found its investors by insisting that it is something different entirely‚Äî ‚Äúspace as service,‚Äù or a platform, or a culture. And it is now bringing that culture into disparate segments of modern life. WeGrow, a proposed private elementary school in one of WeWork‚Äôs New York City offices, aims to groom the next generation of entrepreneurs by teaching children about supply and demand. WeLive has created dorm-room-like apartment buildings in New York City and Washington, DC. The company is currently involved in building wave pools, opening gyms, and buying coding schools, all with the aim of creating ‚Äúa place where we‚Äôre redefining success measured by personal fulfillment, not just the bottom line.‚Äù&lt;/p&gt;
&lt;p&gt;And here, beneath the aspirational jargon, is a nugget of truth: WeWork is in the personal-fulfillment business. Because it‚Äôs offering a service that can be provided by anyone who can wrangle together a few desks and a French press, the product it‚Äôs actually selling is the contact high of being part of something that feels revolutionary. WeWork is promoting a mythology for those in the brave new gig economy: You, precarious worker who will never have a pension, are not a simple cog in a machine. You are an artist, the &lt;span class=&quot;smallcaps&quot;&gt;CEO&lt;/span&gt; of your own company, and the face of a dynamic personal brand. Your work is not merely labour, for which you deserve decent pay and security, but an extension of your personality. You‚Äôre doing what you love and paying $500 per month for the desk from which to do it.&lt;/p&gt;
&lt;p&gt;The appeal of that pitch can wear off quickly. When I spoke to Iconomopoulos in November, she told me that after three months at WeWork, she‚Äôd decided to move on. She had been trying to network‚Äî posting on the app, introducing herself in the common area, and even holding an event‚Äî but as a thirty-seven-year-old surrounded by enthusiastic people a decade younger, she felt old and slightly out of place. She looked at other co-working options and toured Workhaus and Verkspace (which takes its inspiration ‚Äúfrom the Scandinavian way of life‚Äù). The buildings, she said, all felt strangely familiar: they had the same open kitchens, the same glass dividers, the same safely ‚Äúoffbeat‚Äù art on the walls. Near the end of the month, she opted for a private office in Spaces, which is owned by &lt;span class=&quot;smallcaps&quot;&gt;IWG&lt;/span&gt;. The company seemed to be looking for a slightly older demographic and had belatedly adopted some Silicon Valley razzle-dazzle of its own, promising tenants entry into a community of ‚Äúthinkers, achievers and imagineers.‚Äù Perhaps personal fulfillment is a lot to ask of a workplace, but Iconomopoulos was going to give it another try.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;smallcaps&quot;&gt;uring my final&lt;/span&gt; week at WeWork, the building held a party. All six floors were crowded with tenants and guests eagerly drinking WeWork margaritas and awkwardly swaying to Drake. A young, blond exec cut the music for a moment to stand up on a riser and say how much she loved fulfilling the company‚Äôs mission. ‚ÄúThis is more like a bar or a club than a workspace,‚Äù said the local member of provincial parliament, taking in the scene. Entrepreneurial caterers handed out business cards along with their miniature cups of artisanal pho. This party was work, of course, just like work was always a party. I ate a plate of duck-rag√π pasta served on a pillow of cauliflower foam, drank a craft beer called Food Truck, and felt an inexplicable and totally disproportionate sense of despair.&lt;/p&gt;
&lt;p&gt;The next morning, my last at WeWork, the building felt collectively hungover. I wandered in at 10:30 and found the place nearly empty, the desks still pushed to the edges of the office. I drank my citrus water and listlessly checked my email. A member of the cleaning staff‚Äî a young Spanish-speaking woman with a tight ponytail‚Äî was one of the few people actually working. She moved quietly, picking up the dirty mugs that people had left lying about and stacking them into the dishwasher. Her shirt was emblazoned with the company slogan: Do What You Love.&lt;/p&gt;
</description>
<pubDate>Wed, 21 Feb 2018 15:33:36 +0000</pubDate>
<dc:creator>devy</dc:creator>
<og:title>Why It‚Äôs so Hard to Actually Work in Shared Offices</og:title>
<og:description>WeWork offers freelancers a chic workspace and beer on tap‚Äîbut are people productive?</og:description>
<og:type>article</og:type>
<og:url>https://thewalrus.ca/why-its-so-hard-to-actually-work-in-shared-offices/</og:url>
<og:image>https://s3.amazonaws.com/walrus-assets/img/WEB_Hune-Brown_WeWork_art.jpg</og:image>
<dc:language>en-CA</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://thewalrus.ca/why-its-so-hard-to-actually-work-in-shared-offices/</dc:identifier>
</item>
<item>
<title>Practical Tips for Cheating at Design</title>
<link>https://medium.com/refactoring-ui/7-practical-tips-for-cheating-at-design-40c736799886</link>
<guid isPermaLink="true" >https://medium.com/refactoring-ui/7-practical-tips-for-cheating-at-design-40c736799886</guid>
<description>&lt;p&gt;
&lt;h3 name=&quot;fc7a&quot; id=&quot;fc7a&quot; class=&quot;graf graf--h3 graf--leading&quot;&gt;7. Not every button needs a background color&lt;/h3&gt;
&lt;/p&gt;
&lt;div class=&quot;section-inner sectionLayout--outsetColumn&quot;&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*d4cSVthCYgX57KQjcw7SpA.png&quot; data-width=&quot;1530&quot; data-height=&quot;717&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*d4cSVthCYgX57KQjcw7SpA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/2000/1*d4cSVthCYgX57KQjcw7SpA.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;41&quot;&gt;
&lt;p name=&quot;e9ac&quot; id=&quot;e9ac&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;When there are multiple actions a user can take on a page, it‚Äôs easy to fall into the trap of designing those actions based purely on semantics.&lt;/p&gt;
&lt;p name=&quot;1828&quot; id=&quot;1828&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Frameworks like Bootstrap sort of encourage this by giving you a menu of semantic styles to choose from whenever you‚Äôre adding a new button:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*2xkDfSjvq7Xyb_ceInrMpw.png&quot; data-width=&quot;1562&quot; data-height=&quot;630&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*2xkDfSjvq7Xyb_ceInrMpw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*2xkDfSjvq7Xyb_ceInrMpw.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;e583&quot; id=&quot;e583&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--figure&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;‚ÄúIs this a positive action? Make the button green.‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;2e41&quot; id=&quot;2e41&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--p&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;‚ÄúDoes this delete data? Make the button red.‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;9a60&quot; id=&quot;9a60&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Semantics are an important part of button design, but there‚Äôs a more important dimension that‚Äôs commonly forgotten: &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;hierarchy.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;779b&quot; id=&quot;779b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Every action on a page sits somewhere in a pyramid of importance. Most pages only have one true primary action, a couple of less important secondary actions, and a few seldom used tertiary actions.&lt;/p&gt;
&lt;p name=&quot;286d&quot; id=&quot;286d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;When designing these actions, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;it‚Äôs important to communicate their place in the hierarchy.&lt;/strong&gt;&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;25d6&quot; id=&quot;25d6&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Primary actions should be obvious.&lt;/strong&gt; Solid, high contrast background colors work great here.&lt;/li&gt;
&lt;li name=&quot;f3a9&quot; id=&quot;f3a9&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Secondary actions should be clear but not prominent.&lt;/strong&gt; Outline styles or lower contrast background colors are great options.&lt;/li&gt;
&lt;li name=&quot;b61d&quot; id=&quot;b61d&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Tertiary actions should be discoverable but unobtrusive.&lt;/strong&gt; Styling these actions like links is usually the best approach.&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*CiEQ9mpfpfAXp62pTE2fbw.png&quot; data-width=&quot;1400&quot; data-height=&quot;722&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*CiEQ9mpfpfAXp62pTE2fbw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*CiEQ9mpfpfAXp62pTE2fbw.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;8d26&quot; id=&quot;8d26&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--figure&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;‚ÄúWhat about destructive actions, shouldn‚Äôt they always be red?‚Äù&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;93e5&quot; id=&quot;93e5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Not necessarily! If the destructive action isn‚Äôt the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;primary&lt;/em&gt; action on the page, it might be better to give it a secondary or tertiary button treatment.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*pIMOui8TSM_gUX9xNFyBng.png&quot; data-width=&quot;1400&quot; data-height=&quot;322&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*pIMOui8TSM_gUX9xNFyBng.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*pIMOui8TSM_gUX9xNFyBng.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;89a8&quot; id=&quot;89a8&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Save the big, red, and bold styling for when that negative action actually &lt;em class=&quot;markup--em markup--p-em&quot;&gt;is&lt;/em&gt; the primary action in the interface, like in a confirmation dialog:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*cuYcwjOO26sKHImHaY6yFA.png&quot; data-width=&quot;1400&quot; data-height=&quot;808&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*cuYcwjOO26sKHImHaY6yFA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*cuYcwjOO26sKHImHaY6yFA.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Wed, 21 Feb 2018 14:34:21 +0000</pubDate>
<dc:creator>marvinpinto</dc:creator>
<og:title>7 Practical Tips for Cheating at Design ‚Äì Refactoring UI ‚Äì Medium</og:title>
<og:url>https://medium.com/refactoring-ui/7-practical-tips-for-cheating-at-design-40c736799886</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*YgsvBILjjtgtW1vz2L0alA.png</og:image>
<og:description>Improving your designs with tactics instead of talent.</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/refactoring-ui/7-practical-tips-for-cheating-at-design-40c736799886</dc:identifier>
</item>
<item>
<title>Growing a company that sells miniature construction supplies to $17k/mo</title>
<link>https://www.starterstory.com/mini-materials</link>
<guid isPermaLink="true" >https://www.starterstory.com/mini-materials</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;https://www.starterstory.com/mini-materials&quot;&gt;https://www.starterstory.com/mini-materials&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=16429467&quot;&gt;https://news.ycombinator.com/item?id=16429467&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 613&lt;/p&gt;&lt;p&gt;# Comments: 249&lt;/p&gt;</description>
<pubDate>Wed, 21 Feb 2018 14:02:45 +0000</pubDate>
<dc:creator>patwalls</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.starterstory.com/mini-materials</dc:identifier>
</item>
<item>
<title>Physics Makes Aging Inevitable, Not Biology (2016)</title>
<link>http://nautil.us/issue/36/aging/physics-makes-aging-inevitable-not-biology</link>
<guid isPermaLink="true" >http://nautil.us/issue/36/aging/physics-makes-aging-inevitable-not-biology</guid>
<description>&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;he inside of every cell in our body is like a crowded city, filled with tracks, transports, libraries, factories, power plants, and garbage disposal units. The city‚Äôs workers are protein machines, which metabolize food, take out the garbage, or repair DNA. Cargo is moved from one place to another by molecular machines that have been observed walking on two legs along protein tightropes. As these machines go about their business, they are surrounded by thousands of water molecules, which randomly crash into them a trillion times a second. This is what physicists euphemistically call ‚Äúthermal motion.‚Äù Violent thermal chaos would be more apt.&lt;/p&gt;
&lt;p&gt;How any well-meaning molecular machine could do good work under such intolerable circumstances is puzzling. Part of the answer is that the protein machines of our cells, like tiny ratchets, turn the random energy they receive from water bombardment into the very directed motion that makes cells work. They turn chaos into order.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/9222_497d0b20f66cebdedc7935e3ffd46efa.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;Johner Images / Getty
&lt;p&gt;Four years ago, I published a book called &lt;em&gt;Life‚Äôs Ratchet&lt;/em&gt;, which explains how molecular machines create order in our cells. My main concern was how life avoids a descent into chaos. To my great surprise, soon after the book was published, I was contacted by researchers who study biological aging. At first I couldn‚Äôt see the connection. I knew nothing about aging except for what I had learned from being forced to observe the process in my own body.&lt;/p&gt;
&lt;p&gt;Then it dawned on me that by emphasizing the role of thermal chaos in animating molecular machines, I encouraged aging researchers to think more about it as a driver of aging. Thermal motion may seem beneficial in the short run, animating our molecular machines, but could it be detrimental in the long run? After all, in the absence of external energy input, random thermal motion tends to destroy order.&lt;/p&gt;
&lt;p&gt;This tendency is codified in the second law of thermodynamics, which dictates that everything ages and decays: Buildings and roads crumble; ships and rails rust; mountains wash into the sea. Lifeless structures are helpless against the ravages of thermal motion. But life is different: Protein machines constantly heal and renew their cells.&lt;/p&gt;
&lt;p&gt;In this sense, life pits biology against physics in mortal combat. So why do living things die? Is aging the ultimate triumph of physics over biology? Or is aging part of biology itself?&lt;/p&gt;
&lt;div class=&quot;reco&quot;&gt;
&lt;article class=&quot;issue-article&quot;&gt;&lt;div&gt;&lt;a href=&quot;http://nautil.us/issue/34/Adaptation/can-a-living-creature-be-as-big-as-a-galaxy&quot; class=&quot;obnd_lnk&quot; data-trval=&quot;can-a-living-creature-be-as-big-as-a-galaxy&quot; data-trlbl=&quot;foc_rec&quot; data-tract=&quot;internal_art&quot;&gt;&lt;img src=&quot;http://static.nautil.us/8765_f38fef4c0e4988792723c29a0bd3ca98.jpg&quot; alt=&quot;Sapolsky_TH-F1&quot; width=&quot;314&quot; height=&quot;177&quot;/&gt;&lt;/a&gt;&lt;/div&gt;


&lt;/article&gt;&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;I&lt;/span&gt;f there is a founding document for the modern study of aging, it may be &lt;em&gt;An Unsolved Problem of Biology&lt;/em&gt; by Sir Peter Medawar. Medawar was a Nobel Prize-winning biologist, as well as a witty and sometimes scathing writer of essays and books. In &lt;em&gt;An Unsolved Problem of Biology&lt;/em&gt;, Medawar pitted two explanations for aging against each other: On one hand was ‚Äúinnate senescence,‚Äù or aging as biological necessity. On the other was the ‚Äúwearing out‚Äù theory of aging‚Äîaging due to the ‚Äúaccumulated effects of recurrent stress.‚Äù The former is biology, the latter physics. Innate senescence implies that aging and death are dictated by evolution to make space for younger generations.&lt;/p&gt;
&lt;p&gt;The idea of innate senescence suggests that we have a master clock inside of us that counts down the hours of our lives. There are indeed clocks like this. The most famous are telomeres‚Äîlittle snippets of DNA which get shortened each time a cell divides. The study of telomeres has been controversial: It is not clear if telomere shortening is a cause or an effect of aging. Telomeres do not shorten in constant amounts‚Äîwhile there is a minimum amount that comes off at each cell division, they will shorten at a faster rate if the cell has been damaged through other means. Many researchers now believe that telomere shortening is more of a symptom of aging than its cause.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;Life pits biology against physics in mortal combat.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Medawar himself argued for the ‚Äúwearing out‚Äù theory‚Äîthe physics viewpoint on aging. First, he said, it is difficult to see how natural selection could have selected for senescence, because we don‚Äôt reproduce in our elderly years and natural selection is driven by differences in reproduction rates. Second, it is unnecessary to actively kill off older individuals to keep an aging population small. Random chance can accomplish this on its own.&lt;/p&gt;
&lt;p&gt;Medawar argued that a biological master clock for aging is unnecessary. To illustrate why, he pointed to a decidedly non-living example: Test tubes in a lab. Assume test tubes break from time to time by accident. To keep the total number of test tubes constant, a fresh supply is purchased every week. After a few months pass, how many young test tubes are there, and how many are old? If we assume that the probability of accidental breakage is independent of age (a sensible assumption), and plot the number of test tubes versus the age of each test tube, we get a concave, exponential decay curve that looks like a child‚Äôs slide. This ‚Äúlife curve‚Äù has a steep drop at the top, and it‚Äôs flat on the bottom.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/9212_8386fa112ba70c3f60b6907d3812bb9e.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;span class=&quot;caption&quot;&gt;&lt;strong&gt;Death Without Aging:&lt;/strong&gt; A computer simulated life curve for randomly breaking test tubes and an exponential fitting curve (in red). The vertical axis is the number of test tubes in each age group, and the horizontal axis is the age of test tubes in weeks.&lt;/span&gt;&lt;span class=&quot;credit&quot;&gt;Peter Hoffmann&lt;/span&gt;
&lt;p&gt;Although the test tubes are not aging (old test tubes do not break easier than young ones), the constant probability of breakage diminishes the number of old test tubes significantly. Now, suppose that humans, like test tubes, were equally likely to die at any age. The number of old people would still be small. Probability would catch up with us eventually.&lt;/p&gt;
&lt;p&gt;The trouble is, life curves plotted for human populations do not look like Medawar‚Äôs test tube curve. They start out rather flat at the top, with a small number of losses at young age (except at birth). Then at some age, the curve suddenly drops. To obtain such a curve, we need to add another assumption to Medawar‚Äôs test tube model: Test tubes must accumulate tiny cracks over time, increasing their risk of breaking. In other words, they must age. If the risk of breaking increases exponentially, we get something called the Gompertz-Makeham law. This law matches human life curves quite well. In the language of test tubes, the law includes both a constant and an exponentially increasing risk of breakage. This exponential increase has been observed in humans, for whom the risk of death doubles every seven years after age 30.&lt;/p&gt;
&lt;p&gt;What is the origin of this exponential increase? Thermal motion is not the only source of damage in our cells. Some regular processes, especially metabolism in our mitochondria, are not perfect and tend to &lt;a href=&quot;http://nautil.us/issue/36/aging/yes-life-in-the-fast-lane-kills-you&quot; target=&quot;_blank&quot;&gt;produce radicals&lt;/a&gt;‚Äîhighly reactive atoms that can damage DNA.¬† Together, thermal noise and free radical production constitute a background risk of cell damage. The damage is usually repaired, or, &lt;a href=&quot;http://nautil.us/issue/23/dominoes/the-executioner-we-cant-live-without&quot; target=&quot;_blank&quot;&gt;if a cell is deemed beyond repair&lt;/a&gt;, the cell is induced to commit suicide‚Äîa process called apoptosis. Usually, a stem cell replaces it.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;Eliminating cancer or Alzheimer‚Äôs disease would improve lives, but it would not make us immortal, or even allow us to live significantly longer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Eventually, though, the damage accumulates. DNA can only be repaired when there is an intact replica to copy. Damaged proteins unfold and start sticking to each other, forming aggregates. The cell‚Äôs defense and apoptosis mechanism become compromised. ‚ÄúSenescent cells‚Äù start accumulating in organs, leading to inflammation. Stem cells are not activated, or become depleted. Mitochondria become damaged, reducing energy supply in cells, which is needed to power the molecular machines repairing DNA. It‚Äôs a vicious cycle‚Äîor, in technical jargon, a positive feedback loop. Mathematically, this positive feedback loop leads to an exponential increase in risk, which can explain the shape of human life curves.&lt;/p&gt;
&lt;p&gt;The scientific literature is full of explanations for aging: Protein aggregation, DNA damage, inflammation, telomeres. But these are the biological responses to an underlying cause, which is accumulating damage through thermal and chemical degradation. To prove that thermal damage effects really do cause aging, we would need to observe humans living with different internal temperatures. This is not possible‚Äîbut there are organisms that can be subjected to various internal temperatures without immediate harm. In a recent paper in &lt;em&gt;Nature&lt;/em&gt;, a team at Harvard Medical School determined the temperature dependence of aging in the roundworm &lt;em&gt;C. elegans&lt;/em&gt;, a simple and well-studied creature. They found that the shape of the survival curve remained essentially the same, but it was stretched or contracted as the temperature was changed. Creatures raised at lower temperature enjoyed a stretched survival curve, while worms exposed to higher temperature lived shorter lives.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/9233_88052b22c8c2349c0599bd39a654c534.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;Kasman / Pixabay
&lt;p&gt;&lt;span&gt;What‚Äôs more, the stretch factor depended on temperature according to a pattern familiar to every scientist: It was the same as the dependence of the rate of chemical bond breakage on the temperature of random thermal motion.&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;I‚Äôve even seen a potential connection between bond breaking and human aging in my own lab. When I first encountered the Gompertz-Makeham law, it looked oddly familiar. In my lab we study the survival probability of single molecular bonds using an atomic force microscope, which can measure the minute forces acting between two molecules. In a typical experiment, we attach one protein to a flat surface and another to the tip of a small cantilever spring. We let the two proteins bind to each other, then slowly pull on the spring to apply an increasing force to the two molecules. Eventually, the bond between two molecules breaks, and we measure the force needed to achieve that breaking.&lt;/p&gt;
&lt;p&gt;This is a random process, initiated by thermal motion. Each time we do the experiment, the breakage force is different. But the survival probability of the bonds plotted against applied force looks just like human survival plotted versus age. The similarity resonates with the &lt;em&gt;C. elegans&lt;/em&gt; results, which suggest a possible connection between breaking protein bonds and aging‚Äîand between aging and thermal motion.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/9220_e4d09d3f57c2c971c8b2bf8efb416a0a.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;span class=&quot;caption&quot;&gt;&lt;strong&gt;A Common Death:&lt;/strong&gt; Left: Human life curve with Gompertz-Makeham fitting line. Right: Survival plot for single protein bonds subjected to increasing force. The mathematical form of the two curves is identical.&lt;/span&gt;&lt;span class=&quot;credit&quot;&gt;Peter Hoffmann&lt;/span&gt;

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;here is a vigorous discussion inside the aging research community about whether to classify aging as a disease. Many researchers studying specific diseases, cellular systems, or molecular components would like to see their favorite research subject take the mantle of ‚Äúthe cause‚Äù of aging. But the sheer number of possibilities being put forward refutes the very possibility. They can‚Äôt all be the cause of aging. Leonard Hayflick, the original discoverer of cellular aging, pointed out in his provocatively titled article ‚ÄúBiological Aging Is No Longer an Unsolved Problem‚Äù that the ‚Äúcommon denominator that underlies all modern theories of aging is change in molecular structure and, hence, function.‚Äù The ultimate cause, according to Hayflick, is an ‚Äúincreasing loss of molecular fidelity or increasing molecular disorder.‚Äù This loss of fidelity and increase in disorder will manifest itself‚Äîby its very nature‚Äîrandomly and therefore differently for different people. But the ultimate cause remains the same.&lt;/p&gt;
&lt;p&gt;If this interpretation of the data is correct, then aging is a natural process that can be reduced to nanoscale thermal physics‚Äîand not a disease. Up until the 1950s the great strides made in increasing human life expectancy, were almost entirely due to the elimination of infectious diseases, a constant risk factor that is not particularly age dependent. As a result, life expectancy (median age at death) increased dramatically, but the maximum life span of humans did not change. An exponentially increasing risk eventually overwhelms any reduction in constant risk. Tinkering with constant risk is helpful, but only to a point: The constant risk is environmental (accidents, infectious disease), but much of the exponentially increasing risk is due to internal wear. Eliminating cancer or Alzheimer‚Äôs disease would improve lives, but it would not make us immortal, or even allow us to live significantly longer.&lt;/p&gt;
&lt;p&gt;That doesn‚Äôt mean there is nothing we can do. More research into specific molecular changes in aging is needed. This may show us if there are key molecular components that are the first to break down, and whether that breakdown leads to the subsequent cascade of failure. If there are such key components, we would have clear targets for interventions and repair, possibly through nanotechnology, stem cell research, or gene editing. It‚Äôs worth a try. But we need to be clear about one thing: We‚Äôll never defeat the laws of physics.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Peter Hoffmann is a professor of physics at Wayne State University and the associate dean of research in the College of Liberal Arts and Sciences.&lt;/em&gt;&lt;/p&gt;
&lt;section class=&quot;leaderboard-ad-belt&quot;&gt;&lt;div class=&quot;leaderboard-ad-belt-inner adarticle&quot;&gt;&lt;div id=&quot;div-gpt-ad-1380044019755-0&quot; class=&quot;leaderboard-ad&quot;/&gt;
&lt;/div&gt;
&lt;/section&gt;</description>
<pubDate>Wed, 21 Feb 2018 11:14:36 +0000</pubDate>
<dc:creator>dnetesn</dc:creator>
<og:type>website</og:type>
<og:url>http://nautil.us/issue/36/aging/physics-makes-aging-inevitable-not-biology</og:url>
<og:title>Physics Makes Aging Inevitable, Not Biology - Issue 36: Aging - Nautilus</og:title>
<og:description>The inside of every cell in our body is like a crowded city, filled with tracks, transports, libraries, factories, power plants, and&amp;#8230;</og:description>
<og:image>http://static.nautil.us/9230_c879ec4dfeaa4d0f14f8f395a09941c2.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://nautil.us/issue/36/aging/physics-makes-aging-inevitable-not-biology</dc:identifier>
</item>
<item>
<title>Ask HN: What&amp;#039;s the best algorithms and data structures online course?</title>
<link>https://news.ycombinator.com/item?id=16428309</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=16428309</guid>
<description>&lt;tr class=&quot;athing comtr&quot; id=&quot;16428949&quot; readability=&quot;2.8082191780822&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429224&quot; readability=&quot;2.2020202020202&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429567&quot; readability=&quot;2.5510204081633&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.2755102040816&quot;&gt;&lt;tr readability=&quot;2.5510204081633&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Would you say this is good for someone who has recently graduated and has been in industry for a year or so to refresh on?&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430041&quot; readability=&quot;5.584487534626&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.792243767313&quot;&gt;&lt;tr readability=&quot;5.584487534626&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;120&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;They touch on a bunch of decently exotic data structures like van Emde Boas trees and things like cache oblivious data structures. If you're comfortable with data structures and algorithm design (which it sounds like you are, from your description) it should be accessible. At the end of the day it's just a graduate level CS course.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429462&quot; readability=&quot;1.21875&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429725&quot; readability=&quot;11.786480686695&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;5.8932403433476&quot;&gt;&lt;tr readability=&quot;11.786480686695&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;12.239806866953&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;24.059275521405&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I know its not an online course and its a relatively massive tome, but I'd recommend just working through &quot;Introduction to Algorithms&quot; by Cormen/Leiserston/Rivest/Stein [1].&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;This book has great explanations and exercises for everything you could want to learn from the basics of sorting and algorithmic design and analysis, to graph algorithms, linear programming, and dynamic programming.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;It lacks some degree of depth on more advanced topics, but if you work your way through it and actually implement what you read and do the exercises, you will be more than well enough equipped to take on just about any problem.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The key is going to be to actually implement what you read/learn, I think it might take you a little more time than watching an online course, but in the long run it will give you a much deeper knowledge of the material.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;1. &lt;a href=&quot;https://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262531968&quot; rel=&quot;nofollow&quot;&gt;https://www.amazon.com/Introduction-Algorithms-Thomas-H-Corm...&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430307&quot; readability=&quot;2.6007604562738&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16431147&quot; readability=&quot;12.650615901456&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;6.3253079507279&quot;&gt;&lt;tr readability=&quot;12.650615901456&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;12.650615901456&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;24.856651376147&quot;&gt;&lt;span class=&quot;c00&quot;&gt;&amp;gt; Isn't this book too academical for any practical learning of algorithms and data structures?&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I don't think so, I've worked through it and I didn't find it that difficult/academic. But I actually don't read a lot of computer science books / textbooks so I don't really have much to compare it to other than mathematical texts which I do read a lot of.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;If you don't like proofs or math then its probably not the best text to work through, on the other hand, if you like rigorously understanding the material I would highly recommend it.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Either way, from what I remember it gives psuedocode for just about everything and has lots of graphs and pictures for elucidating the material, so you could probably just skip the math if you have an allergy to corrolaries, theorems, and proofs. Admittedly, that extra insight is probably a lot of the reason I liked it so much. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430388&quot; readability=&quot;1.0522388059701&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429791&quot; readability=&quot;3.1742738589212&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430193&quot; readability=&quot;5.6212121212121&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.8106060606061&quot;&gt;&lt;tr readability=&quot;5.6212121212121&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;6.5580808080808&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;12.428940568475&quot;&gt;&lt;span class=&quot;c00&quot;&gt;It's not really a course but I found InterviewBit [0] a great resource. It structures everything around moving from simpler to more complex data structures and algorithms. It's kind of learning by doing lots and lots of questions. You have to complete enough problems in one section to move onto the next.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I spent a hundred hours going through the various tests and it showed the power of functional programming / recursion. The questions that took me much longer than average were the ones where I had a bug and had to track it down. The recursive problems didn't suffer from this, ie I would get the solution pretty much correct first time. This could just have been the questions / my programming style but I found it eye opening.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;[0]: &lt;a href=&quot;https://www.interviewbit.com/&quot; rel=&quot;nofollow&quot;&gt;https://www.interviewbit.com/&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16428392&quot; readability=&quot;1.6094420600858&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429258&quot; readability=&quot;2.0736434108527&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429643&quot; readability=&quot;3.6666666666667&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.8333333333333&quot;&gt;&lt;tr readability=&quot;3.6666666666667&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;3.6666666666667&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;6.86328125&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Tim Roughgarden is a fantastic teacher. I personally love his style and speed. He throws in some humor here and there and makes learning a lot of fun. His lectures on graphs are absolutely brilliant.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;This same course is available on Coursera as well. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430810&quot; readability=&quot;3.0128205128205&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429813&quot; readability=&quot;4.2583732057416&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.1291866028708&quot;&gt;&lt;tr readability=&quot;4.2583732057416&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Yes. Same as Coursera. I took this on Coursera only, before specialization thing happened. Don't know if the course videos and exercises are fully available for audit purpose.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16428486&quot; readability=&quot;4.3681318681319&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.1840659340659&quot;&gt;&lt;tr readability=&quot;4.3681318681319&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Thanks, I've looked at the syllabus and it seems to be exactly what I'm looking for (Asymptotic analysis, and coverage of the most widely known algorithms).&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16428939&quot; readability=&quot;2.4893617021277&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.2446808510638&quot;&gt;&lt;tr readability=&quot;2.4893617021277&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Second this recommendation really enjoyed these courses as well. Tim explains everything in an easy to follow way.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429778&quot; readability=&quot;1.5257731958763&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430232&quot; readability=&quot;7.5578635014837&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.7789317507418&quot;&gt;&lt;tr readability=&quot;7.5578635014837&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;3.3590504451039&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;6.2246835443038&quot;&gt;&lt;span class=&quot;c00&quot;&gt;If your eventual goal after learning the basics is programming questions for interviews, there are a tons of resources like leetcode, interviewbit, geeksforgeeks. I started writing some of the FAQ with explanations here[1], check it out to see if it is of any help for you.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;[1] : &lt;a href=&quot;http://letstalkalgorithms.com/&quot; rel=&quot;nofollow&quot;&gt;http://letstalkalgorithms.com/&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430941&quot; readability=&quot;1.3636363636364&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16431596&quot; readability=&quot;2.5&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.25&quot;&gt;&lt;tr readability=&quot;2.5&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Yeah I'm currently going through this course. Some of it is going over my head but I get the basic concepts&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429950&quot; readability=&quot;3.8449848024316&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429987&quot; readability=&quot;3.3852140077821&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430542&quot; readability=&quot;0.93&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430498&quot; readability=&quot;2.8109452736318&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430120&quot; readability=&quot;1.6461538461538&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16431498&quot; readability=&quot;1.3953488372093&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16429766&quot; readability=&quot;4.5016611295681&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.2508305647841&quot;&gt;&lt;tr readability=&quot;4.5016611295681&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Is it wise to do any of these courses without taking a course in linear algebra first? I started watching Skiena's algorithm course on youtube recently and he seemed to really emphasize finishing a course in linear algebra before taking any algorithm course. Thoughts?&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430732&quot; readability=&quot;2.0913705583756&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430535&quot; readability=&quot;3.1666666666667&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430081&quot; readability=&quot;2.2413793103448&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430111&quot; readability=&quot;5.534328358209&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.7671641791045&quot;&gt;&lt;tr readability=&quot;5.534328358209&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Linear algebra isn‚Äôt a prerequisite for learning data structures and algorithms. Though you typically do find linear algebra to be part of the lower division computer science curriculum at most colleges. I would recommend you take a course in it, I argue it expands one‚Äôs problem-solving mental models.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430624&quot; readability=&quot;5.3497536945813&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.6748768472906&quot;&gt;&lt;tr readability=&quot;5.3497536945813&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;120&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I certainly agree. However, if OP is simply referring to learning the typical algorithms presented in an undergraduate CS course, then linear algebra isn't a strict prerequisite.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;16430298&quot; readability=&quot;3.4146341463415&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.7073170731707&quot;&gt;&lt;tr readability=&quot;3.4146341463415&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I took both of Tim Roughgarden's algorithm courses on Coursera, and in my opinion you don't need to know linear algebra to complete them.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Wed, 21 Feb 2018 10:03:58 +0000</pubDate>
<dc:creator>zabana</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=16428309</dc:identifier>
</item>
<item>
<title>Why has CPU frequency ceased to grow? (2014)</title>
<link>https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow</link>
<guid isPermaLink="true" >https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow</guid>
<description>&lt;p&gt;&lt;span class=&quot;floatLeft&quot;&gt;&lt;img hspace=&quot;10&quot; class=&quot;floatLeft&quot; src=&quot;https://software.intel.com/sites/default/files/managed/45/fa/cpu-frequency-image1.jpg&quot; alt=&quot;&quot;/&gt;&lt;/span&gt; All of you probably recall the rapid rate of CPU frequency advancement at the end of the last century and beginning of this one. ¬†Tens of megahertz rapidly transformed into hundreds, and then hundreds of megahertz quickly became a full gigahertz, then a gigahertz and a bit, finally two gigs and a bit.&lt;/p&gt;
&lt;p&gt;However, in the last few years, we‚Äôve seen the core CPU frequency growth has slowed. The 10 GHz result is still as unreachable now as it was five years ago. Why the slow down? What is the obstacle for an increasingly expanding rate of frequency?&lt;/p&gt;
&lt;h2&gt;&quot;Hot&quot; gigahertz&lt;/h2&gt;
&lt;p&gt;There is an opinion among experts that increased frequency growth will result in highly significant heat emissions. Others think that you can just turn &quot;a switch&quot; that will increase the frequency ‚Äì and it will be increased as desired. But there are also strong concerns that the increased frequency will raise the CPU temperature so much that it will cause an actual physical melt down. ¬†Note that many CPU manufactures will not allow a meltdown to happen, as the CPU has internal temperature monitors and will shut down the CPU before any catastrophic failure occurs.&lt;/p&gt;
&lt;p&gt;This opinion is expressed by computer users and, moreover, it has been proven by overclockers‚Äô successes, as they speed up the processors two and more times as fast, they need to attach as powerful a cooling system as possible.&lt;/p&gt;
&lt;p&gt;We should validate that the &quot;switch&quot; mentioned above actually exists, as well as the heat emission problem, but these are just part of the battle for expanding gigahertz.&lt;/p&gt;
&lt;h2&gt;The main brake&lt;/h2&gt;
&lt;p&gt;Different processor architectures have their own difficulties with overclocking. Specifically here we‚Äôll focus on superscalar architecture including the x86 architecture, which is the most popular among Intel¬Æ products.&lt;/p&gt;
&lt;p&gt;To address the problems related to frequency growth, it‚Äôs important to identify what prevents its development. Depending on architecture research level, there are a variety of limiting parameters. However, there‚Äôs one area of research that focuses on one parameter, which means that there is the only one limitation or main brake that needs to be removed in order to increase frequency.&lt;/p&gt;
&lt;h2&gt;The conveyor&lt;/h2&gt;
&lt;p&gt;The main limitation is found in the conveyor level, which is integral to superscalar structure. Functionally, every execution of a processor‚Äôs instruction is divided into several steps as illustrated by the diagram below.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://software.intel.com/sites/default/files/managed/2c/0e/cpu-frequency-image2.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;These steps follow each other sequentially, and each is executed on a separate computing device.&lt;/p&gt;
&lt;p&gt;When execution of a specific step is completed, the computing device can then be used to execute a different instruction.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://software.intel.com/sites/default/files/managed/42/f9/cpu-frequency-image3.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;As you see on the diagram above, the first computing device executes the first step of the first instruction during the &lt;strong&gt;t1&lt;/strong&gt; time period. By the beginning of the &lt;strong&gt;t2&lt;/strong&gt; period, the first step has been completed and the second step can begin on the second device. The first device is now free and ready to begin the first step of next instruction, and so on. During the &lt;strong&gt;t4&lt;/strong&gt; period, different steps of four instructions can be executed.&lt;/p&gt;
&lt;p&gt;What does this have to do with frequency?¬† Actually, different stages can vary in execution time. At the same time, different steps of the same instruction are executed during different clock ticks. Clock tick length (and frequency as well) of the processor should fit the longest step. The diagram below shows the longest step is the third.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://software.intel.com/sites/default/files/managed/85/92/cpu-frequency-image4.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;There‚Äôs no advantage in setting the clock tick length shorter than the longest step, even though it is possible technologically, as no actual processor acceleration will occur.&lt;/p&gt;
&lt;p&gt;Suppose that the longest step requires 500 ps (picosecond) for execution. This is the clock tick length when the computer frequency is 2 GHz. Then, we set a clock tick two times shorter, which would be 250 ps, and everything but the frequency remains the same. Now, what was identified as the longest step is executed during two clock ticks, which together takes 500 ps as well. Nothing is gained by making this change while designing such a change becomes much more complicated and heat emission increases.&lt;/p&gt;
&lt;p&gt;One could object to this and note that due to shorter clock ticks, the small steps will be executed faster, so the average speed will be greater. However, the following diagram shows that this is not the case.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://software.intel.com/sites/default/files/managed/36/a2/cpu-frequency-image5.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Initially, the execution will be much faster. But, beginning from the fourth clock tick, the third step and all of the following steps in our example will be delayed. This happens because the third computing device will be free every two clock ticks, not every clock tick. While it is busy with the third step of one instruction, the same step of another instruction cannot be executed. So, our hypothetical processor that uses 250 ps clock ticks will work at the same speed as the 500 ps processor, though nominally its frequency is two times higher.&lt;/p&gt;
&lt;h2&gt;The smaller the better&lt;/h2&gt;
&lt;p&gt;So, from the conveyor point of view, the only way to raise the frequency is to shorten the longest step. If we can reduce the longest step, there is a possibility to decrease the clock tick size up to this step‚Äîand, the smaller the clock tick, the higher the frequency.&lt;/p&gt;
&lt;p&gt;There are not many ways to influence the step length using available technologies. One of these ways is to develop a more advanced technological process. By reducing the physical size of the components of a processor, the faster it works. This happens because electrical impulses have to travel shorter distances, transistor switch time decreases, etc. Simply stated, everything speeds up uniformly. All steps are shortened uniformly, including the longest one, and the frequency can be increased as a result.&lt;/p&gt;
&lt;p&gt;It sounds quite simple, but the way down the nanometer scale is very complicated. Increased frequency depends heavily on the current level of technology and advances cannot move beyond these physical limitations. Nevertheless, processor manufacturers are continuously improving the technological processes, so the core CPU frequency is gradually increasing.&lt;/p&gt;
&lt;h2&gt;Cut the patient&lt;/h2&gt;
&lt;p&gt;Another way to raise the frequency in the example above is to divide-up the longest step into smaller steps. The instructions have been cut already. They have been cut several times successfully. Why not go on? The processor will work even faster!¬† Much work has been done by the processor architects to make the steps as efficient as possible and thus further dividing the steps into smaller steps will not only create a challenge, it may significantly impact overall CPU efficiency.&lt;/p&gt;
&lt;p&gt;Let‚Äôs use an analogy to building a house. A house is built floor by floor. We‚Äôll assume a floor is analogous¬† ¬†to an instruction.¬† We‚Äôd like to divide building the floor into several parts. Initially starting with two parts; building of the floor itself and the finishing of that floor. While the finishing is being completed on the previous floor built, we can begin to build another floor, but only if the building and the finishing are performed by different teams. Sounds good.&lt;/p&gt;
&lt;p&gt;Now, let‚Äôs divide the two existing parts. Let‚Äôs split the finishing component into ceiling painting and wall papering. Easy enough. If the painters have finished one floor, they can go to another built floor, even if the paper-hangers haven‚Äôt completed their work on the first floor.&lt;/p&gt;
&lt;p&gt;And what about the actual building of the floors? For example, we‚Äôd like to divide the house building into wall building and ceiling building. We can do that but it isn‚Äôt useful to do so. We cannot build the walls of the next floor if the previous floor isn‚Äôt built. While we‚Äôve made the division theoretically, we cannot fully employ wall and ceiling teams since at any given time, only one team can be working!&lt;/p&gt;
&lt;p&gt;The same problem is true in processors. There are some steps that are dependent on other steps, and it‚Äôs very hard to divide such steps as that would require extensive changes in processor architecture, as would be required to build several floors of a house at the same time.&lt;/p&gt;
&lt;h2&gt;To flip the switch&lt;/h2&gt;
&lt;p&gt;Let‚Äôs address overclockers now. They raise processor voltage for transistors to switch quicker, all the steps become shorter, and the frequency can be increased. It sounds so easy! But there are huge problems with heat emission. Here is the simplified formula of a processor‚Äôs power dissipation:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;P ~ C&lt;sub&gt;dyn&lt;/sub&gt;*V&lt;sup&gt;2&lt;/sup&gt;*f&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;P = Power, C&lt;sub&gt;dyn&lt;/sub&gt; = dynamic capacitance, V = voltage, f = frequency&lt;/p&gt;
&lt;p&gt;Don‚Äôt worry if you don‚Äôt know what &lt;em&gt;dynamic capacitance&lt;/em&gt; is‚Äîthe main thing that‚Äôs important here is voltage. It is squared! Looks awful‚Ä¶&lt;/p&gt;
&lt;p&gt;The reality is even worse. As stated before, voltage makes transistors work. A transistor is a kind of toggle. It needs to accumulate some charge to switch. The accumulation time is proportional to current, so if the current is big, a charge moves quicker. Current, in turn, is proportional to voltage.¬† So, the transistor switch speed is proportional to voltage. We need to take into consideration that processor frequency can be raised proportionally to transistor switch speed only. Let‚Äôs summarize:&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;f ~ V and P ~ C&lt;sub&gt;dyn&lt;/sub&gt;*V&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Linear frequency growth causes power dissipation to be increasingly cubed! If the frequency is raised only twice, there will be eight times greater heat that must be accommodated or the processor will melt or shutdown.&lt;/p&gt;
&lt;p&gt;It‚Äôs obvious that this method of increasing the frequency is not suitable for processor manufacturers because of low efficiency. However, it is used by extreme overclockers.&lt;/p&gt;
&lt;h2&gt;Is that all?&lt;/h2&gt;
&lt;p&gt;There are instances when processor frequency has been increased a bit without voltage changing. It is possible in a very limited range, since processors are designed to work in widely varied conditions (which influences step length), so there is some frequency margin. For example, the longest step might take only 95% of the whole clock tick. This raises the possibility.¬† But remember, wrong overclocking can harm not only processor but you as well.&lt;/p&gt;
&lt;p&gt;There are some other ways to influence the step length which are much less important than what‚Äôs been discussed here. For example, temperature influences all electronic parts, but serious effects are seen only when temperature is very low.&lt;/p&gt;
&lt;p&gt;In conclusion, the struggle for increased frequency is extremely challenging. However, it is in progress, even though the frequency is increasing very slowly. But, take heart! Now that there are multicore processors, there is no reason why computers shouldn‚Äôt begin to work faster, whether due to higher frequency or because of parallel task execution. And with parallel task execution it provides even greater functionality and flexibility!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original Russian blog with threads:&lt;/strong&gt;&lt;a href=&quot;http://habrahabr.ru/company/intel/blog/194836/&quot; rel=&quot;nofollow&quot;&gt;http://habrahabr.ru/company/intel/blog/194836/&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 21 Feb 2018 08:13:53 +0000</pubDate>
<dc:creator>Osiris</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://software.intel.com/en-us/blogs/2014/02/19/why-has-cpu-frequency-ceased-to-grow</dc:identifier>
</item>
<item>
<title>Google AMP Is a Threat to the Open Web</title>
<link>https://www.socpub.com/articles/chris-graham-why-google-amp-threat-open-web-15847</link>
<guid isPermaLink="true" >https://www.socpub.com/articles/chris-graham-why-google-amp-threat-open-web-15847</guid>
<description>&lt;div readability=&quot;33&quot;&gt;
&lt;p&gt;&lt;em&gt;The lead developer for Composr CMS isn't too happy with¬†Google's solution for speeding up mobile pages.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;164.74647297109&quot;&gt;
&lt;p&gt;Like others in the IT community, I have become increasingly concerned with Google's behavior with their AMP technology.&lt;/p&gt;
&lt;p&gt;For those who aren't really aware what AMP is, it's Google's proprietary solution for speeding up mobile pages. Webmasters implement AMP, which is a kind of Google-sanctified and Google-code-driven version of your webpage. It works very well and solves an important problem. Often when you search Google on a mobile device it will show AMP articles at the top of the results, so you get access to fast content first, which is reasonable in and of itself.&lt;/p&gt;
&lt;p&gt;Unfortunately AMP is actually a massive power-grab by Google that threatens the Open Web in a major way. It is reminiscent of the anti-competitive and monopolistic practices Microsoft used to pull on us in the 90s.&lt;/p&gt;
&lt;p&gt;Here are the main issues many have with AMP:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1) Developed in secret and controlled by Google.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For decades we have used open practices for developing new web standards. We have RFCs (public consultation on proposed standards) and various standards bodies such as the W3C. This is really important as it allows fair collaboration on the standards we are all expected to use. That leads to better standards that are free of bias.&lt;/p&gt;
&lt;p&gt;It's never perfect. Often technologies are deployed by corporations before being standardized, but they are rarely aggressively pushed upon us and almost never fully and irrevocably dependent on one particular company's private infrastructure.&lt;/p&gt;
&lt;p&gt;Google developed AMP in secret alongside news corporations who presumably all signed NDAs forbidding it being discussed publicly. By the time it was made public a number of major corporations had already deployed AMP, and Google's spec and infrastructure was basically settled and non-negotiable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Forced on users.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Google is prioritizing AMP in their search results. Not fast pages in general, only AMP. There could be a page consisting of plain HTML with no CSS and JavaScript, and it would display after AMP on the Google search engine.&lt;/p&gt;
&lt;p&gt;This is monopolistic behaviour. Microsoft once got forced to not push Internet Explorer hard on their customers, which they were bundling with Windows. This is exactly the same situation: Google are trying to force people to use their AMP technology by virtue of how popular their search engine is.&lt;/p&gt;
&lt;p&gt;Some might argue that you can still choose your search engine and you can choose which results you click. This may be true, but the same was true of Microsoft - you could choose to use any web browser even while Internet Explorer was bundled. A monopoly isn't defined in terms of whether you do or don't have choice, it's defined by market share. Google is aggressively using their overwhelming market share in one area to take over market share in another area. It doesn't matter so much what you theoretically can do, it matters what people en-masse actually will be led to do. This is exactly why we chose as nations to enact monopoly laws: we can't have big corporations spreading to the extent that effective user choice is extinguished¬†at some point in the future.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) You never leave google.com.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you click an AMP result then you are not visiting a website, you are visiting a cached copy on google.com. Similar to viewing a regular cached web page on Google.&lt;br/&gt;This is a complete power grab that puts Google at the center¬†of everything.&lt;/p&gt;
&lt;h2&gt;What Google needs to do to 'not be evil'&lt;/h2&gt;
&lt;p&gt;There is little that is technically wrong with AMPs approach at the level of implementing the technology on a website. So to make AMP palatable Google just need to:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1) Transfer control.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Google needs to transfer full control of the AMP specification to the W3C, enabling industry and public participation. Google can then provide staff to work on the specs but it has to be through this public forum and open to others (including from Google's competitors, small businesses, and enthusiasts).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Fair search results.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Google must not put a preference on its own tech in search results. It is legitimate to prioritize faster sites and to encourage web developers to not create slow bloated sites, but it is not legitimate for a monopoly to favour its own technology.&lt;/p&gt;
&lt;p&gt;Google should prioritize based on actual page speed. If someone makes a super-fast page in Atom/RSS with an XSLT stylesheet (for example), that should show just as high as any AMP result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3) Allow control of the cache layer.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;AMP users must be able to either opt-out of Google caching (i.e. Google's hosting of copies of your pages), or specify their own cache server for Google to have mobile users prefetch against.&lt;/p&gt;
&lt;p&gt;The whole AMP stack must be a software stack anyone can install, not something only hosted in Google-land. It must ultimately be as simple as installing a pre-built Debian package, with all dependencies managed automatically.¬†This is how we always did it - the early web pioneers contributed to projects like Apache and bind so that we were all together as equals on an open system. We must not give that philosophy up.&lt;/p&gt;
&lt;p&gt;If Google are concerned about users hosting broken AMP stacks they can just deprioritize search results for such scenarios, just like they must already do for broken Apache (or IIS, whatever) web servers that don't respond properly.&lt;/p&gt;
&lt;p&gt;Google must not be allowed to flip the paradigm where they replace open technology systems with systems that all flow through Google.&lt;/p&gt;
&lt;h2&gt;Pre-emptying criticism:&lt;/h2&gt;
&lt;p&gt;The most common defences of Google's AMP are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1) Developers were not solving the problem and it needed to be solved.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It was always reasonable for Google to force developers hand by prioritizing the faster pages. The poor performance of a fraction of developers must not be an excuse for Google to take a massive power grab.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2) Google had to respond to Facebook and Apple.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Both Facebook and Apple have developed their own techniques for faster page loading.&lt;/p&gt;
&lt;p&gt;However, these companies are not the gatekeepers to an Open Web. Users don't search out content directly on Facebook, they discover it - it's a totally different paradigm, and one Facebook created for themselves. Apple has developed a technology based on existing open standards, and are not forcing people to use Apple's systems in the same way. Apple is also a minor player compared to Google. You cannot compare these things to what Google is doing. That's not to say we can't find other strong reasons to criticize Apple and Facebook.&lt;/p&gt;
&lt;p&gt;There has been far too little noise against Google for their behaviour on this. This shows just how successful Google have been in spreading their tentacles and priming people to depend on them. People love their masters if their masters feed and cloth them well. Developers should be cognisant that you should not go along with technologies that provide short-term advantages in return for the long-term stifling of freedom. Don't let companies take your power bit-by-bit, don't cede control of an open platform just because a company provides you nice things and makes overtures to the Open Source community.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Author Bio:&lt;/strong&gt;¬†I am the lead developer for¬†&lt;a href=&quot;https://compo.sr/&quot;&gt;Composr CMS&lt;/a&gt;. Composr is a feature-rich website engine, optimized for ambitious folks who fall somewhere between newbie and coder.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Wed, 21 Feb 2018 06:37:36 +0000</pubDate>
<dc:creator>zodiakzz</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.socpub.com/articles/chris-graham-why-google-amp-threat-open-web-15847</dc:identifier>
</item>
<item>
<title>Mountain View approves 10,000 homes by Google&amp;#039;s North Bayshore project (2017)</title>
<link>https://www.bizjournals.com/sanjose/news/2017/12/13/mountain-view-google-north-bayshore-approval.html</link>
<guid isPermaLink="true" >https://www.bizjournals.com/sanjose/news/2017/12/13/mountain-view-google-north-bayshore-approval.html</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bizjournals.com/sanjose/news/2017/12/13/mountain-view-google-north-bayshore-approval.html&quot;&gt;https://www.bizjournals.com/sanjose/news/2017/12/13/mountain-view-google-north-bayshore-approval.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=16427003&quot;&gt;https://news.ycombinator.com/item?id=16427003&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 230&lt;/p&gt;&lt;p&gt;# Comments: 195&lt;/p&gt;</description>
<pubDate>Wed, 21 Feb 2018 05:08:30 +0000</pubDate>
<dc:creator>luu</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bizjournals.com/sanjose/news/2017/12/13/mountain-view-google-north-bayshore-approval.html</dc:identifier>
</item>
</channel>
</rss>