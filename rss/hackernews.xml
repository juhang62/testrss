<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Lucet: Native WebAssembly Compiler and Runtime</title>
<link>https://www.fastly.com/blog/announcing-lucet-fastly-native-webassembly-compiler-runtime</link>
<guid isPermaLink="true" >https://www.fastly.com/blog/announcing-lucet-fastly-native-webassembly-compiler-runtime</guid>
<description>&lt;p&gt;Today, we are thrilled to announce the &lt;a href=&quot;https://github.com/fastly/lucet&quot; title=&quot;Lucet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;open sourcing of Lucet&lt;/a&gt;, Fastly’s native WebAssembly compiler and runtime. WebAssembly is a technology created to enable web browsers to safely execute programs at near-native speeds. It has been shipping in the four major browsers since early 2017.&lt;/p&gt;
&lt;p&gt;Lucet is designed to take WebAssembly beyond the browser, and build a platform for faster, safer execution on Fastly’s edge cloud. WebAssembly is already supported by many languages including Rust, TypeScript, C, and C++, and many more have WebAssembly support in development. We want to enable our customers to go beyond &lt;a href=&quot;https://docs.fastly.com/vcl/&quot; title=&quot;VCL&quot;&gt;Fastly VCL&lt;/a&gt; and move even more logic to the edge, and use any language they choose. Lucet is the engine behind Terrarium, our experimental platform for edge computation using WebAssembly. Soon, we will make it available on Fastly’s edge cloud as well.&lt;/p&gt;
&lt;p&gt;A major design requirement for Lucet was to be able to execute on every single request that Fastly handles. That means creating a WebAssembly instance for each of the tens of thousands of requests per second in a single process, which requires a dramatically lower runtime footprint than possible with a browser JavaScript engine. Lucet can instantiate WebAssembly modules in under 50 microseconds, with just a few kilobytes of memory overhead. By comparison, Chromium’s V8 engine takes about 5 milliseconds, and tens of megabytes of memory overhead, to instantiate JavaScript or WebAssembly programs.&lt;/p&gt;
&lt;p&gt;With Lucet, Fastly’s edge cloud can execute tens of thousands of WebAssembly programs simultaneously, in the same process, without compromising security. The Lucet compiler and runtime work together to ensure each WebAssembly program is allowed access to only its own resources. This means that Fastly’s customers will be able to write and run programs in more common, general-purpose languages, without compromising the security and safety we’ve always offered.&lt;/p&gt;
&lt;p&gt;Lucet separates responsibility for executing WebAssembly into two components: a compiler, which compiles WebAssembly modules to native code, and a runtime which manages resources and traps runtime faults. Lucet is designed for ahead-of-time (AOT) compilation of WebAssembly to native code, which dramatically simplifies the design and overhead of the runtime compared to the just-in-time (JIT) compilation strategy employed in browser engines.&lt;/p&gt;
&lt;h2&gt;How we built Lucet&lt;/h2&gt;
&lt;p&gt;Lucet is built on top of the &lt;a href=&quot;https://github.com/CraneStation/cranelift&quot; title=&quot;Cranelift code generator&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Cranelift code generator&lt;/a&gt;. The Cranelift project was created by Mozilla for use in Firefox’s WebAssembly and JavaScript JIT engines, and presently can be enabled by a &lt;a href=&quot;https://www.reddit.com/r/rust/comments/9mvnrk/in_firefox_nightly_an_option_has_arrived_to_use/&quot; title=&quot;Reddit Firefox Nightly&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;preference flag in Firefox Nightly&lt;/a&gt;. We &lt;a href=&quot;https://github.com/CraneStation/cranelift/graphs/contributors&quot; title=&quot;Cranelift Github&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;contributed&lt;/a&gt; to the design and implementation of Cranelift, and are excited that our efforts help make the web better for Firefox users as well.&lt;/p&gt;
&lt;p&gt;Lucet supports the &lt;a href=&quot;https://wasi.dev&quot; title=&quot;WASI&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;WebAssembly System Interface (WASI)&lt;/a&gt; — a new proposed standard for safely exposing low-level interfaces to the filesystem, networking, and other system facilities to WebAssembly programs. The Lucet team has partnered with Mozilla and others on the design, implementation, and standardization of this system interface.&lt;/p&gt;
&lt;p&gt;We’ve been working on this project behind the scenes since 2017, so we’re thrilled to finally make it public. Lucet also happens to be the first project started at Fastly using the &lt;a href=&quot;https://www.rust-lang.org/&quot; title=&quot;Rust&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Rust programming language&lt;/a&gt;, and we’re happy to report that Rust has been a huge success on this project. We found that new Rust users were able to become productive with the language quickly, and the library ecosystem provides many mature libraries for working with WebAssembly. Early in Lucet’s development, we implemented the first version of the runtime in C. However, we recently went back and translated the C runtime into Rust, and in the process, discovered and fixed several safety and concurrency bugs.&lt;/p&gt;
&lt;p&gt;As the engine behind the Terrarium project, Lucet has gotten months of production testing, running many thousands of different WebAssembly programs since launching in late 2018. It has also been the subject of an in-depth third-party security assessment.&lt;/p&gt;
&lt;h2&gt;A quick demo of Lucet&lt;/h2&gt;
&lt;p&gt;First, clone the Lucet repository from GitHub.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ git clone --recurse-submodules https://github.com/fastly/lucet&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/fastly/lucet/blob/master/README.md&quot; title=&quot;README&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;README&lt;/a&gt; contains instructions on using Docker to setup a development environment. If you already have Docker installed, there is just one step. It may take a few minutes to complete building everything.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ cd lucet  
$ source devenv_setenv.sh
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now, lets create a small C program, and use Clang to compile it to WebAssembly:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ mkdir demo
$ cd demo
$ cat &amp;gt; hello.c &amp;lt;&amp;lt;EOT

#include &amp;lt;stdio.h&amp;gt;
int main(int argc, char* argv[])
{
    if (argc &amp;gt; 1) {
            printf(&quot;Hello from Lucet, %s!\n&quot;, argv[1]);
    } else {
            puts(&quot;Hello, world!&quot;);
    }
    return 0;
}
EOT
$ wasm32-unknown-wasi-clang hello.c -o hello.wasm
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now we can compile the WebAssembly to native code, using the Lucet compiler, configured for use with WASI:&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$ lucetc-wasi hello.wasm -o hello.so&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Finally, we can execute the native code using the Lucet runtime configured for use with WASI:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ lucet-wasi hello.so  
Hello, world!  
$ lucet-wasi hello.so world  
Hello from Lucet, world!  
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Documentation and additional examples are available in the Lucet repository.&lt;/p&gt;
&lt;h2&gt;Beyond the edge cloud&lt;/h2&gt;
&lt;p&gt;We are excited to open source Lucet because of all the possibilities WebAssembly holds beyond the web browser and edge cloud. For instance, Lucet’s support for WASI is a big step towards WebAssembly programs that can run on whatever platform the user wants — in the cloud, at the edge, on the browser, or natively on your laptop or smartphone — all while keeping the same strong guarantees about security in place. We want to enable WebAssembly to thrive inside any program that allows scripting or extensions, while using fewer resources than current solutions built on dynamic languages, interpreters, and JIT compilers.&lt;/p&gt;
&lt;p&gt;Most importantly, we want to collaborate with the open-source community on Lucet, Cranelift, WASI, and other WebAssembly-enabling technologies. Fastly is built on, and &lt;a href=&quot;https://www.fastly.com/open-source&quot; title=&quot;Open source&quot;&gt;committed to supporting&lt;/a&gt;, open source. Without it, we could never have built Lucet — and we hope that Lucet allows you to build new things that we haven’t even dreamed of.&lt;/p&gt;

&lt;h3&gt;&lt;span data-swiftype-index=&quot;false&quot;&gt;&lt;span&gt;You may also like:&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;section class=&quot;section&quot;&gt;&lt;div class=&quot;cards&quot; readability=&quot;18.5&quot;&gt;
&lt;article class=&quot;card__article&quot; readability=&quot;27&quot;&gt;
&lt;div id=&quot;newsletter-form-content&quot; readability=&quot;32&quot;&gt;
&lt;p class=&quot;post-title&quot;&gt;&lt;span class=&quot;card&quot;&gt;Subscribe to our newsletter&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;post-meta&quot;&gt;&lt;span class=&quot;card&quot;&gt;Get the latest news and industry insights in your inbox.&lt;/span&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;newsletter-form-confirm&quot; class=&quot;hide&quot; readability=&quot;7&quot;&gt;
&lt;p class=&quot;post-title&quot;&gt;Subscribe to our newsletter&lt;/p&gt;
&lt;p class=&quot;post-meta&quot;&gt;Thanks for subscribing&lt;/p&gt;
&lt;/div&gt;
&lt;/article&gt;&lt;span class=&quot;card&quot;/&gt;
&lt;article class=&quot;card__article&quot; readability=&quot;35&quot;&gt;
&lt;p class=&quot;post-title&quot;&gt;&lt;span class=&quot;card&quot;&gt;Memory management in WebAssembly: guide for C and Rust programmers&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;card&quot;&gt;Recently we launched Fastly Terrarium, a multi-language, browser-based editor and deployment platform where you can experiment with edge technology. Now, for those well-versed in C and Rust, we’ll explore WebAssembly memory management and implementation.&lt;/span&gt;&lt;/p&gt;

&lt;/article&gt;&lt;span class=&quot;card&quot;/&gt;
&lt;article class=&quot;card__article&quot; readability=&quot;35&quot;&gt;
&lt;p class=&quot;post-title&quot;&gt;&lt;span class=&quot;card&quot;&gt;Edge programming with Rust and WebAssembly&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;card&quot;&gt;Take a developer deep dive into Terrarium, our multi-language, browser-based editor and deployment platform at the edge. Learn how to compile Rust programs to WebAssembly right on your local machine, interact with the Terrarium system,…&lt;/span&gt;&lt;/p&gt;

&lt;/article&gt;&lt;span class=&quot;card&quot;/&gt;
&lt;article class=&quot;card__article&quot; readability=&quot;32&quot;&gt;
&lt;p class=&quot;post-title&quot;&gt;&lt;span class=&quot;card&quot;&gt;How Terrarium reframes the compiler and sandbox relationship&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;card&quot;&gt;Get hands-on with Terrarium, a Fastly project that lets developers harness the power of edge computing in the languages they already use. See how this technology demonstration came to be (and why we’re even using…&lt;/span&gt;&lt;/p&gt;

&lt;/article&gt;&lt;/div&gt;
&lt;/section&gt;&lt;div class=&quot;blog-authors&quot;&gt;
&lt;h3&gt;&lt;span&gt;Author&lt;/span&gt;&lt;/h3&gt;
&lt;div class=&quot;author&quot; readability=&quot;6.5759162303665&quot;&gt;
&lt;div class=&quot;avatar&quot;&gt;&lt;img src=&quot;https://www.fastly.com/cimages/6pk8mg3yh2ee/5Mo709jMas0gIi2WMsmQ6K/14662889605f1ad566bcc03a5699ca4b/pat-hickey.jpg?width=150&amp;amp;height=150&amp;amp;fit=crop&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;bio&quot; readability=&quot;9.0418848167539&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.fastly.com/blog/pat-hickey&quot;&gt;Pat Hickey | Sr. software engineer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pat Hickey is a senior software engineer on Fastly’s isolation team. Previously, he worked on operating systems and compilers for safety-critical systems.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

</description>
<pubDate>Thu, 28 Mar 2019 17:19:33 +0000</pubDate>
<dc:creator>kickdaddy</dc:creator>
<og:title>Announcing Lucet: Fastly’s native WebAssembly compiler and runtime</og:title>
<og:url>https://www.fastly.com/blog/announcing-lucet-fastly-native-webassembly-compiler-runtime/</og:url>
<og:description>Today, we’re thrilled to announce the open sourcing of Lucet, our native WebAssembly compiler and runtime. WebAssembly is a technology created to enable web browsers to safely execute programs at near-native speeds, and it’s been shipping in the four major browsers since early 2017.</og:description>
<og:image>https://www.fastly.com/cimages/6pk8mg3yh2ee/4RGJh4DHD4HX2Zpa2NY1h8/87fe66f5db89c1f1b105c6306b4f2e3a/social_header_image_lucet.jpg?canvas=1200:630&amp;width=1200&amp;height=630&amp;fit=bounds&amp;bg-color=FFFFFF</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.fastly.com/blog/announcing-lucet-fastly-native-webassembly-compiler-runtime</dc:identifier>
</item>
<item>
<title>The FCC has fined robocallers $208M and collected $7k</title>
<link>https://www.foxnews.com/tech/the-fcc-has-fined-robocallers-208-million-its-collected-6790</link>
<guid isPermaLink="true" >https://www.foxnews.com/tech/the-fcc-has-fined-robocallers-208-million-its-collected-6790</guid>
<description>&lt;p class=&quot;speakable&quot;&gt;America’s telecommunications watchdogs have levied hefty financial penalties against illegal robocallers and demanded that bad actors repay millions to their victims. But years later, little money has been collected.&lt;/p&gt;


&lt;p class=&quot;speakable&quot;&gt;Since 2015, the Federal Communications Commission has ordered violators of the Telephone Consumer Protection Act, a law governing telemarketing and robodialing, to pay $208.4 million. That sum includes so-called forfeiture orders in cases involving robocalling, Do Not Call Registry and telephone solicitation violations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.foxnews.com/tech/palantir-wins-800-million-army-contract-for-battlefield-intelligence-system&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;PALANTIR WINS $800M CONTRACT FOR ARMY'S BATTLEFIELD INTELLIGENCE SYSTEM&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So far, the government has collected $6,790 of that amount, according to records obtained by The Wall Street Journal through a Freedom of Information Act request.&lt;/p&gt;
&lt;p&gt;The total amount of money secured by the Federal Trade Commission through court judgments in cases involving civil penalties for robocalls or National Do Not Call Registry-related violations, plus the sum requested for consumer redress in fraud-related cases, is $1.5 billion since 2004. It has collected $121 million of that total, said Ian Barlow, coordinator of the agency’s Do Not Call program, or about 8%. The agency operates the National Do Not Call Registry and regulates telemarketing.&lt;/p&gt;

&lt;p&gt;“That number stands on its own. We’re proud of it; we think our enforcement program is pretty strong,” Mr. Barlow said.&lt;/p&gt;
&lt;p&gt;An FCC spokesman said his agency lacks the authority to enforce the forfeiture orders it issues and has passed all unpaid penalties to the Justice Department, which has the power to collect the fines. Many of the spoofers and robocallers the agency tries to punish are individuals and small operations, he added, which means they are at times unable to pay the full penalties.&lt;/p&gt;
&lt;p&gt;“Fines serve to penalize bad conduct and deter future misconduct,” the FCC spokesman said. A spokeswoman for the Justice Department, which can settle or drop cases, declined to comment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.foxnews.com/tech/facebook-bans-white-nationalism-and-white-separatism&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;FACEBOOK BANS WHITE NATIONALISM AND WHITE SEPARATISM&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The dearth of financial penalties collected by the U.S. government for violations of telemarketing and auto-dialing rules shows the limits the sister regulators face in putting a stop to illegal robocalls. It also shows why the threat of large fines can fail to deter bad actors.&lt;/p&gt;
&lt;p&gt;“It’s great that we have these laws; it’s great that we have public enforcement, but because there are so many calls and so many callers, the public enforcement is a joke,” said Margot Saunders, senior counsel at consumer advocacy group National Consumer Law Center. “It doesn’t even make a dent.”&lt;/p&gt;
&lt;p&gt;There were 26.3 billion unwanted robocalls made to U.S. mobile phones in 2018, by one measure from robocall-blocking app Hiya. Another company that offers such services, YouMail Inc., puts the number of unwanted and illegal robocalls made in the U.S. last year even higher, at nearly 48 billion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.wsj.com/articles/the-fcc-has-fined-robocallers-208-million-its-collected-6-790-11553770803&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;&lt;em&gt;Read more of this story at The Wall Street Journal, where it was first published. &lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 28 Mar 2019 17:06:53 +0000</pubDate>
<dc:creator>mudil</dc:creator>
<og:title>The FCC has fined robocallers $208 million. It's collected $6,790.</og:title>
<og:description>America’s telecommunications watchdogs have levied hefty financial penalties against illegal robocallers and demanded that bad actors repay millions to their victims. But years later, little money has been collected.</og:description>
<og:type>article</og:type>
<og:image>https://static.foxnews.com/foxnews.com/content/uploads/2018/09/robocall.jpg</og:image>
<og:url>https://www.foxnews.com/tech/the-fcc-has-fined-robocallers-208-million-its-collected-6790</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.foxnews.com/tech/the-fcc-has-fined-robocallers-208-million-its-collected-6790</dc:identifier>
</item>
<item>
<title>Garfield phones beach mystery finally solved</title>
<link>https://www.bbc.com/news/world-europe-47732553</link>
<guid isPermaLink="true" >https://www.bbc.com/news/world-europe-47732553</guid>
<description>&lt;figure class=&quot;media-landscape has-caption full-width lead&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                &lt;img class=&quot;js-image-replace&quot; alt=&quot;A collection of assorted Garfield phone fragments are shown arranged around some seaweed on the beach, with and Ar Viltansou high-viz vest visible&quot; src=&quot;https://ichef.bbci.co.uk/news/320/cpsprodpb/1511D/production/_106210368_0f0e1cbf-880b-42ed-9c61-15ab33ffd32e.jpg&quot; width=&quot;976&quot; height=&quot;549&quot;/&gt;&lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Claire Simonin‎ / Ar Viltansou&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Dismembered orange plastic cats and their electronic innards have plagued Finistère for years
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p class=&quot;story-body__introduction&quot;&gt;A French coastal community has finally cracked the mystery behind the Garfield telephones that have plagued its picturesque beaches for decades.&lt;/p&gt;&lt;p&gt;Since the 1980s, the Iroise coast in Brittany has received a supply of bright orange landline novelty phones shaped like the famous cartoon cat.&lt;/p&gt;&lt;p&gt;Anti-litter campaigners have been collecting fragments of the feline for years as they clean the beaches.&lt;/p&gt;&lt;p&gt;But now, the source of the problem has been found - a lost shipping container.&lt;/p&gt;&lt;p&gt;Last year, campaigners from the Ar Vilantsou anti-litter group made the novelty phone a symbol of the plastic pollution on the beaches of the Finistère region - part of which is a designated marine park.&lt;/p&gt;&lt;p&gt;Once a common household item, its eyes open when the landline receiver is picked up, and thousands were made and sold during the 1980s. &lt;a href=&quot;https://www.ebay.co.uk/sch/i.html?_from=R40&amp;amp;_trksid=p2380057.m570.l1313.TR3.TRC2.A0.H0.Xgarfield+phone.TRS0&amp;amp;_nkw=garfield+phone&amp;amp;_sacat=0&quot; class=&quot;story-body__link-external&quot;&gt;Collectors still buy and sell the vintage Garfield phone&lt;/a&gt; online today.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;The beach-cleaning teams had long suspected that a lost shipping container - perhaps blown overboard - had regurgitated its precious orange cargo. But they had never been able to find it.&lt;/p&gt;&lt;p&gt;The media attention on the new campaign, however, drew the eye of a local farmer who remembered the first&lt;i&gt; téléphone Garfield&lt;/i&gt; appearing after a storm in the early 1980s, when he was a young man. &lt;/p&gt;&lt;p&gt;He also knew the location of the container - in a secluded sea cave accessible only at low tide.&lt;/p&gt;&lt;figure class=&quot;media-landscape has-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Martine and Dominique Leczinski / Ar Viltansou&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Volunteers' rubbish hauls frequently feature ocean-battered cartoon cats
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;a href=&quot;https://www.francetvinfo.fr/monde/environnement/alerte-pollution/grace-a-alertepollution-une-association-retrouve-l-origine-des-telephones-garfield-qui-polluent-les-plages-du-finistere-depuis-plus-de-trente-ans_3249105.html&quot; class=&quot;story-body__link-external&quot;&gt;&quot;You had to really know the area well,&quot; he told Franceinfo, which had covered the campaign&lt;/a&gt;. &quot;We found a container aground in a fissure. It was open. Many of the things were gone, but there was a stock of phones,&quot; he recalled.&lt;/p&gt;&lt;p&gt;Members of the Ar Viltansou group, accompanied by Franceinfo journalists, set out to find it.&lt;/p&gt;&lt;p&gt;Climbing down the slippery rocks to the cave, the team spotted remnants of a destroyed shipping container - and soon, between the rocks, Garfield phones - in a more complete condition than any found before them.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;&quot;This is the first time in our lives that we've seen that,&quot; campaigner Claire Simonin-Le Meur told the reporters.&lt;/p&gt;&lt;p&gt;Inside the rock fissure, they found orange plastic poking out from beneath the rocks. The container appeared to remain somewhat buried after three decades.&lt;/p&gt;&lt;p&gt;The challenge of plastic pollution - a hot-button political issue over the last year - is not necessarily reduced by solving the mystery.&lt;/p&gt;&lt;p&gt;The container remains inaccessible and it is not known how much of its cargo is sealed within it. Another issue is that the novelty items that escaped and continue to wash up on Brittany's beaches will not decompose in a human lifetime.&lt;/p&gt;&lt;p&gt;In the meantime, both Ar Viltansou and local officials say they will continue to harvest Garfields from the coastline.&lt;/p&gt;
            </description>
<pubDate>Thu, 28 Mar 2019 14:20:49 +0000</pubDate>
<dc:creator>fpoling</dc:creator>
<og:title>Garfield phones mystery solved after 35 years</og:title>
<og:type>article</og:type>
<og:description>For years, novelty phones appear on Brittany's beaches - but the source is finally found.</og:description>
<og:url>https://www.bbc.com/news/world-europe-47732553</og:url>
<og:image>https://ichef.bbci.co.uk/news/1024/branded_news/1511D/production/_106210368_0f0e1cbf-880b-42ed-9c61-15ab33ffd32e.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bbc.com/news/world-europe-47732553</dc:identifier>
</item>
<item>
<title>Looking for a new CEO</title>
<link>https://stackoverflow.blog/2019/03/28/the-next-ceo-of-stack-overflow/</link>
<guid isPermaLink="true" >https://stackoverflow.blog/2019/03/28/the-next-ceo-of-stack-overflow/</guid>
<description>&lt;div class=&quot;m-post-card__excerpt&quot; itemprop=&quot;articleBody&quot;&gt;After an exciting end to 2018, we kicked this year off with the first “hackathon” in Stack Overflow’s 10-year history. Unlike a traditional hackathon, it wasn’t just for our developers....&lt;/div&gt;&lt;div class=&quot;m-post-card__excerpt&quot; itemprop=&quot;articleBody&quot;&gt;For nine years, we at Stack Overflow have fielded a survey, asking people who code about their opinions on a variety of topics, from whether they prefer a dark or...&lt;/div&gt;&lt;div class=&quot;m-post-card__excerpt&quot; itemprop=&quot;articleBody&quot;&gt;Stack Overflow and InfoJobs are partnering to bring more job opportunities to the developer community in Spain. Since we launched Stack Overflow Jobs in 2009, its mission has been to...&lt;/div&gt;</description>
<pubDate>Thu, 28 Mar 2019 14:11:28 +0000</pubDate>
<dc:creator>g3rv4</dc:creator>
<og:type>article</og:type>
<og:title>The Next CEO of Stack Overflow - Stack Overflow Blog</og:title>
<og:description>Big news! We’re looking for a new CEO for Stack Overflow. I’m stepping out of the day-to-day and up to the role of Chairman of the Board. Stack Overflow has been around for more than a decade. As I look back, it’s really amazing how far it has come.  Only six months after we had launched Stack Overflow, my co-founder Jeff Atwood and I were invited to speak at a Microsoft conference for developers in Las Vegas. We were there, I think, to demonstrate that you could use their latest ASP.NET MVC technology on a real website without too much of a disaster. (In fact .NET has been a huge, unmitigated success for us, but you kids go ahead and have fun with whatever platform you want mkay? They’re all great, or, at least, above-average).It was a giant conference, held at the Venetian Hotel. This hotel was so big that other hotels stay there when they go on vacation. The main ballroom was the size of, approximately, Ireland. I later learned there were 5,000 developers in that room.I thought it would be a fun thing to ask the developers in the room how many of them had visited Stack Overflow. As I remember, Jeff was very much against this idea. “Joel,” he said, “That is going to be embarrassing and humiliating. Nobody is going to raise their hand.”Well, I asked it anyway. And we were both surprised to see about one-third of the hands go up. We were really making an impact! That felt really good.Anyway, I tried that trick again whenever I spoke to a large audience. It doesn’t work anymore. Today, audiences just laugh. It’s like asking, “Does anyone use gravity? Raise your hand if you use gravity.”Where are we at after 11 years? Practically every developer in the world uses Stack Overflow. Including the Stack Exchange network of 174 sites, we have over 100 million monthly visitors. Every month, over 125,000 wonderful people write answers. According to Alexa, stackoverflow.com is one of the top 50 websites in the world. (That’s without even counting the Stack Exchange network, which is almost as big.) And every time I see a developer write code, they’ve got Stack Overflow open in one of their browser windows. Oh and—hey!—we do not make you sign up or pay to see the answers.The company has been growing, too. Today we are profitable. We have almost 300 amazing employees worldwide and booked $70m in revenue last year. We have talent, advertising, and software products. The SaaS products (Stack Overflow for Teams and Enterprise) are growing at 200% a year. That speaks to the fact that we’ve recruited an incredibly talented team that has produced such fantastic results. But, we have a lot of work ahead of us, and it’s going to take a different type of leader to get us through that work. The type of people Stack Overflow serves has changed, and now, as a part of the developer ecosystem, we have a responsibility to create an online community that is far more diverse, inclusive, and welcoming of newcomers. In the decade or so since Stack Overflow started, the number of people employed as software developers grew by 64% in the US alone. The field is going to keep growing everywhere in the world, and the demand for great software developers far outstrips supply. So a big challenge for Stack Overflow is welcoming those new developers into the fold. As I’ve written:One thing I’m very concerned about, as we try to educate the next generation of developers, and, importantly, get more diversity and inclusiveness in that new generation, is what obstacles we’re putting up for people as they try to learn programming. In many ways Stack Overflow’s specific rules for what is permitted and what is not are obstacles, but an even bigger problem is rudeness, snark, or condescension that newcomers often see.I care a lot about this. Being a developer gives you an unparalleled opportunity to write the script for the future. All the flak that Stack Overflow throws in the face of newbies trying to become developers is actively harmful to people, to society, and to Stack Overflow itself, by driving away potential future contributors. And programming is hard enough; we should see our mission as making it easier.The world has started taking a closer look at tech, and understanding that software and the internet are not just tools; they are shaping the future of society. Big tech companies are struggling with their place in the world. Stack Overflow is situated at the right place to be influential in how that future develops, and that is going to take a new type of leader.It will not be easy to find a CEO who is the right person to lead that mission. We will, no doubt, hire one of those fancy executive headhunters to help us in the search. But, hey, this is Stack Overflow. If there’s one thing I have learned by now, it’s that there’s always someone in the community who can answer the questions I can’t. So we decided to put this announcement out there in hopes of finding great candidates that might have been under the radar. We’re especially focused on identifying candidates from under-represented groups, and making sure that every candidate we consider is deeply committed to making our company and community more welcoming, diverse, and inclusive.Over the years, Fog Creek Software created several incredible hits and many wonderful memories along the way. It is great to watch Trello (under Michael Pryor) and Glitch (under Anil Dash) growing into enormously valuable, successful, and influential products with dedicated leaders who took these products much further than I ever could have, and personally I’m excited to see where Stack Overflow can go and turn my attention to the next thing.</og:description>
<og:url>https://stackoverflow.blog/2019/03/28/the-next-ceo-of-stack-overflow/</og:url>
<og:image>https://zgab33vy595fw5zq-zippykid.netdna-ssl.com/wp-content/uploads/2017/12/SO_pattern.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://stackoverflow.blog/2019/03/28/the-next-ceo-of-stack-overflow/</dc:identifier>
</item>
<item>
<title>Why Bother with What Three Words?</title>
<link>https://shkspr.mobi/blog/2019/03/why-bother-with-what-three-words/</link>
<guid isPermaLink="true" >https://shkspr.mobi/blog/2019/03/why-bother-with-what-three-words/</guid>
<description>&lt;p&gt;I'll be wording this post carefully as &lt;a href=&quot;https://what3words.com/&quot;&gt;What 3 Words&lt;/a&gt; (W3W) have a tenacious PR team and, probably, have a lot more lawyers than I do.&lt;/p&gt;
&lt;p&gt;W3W is a closed product. It is a for-profit company masquerading as an open standard. And that annoys me.&lt;/p&gt;
&lt;p&gt;A brief primer.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The world is a &lt;a href=&quot;https://simple.wikipedia.org/wiki/Oblate_spheroid&quot;&gt;sphere&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We can reference any point on the surface of Earth using two co-ordinates, Longitude and Latitude.&lt;/li&gt;
&lt;li&gt;Long/Lat are numbers. They can be as precise or as vague as needed.&lt;/li&gt;
&lt;li&gt;Humans can't remember long strings of numbers, and reading them out is difficult.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;W3W aims to solve this. It splits the world into a grid, and gives every square a unique three-word phrase.&lt;/p&gt;
&lt;p&gt;So the location &lt;code&gt;51.50799,-0.12803&lt;/code&gt; becomes &lt;code&gt;///mile.crazy.shade&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Brilliant, right?&lt;/p&gt;
&lt;p&gt;No.&lt;/p&gt;
&lt;p&gt;Here's all the problems I have with W3W.&lt;/p&gt;
&lt;h2&gt;It isn't open&lt;/h2&gt;
&lt;p&gt;The algorithm used to generate the words is proprietary. You are not allowed to see it. You cannot find out your location without asking W3W for permission.&lt;/p&gt;
&lt;p&gt;If you want permission, you have to agree to some pretty &lt;a href=&quot;https://what3words.com/terms/&quot;&gt;long terms and conditions&lt;/a&gt;. And understand their &lt;a href=&quot;https://what3words.com/privacy/&quot;&gt;privacy policy&lt;/a&gt;. Oh, and an &lt;a href=&quot;https://what3words.com/developers/api-licence-agreement/&quot;&gt;API agreement&lt;/a&gt;. And then make sure you &lt;a href=&quot;https://what3words.com/patents&quot;&gt;don't infringe their patents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You cannot store locations. You have to let them analyse the locations you look up. Want to use more than 10,000 addresses? Contact them for prices!&lt;/p&gt;
&lt;p&gt;It is the antithesis of open.&lt;/p&gt;
&lt;h2&gt;Cost&lt;/h2&gt;
&lt;p&gt;W3W refuses to publish their prices. You have to contact their sales team if you want to know what it will cost your organisation.&lt;/p&gt;
&lt;p&gt;Open standards are free to use.&lt;/p&gt;
&lt;h2&gt;Earthquakes&lt;/h2&gt;
&lt;p&gt;When an earthquake struck Japan, street addresses didn't change &lt;em&gt;but&lt;/em&gt; that &lt;a href=&quot;https://slate.com/news-and-politics/2011/03/japanese-earthquake-when-tectonic-plates-shift-does-gps-still-work.html&quot;&gt;their physical location did&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That is, a street address is &lt;em&gt;still&lt;/em&gt; 42 Acacia Avenue - but the Longitude and Latitude has changed.&lt;/p&gt;
&lt;p&gt;Perhaps you think this is an edge case? It isn't. &lt;a href=&quot;https://news.nationalgeographic.com/2016/09/australia-moves-gps-coordinates-adjusted-continental-drift/&quot;&gt;Australia is drifting so fast that GPS can't keep up&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;How does W3W deal with this? Their grid is static, so &lt;a href=&quot;https://support.what3words.com/hc/en-us/articles/208506269-How-will-what3words-handle-continental-drift-&quot;&gt;any tectonic activity means your W3W changes&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Internationalisation&lt;/h2&gt;
&lt;p&gt;Numbers are &lt;em&gt;fairly&lt;/em&gt; universal. Lots of countries use 0-9. English words are &lt;em&gt;not&lt;/em&gt; universal. How does W3W deal with this?&lt;/p&gt;
&lt;p&gt;Is &quot;cat.dog.goose&quot; straight translated into French? No! Each language has its own word list.&lt;/p&gt;
&lt;p&gt;There is no way to translate between languages. You have to beg W3W for permission for access to their API. They do not publish their word lists or the mappings between them.&lt;/p&gt;
&lt;p&gt;So, if I want to tell a French speaker where &lt;code&gt;///mile.crazy.shade&lt;/code&gt; is, I have to use &lt;code&gt;///embouchure.adjuger.saladier&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Loosely translated back as &lt;code&gt;///mouth.award.bowl&lt;/code&gt; an &lt;a href=&quot;https://map.what3words.com/mouth.award.bowl&quot;&gt;entirely different location&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;You're not allowed to know what word lists W3W use. They take a &lt;a href=&quot;https://support.what3words.com/hc/en-us/articles/203105521-Is-a-3-word-address-in-French-or-any-other-language-a-translation-of-the-same-3-words-in-English-&quot;&gt;paternalistic attitude&lt;/a&gt; to creating their lists - they know best. You cannot propose changes.&lt;/p&gt;
&lt;p&gt;Anecdotally, their &lt;a href=&quot;https://news.ycombinator.com/item?id=17423421&quot;&gt;non-English word lists are confusing even for native speakers&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Cultural Respect&lt;/h2&gt;
&lt;p&gt;Numbers are (mostly) culturally neutral. Words are not. Is &quot;mile.crazy.shade&quot; a respectful name for a war memorial? How about &lt;a href=&quot;https://map.what3words.com/tribes.hurt.stumpy&quot;&gt;&lt;code&gt;///tribes.hurt.stumpy&lt;/code&gt;&lt;/a&gt; for a temple?&lt;/p&gt;
&lt;p&gt;How do you feel about &lt;a href=&quot;https://map.what3words.com/weepy.lulls.emerge&quot;&gt;&lt;code&gt;///weepy.lulls.emerge&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://map.what3words.com/grouchy.hormone.elevating&quot;&gt;&lt;code&gt;///grouchy.hormone.elevating&lt;/code&gt;&lt;/a&gt; both being at Auschwitz? Or &lt;a href=&quot;https://map.what3words.com/klartext.best%C3%BCckt.vermuten&quot;&gt;&lt;code&gt;///klartext.bestückt.vermuten&lt;/code&gt;&lt;/a&gt; - &quot;cleartext stocked suspect&quot;?&lt;/p&gt;
&lt;p&gt;This is a classic computer science problem. Every sufficiently long word list can eventually be recombined into a potentially offensive phrase.&lt;/p&gt;
&lt;h2&gt;Open Washing&lt;/h2&gt;
&lt;p&gt;W3W know that &lt;a href=&quot;https://wiki.openstreetmap.org/wiki/What3words&quot;&gt;the majority of technical people are not fooled&lt;/a&gt; by their attempts to lock down addressing.&lt;/p&gt;
&lt;p&gt;They include this paragraph to attempt to prove their openness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If we, what3words ltd, are ever unable to maintain the what3words technology or make arrangements for it to be maintained by a third-party (with that third-party being willing to make this same commitment), then we will release our source code into the public domain. We will do this in such a way and with suitable licences and documentation to ensure that any and all users of what3words, whether they are individuals, businesses, charitable organisations, aid agencies, governments or anyone else can continue to rely on the what3words system.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I don't know how they propose to bind a successor organisation. They don't say &lt;em&gt;what&lt;/em&gt; licences they will use. If they go bust, there's no guarantee they'll be legally able to release this code, nor may they have the time to do so.&lt;/p&gt;
&lt;p&gt;There's nothing stopping W3W from releasing their algorithms now, subjecting them to scrutiny by the standards community. They could build up a community of experts to help improve the system, they could work with existing mapping efforts, they could help build a useful and open standard.&lt;/p&gt;
&lt;p&gt;But they don't. They guard their secrets and actively promote their proprietary product in the hope it will become widely accepted and then they can engage in rent-seeking behaviour.&lt;/p&gt;
&lt;h2&gt;This is not a new argument&lt;/h2&gt;
&lt;p&gt;My mate &lt;a href=&quot;https://blog.ldodds.com/2016/06/14/what-3-words-jog-on-mate/&quot;&gt;Leigh wrote about this three years ago&lt;/a&gt;. &lt;a href=&quot;https://knowwhereconsulting.co.uk/blog/location-grid-not-an-address/&quot;&gt;Lots&lt;/a&gt; &lt;a href=&quot;https://medium.com/@piesse/open-location-code-what3words-74a3f810c18d&quot;&gt;of&lt;/a&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=18646650&quot;&gt;people&lt;/a&gt; &lt;a href=&quot;https://www.quora.com/What-is-your-review-of-what3words&quot;&gt;have&lt;/a&gt; &lt;a href=&quot;https://stiobhart.net/2016-01-15-stupidest-idea-ever/&quot;&gt;criticised&lt;/a&gt; &lt;a href=&quot;http://blog.telemapics.com/?p=589&quot;&gt;W3W&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;.&lt;a href=&quot;https://twitter.com/what3words?ref_src=twsrc%5Etfw&quot;&gt;@what3words&lt;/a&gt; is bad technical idea, and ethically terrible too. But all VCs like patented economic rents so the juggernaut rolls on. &lt;a href=&quot;https://twitter.com/hashtag/geomob?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#geomob&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Andy Allan (@gravitystorm) &lt;a href=&quot;https://twitter.com/gravitystorm/status/753653845859962880?ref_src=twsrc%5Etfw&quot;&gt;July 14, 2016&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But W3W have a great PR team - pushing press releases which are then reported as &lt;a href=&quot;https://www.bbc.co.uk/news/technology-40935774&quot;&gt;uncritical&lt;/a&gt; &lt;a href=&quot;https://www.bbc.co.uk/news/technology-47705912&quot;&gt;news&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most recent press release contains a &lt;em&gt;ludicrous&lt;/em&gt; example:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Person dials the emergency services&lt;/li&gt;
&lt;li&gt;Person doesn't know their location&lt;/li&gt;
&lt;li&gt;Emergency services sends the person a link&lt;/li&gt;
&lt;li&gt;Person clicks on link, opens web page&lt;/li&gt;
&lt;li&gt;Web page geolocates user and displays their W3W location&lt;/li&gt;
&lt;li&gt;Person reads out their W3W phrase to the emergency services&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Here's the thing... If the person's phone has a data connection - the web page can just send the geolocation directly back to the emergency services! No need to get a human to read it out, then another human to listen and type it in to a different system.&lt;/p&gt;
&lt;p&gt;There is literally no need for W3W in this scenario. If you have a data connection, you can send your precise location without an intermediary.&lt;/p&gt;
&lt;h2&gt;What Next?&lt;/h2&gt;
&lt;p&gt;W3W succeeds because it has a superficially simple solution to a complex problems. It is a brilliant lesson in how marketing and PR can help a technologically inferior project look like it is a global open solution.&lt;/p&gt;
&lt;p&gt;I'm not joking. Their &lt;a href=&quot;https://www.edelman.co.uk/work/what3words/&quot;&gt;branding firm says&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Edelman helped what3words frame their story to be compelling by tapping into human emotion.&lt;br/&gt;We also created a story for CEO Chris Sheldrick about how having an address can drive social transformation and business efficiency, securing profiling and speaker opportunities.&lt;br/&gt;Through paid social campaigns we re-targeted these stories, getting through to the decision makers that mattered most.&lt;br/&gt;We articulated their purpose narrative and refined their strategy to engage investors and excite the media.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It takes &lt;a href=&quot;https://twitter.com/ziobrando/status/289635060758507521&quot;&gt;too much time to refute all their claims&lt;/a&gt; - but we must. Whenever you see people mentioning What3Words, politely remind them that it is not an open standard and should be avoided.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Your periodic reminder that W3W is a closed and proprietary system, with opaque licencing, hefty pricing, and poor internationalisation.&lt;br/&gt;It does have a very good PR team though. &lt;a href=&quot;https://t.co/Ch3e9cAfsn&quot;&gt;https://t.co/Ch3e9cAfsn&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Terence Eden (@edent) &lt;a href=&quot;https://twitter.com/edent/status/1110606981142925313?ref_src=twsrc%5Etfw&quot;&gt;March 26, 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

</description>
<pubDate>Thu, 28 Mar 2019 13:25:15 +0000</pubDate>
<dc:creator>MagicAndi</dc:creator>
<og:type>article</og:type>
<og:title>Why bother with What Three Words?</og:title>
<og:url>https://shkspr.mobi/blog/2019/03/why-bother-with-what-three-words/</og:url>
<og:description>I’ll be wording this post carefully as What 3 Words (W3W) have a tenacious PR team and, probably, have a lot more lawyers than I do. W3W is a closed product. It is a for-profit company masque…</og:description>
<og:image>https://shkspr.mobi/blog/wp-content/uploads/2014/11/Men-confused-by-a-map.jpg</og:image>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://shkspr.mobi/blog/2019/03/why-bother-with-what-three-words/</dc:identifier>
</item>
<item>
<title>Common statistical tests are linear models</title>
<link>https://lindeloev.github.io/tests-as-linear/</link>
<guid isPermaLink="true" >https://lindeloev.github.io/tests-as-linear/</guid>
<description>&lt;a href=&quot;https://twitter.com/intent/tweet?text=Common%20statistical%20tests%20are%20linear%20models%20(or:%20how%20to%20teach%20stats)%20https%3A%2F%2Flindeloev.github.io%2Ftests-as-linear%20via%20%40jonaslindeloev&quot; class=&quot;twitter-hashtag-button&quot; data-size=&quot;large&quot; data-related=&quot;jonaslindeloev&quot; data-show-count=&quot;false&quot;&gt;Share on Twitter&lt;/a&gt; &lt;p&gt;This document is summarised in the table below. It shows the linear models underlying common parametric and non-parametric tests. Formulating all the tests in the same language highlights the many similarities between them. Get it &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png&quot;&gt;as an image&lt;/a&gt; or &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf&quot;&gt;as a PDF&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf&quot;&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;div id=&quot;the-simplicity-underlying-common-tests&quot; class=&quot;section level1&quot; readability=&quot;41.613259668508&quot;&gt;

&lt;p&gt;Most of the common statistical models (t-test, correlation, ANOVA; chi-square, etc.) are special cases of linear models or a very close approximation. This beautiful simplicity means that there is less to learn. In particular, it all comes down to &lt;span class=&quot;math inline&quot;&gt;\(y = a \cdot x + b\)&lt;/span&gt; which most students know from highschool. Unfortunately, stats intro courses are usually taught as if each test is an independent tool, needlessly making life more complicated for students and teachers alike.&lt;/p&gt;
&lt;p&gt;This needless complexity multiplies when students try to rote learn the parametric assumptions underlying each test separately rather than deducing them from the linear model.&lt;/p&gt;
&lt;p&gt;For this reason, I think that teaching linear models first and foremost and &lt;em&gt;then&lt;/em&gt; name-dropping the special cases along the way makes for an excellent teaching strategy, emphasizing &lt;em&gt;understanding&lt;/em&gt; over rote learning. Since linear models are the same across frequentist, Bayesian, and permutation-based inferences, I’d argue that it’s better to start with modeling than p-values, type-1 errors, Bayes factors, or other inferences.&lt;/p&gt;
&lt;p&gt;Concerning the teaching of &lt;em&gt;non-parametric&lt;/em&gt; tests in intro-courses, I think that we can justify &lt;a href=&quot;https://en.wikipedia.org/wiki/Lie-to-children&quot;&gt;lying-to-children&lt;/a&gt; and teach non-parametric tests as if they are merely ranked versions of the corresponding parametric tests. It is much better for students to think “ranks!” than to believe that you can magically throw away assumptions. Indeed, the Bayesian equivalents of non-parametric tests implemented in &lt;a href=&quot;https://jasp-stats.org&quot;&gt;JASP&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1712.06941&quot;&gt;literally just do (latent) ranking&lt;/a&gt; and that’s it. For the frequentist non-parametric tests considered here, this approach is highly accurate for N &amp;gt; 15.&lt;/p&gt;
&lt;p&gt;Use the menu to jump to your favourite section. There are links to lots of similar (though more scattered) stuff under &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#links&quot;&gt;sources&lt;/a&gt; and &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#course&quot;&gt;teaching materials&lt;/a&gt;. I hope that you will join in suggesting improvements or submitting improvements yourself in &lt;a href=&quot;https://github.com/lindeloev/tests-as-linear&quot;&gt;the Github repo to this page&lt;/a&gt;. Let’s make it awesome!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;settings-and-toy-data&quot; class=&quot;section level1&quot; readability=&quot;50.5&quot;&gt;

Unfold this if you want to see functions and other settings for this notebook:
&lt;div class=&quot;fold s&quot; readability=&quot;50&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Load packages for data handling and plotting
library(tidyverse)
library(patchwork)
library(broom)

# Reproducible &quot;random&quot; results
set.seed(40)

# To show tables. Rounds
print_df = function(D, decimals=4, navigate=FALSE) {
  DT::datatable(mutate_if(D, is.numeric, round, decimals), 
    rownames = FALSE,
    options = list(
      searching=FALSE, 
      lengthChange=FALSE, 
      ordering=FALSE, 
      autoWidth=TRUE, 
      bPaginate=navigate, 
      bInfo=navigate, 
      paging=navigate
    )
  )
}

# Generate normal data with known parameters
rnorm_fixed = function(N, mu=0, sd=1) scale(rnorm(N))*sd + mu

# Plot style.
theme_axis = function(P, jitter=FALSE, xlim=c(-0.5, 2), ylim=c(-0.5, 2), legend.position=NULL) {
  P = P + theme_bw(15) + 
  geom_segment(x=-1000, xend=1000, y=0, yend=0, lty=2, color='dark gray', lwd=0.5) +
  geom_segment(x=0, xend=0, y=-1000, yend=1000, lty=2, color='dark gray', lwd=0.5) +
  coord_cartesian(xlim=xlim, ylim=ylim) +
  theme(axis.title = element_blank(), 
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        panel.border = element_blank(),
        panel.grid = element_blank(),
        legend.position = legend.position)
  
  # Return jittered or non-jittered plot?
  if(jitter) {
    P + geom_jitter(width=0.1, size=2)
  }
  else {
    P + geom_point(size=2)
  }
}&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a start, we’ll keep it simple and play with three standard normals in wide (&lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt;) and long format (&lt;code&gt;value&lt;/code&gt;, &lt;code&gt;group&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Wide format (sort of)
y = rnorm_fixed(50, mu=0.3, sd=2)  # Almost zero mean
x = rnorm_fixed(50, mu=0, sd=1)  # Used in correlation where this is on x-axis
y2 = rnorm_fixed(50, mu=0.5, sd=1.5)  # Used in two means

# Long format data with indicator
value = c(y, y2)
group = rep(c('y1', 'y2'), each = 50)

# We'll need the signed rank function for a lot of the &quot;non-parametric&quot; tests
signed_rank = function(x) sign(x) * rank(abs(x))&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div id=&quot;correlation&quot; class=&quot;section level1&quot; readability=&quot;37.877753129457&quot;&gt;

&lt;div id=&quot;theory-as-linear-models&quot; class=&quot;section level3&quot; readability=&quot;74.567668008671&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;Model: the recipe for &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; is a slope (&lt;span class=&quot;math inline&quot;&gt;\(\beta_1\)&lt;/span&gt;) times &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; plus an intercept (&lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt;, aka a straight line).&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1 x \qquad \mathcal{H}_0: \beta_1 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;… which is a math-y way of writing the good old &lt;span class=&quot;math inline&quot;&gt;\(y = ax + b\)&lt;/span&gt; (here ordered as &lt;span class=&quot;math inline&quot;&gt;\(y = b + ax\)&lt;/span&gt;). In R we are lazy and write &lt;code&gt;y ~ 1 + x&lt;/code&gt; which R reads like &lt;code&gt;y = 1*number + x*othernumber&lt;/code&gt; and the task of t-tests, lm, etc., is simply to find the numbers that best predict &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Either way you write it, it’s an intercept (&lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt;) and a slope (&lt;span class=&quot;math inline&quot;&gt;\(\beta_1\)&lt;/span&gt;) yielding a straight line:&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;41&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Fixed correlation
D_correlation = data.frame(MASS::mvrnorm(30, mu=c(0.9, 0.9), Sigma=matrix(c(1, 0.8, 1, 0.8), ncol=2), empirical=TRUE))  # Correlated data

# Add labels (for next plot)
D_correlation$label_num = sprintf('(%.1f,%.1f)', D_correlation$X1, D_correlation$X2)
D_correlation$label_rank = sprintf('(%i,%i)', rank(D_correlation$X1), rank(D_correlation$X2))

# Plot it
fit = lm(I(X2*0.5+0.4) ~ I(X1*0.5+0.2), D_correlation)
intercept_pearson = coefficients(fit)[1]

P_pearson = ggplot(D_correlation, aes(x=X1*0.5+0.2, y=X2*0.5+0.4)) +
  geom_smooth(method=lm, se=FALSE, lwd=2, aes(colour='beta_1')) + 
  geom_segment(x=-100, xend=100, 
               y=intercept_pearson, yend=intercept_pearson, 
               lwd=2, aes(color=&quot;beta_0&quot;)) + 
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;), bquote(beta[1]*&quot; (slope)&quot;)))
  
theme_axis(P_pearson, legend.position=c(0.4, 0.9))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-4-1.png&quot; width=&quot;576&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This is often simply called a &lt;strong&gt;regression&lt;/strong&gt; model which can be extended to &lt;strong&gt;multiple regression&lt;/strong&gt; where there are several &lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt;s and on the right-hand side multiplied with the predictors. Everything below, from &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;one-sample t-test&lt;/a&gt; to &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova2&quot;&gt;two-way ANOVA&lt;/a&gt; are just special cases of this system. Nothing more, nothing less.&lt;/p&gt;
&lt;p&gt;As the name implies, the &lt;strong&gt;Spearman rank correlation&lt;/strong&gt; is a &lt;strong&gt;Pearson correlation&lt;/strong&gt; on rank-transformed &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(rank(y) = \beta_0 + \beta_1 \cdot rank(x) \qquad \mathcal{H}_0: \beta_1 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The correlation coefficient of the linear model is identical to a “real” Pearson correlation, but p-values are an approximation which is is &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/simulate_spearman.html&quot;&gt;appropriate for samples greater than N=10 and almost perfect when N &amp;gt; 20&lt;/a&gt;. Such a nice and non-mysterious equivalence that many students are left unaware of! Visualizing them side by side including data labels, we see this rank-transformation in action:&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;39&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Spearman intercept
intercept_spearman = coefficients(lm(rank(X2) ~ rank(X1), D_correlation))[1]

# Spearman plot
P_spearman = ggplot(D_correlation, aes(x=rank(X1), y=rank(X2))) +
  geom_smooth(method=lm, se=FALSE, lwd=2, aes(color='beta_1')) + 
  geom_text(aes(label=label_rank), nudge_y=1, size=3, color='dark gray') + 
  geom_segment(x=-100, xend=100, 
               y=intercept_spearman, yend=intercept_spearman, 
               lwd=2, aes(color='beta_0')) + 
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;), bquote(beta[1]*&quot; (slope)&quot;)))

# Stich together using patchwork
(theme_axis(P_pearson, legend.position=c(0.5, 0.1)) + geom_text(aes(label=label_num), nudge_y=0.1, size=3, color='dark gray') + labs(title='         Pearson')) + (theme_axis(P_spearman, xlim=c(-7.5, 30), ylim=c(-7.5, 30), legend.position=c(0.5, 0.1)) + labs(title='         Spearman'))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-5-1.png&quot; width=&quot;768&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-pearson-correlation&quot; class=&quot;section level3&quot; readability=&quot;26.5&quot;&gt;
&lt;h3&gt; R code: Pearson correlation&lt;/h3&gt;
&lt;p&gt;It couldn’t be much simpler to run these models in R. They yield identical &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;t&lt;/code&gt;, but there’s a catch: &lt;code&gt;lm&lt;/code&gt; gives you the &lt;em&gt;slope&lt;/em&gt; and even though that is usually much more interpretable and informative than the &lt;em&gt;correlation coefficient&lt;/em&gt; &lt;em&gt;r&lt;/em&gt;, you may still want &lt;em&gt;r&lt;/em&gt;. Luckily, the slope becomes &lt;code&gt;r&lt;/code&gt; if &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; have a standard deviation of exactly 1. You can do this using &lt;code&gt;scale(x)&lt;/code&gt; or &lt;code&gt;I(x/sd(x))&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;a = cor.test(y, x, method = &quot;pearson&quot;) # Built-in
b = lm(y ~ 1 + x) # Equivalent linear model: y = Beta0*1 + Beta1*x
c = lm(scale(y) ~ 1 + scale(x))  # On scaled vars to recover r&lt;/code&gt;
&lt;/pre&gt;
Results:

&lt;div class=&quot;fold o&quot; readability=&quot;16&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Pearson's product-moment correlation
## 
## data:  y and x
## t = -1.6507, df = 48, p-value = 0.1053
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.47920849  0.04978276
## sample estimates:
##        cor 
## -0.2317767 
## 
## 
## Call:
## lm(formula = y ~ 1 + x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3393 -1.6593  0.3349  1.3629  3.5214 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   0.3000     0.2780   1.079    0.286
## x            -0.4636     0.2808  -1.651    0.105
## 
## Residual standard error: 1.966 on 48 degrees of freedom
## Multiple R-squared:  0.05372,    Adjusted R-squared:  0.03401 
## F-statistic: 2.725 on 1 and 48 DF,  p-value: 0.1053
## 
## 
## Call:
## lm(formula = scale(y) ~ 1 + scale(x))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6697 -0.8297  0.1675  0.6815  1.7607 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept) -1.341e-17  1.390e-01   0.000    1.000
## scale(x)    -2.318e-01  1.404e-01  -1.651    0.105
## 
## Residual standard error: 0.9828 on 48 degrees of freedom
## Multiple R-squared:  0.05372,    Adjusted R-squared:  0.03401 
## F-statistic: 2.725 on 1 and 48 DF,  p-value: 0.1053&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The CIs are not exactly identical, but very close.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-spearman-correlation&quot; class=&quot;section level3&quot; readability=&quot;23&quot;&gt;
&lt;h3&gt; R code: Spearman correlation&lt;/h3&gt;
&lt;p&gt;Note that we can interpret the slope which is the number of ranks &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; change for each rank on &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;. I think that this is a pretty interesting number. However, the intercept is less interpretable since it lies at &lt;span class=&quot;math inline&quot;&gt;\(rank(x) = 0\)&lt;/span&gt; which is impossible since x starts at 1.&lt;/p&gt;
&lt;p&gt;See the identical &lt;code&gt;r&lt;/code&gt; (now “rho”) and &lt;code&gt;p&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Spearman correlation
a = cor.test(y, x, method = &quot;spearman&quot;) # Built-in
b = lm(rank(y) ~ 1 + rank(x)) # Equivalent linear model&lt;/code&gt;
&lt;/pre&gt;
Let’s look at the results:

&lt;div class=&quot;fold o&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Spearman's rank correlation rho
## 
## data:  y and x
## S = 25544, p-value = 0.1135
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##        rho 
## -0.2266026 
## 
## 
## Call:
## lm(formula = rank(y) ~ 1 + rank(x))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -26.4655 -11.5603   0.4458  11.5628  25.6921 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  31.2784     4.1191   7.593 9.11e-10 ***
## rank(x)      -0.2266     0.1406  -1.612    0.114    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 14.35 on 48 degrees of freedom
## Multiple R-squared:  0.05135,    Adjusted R-squared:  0.03159 
## F-statistic: 2.598 on 1 and 48 DF,  p-value: 0.1135&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;one-mean&quot; class=&quot;section level1&quot;&gt;

&lt;div id=&quot;t1&quot; class=&quot;section level2&quot; readability=&quot;26.094102228047&quot;&gt;
&lt;h2&gt; One sample t-test and Wilcoxon signed-rank&lt;/h2&gt;
&lt;div id=&quot;theory-as-linear-models-1&quot; class=&quot;section level3&quot; readability=&quot;48.384573601304&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;t-test&lt;/strong&gt; model: A single number predicts &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 \qquad \mathcal{H}_0: \beta_0 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In other words, it’s our good old &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1*x\)&lt;/span&gt; where the last term is gone since there is no &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; (essentially &lt;span class=&quot;math inline&quot;&gt;\(x=0\)&lt;/span&gt;, see left figure below).&lt;/p&gt;
&lt;p&gt;The same is to a very close approximately true for &lt;strong&gt;Wilcoxon signed-rank test&lt;/strong&gt;, just with the signed ranks of &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; instead of &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; itself (see right panel below and caveat in the end of this section):&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(signed\_rank(y) = \beta_0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/simulate_wilcoxon.html&quot;&gt;This approximation is good enough when the sample size is larger than 14 and almost perfect if the sample size is larger than 50&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;47&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# T-test
D_t1 = data.frame(y=rnorm_fixed(20, 0.5, 0.6),
                  x=runif(20, 0.93, 1.07))  # Fix mean and SD

P_t1 = ggplot(D_t1, aes(y=y, x=0)) + 
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y.., color='beta_0'), lwd=2) +
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;))) +
  
  geom_text(aes(label=round(y, 1)), nudge_x = 0.2, size=3, color='dark gray') + 
  labs(title='         T-test')

# Wilcoxon
D_t1_rank = data.frame(y = signed_rank(D_t1$y))

P_t1_rank = ggplot(D_t1_rank, aes(y=y, x=0)) + 
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y.., color='beta_0'), lwd=2) +
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;), labels=c(bquote(beta[0]*&quot; (intercept)&quot;))) +

  geom_text(aes(label=y), nudge_x=0.2, size=3, color='dark gray') + 
  labs(title='         Wilcoxon')


# Stich together using patchwork
theme_axis(P_t1, ylim=c(-1, 2), legend.position=c(0.6, 0.1)) + 
  theme_axis(P_t1_rank, ylim=NULL,  legend.position=c(0.6, 0.1))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-12-1.png&quot; width=&quot;672&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;One interesting implication is that &lt;em&gt;many “non-parametric tests” are precisely as parametric as their parametric counterparts with means, standard deviations, homogeneity of variance, etc. - just on transformed data&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-one-sample-t-test&quot; class=&quot;section level3&quot; readability=&quot;16.5&quot;&gt;
&lt;h3&gt; R code: One-sample t-test&lt;/h3&gt;
&lt;p&gt;Try running the R code below and see that the linear model (&lt;code&gt;lm&lt;/code&gt;) produces the same &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(r\)&lt;/span&gt; as the built-in &lt;code&gt;t.test&lt;/code&gt;. The confidence interval is not presented in the output of &lt;code&gt;lm&lt;/code&gt; but is also identical if you use &lt;code&gt;confint(lm(...))&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in t-test
a = t.test(y)

# Equivalent linear model: intercept-only
b = lm(y ~ 1)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  One Sample t-test
## 
## data:  y
## t = 1.0607, df = 49, p-value = 0.294
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -0.2683937  0.8683937
## sample estimates:
## mean of x 
##       0.3 
## 
## 
## Call:
## lm(formula = y ~ 1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.521 -1.673  0.481  1.427  3.795 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   0.3000     0.2828   1.061    0.294
## 
## Residual standard error: 2 on 49 degrees of freedom&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-wilcoxon-signed-rank-test&quot; class=&quot;section level3&quot; readability=&quot;19&quot;&gt;
&lt;h3&gt; R code: Wilcoxon signed-rank test&lt;/h3&gt;
&lt;p&gt;In addition to matching &lt;code&gt;p&lt;/code&gt;-values, &lt;code&gt;lm&lt;/code&gt; also gives us the mean signed rank, which I find to be an informative number.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in
a = wilcox.test(y)

# Equivalent linear model
b = lm(signed_rank(y) ~ 1)  # See? Same as above, just on signed ranks

# Bonus: of course also works for one-sample t-test
c = t.test(signed_rank(y))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  y
## V = 731, p-value = 0.3693
## alternative hypothesis: true location is not equal to 0
## 
## 
## Call:
## lm(formula = signed_rank(y) ~ 1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -49.74 -25.49   4.76  22.76  46.26 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)    3.740      4.151   0.901    0.372
## 
## Residual standard error: 29.36 on 49 degrees of freedom
## 
## 
##  One Sample t-test
## 
## data:  signed_rank(y)
## t = 0.90088, df = 49, p-value = 0.3721
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -4.60275 12.08275
## sample estimates:
## mean of x 
##      3.74&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;tpair&quot; class=&quot;section level2&quot; readability=&quot;23.37216&quot;&gt;
&lt;h2&gt; Paired samples t-test and Wilcoxon matched pairs&lt;/h2&gt;
&lt;div id=&quot;theory-as-linear-models-2&quot; class=&quot;section level3&quot; readability=&quot;29.558823529412&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;t-test&lt;/strong&gt; model: a single number (intercept) predicts the pairwise differences.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y_2-y_1 = \beta_0 \qquad \mathcal{H}_0: \beta_0 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This means that there is just one &lt;span class=&quot;math inline&quot;&gt;\(y = y_2 - y_1\)&lt;/span&gt; to predict and it becomes a &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;one-sample t-test&lt;/a&gt; on the pairwise differences. The visualization is therefore also the same as for the one-sample t-test. At the risk of overcomplicating a simple substraction, you can think of these pairwise differences as slopes (see left panel of the figure), which we can represent as y-offsets (see right panel of the figure):&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;21&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Data for plot
N = nrow(D_t1)
start = rnorm_fixed(N, 0.2, 0.3)
D_tpaired = data.frame(x = rep(c(0, 1), each=N), y = c(start, start + D_t1$y), id=1:N)

# Plot
P_tpaired = ggplot(D_tpaired, aes(x=x, y=y)) + 
  geom_line(aes(group=id)) + 
  labs(title='          Pairs')

# Use patchwork to put them side-by-side
theme_axis(P_tpaired) + theme_axis(P_t1, legend.position=c(0.6, 0.1))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-19-1.png&quot; width=&quot;672&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Similarly, the &lt;strong&gt;Wilcoxon matched pairs&lt;/strong&gt; only differ from &lt;strong&gt;Wilcoxon signed-rank&lt;/strong&gt; in that it’s testing the signed ranks of the pairwise &lt;span class=&quot;math inline&quot;&gt;\(y-x\)&lt;/span&gt; differences.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(signed\_rank(y_2-y_1) = \beta_0 \qquad \mathcal{H}_0: \beta_0 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-paired-sample-t-test&quot; class=&quot;section level3&quot; readability=&quot;13.5&quot;&gt;
&lt;h3&gt; R code: Paired sample t-test&lt;/h3&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;a = t.test(y, y2, paired = TRUE) # Built-in paired t-test
b = lm(y - y2 ~ 1) # Equivalent linear model&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Paired t-test
## 
## data:  y and y2
## t = -0.52642, df = 49, p-value = 0.601
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.9634894  0.5634894
## sample estimates:
## mean of the differences 
##                    -0.2 
## 
## 
## Call:
## lm(formula = y - y2 ~ 1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.5253 -1.5642 -0.0844  1.9715  5.0361 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)  -0.2000     0.3799  -0.526    0.601
## 
## Residual standard error: 2.686 on 49 degrees of freedom&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-wilcoxon-matched-pairs&quot; class=&quot;section level3&quot; readability=&quot;24&quot;&gt;
&lt;h3&gt; R code: Wilcoxon matched pairs&lt;/h3&gt;
&lt;p&gt;Again, we do the signed-ranks trick. This is still an approximation, but a close one:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in Wilcoxon matched pairs
a = wilcox.test(y, y2, paired = TRUE)

# Equivalent linear model:
b = lm(signed_rank(y - y2) ~ 1)

# Bonus: identical to one-sample t-test ong signed ranks
c = t.test(signed_rank(y - y2))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  y and y2
## V = 614, p-value = 0.8243
## alternative hypothesis: true location shift is not equal to 0
## 
## 
## Call:
## lm(formula = signed_rank(y - y2) ~ 1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -49.06 -23.81  -2.56  26.19  46.94 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   -0.940      4.184  -0.225    0.823
## 
## Residual standard error: 29.58 on 49 degrees of freedom
## 
## 
##  One Sample t-test
## 
## data:  signed_rank(y - y2)
## t = -0.22469, df = 49, p-value = 0.8232
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -9.347227  7.467227
## sample estimates:
## mean of x 
##     -0.94&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For large sample sizes (N &amp;gt;&amp;gt; 100), this approaches the &lt;strong&gt;sign test&lt;/strong&gt; to a reasonable degree, but this approximation is too inaccurate to flesh out here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;two-means&quot; class=&quot;section level1&quot; readability=&quot;10.762569832402&quot;&gt;

&lt;div id=&quot;t2&quot; class=&quot;section level2&quot; readability=&quot;47.3558125097&quot;&gt;
&lt;h2&gt; Independent t-test and Mann-Whitney U&lt;/h2&gt;
&lt;div id=&quot;theory-as-linear-models-3&quot; class=&quot;section level3&quot; readability=&quot;19.933110367893&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;Independent t-test model: two means predict &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y_i = \beta_0 + \beta_1 x_i \qquad \mathcal{H}_0: \beta_1 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; is an indicator (0 or 1) saying whether data point &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; was sampled from one or the other group. &lt;a href=&quot;https://en.wikipedia.org/wiki/Dummy_variable_(statistics)&quot;&gt;Indicator variables (also called “dummy coding”)&lt;/a&gt; underly a lot of linear models and we’ll take an aside to see how it works in a minute.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mann-Whitney U&lt;/strong&gt; (also known as &lt;strong&gt;Wilcoxon signed-rank test&lt;/strong&gt; for two independent groups) is the same model to a very close approximation, just on the ranks of &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; instead of the actual values:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(rank(y_i) = \beta_0 + \beta_1 x_i \qquad \mathcal{H}_0: \beta_1 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To me, equivalences like this make “non-parametric” statistics much easier to understand. The approximation is appropriate &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/simulate_mannwhitney.html&quot;&gt;when the sample size is larger than 11 in each group and virtually perfect when N &amp;gt; 30 in each group&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;dummy&quot; class=&quot;section level3&quot; readability=&quot;37&quot;&gt;
&lt;h3&gt; Theory: Dummy coding&lt;/h3&gt;
&lt;p&gt;Dummy coding can be understood visually. The indicator is on the x-axis so data points from the first group are located at &lt;span class=&quot;math inline&quot;&gt;\(x = 0\)&lt;/span&gt; and data points from the second group is located at &lt;span class=&quot;math inline&quot;&gt;\(x = 1\)&lt;/span&gt;. Then &lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt; is the intercept (red line) and &lt;span class=&quot;math inline&quot;&gt;\(\beta_1\)&lt;/span&gt; is the slope between the two means (green line). Why? Because when &lt;span class=&quot;math inline&quot;&gt;\(\Delta x = 1\)&lt;/span&gt; the slope equals the difference because:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(slope = \Delta y / \Delta x = \Delta y / 1 = \Delta y = difference\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Magic! Even categorical differences can be modelled using linear models! It’s a true Swizz army knife.&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;49&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Data
N = 20  # Number of data points per group
D_t2 = data.frame(
  x = rep(c(0, 1), each=N),
  y = c(rnorm_fixed(N, 0.3, 0.3), rnorm_fixed(N, 1.3, 0.3))
)

# Plot
P_t2 = ggplot(D_t2, aes(x=x, y=y)) + 
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y.., color='something'), lwd=2) +
  geom_segment(x=-10, xend=10, y=0.3, yend=0.3, lwd=2, aes(color='beta_0')) + 
  geom_segment(x=0, xend=1, y=0.3, yend=1.3, lwd=2, aes(color='beta_1')) + 
  
  scale_color_manual(name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;, &quot;darkblue&quot;), labels=c(bquote(beta[0]*&quot; (group 1 mean)&quot;), bquote(beta[1]*&quot; (slope = difference)&quot;), bquote(beta[0]+beta[1]%.%1*&quot; (group 2 mean)&quot;)))
  #scale_x_discrete(breaks=c(0.5, 1.5), labels=c('1', '2'))

theme_axis(P_t2, jitter=TRUE, xlim=c(-0.3, 2), legend.position=c(0.53, 0.08))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-26-1.png&quot; width=&quot;576&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;dummy2&quot; class=&quot;section level3&quot; readability=&quot;37&quot;&gt;
&lt;h3&gt; Theory: Dummy coding (continued)&lt;/h3&gt;
&lt;p&gt;If you feel like you get dummy coding now, just skip ahead to the next section. Here is a more elaborate explanation of dummy coding:&lt;/p&gt;
&lt;p&gt;If a data point was sampled from the first group, i.e., when &lt;span class=&quot;math inline&quot;&gt;\(x_i = 0\)&lt;/span&gt;, the model simply becomes &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1 \cdot 0 = \beta_0\)&lt;/span&gt;. In other words, the model predicts that that data point is &lt;span class=&quot;math inline&quot;&gt;\(beta_0\)&lt;/span&gt;. It turns out that the &lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt; which best predicts a set of data points is the &lt;em&gt;mean&lt;/em&gt; of those data points, so &lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt; is the mean of group 1.&lt;/p&gt;
&lt;p&gt;On the other hand, data points sampled from the second group would have &lt;span class=&quot;math inline&quot;&gt;\(x_i = 1\)&lt;/span&gt; so the model becomes &lt;span class=&quot;math inline&quot;&gt;\(y_i = \beta_0 + \beta_1\cdot 1 = \beta_0 + \beta_1\)&lt;/span&gt;. In other words, we add &lt;span class=&quot;math inline&quot;&gt;\(\beta_1\)&lt;/span&gt; to “shift” from the mean of the first group to the mean of the second group. Thus &lt;span class=&quot;math inline&quot;&gt;\(\beta_1\)&lt;/span&gt; becomes the &lt;em&gt;mean difference&lt;/em&gt; between the groups.&lt;/p&gt;
&lt;p&gt;As an example, say group 1 is 25 years old (&lt;span class=&quot;math inline&quot;&gt;\(\beta_0 = 25\)&lt;/span&gt;) and group 2 is 28 years old (&lt;span class=&quot;math inline&quot;&gt;\(\beta_1 = 3\)&lt;/span&gt;), then the model for a person in group 1 is &lt;span class=&quot;math inline&quot;&gt;\(y = 25 + 3 \cdot 0 = 25\)&lt;/span&gt; and the model for a person in group 2 is &lt;span class=&quot;math inline&quot;&gt;\(y = 25 + 3 \cdot 1 = 28\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Hooray, it works! For first-timers it takes a few moments to understand dummy coding, but you only need to know addition and multiplication to get there!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-independent-t-test&quot; class=&quot;section level3&quot; readability=&quot;32.5&quot;&gt;
&lt;h3&gt; R code: independent t-test&lt;/h3&gt;
&lt;p&gt;As a reminder, when we write &lt;code&gt;y ~ 1 + x&lt;/code&gt; in R, it is shorthand for &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 \cdot 1 + \beta_1 \cdot x\)&lt;/span&gt; and R goes on computing the &lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt;s for you. Thus &lt;span class=&quot;math inline&quot;&gt;\(y ~ 1 + x\)&lt;/span&gt; is the R-way of writing &lt;span class=&quot;math inline&quot;&gt;\(y = a \cdot x + b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice the identical &lt;code&gt;t&lt;/code&gt;, &lt;code&gt;df&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, and estimates. We can get the confidence interval by running &lt;code&gt;confint(lm(...))&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in independent t-test on wide data
a = t.test(y, y2, var.equal = TRUE)

# Be explicit about the underlying linear model by hand-dummy-coding:
group_y2 = ifelse(group == 'y2', 1, 0)  # 1 if group == y2, 0 otherwise
b = lm(value ~ 1 + group_y2)  # Using our hand-made dummy regressor

# Note: We could also do the dummy-coding in the model 
# specification itself. Same result.
c = lm(value ~ 1 + I(group=='y2'))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;16&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  y and y2
## t = -0.56569, df = 98, p-value = 0.5729
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.9016152  0.5016152
## sample estimates:
## mean of x mean of y 
##       0.3       0.5 
## 
## 
## Call:
## lm(formula = value ~ 1 + group_y2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5211 -1.1259 -0.2124  0.9151  4.9268 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)   0.3000     0.2500   1.200    0.233
## group_y2      0.2000     0.3536   0.566    0.573
## 
## Residual standard error: 1.768 on 98 degrees of freedom
## Multiple R-squared:  0.003255,   Adjusted R-squared:  -0.006916 
## F-statistic:  0.32 on 1 and 98 DF,  p-value: 0.5729
## 
## 
## Call:
## lm(formula = value ~ 1 + I(group == &quot;y2&quot;))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5211 -1.1259 -0.2124  0.9151  4.9268 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)            0.3000     0.2500   1.200    0.233
## I(group == &quot;y2&quot;)TRUE   0.2000     0.3536   0.566    0.573
## 
## Residual standard error: 1.768 on 98 degrees of freedom
## Multiple R-squared:  0.003255,   Adjusted R-squared:  -0.006916 
## F-statistic:  0.32 on 1 and 98 DF,  p-value: 0.5729&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-mann-whitney-u&quot; class=&quot;section level3&quot; readability=&quot;13&quot;&gt;
&lt;h3&gt; R code: Mann-Whitney U&lt;/h3&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Wilcoxon / Mann-Whitney U
a = wilcox.test(y, y2)

# As linear model with our dummy-coded group_y2:
b = lm(rank(value) ~ 1 + group_y2)  # compare to linear model above&lt;/code&gt;
&lt;/pre&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  y and y2
## W = 1211, p-value = 0.7907
## alternative hypothesis: true location shift is not equal to 0
## 
## 
## Call:
## lm(formula = rank(value) ~ 1 + group_y2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -48.72 -24.64  -0.78  24.22  48.72 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   49.720      4.122  12.061   &amp;lt;2e-16 ***
## group_y2       1.560      5.830   0.268     0.79    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 29.15 on 98 degrees of freedom
## Multiple R-squared:  0.0007302,  Adjusted R-squared:  -0.009466 
## F-statistic: 0.07161 on 1 and 98 DF,  p-value: 0.7896&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;welch&quot; class=&quot;section level2&quot; readability=&quot;20.240506329114&quot;&gt;
&lt;h2&gt; Welch’s t-test&lt;/h2&gt;
&lt;p&gt;This is identical to the (Student’s) &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t2&quot;&gt;independent t-test&lt;/a&gt; above except that Student’s assumes identical variances and &lt;strong&gt;Welch’s t-test&lt;/strong&gt; does not. So the linear model is the same and the trick is in the variances, which I won’t go further into here.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in
a = t.test(y, y2, var.equal=FALSE)

# As linear model with per-group variances
b = nlme::gls(value ~ 1 + group_y2, weights = nlme::varIdent(form=~1|group), method=&quot;ML&quot;)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Welch Two Sample t-test
## 
## data:  y and y2
## t = -0.56569, df = 90.875, p-value = 0.573
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.9023034  0.5023034
## sample estimates:
## mean of x mean of y 
##       0.3       0.5 
## 
## Generalized least squares fit by maximum likelihood
##   Model: value ~ 1 + group_y2 
##   Data: NULL 
##        AIC      BIC    logLik
##   399.6287 410.0493 -195.8143
## 
## Variance function:
##  Structure: Different standard deviations per stratum
##  Formula: ~1 | group 
##  Parameter estimates:
##   y1   y2 
## 1.00 0.75 
## 
## Coefficients:
##             Value Std.Error   t-value p-value
## (Intercept)   0.3 0.2828427 1.0606602  0.2915
## group_y2      0.2 0.3535534 0.5656854  0.5729
## 
##  Correlation: 
##          (Intr)
## group_y2 -0.8  
## 
## Standardized residuals:
##        Min         Q1        Med         Q3        Max 
## -1.7784113 -0.7177541 -0.1430665  0.5635189  3.3178730 
## 
## Residual standard error: 1.979899 
## Degrees of freedom: 100 total; 98 residual&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;three-or-more-means&quot; class=&quot;section level1&quot; readability=&quot;29.460461285008&quot;&gt;

&lt;p&gt;ANOVAs are linear models with (only) categorical predictors so they simply extend everything we did above, relying heavily on dummy coding. Do make sure to read &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#dummy&quot;&gt;the section on dummy coding&lt;/a&gt; if you haven’t already.&lt;/p&gt;
&lt;div id=&quot;anova1&quot; class=&quot;section level2&quot; readability=&quot;48.821218074656&quot;&gt;
&lt;h2&gt; One-way ANOVA and Kruskal-Wallis&lt;/h2&gt;
&lt;div id=&quot;theory-as-linear-models-4&quot; class=&quot;section level3&quot; readability=&quot;65.7215815486&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;Model: One mean for each group predicts &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 +... \qquad \mathcal{H}_0: y = \beta_1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; are indicators (&lt;span class=&quot;math inline&quot;&gt;\(x=0\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(x=1\)&lt;/span&gt;) where at most one &lt;span class=&quot;math inline&quot;&gt;\(x_i=1\)&lt;/span&gt; while all others are &lt;span class=&quot;math inline&quot;&gt;\(x_i=0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice how this is just “more of the same” of what we already did in other models above. When there are only two groups, this model is &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1*x\)&lt;/span&gt;, i.e. the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t2&quot;&gt;independent t-test&lt;/a&gt;. If there is only one group, it is &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0\)&lt;/span&gt;, i.e. the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;one-sample t-test&lt;/a&gt;. This is easy to see in the visualization below - just cover up a few groups and see that it matches the other visualizations above, though I did omit adding green lines from the intercept (red) to the group means (blue) for visual clarity.&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;66&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Figure
N = 15
D_anova1 = data.frame(
  y=c(rnorm_fixed(N, 0.5, 0.3), 
      rnorm_fixed(N, 0, 0.3), 
      rnorm_fixed(N, 1, 0.3), 
      rnorm_fixed(N, 0.8, 0.3)), 
  x=rep(0:3, each=15)
)
ymeans = aggregate(y~x, D_anova1, mean)$y
P_anova1 = ggplot(D_anova1, aes(x=x, y=y)) + 
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y.., color='intercepts'), lwd=2) + 
  geom_segment(x=-10, xend=100, y=0.5, yend=0.5, lwd=2, aes(color='beta_0')) +
  geom_segment(x=0, xend=1, y=ymeans[1], yend=ymeans[2], lwd=2, aes(color='betas')) +
  geom_segment(x=1, xend=2, y=ymeans[1], yend=ymeans[3], lwd=2, aes(color='betas')) +
  geom_segment(x=2, xend=3, y=ymeans[1], yend=ymeans[4], lwd=2, aes(color='betas')) +
  
  scale_color_manual(
    name=NULL, values=c(&quot;blue&quot;, &quot;red&quot;, &quot;darkblue&quot;), 
    labels=c(
      bquote(beta[0]*&quot; (group 1 mean)&quot;), 
      bquote(beta[1]*&quot;, &quot;*beta[2]*&quot;,  etc. (slopes/differences to &quot;*beta[0]*&quot;)&quot;),
      bquote(beta[0]*&quot;+&quot;*beta[1]*&quot;, &quot;*beta[0]*&quot;+&quot;*beta[2]*&quot;, etc. (absolute intercepts)&quot;)
    )
  )
  

theme_axis(P_anova1, xlim=c(-0.5, 4), legend.position=c(0.7, 0.1))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-36-1.png&quot; width=&quot;576&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A one-way ANOVA has a log-linear counterpart called &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#goodness&quot;&gt;goodness-of-fit&lt;/a&gt; test which we’ll return to. By the way, since we now regress on more than one &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;, the one-way ANOVA is a &lt;strong&gt;multiple regression&lt;/strong&gt; model.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Kruskal-Wallis&lt;/strong&gt; test is simply a &lt;strong&gt;one-way ANOVA&lt;/strong&gt; on the rank-transformed &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; (&lt;code&gt;value&lt;/code&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(rank(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 +...\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This approximation is &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/simulate_kruskall.html&quot;&gt;good enough for 12 or more data points&lt;/a&gt;. Again, if you do this for just one or two groups, we’re already acquainted with those equations, i.e. the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;Wilcoxon signed-rank test&lt;/a&gt; or the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t2&quot;&gt;Mann-Whitney U test&lt;/a&gt; respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;example-data&quot; class=&quot;section level3&quot; readability=&quot;38.359781121751&quot;&gt;
&lt;h3&gt; Example data&lt;/h3&gt;
&lt;p&gt;We make a three-level factor with the levels &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt;, and &lt;code&gt;c&lt;/code&gt; so that the &lt;strong&gt;one-way ANOVA&lt;/strong&gt; basically becomes a “three-sample t-test”. Then we manually do the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#dummy&quot;&gt;dummy coding&lt;/a&gt; of the groups.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Three variables in &quot;long&quot; format
N = 20  # Number of samples per group
D = data.frame(
   value = c(rnorm_fixed(N, 0), rnorm_fixed(N, 1), rnorm_fixed(N, 0.5)), 
   group = rep(c('a', 'b', 'c'), each=N),
   
   # Explicitly add indicator/dummy variables
   # Could also be done using model.matrix(~D$group)
   group_a = rep(c(1, 0, 0), each=N),
   group_b = rep(c(0, 1, 0), each=N),
   group_c = rep(c(0, 0, 1), each=N))  # N of each level&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;See? Exactly one parameter predicts a &lt;code&gt;value&lt;/code&gt; in a given row while the others are not included in the modeling of that &lt;code&gt;value&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-one-way-anova&quot; class=&quot;section level3&quot; readability=&quot;23&quot;&gt;
&lt;h3&gt; R code: one-way ANOVA&lt;/h3&gt;
&lt;p&gt;OK, let’s see the identity between the built-in &lt;strong&gt;ANOVA&lt;/strong&gt; (&lt;code&gt;car::Anova&lt;/code&gt;) and the dummy-coded in-your-face linear model in &lt;code&gt;lm&lt;/code&gt;. Actually, &lt;code&gt;car::Anova&lt;/code&gt; and &lt;code&gt;aov&lt;/code&gt; are just wrappers around &lt;code&gt;lm&lt;/code&gt; so the identity comes as no surprise. The latter returns parameter estimates as well (bonus!), but we’ll just look at the overall model statistics for now. Note that I do not use the &lt;code&gt;aov&lt;/code&gt; function because it computes type-I sum of squares. There is a BIG polarized debate about whether to use type-II (as &lt;code&gt;car::Anova&lt;/code&gt; does by default) or type-III sum of squares, but let’s skip that for now.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Compare built-in and linear model
a = car::Anova(aov(value ~ group, D))  # Built-in ANOVA
b = lm(value ~ 1 + group_a + group_b + group_c, data=D)  # As in-your-face linear model&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;13&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## Anova Table (Type II tests)
## 
## Response: value
##           Sum Sq Df F value   Pr(&amp;gt;F)   
## group         10  2       5 0.009984 **
## Residuals     57 57                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Call:
## lm(formula = value ~ 1 + group_a + group_b + group_c, data = D)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4402 -0.6427 -0.1393  0.8060  1.8574 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)   0.5000     0.2236   2.236   0.0293 *
## group_a      -0.5000     0.3162  -1.581   0.1194  
## group_b       0.5000     0.3162   1.581   0.1194  
## group_c           NA         NA      NA       NA  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1 on 57 degrees of freedom
## Multiple R-squared:  0.1493, Adjusted R-squared:  0.1194 
## F-statistic:     5 on 2 and 57 DF,  p-value: 0.009984&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-kruskal-wallis&quot; class=&quot;section level3&quot; readability=&quot;18&quot;&gt;
&lt;h3&gt; R code: Kruskal-Wallis&lt;/h3&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;a = kruskal.test(value ~ group, D)  # Built-in
b = lm(rank(value) ~ 1 + group_a + group_b + group_c, D)  # As linear model
c = car::Anova(aov(rank(value) ~ group, D))  # Of course the same using the built-in ANOVA, which is just a wrapper around lm&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;15&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Kruskal-Wallis rank sum test
## 
## data:  value by group
## Kruskal-Wallis chi-squared = 6.6777, df = 2, p-value = 0.03548
## 
## 
## Call:
## lm(formula = rank(value) ~ 1 + group_a + group_b + group_c, data = D)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -28.950 -12.213  -0.675  15.912  26.850 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   30.950      3.741   8.272 2.43e-11 ***
## group_a       -7.800      5.291  -1.474    0.146    
## group_b        6.450      5.291   1.219    0.228    
## group_c           NA         NA      NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 16.73 on 57 degrees of freedom
## Multiple R-squared:  0.1132, Adjusted R-squared:  0.08206 
## F-statistic: 3.637 on 2 and 57 DF,  p-value: 0.03261
## 
## Anova Table (Type II tests)
## 
## Response: rank(value)
##            Sum Sq Df F value  Pr(&amp;gt;F)  
## group      2036.7  2  3.6374 0.03261 *
## Residuals 15958.3 57                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;anova2&quot; class=&quot;section level2&quot; readability=&quot;41.169226283048&quot;&gt;
&lt;h2&gt; Two-way ANOVA (plot in progress!)&lt;/h2&gt;
&lt;div id=&quot;theory-as-linear-models-5&quot; class=&quot;section level3&quot; readability=&quot;47.305957200694&quot;&gt;
&lt;h3&gt; Theory: As linear models&lt;/h3&gt;
&lt;p&gt;Model: one mean per group (main effects) plus these means multiplied across factors (interaction effects). The main effects are the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova1&quot;&gt;one-way ANOVA&lt;/a&gt;s above, though in the context of a larger model. The interaction effect is harder to explain in the abstract even though it’s just a few numbers multiplied with each other. I will leave that to the teachers to keep focus on equivalences here :-)&lt;/p&gt;
&lt;p&gt;Switching to matrix notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 \qquad \mathcal{H}_0: \beta_3 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;span class=&quot;math inline&quot;&gt;\(\beta_i\)&lt;/span&gt; are vectors of betas of which only one is selected by the indicator vector &lt;span class=&quot;math inline&quot;&gt;\(X_i\)&lt;/span&gt;. The &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{H}_0\)&lt;/span&gt; shown here is the interaction effect. Note that the intercept &lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt;, to which all other &lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt;s are relative, is now the mean for the first level of all factors.&lt;/p&gt;
&lt;p&gt;Continuing with the dataset from the one-way ANOVA above, let’s add a crossing factor &lt;code&gt;mood&lt;/code&gt; so that we can test the &lt;code&gt;group:mood&lt;/code&gt; interaction (a 3x2 ANOVA). We also do the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#dummy&quot;&gt;dummy coding&lt;/a&gt; of this factor needed for the linear model.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Crossing factor
D$mood = c('happy', 'sad')

# Dummy coding
D$mood_happy = ifelse(D$mood == 'happy', 1, 0)  # 1 if mood==happy. 0 otherwise.
D$mood_sad = ifelse(D$mood == 'sad', 1, 0)  # Same, but we won't be needing this&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt; is now the happy guys from group a!&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;25&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Add intercept line
# Add cross...
# Use other data?

means = lm(value ~ mood * group, D)$coefficients

P_anova2 = ggplot(D, aes(x=group, y=value, color=mood)) + 
  geom_segment(x=-10, xend=100, y=means[1], yend=0.5, col='blue', lwd=2) +
  stat_summary(fun.y=mean, geom = &quot;errorbar&quot;, aes(ymax = ..y.., ymin = ..y..),  lwd=2)
theme_axis(P_anova2, xlim=c(-0.5, 3.5)) + theme(axis.text.x = element_text())&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-47-1.png&quot; width=&quot;576&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;r-code-two-way-anova&quot; class=&quot;section level3&quot; readability=&quot;52.749112426036&quot;&gt;
&lt;h3&gt; R code: Two-way ANOVA&lt;/h3&gt;
&lt;p&gt;Now let’s turn to the actual modeling in R. We compare the built-in ANOVA function to the linear model using &lt;code&gt;lm&lt;/code&gt;. Notice that in ANOVA, we are testing a full factor interaction all at once which involves many parameters (two in this case), so we can’t look at the overall model fit nor any particular parameter for the result. Therefore, I use a likelihood-ratio test to compare a full two-way ANOVA model (“saturated”) to one without the interaction effect(s). We do so using the &lt;code&gt;anova&lt;/code&gt; function. Even though that looks like cheating, it’s just computing likelihoods, p-values, etc. on the models that were already fitted, so it’s legit!&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in two-way ANOVA.
a = car::Anova(aov(value ~ mood * group, D), type='II')  # Normal notation. &quot;*&quot; both multiplies and adds main effects
b = car::Anova(aov(value ~ mood + group + mood:group, D))  # Identical but more verbose about main effects and interaction

# Testing the interaction terms as linear model.
full = lm(value ~ 1 + group_a + group_b + mood_happy + group_a:mood_happy + group_b:mood_happy, D)  # Full model
null = lm(value ~ 1 + group_a + group_b + mood_happy, D)  # Without interaction
c = anova(null, full)  # whoop whoop, same F, p, and Dfs&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;10&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## Anova Table (Type II tests)
## 
## Response: value
##            Sum Sq Df F value  Pr(&amp;gt;F)  
## mood        0.658  1  0.6314 0.43032  
## group      10.000  2  4.8003 0.01206 *
## mood:group  0.096  2  0.0459 0.95520  
## Residuals  56.247 54                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## Analysis of Variance Table
## 
## Model 1: value ~ 1 + group_a + group_b + mood_happy
## Model 2: value ~ 1 + group_a + group_b + mood_happy + group_a:mood_happy + 
##     group_b:mood_happy
##   Res.Df    RSS Df Sum of Sq      F Pr(&amp;gt;F)
## 1     56 56.342                           
## 2     54 56.247  2  0.095565 0.0459 0.9552&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Below, I present approximate main effect models, though exact calculation of ANOVA main effects &lt;a href=&quot;https://stats.idre.ucla.edu/stata/faq/how-can-i-get-anova-simple-main-effects-with-dummy-coding/&quot;&gt;is more involved&lt;/a&gt; if it is to be accurate and furthermore depend on whether type-II or type-III sum of squares are used for inference.&lt;/p&gt;
&lt;p&gt;Look at the model summary statistics to find values comparable to the &lt;code&gt;Anova&lt;/code&gt;-estimated main effects above.&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Main effect of group.
e = lm(value ~ 1 + group_a + group_b, D)

# Main effect of mood.
f = lm(value ~ 1 + mood_happy, D)&lt;/code&gt;
&lt;/pre&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;16&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
## Call:
## lm(formula = value ~ 1 + group_a + group_b, data = D)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4402 -0.6427 -0.1393  0.8060  1.8574 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)   0.5000     0.2236   2.236   0.0293 *
## group_a      -0.5000     0.3162  -1.581   0.1194  
## group_b       0.5000     0.3162   1.581   0.1194  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1 on 57 degrees of freedom
## Multiple R-squared:  0.1493, Adjusted R-squared:  0.1194 
## F-statistic:     5 on 2 and 57 DF,  p-value: 0.009984
## 
## 
## Call:
## lm(formula = value ~ 1 + mood_happy, data = D)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.65275 -0.70847  0.00213  0.63391  1.88950 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)   
## (Intercept)   0.6047     0.1953   3.097  0.00301 **
## mood_happy   -0.2094     0.2761  -0.758  0.45136   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.07 on 58 degrees of freedom
## Multiple R-squared:  0.009816,   Adjusted R-squared:  -0.007256 
## F-statistic: 0.575 on 1 and 58 DF,  p-value: 0.4514&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;ancova&quot; class=&quot;section level2&quot; readability=&quot;53.614972393634&quot;&gt;
&lt;h2&gt; ANCOVA&lt;/h2&gt;
&lt;p&gt;This is simply ANOVA with a continuous regressor added so that it now contains continuous and (dummy-coded) categorical predictors. For example, if we continue with the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova1&quot;&gt;one-way ANOVA&lt;/a&gt; example, we can add &lt;code&gt;age&lt;/code&gt; and it is now called a &lt;strong&gt;one-way ANCOVA&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_3 age\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;… where &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; are our usual dummy-coded indicator variables. &lt;span class=&quot;math inline&quot;&gt;\(\beta_0\)&lt;/span&gt; is now the mean for the first group at &lt;span class=&quot;math inline&quot;&gt;\(age=0\)&lt;/span&gt;. You can turn all ANOVAs into ANCOVAs this way, e.g. by adding &lt;span class=&quot;math inline&quot;&gt;\(\beta_N \cdot age\)&lt;/span&gt; to our &lt;strong&gt;two-way ANOVA&lt;/strong&gt; in the previous section. But let us go ahead with our one-way ANCOVA, starting by adding &lt;span class=&quot;math inline&quot;&gt;\(age\)&lt;/span&gt; to our dataset:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Update data with a continuous covariate
D$age = D$value + rnorm_fixed(nrow(D), sd=3)  # Correlated to value&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is best visualized using colors for groups instead of x-position. The &lt;span class=&quot;math inline&quot;&gt;\(\beta\)&lt;/span&gt;s are still the average &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;-offset of the data points, only now we model each group using a slope instead of an intercept. In other words, the one-way ANOVA is sort of &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;one-sample t-tests&lt;/a&gt; model for each group (&lt;span class=&quot;math inline&quot;&gt;\(y = \beta_0\)&lt;/span&gt;) while the &lt;strong&gt;one-way ANCOVA&lt;/strong&gt; is sort of &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#correlation&quot;&gt;Pearson correlation&lt;/a&gt; model for each group (&lt;span class=&quot;math inline&quot;&gt;\(y_i = \beta_0 + \beta_i + \beta_1 \cdot age\)&lt;/span&gt;):&lt;/p&gt;
&lt;div class=&quot;fold s&quot; readability=&quot;19&quot;&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# For linear model plot
D$pred = predict(lm(value ~ age + group, D))

# Plot
P_ancova = ggplot(D, aes(x=age, y=value, color=group, shape=group)) + 
  geom_line(aes(y=pred), lwd=2)

# Theme it
theme_axis(P_ancova, xlim=NULL, ylim=NULL, legend.position=c(0.8, 0.2)) + theme(axis.title=element_text())&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;https://lindeloev.github.io/tests-as-linear/index_files/figure-html/unnamed-chunk-55-1.png&quot; width=&quot;576&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;And now some R code to run the one-way ANCOVA as a linear model:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in ANCOVA. The order of factors matter in pure-aov (type-I variance).
# Use type-II or type-III instead; implemented in car::Anova
a = car::Anova(aov(value ~ group + age, D))
#a = aov(value ~ group + age, D)  # Predictor order matters. Not nice!

# As dummy-coded linear model. 
full = lm(value ~ 1 + group_a + group_b + age, D)

# Testing main effect of age using Likelihood-ratio test
null_age = lm(value ~ 1 + group_a + group_b, D)  # Full without age. One-way ANOVA!
result_age = anova(null_age, full)

# Testing main effect of groupusing Likelihood-ratio test
null_group = lm(value ~ 1 + age, D)  # Full without group. Pearson correlation!
result_group = anova(null_group, full)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;10&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## Anova Table (Type II tests)
## 
## Response: value
##           Sum Sq Df F value    Pr(&amp;gt;F)    
## group     10.118  2  6.4258 0.0030738 ** 
## age       12.910  1 16.3967 0.0001595 ***
## Residuals 44.090 56                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## Analysis of Variance Table
## 
## Model 1: value ~ 1 + group_a + group_b
## Model 2: value ~ 1 + group_a + group_b + age
##   Res.Df   RSS Df Sum of Sq      F    Pr(&amp;gt;F)    
## 1     57 57.00                                  
## 2     56 44.09  1     12.91 16.397 0.0001595 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## Analysis of Variance Table
## 
## Model 1: value ~ 1 + age
## Model 2: value ~ 1 + group_a + group_b + age
##   Res.Df    RSS Df Sum of Sq      F   Pr(&amp;gt;F)   
## 1     58 54.209                                
## 2     56 44.090  2    10.118 6.4258 0.003074 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;proportions-chi-square-is-a-log-linear-model&quot; class=&quot;section level1&quot; readability=&quot;13.685546875&quot;&gt;

&lt;p&gt;Recall that when you take the logarithm, you can easily make statements about &lt;em&gt;proportions&lt;/em&gt;, i.e., that for every increase in &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; increases a certain percentage. This turns out to be one of the simplest (and therefore best!) ways to make count data and contingency tables intelligible. See &lt;a href=&quot;https://www.uni-tuebingen.de/fileadmin/Uni_Tuebingen/SFB/SFB_833/A_Bereich/A1/Christoph_Scheepers_-_Statistikworkshop.pdf&quot;&gt;this nice introduction&lt;/a&gt; to Chi-Square tests as linear models.&lt;/p&gt;
&lt;div id=&quot;goodness&quot; class=&quot;section level2&quot; readability=&quot;33.140794223827&quot;&gt;
&lt;h2&gt; Goodness of fit&lt;/h2&gt;
&lt;div id=&quot;theory-as-log-linear-model&quot; class=&quot;section level3&quot; readability=&quot;8.3076923076923&quot;&gt;
&lt;h3&gt; Theory: As log-linear model&lt;/h3&gt;
&lt;p&gt;Model: a single intercept predicts &lt;span class=&quot;math inline&quot;&gt;\(log(y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I’ll refer you to take a look at &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#contingency&quot;&gt;the section on contingency tables&lt;/a&gt; which is basically a “two-way goodness of fit”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;example-data-1&quot; class=&quot;section level3&quot; readability=&quot;21&quot;&gt;
&lt;h3&gt; Example data&lt;/h3&gt;
&lt;p&gt;For this, we need some wide count data:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Data in long format
D = data.frame(mood = c('happy', 'sad', 'meh'),
               counts = c(60, 90, 70))

# Dummy coding for the linear model
D$mood_happy = ifelse(D$mood == 'happy', 1, 0)
D$mood_sad = ifelse(D$mood == 'sad', 1, 0)&lt;/code&gt;
&lt;/pre&gt;

&lt;/div&gt;
&lt;div id=&quot;r-code-goodness-of-fit&quot; class=&quot;section level3&quot; readability=&quot;44.895436164067&quot;&gt;
&lt;h3&gt; R code: Goodness of fit&lt;/h3&gt;
&lt;p&gt;Now let’s see that the Goodness of fit is just a log-linear equivalent to a one-way ANOVA. We set &lt;code&gt;family = poisson()&lt;/code&gt; which defaults to setting a logarithmic &lt;a href=&quot;https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function&quot;&gt;link function&lt;/a&gt; (&lt;code&gt;family = poisson(link='log')&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in test
a = chisq.test(D$counts)

# As log-linear model, comparing to an intercept-only model
full = glm(counts ~ 1 + mood_happy + mood_sad, data=D, family=poisson())
null = glm(counts ~ 1, data=D, family=poisson())
b = anova(null, full, test='Rao')

# Note: glm can also do the dummy coding for you:
c = glm(counts ~ mood, data=D, family=poisson())&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Let’s look at the results:&lt;/p&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Chi-squared test for given probabilities
## 
## data:  D$counts
## X-squared = 6.3636, df = 2, p-value = 0.04151
## 
## Analysis of Deviance Table
## 
## Model 1: counts ~ 1
## Model 2: counts ~ 1 + mood_happy + mood_sad
##   Resid. Df Resid. Dev Df Deviance    Rao Pr(&amp;gt;Chi)  
## 1         2     6.2697                              
## 2         0     0.0000  2   6.2697 6.3636  0.04151 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the strange &lt;code&gt;anova(..., test='Rao')&lt;/code&gt; which merely states that p-values should be computed using the (Rao) &lt;a href=&quot;https://en.wikipedia.org/wiki/Score_test&quot;&gt;score test&lt;/a&gt;. We could also have jotted in &lt;code&gt;test='Chisq'&lt;/code&gt; or &lt;code&gt;test='LRT'&lt;/code&gt; which would have yielded approximate p-values. You may think that we’re cheating here, sneaking in some sort of Chi-Square model post-hoc. However, &lt;code&gt;anova&lt;/code&gt; only specifies how p-values are calculated whereas all the log-linear modeling happened in &lt;code&gt;glm&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;By the way, if there are only two counts and a large sample size (N &amp;gt; 100), this model begins to approximate the &lt;strong&gt;binomial test&lt;/strong&gt;, &lt;code&gt;binom.test&lt;/code&gt;, to a reasonable degree. But this sample size is larger than most use cases, so I won’t raise to a rule-of-thumb and won’t dig deeper into it here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;contingency&quot; class=&quot;section level2&quot; readability=&quot;53.953509244301&quot;&gt;
&lt;h2&gt; Contingency tables&lt;/h2&gt;
&lt;div id=&quot;theory-as-log-linear-model-1&quot; class=&quot;section level3&quot; readability=&quot;46.625325690464&quot;&gt;
&lt;h3&gt; Theory: As log-linear model&lt;/h3&gt;
&lt;p&gt;The theory here will be a bit more convoluted, and I mainly write it up so that you can get the &lt;em&gt;feeling&lt;/em&gt; that it really is just a log-linear &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova2&quot;&gt;two-way ANOVA model&lt;/a&gt;. Let’s get started…&lt;/p&gt;
&lt;p&gt;For a two-way contingency table, the model of the count variable &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; is a modeled using the marginal proportions of a contingency table. Why this makes sense, is too involved to go into here, but &lt;a href=&quot;https://www.uni-tuebingen.de/fileadmin/Uni_Tuebingen/SFB/SFB_833/A_Bereich/A1/Christoph_Scheepers_-_Statistikworkshop.pdf&quot;&gt;see the relevant slides by Christoph Scheepers here&lt;/a&gt; for an excellent exposition. The model is composed of a lot of counts and the regression coefficients &lt;span class=&quot;math inline&quot;&gt;\(A_i\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(B_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y_i = N \cdot x_i(A_i/N) \cdot z_j(B_j/N) \cdot x_{ij}/((A_i x_i)/(B_j z_j)/N)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What a mess!!! Here, &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; is the row index, &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt; is the column index, &lt;span class=&quot;math inline&quot;&gt;\(x_{something}\)&lt;/span&gt; is the sum of that row and/or column, &lt;span class=&quot;math inline&quot;&gt;\(N = sum(y)\)&lt;/span&gt;. Remember that &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; is a count variable, so &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; is just the total count.&lt;/p&gt;
&lt;p&gt;We can simplify the notation by defining the &lt;em&gt;proportions&lt;/em&gt;: &lt;span class=&quot;math inline&quot;&gt;\(\alpha_i = x_i(A_i/N)\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\beta_i = x_j(B_i/N)\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(\alpha_i\beta_j = x_{ij}/(A_i x_i)/(B_j z_j)/N\)&lt;/span&gt;. Let’s write the model again:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(y_i = N \cdot \alpha_i \cdot \beta_j \cdot \alpha_i\beta_j\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Ah, much prettier. However, there is still lot’s of multiplication which makes it hard to get an intuition about how the actual numbers interact. We can make it much more intelligible when we remember that &lt;span class=&quot;math inline&quot;&gt;\(log(A \cdot B) = log(A) + log(B)\)&lt;/span&gt;. Doing logarithms on both sides, we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(log(y_i) = log(N) + log(\alpha_i) + log(\beta_j) + log(\alpha_i\beta_j)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Snuggly! Now we can get a better grasp on how the regression coefficients (which are proportions) independently contribute to &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt;. This is why logarithms are so nice for proportions. Note that this is just &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova2&quot;&gt;the two-way ANOVA model&lt;/a&gt; with some logarithms added, so we are back to our good old linear models - only the interpretation of the regression coefficients have changed! And we cannot use &lt;code&gt;lm&lt;/code&gt; anymore in R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;example-data-2&quot; class=&quot;section level3&quot; readability=&quot;36&quot;&gt;
&lt;h3&gt; Example data&lt;/h3&gt;
&lt;p&gt;Here we need some long data and we need it in table format for &lt;code&gt;chisq.test&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Contingency data in long format for linear model
D = data.frame(mood = c('happy', 'happy', 'meh', 'meh', 'sad', 'sad'),
               sex = c('male', 'female', 'male', 'female', 'male', 'female'),
               Freq = c(100, 70, 30, 32, 110, 120))

# ... and as table for chisq.test
D_table = D %&amp;gt;% 
  spread(key=mood, value=Freq) %&amp;gt;%  # Mood to columns
  select(-sex) %&amp;gt;%  # Remove sex column
  as.matrix()

# Dummy coding of D for linear model (skipping mood==&quot;sad&quot; and gender==&quot;female&quot;)
# We could also use model.matrix(D$Freq~D$mood*D$sex)
D$mood_happy = ifelse(D$mood == 'happy', 1, 0)
D$mood_meh = ifelse(D$mood == 'meh', 1, 0)
D$sex_male = ifelse(D$sex == 'male', 1, 0)&lt;/code&gt;
&lt;/pre&gt;

&lt;/div&gt;
&lt;div id=&quot;r-code-chi-square-test&quot; class=&quot;section level3&quot; readability=&quot;35.341081267218&quot;&gt;
&lt;h3&gt; R code: Chi-square test&lt;/h3&gt;
&lt;p&gt;Now let’s show the equivalence between a chi-square model and a log-linear model. This is very similar to our &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova2&quot;&gt;two-way ANOVA&lt;/a&gt; above:&lt;/p&gt;
&lt;pre class=&quot;r&quot;&gt;
&lt;code&gt;# Built-in chi-square. It requires matrix format.
a = chisq.test(D_table)

# Using glm to do a log-linear model, we get identical results when testing the interaction term:
full = glm(Freq ~ 1 + mood_happy + mood_meh + sex_male + mood_happy*sex_male + mood_meh*sex_male, data=D, family=poisson())
null = glm(Freq ~ 1 + mood_happy + mood_meh + sex_male, data=D, family=poisson())
b = anova(null, full, test='Rao')  # Could also use test='LRT' or test='Chisq'

# Note: let glm do the dummy coding for you
full = glm(Freq ~ mood * sex, family=poisson(), data=D)
c = anova(full, test='Rao')

# Note: even simpler syntax using MASS:loglm (&quot;log-linear model&quot;)
d = MASS::loglm(Freq ~ mood + sex, D)&lt;/code&gt;
&lt;/pre&gt;

&lt;div class=&quot;fold o&quot; readability=&quot;16&quot;&gt;
&lt;pre&gt;
&lt;code&gt;## 
##  Pearson's Chi-squared test
## 
## data:  D_table
## X-squared = 5.0999, df = 2, p-value = 0.07809
## 
## Analysis of Deviance Table
## 
## Model 1: Freq ~ 1 + mood_happy + mood_meh + sex_male
## Model 2: Freq ~ 1 + mood_happy + mood_meh + sex_male + mood_happy * sex_male + 
##     mood_meh * sex_male
##   Resid. Df Resid. Dev Df Deviance    Rao Pr(&amp;gt;Chi)  
## 1         2     5.1199                              
## 2         0     0.0000  2   5.1199 5.0999  0.07809 .
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## Analysis of Deviance Table
## 
## Model: poisson, link: log
## 
## Response: Freq
## 
## Terms added sequentially (first to last)
## 
## 
##          Df Deviance Resid. Df Resid. Dev    Rao Pr(&amp;gt;Chi)    
## NULL                         5    111.130                    
## mood      2  105.308         3      5.821 94.132  &amp;lt; 2e-16 ***
## sex       1    0.701         2      5.120  0.701  0.40235    
## mood:sex  2    5.120         0      0.000  5.100  0.07809 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## Call:
## MASS::loglm(formula = Freq ~ mood + sex, data = D)
## 
## Statistics:
##                       X^2 df   P(&amp;gt; X^2)
## Likelihood Ratio 5.119915  2 0.07730804
## Pearson          5.099859  2 0.07808717
## 
## Call:
## glm(formula = Freq ~ mood * sex, family = poisson(), data = D)
## 
## Deviance Residuals: 
## [1]  0  0  0  0  0  0
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)       4.2485     0.1195  35.545  &amp;lt; 2e-16 ***
## moodmeh          -0.7828     0.2134  -3.668 0.000244 ***
## moodsad           0.5390     0.1504   3.584 0.000339 ***
## sexmale           0.3567     0.1558   2.289 0.022094 *  
## moodmeh:sexmale  -0.4212     0.2981  -1.413 0.157670    
## moodsad:sexmale  -0.4437     0.2042  -2.172 0.029819 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 1.1113e+02  on 5  degrees of freedom
## Residual deviance: 3.9968e-15  on 0  degrees of freedom
## AIC: 48.254
## 
## Number of Fisher Scoring iterations: 3&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you unfold the raw R output, I’ve included &lt;code&gt;summary(full)&lt;/code&gt; so that you can see the raw regression coefficients. Being a log-linear model, these are the &lt;em&gt;percentage increase&lt;/em&gt;in &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; over and above the intercept if that category obtains.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;links&quot; class=&quot;section level1&quot; readability=&quot;7.2&quot;&gt;

&lt;p&gt;Here are links to other sources who have exposed bits and pieces of this puzzle, including many further equivalences not covered here:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;course&quot; class=&quot;section level1&quot; readability=&quot;24.169701520556&quot;&gt;

&lt;p&gt;Most advanced stats books (and some intro-books) take the “everything is GLMM” approach as well. However, the “linear model” part often stay at the conceptual level. I wanted to make linear models the &lt;em&gt;tool&lt;/em&gt; in a really concise way. Luckily, more beginnier-friendly materials have emerged lately:&lt;/p&gt;
&lt;p&gt;Here are my own thoughts on what I’d do. I’ve done parts of this with great success already, but not the whole lot since I’m not assigned to do a full course yet.&lt;/p&gt;
&lt;p&gt;I would spend 50% of the time on linear modeling of data (bullet 1 below) since this contains 70% of what students need to know. The rest of the course is just fleshing out what happens if you have one group, two groups, etc.&lt;/p&gt;
&lt;p&gt;Note that whereas the understanding of sampling and hypothesis testing is usually the first focus of mainstream stats courses, it is saved for later here to make way for modeling.&lt;/p&gt;
&lt;ol readability=&quot;6.9374086702387&quot;&gt;&lt;li readability=&quot;-0.92611251049538&quot;&gt;
&lt;p&gt;&lt;strong&gt;Fundamentals of regression:&lt;/strong&gt;&lt;/p&gt;
&lt;ol readability=&quot;11.092003439381&quot;&gt;&lt;li readability=&quot;8.2130044843049&quot;&gt;
&lt;p&gt;Recall from high-school: &lt;span class=&quot;math inline&quot;&gt;\(y = a \cdot x + b\)&lt;/span&gt;, and getting a really good intuition about slopes and intercepts. Understanding that this can be written using all variable names, e.g., &lt;code&gt;money = profit * time + starting_money&lt;/code&gt; or &lt;span class=&quot;math inline&quot;&gt;\(y = \beta_1x + \beta_2*1\)&lt;/span&gt; or, suppressing the coefficients, as &lt;code&gt;y ~ x + 1&lt;/code&gt;. If the audience is receptive, convey the idea of these models &lt;a href=&quot;https://magesblog.com/post/modelling-change&quot;&gt;as a solution to differential equations&lt;/a&gt;, specifying how &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; &lt;em&gt;changes&lt;/em&gt; with &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Extend to a few multiple regression as models. Make sure to include plenty of real-life examples and exercises at this point to make all of this really intuitive. Marvel at how briefly these models allow us to represent large datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Introduce the idea of rank-transforming non-metric data and try it out.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Teach the three assumptions: independence of data points, normality of residuals, and homoscedasticity.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Confidence/credible intervals on the parameters. Stress that the Maximum-Likelihood estimate is extremely unlikely, so intervals are more important.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Briefly introduce &lt;span class=&quot;math inline&quot;&gt;\(R^2\)&lt;/span&gt; for the simple regression models above. Mention in passing that this is called &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#correlation&quot;&gt;the Pearson and Spearman correlation coefficients&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li readability=&quot;0.88608776844071&quot;&gt;
&lt;p&gt;&lt;strong&gt;Special case #1: One or two means (t-tests, Wilcoxon, Mann-Whitney):&lt;/strong&gt;&lt;/p&gt;
&lt;ol readability=&quot;7.0259481037924&quot;&gt;&lt;li readability=&quot;3.3220338983051&quot;&gt;
&lt;p&gt;*One mean:** When there is only one x-value, the regression model simplifies to &lt;span class=&quot;math inline&quot;&gt;\(y = b\)&lt;/span&gt;. If &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; is non-metric, you can rank-transform it. Apply the assumptions (homoscedasticity doesn’t apply since there is only one &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;). Mention in passing that these intercept-only models are called &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t1&quot;&gt;one-sample t-test and Wilcoxon Signed Rank test respectively&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5.7556561085973&quot;&gt;
&lt;p&gt;&lt;strong&gt;Two means:&lt;/strong&gt; If we put two variables 1 apart on the x-axis, the difference between the means is the slope. Great! It is accessible to our swizz army knife called linear modeling. Apply the assumption checks to see that homoscedasticity reduces to equal variance between groups. This is called an &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#t2&quot;&gt;independent t-test&lt;/a&gt;. Do a few worked examples and exercises, maybe adding Welch’s test, and do the rank-transformed version, called Mann-Whitney U.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2.3529411764706&quot;&gt;
&lt;p&gt;&lt;em&gt;Paired samples:&lt;/em&gt; Violates the independence assumption. After computing pairwise differences, this is equivalent to 2.1 (one intercept), though it is called the &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#tpair&quot;&gt;paired t-test and Wilcoxon’s matched pairs&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li readability=&quot;-0.89839572192513&quot;&gt;
&lt;p&gt;&lt;strong&gt;Special case #2: Three or more means (ANOVAs)&lt;/strong&gt;&lt;/p&gt;
&lt;ol readability=&quot;0.88414634146341&quot;&gt;&lt;li readability=&quot;0.95219123505976&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#dummy&quot;&gt;Dummy coding&lt;/a&gt; of categories:&lt;/em&gt; How one regression coefficient for each level of a factor models an intercept for each level when multiplied by a binary indicator. This is just extending what we did in 2.1. to make this data accessible to linear modeling.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.64864864864865&quot;&gt;
&lt;p&gt;&lt;em&gt;Means of one variable:&lt;/em&gt; &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova1&quot;&gt;One-way ANOVA&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.65789473684211&quot;&gt;
&lt;p&gt;&lt;em&gt;Means of two variables:&lt;/em&gt; &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#anova2&quot;&gt;Two-way ANOVA&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li readability=&quot;-0.90977443609023&quot;&gt;
&lt;p&gt;&lt;strong&gt;Special case #3: Three or more proportions (Chi-Square)&lt;/strong&gt;&lt;/p&gt;
&lt;ol readability=&quot;3.1551724137931&quot;&gt;&lt;li readability=&quot;5.6842105263158&quot;&gt;
&lt;p&gt;&lt;em&gt;Logarithmic transformation:&lt;/em&gt; Making multiplicative models linear using logarithms, thus modeling proportions. See &lt;a href=&quot;https://www.uni-tuebingen.de/fileadmin/Uni_Tuebingen/SFB/SFB_833/A_Bereich/A1/Christoph_Scheepers_-_Statistikworkshop.pdf&quot;&gt;this excellent introduction&lt;/a&gt; to the equivalence of log-linear models and Chi-Square tests as models of proportions. Also needs to introduce (log-)odds ratios. When the multiplicative model is made summative using logarithms, we just add the dummy-coding trick from 3.1, and see that the models are identical to the ANOVA models in 3.2 and 3.3, only the interpretation of the coefficients have changed.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.66666666666667&quot;&gt;
&lt;p&gt;&lt;em&gt;Proportions of one variable:&lt;/em&gt; &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#goodness&quot;&gt;Goodness of fit&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.63265306122449&quot;&gt;
&lt;p&gt;&lt;em&gt;Proportions of two variables:&lt;/em&gt; &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#contingency&quot;&gt;Contingency tables&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;strong&gt;Hypothesis testing:&lt;/strong&gt; Hypothesis testing is the act of choosing between a full model and one where a parameter is set to zero (effectively excluded from the model) instead of being estimated. For example, when set one of the two means in the t-test to be zero, we study how well the remaining mean explains all the data from both groups. If it does a good job, we prefer this model over the two-mean model because it is simpler. So hypothesis testing is just comparing linear models to make more qualitative statements than the truly quantitative statements which were covered in bullets 1-4 above. Therefore, hypothesis testing is less interesting and is mostly covered as an introduction to the general literature. Mention P-values (and misconceptions about them), Bayes Factors, and cross-validation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
&lt;div id=&quot;limitations&quot; class=&quot;section level1&quot; readability=&quot;6.6152671755725&quot;&gt;

&lt;p&gt;I have made a few simplifications for clarity:&lt;/p&gt;
&lt;ol readability=&quot;10.36690647482&quot;&gt;&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;I have not covered assumptions in the examples. This will be another post! But all assumptions of all tests come down to the usual three: a) independence of data points, b) normally distributed residuals, and c) homoscedasticity.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;I assume that all null hypotheses are the absence of an effect, but everything works the same for non-zero null hypotheses.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5.8318965517241&quot;&gt;
&lt;p&gt;I have not discussed inference. I am only including p-values in the comparisons as a crude way to show the equivalences between the underlying models since people care about p-values. Parameter estimates will show the same equivalence. How to do &lt;em&gt;inference&lt;/em&gt; is another matter. Personally, I’m a Bayesian, but going Bayesian here would render it less accessible to the wider audience. Also, doing &lt;a href=&quot;https://en.wikipedia.org/wiki/Robust_statistics&quot;&gt;robust models&lt;/a&gt; would be preferable, but fail to show the equivalence.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5.1805555555556&quot;&gt;
&lt;p&gt;Several named tests are still missing from the list and may be added at a later time. This includes the Sign test (require large N to be reasonably approximated by a linear model), Friedman as RM-ANOVA on &lt;code&gt;rank(y)&lt;/code&gt;, McNemar, and Binomial/Multinomial. See stuff on these in &lt;a href=&quot;https://lindeloev.github.io/tests-as-linear/#links&quot;&gt;the section on links to further equivalences&lt;/a&gt;. If you think that they should be included here, feel free to submit “solutions” to &lt;a href=&quot;https://github.com/lindeloev/tests-as-linear/&quot;&gt;the github repo&lt;/a&gt; of this doc!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
</description>
<pubDate>Thu, 28 Mar 2019 12:18:33 +0000</pubDate>
<dc:creator>homarp</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://lindeloev.github.io/tests-as-linear/</dc:identifier>
</item>
<item>
<title>BBC podcasts on third-party apps</title>
<link>http://www.bbc.co.uk/blogs/aboutthebbc/entries/d68712d7-bd24-440f-94a0-1c6a4cdee71a</link>
<guid isPermaLink="true" >http://www.bbc.co.uk/blogs/aboutthebbc/entries/d68712d7-bd24-440f-94a0-1c6a4cdee71a</guid>
<description>&lt;p&gt;&lt;strong&gt;If you’re like me, you might be feeling spoilt for choice at the moment with fantastic range of new and established podcasts out there to listen to.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We’ve just announced a &lt;a href=&quot;https://www.bbc.co.uk/programmes/p06kyljg&quot;&gt;second run of That Peter Crouch podcast&lt;/a&gt; - with a lots more coming soon.&lt;/p&gt;
&lt;p&gt;You might have also seen that our podcasts are no longer available on certain Google products - including the Google Podcast app and Google assistant. I want to explain a little bit about why that has happened.&lt;/p&gt;
&lt;p&gt;Last year, Google launched its own podcast app for Android users - they’ve also said they will launch a browser version for computers soon. Google has since begun to direct people who search for a BBC podcast into its own podcast service, rather than BBC Sounds or other third party services, which reduces people’s choice - an approach that the BBC is not comfortable with and has consistently expressed strong concerns about. We asked them to exclude the BBC from this specific feature but they have refused.&lt;/p&gt;
&lt;p&gt;As a public service, we want our content and services to be available to as many people as possible and we make these available for free on a range of third-party apps. But as the BBC, funded by the licence fee payers in the UK, we have to ensure it is done in a way that is good for all audiences, according to our &lt;a href=&quot;http://downloads.bbc.co.uk/aboutthebbc/insidethebbc/howwework/policiesandguidelines/pdf/bbc_distribution_policy.pdf&quot;&gt;Distribution Policy&lt;/a&gt; - which has been agreed with Ofcom.&lt;/p&gt;
&lt;p&gt;What we think is important is pretty simple. We want people to have easy access to the wide range of BBC programmes, not just a select few, and be able to discover and listen to new ones really easily.&lt;/p&gt;
&lt;p&gt;In the UK we have a creative and flourishing radio and podcast industry and the BBC plays a significant part in this, which we’re really proud of. So we want to make sure podcasts made in, and championing the UK, are prominent on global platforms. We also want to make our programmes and services as good as they can possibly be - this means us getting hold of meaningful audience data. This helps us do a number of things; make more types of programmes we know people like, make our services even more personalised and relevant to people using them, and equally importantly, identify gaps in our commissioning to ensure we’re making something for all audiences.&lt;/p&gt;
&lt;p&gt;Unfortunately, given the way the Google podcast service operates, we can’t do any of the above.&lt;/p&gt;
&lt;p&gt;We don’t like removing our content from services and certainly don’t do it lightly - but unfortunately until Google changes the way they look at this, for the good of listeners, our podcasts will not be available on some of their services. We are in discussions with Google to try and resolve the situation and will continue to work with them to try and come to a solution that’s in the best interests of all listeners.&lt;/p&gt;</description>
<pubDate>Thu, 28 Mar 2019 11:12:24 +0000</pubDate>
<dc:creator>open-source-ux</dc:creator>
<og:type>article</og:type>
<og:title>BBC podcasts on third-party apps</og:title>
<og:url>http://www.bbc.co.uk/blogs/aboutthebbc/entries/d68712d7-bd24-440f-94a0-1c6a4cdee71a</og:url>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.bbc.co.uk/blogs/aboutthebbc/entries/d68712d7-bd24-440f-94a0-1c6a4cdee71a</dc:identifier>
</item>
<item>
<title>Software Won’t Fix Boeing’s ‘Faulty’ Airframe</title>
<link>https://www.eetimes.com/document.asp?doc_id=1334482</link>
<guid isPermaLink="true" >https://www.eetimes.com/document.asp?doc_id=1334482</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.eetimes.com/document.asp?doc_id=1334482&quot;&gt;https://www.eetimes.com/document.asp?doc_id=1334482&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19509618&quot;&gt;https://news.ycombinator.com/item?id=19509618&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 386&lt;/p&gt;
&lt;p&gt;# Comments: 386&lt;/p&gt;
</description>
<pubDate>Thu, 28 Mar 2019 05:52:50 +0000</pubDate>
<dc:creator>farseer</dc:creator>
<dc:identifier>https://www.eetimes.com/document.asp?doc_id=1334482</dc:identifier>
</item>
<item>
<title>YC Interviews in India</title>
<link>https://blog.ycombinator.com/yc-interviews-in-india/</link>
<guid isPermaLink="true" >https://blog.ycombinator.com/yc-interviews-in-india/</guid>
<description>&lt;p&gt;We’re excited to announce that we’ll be holding interviews for the Summer 2019 batch in Bangalore for founders based in India. This will take place from May 7 to 9.&lt;/p&gt;
&lt;p&gt;We’ve received an increasing number of applications from India and this will allow us to interview more Indian founders. So far, we’ve funded over 40 companies from India, including ClearTax, Meesho and Razorpay. And we hope more of you will join the YC network soon!&lt;/p&gt;
&lt;p&gt;These will be just like regular YC interviews in every way. There is no difference to the application, what we look for in an application, and our criteria for acceptance. If accepted, you’ll join the YC Summer 2019 batch immediately. The in-person program starts in June in Mountain View, California, but you’ll get funding and have access to YC partners and resources right away.&lt;/p&gt;
&lt;p&gt;The deadline to apply to meet us in Bangalore is Friday, April 12th at 8pm IST. We’ll let you know if you’re invited to interview by April 16th.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://apply.ycombinator.com/&quot;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We look forward to meeting you soon! If you have any questions, send us an email at &lt;a href=&quot;mailto:apply@ycombinator.com&quot;&gt;apply@ycombinator.com&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;h2&gt;FAQ&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Q: Can I apply to meet you in Bangalore even if I don’t live there?&lt;/strong&gt;&lt;br/&gt;A. Yes. If you live in India, we highly encourage you to apply. We’ll reimburse certain travel expenses to and from Bangalore.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: What does it mean if I don’t hear from you by April 16th?&lt;/strong&gt;&lt;br/&gt;A: If you don’t hear from us, it’s because your application is still under review. We’ll consider it for interviews in Mountain View, California.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Do I have to re-apply for the Banglore interviews if I submitted an application before you announced them?&lt;/strong&gt;&lt;br/&gt;A: No. We’ll consider any application submitted by April 12th for the Bangalore interviews.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: How do I let you know that my application is for Bangalore interviews?&lt;/strong&gt;&lt;br/&gt;A: No need to indicate. Just apply and we’ll let you know. You don’t need to live in Bangalore.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q. Are travel expenses covered for the interviews?&lt;/strong&gt;&lt;br/&gt;A. Yes, travel expenses to and from Bangalore will be reimbursed if you live in India. We’ll help you cover the cost of economy class flights, intercity ground transportation, and one night of accommodation. You will need to submit your &lt;a href=&quot;https://ssapp.wufoo.com/forms/k1gs4i5q0ph3w6i/&quot;&gt;travel expense reimbursement&lt;/a&gt; claim after the interview. All travel related questions should be directed to &lt;a href=&quot;mailto:travel@ycombinator.com&quot;&gt;travel@ycombinator.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q. Can I apply after April 12th?&lt;/strong&gt;&lt;br/&gt;A. Yes. If you do, we may not be able to meet you in Bangalore. We’ll consider it for interviews in Mountain View.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Do I need to be incorporated as a US Delaware-C corp to apply?&lt;/strong&gt;&lt;br/&gt;No. We’ll interview any type of company even if you haven’t incorporated yet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: Are you considering conducting interviews in cities besides Bangalore?&lt;/strong&gt;&lt;br/&gt;A: Yes. You can check our blog for updates: &lt;a href=&quot;https://blog.ycombinator.com&quot;&gt;https://blog.ycombinator.com&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 28 Mar 2019 04:33:53 +0000</pubDate>
<dc:creator>katm</dc:creator>
<og:title>YC Interviews in India</og:title>
<og:url>https://blog.ycombinator.com/yc-interviews-in-india/</og:url>
<og:type>article</og:type>
<og:description>We’re excited to announce that we’ll be holding interviews for the Summer 2019 batch in Bangalore for founders based in India. This will take place from May 7 to 9.</og:description>
<og:image>https://blog.ycombinator.com/wp-content/uploads/2019/03/YC-Interviews-in-India.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.ycombinator.com/yc-interviews-in-india/</dc:identifier>
</item>
<item>
<title>Mistakes, we’ve drawn a few</title>
<link>https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368</link>
<guid isPermaLink="true" >https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368</guid>
<description>&lt;h3 name=&quot;62b0&quot; id=&quot;62b0&quot; class=&quot;graf graf--h3 graf--leading&quot;&gt;Misleading charts&lt;/h3&gt;
&lt;p name=&quot;24da&quot; id=&quot;24da&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Let’s start with the worst of crimes in data visualisation: presenting data in a misleading way. We &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;never&lt;/em&gt;&lt;/strong&gt; do this on purpose! But it does happen every now and then. Let’s look at the three examples from our archive.&lt;/p&gt;
&lt;blockquote name=&quot;8185&quot; id=&quot;8185&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;Mistake: Truncating the scale&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9QE_yL3boSLqopJkSBfL5A.png&quot; data-width=&quot;1280&quot; data-height=&quot;569&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*9QE_yL3boSLqopJkSBfL5A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*9QE_yL3boSLqopJkSBfL5A.png&quot;/&gt;&lt;/div&gt;
A bit left-field
&lt;p name=&quot;f617&quot; id=&quot;f617&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;&lt;a href=&quot;https://www.economist.com/britain/2016/08/13/the-metamorphosis&quot; data-href=&quot;https://www.economist.com/britain/2016/08/13/the-metamorphosis&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;This chart&lt;/a&gt; shows the average number of Facebook likes on posts by pages of the political left. The point of this chart was to show the disparity between Mr Corbyn’s posts and others.&lt;/p&gt;
&lt;p name=&quot;9d56&quot; id=&quot;9d56&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The original chart not only downplays the number of Mr Corbyn’s likes but also exaggerates those on other posts. In the redesigned version, we show Mr Corbyn’s bar in its entirety. All other bars remain visible. &lt;em class=&quot;markup--em markup--p-em&quot;&gt;(Avid followers of this blog will have seen&lt;/em&gt; &lt;a href=&quot;https://medium.economist.com/the-challenges-of-charting-regional-inequality-a9376718348&quot; data-href=&quot;https://medium.economist.com/the-challenges-of-charting-regional-inequality-a9376718348&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;another example&lt;/em&gt;&lt;/a&gt; &lt;em class=&quot;markup--em markup--p-em&quot;&gt;of this bad practice.)&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;d886&quot; id=&quot;d886&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Another odd thing is the choice of colour. In an attempt to emulate Labour’s colour scheme, we used three shades of orange/red to distinguish between Jeremy Corbyn, other MPs and parties/groups. We don’t explain this. While the logic behind the colours might be obvious to a lot of readers, it perhaps makes little sense for those less familiar with British politics.&lt;/p&gt;
&lt;p name=&quot;a1cc&quot; id=&quot;a1cc&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;a href=&quot;http://infographics.economist.com/databank/Economist_corbyn.csv&quot; data-href=&quot;http://infographics.economist.com/databank/Economist_corbyn.csv&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Download chart data&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote name=&quot;d89a&quot; id=&quot;d89a&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;Mistake: Forcing a relationship by cherry-picking scales&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*H21mduPmvzot3oaMThNfFQ.png&quot; data-width=&quot;1280&quot; data-height=&quot;670&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*H21mduPmvzot3oaMThNfFQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*H21mduPmvzot3oaMThNfFQ.png&quot;/&gt;&lt;/div&gt;
&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;A rare perfect correlation? Not really.&lt;/em&gt;
&lt;p name=&quot;af17&quot; id=&quot;af17&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The chart above accompanied a &lt;a href=&quot;https://www.economist.com/britain/2016/08/13/subwoofers&quot; data-href=&quot;https://www.economist.com/britain/2016/08/13/subwoofers&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;story on the decline of dog weights&lt;/a&gt;. On first glance, it appears that the weight and neck sizes of dogs are perfectly correlated. But is this true? Only to some extent.&lt;/p&gt;
&lt;p name=&quot;10c7&quot; id=&quot;10c7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In the original chart, both scales decrease by three units (from 21 to 18 on the left; from 45 to 42 on the right). In percentage terms, the left scale decreases by 14% while the right goes down by 7%. In the redesigned chart, I retained the double scale but adjusted their ranges to reflect a comparable proportional change.&lt;/p&gt;
&lt;p name=&quot;9e80&quot; id=&quot;9e80&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Considering the jolly topic of this chart, this mistake may seem relatively minor. The message of the chart, after all, is the same in both versions. But the takeaway is important: if two series follow each other too closely, it is probably a good idea to have a closer look at the scales.&lt;/p&gt;
&lt;p name=&quot;7413&quot; id=&quot;7413&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;a href=&quot;http://infographics.economist.com/databank/Economist_dogs.csv&quot; data-href=&quot;http://infographics.economist.com/databank/Economist_dogs.csv&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Download chart data&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;blockquote name=&quot;182b&quot; id=&quot;182b&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;Mistake: Choosing the wrong visualisation method&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9GzHVtm4y_LeVmFCjqV3Ww.png&quot; data-width=&quot;1280&quot; data-height=&quot;670&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*9GzHVtm4y_LeVmFCjqV3Ww.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*9GzHVtm4y_LeVmFCjqV3Ww.png&quot;/&gt;&lt;/div&gt;
&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;Views on Brexit almost as erratic as its negotiations&lt;/em&gt;
&lt;p name=&quot;1a11&quot; id=&quot;1a11&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We published this polling chart in Espresso, our daily news app. It shows attitudes to the outcome of the EU referendum, plotted as a line chart. Looking at the data, it appears as if respondents had a rather erratic view of the referendum result — increasing and decreasing by a couple of percentage points from one week to the next.&lt;/p&gt;
&lt;p name=&quot;a5a5&quot; id=&quot;a5a5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Instead of plotting the individual polls with a smoothed curve to show the trend, we connected the actual values of each individual poll. This happened, primarily, because our in-house charting tool does not plot smoothed lines. Until fairly recently, we were less comfortable with statistical software (like R) that allows more sophisticated visualisations. Today, all of us are able to plot a polling chart like the redesigned one above.&lt;/p&gt;
&lt;p name=&quot;de86&quot; id=&quot;de86&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Another thing to note in this chart is the way in which the scale was broken. The original chart spreads the data wider than it should. In the redesigned version, I have left some more space between the start of the scale and the smallest data point. &lt;a href=&quot;https://www.chezvoila.com/blog/yaxis&quot; data-href=&quot;https://www.chezvoila.com/blog/yaxis&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Francis Gagnon&lt;/a&gt; has put together a nice formula for this: aim for leaving at least 33% of the plot area free under a line chart that doesn’t start at zero.&lt;/p&gt;
&lt;p name=&quot;fe46&quot; id=&quot;fe46&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;&lt;a href=&quot;http://infographics.economist.com/databank/Economist_brexit.csv&quot; data-href=&quot;http://infographics.economist.com/databank/Economist_brexit.csv&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Download chart data&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 28 Mar 2019 02:48:30 +0000</pubDate>
<dc:creator>tysone</dc:creator>
<og:title>Mistakes, we’ve drawn a few</og:title>
<og:url>https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*GB8vGeGzMeueEbkpGTTZVQ.png</og:image>
<og:description>Learning from our errors in data visualisation</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.economist.com/mistakes-weve-drawn-a-few-8cdd8a42d368?gi=995f537e2c8</dc:identifier>
</item>
</channel>
</rss>