<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Why we should care about the Nate Silver vs. Nassim Taleb Twitter war</title>
<link>https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc</link>
<guid isPermaLink="true" >https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc</guid>
<description>&lt;h2 name=&quot;aece&quot; id=&quot;aece&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;How can two data experts disagree so much?&lt;/h2&gt;
&lt;div class=&quot;uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup&quot;&gt;
&lt;div class=&quot;u-flex0&quot;&gt;&lt;a class=&quot;link u-baseColor--link avatar&quot; href=&quot;https://towardsdatascience.com/@isaacfaber?source=post_header_lockup&quot; data-action=&quot;show-user-card&quot; data-action-source=&quot;post_header_lockup&quot; data-action-value=&quot;81d6b9acba62&quot; data-action-type=&quot;hover&quot; data-user-id=&quot;81d6b9acba62&quot; data-collection-slug=&quot;towards-data-science&quot; dir=&quot;auto&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/fit/c/100/100/1*oQEv0ES7AUPNp-zPsIOKkw.jpeg&quot; class=&quot;avatar-image u-size50x50&quot; alt=&quot;Go to the profile of Isaac Faber&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;p name=&quot;1bb2&quot; id=&quot;1bb2&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;An obscure controversy has reared its ugly head again this past month. Two icons of the quantitative analysis community have locked horns on the greatest of public stages, Twitter. You may be forgiven for not following the controversy: I’ll do a quick review for the uninitiated. &lt;a href=&quot;https://community.platform.matrixds.com/community/project/5c09a98a5fd07d6fa67f645d/files&quot; data-href=&quot;https://community.platform.matrixds.com/community/project/5c09a98a5fd07d6fa67f645d/files&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;All code and data used to create this article can be forklifted from this MatrixDS project.&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;0fb3&quot; id=&quot;0fb3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Nate Silver is the co-founder of &lt;a href=&quot;https://fivethirtyeight.com/&quot; data-href=&quot;https://fivethirtyeight.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;FiveThirtyEight&lt;/a&gt;. A massively popular data focused blog that gained fame for its accuracy predicting the outcomes for the U.S. elections in 2008. Silver generates predictions using a clever poll aggregating technique which accounts for biases, such as pollsters who only call people with landlines.&lt;/p&gt;
&lt;p name=&quot;f27e&quot; id=&quot;f27e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A trained statistician, via economics, he directed his passion for baseball (sabermetrics) and poker analytics into the arena of politics. In fact, the name FiveThirtyEight is a nod to the number of U.S. electoral votes (538 of them). However, the blog also covers other interest areas like sports. Nate sold his blog to ESPN and took the job of Editor in Chief. They (ESPN) used it as a platform to feed their audience with forecasts of sporting events, FiveThirtyEight has since moved to ABC. A routine visit to their website is greeted with a mix of political and sports articles with detailed predictions and data visualizations.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9FfCONibVqj8F1XIR-aDkw.jpeg&quot; data-width=&quot;512&quot; data-height=&quot;341&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*9FfCONibVqj8F1XIR-aDkw.jpeg&quot;/&gt;&lt;/div&gt;
&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Nate_Silver_in_Conversation_with_NY1%27s_Pat_Kiernan.jpg&quot; data-href=&quot;https://commons.wikimedia.org/wiki/File:Nate_Silver_in_Conversation_with_NY1%27s_Pat_Kiernan.jpg&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Silver (left) in conversation with NY1’s Pat Kiernan&lt;/a&gt;
&lt;p name=&quot;7e2b&quot; id=&quot;7e2b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Nate’s forecasting prowess has become accepted as canon in the popular media. He is a routine guest on many nationally televised shows to discuss his predictions during every national election cycle. So it came as quite a shock when Nassim Taleb, a best-selling author, and quantitative risk expert, publicly announced that FiveThirtyEight does not know how to forecast elections properly!&lt;/p&gt;
&lt;p name=&quot;72f7&quot; id=&quot;72f7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For his part, Taleb has become extremely successful due to his shrewd understanding of probability in the real world. His books are both philosophical and technical, with a focus on uncertainty and risk. Specifically, he believes that the vast majority of quantitative models used in practice do not sufficiently account for real-world risk. Instead, they give the illusion of short-term value (like being accurate in some well-understood situations) but expose the unknowing users to enormous systemic risk when they experience situations the models are not designed to understand.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*oliaBOQkldY-2V4l1KYMcg.jpeg&quot; data-width=&quot;256&quot; data-height=&quot;250&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*oliaBOQkldY-2V4l1KYMcg.jpeg&quot;/&gt;&lt;/div&gt;
Nassim Taleb
&lt;p name=&quot;31d4&quot; id=&quot;31d4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Taleb gained fame, in part, because he puts his philosophy into action by exposing his wealth. &lt;a href=&quot;http://faculty.sites.uci.edu/pjorion/files/2018/03/NYorker2002-blowingup.pdf&quot; data-href=&quot;http://faculty.sites.uci.edu/pjorion/files/2018/03/NYorker2002-blowingup.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Malcolm Gladwell wrote an article in the New Yorker on how Taleb turned his philosophy on risk into an incredibly successful investment strategy.&lt;/a&gt; He has since gained significant wealth during unforeseen market events such as the Russian debt default, 9/11, and the financial crisis of 2008. Taleb now spends much of his time writing and deadlifting (I’m jealous of this bit). He is not shy about telling someone publicly that he disagrees with them: One of those people is Nate Silver.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9KxNUeOd9CZF1xiVKBei3Q.png&quot; data-width=&quot;1252&quot; data-height=&quot;1606&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*9KxNUeOd9CZF1xiVKBei3Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*9KxNUeOd9CZF1xiVKBei3Q.png&quot;/&gt;&lt;/div&gt;
Taleb’s Tweets directed at Silver November 2018
&lt;p name=&quot;238a&quot; id=&quot;238a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;However, Silver isn’t taking the insults lying down!&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*_g_Z_-yZr1Evt27FuWPeHg.png&quot; data-width=&quot;1204&quot; data-height=&quot;1060&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*_g_Z_-yZr1Evt27FuWPeHg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*_g_Z_-yZr1Evt27FuWPeHg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;c85a&quot; id=&quot;c85a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Silver and Taleb, with three million and 300k followers respectively, create an enormous buzz with these exchanges (starting back in 2016). However, a quick read through the comment threads and you will realize that few people understand the arguments. Even Silver himself seems taken off guard by Taleb’s attack.&lt;/p&gt;
&lt;p name=&quot;35b1&quot; id=&quot;35b1&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I think, however, this is a great opportunity for a data science professional (or aspiring professional) to dig deeper into what is being said. There are implications on how we choose to model and present our work in a reliable and verifiable way. You must decide for yourself if Taleb has a point or is a just another crazy rich person with too much time on his hands.&lt;/p&gt;
&lt;h4 name=&quot;9e6e&quot; id=&quot;9e6e&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Not all real numbers between 0 and 1 are probabilities&lt;/h4&gt;
&lt;p name=&quot;f48e&quot; id=&quot;f48e&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;The primary source of controversy and confusion surrounding FiveThirtyEight’s predictions is that they are ‘probabilistic.’ Practically what this means is that they do not predict a winner or looser but instead report a likelihood. Further complicating the issue, these predictions are reported as point estimates (sometimes with model implied error), well in advance of the event. For example, six months before polls open, this was their forecast of the 2016 presidential election.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*KMG4rwNgtELoFVMg_aNtkQ.png&quot; data-width=&quot;743&quot; data-height=&quot;453&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*KMG4rwNgtELoFVMg_aNtkQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*KMG4rwNgtELoFVMg_aNtkQ.png&quot;/&gt;&lt;/div&gt;
FiveThirtyEight Running Forecast of the 2016 Presidential Election
&lt;p name=&quot;880a&quot; id=&quot;880a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Their forecast process is to build a quantitative replica of a system with expert knowledge (elections, sporting events, etc.) then run a &lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot; data-href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Monte Carlo simulation&lt;/a&gt;. If the model closely represents the real-world, the simulation averages can be reliably used for probabilistic statements. So what FiveThirtyEight is actually saying is:&lt;/p&gt;
&lt;blockquote name=&quot;494c&quot; id=&quot;494c&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;x% of the time our Monte Carlo simulation resulted in this particular outcome&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;21c7&quot; id=&quot;21c7&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;The problem is that models are not perfect replicas of the real world and are, as a matter of fact, always wrong in some way. This type of model building allows for some amount of subjectivity in construction. &lt;a href=&quot;https://fivethirtyeight.com/features/the-media-has-a-probability-problem/&quot; data-href=&quot;https://fivethirtyeight.com/features/the-media-has-a-probability-problem/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;For example, Silver has said on numerous occasions that other competitive models do not correctly incorporate correlation.&lt;/a&gt; When describing modeling approaches, he also makes clear that they tune outcomes &lt;a href=&quot;https://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/&quot; data-href=&quot;https://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;(like artificially increasing variance based on the time until an event or similar adjustments)&lt;/a&gt;. This creates an infinitely recursive debate as to whose model is the ‘best’ or most like the real world. Of course, to judge this, you could look at who performed better in the long run. This is where things go off the rails a bit.&lt;/p&gt;
&lt;p name=&quot;e736&quot; id=&quot;e736&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Because FiveThirtyEight only predicts probabilities, they do not ever take an absolute stand on an outcome: No ‘skin in the game’ as Taleb would say. This is not, however, something their readers follow suit on. In the public eye, they (FiveThirtyEight) are judged on how many events with forecasted probabilities above and below 50% happened or didn’t respectively (in a binary setting). Or, they (the readers) just pick the highest reported probability as the intended forecast. For example, they were showered with accolades when after, ‘calling 49 of 50 states in the 2008 presidential race correctly’ &lt;a href=&quot;http://content.time.com/time/specials/packages/article/0,28804,1894410_1893209_1893477,00.html&quot; data-href=&quot;http://content.time.com/time/specials/packages/article/0,28804,1894410_1893209_1893477,00.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Nate Silver was placed on Times 100 most influential people lis&lt;/a&gt;t. He should not have accepted the honor if he didn’t call a winner in any of the states!&lt;/p&gt;
&lt;p name=&quot;bc91&quot; id=&quot;bc91&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The public can be excused for using the 50% rule without asking. For example, in supervised machine learning, a classification model must have a characteristic called a ‘decision boundary.’ This is often decided a priori and is a fundamental part of understanding the quality of the model after it is trained. Above this boundary, the machine believes one thing and below it the opposite (in the binary case).&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*EOVG2D1GD8kLz56sb_NKsA.png&quot; data-width=&quot;256&quot; data-height=&quot;116&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*EOVG2D1GD8kLz56sb_NKsA.png&quot;/&gt;&lt;/div&gt;
Example Decision Boundary in Classification Problems
&lt;p name=&quot;d1e3&quot; id=&quot;d1e3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;For standard models, like logistic regression, the default decision boundary is assumed to be 50% (or 0.5 on a 0 to 1 scale) or the alternative with the highest value. Classical neural networks designed for classification often use softmax functions which are interpreted in just this way. Here is an example Convolutional Neural Network performing an image classification using computer vision. Even this basic Artifical Intelligence model manages to make a decision.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*hkXhAIqR-1IB78_35DMDeg.gif&quot; data-width=&quot;480&quot; data-height=&quot;200&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*hkXhAIqR-1IB78_35DMDeg.gif&quot;/&gt;&lt;/div&gt;
&lt;a href=&quot;http://cs231n.stanford.edu/&quot; data-href=&quot;http://cs231n.stanford.edu/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Convolutional Neural Network with Decision Boundary Prediction&lt;/a&gt;
&lt;p name=&quot;209a&quot; id=&quot;209a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;If FiveThirtyEight has no stated decision boundary, it can be difficult to know how good their model actually is. The confusion is compounded when they are crowned, and gladly accept it, with platitudes of crystal ball-like precision in 2008 and 2012, due to the implied decision boundary. However, when they are accused of being wrong they fall back to a simple quip:&lt;/p&gt;
&lt;blockquote name=&quot;54bf&quot; id=&quot;54bf&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/NateSilver538/status/1059149034693316608&quot; data-href=&quot;https://twitter.com/NateSilver538/status/1059149034693316608&quot; class=&quot;markup--anchor markup--blockquote-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;You just don’t understand math and probability.&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;4c86&quot; id=&quot;4c86&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Often this is f&lt;a href=&quot;https://fivethirtyeight.com/features/the-media-has-a-probability-problem/&quot; data-href=&quot;https://fivethirtyeight.com/features/the-media-has-a-probability-problem/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;ollowed up with an exposé about how they only reported x%, so that means that (1-x)% can also happen&lt;/a&gt;. It’s a perfect scenario; they can never be wrong! We should all be so lucky. Of course, this probabilistic argument may be valid, but it can cause some angst if it seems disingenuous. &lt;a href=&quot;https://www.washingtonpost.com/opinions/no-matter-who-wins-the-presidential-election-nate-silver-was-right/2016/11/08/540825dc-a5eb-11e6-ba59-a7d93165c6d4_story.html?noredirect=on&amp;amp;utm_term=.47a35c316189&quot; data-href=&quot;https://www.washingtonpost.com/opinions/no-matter-who-wins-the-presidential-election-nate-silver-was-right/2016/11/08/540825dc-a5eb-11e6-ba59-a7d93165c6d4_story.html?noredirect=on&amp;amp;utm_term=.47a35c316189&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Even the Washington Post had an opinion piece which opined as much during the 2016 election.&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;b172&quot; id=&quot;b172&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;What is not clear is that there is a factor hidden from the FiveThirtyEight reader. &lt;a href=&quot;https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty&quot; data-href=&quot;https://en.wikipedia.org/wiki/Uncertainty_quantification#Aleatoric_and_epistemic_uncertainty&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Predictions have two types of uncertainty; &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;aleatory&lt;/strong&gt; and &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;epistemic&lt;/strong&gt;.&lt;/a&gt; Aleatory uncertainty is concerned with the fundamental system (probability of rolling a six on a standard die). Epistemic uncertainty is concerned with the uncertainty of the system (how many sides does a die have? And what is the probability of rolling a six?). With the later, you have to guess the game and the outcome; like an election!&lt;/p&gt;
&lt;p name=&quot;145a&quot; id=&quot;145a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Bespoke models, like FiveThirtyEight’s, only report to the public aleatory uncertainty as it concerns their statistical outputs (inference by Monte Carlo in this case). The trouble is that epistemic uncertainty is very difficult (sometimes impossible) to estimate. &lt;a href=&quot;https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/&quot; data-href=&quot;https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;For example, why didn’t FiveThirtyEight’s model incorporate,&lt;/a&gt; &lt;a href=&quot;https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/&quot; data-href=&quot;https://fivethirtyeight.com/features/the-comey-letter-probably-cost-clinton-the-election/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;before it happened, a chance that Comey would re-open his investigation into Clintons emails?&lt;/a&gt; Instead, this seems to have caused a massive spike in the variation of the prediction. Likely because this event was impossible to forecast.&lt;/p&gt;
&lt;p name=&quot;2655&quot; id=&quot;2655&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Instead, epistemically uncertain events are ignored a priori and then FiveThirtyEight assumes wild fluctuations in a prediction from unforeseen events are a normal part of forecasting. Which should lead us to ask ‘If the model is ignoring some of the most consequential uncertainties, are we really getting a reliable probability?’&lt;/p&gt;
&lt;p name=&quot;1692&quot; id=&quot;1692&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To expand on this further, I have consolidated some of FiveThirtyEight’s predictions, &lt;a href=&quot;https://data.fivethirtyeight.com/&quot; data-href=&quot;https://data.fivethirtyeight.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;using their open source data&lt;/a&gt;, for two very different types of events; U.S. Senate elections and National Football Leauge (NFL) Games. Here is a comparison to the final forecast probability and the actual proportion of outcomes.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*aDZ5EWPzWCdd1cpOWkiB6A.png&quot; data-width=&quot;862&quot; data-height=&quot;409&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*aDZ5EWPzWCdd1cpOWkiB6A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*aDZ5EWPzWCdd1cpOWkiB6A.png&quot;/&gt;&lt;/div&gt;
Stated Probabilities Compared with Average Portions
&lt;p name=&quot;b1e4&quot; id=&quot;b1e4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The sports data (NFL games) has an excellent linear relationship. These proportions are built using 30K data points, so, if we assume the system is stable, we have averaged out any sampling error. However, as you can see, there is still a noticable variation of 2–5% of actual proportion to predictions. This is a signal of un-addressed epistemic uncertainty. It also means you cannot take one of these forecast probabilities at face value.&lt;/p&gt;
&lt;p name=&quot;d6f4&quot; id=&quot;d6f4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Sports, like other games of chance, have very well defined mechanisms which lend themselves to statistical analysis. On the other hand, highly non-linear events, like contested elections, may not. With much fewer data points you can see the variation of the Senate predictions is enormous. Gauging the performance of models on these types of events becomes doubly difficult. It isn’t clear if a prediction is wrong owing to the quality of the model (epistemic) or just luck (aleatory).&lt;/p&gt;
&lt;p name=&quot;c1d2&quot; id=&quot;c1d2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;One of the most troubling things about this approach to forecasting is that it opens pandora’s box for narrative fallacies. Why did Clinton lose? Comey? Email servers? People can then justify possibly spurious inferences by eyeballing events which occur around the forecast variation. ‘Just look at how the forecast is changing will all this news!’&lt;/p&gt;
&lt;p name=&quot;2ad8&quot; id=&quot;2ad8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I think this is what has Taleb up in arms. The blog feels more like a slick sales pitch, complete with quantitative buzzwords, than unbiased analysis (though it may very well be). If a prediction does not obey some fundamental characteristics, it should not be marketed as a probability. More importantly, a prediction should be judged from the time it is given to the public and not just the moment before the event. A forecaster should be held responsible for both aleatory and epistemic uncertainty.&lt;/p&gt;
&lt;p name=&quot;c2c5&quot; id=&quot;c2c5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;When viewed this way, it is clear that FiveThirtyEight reports too much noise leading up to an event and not enough signal. This is great for driving users to read long series of related articles on the same topic but not so rigorous to bet your fortune on. Taleb and Silvers take on how FiveThirtyEight should be judged can be visualized like this.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*D_tidaT-fHMY3DRLgjwekw.png&quot; data-width=&quot;1800&quot; data-height=&quot;1200&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*D_tidaT-fHMY3DRLgjwekw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*D_tidaT-fHMY3DRLgjwekw.png&quot;/&gt;&lt;/div&gt;
Taleb vs. Silvers Different Take On How FiveThirtyEight Should be Judged in 2016
&lt;p name=&quot;5590&quot; id=&quot;5590&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Because there is so much uncertainty around non-linear events, like an election, it could reasonably be considered frivolous to report early stage forecasts. The only conceivable reason to do so is to capture (and monetize?) the interest of a public which is hungry to know the future. &lt;a href=&quot;https://arxiv.org/pdf/1703.06351.pdf&quot; data-href=&quot;https://arxiv.org/pdf/1703.06351.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;I will not go into the technical arguments, Taleb has written and published a paper on the key issues with a solution.&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;bc1b&quot; id=&quot;bc1b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Here we can say, with some confidence that FiveThirtyEight predictions are not reliable probabilities. However, they masquerade as one, being between 0 and 1 and all. This is Taleb’s primary argument; FiveThirtyEight’s predictions do not behave like probabilities that incorporate all uncertainty and should not be passed off as them.&lt;/p&gt;
&lt;p name=&quot;9de0&quot; id=&quot;9de0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I do not want to suggest that FiveThirtyEight is bad at their craft. They are, likely, the best poll aggregator in the business. If we only look at the last reported probabilistic forecast and use the public’s decision boundary, they are more successful than any other source attempting the same task. However, positioning yourself to appear correct regardless of the outcome, making users infer their own decision boundaries, over-reporting of predictions, and ignoring epistemic uncertainty should not be overlooked. How goes FiveThirtyEight’s reputation, so goes much of the data community’s reputation.&lt;/p&gt;
&lt;p name=&quot;8029&quot; id=&quot;8029&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Be clear on your suggested decision boundary, probabilistic statements, assumptions about uncertainty and you’ll be less likely to misguide stakeholders.&lt;/p&gt;
&lt;p name=&quot;0490&quot; id=&quot;0490&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;a href=&quot;https://community.platform.matrixds.com/community/project/5c09a98a5fd07d6fa67f645d/files&quot; data-href=&quot;https://community.platform.matrixds.com/community/project/5c09a98a5fd07d6fa67f645d/files&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;All code and data can be forklifted from this MatrixDS project.&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;7153&quot; id=&quot;7153&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Follow me on LinkedIn: &lt;a href=&quot;https://www.linkedin.com/in/isaacfaber/&quot; data-href=&quot;https://www.linkedin.com/in/isaacfaber/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener noopener&quot; target=&quot;_blank&quot;&gt;https://www.linkedin.com/in/isaacfaber/&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;0470&quot; id=&quot;0470&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;Follow me on MatrixDS: &lt;a href=&quot;https://community.platform.matrixds.com/community/isaacfab/overview&quot; data-href=&quot;https://community.platform.matrixds.com/community/isaacfab/overview&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener noopener&quot; target=&quot;_blank&quot;&gt;https://community.platform.matrixds.com/community/isaacfab/overview&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 18 Dec 2018 03:45:29 +0000</pubDate>
<dc:creator>oska</dc:creator>
<og:title>Why you should care about the Nate Silver vs. Nassim Taleb Twitter war</og:title>
<og:url>https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*D_tidaT-fHMY3DRLgjwekw.png</og:image>
<og:description>How can two data experts disagree so much?</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc?gi=adf682f2c20d</dc:identifier>
</item>
<item>
<title>Dotsies (2012)</title>
<link>http://dotsies.org/</link>
<guid isPermaLink="true" >http://dotsies.org/</guid>
<description>&lt;div class=&quot;underline&quot; readability=&quot;38&quot;&gt;
&lt;p class=&quot;connected&quot;&gt;hi there!&lt;/p&gt;
&lt;p class=&quot;connected&quot;&gt;can you read this?&lt;/p&gt;
&lt;p class=&quot;connected&quot;&gt;if you can read this, keep going.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;connected&quot;&gt;this section gr&lt;/span&gt;adua&lt;span class=&quot;connected&quot;&gt;ll&lt;/span&gt;y teaches &lt;span class=&quot;connected&quot;&gt;y&lt;/span&gt;ou to read d&lt;span class=&quot;connected&quot;&gt;o&lt;/span&gt;tsies.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;connected&quot;&gt;t&lt;/span&gt;he d&lt;span class=&quot;connected&quot;&gt;o&lt;/span&gt;tsies font is &lt;span class=&quot;connected&quot;&gt;m&lt;/span&gt;ade out of o&lt;span class=&quot;connected&quot;&gt;nl&lt;/span&gt;y dots.&lt;/p&gt;
&lt;p&gt;here are s&lt;span class=&quot;connected&quot;&gt;o&lt;/span&gt;me &lt;span class=&quot;connected&quot;&gt;w&lt;/span&gt;ords in dotsies: &lt;strong&gt;nice job&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;connected&quot;&gt;m&lt;/span&gt;ove &lt;span class=&quot;connected&quot;&gt;y&lt;/span&gt;our mouse over the light underline to see what they say.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;from here on out, some letters will be in dotsies.&lt;/p&gt;
&lt;p&gt;move your &lt;strong&gt;mouse&lt;/strong&gt; over their underlines when you need to.&lt;/p&gt;
&lt;p&gt;if you can get all &lt;strong&gt;the&lt;/strong&gt; way to &lt;strong&gt;the&lt;/strong&gt; bottom, you'll be reading &lt;strong&gt;the&lt;/strong&gt; dotsies font.&lt;/p&gt;
&lt;p&gt;believe &lt;strong&gt;it&lt;/strong&gt; or not, you've already made &lt;strong&gt;a&lt;/strong&gt; lot of progress!&lt;/p&gt;
&lt;p&gt;you &lt;strong&gt;are&lt;/strong&gt; on your &lt;strong&gt;way&lt;/strong&gt; to doing what only &lt;strong&gt;a&lt;/strong&gt; few people &lt;strong&gt;have&lt;/strong&gt; done.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; top dot means an &lt;strong&gt;a&lt;/strong&gt;, right? let's t&lt;strong&gt;a&lt;/strong&gt;ke &lt;strong&gt;the&lt;/strong&gt;m &lt;strong&gt;a&lt;/strong&gt;w&lt;strong&gt;a&lt;/strong&gt;y.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;a&lt;/strong&gt;nd &lt;strong&gt;the&lt;/strong&gt; bottom dot is an &lt;strong&gt;e&lt;/strong&gt;, so l&lt;strong&gt;e&lt;/strong&gt;t's hid&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;the&lt;/strong&gt;m too.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;the&lt;/strong&gt;s&lt;strong&gt;e&lt;/strong&gt; gr&lt;strong&gt;a&lt;/strong&gt;y und&lt;strong&gt;e&lt;/strong&gt;rlin&lt;strong&gt;e&lt;/strong&gt;s &lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; g&lt;strong&gt;e&lt;/strong&gt;tting clutt&lt;strong&gt;e&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt;d, so l&lt;strong&gt;e&lt;/strong&gt;t's r&lt;strong&gt;e&lt;/strong&gt;mov&lt;strong&gt;e&lt;/strong&gt; th&lt;strong&gt;e&lt;/strong&gt;m.&lt;/p&gt;
&lt;/div&gt;
&lt;hr/&gt;&lt;p&gt;you c&lt;strong&gt;a&lt;/strong&gt;n still mov&lt;strong&gt;e&lt;/strong&gt; your mous&lt;strong&gt;e&lt;/strong&gt; und&lt;strong&gt;e&lt;/strong&gt;r &lt;strong&gt;the&lt;/strong&gt; l&lt;strong&gt;e&lt;/strong&gt;tt&lt;strong&gt;e&lt;/strong&gt;rs to &lt;strong&gt;e&lt;/strong&gt;xpos&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;the&lt;/strong&gt;m.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; most common conson&lt;strong&gt;a&lt;/strong&gt;nt is &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;. goodbye &lt;strong&gt;t&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;ok, w&lt;strong&gt;e&lt;/strong&gt;'v&lt;strong&gt;e&lt;/strong&gt; go&lt;strong&gt;tte&lt;/strong&gt;n rid of &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;hr&lt;strong&gt;ee&lt;/strong&gt; mos&lt;strong&gt;t&lt;/strong&gt; common l&lt;strong&gt;ette&lt;/strong&gt;rs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; four&lt;strong&gt;t&lt;/strong&gt;h mos&lt;strong&gt;t&lt;/strong&gt; common l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;r is &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;o&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;if y&lt;strong&gt;o&lt;/strong&gt;u g&lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;his f&lt;strong&gt;a&lt;/strong&gt;r y&lt;strong&gt;o&lt;/strong&gt;u &lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; d&lt;strong&gt;o&lt;/strong&gt;ing r&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;a&lt;/strong&gt;lly w&lt;strong&gt;e&lt;/strong&gt;ll!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;o&lt;/strong&gt;nly &lt;strong&gt;t&lt;/strong&gt;w&lt;strong&gt;o&lt;/strong&gt; m&lt;strong&gt;o&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;a&lt;/strong&gt;nd h&lt;strong&gt;a&lt;/strong&gt;lf &lt;strong&gt;o&lt;/strong&gt;f &lt;strong&gt;a&lt;/strong&gt;ll &lt;strong&gt;the&lt;/strong&gt; l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;rs will b&lt;strong&gt;e&lt;/strong&gt; hidd&lt;strong&gt;e&lt;/strong&gt;n!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;'s b&lt;strong&gt;e&lt;/strong&gt;c&lt;strong&gt;a&lt;/strong&gt;us&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;the&lt;/strong&gt; six m&lt;strong&gt;o&lt;/strong&gt;s&lt;strong&gt;t&lt;/strong&gt; c&lt;strong&gt;o&lt;/strong&gt;mm&lt;strong&gt;o&lt;/strong&gt;n l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;rs &lt;strong&gt;o&lt;/strong&gt;ccur h&lt;strong&gt;a&lt;/strong&gt;lf &lt;strong&gt;the&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;im&lt;strong&gt;e&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;rs &lt;strong&gt;i&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;nd &lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;x&lt;strong&gt;t&lt;/strong&gt;. &lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt; w&lt;strong&gt;a&lt;/strong&gt;s&lt;strong&gt;n&lt;/strong&gt;'&lt;strong&gt;t&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt; b&lt;strong&gt;a&lt;/strong&gt;d, r&lt;strong&gt;i&lt;/strong&gt;gh&lt;strong&gt;t&lt;/strong&gt;?&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;h&lt;strong&gt;e&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt; gr&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt; c&lt;strong&gt;o&lt;/strong&gt;upl&lt;strong&gt;e&lt;/strong&gt; l&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;s fr&lt;strong&gt;o&lt;/strong&gt;m s&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;v&lt;strong&gt;e&lt;/strong&gt; j&lt;strong&gt;o&lt;/strong&gt;bs:&lt;/p&gt;
&lt;p&gt;&quot;l&lt;strong&gt;i&lt;/strong&gt;f&lt;strong&gt;e&lt;/strong&gt; c&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; b&lt;strong&gt;e&lt;/strong&gt; much br&lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;a&lt;/strong&gt;d&lt;strong&gt;e&lt;/strong&gt;r &lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;c&lt;strong&gt;e&lt;/strong&gt; y&lt;strong&gt;o&lt;/strong&gt;u d&lt;strong&gt;i&lt;/strong&gt;sc&lt;strong&gt;o&lt;/strong&gt;v&lt;strong&gt;e&lt;/strong&gt;r &lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt; s&lt;strong&gt;i&lt;/strong&gt;mpl&lt;strong&gt;e&lt;/strong&gt; f&lt;strong&gt;a&lt;/strong&gt;c&lt;strong&gt;t&lt;/strong&gt;; &lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt; &lt;strong&gt;e&lt;/strong&gt;v&lt;strong&gt;e&lt;/strong&gt;ry&lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;g &lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;o&lt;/strong&gt;u&lt;strong&gt;n&lt;/strong&gt;d y&lt;strong&gt;o&lt;/strong&gt;u &lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt; y&lt;strong&gt;o&lt;/strong&gt;u c&lt;strong&gt;a&lt;/strong&gt;ll l&lt;strong&gt;i&lt;/strong&gt;f&lt;strong&gt;e&lt;/strong&gt; w&lt;strong&gt;a&lt;/strong&gt;s m&lt;strong&gt;a&lt;/strong&gt;d&lt;strong&gt;e&lt;/strong&gt; up by p&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt;pl&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt; sm&lt;strong&gt;a&lt;/strong&gt;r&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;r &lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; y&lt;strong&gt;o&lt;/strong&gt;u.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;the&lt;/strong&gt; m&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;u&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt; y&lt;strong&gt;o&lt;/strong&gt;u u&lt;strong&gt;n&lt;/strong&gt;d&lt;strong&gt;e&lt;/strong&gt;rs&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;d &lt;strong&gt;t&lt;/strong&gt;h&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;, y&lt;strong&gt;o&lt;/strong&gt;u c&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; p&lt;strong&gt;o&lt;/strong&gt;k&lt;strong&gt;e&lt;/strong&gt; l&lt;strong&gt;i&lt;/strong&gt;f&lt;strong&gt;e&lt;/strong&gt;; y&lt;strong&gt;o&lt;/strong&gt;u c&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; ch&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;g&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;, y&lt;strong&gt;o&lt;/strong&gt;u c&lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; m&lt;strong&gt;o&lt;/strong&gt;ld &lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;mbr&lt;strong&gt;a&lt;/strong&gt;c&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;, m&lt;strong&gt;a&lt;/strong&gt;k&lt;strong&gt;e&lt;/strong&gt; y&lt;strong&gt;o&lt;/strong&gt;ur m&lt;strong&gt;a&lt;/strong&gt;rk up&lt;strong&gt;o&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;i&lt;/strong&gt;t.&quot;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;'s r&lt;strong&gt;e&lt;/strong&gt;m&lt;strong&gt;o&lt;/strong&gt;v&lt;strong&gt;e&lt;/strong&gt; s&lt;strong&gt;o&lt;/strong&gt;m&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;o&lt;/strong&gt;f &lt;strong&gt;the&lt;/strong&gt; m&lt;strong&gt;o&lt;/strong&gt;us&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt;v&lt;strong&gt;e&lt;/strong&gt;r h&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;s, b&lt;strong&gt;e&lt;/strong&gt;f&lt;strong&gt;o&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; &lt;strong&gt;a&lt;/strong&gt;dd&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;g l&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;e&lt;/strong&gt;rs.&lt;/p&gt;
&lt;p&gt;y&lt;strong&gt;o&lt;/strong&gt;u pr&lt;strong&gt;o&lt;/strong&gt;b&lt;strong&gt;a&lt;/strong&gt;bly k&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt;w &lt;strong&gt;a&lt;/strong&gt;, &lt;strong&gt;e&lt;/strong&gt;, &lt;strong&gt;a&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;d &lt;strong&gt;o&lt;/strong&gt; pr&lt;strong&gt;e&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;y w&lt;strong&gt;e&lt;/strong&gt;ll, s&lt;strong&gt;o&lt;/strong&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;o&lt;/strong&gt; m&lt;strong&gt;o&lt;/strong&gt;r&lt;strong&gt;e&lt;/strong&gt; h&lt;strong&gt;i&lt;/strong&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;s f&lt;strong&gt;o&lt;/strong&gt;r &lt;strong&gt;the&lt;/strong&gt;m.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;a&lt;/u&gt;ls&lt;u&gt;o&lt;/u&gt;, &lt;strong&gt;i&lt;/strong&gt; &lt;strong&gt;i&lt;/strong&gt;s pr&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;y &lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;sy &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;d l&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;ks l&lt;strong&gt;i&lt;/strong&gt;k&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;i&lt;/strong&gt;, s&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; h&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;s f&lt;u&gt;o&lt;/u&gt;r &lt;strong&gt;i&lt;/strong&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;w.&lt;/p&gt;
&lt;p&gt;y&lt;u&gt;o&lt;/u&gt;u &lt;u&gt;a&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;g v&lt;u&gt;e&lt;/u&gt;ry w&lt;u&gt;e&lt;/u&gt;ll &lt;u&gt;i&lt;/u&gt;f y&lt;u&gt;o&lt;/u&gt;u'v&lt;u&gt;e&lt;/u&gt; g&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt; &lt;strong&gt;t&lt;/strong&gt;h&lt;u&gt;i&lt;/u&gt;s f&lt;u&gt;a&lt;/u&gt;r.&lt;/p&gt;
&lt;p&gt;l&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;t&lt;/strong&gt;'s d&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; m&lt;u&gt;o&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;t&lt;/strong&gt; h&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;strong&gt;t&lt;/strong&gt;s. &lt;strong&gt;t&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;s &lt;u&gt;e&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;rywh&lt;u&gt;e&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;f w&lt;u&gt;e&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;n&lt;/strong&gt; h&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;s, w&lt;u&gt;e&lt;/u&gt; c&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt; m&lt;u&gt;o&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; m&lt;u&gt;o&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;rs.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;h&lt;u&gt;i&lt;/u&gt;s c&lt;u&gt;o&lt;/u&gt;uld b&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt; g&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;d &lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;m&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; g&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;a href=&quot;http://memorize.com/dotsies&quot;&gt;m&lt;u&gt;e&lt;/u&gt;m&lt;u&gt;o&lt;/u&gt;r&lt;u&gt;i&lt;/u&gt;z&lt;u&gt;e&lt;/u&gt;.c&lt;u&gt;o&lt;/u&gt;m/d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;s&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;s&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;r, &lt;u&gt;i&lt;/u&gt;f y&lt;u&gt;o&lt;/u&gt;u'r&lt;u&gt;e&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;j&lt;u&gt;o&lt;/u&gt;y&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g jus&lt;u&gt;t&lt;/u&gt; r&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;d&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;u&gt;t&lt;/u&gt;h&lt;u&gt;i&lt;/u&gt;s w&lt;u&gt;a&lt;/u&gt;y, &lt;u&gt;t&lt;/u&gt;h&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'s g&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;d &lt;u&gt;a&lt;/u&gt;ls&lt;u&gt;o&lt;/u&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;s &lt;u&gt;i&lt;/u&gt;s &lt;u&gt;t&lt;/u&gt;h&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;x&lt;u&gt;t&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;r. &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;s &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;sy &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; l&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;k&lt;strong&gt;s&lt;/strong&gt; l&lt;u&gt;i&lt;/u&gt;k&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;m&lt;u&gt;a&lt;/u&gt;ll &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;k&lt;u&gt;e&lt;/u&gt;. &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; h&lt;u&gt;o&lt;/u&gt;w &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;h&lt;u&gt;e&lt;/u&gt; m&lt;u&gt;i&lt;/u&gt;ddl&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;r h &lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;f&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;r &lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;h&lt;u&gt;e&lt;/u&gt; fr&lt;u&gt;e&lt;/u&gt;qu&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;cy l&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;h&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;rd&lt;u&gt;e&lt;/u&gt;r &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt; bu&lt;u&gt;t&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;g&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;r &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;d&lt;u&gt;i&lt;/u&gt;d &lt;u&gt;i&lt;/u&gt; m&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; y&lt;u&gt;o&lt;/u&gt;u &lt;u&gt;a&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g f&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;c&lt;u&gt;a&lt;/u&gt;lly w&lt;u&gt;e&lt;/u&gt;ll?&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;ly &lt;u&gt;t&lt;/u&gt;w&lt;u&gt;o&lt;/u&gt; m&lt;u&gt;o&lt;/u&gt;r&lt;u&gt;e&lt;/u&gt;, &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;d &lt;u&gt;a&lt;/u&gt;b&lt;u&gt;o&lt;/u&gt;u&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;r&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;o&lt;/u&gt;ur&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt; w&lt;u&gt;i&lt;/u&gt;ll b&lt;u&gt;e&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;r k&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;d &lt;u&gt;o&lt;/u&gt;f l&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;k&lt;strong&gt;s&lt;/strong&gt; l&lt;u&gt;i&lt;/u&gt;k&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; r, w&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;l&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;p.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;r&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;'&lt;u&gt;t&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt; w&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;d&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;d &lt;u&gt;i&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt; d&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;b&lt;u&gt;o&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;, &lt;u&gt;o&lt;/u&gt;f c&lt;u&gt;o&lt;/u&gt;u&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;, &lt;strong&gt;d&lt;/strong&gt; w&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt; u&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; m&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;strong&gt;d&lt;/strong&gt;l&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p class=&quot;sans&quot;&gt;Great job so far! Tweet me at &lt;a href=&quot;http://twitter.com/DotsiesFont&quot;&gt;@DotsiesFont&lt;/a&gt; and tell me your thoughts!&lt;br/&gt;If you want to continue with something a little less intense, try reading a story &lt;a href=&quot;http://dotsies.org/reader&quot;&gt;here&lt;/a&gt;. Or, if you are enjoying the tutorial, continue on:&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;u&gt;e&lt;/u&gt;xc&lt;u&gt;e&lt;/u&gt;ll&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;, &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;k&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; c&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;o&lt;/u&gt;u&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;f l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;a&lt;/u&gt;f&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;x&lt;u&gt;t&lt;/u&gt; g&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;up w&lt;u&gt;e&lt;/u&gt;'ll b&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;b&lt;u&gt;o&lt;/u&gt;u&lt;u&gt;t&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;y p&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;c&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;!&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w &lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;w&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;bl&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;a href=&quot;http://memorize.com/dotsies&quot;&gt;m&lt;u&gt;e&lt;/u&gt;m&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;z&lt;u&gt;e&lt;/u&gt;.c&lt;u&gt;o&lt;/u&gt;m/&lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;x&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;bl&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;, &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;c&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;y c&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt;p&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;ul&lt;strong&gt;d&lt;/strong&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;lp y&lt;u&gt;o&lt;/u&gt;u b&lt;u&gt;e&lt;/u&gt; c&lt;u&gt;o&lt;/u&gt;mf&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;bl&lt;u&gt;e&lt;/u&gt; w&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;strong&gt;s&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w w&lt;u&gt;e&lt;/u&gt; c&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;u&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;m&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;s&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;ff.&lt;/p&gt;
&lt;p&gt;l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;strong&gt;s&lt;/strong&gt; &lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;m&lt;u&gt;o&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt; b&lt;u&gt;e&lt;/u&gt;c&lt;u&gt;a&lt;/u&gt;u&lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;strong&gt;s&lt;/strong&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;s&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;mpl&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w &lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; l&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;strong&gt;h&lt;/strong&gt; &lt;strong&gt;h&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; f&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;ly &lt;u&gt;t&lt;/u&gt;w&lt;u&gt;o&lt;/u&gt; m&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;f&lt;u&gt;t&lt;/u&gt;, &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt; w&lt;u&gt;e&lt;/u&gt;'&lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;y f&lt;u&gt;i&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; p&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;r&lt;/strong&gt;c&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;r&lt;/strong&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;l&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;p, &lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;u&gt;s&lt;/u&gt; &lt;strong&gt;r&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;m&lt;u&gt;o&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;d&lt;/strong&gt; &lt;strong&gt;d&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;gl&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;, &lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;u&gt;s&lt;/u&gt; &lt;strong&gt;d&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;w&lt;u&gt;o&lt;/u&gt;w, &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;o&lt;/u&gt;u&lt;u&gt;r&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; l&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; f&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;c! b&lt;u&gt;e&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;u&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;ll u&lt;u&gt;s&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;w&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;w&lt;u&gt;n&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;ll f&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;m &lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;bl&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;a href=&quot;http://memorize.com/dotsies&quot;&gt;m&lt;u&gt;e&lt;/u&gt;m&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;z&lt;u&gt;e&lt;/u&gt;.c&lt;u&gt;o&lt;/u&gt;m/&lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; w&lt;u&gt;a&lt;/u&gt;y w&lt;u&gt;e&lt;/u&gt; c&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;x&lt;u&gt;t&lt;/u&gt; f&lt;u&gt;i&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;ll &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;c&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;y &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; l, c, u, m &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; w.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;k, &lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;w&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;? y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;'&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;w&lt;/strong&gt; &lt;strong&gt;u&lt;/strong&gt;p &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;y p&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;strong&gt;c&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;!&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt;p&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g&lt;strong&gt;l&lt;/strong&gt;y &lt;u&gt;r&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;y.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;x &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;y &lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;strong&gt;w&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt; p&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;strong&gt;c&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;g&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;y &lt;u&gt;s&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;d&lt;/u&gt; b&lt;u&gt;e&lt;/u&gt; &lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; f&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;m&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;p&lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;b&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;u&gt;s&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;w&lt;/strong&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;w&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;w&lt;/strong&gt;: &lt;u&gt;a&lt;/u&gt;b&lt;strong&gt;c&lt;/strong&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;fg&lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;jk&lt;strong&gt;l&lt;/strong&gt;&lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;pq&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;v&lt;strong&gt;w&lt;/strong&gt;xyz.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; b&lt;u&gt;a&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;! y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;u&gt;s&lt;/u&gt;p&lt;strong&gt;l&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;y &lt;u&gt;i&lt;/u&gt;f y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;'r&lt;u&gt;e&lt;/u&gt; &lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;!&lt;/p&gt;
&lt;p&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;c&lt;/strong&gt;&lt;u&gt;h&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt; b&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;c&lt;/strong&gt;k &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; g&lt;u&gt;i&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt; y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;f &lt;u&gt;a&lt;/u&gt; g&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; p&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt; &lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;yb&lt;u&gt;e&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;a&lt;/u&gt; q&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;c&lt;/strong&gt;k &lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;g&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;k, &lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;p &lt;strong&gt;m&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;g&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;g y&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;u&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;l&lt;/strong&gt;f. &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;ff!&lt;/p&gt;
&lt;p&gt;&lt;u&gt;w&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;'&lt;u&gt;l&lt;/u&gt;&lt;u&gt;l&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;x&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;w&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;b&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; q&lt;u&gt;u&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;k&lt;u&gt;l&lt;/u&gt;y.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; y&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt; k&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt; &lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;f &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; y&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt; &lt;u&gt;c&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;f&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; g&lt;u&gt;u&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt; g&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;a href=&quot;http://memorize.com/dotsies&quot;&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;z&lt;u&gt;e&lt;/u&gt;.&lt;u&gt;c&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;/&lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;/a&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; f&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;b&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; f, g, y, p &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; b.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt;'&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;strong&gt;f&lt;/strong&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt;, &lt;strong&gt;b&lt;/strong&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;'&lt;u&gt;t&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;k &lt;u&gt;m&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;&lt;u&gt;h&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;, &lt;u&gt;e&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;?&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;'&lt;u&gt;s&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;l&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt;. &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;g&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;strong&gt;p&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;'&lt;u&gt;m&lt;/u&gt; &lt;strong&gt;g&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;g&lt;/strong&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;strong&gt;f&lt;/strong&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;a&lt;/u&gt; &lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;strong&gt;y&lt;/strong&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt; &lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt;? &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;strong&gt;g&lt;/strong&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;strong&gt;p&lt;/strong&gt; &lt;u&gt;w&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;r&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt;&lt;u&gt;w&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;strong&gt;y&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;k, &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;strong&gt;f&lt;/strong&gt;&lt;strong&gt;f&lt;/strong&gt; &lt;u&gt;a&lt;/u&gt;&lt;strong&gt;g&lt;/strong&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt;, &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; v&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;y&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;g&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;p&lt;/u&gt;!&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;u&gt;g&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;a href=&quot;http://memorize.com/dotsies&quot;&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;z&lt;u&gt;e&lt;/u&gt;.&lt;u&gt;c&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;/&lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;.&lt;/a&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;b&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; v, k, j, x, q &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; z &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;f&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;!&lt;/p&gt;
&lt;p&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;m&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;strong&gt;v&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;y&lt;/u&gt; &lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;g&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;f&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;d&lt;/u&gt; &lt;u&gt;y&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;'&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;a&lt;/u&gt; &lt;u&gt;f&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;g&lt;/u&gt;&lt;u&gt;g&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;e&lt;/u&gt;&lt;strong&gt;x&lt;/strong&gt;&lt;u&gt;p&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; :).&lt;/p&gt;
&lt;p&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;f&lt;/u&gt; &lt;u&gt;y&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;d&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;'&lt;u&gt;t&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;, &lt;u&gt;c&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;&lt;strong&gt;k&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &quot;&lt;u&gt;c&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;b&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&quot; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;strong&gt;k&lt;/strong&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;m&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;u&gt;e&lt;/u&gt;.&lt;u&gt;c&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;m&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;y&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt; &lt;u&gt;c&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;n&lt;/u&gt; &lt;u&gt;u&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;i&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;d&lt;/u&gt;&lt;u&gt;o&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;l&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;b&lt;/u&gt;&lt;u&gt;l&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;c&lt;/u&gt;&lt;u&gt;e&lt;/u&gt;.&lt;/p&gt;
&lt;p&gt;&lt;u&gt;o&lt;/u&gt;&lt;strong&gt;k&lt;/strong&gt;, &lt;u&gt;h&lt;/u&gt;&lt;u&gt;i&lt;/u&gt;&lt;u&gt;n&lt;/u&gt;&lt;u&gt;t&lt;/u&gt;&lt;u&gt;s&lt;/u&gt; &lt;u&gt;a&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;n&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;w&lt;/u&gt; &lt;u&gt;o&lt;/u&gt;&lt;u&gt;f&lt;/u&gt;&lt;u&gt;f&lt;/u&gt; &lt;u&gt;f&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;r&lt;/u&gt; &lt;u&gt;t&lt;/u&gt;&lt;u&gt;h&lt;/u&gt;&lt;u&gt;e&lt;/u&gt; &lt;u&gt;l&lt;/u&gt;&lt;u&gt;a&lt;/u&gt;&lt;u&gt;s&lt;/u&gt;&lt;u&gt;t&lt;/u&gt; &lt;u&gt;g&lt;/u&gt;&lt;u&gt;r&lt;/u&gt;&lt;u&gt;o&lt;/u&gt;&lt;u&gt;u&lt;/u&gt;&lt;u&gt;p&lt;/u&gt;!&lt;/p&gt;
&lt;hr/&gt;&lt;div class=&quot;dotsies&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;this is the full dotsies font without the hints!&lt;/p&gt;
&lt;p&gt;you may notice that it's no longer stretched out as well.&lt;/p&gt;
&lt;p&gt;the dots are very close to squares.&lt;/p&gt;
&lt;p&gt;and here it is a bit smaller. congrats - well done! this makes you one of a pretty select group. chat with us on twitter!&lt;/p&gt;
&lt;/div&gt;
&lt;p class=&quot;sans&quot;&gt;How far did you get? Tweet me at &lt;a href=&quot;http://twitter.com/DotsiesFont&quot;&gt;@DotsiesFont&lt;/a&gt; and tell me what you think! Maybe try &lt;a href=&quot;http://dotsies.org/reader&quot;&gt;reading a story&lt;/a&gt;. Or see &lt;a href=&quot;http://dotsies.org/learn&quot;&gt;How to Learn&lt;/a&gt;.  &lt;/p&gt;
</description>
<pubDate>Tue, 18 Dec 2018 01:16:54 +0000</pubDate>
<dc:creator>severine</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://dotsies.org/</dc:identifier>
</item>
<item>
<title>Millitext – A subpixel text encoding font</title>
<link>https://advent.morr.cc/2018/17</link>
<guid isPermaLink="true" >https://advent.morr.cc/2018/17</guid>
<description>&lt;head&gt;&lt;title&gt;Millitext · Advent Calendar of Curiosities 2018&lt;/title&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;/css/default.css&quot; type=&quot;text/css&quot;/&gt;&lt;link rel=&quot;alternate&quot; type=&quot;application/atom+xml&quot; href=&quot;/feed&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=0.5, user-scalable=yes&quot;/&gt;&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot;/&gt;&lt;meta name=&quot;twitter:site&quot; content=&quot;@blinry&quot;/&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;Millitext&quot;/&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;Pixels of LCD monitors are usually composed of three stripes of the colors red, green, and blue. Tech enthusiast Matt Sarnoff used this property to his advantage when inventing [a subpixel text encoding font](http://www.msarnoff.org/millitext/). Its glyphs are comprised of colored strips only one pixel wide. But when viewed on LCD monitors, the color strips clearly form letters and numbers! Fun fact: In 2013, I had a [Twitter avatar containing millitext](https://files.morr.cc/avatar-2013-04-16-small.png)! Can you decipher it?&quot;/&gt;&lt;meta name=&quot;twitter:creator&quot; content=&quot;@blinry&quot;/&gt;&lt;meta name=&quot;twitter:image&quot; content=&quot;/2018/17/image&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;24.082291666667&quot;&gt;

&lt;div class=&quot;door&quot; readability=&quot;13.056277056277&quot;&gt;

 &lt;a href=&quot;https://advent.morr.cc/2018/17/image&quot;&gt;&lt;img class=&quot;small&quot; src=&quot;https://advent.morr.cc/2018/17/image&quot;/&gt;&lt;/a&gt;
&lt;p&gt;Pixels of LCD monitors are usually composed of three stripes of the colors red, green, and blue. Tech enthusiast Matt Sarnoff used this property to his advantage when inventing &lt;a href=&quot;http://www.msarnoff.org/millitext/&quot;&gt;a subpixel text encoding font&lt;/a&gt;. Its glyphs are comprised of colored strips only one pixel wide. But when viewed on LCD monitors, the color strips clearly form letters and numbers!&lt;/p&gt;
&lt;p&gt;Fun fact: In 2013, I had a &lt;a href=&quot;https://files.morr.cc/avatar-2013-04-16-small.png&quot;&gt;Twitter avatar containing millitext&lt;/a&gt;! Can you decipher it?&lt;/p&gt;
&lt;/div&gt;

&lt;/body&gt;</description>
<pubDate>Mon, 17 Dec 2018 22:36:03 +0000</pubDate>
<dc:creator>jonshariat</dc:creator>
<dc:language>de</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://advent.morr.cc/2018/17</dc:identifier>
</item>
<item>
<title>Neural Networks as Ordinary Differential Equations</title>
<link>https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</link>
<guid isPermaLink="true" >https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</guid>
<description>&lt;p&gt;Recently I found a paper being presented at NeurIPS this year, entitled &lt;a href=&quot;https://arxiv.org/abs/1806.07366&quot;&gt;Neural Ordinary Differential Equations&lt;/a&gt;, written by Ricky Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud from the University of Toronto. The core idea is that certain types of neural networks are analogous to a discretized differential equation, so maybe using off-the-shelf differential equation solvers will help get better results. This led me down a bit of a rabbit hole of papers that I found very interesting, so I thought I would share a short summary/view-from-30,000 feet on this idea.&lt;/p&gt;
&lt;p&gt;Typically, we think about neural networks as a series of discrete layers, each one taking in a previous state vector &lt;span class=&quot;math&quot;&gt;\( \mathbf{h}_n \)&lt;/span&gt; and producing a new state vector &lt;span class=&quot;math&quot;&gt;\( \mathbf{h}_{n+1} = F(\mathbf{h}_{n}) \)&lt;/span&gt;. Here, let's assume that each layer is the same width (e.g. &lt;span class=&quot;math&quot;&gt;\( \mathbf{h}_{n} \)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\( \mathbf{h}_{n+1} \)&lt;/span&gt; have the same dimension, for every &lt;span class=&quot;math&quot;&gt;\( n \)&lt;/span&gt;). Note that we don't particularly care about what &lt;span class=&quot;math&quot;&gt;\(F\)&lt;/span&gt; looks like, but typically it's something like &lt;span class=&quot;math&quot;&gt;\(F(x) = \sigma(\sum_{i}\theta_i x_i)\)&lt;/span&gt;, where &lt;span class=&quot;math&quot;&gt;\(\sigma\)&lt;/span&gt; is an activation function (e.g. &lt;a href=&quot;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&quot;&gt;relu&lt;/a&gt; or a &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_function&quot;&gt;sigmoid&lt;/a&gt;), and &lt;span class=&quot;math&quot;&gt;\(\theta\)&lt;/span&gt; is a vector of parameters we're learning. This core formulation has some problems - notably, adding more layers, while theoretically increasing the ability of the network to learn, can actually &lt;em&gt;decrease&lt;/em&gt; the accuracy of it, both in training and test results.&lt;/p&gt;
&lt;p&gt;This problem was adressed by &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;Deep Residual Learning&lt;/a&gt;, from Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun out of Microsoft Reasearch. The idea in a nutshell is to learn a function of the difference between layers: &lt;span class=&quot;math&quot;&gt;\( \mathbf{h}_{n+1} = F(\mathbf{h}_n) + \mathbf{h}_n \)&lt;/span&gt;. In the paper, they show this simple transformation in what you're learning allows the networks to keep improving as they add more layers. To me, this reminds me of &lt;a href=&quot;https://en.wikipedia.org/wiki/Delta_encoding&quot;&gt;delta encoding&lt;/a&gt;, in which you represent a stream of data as a series of changes from the previous state. This can make certain types of data much more suitable to compression (see, e.g. &lt;a href=&quot;https://gafferongames.com/post/snapshot_compression/&quot;&gt;this article&lt;/a&gt; from Glenn Fiedler on compressing physics data to send over a network). It makes some kind of sense that if delta encoding can make data easier to compress, it could also make it easier to represent for a neural network.&lt;/p&gt;
&lt;h3 id=&quot;eulers-method-and-residual-networks&quot;&gt;Euler's method and residual networks&lt;/h3&gt;
&lt;p&gt;But how do residual networks relate to differential equations? Suppose we have some constant that we'll call &lt;span class=&quot;math&quot;&gt;\( \Delta t \in \mathbb{R}\)&lt;/span&gt;. Then we can write the state update of our neural network as&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \begin{aligned} \mathbf{h}_{t+1} &amp;amp;= F(\mathbf{h}_t) + \mathbf{h}_t \\ \\ &amp;amp;= \frac{\Delta t}{\Delta t} F(\mathbf{h}_t) + \mathbf{h}_t \\ \\ &amp;amp;= \Delta t G(\mathbf{h}_t) + \mathbf{h}_t \end{aligned} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where now we're learning &lt;span class=&quot;math&quot;&gt;\( G(\mathbf{h}_t) = F(\mathbf{h}_t)/\Delta t \)&lt;/span&gt;. If you have experience with differential equations, this formulation looks very familiar - it is a single step of &lt;a href=&quot;https://en.wikipedia.org/wiki/Euler_method&quot;&gt;Euler's method&lt;/a&gt; for solving ordinary differential equations. It seems this was first noticed by Weinan E in &lt;a href=&quot;https://link.springer.com/article/10.1007/s40304-017-0103-z&quot;&gt;A proposal on Machine Learning via Dynamical Systems&lt;/a&gt;, and expanded upon by Yiping Lu et al. in &lt;a href=&quot;https://arxiv.org/pdf/1710.10121.pdf&quot;&gt;Beyond Finite Layer Neural Networks&lt;/a&gt;. However, Lu et al. continue to treat the network as a series of discrete steps, and use a discrete solver with fixed timesteps to come up with a novel neural network architecture. The reason for this is that we need to be able to train the networks, and it's not really clear how to &quot;learn&quot; a differential system. Chen, Rubanova, Bettencourt and Duvenaud solve this problem by using some clever math which enables them to compute the gradients they need for backpropagation.&lt;/p&gt;
&lt;h3 id=&quot;evaluating-odes&quot;&gt;Evaluating ODEs&lt;/h3&gt;
&lt;p&gt;Before we get to that, let's look at what we're trying to solve. If we consider a layer of our neural network to be doing a step of Euler's method, then we can model our system by the differential equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \frac{d \mathbf{h}(t)}{dt} = G(\mathbf{h}(t), t, \theta) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we've made explicit &lt;span class=&quot;math&quot;&gt;\(G\)&lt;/span&gt;'s dependency on &lt;span class=&quot;math&quot;&gt;\(t\)&lt;/span&gt;, as well as some parameters &lt;span class=&quot;math&quot;&gt;\(\theta\)&lt;/span&gt; which we will train on. In this formulation, the output of our &quot;network&quot; is the state &lt;span class=&quot;math&quot;&gt;\(\mathbf{h}(t_1)\)&lt;/span&gt; at some time &lt;span class=&quot;math&quot;&gt;\(t_1\)&lt;/span&gt;. Therefore, if we know how to describe the function &lt;span class=&quot;math&quot;&gt;\(G\)&lt;/span&gt;, we can use any number of off-the-shelf ODE solvers to evaluate the neural network.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \mathbf{h}(t_1) = \text{ODESolve}(\mathbf{h}(t_0), G, t_0, t_1, \theta) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There is a ton of research on different methods that can be used as our ODESolve function, but for now we'll treat it as a black box. What matters is that if you substitute in Euler's method, you get exactly the residual state update from above, with&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \text{ODESolve}(\mathbf{h}(t_0), G, t_0, t_1, \theta) = \mathbf{h}(t_0) + (t_1 - t_0)G(\mathbf{h}(t_0), t_0, \theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, we don't need to limit ourselves to Euler's method, and in fact will do much better if we use more modern approaches.&lt;/p&gt;
&lt;h3 id=&quot;training-the-beast&quot;&gt;Training the beast&lt;/h3&gt;
&lt;p&gt;So how to train it? Suppose we have a loss function&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ L(h(t_1)) = L(\text{ODESolve}(\mathbf{h}(t_0), G, t_0, t_1, \theta))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To optimize &lt;span class=&quot;math&quot;&gt;\(L\)&lt;/span&gt;, we require gradients with respect to its parameters &lt;span class=&quot;math&quot;&gt;\(\mathbf{h}(t)\)&lt;/span&gt; (the state of our system at time &lt;span class=&quot;math&quot;&gt;\(t\)&lt;/span&gt;), &lt;span class=&quot;math&quot;&gt;\(t\)&lt;/span&gt; (our &quot;time&quot; variable, which is sort of a continuous analog to depth), and &lt;span class=&quot;math&quot;&gt;\(\theta\)&lt;/span&gt;, our training parameters. &lt;a href=&quot;https://cs.stanford.edu/~ambrad/adjoint_tutorial.pdf&quot;&gt;The adjoint method&lt;/a&gt; describes a way to come up with this. The adjoint method is a neat trick which uses a simple substitution of variables to make solving certain linear systems easier. Part of the reason this paper grabbed my eye is because I've seen the adjoint method before, in a completely unrelated area: fluid simulation! In &lt;a href=&quot;http://grail.cs.washington.edu/projects/control/fluidAdjoint.pdf&quot;&gt;this&lt;/a&gt; paper from McNamara et al., they make controlling a fluid simulation easier by using the adjoint method to efficiently compute some gradients with respect to user controlled parameters. That certainly sounds similar to our problem.&lt;/p&gt;
&lt;p&gt;So what is the adjoint method? Suppose we have 2 known matrices &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(C\)&lt;/span&gt;, and a known vector &lt;span class=&quot;math&quot;&gt;\(\mathbf{u}\)&lt;/span&gt; and we would like to compute a product&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[\mathbf{u}^{\intercal}B \text{ such that } AB=C\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We could first solve the linear system to find the unknown matrix &lt;span class=&quot;math&quot;&gt;\(B\)&lt;/span&gt;, then compute the product, but solving the linear system could be expensive. Instead, let's solve a different problem. Let's find a vector &lt;span class=&quot;math&quot;&gt;\(\mathbf{v}\)&lt;/span&gt; and compute&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[\mathbf{v}^{\intercal}C \text{ such that } A^{\intercal}\mathbf{v}=\mathbf{u}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can show that these are in fact the same problem:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[\mathbf{v}^{\intercal}C = \mathbf{v}^{\intercal}AB = (A^{\intercal}\mathbf{v})^{\intercal}B = \mathbf{u}^{\intercal}B\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Through this transformation, we've reduced the problem from solving for a matrix, and reduced it to solving for a vector. This can be a big computational win! So how do we use it to train networks? I'm not going to go into the complete details here as it's slightly involved, but Appendix B in the paper has the complete derivation. In brief, we define the adjoint state as&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ a(t) = -\partial L / \partial \mathbf{h}(t) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And we can describe its dynamics via&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \frac{da(t)}{dt} = - a(t)^{\intercal} \frac{\partial G(\mathbf{h}(t), t, \theta)}{\partial \mathbf{h}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can compute the derivative of &lt;span class=&quot;math&quot;&gt;\(G\)&lt;/span&gt; with respect to &lt;span class=&quot;math&quot;&gt;\(\mathbf{h}\)&lt;/span&gt; already - we compute this gradient during backpropagation of traditional neural networks. With this, we can then compute &lt;span class=&quot;math&quot;&gt;\(a(t)\)&lt;/span&gt; by using another call to an ODE solver. There is one other derivative, &lt;span class=&quot;math&quot;&gt;\(dL/d\theta\)&lt;/span&gt;, that can be computed similarly. The paper shows that we can wrap up all of these ODE solves into a single call to an ODE solver, which computes all the necessary gradients for training the system.&lt;/p&gt;
&lt;h3 id=&quot;whats-the-point&quot;&gt;What's the point?&lt;/h3&gt;
&lt;p&gt;Why do we want to do this? According to the paper, we're able to train a model with much less memory, with fewer parameters, and we are able to backpropagate more efficiently. All of these seem like good things! Modern ODE solvers are also adaptive, and can do more work only when needed to get an accurate solution. In the paper they show an experiment where the number of function evaluations that the ODE solver does increases with the number of training epochs - effectively, the system can quickly reach a rough solution, then take more time to refine the training.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;img src=&quot;https://rkevingibson.github.io/img/ode_networks_1.png&quot; alt=&quot;illustration of an ode network compared to a residual network&quot; title=&quot;An example from the paper showing how using an ode solver can adaptively evaluate the function. Circles represent function evaluations.&quot;/&gt;An example from the paper showing how using an ode solver can adaptively evaluate the function. Circles represent function evaluations.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;I think the most interesting aspect is that treating our system like a continuous time model, allows us to predict continuous time systems. They show a way to take data which arrives at arbitrary times, rather than at fixed intervals, and they can predict the output at arbitrary future times. They test this on fairly simple synthetic data, predicting trajectories of spirals, but get really nice results. I definitely want to see more of this type of work in the future, on larger real-world problems, to see how it does. Being able to draw on the &amp;gt; 100 years of research in solving differential equations could be very useful for a young field like deep learning, and hey, I just find the math neat.&lt;/p&gt;
&lt;p class=&quot;back-to-posts&quot;&gt;&lt;a href=&quot;https://rkevingibson.github.io/blog&quot;&gt;Back to posts&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 17 Dec 2018 21:58:37 +0000</pubDate>
<dc:creator>agronaut</dc:creator>
<og:title>Neural networks as Ordinary Differential Equations</og:title>
<og:description>Recently I found a paper being presented at NeurIPS this year, entitled Neural Ordinary Differential Equations, written by Ricky Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud from the University of Toronto. The core idea is that certain types of neural networks are analogous to a discretized differential equation, so maybe using off-the-shelf differential equation solvers will help get better results. This led me down a bit of a rabbit hole of papers that I found very interesting, so I thought I would share a short summary/view-from-30,000 feet on this idea.</og:description>
<og:type>article</og:type>
<og:url>https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</og:url>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</dc:identifier>
</item>
<item>
<title>Show HN: Fancy fonts you can use almost anywhere</title>
<link>https://beautifuldingbats.com/hey-howd-you-do-that</link>
<guid isPermaLink="true" >https://beautifuldingbats.com/hey-howd-you-do-that</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://beautifuldingbats.com/hey-howd-you-do-that&quot;&gt;https://beautifuldingbats.com/hey-howd-you-do-that&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=18701772&quot;&gt;https://news.ycombinator.com/item?id=18701772&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 221&lt;/p&gt;
&lt;p&gt;# Comments: 156&lt;/p&gt;
</description>
<pubDate>Mon, 17 Dec 2018 20:14:12 +0000</pubDate>
<dc:creator>shadowfaxRodeo</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://beautifuldingbats.com/hey-howd-you-do-that</dc:identifier>
</item>
<item>
<title>MIPS Goes Open Source</title>
<link>https://www.eetimes.com/document.asp?doc_id=1334087</link>
<guid isPermaLink="true" >https://www.eetimes.com/document.asp?doc_id=1334087</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.eetimes.com/document.asp?doc_id=1334087&quot;&gt;https://www.eetimes.com/document.asp?doc_id=1334087&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=18701145&quot;&gt;https://news.ycombinator.com/item?id=18701145&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 476&lt;/p&gt;
&lt;p&gt;# Comments: 139&lt;/p&gt;
</description>
<pubDate>Mon, 17 Dec 2018 18:58:26 +0000</pubDate>
<dc:creator>walterbell</dc:creator>
<dc:identifier>https://www.eetimes.com/document.asp?doc_id=1334087</dc:identifier>
</item>
<item>
<title>Clojure 1.10 release</title>
<link>https://clojure.org/news/2018/12/17/clojure110</link>
<guid isPermaLink="true" >https://clojure.org/news/2018/12/17/clojure110</guid>
<description>&lt;div class=&quot;paragraph&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Clojure 1.10 focuses on two major areas: improved error reporting and Java compatibility.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot; readability=&quot;14.646680942184&quot;&gt;
&lt;p&gt;Error reporting at the REPL now &lt;a href=&quot;https://clojure.org/reference/repl_and_main#_error_printing&quot;&gt;categorizes&lt;/a&gt; errors based on their phase of execution (read, macroexpand, compile, etc). Errors carry additional information about location and context as data, and present phase-specific error messages with better location reporting. This functionality is built into the clojure.main REPL, but the functionality is also available to other REPLs and tools with the ability to use and/or modify the data to produce better error messages.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;Clojure 1.10 now requires Java 8 or above and has been updated particularly for compatibility with Java 8 and Java 11. Changes included bytecode-related bug fixes, removed use of deprecated APIs, and updates related to the module system introduced in Java 9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;See the &lt;a href=&quot;https://github.com/clojure/clojure/blob/master/changes.md#changes-to-clojure-in-version-110&quot;&gt;change log&lt;/a&gt; for a complete list of all fixes, enhancements, and new features in Clojure 1.10.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Mon, 17 Dec 2018 17:42:57 +0000</pubDate>
<dc:creator>finalfantasia</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://clojure.org/news/2018/12/17/clojure110</dc:identifier>
</item>
<item>
<title>Google’s Secret China Project “Effectively Ended” After Internal Confrontation</title>
<link>https://theintercept.com/2018/12/17/google-china-censored-search-engine-2/</link>
<guid isPermaLink="true" >https://theintercept.com/2018/12/17/google-china-censored-search-engine-2/</guid>
<description>&lt;div data-reactid=&quot;200&quot; readability=&quot;29.779208831647&quot;&gt;
&lt;p&gt;&lt;u&gt;Google has been&lt;/u&gt; forced to shut down a data analysis system it was using to develop a censored search engine for China after members of the company’s privacy team raised internal complaints that it had been kept secret from them, The Intercept has learned.&lt;/p&gt;
&lt;p&gt;The internal rift over the system has had massive ramifications, effectively ending work on the censored search engine, known as Dragonfly, according to two sources familiar with the plans. The incident represents a major blow to top Google executives, including CEO Sundar Pichai, who have over the last two years made the China project one of their main priorities.&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;The dispute began in mid-August, when the The Intercept &lt;a href=&quot;https://theintercept.com/2018/08/08/google-censorship-china-blacklist/&quot;&gt;revealed&lt;/a&gt; that Google employees working on Dragonfly had been using a Beijing-based website to help develop blacklists for the censored search engine, which was designed to block out broad categories of information related to democracy, human rights, and peaceful protest, in accordance with strict rules on censorship in China that are enforced by the country’s authoritarian Communist Party government.&lt;/p&gt;

&lt;/div&gt;
&lt;div data-reactid=&quot;202&quot; readability=&quot;74.344305120167&quot;&gt;
&lt;p lang=&quot;en-US&quot;&gt;The Beijing-based website, 265.com, is a Chinese-language web directory service that claims to be “China’s most used homepage.” Google purchased the site in 2008 from Cai Wensheng, a billionaire Chinese entrepreneur. 265.com provides its Chinese visitors with news updates, information about financial markets, horoscopes, and advertisements for cheap flights and hotels. It also has a function that allows people to search for websites, images, and videos. However, search queries entered on 265.com are redirected to Baidu, the most popular search engine in China and Google’s main competitor in the country. As The Intercept &lt;a href=&quot;https://theintercept.com/2018/08/08/google-censorship-china-blacklist/&quot;&gt;reported&lt;/a&gt; in August, it appears that Google has used 265.com as a honeypot for market research, storing information about Chinese users’ searches before sending them along to Baidu.&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;According to two Google sources, engineers working on Dragonfly obtained large datasets showing queries that Chinese people were entering into the 265.com search engine. At least one of the engineers obtained a key needed to access an “application programming interface,” or API, associated with 265.com, and used it to harvest search data from the site. Members of Google’s privacy team, however, were kept in the dark about the use of 265.com.&lt;/p&gt;

&lt;blockquote class=&quot;stylized pull-right&quot; data-shortcode-type=&quot;pullquote&quot; data-pull=&quot;right&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;Several groups of engineers have now been moved off of Dragonfly completely and told to shift their attention away from China.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span lang=&quot;en-US&quot;&gt;The engineers used the data they pulled from 265.com to learn about the kinds of things that people located in mainland China routinely search for in Mandarin. This helped them to build a prototype of Dragonfly. The engineers used the sample queries from 265.com, for instance, to review lists of websites Chinese people would see if they typed the same word or phrase into Google. They then used a&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;tool they called “BeaconTower” to check whether&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;any websites in the Google search results would be&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;blocked by&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;China’s internet censorship system, known as the&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;Great Firewall.&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;Through this process, the engineers compiled a list of thousands of banned websites, which they integrated into the Dragonfly search platform so that it would purge links to websites prohibited in China, such as those&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;of&lt;/span&gt; the online encyclopedia Wikipedia and British news broadcaster BBC.&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;Under normal company protocol, analysis of people’s search queries is subject to tight constraints and should be reviewed by the company’s privacy staff, whose job is to safeguard user rights. But the privacy team only found out about the 265.com data access after The Intercept revealed it, and were “really pissed,” according to one Google source. Members of the privacy team confronted the executives responsible for managing Dragonfly. Following a series of discussions, two sources said, Google engineers were told that they were no longer permitted to continue using the 265.com data to help develop Dragonfly, which has since had severe consequences for the project.&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;“The 265 data was integral to Dragonfly,” said one source. “Access to the data has been suspended now, which has stopped progress.”&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;In recent weeks, teams working on Dragonfly have been told to use different datasets for their work. They are no longer gathering search queries from mainland China and are instead now studying “global Chinese” queries that are entered into Google from people living in countries such as the United States and Malaysia; those queries are qualitatively different from searches originating from within China itself, making it virtually impossible for the Dragonfly team to hone the accuracy of results. Significantly, several groups of engineers have now been moved off of Dragonfly completely, and told to shift their attention away from China to instead work on projects related to India, Indonesia, Russia, the Middle East and Brazil.&lt;/p&gt;

&lt;/div&gt;
&lt;div class=&quot;Newsletter-shortcode Newsletter-shortcode-layout-full&quot; data-reactid=&quot;203&quot;&gt;
&lt;div class=&quot;Newsletter-shortcode-container&quot; data-reactid=&quot;205&quot;&gt;
&lt;h3 class=&quot;Newsletter-shortcode-headline&quot; data-reactid=&quot;206&quot;&gt;Join Our Newsletter&lt;/h3&gt;
&lt;h3 class=&quot;Newsletter-shortcode-subhead&quot; data-reactid=&quot;207&quot;&gt;Original reporting. Fearless journalism. Delivered to you.&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;Newsletter-shortcode-link-cta&quot; data-reactid=&quot;209&quot;&gt;I’m in&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div data-reactid=&quot;212&quot; readability=&quot;38.081902893575&quot;&gt;
&lt;p lang=&quot;en-US&quot;&gt;Records show that 265.com is still hosted on Google servers, but its physical address is listed under the name of the “Beijing Guxiang Information and Technology Co.,” which has an office space on the third floor of a tower building in northwest Beijing’s Haidian district. 265.com is operated as a Google subsidiary, but unlike most Google-owned websites — such as YouTube and Google.com — it is not blocked in China and can be freely accessed by people in the country using any standard internet browser.&lt;/p&gt;
&lt;p&gt;&lt;span lang=&quot;en-US&quot;&gt;The internal dispute at Google over the 265.com data access is not the first time important information related to Dragonfly has been withheld from the company’s privacy team. The Intercept&lt;/span&gt; &lt;a href=&quot;https://theintercept.com/2018/11/29/google-china-censored-search/&quot;&gt;reported&lt;/a&gt; &lt;span lang=&quot;en-US&quot;&gt;in November that privacy and security employees working on the project had been shut out of key meetings and felt that senior executives had sidelined them. Yonatan Zunger,&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;formerly&lt;/span&gt; a 14-year veteran of Google and one of the leading engineers at the company, &lt;span lang=&quot;en-US&quot;&gt;worked on Dragonfly for several months last year and said the project was shrouded in extreme secrecy and handled in a “highly unusual” way from the outset. Scott Beaumont, Google’s leader in China and a key architect of the Dragonfly project, “&lt;/span&gt;&lt;span lang=&quot;en-US&quot;&gt;did not feel that the security, privacy, and legal teams should be able to question his product decisions,”&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;according to Zunger,&lt;/span&gt; &lt;span lang=&quot;en-US&quot;&gt;“and maintained an openly adversarial relationship with them — quite outside the Google norm.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Last week, Pichai, Google’s CEO, appeared before Congress, where he &lt;a href=&quot;https://theintercept.com/2018/12/11/google-congressional-hearing/&quot;&gt;faced questions&lt;/a&gt; on Dragonfly. Pichai stated that “right now” there were no plans to launch the search engine, though refused to rule it out in the future. Google had originally &lt;a href=&quot;https://theintercept.com/2018/10/09/google-china-censored-search-engine/&quot;&gt;aimed&lt;/a&gt; to launch Dragonfly between January and April 2019. Leaks about the plan and the extraordinary backlash that ensued both &lt;a href=&quot;https://theintercept.com/2018/11/27/hundreds-of-google-employees-tell-bosses-to-cancel-censored-search-amid-worldwide-protests/&quot;&gt;internally&lt;/a&gt; and &lt;a href=&quot;https://theintercept.com/2018/12/10/rights-groups-pressure-google-on-china-censorship-ahead-of-congressional-hearing/&quot;&gt;externally&lt;/a&gt; appear to have forced company executives to shelve it at least in the short term, two sources familiar with the project said.&lt;/p&gt;
&lt;p lang=&quot;en-US&quot;&gt;Google did not respond to requests for comment.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Mon, 17 Dec 2018 17:35:31 +0000</pubDate>
<dc:creator>uptown</dc:creator>
<og:url>https://theintercept.com/2018/12/17/google-china-censored-search-engine-2/</og:url>
<og:description>Google reassigned several groups of engineers away from a planned censored search engine after a rift over its use of real internet queries in China for testing.</og:description>
<og:image>https://theintercept.imgix.net/wp-uploads/sites/1/2018/12/GettyImages-1040611142-1544738780-e1544738960291.jpg?auto=compress%2Cformat&amp;q=90&amp;fit=crop&amp;w=1200&amp;h=800</og:image>
<og:type>article</og:type>
<og:title>Google’s Secret China Project “Effectively Ended” After Internal Confrontation</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://theintercept.com/2018/12/17/google-china-censored-search-engine-2/</dc:identifier>
</item>
<item>
<title>Inventor May Have Cured Motion Sickness Without Drugs</title>
<link>https://www.defenseone.com/technology/2018/11/inventor-may-have-cured-motion-sickness-without-drugs-and-could-mean-lot-us-military/152960/</link>
<guid isPermaLink="true" >https://www.defenseone.com/technology/2018/11/inventor-may-have-cured-motion-sickness-without-drugs-and-could-mean-lot-us-military/152960/</guid>
<description>&lt;p&gt;&lt;span class=&quot;d1-article-subhead-inner-a&quot;&gt;&lt;span class=&quot;d1-article-subhead-inner-b&quot;&gt;&lt;span class=&quot;d1-article-subhead-inner-c&quot;&gt;One manufacturer of virtual-reality trainers has already begun including the devices in its simulators.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;div readability=&quot;149.25731095345&quot;&gt;
&lt;p&gt;&lt;strong&gt;An inventor may have discovered a non-&lt;/strong&gt;pharmaceutical cure for car sickness that could revolutionize the way people experience everything from travel to the newest virtual-reality headsets. That, in turn, could affect how the military trains, fights, and navigates.&lt;/p&gt;
&lt;p&gt;Just like civilians, troops get motion-sick. A 2009 study by the &lt;a href=&quot;https://www.researchgate.net/publication/235208813_Evaluation_of_Several_Common_Antimotion_Sickness_Medications_and_Recommendations_Concerning_Their_Potential_Usefulness_During_Special_Operations&quot;&gt;Naval Aerospace Medical Research Laboratory&lt;/a&gt; found that more than half of soldiers got sick while riding in Army vehicles. Roughly 25 percent of military personnel got sick on “moderate seas” and 70 percent on “rough seas.” In the air, as many as 50 percent of personnel get airsick; even 64 percent of parachutists reported episodes.&lt;/p&gt;
&lt;p&gt;To treat symptoms, troops typically take a drug called &lt;a href=&quot;https://www.rxlist.com/consumer_scopolamine/drugs-condition.htm&quot;&gt;scopolamine&lt;/a&gt;. It has serious side effects, most notably drowsiness, so soldiers often take it with an amphetamine that carries its own downsides and side effects. It’s like being on uppers and downers at once, which makes for a fatiguing Friday night, much less a war.&lt;/p&gt;
&lt;p&gt;The military’s problems with motion sickness will worsen considerably as more and more training is conducted in virtual reality.&lt;/p&gt;
&lt;div class=&quot;d1-article-left-rail-subscribe-container&quot; readability=&quot;36&quot;&gt;
&lt;p class=&quot;d1-article-sidebar-subscribe-title&quot;&gt;&lt;span class=&quot;d1-article-sidebar-subscribe-title-text&quot;&gt;Subscribe&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;d1-article-sidebar-subscribe-msg&quot;&gt;&lt;em&gt;Receive daily email updates:&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;d1-article-sidebar-subscribe-msg&quot;&gt;Subscribe to the Defense One daily.&lt;/p&gt;
&lt;p class=&quot;d1-article-sidebar-subscribe-msg&quot;&gt;Be the first to receive updates.&lt;/p&gt;

&lt;/div&gt;
&lt;h3 class=&quot;wysiwyg&quot;&gt;Related: &lt;a href=&quot;https://www.defenseone.com/technology/2018/09/darpa-funded-work-may-help-troops-see-around-corners/151399/?oref=d1-related-article&quot;&gt;&lt;span class=&quot;caps&quot;&gt;DARPA&lt;/span&gt;-Funded Work May Help Troops See Around Corners&lt;/a&gt;&lt;/h3&gt;
&lt;h3 class=&quot;wysiwyg&quot;&gt;Related: &lt;a href=&quot;https://www.defenseone.com/technology/2018/09/spacex-well-consider-launching-space-weapons-if-asked/151328/?oref=d1-related-article&quot;&gt;SpaceX: We’ll Consider Launching Space Weapons If Asked&lt;/a&gt;&lt;/h3&gt;
&lt;h3 class=&quot;wysiwyg&quot;&gt;Related: &lt;a href=&quot;https://www.defenseone.com/technology/2018/09/retired-marine-four-star-patents-jet-killing-drone-boat/151327/?oref=d1-related-article&quot;&gt;Retired Marine Four-Star Patents Jet-Killing Drone Boat&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;“The availability of immersive learning environments like virtual-augmented-mixed reality afforded by commercial off-the-shelf technology fosters has the potential to create the paradigm shift necessary to deliver the most ready force ever known,” said Lt. Col. Matthew Strohmeyer, the 560th Flying Training Squadron commander. Yet &lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; training, in particular, can make troops sick. “Though we have made great strides in understanding the true causes of air sickness, from a cellular physiology perspective, much is still to be learned, especially when it comes to cyber sickness.”&lt;/p&gt;
&lt;p&gt;The Air Force Research Lab is currently looking at the effects of motion sickness among a small group of “future instructor pilots” that are training with a new syllabus that uses virtual reality. The research brings in experts from the South Dakota School of Mines and Technology as well as physiologists and small businesses. “Our findings will further inform safety countermeasures to ensure aviators can meet the demand of any physiological threat that presents itself,” said Strohmeyer.&lt;/p&gt;
&lt;p&gt;The Air Force isn’t just looking to use &lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; for pilots. They’ve &lt;a href=&quot;https://admin.govexec.com/media/vrmc-usaf_press_release_9-20-18_final_(1).pdf&quot;&gt;contracted&lt;/a&gt; with a Portland, Oregon-based company called &lt;a href=&quot;https://vrmotioncorp.com/about-us/&quot;&gt;&lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; Motion&lt;/a&gt; to train truck drivers. “What we’ve learned is that the current method for training hasn’t been updated for decades,” said Keith Maher, the company’s founder and &lt;span class=&quot;caps&quot;&gt;CEO&lt;/span&gt;. “Driving a large combat vehicle like a Humvee, or an up-armored Humvee on public roads, is actually counter to what they [the Humvees] are designed to do. On public roads, there will be pedestrians and small vehicles. The large blind spots that you have in a Humvee are something you need to train for…With our virtual reality technology we can recreate high-hazard situations whenever we want.”&lt;/p&gt;
&lt;p&gt;But as many gamers are today discovering, &lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; can have big motion-sickness effects.&lt;/p&gt;
&lt;p&gt;“Historically, we’ve seen about a 20 to 30 percent discomfort level” with &lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; training, Maher said. “That’s a big number for us if we want our product to go out and change the lives of millions of people.”&lt;/p&gt;
&lt;p&gt;Enter a young inventor named Samuel Owen, who has developed a prototype device called the &lt;a href=&quot;https://admin.govexec.com/media/otolith_pamphlet_.pdf&quot;&gt;OtoTech&lt;/a&gt;, from &lt;a href=&quot;https://otolithlabs.com/&quot;&gt;Otolith Labs&lt;/a&gt;. Worn on a headband behind the ear, it uses subtle vibrations to change the way the brain computes the fact that the body that it’s attached to is in motion. Early tests show it relieves motion sickness without the side effects of drugs, Owen said, though he admits the science is so young that it’s not clear just how.&lt;/p&gt;
&lt;p&gt;The vibrations emanating from the OtoTech gently target two of the four fibers that carry data about body motion to the brain via a system of inner ear sensors called the &lt;a href=&quot;https://en.wikipedia.org/wiki/Vestibulocochlear_nerve&quot;&gt;vestibulocochlear nerve&lt;/a&gt;. “Two [of the four vestibulocochlear nerve fibers] go to the brain, two go to your reflexes,” Owen said. The trick is to affect the former and not the latter.&lt;/p&gt;
&lt;p&gt;“The working hypothesis is that [the vibration] causes a chaotic and noninformative stimulus to go to the brain. Somewhere, probably the cerebellum, there’s a filtering mechanism that filters out noninformative sensed information. It’s the reason you don’t notice the shirt on your back right now,” he said.&lt;/p&gt;
&lt;p&gt;In other words, while you remain consciously aware that you’re moving, the balance portion of your brain stops noticing the fact; the data has been drowned out in white noise from the device.&lt;/p&gt;
&lt;p&gt;So far, he says, initial testing shows that it works to prevent motion sickness without affecting balance, vision, alertness, or anything else it’s not supposed to.&lt;/p&gt;
&lt;p&gt;Researchers at Jaguar Land Rover are conducting double-blind trials with the device, moving toward publication, he says. Medical researchers at Coventry University in the &lt;span class=&quot;caps&quot;&gt;U.K.&lt;/span&gt; and the University of Miami are looking at therapeutic applications related to treating vertigo.&lt;/p&gt;
&lt;p&gt;Owen says that he has initially marketed the device to vertigo sufferers, and not yet to the military, or even the motion-sickness market.  But Maher has begun to incorporate Owen’s device into his &lt;span class=&quot;caps&quot;&gt;VR&lt;/span&gt; trainers.&lt;/p&gt;
&lt;p&gt;“We noticed that it would improve the overall virtual reality experience,” said Maher. “We’ve started to use it in our military devices. The initial reaction is, it looks unusual, but afterwards, people don’t event notice.”&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Related podcast: &lt;/strong&gt;&lt;/em&gt;&lt;img class=&quot;tombstone&quot; src=&quot;https://cdn.defenseone.com/a/defenseone/img/article-end.png&quot;/&gt;&lt;/p&gt;

&lt;/div&gt;</description>
<pubDate>Mon, 17 Dec 2018 17:33:22 +0000</pubDate>
<dc:creator>aresant</dc:creator>
<og:type>article</og:type>
<og:title>This Inventor May Have Cured Motion Sickness Without Drugs. And That Could Mean a Lot to the US Military</og:title>
<og:url>https://www.defenseone.com/technology/2018/11/inventor-may-have-cured-motion-sickness-without-drugs-and-could-mean-lot-us-military/152960/</og:url>
<og:description>One manufacturer of virtual-reality trainers has already begun including the devices in its simulators.</og:description>
<og:image>https://cdn.defenseone.com/media/img/upload/2018/11/20/IMG_3725_2/open-graph.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.defenseone.com/technology/2018/11/inventor-may-have-cured-motion-sickness-without-drugs-and-could-mean-lot-us-military/152960/</dc:identifier>
</item>
<item>
<title>Robinhood Checking Moved Fast and Broke</title>
<link>https://www.bloomberg.com/opinion/articles/2018-12-17/robinhood-checking-moved-fast-and-broke</link>
<guid isPermaLink="true" >https://www.bloomberg.com/opinion/articles/2018-12-17/robinhood-checking-moved-fast-and-broke</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/opinion/articles/2018-12-17/robinhood-checking-moved-fast-and-broke&quot;&gt;https://www.bloomberg.com/opinion/articles/2018-12-17/robinhood-checking-moved-fast-and-broke&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=18699995&quot;&gt;https://news.ycombinator.com/item?id=18699995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 447&lt;/p&gt;
&lt;p&gt;# Comments: 245&lt;/p&gt;
</description>
<pubDate>Mon, 17 Dec 2018 16:38:48 +0000</pubDate>
<dc:creator>kgwgk</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=31cfad80-02de-11e9-8308-39d4596bb60e&amp;url=L29waW5pb24vYXJ0aWNsZXMvMjAxOC0xMi0xNy9yb2Jpbmhvb2QtY2hlY2tpbmctbW92ZWQtZmFzdC1hbmQtYnJva2U=</dc:identifier>
</item>
</channel>
</rss>