<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Carl Malamud accused of ‘terrorism’ for putting legal materials online</title>
<link>https://www.nytimes.com/2019/05/13/us/politics/georgia-official-code-copyright.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2019/05/13/us/politics/georgia-official-code-copyright.html</guid>
<description>&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;WASHINGTON — Carl Malamud believes in open access to government records, and he has spent more than a decade putting them online. You might think states would welcome the help.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;But when Mr. Malamud’s group posted the Official Code of Georgia Annotated, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://law.resource.org/pub/us/code/ga/pro_v_georgia/gov.uscourts.gand.218354.1.0.pdf&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;the state sued for copyright infringement&lt;/a&gt;. Providing public access to the state’s laws and related legal materials, Georgia’s lawyers said, was part of a “strategy of terrorism.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;A federal appeals court &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;http://media.ca11.uscourts.gov/opinions/pub/files/201711589.pdf&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;ruled against the state&lt;/a&gt;, which has &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.supremecourt.gov/DocketPDF/18/18-1150/90326/20190301141255519_Georgia%20Certiorari%20Petition%20-%20to%20Printer.pdf&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;asked the Supreme Court to step in&lt;/a&gt;. On Friday, in an unusual move, Mr. Malamud’s group, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://public.resource.org/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Public.Resource.Org&lt;/a&gt;, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.supremecourt.gov/DocketPDF/18/18-1150/99349/20190510172703141_GA%20Code%20BIO%20Final.pdf&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;also urged the court to hear the dispute&lt;/a&gt;, saying that the question of who owns the law is an urgent one, as about 20 other states have claimed that parts of similar annotated codes are copyrighted.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The issue, the group said, is whether citizens can have access to “the raw materials of our democracy.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The case, Georgia v. Public.Resource.Org, No. 18-1150, concerns the 54 volumes of the &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://store.lexisnexis.com/products/official-code-of-georgia-annotated-skuSKU6647/details&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Official Code of Georgia Annotated&lt;/a&gt;, which contain state statutes and related materials.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The state, through a legal publisher, makes the statutes themselves available online, and it has said it does not object to Mr. Malamud doing the same thing. But people who want to see other materials in the books, the state says, must pay the publisher.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;This is part of a disturbing trend, according to &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://osf.io/preprints/lawarxiv/xnbcp/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;a new law review article&lt;/a&gt;, “Who Owns the Law? Why We Must Restore Public Ownership of Legal Publishing,” by &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://law.mercer.edu/faculty/directory/leslie-street.cfm&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Leslie Street&lt;/a&gt;, a law professor and librarian at Mercer University in Macon, Ga., and &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://library.duke.edu/about/directory/staff/7717&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;David Hansen&lt;/a&gt;, a librarian at Duke. It will be published in &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://digitalcommons.law.uga.edu/jipl/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The Journal of Intellectual Property Law&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;States have struck deals with legal publishers, the article said, that have effectively privatized the law. “Publishers now use powerful legal tools to control who has access to the text of the law, how much they must pay and under what terms,” the article said.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Mr. Malamud said those arrangements have complicated his efforts. “When I started Public Resource,” he said, “I thought our mission would be a focus on making the laws easier to use and read, but because of a buzz saw of opposition we have spent much of our time fighting back takedown notices and lawsuits.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;There is no question that judicial opinions cannot be copyrighted. The last time the Supreme Court &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.law.cornell.edu/supremecourt/text/128/244&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;addressed the matter&lt;/a&gt;, in 1888, it ruled that “the whole work done by the judges constitutes the authentic exposition and interpretation of the law, which, binding every citizen, is free for publication to all.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;&lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.law.cornell.edu/copyright/cases/293_F3d_791.htm&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Lower courts&lt;/a&gt; have said the same thing about statutes. But the status of other sorts of legal materials has not been definitively resolved. In the Georgia case, the question is whether annotations commissioned and approved by the state may be copyrighted.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The annotations include descriptions of judicial decisions interpreting the statutes. Only a very bad lawyer would fail to consult them in determining the meaning of a statute.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;For instance, Georgia has a law on the books &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://codes.findlaw.com/ga/title-16-crimes-and-offenses/ga-code-sect-16-6-2.html&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;making sodomy a crime&lt;/a&gt;. An annotation tells the reader that &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://caselaw.findlaw.com/ga-supreme-court/1443153.html&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;the law has been held unconstitutional&lt;/a&gt; “insofar as it criminalizes the performance of private, unforced, noncommercial acts of sexual intimacy between persons legally able to consent.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Professor Street said she tells her law students to be sure to consult the annotations in Georgia’s official code.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“When you go to a statute, you see the language of the statute, but that doesn’t necessarily tell you the meaning,” she said. “You go to the annotations, which leads you to the court decisions, where the judges actually tell you what the words mean.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;In ruling for Mr. Malamud, the appeals court made a similar point.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“The annotations clearly have authoritative weight in explicating and establishing the meaning and effect of Georgia’s laws,” &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;http://www.ca11.uscourts.gov/judges/hon-stanley-marcus&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Judge Stanley Marcus&lt;/a&gt; wrote for a unanimous three-judge panel of the court, the United States Court of Appeals for the 11th Circuit, in Atlanta. “Georgia’s courts have cited to the annotations as authoritative sources on statutory meaning and legislative intent.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Still, the annotations are not themselves law, Judge Marcus wrote, making the case a hard one. But he concluded that the annotations were “sufficiently lawlike” that they could not be copyrighted.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The annotations were prepared by lawyers working for LexisNexis as part of a financial arrangement with the state. Georgia holds the copyright to the annotations, but the company has the right to sell them while paying the state a royalty.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The state says this is a sensible cost-saving measure, “minimizing burdens on taxpayers” by sparing them from paying for the preparation of annotations.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Professor Street said there was no good reason for the state to outsource the task. “States are privatizing the functions of government,” she said. “But the incentives are different for a private company when it comes to publishing the law than it is for a state government.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;I asked Mr. Malamud why he had urged the Supreme Court to hear his case even though he had won in the appeals court.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“Repeating the laws of our country should not be considered a crime,” he said. “I would like the Supreme Court to tell us which laws we are allowed to speak.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Mon, 13 May 2019 11:09:24 +0000</pubDate>
<dc:creator>pseudolus</dc:creator>
<og:url>https://www.nytimes.com/2019/05/13/us/politics/georgia-official-code-copyright.html</og:url>
<og:type>article</og:type>
<og:title>Accused of ‘Terrorism’ for Putting Legal Materials Online</og:title>
<og:image>https://static01.nyt.com/images/2019/05/12/us/13DC-BAR/13DC-BAR-facebookJumbo.jpg</og:image>
<og:description>After Carl Malamud posted Georgia’s annotated laws, the state sued for copyright infringement. Both sides have asked the Supreme Court to step in.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2019/05/13/us/politics/georgia-official-code-copyright.html</dc:identifier>
</item>
<item>
<title>China is blocking all language editions of Wikipedia</title>
<link>https://ooni.torproject.org/post/2019-china-wikipedia-blocking/</link>
<guid isPermaLink="true" >https://ooni.torproject.org/post/2019-china-wikipedia-blocking/</guid>
<description>&lt;nav&gt;&lt;div class=&quot;col-1&quot;&gt;&lt;a href=&quot;https://ooni.torproject.org/&quot;&gt;&lt;img class=&quot;logo&quot; src=&quot;https://ooni.torproject.org/images/ooni-header-mascot.png&quot; width=&quot;25&quot; height=&quot;25&quot;/&gt;&lt;img class=&quot;wordmark&quot; src=&quot;https://ooni.torproject.org/images/wordmark.png&quot; alt=&quot;OONI&quot; height=&quot;14&quot; width=&quot;53&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/nav&gt;
&lt;p&gt;&lt;span class=&quot;author&quot;&gt;iyouport.org, Sukhbir Singh (Open Web Fellow, Mozilla Foundation), Arturo Filastò (OONI), Maria Xynou (OONI)&lt;/span&gt; &lt;span class=&quot;date&quot;&gt;2019-05-04&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking.zh/&quot;&gt;translation: 中国封锁了所有语言版本的维基百科&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;China recently started blocking all language editions of Wikipedia. Previously, the blocking was limited to the Chinese language edition of Wikipedia (zh.wikipedia.org), but has now expanded to include all &lt;code&gt;*.wikipedia.org&lt;/code&gt; language editions.&lt;/p&gt;
&lt;p&gt;In this post, we share &lt;a href=&quot;https://api.ooni.io/files/by_country/CN&quot;&gt;OONI network measurement data&lt;/a&gt; on the blocking of Wikipedia in China. We found that all wikipedia.org sub-domains are blocked in China by means of DNS injection and SNI filtering.&lt;/p&gt;
&lt;h2 id=&quot;dns-injection&quot;&gt;DNS injection&lt;/h2&gt;
&lt;p&gt;Through the use of &lt;a href=&quot;https://ooni.io/install/&quot;&gt;OONI Probe&lt;/a&gt;, Wikipedia domains have been &lt;a href=&quot;https://api.ooni.io/files/by_country/CN&quot;&gt;tested&lt;/a&gt; from multiple local vantage points in China since 2015. Most measurements have been collected from China Telecom (AS4134).  &lt;/p&gt;
&lt;p&gt;OONI’s &lt;a href=&quot;https://ooni.io/nettest/web-connectivity/&quot;&gt;Web Connectivity test&lt;/a&gt; (available in the OONI Probe apps) is designed to measure the TCP/IP, HTTP, and DNS blocking of websites. &lt;a href=&quot;https://api.ooni.io/files/by_country/CN&quot;&gt;Network measurement data&lt;/a&gt; collected through this test has shown that most Wikipedia language editions were previously accessible in China, except for the Chinese edition, which has &lt;a href=&quot;https://www.theepochtimes.com/china-now-blocked-from-accessing-wikipedia_1384917.html&quot;&gt;reportedly been blocked&lt;/a&gt; since 19th May 2015.&lt;/p&gt;
&lt;p&gt;OONI data shows that China Telecom (AS4134) has been &lt;a href=&quot;https://explorer.ooni.io/measurement/20161110T035949Z_AS4134_oAjg1SM4bjI5yI2D9yhu8Rfq830QL5avcxPg8LmhEAN1u0pvYq?input=http:%2F%2Fzh.wikipedia.org%2Fwiki%2Fwikipedia:%25e9%25a6%2596%25e9%25a1%25b5&quot;&gt;blocking zh.wikipedia.org since at least the 10th November 2016&lt;/a&gt; (previous OONI measurements show that zh.wikipedia.org was &lt;a href=&quot;http://api.ooni.io/files/download/2015-03-04/20150304T232111Z-CN-AS4808-http_requests-no_report_id-0.1.0-probe.yaml&quot;&gt;accessible in March 2015&lt;/a&gt; on that network).  &lt;/p&gt;
&lt;p&gt;The following chart, based on &lt;a href=&quot;https://api.ooni.io/files/by_country/CN&quot;&gt;OONI data&lt;/a&gt;, illustrates that multiple language editions of Wikipedia have been blocked in China as of April 2019.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking/ooni-china-blocks-wikipedia.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt;Blocking of Wikipedia domains in China, Open Observatory of Network Interference (OONI) data: China, &lt;a href=&quot;https://api.ooni.io/files/by_country/CN&quot;&gt;https://api.ooni.io/files/by_country/CN&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Our analysis of OONI measurements, used to produce the above chart, is available &lt;strong&gt;&lt;a href=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking/20190502-china-wikipedia.csv&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;OONI measurements show that many of these Wikipedia domains were &lt;a href=&quot;https://explorer.ooni.io/measurement/20190218T081331Z_AS4134_2CoUgy8tf1A7DF2JZNghiXWuK7ndJRnTVHsnWTTVjkJFDb1mcd?input=https:%2F%2Fen.wikipedia.org%2Fwiki%2Fthe_holocaust&quot;&gt;previously accessible&lt;/a&gt;, but all measurements collected from &lt;a href=&quot;https://explorer.ooni.io/measurement/20190425T070917Z_AS4134_45hNnx6LkceBitzcVB1oAGfpHyJuKFKR7NkLU5XTCdz2JKVPhy?input=https:%2F%2Fen.wikipedia.org&quot;&gt;25th April 2019 onwards&lt;/a&gt; &lt;a href=&quot;https://explorer.ooni.io/measurement/20190425T070917Z_AS4134_45hNnx6LkceBitzcVB1oAGfpHyJuKFKR7NkLU5XTCdz2JKVPhy?input=https:%2F%2Fen.wikipedia.org&quot;&gt;present the same DNS anomalies&lt;/a&gt; for all Wikipedia sub-domains. The few DNS anomalies that occurred in previous months were false positives, whereas the DNS anomalies from April 2019 onwards show that Wikipedia domains are blocked by means of DNS injection. Most measurements were collected from China Telecom (AS4134).&lt;/p&gt;
&lt;p&gt;Since OONI measurements collected from China suggest blocking by means of DNS injection, we can further measure the DNS-based blocking from outside of China as well. To this end, we ran the &lt;a href=&quot;https://github.com/ooni/spec/blob/master/nettests/ts-012-dns-injection.md&quot;&gt;OONI Probe DNS injection test&lt;/a&gt; from a vantage point outside of the country, pointing towards an IP address in China.&lt;/p&gt;
&lt;p&gt;This test relies on the fact that the Chinese firewall will “inject” DNS requests for restricted domains, even if the request is coming from outside the country and directed at an IP address which does not run a DNS resolver. The expectation was, therefore, that if the DNS query timed out, no blocking was happening, but if we saw a response, then that response was injected by the censor.&lt;/p&gt;
&lt;p&gt;The OONI Probe DNS injection test is very fast. It allowed us to &lt;a href=&quot;https://api.ooni.io/files/download/2019-04-24/20190424T200655Z-IS-AS47172-dns_injection-20190424T200655Z_AS47172_Peuv89addXJ1NZ5nTzY7i94X0rTag3QqGLwXKQcaoDTnHu9hu7-0.2.0-probe.json&quot;&gt;scan more than 2,000 Wikipedia domain names&lt;/a&gt; in less than a minute and to determine which ones were blocked.&lt;/p&gt;
&lt;p&gt;By analyzing the &lt;a href=&quot;https://api.ooni.io/files/download/2019-04-24/20190424T200655Z-IS-AS47172-dns_injection-20190424T200655Z_AS47172_Peuv89addXJ1NZ5nTzY7i94X0rTag3QqGLwXKQcaoDTnHu9hu7-0.2.0-probe.json&quot;&gt;results&lt;/a&gt; of the OONI Probe DNS injection test, we were able to understand that the restriction appears to be targeting any subdomain/language edition of wikipedia.org (i.e. &lt;code&gt;*.wikipedia.org&lt;/code&gt;, &lt;code&gt;zh.wikipedia.org&lt;/code&gt;, &lt;code&gt;en.wikipedia.org&lt;/code&gt;, etc.) - including &lt;code&gt;wikipedia.org&lt;/code&gt; - but to not be affecting any other Wikimedia resources, beyond zh.wikinews.org.&lt;/p&gt;
&lt;p&gt;The blocking appears to be targeting wikipedia.org subdomains irrespective of whether they actually exist or not (for example, even &lt;a href=&quot;https://api.ooni.io/files/download/2019-04-24/20190424T200655Z-IS-AS47172-dns_injection-20190424T200655Z_AS47172_Peuv89addXJ1NZ5nTzY7i94X0rTag3QqGLwXKQcaoDTnHu9hu7-0.2.0-probe.json&quot;&gt;doesnotexist.wikipedia.org&lt;/a&gt; was blocked!). The IP address returned in the injected DNS response also appears to be pretty random (examples of prior work analyzing the distribution of IP addresses returned by the Great Firewall include “&lt;a href=&quot;https://censorbib.nymity.ch/pdf/Lowe2007a.pdf&quot;&gt;The Great DNS Wall of China&lt;/a&gt;” and “&lt;a href=&quot;https://www.usenix.org/system/files/conference/foci14/foci14-anonymous.pdf&quot;&gt;Towards a Comprehensive Picture of the Great Firewall’s DNS Censorship&lt;/a&gt;”).&lt;/p&gt;
&lt;h2 id=&quot;sni-filtering&quot;&gt;SNI filtering&lt;/h2&gt;
&lt;p&gt;To check whether the blocking of Wikipedia domains could be circumvented by merely encrypting DNS traffic, we attempted to enable DNS over HTTPS in Firefox.&lt;/p&gt;
&lt;p&gt;To this end, we ran:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;curl -H 'accept: application/dns-json' https://cloudflare-dns.com/dns-query?name=www.wikipedia.org&amp;amp;type=A
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We were able to resolve the &lt;a href=&quot;http://www.wikipedia.org&quot;&gt;www.wikipedia.org&lt;/a&gt; domain name successfully with DNS over HTTPS.&lt;/p&gt;
&lt;p&gt;These tests were also validated by &lt;a href=&quot;https://wiki.mozilla.org/Trusted_Recursive_Resolver&quot;&gt;enabling DNS over HTTPS inside of Firefox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking/firefox-1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Yet, the page was still not accessible.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking/firefox-2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;We were only able to access the bare IP address from China, indicating that SNI filtering may be in place.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ooni.torproject.org/post/2019-china-wikipedia-blocking/firefox-3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;To further validate the theory that filtering was happening based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Server_Name_Indication&quot;&gt;SNI filtering&lt;/a&gt;, we ran the following curl tests (&lt;a href=&quot;https://ooni.torproject.org/post/venezuela-blocking-wikipedia-and-social-media-2019/&quot;&gt;we ran similar tests in Venezuela to confirm the same hypothesis&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ curl -v --connect-to ::www.kernel.org: https://www.wikipedia.org

* Rebuilt URL to: https://www.wikipedia.org/
* Connecting to hostname: www.kernel.org
*   Trying 147.75.46.191...
* TCP_NODELAY set
*   Trying 2604:1380:4080:c00::1...
* TCP_NODELAY set
* Immediate connect fail for 2604:1380:4080:c00::1: 网络不可达
* Connected to www.wikipedia.org (147.75.46.191) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* Unknown SSL protocol error in connection to www.wikipedia.org:443
* Curl_http_done: called premature == 1
* stopped the pause stream!
* Closing connection 0
curl: (35) Unknown SSL protocol error in connection to www.wikipedia.org:443
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above curl test is connecting to &lt;a href=&quot;http://www.kernel.org&quot;&gt;www.kernel.org&lt;/a&gt; (IP 147.75.46.191), but attempting to do a TLS handshake using the SNI of &lt;a href=&quot;http://www.wikipedia.org&quot;&gt;www.wikipedia.org&lt;/a&gt;. As we can see from the output above, as soon as the &lt;code&gt;TLS handshake, Client hello&lt;/code&gt; is sent, the connection is aborted.&lt;/p&gt;
&lt;p&gt;Conversely, as seen below, if we attempt to use the SNI of &lt;a href=&quot;http://www.kernel.org&quot;&gt;www.kernel.org&lt;/a&gt; when doing a TLS handshake with &lt;a href=&quot;http://www.wikipedia.org&quot;&gt;www.wikipedia.org&lt;/a&gt; (we use the &lt;code&gt;--resolve&lt;/code&gt; option to skip the DNS resolution), the request is successful and we are able to finish the TLS handshake.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ curl -v --resolve 'www.wikipedia.org:443:91.198.174.192' --connect-to ::www.wikipedia.org: https://www.kernel.org

* Added www.wikipedia.org:443:91.198.174.192 to DNS cache
* Rebuilt URL to: https://www.kernel.org/
* Connecting to hostname: www.wikipedia.org
* Hostname www.wikipedia.org was found in DNS cache
*   Trying 91.198.174.192...
* TCP_NODELAY set
* Connected to www.kernel.org (91.198.174.192) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH
* successfully set certificate verify locations:
*   CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: /etc/ssl/certs
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Client hello (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS change cipher, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-ECDSA-AES256-GCM-SHA384
* ALPN, server accepted to use h2
* Server certificate:
*  subject: C=US; ST=California; L=San Francisco; O=Wikimedia Foundation, Inc.; CN=*.wikipedia.org
*  start date: Nov  8 21:21:04 2018 GMT
*  expire date: Nov 22 07:59:59 2019 GMT
*  subjectAltName does not match www.kernel.org
* SSL: no alternative certificate subject name matches target host name 'www.kernel.org'
* Curl_http_done: called premature == 1
* stopped the pause stream!
* Closing connection 0
* TLSv1.2 (OUT), TLS alert, Client hello (1):
curl: (51) SSL: no alternative certificate subject name matches target host name 'www.kernel.org'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Based on these tests, we were able to conclude that China Telecom does in fact block all language editions of Wikipedia by means of both DNS injection and SNI filtering.&lt;/p&gt;
&lt;p&gt;Similarly to &lt;a href=&quot;https://ooni.torproject.org/post/egypt-internet-censorship/&quot;&gt;censorship implemented in Egypt&lt;/a&gt;, perhaps this can be viewed as a “&lt;a href=&quot;https://en.wikipedia.org/wiki/Defense_in_depth_(computing)&quot;&gt;defense in depth&lt;/a&gt;” tactic for network filtering. By implementing both DNS and SNI-based filtering, China Telecom creates multiple layers of censorship that make circumvention harder.&lt;/p&gt;
&lt;p&gt;The use of an encrypted DNS resolver (such as DNS over HTTPS) together with &lt;a href=&quot;https://datatracker.ietf.org/doc/draft-ietf-tls-esni/&quot;&gt;Encrypted SNI (ESNI)&lt;/a&gt; could potentially work as a circumvention strategy. Wikipedia.org does not currently support ESNI, but there have been &lt;a href=&quot;https://phabricator.wikimedia.org/T205378&quot;&gt;discussions&lt;/a&gt; about enabling it.&lt;/p&gt;
</description>
<pubDate>Mon, 13 May 2019 09:01:53 +0000</pubDate>
<dc:creator>JayXon</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://ooni.torproject.org/post/2019-china-wikipedia-blocking/</dc:identifier>
</item>
<item>
<title>CO2 in the atmosphere just exceeded 415ppm for the first time in human history</title>
<link>https://techcrunch.com/2019/05/12/co2-in-the-atmosphere-just-exceeded-415-parts-per-million-for-the-first-time-in-human-history/</link>
<guid isPermaLink="true" >https://techcrunch.com/2019/05/12/co2-in-the-atmosphere-just-exceeded-415-parts-per-million-for-the-first-time-in-human-history/</guid>
<description>&lt;p id=&quot;speakable-summary&quot;&gt;The human race has broken another record on its race to ecological collapse. Congratulations humanity!&lt;/p&gt;
&lt;p&gt;For the first time in human history — not recorded history, but since humans have existed on Earth — carbon dioxide in the atmosphere has topped 415 parts per million, reaching 415.26 parts per million, according to sensors at the &lt;a href=&quot;https://www.esrl.noaa.gov/gmd/obop/mlo/&quot;&gt;Mauna Loa Observatory&lt;/a&gt;, a research outpost of the &lt;a href=&quot;https://www.noaa.gov/&quot;&gt;National Oceanic and Atmospheric Agency&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&quot;attachment_1826234&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;25.248322147651&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-1826234&quot; class=&quot;breakout wp-image-1826234 size-full&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;617&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png 1994w, https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png?resize=150,90 150w, https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png?resize=300,181 300w, https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png?resize=768,463 768w, https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png?resize=680,410 680w, https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.23.36-PM.png?resize=50,30 50w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot;/&gt;&lt;p id=&quot;caption-attachment-1826234&quot; class=&quot;wp-caption-text&quot;&gt;CO2 emissions over time as recorded by measurements of Arctic ice and the Mauna Loa Observatory. Courtesy of the &lt;a href=&quot;https://scripps.ucsd.edu/programs/keelingcurve/&quot;&gt;Scripps Institution of Oceanography&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The macabre milestone was &lt;a href=&quot;https://twitter.com/EricHolthaus/status/1127681719216353280&quot;&gt;noted on Twitter&lt;/a&gt; by the climate reporter Eric Holthaus, based on the data recorded and presented by the &lt;a href=&quot;https://scripps.ucsd.edu/programs/keelingcurve/&quot;&gt;Scripps Institution of Oceanography at the University of California, San Diego&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;embed breakout&quot; readability=&quot;9.8936781609195&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot; readability=&quot;13.491379310345&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This is the first time in human history our planet's atmosphere has had more than 415ppm CO2.&lt;/p&gt;
&lt;p&gt;Not just in recorded history, not just since the invention of agriculture 10,000 years ago. Since before modern humans existed millions of years ago.&lt;/p&gt;
&lt;p&gt;We don't know a planet like this. &lt;a href=&quot;https://t.co/azVukskDWr&quot;&gt;https://t.co/azVukskDWr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Eric Holthaus (@EricHolthaus) &lt;a href=&quot;https://twitter.com/EricHolthaus/status/1127681719216353280?ref_src=twsrc%5Etfw&quot;&gt;May 12, 2019&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;If the threshold seems unremarkable (it shouldn’t), it’s yet another indication of the unprecedented territory humanity is now charting as it blazes new trails toward environmental catastrophe.&lt;/p&gt;
&lt;p&gt;Just last week a report revealed that &lt;a href=&quot;https://techcrunch.com/2019/05/06/new-study-shows-human-development-is-destroying-the-planet-at-an-unprecedented-rate/&quot;&gt;at least 1 million species were at risk of extinction&lt;/a&gt; thanks to human activity and the carbon emissions that are a byproduct of economic development.&lt;/p&gt;

&lt;p&gt;That’s on top of news that climate change, which has been inextricably linked to carbon emissions, will cost the U.S. alone &lt;a href=&quot;https://techcrunch.com/2018/11/23/new-u-s-report-says-that-climate-change-could-cost-nearly-500-billion-per-year-by-2090/&quot;&gt;some $500 billion per year by 2090&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The increasing proportion of carbon dioxide in the atmosphere is important because of its heat absorbing properties. The land and seas on the planet absorb and emit heat, and that heat is trapped in carbon dioxide molecules. The NOAA likens CO2 to leaving bricks in a fireplace that still emit heat after a fire goes out.&lt;/p&gt;
&lt;p&gt;Greenhouse gases contribute to the planet maintaining a temperature that can sustain life, but too much can impact the entire ecosystem that sustains us. That’s what’s happening now. &lt;a href=&quot;https://www.climate.gov/news-features/understanding-climate/climate-change-atmospheric-carbon-dioxide&quot;&gt;As the NOAA notes&lt;/a&gt;, “increases in greenhouse gases have tipped the Earth’s energy budget out of balance, trapping additional heat and raising Earth’s average temperature.”&lt;/p&gt;
&lt;p&gt;The properties of CO2 also mean that it adds to the greenhouse effect in a way that other emissions do not, thanks to its ability to absorb wavelengths of thermal energy that things like water vapor can’t. That’s why increases of atmospheric carbon dioxide are responsible for about two-thirds of the total energy imbalance causing Earth’s temperature to rise, according to the NOAA.&lt;/p&gt;
</description>
<pubDate>Mon, 13 May 2019 08:46:24 +0000</pubDate>
<dc:creator>wang42</dc:creator>
<og:title>CO2 in the atmosphere just exceeded 415 parts per million for the first time in human history – TechCrunch</og:title>
<og:description>The human race has broken another record on its race to ecological collapse. Congratulations humanity! For the first time in human history — not recorded history, but since humans have existed on Earth — carbon dioxide in the atmosphere has topped 415 parts per million, reaching 415.26 …</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2019/05/Screen-Shot-2019-05-12-at-3.33.05-PM.png?w=764</og:image>
<og:url>http://social.techcrunch.com/2019/05/12/co2-in-the-atmosphere-just-exceeded-415-parts-per-million-for-the-first-time-in-human-history/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2019/05/12/co2-in-the-atmosphere-just-exceeded-415-parts-per-million-for-the-first-time-in-human-history/</dc:identifier>
</item>
<item>
<title>Ask HN: Which books teach mental models?</title>
<link>https://news.ycombinator.com/item?id=19895407</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=19895407</guid>
<description>&lt;tr readability=&quot;0.58823529411765&quot;&gt;&lt;td bgcolor=&quot;#FF6600&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr id=&quot;pagespace&quot; title=&quot;Ask HN: Which books teach mental models?&quot;&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;12.614583333333&quot;&gt;&lt;td&gt;
&lt;table class=&quot;fatitem&quot; border=&quot;0&quot; readability=&quot;8.109375&quot;&gt;&lt;tr class=&quot;athing&quot; id=&quot;19895407&quot; readability=&quot;0&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;title&quot;/&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;title&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=19895407&quot; class=&quot;storylink&quot;&gt;Ask HN: Which books teach mental models?&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.70588235294118&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td class=&quot;subtext&quot;&gt;&lt;span class=&quot;score&quot; id=&quot;score_19895407&quot;&gt;358 points&lt;/span&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=wintercarver&quot; class=&quot;hnuser&quot;&gt;wintercarver&lt;/a&gt; &lt;span class=&quot;age&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=19895407&quot;&gt;16 hours ago&lt;/a&gt;&lt;/span&gt; &lt;span id=&quot;unv_19895407&quot;/&gt; | &lt;a href=&quot;https://news.ycombinator.com/hide?id=19895407&amp;amp;goto=item%3Fid%3D19895407&quot;&gt;hide&lt;/a&gt; | &lt;a href=&quot;https://hn.algolia.com/?query=Ask%20HN%3A%20Which%20books%20teach%20mental%20models%3F&amp;amp;sort=byDate&amp;amp;dateRange=all&amp;amp;type=story&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0&quot; class=&quot;hnpast&quot;&gt;past&lt;/a&gt; | &lt;a href=&quot;https://www.google.com/search?q=Ask%20HN%3A%20Which%20books%20teach%20mental%20models%3F&quot;&gt;web&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/fave?id=19895407&amp;amp;auth=1755443e9fca8beea3094abab97a6d5980de7c21&quot;&gt;favorite&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/item?id=19895407&quot;&gt;74 comments&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;24&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td readability=&quot;23&quot;&gt;Read the piece by Mutaschak[1] yesterday and found myself agreeing with components of the piece but distinctly feeling that some well-crafted books take a more active role in conveying and teaching mental models than simply summarized facts (which can be done well, but is subject to being forgotten).
&lt;p&gt;The question, then, is what books effectively introduced a new mental model or perspective?&lt;/p&gt;
&lt;p&gt;Two recent examples from my own reading, non-fiction and fiction:&lt;/p&gt;
&lt;p&gt;Loonshots (Bahcall) - model &amp;amp; &quot;rules&quot; for structure of innovation in orgs is introduced, discussed from various perspectives, examples given, summarized in text, repeated.&lt;/p&gt;
&lt;p&gt;Overstory (Powers) - character stories all reinforce the perspective of an alternative relationship with trees and plants, the giant ecosystem and systems thinking.&lt;/p&gt;
&lt;p&gt;[1] https://andymatuschak.org/books/&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;1&quot;&gt;&lt;td&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;10&quot; width=&quot;0&quot;/&gt;&lt;br/&gt;&lt;center&gt;&lt;span class=&quot;yclinks&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/newsguidelines.html&quot;&gt;Guidelines&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/newsfaq.html&quot;&gt;FAQ&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Support&lt;/a&gt; | &lt;a href=&quot;https://github.com/HackerNews/API&quot;&gt;API&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/security.html&quot;&gt;Security&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/lists&quot;&gt;Lists&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/bookmarklet.html&quot; rel=&quot;nofollow&quot;&gt;Bookmarklet&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/legal/&quot;&gt;Legal&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/apply/&quot;&gt;Apply to YC&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Contact&lt;/a&gt;&lt;/span&gt;
&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Sun, 12 May 2019 23:55:25 +0000</pubDate>
<dc:creator>wintercarver</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=19895407</dc:identifier>
</item>
<item>
<title>How I Run a Company with ADHD</title>
<link>https://www.andrewaskins.com/how-i-run-a-company-with-adhd/</link>
<guid isPermaLink="true" >https://www.andrewaskins.com/how-i-run-a-company-with-adhd/</guid>
<description>&lt;p&gt;For the first 19 years of my life &lt;strong&gt;I knew I was lazy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I was smart, but I constantly slacked off. I found the smallest amount of work I could get away with doing and did no more.&lt;/p&gt;
&lt;p&gt;I was often reminded that I wasn't living up to my potential. &quot;If B's were all you were capable of that would be fine, but you're so smart. You could easily get an A,&quot; was a regular refrain in my house.&lt;/p&gt;
&lt;p&gt;Everyone around me seemed to find it easy to get work done. Why wasn't I that way? What the fuck was wrong with me? Why was I so weak that I couldn't stand to do anything remotely hard?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It filled me with shame.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I'm a nerd, always have been. I wasn't fast or strong, my face was covered in zits and I got super nervous around girls. I was happiest with my nose in the book or hanging out with my Boy Scout troop. School was the only thing I was good at. And I kept fucking up at that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It got worse in college.&lt;/strong&gt; First, smart phones became a part of the classroom. Then I discovered I could get away with skipping most of my classes all together.&lt;/p&gt;
&lt;p&gt;My grades started slipping, and I went from skating by, to struggling to keep up. The B's turned into C's and D's. Now piled on top of the shame, I started doubting my abilities for the first time.&lt;/p&gt;
&lt;p&gt;The year I turned 20 &lt;strong&gt;there were two big changes&lt;/strong&gt;:&lt;/p&gt;
&lt;h2 id=&quot;1-i-got-diagnosed-with-adhd&quot;&gt;1) I got diagnosed with ADHD&lt;/h2&gt;
&lt;p&gt;As my grades started slipping the shame piled up. I had to admit something wasn't working. I was beating myself up, and my grades weren't getting better. I take pride in being self-reliant, asking for help made me feel weak.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: this was a dumb way of looking at life. It's never bad to ask for help.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;But the direction I was heading scared me. So I went to see a psychologist. Within 4 sessions I got a diagnosis. &lt;strong&gt;I had Attention Deficit Hyperactivity Disorder.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Puzzle pieces started to fall into place. When you have ADHD it feels like your head is constantly buzzing.&lt;/p&gt;
&lt;img src=&quot;https://www.andrewaskins.com/content/images/2019/05/adhd-storytelling.jpg&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;Tasks as innocuous as putting away clothes can be a trap. The extra time it takes to do something as simple as open the drawer is time for your brain to get distracted. And then it could be an hour before you remember what you were doing.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: If you're struggling with ADHD, check out the book&lt;/em&gt; &lt;a href=&quot;https://ws-na.amazon-adsystem.com/widgets/q?ServiceVersion=20070822&amp;amp;OneJS=1&amp;amp;Operation=GetAdHtml&amp;amp;MarketPlace=US&amp;amp;source=ac&amp;amp;ref=tf_til&amp;amp;ad_type=product_link&amp;amp;tracking_id=andrewaskins-20&amp;amp;marketplace=amazon&amp;amp;region=US&amp;amp;placement=1592335128&amp;amp;asins=1592335128&amp;amp;linkId=ef5420c0727f83d70758a9101a625f83&amp;amp;show_border=false&amp;amp;link_opens_in_new_window=false&amp;amp;price_color=333333&amp;amp;title_color=0066C0&amp;amp;bg_color=FFFFFF&quot;&gt;Organizing Solutions for People With ADHD&lt;/a&gt;&lt;em&gt;. It gave me useful tips, but more importantly taught me about how ADHD works.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-i-started-a-company&quot;&gt;&lt;strong&gt;2) I started a company&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The spring of 2014 I started my company, Krit.&lt;/p&gt;
&lt;img src=&quot;https://blurt.nyc3.digitaloceanspaces.com/assets/images/5c317886e7e201010a7e2295/image_HG0Y6_a7B&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;As the date of our first launch approached I barely slept. Around 4 am I would call an Uber, then my dad would drop me off on his way to work at 8. Some nights I didn't even leave the office.&lt;/p&gt;
&lt;img src=&quot;https://blurt.nyc3.digitaloceanspaces.com/assets/images/5c317886e7e201010a7e2295/image__GKMdQ-Ji&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;Healthy? No. But I was working harder than I ever had before.&lt;/p&gt;
&lt;p&gt;It's one of the greatest gifts I've ever received. Because one day it hit me. &lt;strong&gt;I was working harder than I ever had before.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I wasn't lazy.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I could work!&lt;/p&gt;
&lt;p&gt;I actually got immense pride from working. It just took the right combination of motivation and constraints.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;That spring was almost 5 years ago.&lt;/strong&gt; Krit should make over $600,000 in revenue this year, and is now a team of 8. We've helped clients launch 8 new startups and 2 non-profit initiatives. We've built a small audience and products of our own. Our clients have sold their software to names like Facebook, Red Bull and Yale.&lt;/p&gt;
&lt;p&gt;Getting diagnosed and starting the company showed me that I wasn't lazy. &lt;strong&gt;Now I had hope.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-pressure-of-adhd-as-a-founder&quot;&gt;The pressure of ADHD as a founder&lt;/h2&gt;
&lt;p&gt;Unfortunately, hope is only the first ingredient. Learning I'm not lazy was powerful, but 5 years later I'm still fighting to make myself believe it. &lt;strong&gt;Those feelings didn't magically go away forever.&lt;/strong&gt; They still come back in waves until I fight them back again.&lt;/p&gt;
&lt;p&gt;Now, when they come back, the pressure is even higher. If I have a bad day I worry I'm setting a bad example for my employees or letting down a client. Getting behind can have major consequences. I often work extra hours to make up for the time I spent distracted and then feel myself burning out.&lt;/p&gt;
&lt;p&gt;ADHD seems like an innocuous disorder. Everyone procrastinates right? So what, you can't stop bouncing your leg at the restaurant? So what you get distracted every time you see something shiny?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What makes ADHD dangerous is the stress and shame that come with it.&lt;/strong&gt; The shame of feeling lazy, or dumb or socially awkward. The stress of not doing enough.&lt;/p&gt;
&lt;img src=&quot;https://www.andrewaskins.com/content/images/2019/05/simple-tasks.jpg&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;I'll be halfway through a team meeting and realize I haven't heard anything that was said. That's ADHD.&lt;/p&gt;
&lt;p&gt;I'll take a break to read an article and hours will go by before I realize what's happened. Not minutes, hours. That's ADHD.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ADHD can feel almost like time-traveling.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It doesn't have to be something unproductive like Twitter that initiates it either. I'll be working on a task, when I'll remember something else that needs to get done. I'll start working on that, and hours later realize the first task is still half-completed.&lt;/p&gt;
&lt;img src=&quot;https://www.andrewaskins.com/content/images/2019/05/hyperfocus.jpg&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;As I'm writing this I've stopped half a dozen times to wipe dog hair from my computer. Pulling my mind back to the article feels like reigning in an unruly horse.&lt;/p&gt;
&lt;h2 id=&quot;how-i-deal-with-adhd-as-a-founder&quot;&gt;How I deal with ADHD as a founder&lt;/h2&gt;
&lt;p&gt;Over time though, I am getting better at managing myself. Here are ways I've found of coping:&lt;/p&gt;
&lt;h3 id=&quot;prioritize-obsessively&quot;&gt;Prioritize obsessively&lt;/h3&gt;
&lt;p&gt;Start every day/week with a task list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When you're fighting ADHD, external stimuli are your enemies&lt;/strong&gt;. Unstructured time (when your mind is wandering) is exactly when those stimuli attack.&lt;/p&gt;
&lt;p&gt;I use Todoist these days to keep my list up to date. In the past I've used a straight up note or even a Slack message to myself. The key for me is for it to be super fast to add and reorganize tasks. The structure also has to be super simple. Too much organization is bad for someone with ADHD, because it's a chance to get distracted.&lt;/p&gt;
&lt;p&gt;Todoist has a great mobile app, and makes it super easy to prioritize tasks and move them between days.&lt;/p&gt;
&lt;img src=&quot;https://blurt.nyc3.digitaloceanspaces.com/assets/images/5c317886e7e201010a7e2295/image_xdT5zg9Qt&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;Make sure to keep your daily list small (I try to cap it at 5-6 tasks) and don't beat yourself up for moving things around. That unstructured time where you get distracted can also generate creative ideas.&lt;/p&gt;
&lt;h3 id=&quot;start-with-a-small-win&quot;&gt;Start with a small win&lt;/h3&gt;
&lt;p&gt;I often start with a lower priority task that I can get knocked out quickly. I've heard lots of people advise you to start your day with the biggest hardest task. For me that's often a recipe for disaster. &lt;strong&gt;A big, hard task is a chance to get distracted&lt;/strong&gt;. Then I feel worse about myself, and I spiral.&lt;/p&gt;
&lt;p&gt;Starting my day with a small win is a good way to remind myself that being productive feels good. Sometimes I'll also knock out a low priority task first just because it seems fun. Because fuck it, I'm my own boss, and it's okay to enjoy that.&lt;/p&gt;
&lt;h3 id=&quot;embrace-minimalism&quot;&gt;Embrace minimalism&lt;/h3&gt;
&lt;p&gt;Reducing the visual clutter in my world is a must. My brain constantly feels busy and out of control. &lt;strong&gt;Visual order in my surroundings help to soothe that feeling of chaos.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Minimalism also serves a practical purpose for people with ADHD. Every thing that occupies my plane of view is a chance to get distracted.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;I've been meaning to finish this book, I'll skim through and try to find my place... Oh I forgot about these grip trainers, I really ought to get back into rock climbing... Charger! Dammit Andrew you came over here to charge your phone!! That was the whole point... god I need to do my dishes, but first let me sweep...&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The same goes for digital minimalism. Notifications are a rabbit hole. Fun looking icons are a rabbit hole. &lt;em&gt;&lt;strong&gt;Everything&lt;/strong&gt;&lt;/em&gt; &lt;strong&gt;is a potential rabbit hole&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I've been working on making my phone dumb again. First, I removed all of the social apps. Then I turned off notifications for everything except a few apps I use to communicate with my friends. I even turned off the notification bubbles on my email app.&lt;/p&gt;
&lt;img src=&quot;https://www.andrewaskins.com/content/images/2019/05/phone.png&quot; class=&quot;kg-image&quot;/&gt;My phone
&lt;h3 id=&quot;raise-money&quot;&gt;Raise money&lt;/h3&gt;
&lt;p&gt;I'm a big fan of bootstrapping. I love the conversations the bootstrapped community has around health and entrepreneurship. This group of people is bucking the idea that you have to build something massive and work yourself to the bone in order to be successful.&lt;/p&gt;
&lt;p&gt;But raising money has one underrated advantage. It lights one hell of a fire under your ass.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;Raising money has one underrated advantage... it lights one hell of a fire under your ass.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I consider myself a bootstrapper.&lt;/p&gt;
&lt;p&gt;That's only partially true. When we started the company, my co-founders and I had been trying to get something off the ground for several months. Then we got accepted into an accelerator program. They put $16,000 into our company.&lt;/p&gt;
&lt;img src=&quot;https://www.andrewaskins.com/content/images/2019/05/Screen-Shot-2019-05-05-at-6.15.20-PM.png&quot; class=&quot;kg-image&quot;/&gt;The email where I found out we were getting $16,000 to start Krit. 
&lt;p&gt;While it wasn't much, without that money our company wouldn't exist today. &lt;strong&gt;It held us accountable&lt;/strong&gt;. It gave us the pressure we needed to get started. That's a powerful feature.&lt;/p&gt;
&lt;h3 id=&quot;hire-a-coach-or-editor-&quot;&gt;Hire a coach (or editor)&lt;/h3&gt;
&lt;p&gt;Raising money won't always be an option, so you'll have to find other ways to hold yourself accountable. &lt;strong&gt;Start by figuring out what excites and motivates you&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For me it's people and relationships. The surest way for me to get something done, is to know that I will be letting someone down if I don't.&lt;/p&gt;
&lt;p&gt;The capital we raised was the first example. But a more accessible option is to hire a coach. Or, if you're a writer, hire an editor. Regular meetings with another person are a fantastic forcing function. I may get the work I promised them done the day before our meeting, but I'm going to get it done.&lt;/p&gt;
&lt;h3 id=&quot;do-work-you-enjoy&quot;&gt;Do work you enjoy&lt;/h3&gt;
&lt;p&gt;Everyone should be lucky enough to find work they enjoy. But &lt;strong&gt;for those with ADHD it's a requirement if you want to be successful&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I started my career as a programmer. I enjoyed the problem-solving aspect of it, but I hated cranking out code day after day.&lt;/p&gt;
&lt;p&gt;As a result, I wasn't very productive as a programmer. I regularly missed deadlines, and the quality of my code was average at best.&lt;/p&gt;
&lt;p&gt;The biggest boost for my productivity was stepping away from writing code full-time. Two years ago I transitioned from coding to a more traditional CEO role. I handle sales, marketing and whatever needs doing on a daily basis. I'm much happier than I was when I was programming.&lt;/p&gt;
&lt;p&gt;I was privileged enough to be in a position where, at 23, I was running my own company. Because of that position I was able to change my role without getting a new job, or anyone's permission. For that I am incredibly lucky (and thankful).&lt;/p&gt;
&lt;p&gt;If you're not in that position, it will be harder to experiment and find the work you love. But &lt;strong&gt;if you have ADHD, you have to try&lt;/strong&gt;. Like every other tip here, it's about reducing the chances of getting distracted. When you enjoy the work, it becomes easier to get started, and you're less likely to look for distractions.&lt;/p&gt;
Listening to this podcast about chasing your motivation was super affirming for me.
&lt;p&gt;Finding working you enjoy isn't a magic cure-all. I still have to do things I don't want to do. I still have to constantly work on my self-discipline. But it is leaps and bounds better now than it was just a couple of years ago.&lt;/p&gt;
&lt;h3 id=&quot;the-advantages-of-adhd&quot;&gt;The advantages of ADHD&lt;/h3&gt;
&lt;p&gt;The disorder also has advantages. &lt;strong&gt;The biggest advantage of ADHD is the ability it gives me to think creatively.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That sounds vague, so let me try to explain. I'm horrible at completing simple, repetitive tasks. I'm terrible at time management, and conceptualizing time in general.&lt;/p&gt;
&lt;p&gt;But the types of tasks that the world puts into the bucket of &quot;creative&quot; I tend to be quite good at.&lt;/p&gt;
&lt;p&gt;✅ Thinking up names for new products&lt;/p&gt;
&lt;p&gt;✅ Creating a high level strategy&lt;/p&gt;
&lt;p&gt;✅ Figuring out the perfect wording for a company goal&lt;/p&gt;
&lt;p&gt;✅ Writing (when I can get started)&lt;/p&gt;
&lt;p&gt;✅ Persuasive conversations (aka sales)&lt;/p&gt;
&lt;p&gt;✅ Solving poorly-defined or murky problems&lt;/p&gt;
&lt;p&gt;These are all things I can do.&lt;/p&gt;
&lt;p&gt;While I have no proof that ADHD is the cause, there have been studies that indicate it is connected to creative thinking. And from my own anecdotal evidence it holds up.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A note on medication: When I take medication I feel these skills being dulled. I can't think as quickly, I'm not as outgoing. ADHD medication also contributes to an increase in anxiety, and so for me it's not worth it. If you're struggling you should absolutely try it. For some people, medication is a must. Don't ever feel bad about taking it if it helps you.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;I don't ever want to use ADHD as an excuse or a crutch.&lt;/strong&gt; But if I don't acknowledge the challenges the disorder creates I can't develop coping mechanisms. So I'm acknowledging those challenges and I'm sharing them here. My hope is that others facing the same challenges know they're not alone.&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;If I don't acknowledge the challenges the disorder creates I can't develop coping mechanisms.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here's what works for me:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Fight shame&lt;/strong&gt;. It's an asshole of an emotion. Don't let it rule your life. You're not a bad person just because you have done bad things.&lt;/li&gt;
&lt;li&gt;Find a way to &lt;strong&gt;create healthy pressure&lt;/strong&gt;. Raise money or hire a coach. Whatever it is, invest time learning what motivates you.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prioritize obsessively&lt;/strong&gt;. Lists, lists, lists. Don't overcomplicate it to start, a blank sheet of paper works great.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Start small&lt;/strong&gt;. Look for a small win to start your day off and build momentum from there.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embrace minimalism&lt;/strong&gt; in your digital and physical life. The fewer visual triggers to distract you the better.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Find the work you love.&lt;/strong&gt; The internet has created a new world where any niche can turn into a sustainable profession. Worry about the finances later, focus on finding work you enjoy.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;And remember, like everything in life ADHD is a two-sided coin. The disorder that drives you crazy is likely responsible for some of the best parts of you.&lt;/p&gt;
&lt;p&gt;Chin up, you got this.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Final note: It took me 4 months of leaving this piece half finished and coming back to it in order to finish it.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;The awesome illustrations in this blog post are all courtesy of Dani Donovan. Follow her on &lt;a href=&quot;https://twitter.com/danidonovan&quot;&gt;Twitter&lt;/a&gt; or &lt;a href=&quot;https://www.patreon.com/danidonovan&quot;&gt;subscribe to her patreon&lt;/a&gt; to help support her awesome work.&lt;/p&gt;
</description>
<pubDate>Sun, 12 May 2019 23:35:01 +0000</pubDate>
<dc:creator>askins4trouble</dc:creator>
<og:type>article</og:type>
<og:title>How I run a company with ADHD</og:title>
<og:description>For the first 19 years of my life I was convinced I was lazy. Then I was diagnosed with ADHD. I've since started a company and grown it to a team of 8. Here's how I've learned to not just cope, but embrace the disorder.</og:description>
<og:url>https://www.andrewaskins.com/how-i-run-a-company-with-adhd/</og:url>
<og:image>https://www.andrewaskins.com/content/images/2019/05/IMG_3744.JPG</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.andrewaskins.com/how-i-run-a-company-with-adhd/</dc:identifier>
</item>
<item>
<title>3D Game Shaders for Beginners</title>
<link>https://github.com/lettier/3d-game-shaders-for-beginners</link>
<guid isPermaLink="true" >https://github.com/lettier/3d-game-shaders-for-beginners</guid>
<description>&lt;div class=&quot;Box-body&quot;&gt;
&lt;article class=&quot;markdown-body entry-content p-5&quot; itemprop=&quot;text&quot;&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/cd122c106851fd338866ff521bb26849e618f663/68747470733a2f2f692e696d6775722e636f6d2f617642537848442e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/cd122c106851fd338866ff521bb26849e618f663/68747470733a2f2f692e696d6775722e636f6d2f617642537848442e676966&quot; alt=&quot;3d Game Shaders For Beginners&quot; data-canonical-src=&quot;https://i.imgur.com/avBSxHD.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Interested in adding textures, lighting, shadows, normal maps, glowing objects, ambient occlusion, and more to your 3D game? Great! Below is a collection of shading techniques that will take your game visuals to new heights. I've explained each technique in such a way that you can take what you learn here and apply/port it to whatever stack you use—be it Godot, Unity, or something else. For the glue in between the shaders, I've chosen the fabulous Panda3D game engine and the OpenGL Shading Language (GLSL). So if that is your stack, then you'll also get the benefit of learning how to use these shading techniques with Panda3D and OpenGL specifically.&lt;/p&gt;
&lt;h2&gt;Table Of Contents&lt;/h2&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;Below is the setup used to develop and test the example code.&lt;/p&gt;
&lt;h3&gt;Environment&lt;/h3&gt;
&lt;p&gt;The example code was developed and tested using the following environment.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Linux manjaro 4.9.135-1-MANJARO&lt;/li&gt;
&lt;li&gt;OpenGL renderer string: GeForce GTX 970/PCIe/SSE2&lt;/li&gt;
&lt;li&gt;OpenGL version string: 4.6.0 NVIDIA 410.73&lt;/li&gt;
&lt;li&gt;g++ (GCC) 8.2.1 20180831&lt;/li&gt;
&lt;li&gt;Panda3D 1.10.1-1&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Materials&lt;/h3&gt;
&lt;p&gt;Each &lt;a href=&quot;https://blender.org&quot; rel=&quot;nofollow&quot;&gt;Blender&lt;/a&gt; material used to build &lt;code&gt;mill-scene.egg&lt;/code&gt; has three textures. The first texture is the normal map and the second is the diffuse map. If an object uses its vertex normals, a &quot;flat blue&quot; normal map is used. By having the same maps in the same positions for all models, the shaders can be generalized and applied to the root node in the scene graph.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/79cef24c65616c0369b14d21556f6503f6a37c9c/68747470733a2f2f692e696d6775722e636f6d2f74466d4b676f482e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/79cef24c65616c0369b14d21556f6503f6a37c9c/68747470733a2f2f692e696d6775722e636f6d2f74466d4b676f482e706e67&quot; alt=&quot;A flat normal map.&quot; data-canonical-src=&quot;https://i.imgur.com/tFmKgoH.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is a flat normal map which maps &lt;code&gt;(0, 0, 1)&lt;/code&gt; to &lt;code&gt;(128, 128, 255)&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
[ &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
] =
[ round((&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;) * &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt;)
, round((&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;) * &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt;)
, round((&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;) * &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt;)
] =
[ &lt;span class=&quot;pl-c1&quot;&gt;128&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;128&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt;
] =
[ round(&lt;span class=&quot;pl-c1&quot;&gt;128&lt;/span&gt; / &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
, round(&lt;span class=&quot;pl-c1&quot;&gt;128&lt;/span&gt; / &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
, round(&lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt; / &lt;span class=&quot;pl-c1&quot;&gt;255&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
] =
[ &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
]
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Panda3D&lt;/h3&gt;
&lt;p&gt;The develop the example code, &lt;code&gt;gl-coordinate-system default&lt;/code&gt;, &lt;code&gt;textures-power-2 down&lt;/code&gt;, and &lt;code&gt;textures-auto-power-2 1&lt;/code&gt; were added to &lt;code&gt;Config.prc&lt;/code&gt;. Panda3D defaults to a z-up, right-handed coordinate system while OpenGL uses a y-up, right-handed system. &lt;code&gt;gl-coordinate-system default&lt;/code&gt; keeps you from having to translate between the two inside your shaders. &lt;code&gt;textures-auto-power-2 1&lt;/code&gt; allows us to use texture sizes that are not a power of two if the system supports it. This comes in handy when doing SSAO and other screen/window sized related techniques since the screen/window size is usually not a power of two. &lt;code&gt;textures-power-2 down&lt;/code&gt; downsizes our textures to a power of two if the system only supports texture sizes being a power of two.&lt;/p&gt;
&lt;h2&gt;Building The Example Code&lt;/h2&gt;
&lt;p&gt;If you'd like to run the example code you'll need to build it first. Note that Panda3D works on Linux, Mac, and Windows.&lt;/p&gt;
&lt;h3&gt;Linux&lt;/h3&gt;
&lt;p&gt;Start by &lt;a href=&quot;https://www.panda3d.org/manual/?title=Installing_Panda3D_in_Linux&quot; rel=&quot;nofollow&quot;&gt;installing&lt;/a&gt; the &lt;a href=&quot;https://www.panda3d.org/download/sdk-1-10-1/&quot; rel=&quot;nofollow&quot;&gt;Panda3D SDK&lt;/a&gt; for your distribution.&lt;/p&gt;
&lt;p&gt;Make sure to locate where the Panda3D headers and libraries are. The headers and libraries are most likely in &lt;code&gt;/usr/include/panda3d/&lt;/code&gt; and &lt;code&gt;/usr/lib/panda3d/&lt;/code&gt; respectively.&lt;/p&gt;
&lt;p&gt;Next clone this repository and change directory into it.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
git clone https://github.com/lettier/3d-game-shaders-for-beginners.git
&lt;span class=&quot;pl-c1&quot;&gt;cd&lt;/span&gt; 3d-game-shaders-for-beginners
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now compile the source code into an object file.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
g++ \
  -c main.cxx \
  -o 3d-game-shaders-for-beginners.o \
  -std=gnu++11 \
  -O2 \
  -I/usr/include/python2.7/ \
  -I/usr/include/panda3d/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With the object file created, create the executable by linking the object file to its dependencies.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;g++ \
  3d-game-shaders-for-beginners.o \
  -o 3d-game-shaders-for-beginners \
  -L/usr/lib/panda3d \
  -lp3framework \
  -lpanda \
  -lpandafx \
  -lpandaexpress \
  -lp3dtoolconfig \
  -lp3dtool \
  -lp3pystub \
  -lp3direct \
  -lpthread
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You can now run the example code like so.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
./3d-game-shaders-for-beginners
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more help, see the &lt;a href=&quot;https://www.panda3d.org/manual/?title=How_to_compile_a_C++_Panda3D_program_on_Linux&quot; rel=&quot;nofollow&quot;&gt;Panda3D manual&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Mac&lt;/h3&gt;
&lt;p&gt;Start by installing the &lt;a href=&quot;https://www.panda3d.org/download/sdk-1-10-1/&quot; rel=&quot;nofollow&quot;&gt;Panda3D SDK&lt;/a&gt; for Mac.&lt;/p&gt;
&lt;p&gt;Make sure to locate where the Panda3D headers and libraries are.&lt;/p&gt;
&lt;p&gt;Next clone this repository and change directory into it.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
git clone https://github.com/lettier/3d-game-shaders-for-beginners.git
&lt;span class=&quot;pl-c1&quot;&gt;cd&lt;/span&gt; 3d-game-shaders-for-beginners
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now compile the source code into an object file. You'll have to find where the Python 2.7 and Panda3D include directories are.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
clang++ \
  -c main.cxx \
  -o 3d-game-shaders-for-beginners.o \
  -std=gnu++11 \
  -g \
  -O2 \
  -I/usr/include/python2.7/ \
  -I/Developer/Panda3D/include/
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With the object file created, create the executable by linking the object file to its dependencies. You'll need to track down where the Panda3D libraries are located.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;clang++ \
  3d-game-shaders-for-beginners.o \
  -o 3d-game-shaders-for-beginners \
  -L/Developer/Panda3D/lib \
  -lp3framework \
  -lpanda \
  -lpandafx \
  -lpandaexpress \
  -lp3dtoolconfig \
  -lp3dtool \
  -lp3pystub \
  -lp3direct \
  -lpthread
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You can now run the example code like so.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
./3d-game-shaders-for-beginners
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more help, see the &lt;a href=&quot;https://www.panda3d.org/manual/?title=How_to_compile_a_C++_Panda3D_program_on_macOS&quot; rel=&quot;nofollow&quot;&gt;Panda3D manual&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Windows&lt;/h3&gt;
&lt;p&gt;Start by &lt;a href=&quot;https://www.panda3d.org/manual/?title=Installing_Panda3D_in_Windows&quot; rel=&quot;nofollow&quot;&gt;installing&lt;/a&gt; the &lt;a href=&quot;https://www.panda3d.org/download/sdk-1-10-1/&quot; rel=&quot;nofollow&quot;&gt;Panda3D SDK&lt;/a&gt; for Windows.&lt;/p&gt;
&lt;p&gt;Make sure to locate where the Panda3D headers and libraries are.&lt;/p&gt;
&lt;p&gt;Next clone this repository and change directory into it.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
git clone https://github.com/lettier/3d-game-shaders-for-beginners.git
&lt;span class=&quot;pl-c1&quot;&gt;cd&lt;/span&gt; 3d-game-shaders-for-beginners
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For more help, see the &lt;a href=&quot;https://www.panda3d.org/manual/?title=Running_your_Program&amp;amp;language=cxx&quot; rel=&quot;nofollow&quot;&gt;Panda3D manual&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Reference Frames&lt;/h2&gt;
&lt;p&gt;Before you write any shaders you should be familiar with the following frames of reference or coordinate systems. All of them boil down what origin &lt;code&gt;(0, 0, 0)&lt;/code&gt; are these coordinates currently relative to? Once you know that, you can then transform them, via some matrix, to some other vector space if need be. Typically, when the output of some shader looks wrong, it's because of some coordinate system mix up.&lt;/p&gt;
&lt;h3&gt;Model&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/7d048a40ae960bfa46e25fea020fd8e3c58a3f5a/68747470733a2f2f692e696d6775722e636f6d2f38787074616a552e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7d048a40ae960bfa46e25fea020fd8e3c58a3f5a/68747470733a2f2f692e696d6775722e636f6d2f38787074616a552e676966&quot; alt=&quot;Model Space&quot; data-canonical-src=&quot;https://i.imgur.com/8xptajU.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The model or object coordinate system is relative to the origin of the model. This is typically set to the center of the model's geometry in a modeling program like Blender.&lt;/p&gt;
&lt;h3&gt;World&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/7a7319eea3dc79f90186275ede73f49cb41c11f8/68747470733a2f2f692e696d6775722e636f6d2f66486c346f68582e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7a7319eea3dc79f90186275ede73f49cb41c11f8/68747470733a2f2f692e696d6775722e636f6d2f66486c346f68582e676966&quot; alt=&quot;World Space&quot; data-canonical-src=&quot;https://i.imgur.com/fHl4ohX.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The world space is relative to the origin of the scene/level/universe that you've created.&lt;/p&gt;
&lt;h3&gt;View&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/3510b8bba061a37c75437d0f84190395d1f0f715/68747470733a2f2f692e696d6775722e636f6d2f336234534747482e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/3510b8bba061a37c75437d0f84190395d1f0f715/68747470733a2f2f692e696d6775722e636f6d2f336234534747482e676966&quot; alt=&quot;View Space&quot; data-canonical-src=&quot;https://i.imgur.com/3b4SGGH.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The view or eye coordinate space is relative to the position of the active camera.&lt;/p&gt;
&lt;h3&gt;Clip&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/21a520918a14d8443589a02e68a17e05edd7d7e3/68747470733a2f2f692e696d6775722e636f6d2f695345575339592e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/21a520918a14d8443589a02e68a17e05edd7d7e3/68747470733a2f2f692e696d6775722e636f6d2f695345575339592e706e67&quot; alt=&quot;Clip Space&quot; data-canonical-src=&quot;https://i.imgur.com/iSEWS9Y.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The clip space is relative to the center of the camera's film. All coordinates are now homogeneous, ranging from negative one to one &lt;code&gt;(-1, 1)&lt;/code&gt;. X and y are parallel with the camera's film and the z coordinate is the depth.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/6a8c4073736e3f6ec2335ed14bf5b09700d6df3d/68747470733a2f2f692e696d6775722e636f6d2f4d68676d4f4c762e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/6a8c4073736e3f6ec2335ed14bf5b09700d6df3d/68747470733a2f2f692e696d6775722e636f6d2f4d68676d4f4c762e676966&quot; alt=&quot;Frustum&quot; data-canonical-src=&quot;https://i.imgur.com/MhgmOLv.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Any vertex not within the bounds of the camera's frustum or view volume is clipped or discarded. You can see this happening with the cube towards the back, clipped by the camera's far plane, and the cube off to the side.&lt;/p&gt;
&lt;h3&gt;Screen&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/b69bcbf35fa7609639f7fb6cf1f3dd49af1a5994/68747470733a2f2f692e696d6775722e636f6d2f624848726a4f6c2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/b69bcbf35fa7609639f7fb6cf1f3dd49af1a5994/68747470733a2f2f692e696d6775722e636f6d2f624848726a4f6c2e706e67&quot; alt=&quot;Screen Space&quot; data-canonical-src=&quot;https://i.imgur.com/bHHrjOl.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The screen space is (typically) relative to the lower left corner of the screen. X goes from zero to the screen width. Y goes from zero to the screen height.&lt;/p&gt;
&lt;h2&gt;GLSL&lt;/h2&gt;
&lt;p&gt;Instead of using the fixed-function pipeline, you'll be using the programmable GPU rendering pipeline. Since it is programmable, it is up to you to supply the programming in the form of shaders. A shader is a (typically small) program you write using a syntax reminiscent of C. The programmable GPU rendering pipeline has various different stages that you can program with shaders. The different types of shaders include vertex, tessellation, geometry, fragment, and compute. You'll only need to focus on the vertex and fragment stages for the techniques below.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() {}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here is a bare-bones GLSL shader consisting of the GLSL version number and the main function.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

uniform mat4 p3d_ModelViewProjectionMatrix;

in vec4 p3d_Vertex;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;()
{
  gl_Position = p3d_ModelViewProjectionMatrix * p3d_Vertex;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here is a stripped down GLSL vertex shader that transforms an incoming vertex to clip space and outputs this new position as the vertex's homogeneous position. The &lt;code&gt;main&lt;/code&gt; procedure doesn't return anything since it is &lt;code&gt;void&lt;/code&gt; and the &lt;code&gt;gl_Position&lt;/code&gt; variable is a built-in output.&lt;/p&gt;
&lt;p&gt;Note the two keywords &lt;code&gt;uniform&lt;/code&gt; and &lt;code&gt;in&lt;/code&gt;. The &lt;code&gt;uniform&lt;/code&gt; keyword means this global variable is the same for all vertexes. Panda3D sets the &lt;code&gt;p3d_ModelViewProjectionMatrix&lt;/code&gt; for you and it is the same matrix for each vertex. The &lt;code&gt;in&lt;/code&gt; keyword means this global variable is being given to the shader. The vertex shader receives each vertex that makes up the geometry the vertex shader is attached to.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

out vec4 fragColor;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() {
  fragColor = &lt;span class=&quot;pl-c1&quot;&gt;vec4&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here is a stripped down GLSL fragment shader that outputs the fragment color as solid green. Kind in mind that a fragment affects at most one screen pixel but a single pixel can be affected by many fragments.&lt;/p&gt;
&lt;p&gt;Note the keyword &lt;code&gt;out&lt;/code&gt;. The &lt;code&gt;out&lt;/code&gt; keyword means this global variable is being set by the shader. The name &lt;code&gt;fragColor&lt;/code&gt; is arbitrary so feel free to choose a different one.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/1a22f9fd1e15006448391a87b5b1605ccbcc9af1/68747470733a2f2f692e696d6775722e636f6d2f563235557a4d612e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/1a22f9fd1e15006448391a87b5b1605ccbcc9af1/68747470733a2f2f692e696d6775722e636f6d2f563235557a4d612e676966&quot; alt=&quot;Output of the stripped down shaders.&quot; data-canonical-src=&quot;https://i.imgur.com/V25UzMa.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is the output of the two shaders shown above.&lt;/p&gt;
&lt;h2&gt;Render To Texture&lt;/h2&gt;
&lt;p&gt;Instead of rendering/drawing/painting directly to the screen, the example code uses a technique called &quot;render to texture&quot;. In order to render to a texture, you'll need to set up a framebuffer and bind a texture to it. Multiple textures can be bound to a single framebuffer.&lt;/p&gt;
&lt;p&gt;The textures bound to the framebuffer hold the vector(s) returned by the fragment shader. Typically these vectors are color vectors &lt;code&gt;(r, g, b, a)&lt;/code&gt; but they could also be position or normal vectors &lt;code&gt;(x, y, z, w)&lt;/code&gt;. For each bound texture, the fragment shader can output a different vector. For example you could output a vertex's position and normal in a single pass.&lt;/p&gt;
&lt;p&gt;Most of the example code dealing with Panda3D involves setting up &lt;a href=&quot;https://www.panda3d.org/manual/?title=Render-to-Texture_and_Image_Postprocessing&quot; rel=&quot;nofollow&quot;&gt;framebuffer textures&lt;/a&gt;. To keep things straightforward, each fragment shader in the example code has only one output. However, you'll want to output as much as you can each render pass to keep your frames per second (FPS) high.&lt;/p&gt;
&lt;p&gt;There are two framebuffer texture setups found in the example code.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/9b21c5e88c62bc7078b7145e039a849fdd775f6f/68747470733a2f2f692e696d6775722e636f6d2f7433694c4b68782e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/9b21c5e88c62bc7078b7145e039a849fdd775f6f/68747470733a2f2f692e696d6775722e636f6d2f7433694c4b68782e676966&quot; alt=&quot;The first framebuffer texture setup.&quot; data-canonical-src=&quot;https://i.imgur.com/t3iLKhx.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first setup renders the mill scene into a framebuffer texture using a variety of vertex and fragment shaders. This setup will go through each of the mill scene's vertexes and corresponding fragments.&lt;/p&gt;
&lt;p&gt;In this setup, the example code performs the following.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Stores geometry data (like vertex position or normal) for later use.&lt;/li&gt;
&lt;li&gt;Stores material data (like the diffuse color) for later use.&lt;/li&gt;
&lt;li&gt;UV maps the various textures (diffuse, normal, shadow, etc.).&lt;/li&gt;
&lt;li&gt;Calculates the ambient, diffuse, specular, and emission lighting.&lt;/li&gt;
&lt;li&gt;Renders the fog.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/7d03425e9432dd287e6444fa6416b624d2d19e69/68747470733a2f2f692e696d6775722e636f6d2f724448423832572e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7d03425e9432dd287e6444fa6416b624d2d19e69/68747470733a2f2f692e696d6775722e636f6d2f724448423832572e706e67&quot; alt=&quot;The second framebuffer texture setup.&quot; data-canonical-src=&quot;https://i.imgur.com/rDHB82W.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The second setup is an orthographic camera pointed at a screen-shaped rectangle. This setup will go through just the four vertexes and their corresponding fragments.&lt;/p&gt;
&lt;p&gt;In this second setup, the example code performs the following.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Manipulates the output of another framebuffer texture.&lt;/li&gt;
&lt;li&gt;Combines various framebuffer textures into one.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In the example code, you can see the output of a particular framebuffer texture by setting its corresponding flag to true and the others to false.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c++&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showPositionBuffer        = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showNormalBuffer          = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showSsaoBuffer            = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showSsaoBlurBuffer        = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showMaterialDiffuseBuffer = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showOutlineBuffer         = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showBaseBuffer            = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showBloomBuffer           = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showCombineBuffer         = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showCombineBlurBuffer     = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showDepthOfFieldBuffer    = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showPosterizeBuffer       = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; showPixelizeBuffer        = &lt;span class=&quot;pl-c1&quot;&gt;true&lt;/span&gt;;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Texturing&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/9e1caf35b4326cfcd1d8f85a891ff9e2b13d0634/68747470733a2f2f692e696d6775722e636f6d2f674b6349564d352e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/9e1caf35b4326cfcd1d8f85a891ff9e2b13d0634/68747470733a2f2f692e696d6775722e636f6d2f674b6349564d352e676966&quot; alt=&quot;Diffuse Texture Only&quot; data-canonical-src=&quot;https://i.imgur.com/gKcIVM5.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Texturing involves mapping some color or some other kind of vector to a fragment using UV coordinates. Both U and V range from zero to one. Each vertex gets a UV coordinate and this is outputted in the vertex shader.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/fc17fcfb4f550f858f3d60368566dd43d9c73e5c/68747470733a2f2f692e696d6775722e636f6d2f767344697873372e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/fc17fcfb4f550f858f3d60368566dd43d9c73e5c/68747470733a2f2f692e696d6775722e636f6d2f767344697873372e706e67&quot; alt=&quot;UV Interpolation&quot; data-canonical-src=&quot;https://i.imgur.com/vsDixs7.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The fragment shader receives the UV coordinate interpolated. Interpolated meaning the UV coordinate for the fragment is somewhere between the UV coordinates for the vertexes that make up the triangle face.&lt;/p&gt;
&lt;h3&gt;Vertex&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

uniform mat4 p3d_ModelViewProjectionMatrix;

in vec2 p3d_MultiTexCoord0;

in vec4 p3d_Vertex;

out vec2 texCoord;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;()
{
  texCoord = p3d_MultiTexCoord0;

  gl_Position = p3d_ModelViewProjectionMatrix * p3d_Vertex;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here you see the vertex shader outputting the texture coordinate to the fragment shader. Notice how it's a two dimensional vector. One dimension for U and one for V.&lt;/p&gt;
&lt;h3&gt;Fragment&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

uniform sampler2D p3d_Texture0;

in vec2 texCoord;

out vec2 fragColor;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;()
{
  texColor = &lt;span class=&quot;pl-c1&quot;&gt;texture&lt;/span&gt;(p3d_Texture0, texCoord);

  fragColor = texColor;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here you see the fragment shader looking up the color at its UV coordinate and outputting that as the fragment color.&lt;/p&gt;
&lt;h4&gt;Screen Filled Texture&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
#version &lt;span class=&quot;pl-c1&quot;&gt;140&lt;/span&gt;

uniform sampler2D screenSizedTexture;

out vec2 fragColor;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;()
{
  vec2 texSize  = &lt;span class=&quot;pl-c1&quot;&gt;textureSize&lt;/span&gt;(texture, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;).&lt;span class=&quot;pl-smi&quot;&gt;xy&lt;/span&gt;;
  vec2 texCoord = gl_FragCoord.&lt;span class=&quot;pl-smi&quot;&gt;xy&lt;/span&gt; / texSize;

  texColor = &lt;span class=&quot;pl-c1&quot;&gt;texture&lt;/span&gt;(screenSizedTexture, texCoord);

  fragColor = texColor;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When performing render to texture, the mesh is a flat rectangle with the same aspect ratio as the screen. Because of this, you can calculate the UV coordinates knowing only the width and height of the screen sized texture being UV mapped to the rectangle and the fragment's x and y coordinate. To map x to U, divide x by the width of the input texture. Similarly, to map y to V, divide y by the height of the input texture. You'll see this technique used in the example code.&lt;/p&gt;
&lt;h2&gt;Lighting&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/2c184c92e670e6356b945d84a127dd12d573ae57/68747470733a2f2f692e696d6775722e636f6d2f31534a3545504d2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2c184c92e670e6356b945d84a127dd12d573ae57/68747470733a2f2f692e696d6775722e636f6d2f31534a3545504d2e676966&quot; alt=&quot;Lighting&quot; data-canonical-src=&quot;https://i.imgur.com/1SJ5EPM.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Completing the lighting involves calculating and combining the ambient, diffuse, specular, and emission light aspects. The example code uses Phong lighting.&lt;/p&gt;
&lt;h3&gt;Vertex&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; p3d_LightSourceParameters
  { vec4 color

  ; vec4 ambient
  ; vec4 diffuse
  ; vec4 specular

  ; vec4 position

  ; vec3  spotDirection
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; spotExponent
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; spotCutoff
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; spotCosCutoff

  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; constantAttenuation
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; linearAttenuation
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; quadraticAttenuation

  ; vec3 attenuation

  ; sampler2DShadow shadowMap

  ; mat4 shadowViewMatrix
  ;
  } p3d_LightSource[NUMBER_OF_LIGHTS];

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For every light, minus the ambient light, Panda3D gives you this convenient struct which is available to both the vertex and fragment shaders. The biggest convenience being the shadow map and shadow view matrix for transforming vertexes to shadow or light space.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vertexPosition = p3d_ModelViewMatrix * p3d_Vertex;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; p3d_LightSource.length(); ++i) {
    vertexInShadowSpaces[i] = p3d_LightSource[i].&lt;span class=&quot;pl-smi&quot;&gt;shadowViewMatrix&lt;/span&gt; * vertexPosition;
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Starting in the vertex shader, you'll need to transform and output the vertex from view space to shadow or light space for each light in your scene. You'll need this later in the fragment shader in order to render the shadows. Shadow or light space is where every coordinate is relative to the light position (the light is the origin).&lt;/p&gt;
&lt;h3&gt;Fragment&lt;/h3&gt;
&lt;p&gt;The fragment shader is where most of the lighting calculations take place.&lt;/p&gt;
&lt;h4&gt;Material&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt;
  { vec4 ambient
  ; vec4 diffuse
  ; vec4 emission
  ; vec3 specular
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; shininess
  ;
  } p3d_Material;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Panda3D gives us the material (in the form of a struct) for the mesh or model you are currently rendering.&lt;/p&gt;
&lt;h4&gt;Multiple Lights&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 diffuseSpecular = vec4(&lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before you loop through the scene's lights, create an accumulator that will hold both the diffuse and specular colors.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; p3d_LightSource.length(); ++i) {
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can loop through the lights, calculating the diffuse and specular colors for each one.&lt;/p&gt;
&lt;h4&gt;Light Related Vectors&lt;/h4&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/8f8f5fa28f71dff50afa17f3d48f6444909b4662/68747470733a2f2f692e696d6775722e636f6d2f30707a4e6835642e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/8f8f5fa28f71dff50afa17f3d48f6444909b4662/68747470733a2f2f692e696d6775722e636f6d2f30707a4e6835642e676966&quot; alt=&quot;Phong Lighting Model&quot; data-canonical-src=&quot;https://i.imgur.com/0pzNh5d.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see the four major vectors you'll need to calculate the diffuse and specular colors contributed by each light. The light direction vector is the light blue arrow pointing to the light. The normal vector is the green arrow standing straight up. The reflection vector is the dark blue arrow mirroring the light direction vector. The eye or view vector is the orange arrow pointing towards the camera.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec3 lightDirection =
        p3d_LightSource[i].position.xyz
      - vertexPosition.xyz
      * p3d_LightSource[i].position.w;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The light direction is from the vertex's position to the light's position.&lt;/p&gt;
&lt;p&gt;Panda3D sets &lt;code&gt;p3d_LightSource[i].position.w&lt;/code&gt; to zero if this is a directional light. Directional lights do not have a position as they only have a direction. So if this is a directional light, the light direction will be the negative or opposite direction of the light as Panda3D sets &lt;code&gt;p3d_LightSource[i].position.xyz&lt;/code&gt; to be &lt;code&gt;-direction&lt;/code&gt; for directional lights.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt; = normalize(vertexNormal);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll need the vertex normal to be a unit vector. Unit vectors have a length of magnitude of one.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec3 unitLightDirection = normalize(lightDirection);
    vec3 eyeDirection       = normalize(-vertexPosition.xyz);
    vec3 reflectedDirection = normalize(-reflect(unitLightDirection, &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;));

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Next you'll need three more vectors.&lt;/p&gt;
&lt;p&gt;You'll need to take the dot product involving the light direction so its best to normalize it. This gives it a distance or magnitude of one (unit vector).&lt;/p&gt;
&lt;p&gt;The eye direction is the opposite of the vertex/fragment position since the vertex/fragment position is relative to the camera's position. Remember that the vertex/fragment position is in view space. So instead of going from the camera (eye) to the vertex/fragment, you go from the vertex/fragment to the eye (camera).&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://asawicki.info/news_1301_reflect_and_refract_functions.html&quot; rel=&quot;nofollow&quot;&gt;reflection vector&lt;/a&gt; is a reflection of the light direction at the surface normal. As the light &quot;ray&quot; hits the surface, it bounces off at the same angle it came in at. The angle between the light direction vector and the normal is known as the &quot;angle of incidence&quot;. The angle between the reflection vector and the normal is known as the &quot;angle of reflection&quot;.&lt;/p&gt;
&lt;p&gt;You'll have to negate the reflected light vector as it needs to point in the same direction as the eye vector. Remember the eye direction is from the vertex/fragment to the camera position. You'll use the reflection vector to calculate the intensity of the specular highlight.&lt;/p&gt;
&lt;h4&gt;Diffuse&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; diffuseIntensity  = max(dot(&lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;, unitLightDirection), &lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;);

    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (diffuseIntensity &amp;gt; &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;) {
      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
    }

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The diffuse intensity is the dot product between the surface normal and the unit vector light direction. The dot product can range from negative one to one. If both vectors point in the same direction, the intensity is one. Any other case will be less than one.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/1a7c4d86e34efad44f27682356833c6739adda5d/68747470733a2f2f692e696d6775722e636f6d2f4e6237387a39362e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/1a7c4d86e34efad44f27682356833c6739adda5d/68747470733a2f2f692e696d6775722e636f6d2f4e6237387a39362e676966&quot; alt=&quot;The light direction versus the normal direction.&quot; data-canonical-src=&quot;https://i.imgur.com/Nb78z96.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As the light vector approaches the same direction as the normal, the diffuse intensity approaches one.&lt;/p&gt;
&lt;p&gt;Note that if the diffuse intensity is zero or less, move on to the next light.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

      vec4 diffuse =
        vec4
          ( clamp
              (   diffuseTex.rgb
                * p3d_LightSource[i].diffuse.rgb
                * diffuseIntensity
              , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
              , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
              )
          , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
          );

      diffuse.r = clamp(diffuse.r, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, diffuseTex.r);
      diffuse.g = clamp(diffuse.g, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, diffuseTex.g);
      diffuse.b = clamp(diffuse.b, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, diffuseTex.b);

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can now calculate the diffuse color contributed by this light. If the diffuse intensity is one, the diffuse color will be a mix between the diffuse texture color and the lights color. Any other intensity will cause the diffuse color to be darker.&lt;/p&gt;
&lt;p&gt;Note how I clamp the diffuse color to be only as bright as the diffuse texture color is. This will protect the scene from being over exposed.&lt;/p&gt;
&lt;h4&gt;Specular&lt;/h4&gt;
&lt;p&gt;After diffuse, comes specular.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/a5ace4a7445ebfe363b045320c77f952bd541328/68747470733a2f2f692e696d6775722e636f6d2f75354c336b43692e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/a5ace4a7445ebfe363b045320c77f952bd541328/68747470733a2f2f692e696d6775722e636f6d2f75354c336b43692e676966&quot; alt=&quot;Specular Intensity&quot; data-canonical-src=&quot;https://i.imgur.com/u5L3kCi.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

      vec4 specular =
        clamp
          (   vec4(p3d_Material.specular, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
            * p3d_LightSource[i].specular
            * pow
                ( max(dot(reflectedDirection, eyeDirection), &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;)
                , p3d_Material.shininess
                )
          , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
          , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
          );

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The specular intensity is the dot product between the eye vector and the reflection vector. As with the diffuse intensity, if the two vectors point in the same direction, the specular intensity is one. Any other intensity will diminish the amount of specular color contributed by this light.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/627bbf1ff11ed43ec57bd92be101df0f593fbd11/68747470733a2f2f692e696d6775722e636f6d2f34723677714c502e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/627bbf1ff11ed43ec57bd92be101df0f593fbd11/68747470733a2f2f692e696d6775722e636f6d2f34723677714c502e676966&quot; alt=&quot;Shininess&quot; data-canonical-src=&quot;https://i.imgur.com/4r6wqLP.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The material shininess determines how spread out the specular highlight is. This is typically set in a modeling program like Blender. In Blender it's known as the specular hardness.&lt;/p&gt;
&lt;h4&gt;Spotlights&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

      &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; unitLightDirectionDelta =
        dot
          ( normalize(p3d_LightSource[i].spotDirection)
          , -unitLightDirection
          );

      &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (unitLightDirectionDelta &amp;gt;= p3d_LightSource[i].spotCosCutoff) {
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
      }

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This snippet keeps fragments outside of a spotlight's cone or frustum from being affected by the light. Fortunately, Panda3D &lt;a href=&quot;https://github.com/panda3d/panda3d/blob/daa57733cb9b4ccdb23e28153585e8e20b5ccdb5/panda/src/display/graphicsStateGuardian.cxx#L1705&quot;&gt;sets up&lt;/a&gt; &lt;code&gt;spotDirection&lt;/code&gt; and &lt;code&gt;spotCosCutoff&lt;/code&gt; to also work for directional lights and points lights. Spotlights have both a position and direction. However, directional lights have only a direction and point lights have only a position. Still, this code works for all three lights avoiding the need for noisy if statements.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
spotCosCutoff = cosine(&lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; * spotlightLensFovAngle);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For a spotlight, if the dot product between the fragment-to-light vector and the spotlight's direction vector is less than the cosine of half the spotlight's field of view angle, the shader disregards this light's influence.&lt;/p&gt;
&lt;p&gt;Note that you must negate &lt;code&gt;unitLightDirection&lt;/code&gt; because it goes from the fragment to the spotlight and you need it to go from the spotlight to the fragment since the &lt;code&gt;spotDirection&lt;/code&gt; goes directly down the center of the spotlight's frustum some distance away from the spotlight's position.&lt;/p&gt;
&lt;p&gt;For directional lights and point lights, Panda3D sets &lt;code&gt;spotCosCutoff&lt;/code&gt; to negative one. Recall that the dot product ranges from negative one to one. So it doesn't matter what the &lt;code&gt;unitLightDirectionDelta&lt;/code&gt; is because it will always be greater than or equal to negative one.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

        diffuse *= pow(unitLightDirectionDelta, p3d_LightSource[i].spotExponent);

        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Like the &lt;code&gt;unitLightDirectionDelta&lt;/code&gt; snippet, this snippet also works for all three light types. For spotlights, this will make the fragments brighter as you move closer to the center of the spotlight's frustum. For directional lights and point lights, &lt;code&gt;spotExponent&lt;/code&gt; is zero. Recall that anything to the power of zero is one so the diffuse color is one times itself meaning it is unchanged.&lt;/p&gt;
&lt;h4&gt;Shadows&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

        &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;shadow&lt;/span&gt; =
          textureProj
            ( p3d_LightSource[i].shadowMap
            , vertexInShadowSpaces[i]
            );

        diffuse.rgb  *= &lt;span class=&quot;pl-c1&quot;&gt;shadow&lt;/span&gt;;
        specular.rgb *= &lt;span class=&quot;pl-c1&quot;&gt;shadow&lt;/span&gt;;

        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Panda3D makes applying shadows relatively easy by providing the shadow map and shadow transformation matrix for every scene light. To create the shadow transformation matrix yourself, you'll need to assemble a matrix that transforms view space coordinates to light space (coordinates are relative to the light's position). To create the shadow map yourself, you'll need to render the scene from the perspective of the light to a framebuffer texture. The framebuffer texture must hold the distances from the light to the fragments. This is known as a &quot;depth map&quot;. Lastly, you'll need to manually give to your shader your DIY depth map as a &lt;code&gt;uniform sampler2DShadow&lt;/code&gt; and your DIY shadow transformation matrix as a &lt;code&gt;uniform mat4&lt;/code&gt;. At this point, you've recreated what Panda3D does for you automatically.&lt;/p&gt;
&lt;p&gt;The shadow snippet shown uses &lt;code&gt;textureProj&lt;/code&gt; which is different from the &lt;code&gt;texure&lt;/code&gt; function shown earlier. &lt;code&gt;textureProj&lt;/code&gt; first divides &lt;code&gt;vertexInShadowSpaces[i].xyz&lt;/code&gt; by &lt;code&gt;vertexInShadowSpaces[i].w&lt;/code&gt;. After this, it uses &lt;code&gt;vertexInShadowSpaces[i].xy&lt;/code&gt; to locate the depth stored in the shadow map. Next it uses &lt;code&gt;vertexInShadowSpaces[i].z&lt;/code&gt; to compare this vertex's depth against the shadow map depth at &lt;code&gt;vertexInShadowSpaces[i].xy&lt;/code&gt;. If the comparison passes, &lt;code&gt;textureProj&lt;/code&gt; will return one. Otherwise, it will return zero. Zero meaning this vertex/fragment is in the shadow and one meaning this vertex/fragment is not in the shadow.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;textureProj&lt;/code&gt; can also return a value between zero and one depending on how the shadow map was set up. In this instance, &lt;code&gt;textureProj&lt;/code&gt; performs multiple depth tests using neighboring depth values and returns a weighted average. This weighted average can give shadows a softer look.&lt;/p&gt;
&lt;h4&gt;Attenuation&lt;/h4&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/49f33cffccf33b1e4a2f0d1fb61e662e6fc54861/68747470733a2f2f692e696d6775722e636f6d2f6a79617472376c2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/49f33cffccf33b1e4a2f0d1fb61e662e6fc54861/68747470733a2f2f692e696d6775722e636f6d2f6a79617472376c2e706e67&quot; alt=&quot;Attenuation&quot; data-canonical-src=&quot;https://i.imgur.com/jyatr7l.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

        &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; lightDistance = length(lightDirection);

        &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; attenuation =
            &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
          / ( p3d_LightSource[i].constantAttenuation
            + p3d_LightSource[i].linearAttenuation
            * lightDistance
            + p3d_LightSource[i].quadraticAttenuation
            * (lightDistance * lightDistance)
            );

        diffuse.rgb  *= attenuation;
        specular.rgb *= attenuation;

        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The light's distance is just the magnitude or length of the light direction vector. Notice it's not using the normalized light direction as that distance would be one.&lt;/p&gt;
&lt;p&gt;You'll need the light distance to calculate the attenuation. Attenuation meaning the light's influence diminishes as you get further away from it.&lt;/p&gt;
&lt;p&gt;You can set &lt;code&gt;constantAttenuation&lt;/code&gt;, &lt;code&gt;linearAttenuation&lt;/code&gt;, and &lt;code&gt;quadraticAttenuation&lt;/code&gt; to whatever values you would like. A good starting point is &lt;code&gt;constantAttenuation = 1&lt;/code&gt;, &lt;code&gt;linearAttenuation = 0&lt;/code&gt;, and &lt;code&gt;quadraticAttenuation = 1&lt;/code&gt;. With these settings, the attenuation is one at the light's position and approaches zero as you move further away.&lt;/p&gt;
&lt;h4&gt;Final Light Color&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

        diffuseSpecular += (diffuse + specular);

        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To calculate the final light color, add the diffuse and specular together. Be sure to add this to the accumulator as you loop through the scene's lights.&lt;/p&gt;
&lt;h4&gt;Ambient&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform sampler2D p3d_Texture1;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt;
  { vec4 ambient
  ;
  } p3d_LightModel;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

in vec2 diffuseCoord;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 diffuseTex  = texture(p3d_Texture1, diffuseCoord);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 ambient = p3d_Material.ambient * p3d_LightModel.ambient * diffuseTex;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The ambient component to the lighting model is based on the material's ambient color, the ambient light's color, and the diffuse texture color.&lt;/p&gt;
&lt;p&gt;There should only ever be one ambient light so this calculation only needs to occur once versus how the diffuse and specular color was accumulated for each spot/directional/point light.&lt;/p&gt;
&lt;p&gt;Note that you'll revisit the ambient color when performing SSAO.&lt;/p&gt;
&lt;h4&gt;Putting It All Together&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 outputColor = ambient + diffuseSpecular + p3d_Material.emission;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final color is the sum of the ambient color, diffuse color, specular color, and the emission color.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Normal Mapping&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/2dc7e5b6aa39bfec1ebf531d24038ebcab52d5aa/68747470733a2f2f692e696d6775722e636f6d2f374f36534732672e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2dc7e5b6aa39bfec1ebf531d24038ebcab52d5aa/68747470733a2f2f692e696d6775722e636f6d2f374f36534732672e676966&quot; alt=&quot;Normal Mapping&quot; data-canonical-src=&quot;https://i.imgur.com/7O6SG2g.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Normal mapping allows you to add surface details without adding any geometry. Typically, in a modeling program like Blender, you create a high poly and a low poly version of your mesh. You take the vertex normals from the high poly mesh and bake them into a texture. This texture is the normal map. Then inside the fragment shader, you replace the low poly mesh's vertex normals with the high poly mesh's normals you baked into the normal map. Now when you light your mesh, it will appear to have more polygons than it really has. This will keep your FPS high while at the same time retain most of the details from the high poly version.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/d4156070b46e944f777f1db56c7422bb3566962b/68747470733a2f2f692e696d6775722e636f6d2f6e5359394157342e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/d4156070b46e944f777f1db56c7422bb3566962b/68747470733a2f2f692e696d6775722e636f6d2f6e5359394157342e676966&quot; alt=&quot;From high to low poly with normal mapping.&quot; data-canonical-src=&quot;https://i.imgur.com/nSY9AW4.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see the progression from the high poly model to the low poly model to the low poly model with the normal map applied.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/71101ada2d89e43eaaaf86d707dd004819e3209a/68747470733a2f2f692e696d6775722e636f6d2f6a766b525045372e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/71101ada2d89e43eaaaf86d707dd004819e3209a/68747470733a2f2f692e696d6775722e636f6d2f6a766b525045372e676966&quot; alt=&quot;Normal Map Illusion&quot; data-canonical-src=&quot;https://i.imgur.com/jvkRPE7.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Keep in mind though, normal mapping is only an illusion. After a certain angle, the surface will look flat again.&lt;/p&gt;
&lt;h3&gt;Vertex&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform mat3 p3d_NormalMatrix;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

in vec3 p3d_Normal;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

in vec3 p3d_Binormal;
in vec3 p3d_Tangent;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vertexNormal = normalize(p3d_NormalMatrix * p3d_Normal);
  binormal     = normalize(p3d_NormalMatrix * p3d_Binormal);
  tangent      = normalize(p3d_NormalMatrix * p3d_Tangent);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Starting in the vertex shader, you'll need to output to the fragment shader the normal vector, binormal vector, and the tangent vector. These vectors are used, in the fragment shader, to transform the normal map normal from tangent space to view space.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;p3d_NormalMatrix&lt;/code&gt; transforms the vertex normal, binormal, and tangent vectors to view space. Remember that in view space, all of the coordinates are relative to the camera's position.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[p3d_NormalMatrix] is the upper 3x3 of the inverse transpose of the ModelViewMatrix. It is used to transform the normal vector into view-space coordinates.&lt;/p&gt;&lt;p&gt;[Source](&lt;a href=&quot;http://www.panda3d.org/manual/?title=List_of_GLSL_Shader_Inputs&quot; rel=&quot;nofollow&quot;&gt;http://www.panda3d.org/manual/?title=List_of_GLSL_Shader_Inputs&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

in vec2 p3d_MultiTexCoord0;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

out vec2 normalCoord;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  normalCoord   = p3d_MultiTexCoord0;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/50fef9d3db0d507bcf247c51b1d21de4e0d8c45e/68747470733a2f2f692e696d6775722e636f6d2f744c49413648752e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/50fef9d3db0d507bcf247c51b1d21de4e0d8c45e/68747470733a2f2f692e696d6775722e636f6d2f744c49413648752e676966&quot; alt=&quot;Normal Maps&quot; data-canonical-src=&quot;https://i.imgur.com/tLIA6Hu.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You'll also need to output, to the fragment shader, the UV coordinates for the normal map.&lt;/p&gt;
&lt;h3&gt;Fragment&lt;/h3&gt;
&lt;p&gt;Recall that the vertex normal was used to calculate the lighting. However, the normal map provides us with different normals to use when calculating the lighting. In the fragment shader, you need to swap out the vertex normals for the normals found in the normal map.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform sampler2D p3d_Texture0;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

in vec2 normalCoord;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; Find &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
  vec4 normalTex   = texture(p3d_Texture0, normalCoord);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the normal map coordinates the vertex shader sent, pull out the normal from the normal map.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec3 &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; Unpack &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
    &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt; =
      normalize
        ( normalTex.rgb
        * &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt;
        - &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;
        );

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Earlier I showed how the normals are mapped to colors to create the normal map. Now this process needs to be reversed so you can get back the original normals that were baked into the map.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
[ r, g, b] =
  [ r * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, g * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, b * &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;] =
    [ x, y, z]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's the process for unpacking the normals from the normal map.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; Transform &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
    &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt; =
      normalize
        ( mat3
            ( tangent
            , binormal
            , vertexNormal
            )
        * &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;
        );

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The normals you get back from the normal map are typically in tangent space. They could be in another space, however. For example, Blender allows you to bake the normals in tangent, object, world, or camera space.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/f624ed178a5c863b9b2da02d1bd58bdf2b6000ff/68747470733a2f2f692e696d6775722e636f6d2f457a484a5064342e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/f624ed178a5c863b9b2da02d1bd58bdf2b6000ff/68747470733a2f2f692e696d6775722e636f6d2f457a484a5064342e676966&quot; alt=&quot;Replacing the vertex normals with the normal map normals.&quot; data-canonical-src=&quot;https://i.imgur.com/EzHJPd4.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To take the normal map normal from tangent space to view pace, construct a three by three matrix using the tangent, binormal, and vertex normal vectors. Multiply the normal by this matrix and be sure to normalize it.&lt;/p&gt;
&lt;p&gt;At this point, you're done. The rest of the lighting calculations are the same.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Outlining&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/034119b7a124545225972bdca60aebc93e3b10ad/68747470733a2f2f692e696d6775722e636f6d2f32616a4e6f50552e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/034119b7a124545225972bdca60aebc93e3b10ad/68747470733a2f2f692e696d6775722e636f6d2f32616a4e6f50552e676966&quot; alt=&quot;Outlining the scene.&quot; data-canonical-src=&quot;https://i.imgur.com/2ajNoPU.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Outlining your scene's geometry can give your game a distinctive look, reminiscent of comic books and cartoons.&lt;/p&gt;
&lt;h3&gt;Material Diffuse&lt;/h3&gt;
&lt;p&gt;The outline shader needs an input texture for detecting and coloring in the edges. Candidates for this input texture include the diffuse color from materials, the colors from the diffuse textures, the normals from the vertexes, or even the colors from the normal maps.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
uniform &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; {
  vec4 diffuse;
} p3d_Material;

out vec4 fragColor;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() {
  vec3 diffuseColor = p3d_Material.&lt;span class=&quot;pl-smi&quot;&gt;diffuse&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;rgb&lt;/span&gt;;
  fragColor = &lt;span class=&quot;pl-c1&quot;&gt;vec4&lt;/span&gt;(diffuseColor, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here you see a small fragment shader that renders the geometry's material diffuse color into a framebuffer texture. This material diffuse framebuffer texture will be the input texture to the outline shader.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/702bc5a717ce83a018889f2cd9d97b471d7d96e2/68747470733a2f2f692e696d6775722e636f6d2f634b69655850642e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/702bc5a717ce83a018889f2cd9d97b471d7d96e2/68747470733a2f2f692e696d6775722e636f6d2f634b69655850642e676966&quot; alt=&quot;Material diffuse framebuffer texture.&quot; data-canonical-src=&quot;https://i.imgur.com/cKieXPd.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is the material diffuse framebuffer texture. The outline shader will detect the edges in the scene and color them in. Note that using the material diffuse color won't work unless the distinctive pieces of the scene have their own material diffuse color. The mill scene's material diffuse colors were set in Blender.&lt;/p&gt;
&lt;h3&gt;Creating The Edges&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/928af5cdcce1119778b4a236de8f7e01832a54f3/68747470733a2f2f692e696d6775722e636f6d2f377970586361522e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/928af5cdcce1119778b4a236de8f7e01832a54f3/68747470733a2f2f692e696d6775722e636f6d2f377970586361522e676966&quot; alt=&quot;Scene Edges&quot; data-canonical-src=&quot;https://i.imgur.com/7ypXcaR.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Creating the edges is similar to using the edge-detect filters in &lt;a href=&quot;https://gimp.org&quot; rel=&quot;nofollow&quot;&gt;GIMP&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this shading technique, all of the calculations take space inside a fragment shader. The vertex shader for outlining only needs to output the four vertexes for the screen shaped rectangle mesh.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform sampler2D materialDiffuseTexture;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec2 texSize  = textureSize(materialDiffuseTexture, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;).xy;
  vec2 texCoord = gl_FragCoord.xy;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Before you detect the edges, you'll need to set up the input texture you'll operate on. Since the texture is the size of the screen, you can calculate the UV coordinates knowing the fragment's coordinates and the size of the input texture.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; separation = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One parameter you can tweak, according to your taste, is the &lt;code&gt;separation&lt;/code&gt;. The larger the separation, the larger the edges or lines are.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; threshold = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 mx = vec4(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;);
  vec4 mn = vec4(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;);

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; x = -&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; y = -&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;pl-c1&quot;&gt;9&lt;/span&gt;; ++i) {
    vec4 color =
      texture
        ( materialDiffuseTexture
        , (texCoord + (&lt;span class=&quot;pl-c1&quot;&gt;vec2&lt;/span&gt;(x, y) * separation)) / texSize
        );
    mx = &lt;span class=&quot;pl-c1&quot;&gt;max&lt;/span&gt;(color, mx);
    mn = &lt;span class=&quot;pl-c1&quot;&gt;min&lt;/span&gt;(color, mn);
    x += &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (x &amp;gt;= &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;) {
      x  = -&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
      y +=  &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
    }
  }

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; alpha = ((mx.r + mx.g + mx.b) / &lt;span class=&quot;pl-c1&quot;&gt;3&lt;/span&gt;) - ((mn.r + mn.g + mn.b) / &lt;span class=&quot;pl-c1&quot;&gt;3&lt;/span&gt;);
  &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (alpha &amp;gt; threshold) { alpha = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;; }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/9371e27002c4ca89045005dac6df0872e91c48f7/68747470733a2f2f692e696d6775722e636f6d2f78414d5247686e2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/9371e27002c4ca89045005dac6df0872e91c48f7/68747470733a2f2f692e696d6775722e636f6d2f78414d5247686e2e676966&quot; alt=&quot;Making edges.&quot; data-canonical-src=&quot;https://i.imgur.com/xAMRGhn.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The edge detection technique works by finding changes in the colors of the input texture. Centered on the current fragment, it uses a 3x3 fragment window to find the brightest and darkest color among the nine samples. It then subtracts the two color's intensities giving it a difference. If there is any difference, it makes the difference one.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec3 lineRgb    = vec3(&lt;span class=&quot;pl-c1&quot;&gt;0.012&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0.014&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0.022&lt;/span&gt;);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 lineColor = vec4(lineRgb, alpha);

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

      fragColor = lineColor;

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This difference is used in the alpha channel of the outputted color. So if there is no difference, no edge or line is drawn. Where there is a difference, an edge is drawn.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; threshold = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (alpha &amp;gt; threshold) { alpha = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;; }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Feel free to adjust the threshold. Currently the threshold is zero. Anything over zero becomes an edge. But you could change this to something else. This will be particularly helpful for more noisy input textures with small differences everywhere. In the case of a noisy input texture, you'd only want to outline the large differences.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Fog&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/582e49d253b3628821fe6c3181202e453dfc1307/68747470733a2f2f692e696d6775722e636f6d2f7950626a62516c2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/582e49d253b3628821fe6c3181202e453dfc1307/68747470733a2f2f692e696d6775722e636f6d2f7950626a62516c2e676966&quot; alt=&quot;Fog&quot; data-canonical-src=&quot;https://i.imgur.com/yPbjbQl.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fog (or mist as it's called in Blender) adds atmospheric haze to a scene, providing mystique and softening pop-ins. A pop-in occurs when some geometry suddenly enters into the camera's frustum.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; p3d_FogParameters
  { vec4 color
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; start
  ; &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; end
  ;
  } p3d_Fog;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Panda3D provides a nice data structure that holds all of the fog parameters but you can pass this to your shader manually.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; fogIntensity =
      clamp
        (   ( p3d_Fog.end - vertexPosition.y)
          / ( p3d_Fog.end - p3d_Fog.start)
        , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
        , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
        );
    fogIntensity = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt; - fogIntensity;


    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The example code uses a linear model for calculating the fog's intensity as you move further away from the camera. There's also an exponential model you could use. The fog's intensity is zero before or at the start of the fog. As the vertex position gets closer to the end of the fog, the &lt;code&gt;fogIntensity&lt;/code&gt; moves closer to one. For any vertexes after the end of the fog, the &lt;code&gt;fogIntensity&lt;/code&gt; is clamped to 1.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    fragColor =
      mix
        ( outputColor
        , p3d_Fog.color
        , fogIntensity
        );

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Based on the fog intensity, mix the fog color with the output color. As &lt;code&gt;fogIntensity&lt;/code&gt; gets closer to one, you'll have less of the &lt;code&gt;outputColor&lt;/code&gt; and more of the fog color. When &lt;code&gt;fogIntensity&lt;/code&gt; reaches one, you'll have all fog color and no output color&lt;/p&gt;
&lt;h3&gt;Outline Fog&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/59ef3aeb04966032ddd9a5d3b5cab3eb2b47283e/68747470733a2f2f692e696d6775722e636f6d2f3844486a68434c2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/59ef3aeb04966032ddd9a5d3b5cab3eb2b47283e/68747470733a2f2f692e696d6775722e636f6d2f3844486a68434c2e676966&quot; alt=&quot;Outline Fog&quot; data-canonical-src=&quot;https://i.imgur.com/8DHjhCL.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

uniform sampler2D positionTexture;

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

      vec4 position = texture(positionTexture, texCoord / texSize);

      &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; fogIntensity =
        clamp
          (   ( p3d_Fog.end - position.y)
            / ( p3d_Fog.end - p3d_Fog.start)
          , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
          , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
          );
      fogIntensity = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt; - fogIntensity;

      vec4 lineWithFogColor =
        mix
          ( lineColor
          , p3d_Fog.color
          , fogIntensity
          );

      fragColor = vec4(lineWithFogColor.rgb, alpha);

      &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The outline shader also uses fog to color the edges for a more consistent look. If it didn't, it would outline geometry obscured by the fog which tends to look odd. It does, however, still outline the very outer edges of the mill scene's geometry since the edges extend out past the geometry where there are no vertex positions.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;positionTexture&lt;/code&gt; is a framebuffer texture that holds the view space vertex positions. You'll learn about this when implementing the SSAO shader.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Bloom&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/e82cecd42e03967081d6e4ac1f97018f43506f5d/68747470733a2f2f692e696d6775722e636f6d2f70763270516a4f2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/e82cecd42e03967081d6e4ac1f97018f43506f5d/68747470733a2f2f692e696d6775722e636f6d2f70763270516a4f2e676966&quot; alt=&quot;Window Bloom&quot; data-canonical-src=&quot;https://i.imgur.com/pv2pQjO.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Adding bloom to a scene can really sell the illusion of the lighting model. Light emitting objects are more believable and specular highlights get an extra dose of shimmer.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt;...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; separation = &lt;span class=&quot;pl-c1&quot;&gt;3&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;   samples    = &lt;span class=&quot;pl-c1&quot;&gt;15&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; threshold  = &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; amount     = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Feel free to tweak these parameters to your liking. Separation increases the size of the blur. Samples determines how blurred the effect is. Threshold controls what does and does not get picked up by the effect. Amount controls how much bloom is outputted.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; size  = samples;
  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; size2 = size * size;

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; x = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; y = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; value = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;

  vec4 result = vec4(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;);
  vec4 color = vec4(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; size2; ++i) {
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The technique starts off by running through a &lt;code&gt;samples&lt;/code&gt; by &lt;code&gt;samples&lt;/code&gt; window centered over the current fragment. This is similar to the window used for outlining.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    color =
      texture
        ( bloomTexture
        ,   ( gl_FragCoord.xy
            + vec2(x * separation, y * separation)
            )
          / texSize
        );

    value = ((&lt;span class=&quot;pl-c1&quot;&gt;0.3&lt;/span&gt; * color.r) + (&lt;span class=&quot;pl-c1&quot;&gt;0.59&lt;/span&gt; * color.g) + (&lt;span class=&quot;pl-c1&quot;&gt;0.11&lt;/span&gt; * color.b));
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (value &amp;lt; threshold) { color = &lt;span class=&quot;pl-c1&quot;&gt;vec4&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;); }

    result += color;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It retrieves the color from the input texture and turns the red, green, and blue values into a greyscale value. If this greyscale value is less than the threshold, it discards this color by making it solid black.&lt;/p&gt;
&lt;p&gt;As it loops through all the samples in the window, it accumulates all of their values into &lt;code&gt;result&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  result = result / size2;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After it's done gathering up the samples, it divides the sum of the color samples by the number of samples taken. The result is the average color of itself and its neighbors. By doing this for every fragment, you end up with a blurred image. This form of blurring is known as box blur.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/b99d07cde9e536641916ee2e2eb0717e0789fede/68747470733a2f2f692e696d6775722e636f6d2f6d34796564724d2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/b99d07cde9e536641916ee2e2eb0717e0789fede/68747470733a2f2f692e696d6775722e636f6d2f6d34796564724d2e676966&quot; alt=&quot;Bloom progresssion.&quot; data-canonical-src=&quot;https://i.imgur.com/m4yedrM.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see the progression of the bloom algorithm.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Screen Space Ambient Occlusion (SSAO)&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/29f0cc87885113557e781dfc9c2bbb426877f602/68747470733a2f2f692e696d6775722e636f6d2f5062677a3658312e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/29f0cc87885113557e781dfc9c2bbb426877f602/68747470733a2f2f692e696d6775722e636f6d2f5062677a3658312e676966&quot; alt=&quot;SSAO&quot; data-canonical-src=&quot;https://i.imgur.com/Pbgz6X1.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SSAO is one of those effects you never knew you needed and can't live without once you have it. It can take a scene from mediocre to wow! For fairly static scenes, you can bake ambient occlusion into a texture but for more dynamic scenes, you'll need a shader. SSAO is one of the more fairly involved shading techniques, but once you pull it off, you'll feel like a shader master.&lt;/p&gt;
&lt;p&gt;Note that &quot;screen space&quot; ambient occlusion is a bit of a misnomer as not all of the calculations are done in screen space.&lt;/p&gt;
&lt;h3&gt;Inputs&lt;/h3&gt;
&lt;p&gt;The SSAO shader will need the following inputs.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Vertex position vectors in view space.&lt;/li&gt;
&lt;li&gt;Vertex normal vectors in view space.&lt;/li&gt;
&lt;li&gt;Sample vectors in tangent space.&lt;/li&gt;
&lt;li&gt;Noise vectors in tangent space.&lt;/li&gt;
&lt;li&gt;The camera lens' projection matrix.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Position&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/2fdb49470f9cd028fd12456169f7f28f99ff6814/68747470733a2f2f692e696d6775722e636f6d2f67723749784b762e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2fdb49470f9cd028fd12456169f7f28f99ff6814/68747470733a2f2f692e696d6775722e636f6d2f67723749784b762e706e67&quot; alt=&quot;Panda3D Vertex Positions&quot; data-canonical-src=&quot;https://i.imgur.com/gr7IxKv.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Storing the vertex positions into a framebuffer texture is not a necessity. You can recreate them from the &lt;a href=&quot;http://theorangeduck.com/page/pure-depth-ssao&quot; rel=&quot;nofollow&quot;&gt;camera's depth buffer&lt;/a&gt;. This being a beginners guide, I'll avoid this optimization and keep it straight forward. Feel free to use the depth buffer, however, for your implementation.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c++&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;PT&lt;/span&gt;(Texture) depthTexture = new Texture(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;depthTexture&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;);
depthTexture-&amp;gt;&lt;span class=&quot;pl-en&quot;&gt;set_format&lt;/span&gt;(Texture::Format::F_depth_component32);

&lt;span class=&quot;pl-en&quot;&gt;PT&lt;/span&gt;(GraphicsOutput) depthBuffer = graphicsOutput-&amp;gt;make_texture_buffer(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;depthBuffer&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, depthTexture);
depthBuffer-&amp;gt;&lt;span class=&quot;pl-en&quot;&gt;set_clear_color&lt;/span&gt;(LVecBase4f(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;));

NodePath depthCameraNP = window-&amp;gt;&lt;span class=&quot;pl-en&quot;&gt;make_camera&lt;/span&gt;();
&lt;span class=&quot;pl-en&quot;&gt;DCAST&lt;/span&gt;(Camera, depthCameraNP.node())-&amp;gt;set_lens(window-&amp;gt;&lt;span class=&quot;pl-en&quot;&gt;get_camera&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;)-&amp;gt;get_lens());
&lt;span class=&quot;pl-en&quot;&gt;PT&lt;/span&gt;(DisplayRegion) depthBufferRegion = depthBuffer-&amp;gt;make_display_region(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;);
depthBufferRegion-&amp;gt;&lt;span class=&quot;pl-en&quot;&gt;set_camera&lt;/span&gt;(depthCameraNP);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you do decide to use the depth buffer, here's how you can set it up using Panda3D.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;in vec4 vertexPosition;

out vec4 fragColor;

void main() {
  fragColor = vertexPosition;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here's the simple shader used to render out the view space vertex positions into a framebuffer texture. The more involved work is setting up the framebuffer texture such that the fragment vector components it receives are not clamped to &lt;code&gt;[0, 1]&lt;/code&gt; and that each one has a high enough precision (a high enough number of bits). For example, if a particular interpolated vertex position is &lt;code&gt;&amp;lt;-139.444444566, 0.00000034343, 2.5&amp;gt;&lt;/code&gt;, you don't want it stored into the texture as &lt;code&gt;&amp;lt;0.0, 0.0, 1.0&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  FrameBufferProperties fbp = FrameBufferProperties::get_default();

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  fbp.set_rgba_bits(&lt;span class=&quot;pl-c1&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;32&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;32&lt;/span&gt;);
  fbp.set_rgb_color(&lt;span class=&quot;pl-c1&quot;&gt;true&lt;/span&gt;);
  fbp.set_float_color(&lt;span class=&quot;pl-c1&quot;&gt;true&lt;/span&gt;);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's how the example code sets up the framebuffer texture to store the vertex positions. It wants 32 bits per red, green, blue, and alpha components and disables clamping the values to &lt;code&gt;[0, 1]&lt;/code&gt; The &lt;code&gt;set_rgba_bits(32, 32, 32, 32)&lt;/code&gt; call sets the bits and also disables the clamping.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  glTexImage2D
    ( GL_TEXTURE_2D
    , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
    , GL_RGB32F
    , &lt;span class=&quot;pl-c1&quot;&gt;1200&lt;/span&gt;
    , &lt;span class=&quot;pl-c1&quot;&gt;900&lt;/span&gt;
    , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
    , GL_RGB
    , GL_FLOAT
    , nullptr
    );
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's the equivalent OpenGL call. &lt;code&gt;GL_RGB32F&lt;/code&gt; sets the bits and also disables the clamping.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If the color buffer is fixed-point, the components of the source and destination values and blend factors are each clamped to [0, 1] or [−1, 1] respectively for an unsigned normalized or signed normalized color buffer prior to evaluating the blend equation. If the color buffer is floating-point, no clamping occurs.&lt;/p&gt;&lt;p&gt;[Source](&lt;a href=&quot;https://www.khronos.org/registry/OpenGL/specs/gl/glspec44.core.pdf&quot; rel=&quot;nofollow&quot;&gt;https://www.khronos.org/registry/OpenGL/specs/gl/glspec44.core.pdf&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/fc7dfb06b061ba08a4bb2652d6b5508eacc31d35/68747470733a2f2f692e696d6775722e636f6d2f56346e45544d452e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/fc7dfb06b061ba08a4bb2652d6b5508eacc31d35/68747470733a2f2f692e696d6775722e636f6d2f56346e45544d452e706e67&quot; alt=&quot;OpenGL Vertex Positions&quot; data-canonical-src=&quot;https://i.imgur.com/V4nETME.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see the vertex positions with y being the up vector.&lt;/p&gt;
&lt;p&gt;Recall that Panda3D sets z as the up vector but OpenGL uses y as the up vector. The position shader outputs the vertex positions with z being up since Panda3D was configured with &lt;code&gt;gl-coordinate-system default&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Normal&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/994305096035bc97f5a313fd085d03fcd763f4bf/68747470733a2f2f692e696d6775722e636f6d2f696c6e626b7a712e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/994305096035bc97f5a313fd085d03fcd763f4bf/68747470733a2f2f692e696d6775722e636f6d2f696c6e626b7a712e676966&quot; alt=&quot;Panda3d Vertex Normals&quot; data-canonical-src=&quot;https://i.imgur.com/ilnbkzq.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You'll need the vertex normals to correctly orient the samples you'll take in the SSAO shader. The example code generates multiple sample vectors distributed in a hemisphere but you could use a sphere and do away with the need for normals all together.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
in vec3 vertexNormal;

out vec4 fragColor;

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;main&lt;/span&gt;() {
  vec3 &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;normalize&lt;/span&gt;(vertexNormal);

  fragColor = &lt;span class=&quot;pl-c1&quot;&gt;vec4&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Like the position shader, the normal shader is simple as well. Be sure to normalize the vertex normal and remember that they are in view space.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/caa74122db93f09f89c6f8dca1b89f047ed697c4/68747470733a2f2f692e696d6775722e636f6d2f75636478394b702e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/caa74122db93f09f89c6f8dca1b89f047ed697c4/68747470733a2f2f692e696d6775722e636f6d2f75636478394b702e676966&quot; alt=&quot;OpenGL Vertex Normals&quot; data-canonical-src=&quot;https://i.imgur.com/ucdx9Kp.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see the vertex normals with y being the up vector.&lt;/p&gt;
&lt;p&gt;Recall that Panda3D sets z as the up vector but OpenGL uses y as the up vector. The normal shader outputs the vertex positions with z being up since Panda3D was configured with &lt;code&gt;gl-coordinate-system default&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Samples&lt;/h3&gt;
&lt;p&gt;To determine the amount of ambient occlusion for any particular fragment, you'll need to sample the surrounding area.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c++&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; numberOfSamples; ++i) {
    LVecBase3f sample = &lt;span class=&quot;pl-c1&quot;&gt;LVecBase3f&lt;/span&gt;(
      &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator) * &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;,
      &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator) * &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;,
      &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator)
    ).&lt;span class=&quot;pl-c1&quot;&gt;normalized&lt;/span&gt;();

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;rand&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator);
    sample[&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;] *= &lt;span class=&quot;pl-c1&quot;&gt;rand&lt;/span&gt;;
    sample[&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;] *= &lt;span class=&quot;pl-c1&quot;&gt;rand&lt;/span&gt;;
    sample[&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;] *= &lt;span class=&quot;pl-c1&quot;&gt;rand&lt;/span&gt;;

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; scale = (&lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt;) i / (&lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt;) numberOfSamples;
    scale = &lt;span class=&quot;pl-c1&quot;&gt;lerp&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;0.1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;, scale * scale);
    sample[&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;] *= scale;
    sample[&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;] *= scale;
    sample[&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;] *= scale;

    ssaoSamples.&lt;span class=&quot;pl-c1&quot;&gt;push_back&lt;/span&gt;(sample);
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The example code generates 64 random samples distributed in a hemisphere. These &lt;code&gt;ssaoSamples&lt;/code&gt; will be sent to the SSAO shader.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c++&quot;&gt;
&lt;pre&gt;
    LVecBase3f sample = LVecBase3f(
      &lt;span class=&quot;pl-en&quot;&gt;randomFloats&lt;/span&gt;(generator) * 2.0 - 1.0,
      randomFloats(generator) * 2.0 - 1.0,
      randomFloats(generator) * 2.0 - 1.0,
    ).normalized();
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If you'd like to distribute your samples in a sphere instead, change the random z component to range from negative one to one.&lt;/p&gt;
&lt;h3&gt;Noise&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;pl-c1&quot;&gt;16&lt;/span&gt;; ++i) {
    LVecBase3f noise = &lt;span class=&quot;pl-c1&quot;&gt;LVecBase3f&lt;/span&gt;(
      &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator) * &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;,
      &lt;span class=&quot;pl-c1&quot;&gt;randomFloats&lt;/span&gt;(generator) * &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;,
      &lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;
    );

    ssaoNoise.&lt;span class=&quot;pl-c1&quot;&gt;push_back&lt;/span&gt;(noise);
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To get a good sweep of the sampled area, you'll need to generate some noise vectors. These noise vectors will rotate the samples around the top of the surface.&lt;/p&gt;
&lt;h3&gt;Ambient Occlusion&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/3a4eba6d14c1f64a936d50329421d99620548fbf/68747470733a2f2f692e696d6775722e636f6d2f5641636f634c772e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/3a4eba6d14c1f64a936d50329421d99620548fbf/68747470733a2f2f692e696d6775722e636f6d2f5641636f634c772e676966&quot; alt=&quot;SSAO Texture&quot; data-canonical-src=&quot;https://i.imgur.com/VAcocLw.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;SSAO works by sampling the view space around a fragment. The more samples that are below a surface, the darker the fragment color. These samples are positioned at the fragment and pointed in the general direction of the vertex normal. Each sample is used to look up a position in the position framebuffer texture. The position returned is compared to the sample. If the sample is farther away from the camera than the position, the sample counts towards the fragment being occluded.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/63b4c05030bdea603136a1bdcb6eba67fb5fc217/68747470733a2f2f692e696d6775722e636f6d2f7475616e3863462e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/63b4c05030bdea603136a1bdcb6eba67fb5fc217/68747470733a2f2f692e696d6775722e636f6d2f7475616e3863462e676966&quot; alt=&quot;SSAO Sampling&quot; data-canonical-src=&quot;https://i.imgur.com/tuan8cF.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here you see see the space above the surface being sampled for occlusion.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; radius     = &lt;span class=&quot;pl-c1&quot;&gt;1.1&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; bias       = &lt;span class=&quot;pl-c1&quot;&gt;0.026&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; lowerRange = -&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; upperRange =  &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Like some of the other techniques, the SSAO shader has a few control knobs you can tweak to get the exact look you're going for. The bias adds to the sample's distance from the camera. You can use the bias to combat &quot;acne&quot;. The radius increases or decreases the coverage area of the sample space. The upper and lower range changes the default range for the occlusion factor from &lt;code&gt;[0, 1]&lt;/code&gt; to whatever you choose. By increasing the range, you can increase the contrast.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 position = texture(positionTexture, texCoord);
  vec3 &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;   = texture(normalTexture, texCoord).xyz;

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;  noiseX = &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;(gl_FragCoord.x - &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;) % &lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;  noiseY = &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;(gl_FragCoord.y - &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;) % &lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;;
  vec3 random = noise[noiseX + (noiseY * &lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;)];

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Retrieve the position, normal, and random vector for later use. Recall that the example code created 16 random vectors. The random vector is chosen based on the current fragments screen position.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec3 tangent  = normalize(random - &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt; * &lt;span class=&quot;pl-en&quot;&gt;dot&lt;/span&gt;(random, &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;));
  vec3 binormal = cross(&lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;, tangent);
  mat3 tbn      = mat3(tangent, binormal, &lt;span class=&quot;pl-c1&quot;&gt;normal&lt;/span&gt;);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the random and normal vectors, assemble the tangent, binormal, and normal matrix. You'll need this matrix to transform the sample vectors from tangent space to view space.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; occlusion = NUM_SAMPLES;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; NUM_SAMPLES; ++i) {
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
  }

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;With the matrix in hand, the shader can now loop through the samples, subtracting how many are not occluded.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec3 sample = tbn * samples[i];
         sample = position.xyz + sample * radius;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the matrix, position the sample near the vertex/fragment position and scale it by the radius.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec4 offset      = vec4(sample, &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;);
         offset      = lensProjection * offset;
         offset.xyz /= offset.w;
         offset.xyz  = offset.xyz * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt;;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the sample's position in view space, transform it from view space to clip space to UV space.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
-&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
 &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt; * &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;0.5&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Recall that clip space components range from negative one to one and that UV coordinates range from zero to one. To transform clip space coordinates to UV coordinates, multiply by one half and add one half.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec4 offsetPosition = texture(positionTexture, offset.xy);

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; occluded = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (sample.y + bias &amp;lt;= offsetPosition.y) { occluded = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; } &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt; { occluded = &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;; }

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Using the offset UV coordinates, created by projecting the 3D sample onto the 2D position texture, find the corresponding position vector. This takes you from view space to clip space to UV space back to view space. The shader takes this round trip to find out if some geometry is behind, at, or in front of this sample. If the sample is in front of or at some geometry, this sample doesn't count towards the fragment being occluded. If the sample is behind some geometry, this sample counts towards the fragment being occluded.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; intensity = smoothstep(&lt;span class=&quot;pl-c1&quot;&gt;0.0&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1.0&lt;/span&gt;, radius / abs(position.y - offsetPosition.y));
    occluded *= intensity;

    occlusion -= occluded;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now weight this sample by how far it is inside or outside the radius. Finally, subtract this sample from the occlusion factor since it assumes all of the samples are occluded before the loop.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    occlusion /= NUM_SAMPLES;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    fragColor = vec4(vec3(occlusion), position.a);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Divide the occluded count by the number of samples to give you the average number of occluded samples. This is the occlusion factor. Assign the occlusion factor to the fragment's color and you're done.&lt;/p&gt;
&lt;p&gt;Note that the example code sets the alpha channel to the position framebuffer texture's alpha to avoid occluding the background.&lt;/p&gt;
&lt;h3&gt;Blurring&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/626c5f948ad013f32ba09eea360799095f7fcb55/68747470733a2f2f692e696d6775722e636f6d2f425461346e33322e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/626c5f948ad013f32ba09eea360799095f7fcb55/68747470733a2f2f692e696d6775722e636f6d2f425461346e33322e676966&quot; alt=&quot;SSAO Blur Texture&quot; data-canonical-src=&quot;https://i.imgur.com/BTa4n32.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The SSAO framebuffer texture is a bit noisy so you'll want to blur it in order to smooth it out.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; size2; ++i) {
    x = size - xCount;
    y = yCount - size;

    result +=
      texture
        ( ssaoTexture
        ,   texCoord
          + &lt;span class=&quot;pl-c1&quot;&gt;vec2&lt;/span&gt;(x * separation, y * separation)
        ).&lt;span class=&quot;pl-smi&quot;&gt;rgb&lt;/span&gt;;

    xCount -= &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (xCount &amp;lt; countMin) { xCount = countMax; yCount -= &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;; }
  }

  result = result / size2;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The SSAO blurring shader is a simple blox blur. Like the bloom shader, it passes a window over the input texture and averages each fragment with its neighbors.&lt;/p&gt;
&lt;h3&gt;Ambient Color&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec2 ssaoBlurTexSize  = textureSize(ssaoBlurTexture, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;).xy;
  vec2 ssaoBlurTexCoord = gl_FragCoord.xy / ssaoBlurTexSize;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; ssao            = texture(ssaoBlurTexture, ssaoBlurTexCoord).r;

  vec4 ambient = p3d_Material.ambient * p3d_LightModel.ambient * diffuseTex * ssao;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The final stop for SSAO is back in the lighting calculation. Here you see the occlusion factor being looked up in the SSAO framebuffer texture and then included in the ambient light calculation.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Depth Of Field&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/21332f2f7075f58eece4894d84c40de019584a38/68747470733a2f2f692e696d6775722e636f6d2f3659697979314d2e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/21332f2f7075f58eece4894d84c40de019584a38/68747470733a2f2f692e696d6775722e636f6d2f3659697979314d2e676966&quot; alt=&quot;Depth Of Field&quot; data-canonical-src=&quot;https://i.imgur.com/6Yiyy1M.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Like SSAO, depth of field is an effect you can't live without once you've used it. Artistically, you can use it to draw your viewer's eye to a certain subject. But in general, depth of field adds a lot of realism for a little bit of effort.&lt;/p&gt;
&lt;h3&gt;In Focus&lt;/h3&gt;
&lt;p&gt;The first step is to render your scene completely in focus. Be sure to render this into a framebuffer texture. This will be one of the inputs to the depth of field shader.&lt;/p&gt;
&lt;h3&gt;Out Of Focus&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 result = vec4(&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;);

  &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; size2; ++i) {
    x = size - xCount;
    y = yCount - size;

    result +=
      texture
        ( blurTexture
        ,   texCoord
          + &lt;span class=&quot;pl-c1&quot;&gt;vec2&lt;/span&gt;(x * separation, y * separation)
        );

    xCount -= &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (xCount &amp;lt; countMin) { xCount = countMax; yCount -= &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;; }
  }

  result = result / size2;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The second step is to blur the scene as if it was completely out of focus. Like bloom and SSAO, you can use a box blur. Be sure to render this out-of-focus-scene into a framebuffer texture. This will be one of the inputs to the depth of field shader.&lt;/p&gt;
&lt;h3&gt;Mixing&lt;/h3&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/c4f3b17b11072b9605bb127734399b536ae5cd07/68747470733a2f2f692e696d6775722e636f6d2f7350304a634b342e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/c4f3b17b11072b9605bb127734399b536ae5cd07/68747470733a2f2f692e696d6775722e636f6d2f7350304a634b342e676966&quot; alt=&quot;Depth Of Field Graph&quot; data-canonical-src=&quot;https://i.imgur.com/sP0JcK4.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; focalLengthSharpness = &lt;span class=&quot;pl-c1&quot;&gt;100&lt;/span&gt;;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; blurRate             = &lt;span class=&quot;pl-c1&quot;&gt;6&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Feel free to tweak these two parameters. &lt;code&gt;focalLengthSharpness&lt;/code&gt; affects how out of focus the scene is at the focal length. The smaller &lt;code&gt;focalLengthSharpness&lt;/code&gt; is, the more out of focus the scene is at the focal length. &lt;code&gt;blurRate&lt;/code&gt; affects how fast the scene blurs as it moves away from the focal length. The smaller the &lt;code&gt;blurRate&lt;/code&gt; is, the less blurry the scene is as you move away from the focal point.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 focusColor      = texture(focusTexture, texCoord);
  vec4 outOfFocusColor = texture(outOfFocusTexture, texCoord);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll need the in focus and out of focus colors.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 position = texture(positionTexture, texCoord);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll also need the vertex position in view space. You can reuse the position framebuffer texture you used for SSAO.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; blur =
    clamp
      (   pow
            ( blurRate
            , abs(position.y - focalLength.x)
            )
        / focalLengthSharpness
      , &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
      , &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;
      );

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    fragColor = mix(focusColor, outOfFocusColor, blur);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here's the actual mixing. The closer &lt;code&gt;blur&lt;/code&gt; is to one, the more it will use the &lt;code&gt;outOfFocusColor&lt;/code&gt;. Zero &lt;code&gt;blur&lt;/code&gt; means this fragment is entirely in focus. At &lt;code&gt;blur &amp;gt;= 1&lt;/code&gt;, this fragment is completely out of focus.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Posterization&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/cddc2c4b6e631bbb9ff530b39644b0ff385fd72e/68747470733a2f2f692e696d6775722e636f6d2f516a74505970382e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/cddc2c4b6e631bbb9ff530b39644b0ff385fd72e/68747470733a2f2f692e696d6775722e636f6d2f516a74505970382e676966&quot; alt=&quot;Posterization&quot; data-canonical-src=&quot;https://i.imgur.com/QjtPYp8.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Posterization or color quantization is the process of reducing the number of unique colors in an image. You can use this shader to give your game a comic book or retro look. Combine it with outlining for a true cartoon look.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; levels = &lt;span class=&quot;pl-c1&quot;&gt;8&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Feel free to tweak this parameter. The higher it is, the more colors you'll end up with. The lower it is, the less colors you'll end up with.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  vec4 texColor = texture(posterizeTexture, texCoord);

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You'll need the input color.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    vec3 grey  = vec3((texColor.r + texColor.g + texColor.b) / &lt;span class=&quot;pl-c1&quot;&gt;3.0&lt;/span&gt;);
    vec3 grey1 = grey;

    grey = floor(grey * levels) / levels;

    texColor.rgb += (grey - grey1);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This method of posterization I haven't seen before. After having come up with it, I found it produced a nicer result than the typical methods.&lt;/p&gt;
&lt;p&gt;To reduce the color palette, first convert the color to a greyscale value. Quantize the color by mapping it to one of the levels. Calculate the difference between the quantized greyscale value with the non-quantized greyscale value. Add this difference to the input color. This difference is the amount the color has to go up or down to reach the quantized greyscale value.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  fragColor = texColor;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Be sure to assign the input color to the fragment color.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Pixelization&lt;/h2&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/cda6df2a8e217e87e212c7ec1d6868baa084037b/68747470733a2f2f692e696d6775722e636f6d2f745175314b4c372e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/cda6df2a8e217e87e212c7ec1d6868baa084037b/68747470733a2f2f692e696d6775722e636f6d2f745175314b4c372e676966&quot; alt=&quot;Pixelization&quot; data-canonical-src=&quot;https://i.imgur.com/tQu1KL7.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pixelizing your 3D game can give it a interesting look and possibly save you time by not having to create all of the pixel art by hand. Combine it with the posterization for a true retro look.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; pixelSize = &lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Feel free to adjust the pixel size. The bigger the pixel size, the blockier the image will be.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/c6c9b6380369c1b260ce38585820bf1181c6bb66/68747470733a2f2f692e696d6775722e636f6d2f5746354d6d4d302e676966&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/c6c9b6380369c1b260ce38585820bf1181c6bb66/68747470733a2f2f692e696d6775722e636f6d2f5746354d6d4d302e676966&quot; alt=&quot;Pixelization Process&quot; data-canonical-src=&quot;https://i.imgur.com/WF5MmM0.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; x = &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;(gl_FragCoord.x) % pixelSize;
  &lt;span class=&quot;pl-k&quot;&gt;float&lt;/span&gt; y = &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;(gl_FragCoord.y) % pixelSize;

  x = floor(pixelSize / &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt;) - x;
  y = floor(pixelSize / &lt;span class=&quot;pl-c1&quot;&gt;2.0&lt;/span&gt;) - y;

  x = gl_FragCoord.x + x;
  y = gl_FragCoord.y + y;

  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The technique works by mapping each fragment to the center of its closest, non-overlapping pixel-sized window. These windows are laid out in a grid over the input texture. The center-of-the-window fragments determine the color for the other fragments in their window.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;

    fragColor = texture(pixelizeTexture, vec2(x, y) / texSize);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; ...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you have determined the correct fragment coordinate to use, pull its color from the input texture and assign that to the fragment color.&lt;/p&gt;
&lt;h3&gt;Source&lt;/h3&gt;
&lt;h2&gt;Attributions&lt;/h2&gt;
&lt;h2&gt;Copyright&lt;/h2&gt;
&lt;p&gt;(C) 2019 David Lettier &lt;a href=&quot;https://www.lettier.com&quot; rel=&quot;nofollow&quot;&gt;lettier.com&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Sun, 12 May 2019 23:14:32 +0000</pubDate>
<dc:creator>lettier</dc:creator>
<og:image>https://avatars2.githubusercontent.com/u/1661343?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>lettier/3d-game-shaders-for-beginners</og:title>
<og:url>https://github.com/lettier/3d-game-shaders-for-beginners</og:url>
<og:description>🎮 A step-by-step guide on how to implement SSAO, depth of field, lighting, normal mapping, and more for your 3D game. - lettier/3d-game-shaders-for-beginners</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/lettier/3d-game-shaders-for-beginners</dc:identifier>
</item>
<item>
<title>Facebook sues analytics firm Rankwave over data misuse</title>
<link>https://techcrunch.com/2019/05/10/facebook-rankwave-lawsuit/</link>
<guid isPermaLink="true" >https://techcrunch.com/2019/05/10/facebook-rankwave-lawsuit/</guid>
<description>&lt;p id=&quot;speakable-summary&quot;&gt;Facebook might have another Cambridge Analytica on its hands. In a late Friday news dump, &lt;span&gt;Facebook &lt;a href=&quot;https://newsroom.fb.com/news/2019/05/enforcing-our-platform-policies/&quot;&gt;revealed&lt;/a&gt; that today it filed a lawsuit alleging South Korean analytics firm &lt;a href=&quot;http://www.rankwave.com/&quot;&gt;Rankwave&lt;/a&gt; abused its developer platform’s data, and has refused to cooperate with a mandatory compliance audit and request to delete the data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Facebook’s lawsuit centers around Rankwave offering to help businesses build a Facebook authorization step into their apps so they can pass all the user data to Rankwave, which then analyzes biographic and behavioral traits to supply user contact info and ad targeting assistance to the business. Rankwave also apparently misused data sucked in by its own consumer app for checking your social media “influencer score”. That app could pull data about your Facebook activity such as location checkins, determine that you’ve checked into a baseball stadium, and then Rankwave could help its clients target you with ads for baseball tickets.&lt;/p&gt;
&lt;p&gt;The use of a seemingly fun app to slurp up user data and repurpose it for other business goals is strikingly similar to how Cambridge Analytica’s personality quiz app tempted millions of users to provide data about themselves and their friends.&lt;/p&gt;
&lt;div id=&quot;attachment_1825994&quot; class=&quot;wp-caption alignnone&quot; readability=&quot;32&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-1825994&quot; class=&quot;breakout wp-image-1825994&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?w=680&quot; alt=&quot;&quot; width=&quot;1000&quot; height=&quot;574&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png 1251w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?resize=150,86 150w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?resize=300,172 300w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?resize=768,441 768w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?resize=680,390 680w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Facebook-Market-Opportunity.png?resize=50,29 50w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot;/&gt;&lt;p id=&quot;caption-attachment-1825994&quot; class=&quot;wp-caption-text&quot;&gt;Rankwave touts its Facebook data usage in this 2014 pitch deck&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://techcrunch.com/wp-content/uploads/2019/05/TechCrunch-Facebook-Rankwave-Lawsuit.pdf&quot;&gt;TechCrunch has attained a copy of the lawsuit&lt;/a&gt; that alleges that Rankwave misused Facebook data outside of the apps where it was collected, purposefully delayed responding to a cease-and-desist order, claimed it didn’t violate Facebook policy, lied about not using its apps since 2018 when they were accessed in April 2019, and then refused to comply with a mandatory audit of its data practices. Facebook Platform data is not supposed to be repurposed for other business goals, only for the developer to improve their app’s user experience.&lt;/p&gt;
&lt;p&gt;“By filing the lawsuit, we are sending a message to developers that Facebook is serious about enforcing our policies, including requiring developers to cooperate with us during an investigation” Facebook’s director of platform enforcement and litigation Jessica Romero wrote. Facebook tells TechCrunch that “To date Rankwave has not participated in our investigation and we are trying to get more info from them to determine if there was any misuse of Pages data.” We’ve reached out to Rankwave for its response.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;breakout aligncenter wp-image-1826004&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?w=680&quot; alt=&quot;&quot; width=&quot;1000&quot; height=&quot;719&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png 1046w, https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?resize=150,108 150w, https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?resize=300,216 300w, https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?resize=768,552 768w, https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?resize=680,489 680w, https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave-Lawsuit.png?resize=50,36 50w&quot; sizes=&quot;(max-width: 1000px) 100vw, 1000px&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;Cambridge Analytic-ish&lt;/h2&gt;
&lt;p&gt;Facebook’s lawsuit details that “Rankwave used the Facebook data associated with Rankwave’s apps to create and sell advertising and marketing analytics and models — which violated Facebook’s policies and terms” and that it “failed to comply with Facebook’s requests for proof of Rankwave’s compliance with Facebook policies, including an audit.” Rankwave apparently accessed data from over thirty apps, including those created by its clients.&lt;/p&gt;
&lt;p&gt;Specifically, Facebook cites that its “Platform Policies largely restrict Developers from using Facebook data outside of the environment of the app, for any purpose other than enhancing the app users’ experience on the app.” But Rankwave allegedly used Facebook data outside those apps.&lt;/p&gt;
&lt;div id=&quot;attachment_1826046&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-1826046&quot; class=&quot;size-large wp-image-1826046&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?w=680&quot; alt=&quot;&quot; width=&quot;680&quot; height=&quot;308&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png 1977w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?resize=150,68 150w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?resize=300,136 300w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?resize=768,348 768w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?resize=680,308 680w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Rank.cloud-data-extraction.png?resize=50,23 50w&quot; sizes=&quot;(max-width: 680px) 100vw, 680px&quot;/&gt;&lt;p id=&quot;caption-attachment-1826046&quot; class=&quot;wp-caption-text&quot;&gt;Rankwave describes how it extracts contact info and ad targeting data from Facebook data&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Facebook’s suit claims that “Rankwave’s B2B apps were installed and used by businesses to track and analyze activity on their Facebook Pages . . . Rankwave operated a consumer app called the ‘Rankwave App.’ This consumer app was designed to measure the app user’s popularity on Facebook by analyzing the level of interaction that other users had with the app user’s Facebook posts. On its website, Rankwave claimed that this app calculated a user’s ‘Social influence score’ by ‘evaluating your social activities’ and receiving ‘responses from your friends.'”&lt;/p&gt;
&lt;p&gt;TechCrunch has found that Rankwave still offers an &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.rankwave&amp;amp;hl=en_US&quot;&gt;Android app&lt;/a&gt; that asks for you to login with Facebook so it can assess the popularity of your posts and give you a “Social Influencer Score”. &lt;a href=&quot;https://techcrunch.com/2015/04/28/facebook-api-shut-down/&quot;&gt;Until 2015 when Facebook tightened its policies&lt;/a&gt;, this kind of app could ingest not only a user’s own data but that about their Facebook friends. As with Cambridge Analytica, this likely massively compounded Rankwave’s total data access.&lt;/p&gt;
&lt;div id=&quot;attachment_1826044&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;33&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-1826044&quot; class=&quot;wp-image-1826044 size-large&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?w=680&quot; alt=&quot;&quot; width=&quot;680&quot; height=&quot;394&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png 1116w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?resize=150,87 150w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?resize=300,174 300w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?resize=768,445 768w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?resize=680,394 680w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-App.png?resize=50,29 50w&quot; sizes=&quot;(max-width: 680px) 100vw, 680px&quot;/&gt;&lt;p id=&quot;caption-attachment-1826044&quot; class=&quot;wp-caption-text&quot;&gt;Rankwave’s Android app asks for users’ Facebook data in exchange for providing them a Social Influencer Score&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;Facebook Delays Coming After Rankwave&lt;/h2&gt;
&lt;p&gt;Founded in 2012 by Sungwha Shim, Rankwave came into Facebook’s crosshairs in June 2018 after it was sold to a Korean entertainment company in May 2017. Facebook assesses that the value of its data at the time of the buyout was $9.8 million.&lt;/p&gt;
&lt;p&gt;Worryingly, Facebook didn’t reach out to Rankwave until January 2019 for information proving it complied with the social network’s policies. After receiving no response, Facebook issued a cease-and-desist order in February, which Rankwave replied to seeking more time because it’s CTO had resigned, which Facebook calls “false representations”. Later that month, Rankwave denied violating Facebook’s policies but refused to provide proof. Facebook gave it more time to provide proof, but Rankwave didn’t respond. Facebook has now shut down Rankwave’s apps.&lt;/p&gt;
&lt;div id=&quot;attachment_1826064&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;img aria-describedby=&quot;caption-attachment-1826064&quot; class=&quot;wp-image-1826064 size-large&quot; src=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?w=680&quot; alt=&quot;&quot; width=&quot;680&quot; height=&quot;365&quot; srcset=&quot;https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png 1352w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?resize=150,81 150w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?resize=300,161 300w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?resize=768,412 768w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?resize=680,365 680w, https://techcrunch.com/wp-content/uploads/2019/05/Rankwave-Targeting-Data.png?resize=50,27 50w&quot; sizes=&quot;(max-width: 680px) 100vw, 680px&quot;/&gt;&lt;p id=&quot;caption-attachment-1826064&quot; class=&quot;wp-caption-text&quot;&gt;Rankwave claims to be able to extract a wide array of ad targeting data from Facebook data&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now Facebook is seeking money to cover the $9.8 million value of the data, additional monetary damages and legal fees, plus injunctive relief restraining Rankwave from accessing the Facebook Platform, requiring it to comply with Facebook’s audit, requiring that it delete all Facebook data.&lt;/p&gt;
&lt;p&gt;The fact that Rankwave was openly promoting these services that blatantly violate Facebook’s policies casts further doubt on how the social network was policing its platform. And the six month delay between Facebook identifying a potential issue with Rankwave and it even reaching out for information, plus another several months before it blocked Rankwave’s app shows a failure to move swiftly to enforce its policies. These blunders might explain why Facebook buried the news by announcing it on a Friday afternoon when many reporters and readers have already signed off for the weekend.&lt;/p&gt;
&lt;p&gt;For now there’s no evidence of wholesale transfer of Rankwave’s data to other parties or its misuse for especially nefarious purposes like influencing an election as with Cambridge Analytica. The lawsuit merely alleges data was wrongly harnessed to make money, which may not spur the same level of backlash. But the case further proves that Facebook was too busy growing itself thanks to the platform to properly safeguard it against abuse.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;You can learn more about Rankwave’s analytics practices from this 2014 presentation.&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 12 May 2019 18:46:53 +0000</pubDate>
<dc:creator>JumpCrisscross</dc:creator>
<og:title>Facebook sues analytics firm Rankwave over data misuse – TechCrunch</og:title>
<og:description>Facebook might have another Cambridge Analytica on its hands. In a late Friday news dump, Facebook revealed that today it filed a lawsuit alleging South Korean analytics firm Rankwave abused its developer platform’s data, and has refused to cooperate with a mandatory compliance audit and requ…</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2019/05/Facebook-Rankwave.png?w=740</og:image>
<og:url>http://social.techcrunch.com/2019/05/10/facebook-rankwave-lawsuit/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2019/05/10/facebook-rankwave-lawsuit/</dc:identifier>
</item>
<item>
<title>Senate Testimony on Privacy Rights and Data Collection in a Digital Economy</title>
<link>https://idlewords.com/talks/senate_testimony.2019.5.htm</link>
<guid isPermaLink="true" >https://idlewords.com/talks/senate_testimony.2019.5.htm</guid>
<description>&lt;p&gt;&lt;a href=&quot;http://idlewords.com/&quot;&gt;Idle Words&lt;/a&gt; &amp;gt; &lt;a href=&quot;http://idlewords.com/talks/&quot;&gt;Talks&lt;/a&gt; &amp;gt; Privacy Rights and Data Collection in a Digital Economy (Senate hearing)&lt;/p&gt;&lt;div readability=&quot;776.51170966521&quot;&gt;

&lt;p&gt;Thank you for the opportunity to address you today.&lt;/p&gt;
&lt;p&gt;I am the founder and sole employee of &lt;a href=&quot;https://pinboard.in&quot;&gt;Pinboard&lt;/a&gt;, a small for-profit archiving service founded in 2009 that competes in part on the basis of personal privacy. I have also been a frequent critic of Silicon Valley’s reliance on business models requiring mass surveillance, &lt;a href=&quot;https://idlewords.com/talks/&quot;&gt;speaking on the topic&lt;/a&gt; at conferences both in the United States and abroad.&lt;/p&gt;
&lt;p&gt;As someone who earns his living through data collection, I am acutely aware of the power the tools we are building give us over our fellow citizens’ private lives, and the danger they pose to our liberty. I am grateful to Chairman Crapo, ranking member Brown, and the committee for the opportunity to testify on this vital matter.&lt;/p&gt;
&lt;p&gt;The internet economy in 2019 is dominated by five American tech companies: Apple, Microsoft, Google, Facebook, and Amazon. These are also the five most valuable corporations in the world, with a combined market capitalization exceeding four trillion dollars. &lt;/p&gt;
&lt;p&gt;Between them, these companies control the market for online advertising, mobile and desktop operating systems, office software, document storage, search, cloud computing, and many other areas of the digital economy. They also own and operate a significant portion of the physical infrastructure of the internet, and act as its &lt;em&gt;de facto&lt;/em&gt; regulating authority.&lt;/p&gt;
&lt;p&gt;The concentration of power in the hands of these giant firms is the epilogue to a spectacular story of American innovation and dynamism. The technologies underpinning the internet were all developed here in the United States, and the many fortunes that they produced owe their thanks to fruitful cooperation between government, industry, and the research community. Working together, the public and private sectors created the conditions for a startup culture unlike any other in the world.&lt;/p&gt;
&lt;p&gt;Today, however, that culture of dynamism is at risk. The surveillance business model has eroded user trust to such a point that it is impeding our ability to innovate.&lt;/p&gt;
&lt;p&gt;In many ways, the five internet giants operate like sovereign states. Their operations are global, and decisions they take unilaterally can affect entire societies. Denmark has gone so far as to &lt;a href=&quot;https://www.mercurynews.com/2019/04/04/denmarks-tech-ambassador-thinks-silicon-valley-could-learn-a-few-things-from-his-country/&quot;&gt;send an ambassador to Silicon Valley&lt;/a&gt;. When Jeff Bezos, the CEO of Amazon, met recently with the Canadian prime minister, the occasion was &lt;a href=&quot;https://www.reuters.com/article/us-usa-canada-trudeau-amazon-com/canadas-trudeau-to-meet-amazon-ceo-bezos-during-u-s-visit-idUSKBN1FR2KT&quot;&gt;covered in the press like a state visit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The emergence of this tech oligopoly reflects a profound shift in our society, the migration of every area of commercial, social, and personal life into an online realm where human interactions are mediated by software.&lt;/p&gt;
&lt;p&gt;To an extent that has no precedent, the daily activities of most Americans are now tracked and permanently recorded by automated systems. It is likely that every person in this hearing room carries with them a mobile phone that keeps a history of their location, is privy to their most private conversations, and contains a rich history of their private life. Some of you may even have an always-on microphone in your car or home that responds to your voice commands.&lt;/p&gt;
&lt;p&gt;Emerging technologies promise to afford these systems even more intimate glimpses into our private lives—phones that monitor our facial expressions as we read, and connected homes that watch over us while we sleep. Scenarios that were once the province of dystopian dime fiction have become an unremarkable consumer reality.&lt;/p&gt;
&lt;p&gt;The sudden ubiquity of this architecture of mass surveillance, and its enshrinement as the default business model of the online economy, mean that we can no longer put off hard conversations about the threats it poses to liberty.&lt;/p&gt;
&lt;p&gt;Adding to this urgency is the empirical fact that, while our online economy depends on the collection and permanent storage of highly personal data, we do not have the capacity to keep such large collections of user data safe over time.&lt;/p&gt;
&lt;p&gt;The litany of known data breaches is too long to recite here, but includes every one of the top five tech companies, as well as health and financial firms and government agencies. Every year brings new and more spectacular examples of our inability to protect our users. At Yahoo, an internet giant at the time with a world-class security team, over 3 billion user accounts were &lt;a href=&quot;https://www.nytimes.com/2017/10/03/technology/yahoo-hack-3-billion-users.html&quot;&gt;compromised in a 2013 breach&lt;/a&gt;. In 2015, the US Office of Personnel Management allowed &lt;a href=&quot;https://en.wikipedia.org/wiki/Office_of_Personnel_Management_data_breach&quot;&gt;unauthorized access to the records of over four million people&lt;/a&gt;, including many with highly sensitive security clearances. And in 2017, &lt;a href=&quot;https://en.wikipedia.org/wiki/Equifax#May%E2%80%93July_2017_data_breach&quot;&gt;Equifax exposed data&lt;/a&gt;, including social security numbers, on 147 million Americans, nearly half the US population.&lt;/p&gt;
&lt;p&gt;While many individual data breaches are due to negligence or poor practices, their overall number reflects an uncomfortable truth well known to computer professionals—that our ability to attack computer systems far exceeds our ability to defend them, and will for the foreseeable future.&lt;/p&gt;
&lt;p&gt;The current situation, therefore, is not tenable. The internet economy today resembles the earliest days of the nuclear industry. We have a technology of unprecedented potential, we have made glowing promises about how it will transform the daily lives of our fellow Americans, but we don’t know how to keep its dangerous byproducts safe.&lt;/p&gt;

&lt;p&gt;Discussing privacy in the context of regulation can be vexing, because the companies doing the most to erode our privacy are equally sincere in their conviction that they are its champions.&lt;/p&gt;
&lt;p&gt;The confusion stems from two different ways in which we use the word privacy, leading us to sometimes talk past each other.&lt;/p&gt;
&lt;p&gt;In the regulatory context, discussion of privacy invariably means data privacy—the idea of protecting designated sensitive material from unauthorized access.&lt;/p&gt;
&lt;p&gt;Laws like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Health_Insurance_Portability_and_Accountability_Act&quot;&gt;Health Insurance Portability and Accountability Act&lt;/a&gt; (HIPAA) and the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gramm%E2%80%93Leach%E2%80%93Bliley_Act&quot;&gt;Gramm-Leach-Bliley Act&lt;/a&gt; (GLBA) delimit certain categories of sensitive information that require extra protection, and mandate ways in which health and financial institutions have to safeguard this data, or report when those safeguards have failed. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Children%27s_Online_Privacy_Protection_Act&quot;&gt;Children's Online Privacy Protection Act&lt;/a&gt; of 1998 extends similar protection to all data associated with children.&lt;/p&gt;
&lt;p&gt;We continue to use this framework of data privacy today, including in the recently enacted &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;&gt;General Data Protection Regulation&lt;/a&gt; (GDPR).&lt;/p&gt;
&lt;p&gt;It is true that, when it comes to protecting specific collections of data, the companies that profit most from the surveillance economy are the ones working hardest to defend them against unauthorized access.&lt;/p&gt;
&lt;p&gt;But there is a second, more fundamental sense of the word privacy, one which until recently was so common and unremarkable that it would have made no sense to try to describe it.&lt;/p&gt;
&lt;p&gt;That is the idea that there exists a sphere of life that should remain outside public scrutiny, in which we can be sure that our words, actions, thoughts and feelings are not being indelibly recorded. This includes not only intimate spaces like the home, but also the many semi-private places where people gather and engage with one another in the common activities of daily life—the workplace, church, club or union hall. As these interactions move online, our privacy in this deeper sense withers away.&lt;/p&gt;
&lt;p&gt;Until recently, even people living in a police state could count on the fact that the authorities didn’t have enough equipment or manpower to observe everyone, everywhere, and so enjoyed more freedom from monitoring than we do living in a free society today. .&lt;/p&gt;
&lt;p&gt;A characteristic of this new world of ambient surveillance is that we cannot opt out of it, any more than we might opt out of automobile culture by refusing to drive. However sincere our commitment to walking, the world around us would still be a world built for cars. We would still have to contend with roads, traffic jams, air pollution, and run the risk of being hit by a bus.&lt;/p&gt;
&lt;p&gt;Similarly, while it is possible in principle to throw one’s laptop into the sea and renounce all technology, it is no longer be possible to opt out of a surveillance society.&lt;/p&gt;
&lt;p&gt;When we talk about privacy in this second, more basic sense, the giant tech companies are not the guardians of privacy, but its gravediggers.&lt;/p&gt;
&lt;p&gt;The tension between these interpretations of what privacy entails, and who is trying to defend it, complicates attempts to discuss regulation.&lt;/p&gt;
&lt;p&gt;Tech companies will correctly point out that their customers have willingly traded their private data for an almost miraculous collection of useful services, services that have unquestionably made their lives better, and that the business model that allows them to offer these services for free creates far more value than harm for their customers.&lt;/p&gt;
&lt;p&gt;Consumers will just as rightly point out that they never consented to be the subjects in an uncontrolled social experiment, that the companies engaged in reshaping our world have consistently refused to honestly discuss their business models or data collection practices, and that in a democratic society, profound social change requires consensus and accountability.&lt;/p&gt;

&lt;p&gt;Further complicating the debate on privacy is the novel nature of the data being collected. While the laws around protecting data have always focused on intentional communications—documents that can be intercepted, conversations that can be eavesdropped upon—much of what computer systems capture about us is behavioral data: incidental observations of human behavior that don’t seem to convey any information at all.&lt;/p&gt;
&lt;p&gt;Behavioral data encompasses anything people do while interacting with a computer system. It can include the queries we type into a search engine, our physical location, the hyperlinks we click on, whether we are sitting or standing, how quickly we scroll down a document, how jauntily we walk down a corridor, whether our eyes linger on a photo, whether we start to write a comment and then delete it—even the changes in our facial expression as we are shown an online ad.&lt;/p&gt;
&lt;p&gt;This incidental data has proven to be such a valuable raw material that an entire industry now specializes in finding ways to mine it. The devices used to spy on us include our computers, cell phones, televisions, cars, security cameras, our children’s toys, home appliances, wifi access points, even at one point &lt;a href=&quot;https://gizmodo.com/brave-new-garbage-londons-trash-cans-track-you-using-1071610114&quot;&gt;trash cans in the street&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The extent to which anyone consents—or can consent—to this kind of tracking is the thorny question in attempting to regulate the relationship between people and software.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/General_Data_Protection_Regulation&quot;&gt;General Data Protection Regulation&lt;/a&gt; (GDPR), enacted in May of 2018, is the most ambitious attempt thus far to regulate online privacy. It takes a very traditional view of the relationship between people and data.&lt;/p&gt;
&lt;p&gt;In the eyes of the GDPR, people own their data. They make an affirmative choice to share their data with online services, and can revoke that choice. The consent they give must be explicit and limited to a specified purpose—the recipient does not have carte blanche to use the data as they please, or to share it with third parties, with some complicating caveats.&lt;/p&gt;
&lt;p&gt;People have the right to request a full download of their data from the services they have entrusted it to, and they have the right to demand that it be permanently erased.&lt;/p&gt;
&lt;p&gt;The GDPR imposes a notification requirement for data breaches, and requires affirmative consent for the sale of user data. It also restricts the movement of data to outside jurisdictions (though in the case of the United States, this restriction is superseded by the US-EU Privacy Shield framework).&lt;/p&gt;
&lt;p&gt;Finally, the GDPR mandates that privacy safeguards like &lt;a href=&quot;https://en.wikipedia.org/wiki/Tokenization_(data_security)&quot;&gt;data tokenization&lt;/a&gt; and encryption be built in to new systems, and that companies appoint a dedicated privacy officer.&lt;/p&gt;
&lt;p&gt;The GDPR is not a simple regulation, and many of its most potentially significant provisions (such as the scope of a data controller’s ‘legitimate interests’, or what the right to erasure means in the context of a machine learning model) await interpretation by regulators.&lt;/p&gt;
&lt;p&gt;What limits, if any, the GDPR will place on the application of machine learning is a particularly important open question. The law on its face prohibits automated decision making that has a “legal or similarly significant effect” on data subjects, but the definition of “significant effect” is not clear, nor is it clear whether having a human being simply countersign an algorithmic decision would be enough to satisfy regulators that the decision process is not fully automated.&lt;/p&gt;

&lt;p&gt;As it is so new, the GDPR’s ultimate impact on online privacy in the EU is unclear. Some of the dramatic early impacts (like major US newspapers &lt;a href=&quot;https://venturebeat.com/2018/05/25/gdpr-claims-its-first-victims-u-s-newspapers/&quot;&gt;going offline&lt;/a&gt;) have proven to be transient, while many of the biggest impacts hinge on future decisions by EU regulators.&lt;/p&gt;
&lt;p&gt;Enough has happened, however, to draw some preliminary conclusions.&lt;/p&gt;
&lt;p&gt;The GDPR so far has made life hard on internet users. It is not clear that this is the GDPR’s fault.&lt;/p&gt;
&lt;p&gt;The plain language of the GDPR is so plainly at odds with the business model of surveillance advertising that contorting the real-time ad brokerages into something resembling compliance has required acrobatics that have left essentially everybody unhappy.&lt;/p&gt;
&lt;p&gt;The leading ad networks in the European Union have chosen to respond to the GDPR by stitching together a sort of Frankenstein’s monster of consent, a mechanism whereby a user wishing to visit, say, a weather forecast page is first prompted to agree to share data with a consortium of 119 entities, including the aptly named “A Million Ads” network. The user can scroll through this list of intermediaries one by one, or give or withhold consent en bloc, but either way she must wait a further two minutes for the consent collection process to terminate before she is allowed to find out whether or not it is going to rain.&lt;/p&gt;
&lt;p&gt;This majestically baroque consent mechanism also hinders Europeans from using the privacy preserving features built into their web browsers, or from turning off invasive tracking technologies like third-party cookies, since the mechanism depends on their being present.&lt;/p&gt;
&lt;p&gt;For the average EU citizen, therefore, the immediate effect of the GDPR has been to add friction to their internet browsing experience along the lines of the infamous 2011 EU Privacy Directive (“&lt;a href=&quot;https://en.wikipedia.org/wiki/Privacy_and_Electronic_Communications_Directive_2002&quot;&gt;EU cookie law&lt;/a&gt;”) that added consent dialogs to nearly every site on the internet.&lt;/p&gt;
&lt;p&gt;The GDPR rollout has also demonstrated to what extent the European ad market depends on Google, who has assumed the role of de facto technical regulatory authority due to its overwhelming market share. Google waited until the night before the regulation went into effect to announce its intentions, leaving ad networks scrambling.&lt;/p&gt;
&lt;p&gt;It is significant that Google and Facebook also took advantage of the US-EU privacy shield to &lt;a href=&quot;https://www.theguardian.com/technology/2018/apr/19/facebook-moves-15bn-users-out-of-reach-of-new-european-privacy-law&quot;&gt;move 1.5 billion non-EU user records&lt;/a&gt; out of EU jurisdiction to servers in the United States. Overall, the GDPR has significantly strengthened Facebook and Google at the expense of smaller players in the surveillance economy.&lt;/p&gt;
&lt;p&gt;The data protection provisions of the GDPR, particularly the right to erase, imposed significant compliance costs on internet companies. In some cases, these compliance costs just show the legislation working as intended. Companies who were not keeping adequate track of personal data were forced to retrofit costly controls, and that data is now safer for it.&lt;/p&gt;
&lt;p&gt;But in other cases, companies with a strong commitment to privacy also found themselves expending significant resources on retooling. Personally identifying information has a way of seeping in to odd corners of computer systems (for example, users will sometimes accidentally paste their password into a search box), and tracking down all of these special cases can be challenging in a complex system. The requirements around erasure, particularly as they interact with backups, also impose a special burden, as most computer systems are designed with a bias to never losing data, rather than making it easy to expunge.&lt;/p&gt;
&lt;p&gt;A final, and extremely interesting outcome of the GDPR, was an inadvertent experiment conducted by the New York Times. Privacy advocates have long argued that intrusive third-party advertising does not provide more value to publishers than the traditional pre-internet style of advertising based off of content, but there has never been a major publisher willing to publicly run the experiment.&lt;/p&gt;
&lt;p&gt;The New York Times tested this theory by &lt;a href=&quot;https://digiday.com/media/gumgumtest-new-york-times-gdpr-cut-off-ad-exchanges-europe-ad-revenue/&quot;&gt;cutting off all ad networks in Europe&lt;/a&gt;, and running only direct sold ads to its European visitors. The paper found that ad revenue increased significantly, and stayed elevated into 2019, bolstering the argument that surveillance-based advertising offers no advantage to publishers, and may in fact harm them.&lt;/p&gt;

&lt;p&gt;While it is too soon to draw definitive conclusions about the GDPR, there is a tension between its concept of user consent and the reality of a surveillance economy that is worth examining in more detail.&lt;/p&gt;
&lt;p&gt;A key assumption of the consent model is any user can choose to withhold consent from online services. But not all services are created equal—there are some that you really can’t say no to.&lt;/p&gt;
&lt;p&gt;Take the example of Facebook. Both landlords and employers in the United States have begun &lt;a href=&quot;https://www.washingtonpost.com/news/the-intersect/wp/2016/06/09/creepy-startup-will-help-landlords-employers-and-online-dates-strip-mine-intimate-data-from-your-facebook-page/&quot;&gt;demanding to see Facebook accounts as a condition of housing or employment&lt;/a&gt;. The United States Border Patrol has &lt;a href=&quot;https://www.federalregister.gov/documents/2016/06/23/2016-14848/agency-information-collection-activities-arrival-and-departure-record-forms-i-94-and-i-94w-and#h-11&quot;&gt;made a formal request&lt;/a&gt; to begin collecting social media to help vet people arriving in the country. In both those contexts, not having a Facebook account might stand out too much to be a viable option. Many schools now communicate with parents via Facebook; Facebook groups are also the locus for political organizing and online activism across the political spectrum.&lt;/p&gt;
&lt;p&gt;Analogous arguments can be made for social products offered by the other major tech companies. But if you can’t afford to opt out, what does it mean to consent?&lt;/p&gt;
&lt;p&gt;Opting out can also be impossible because of how deeply the internet giants have embedded themselves in the fabric of the internet. For example, major media properties in the EU use a technology called ReCaptcha on their GDPR consent forms. These forms must be completed before a user can access the website they are gathering consent for, but since the ReCaptcha service is run by Google, and the form cannot be submitted without completing the Google-generated challenge (which incidentally performs free image classification labor for the company), a user who refuses to give Google access to her browser will find herself denied access to a large portion of the internet.&lt;/p&gt;
&lt;p&gt;While this specific example may change when it comes to the attention of an EU regulator, the broader issue remains. The sheer reach of the tech oligopoly makes it impossible to avoid using their services. When a company like Google controls the market-leading browser, mobile operating system, email service and analytics suite, exercises a monopoly over search in the EU, runs the largest ad network in Europe, and happens to &lt;a href=&quot;https://broadbandnow.com/report/google-content-providers-submarine-cable-ownership/&quot;&gt;own many of the undersea cables&lt;/a&gt; that connect Europe to the rest of the world, how do you possibly say ‘no’?&lt;/p&gt;

&lt;p&gt;Beyond one’s basic ability to consent, there is the question of what it means to give informed consent. Presumably we are not opting in or out of the services we use for capricious reasons, but because we can make a rational choice about what is in our interest.&lt;/p&gt;
&lt;p&gt;In practice, however, obtaining this information is not possible, even assuming superhuman reserves of patience.&lt;/p&gt;
&lt;p&gt;For example, anyone visiting the popular &lt;a href=&quot;https://en.wikipedia.org/wiki/Tumblr&quot;&gt;Tumblr blogging platform&lt;/a&gt; from a European IP address must first decide whether to share data with Tumblr’s 201 advertising partners, and read five separate privacy policies from Tumblr’s several web analytics providers.&lt;/p&gt;
&lt;p&gt;Despite being a domain expert in the field, and spending an hour clicking into these policies, I am unable to communicate what it is that Tumblr is tracking, or what data of mine will be used for what purposes by their data partners (each of whom has its own voluminous terms of service). This opacity exists in part because the intermediaries have fought hard to keep their business practices and data sharing processes a secret, even in the teeth of strong European regulation.&lt;/p&gt;
&lt;p&gt;Organizations like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Interactive_Advertising_Bureau&quot;&gt;Interactive Advertising Bureau Europe&lt;/a&gt; (IABE) defeat the spirit of the GDPR by bundling consent and requiring it across many ad-supported properties in Europe. If regulators block the bundling in its current incarnation, it will no doubt rise from the dead in a modified form, reflecting the undying spirit of surveillance advertising. But at no point will internet users have the information they would need to make a truly informed choice (leaving aside the ridiculousness of requiring a legal education and two hours of sustained close reading in order to watch a cat video).&lt;/p&gt;


&lt;p&gt;Finally, there is a sense in which machine learning and the power of predictive inference may be making the whole idea of consent irrelevant. At this point, companies have collected so much data about entire populations that they can simply make guesses about us, often with astonishing accuracy.&lt;/p&gt;
&lt;p&gt;A useful analogy here is a jigsaw puzzle. If you give me a puzzle with one piece missing, I can still assemble it, reconstruct the contours of the missing piece by looking at the shape of the pieces around it and, if the piece is small compared to the whole, easily interpolate the missing part of the image.&lt;/p&gt;
&lt;p&gt;This is exactly what computer systems do to us when we deny them our personal information. Experts have long known that it takes a very small amount of data to make reliable inferences about a person. Most people in the United States, for example, &lt;a href=&quot;https://dataprivacylab.org/projects/identifiability/paper1.pdf&quot;&gt;can be uniquely identified&lt;/a&gt; by just the combination of their date of birth, gender, and zip code.&lt;/p&gt;
&lt;p&gt;But machine learning is honing this ability to fill in the blanks to surprising levels of accuracy, raising troubling questions about what it means to have any categories of protected data at all.&lt;/p&gt;
&lt;p&gt;For example, imagine that an algorithm could inspect your online purchasing history and, with high confidence, infer that you suffer from an anxiety disorder. Ordinarily, this kind of sensitive medical information would be protected by HIPAA, but is the inference similarly protected? What if the algorithm is only reasonably certain? What if the algorithm knows that you’re healthy now, but will suffer from such a disorder in the future?&lt;/p&gt;
&lt;p&gt;The question is not hypothetical—&lt;a href=&quot;https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0110-z&quot;&gt;a 2017 study&lt;/a&gt; showed that a machine learning algorithm examining photos posted to the image-sharing site Instagram was able to detect signs of depression before it was diagnosed in the subjects, and outperformed medical doctors on the task.&lt;/p&gt;
&lt;p&gt;The paradigm of automatic ownership of personal data does not mesh well with a world where such private data can not only interpolated and reconstructed, but independently discovered by an algorithm!&lt;/p&gt;
&lt;p&gt;And if I can infer such important facts about your life by applying machine learning to public data, then I have deprived you of privacy just as effectively as I would have by direct eavesdropping.&lt;/p&gt;
&lt;p&gt;In order to talk meaningfully about consent in online systems, the locus of regulation will need to expand beyond data collection, to cover how those data collections, and the algorithms trained on them, are used. But to do this, we will first need far greater visibility into the workings of surveillance-dependent tech companies than they have so far been willing to grant us.&lt;/p&gt;
&lt;p&gt;As it stands, the consent framework exemplified in the GDPR is simply not adequate to safeguard privacy. As much as we would like to be the masters of our data, we are not. And the real masters aren’t talking.&lt;/p&gt;

&lt;p&gt;Absent a clear understanding of how our data is being used, and the role it plays in surveillance-based business models, it is hard to lay out a specific regulatory program.&lt;/p&gt;
&lt;p&gt;Nevertheless, there are some general goals we can pursue based on the experience of regulation attempts in Europe, and what we know about the surveillance economy.&lt;/p&gt;

&lt;p&gt;Privacy regulation should be understandable, both for users of the technology, and for the companies the regulations govern. Users especially should not be required to make complex and irrevocable decisions about privacy. To the extent possible, intuitions about privacy from the human world (“a casual conversation between friends is not recorded forever”) should carry over into the digital world.&lt;/p&gt;

&lt;p&gt;At the risk of sounding tautological, privacy regulation should not punish people for seeking privacy. It should not be necessary to turn on invasive tracking technologies in one’s browser in order to express the desire to not to be tracked.&lt;/p&gt;

&lt;p&gt;Knowing that we lack the capacity to keep data collections safe over time, we can reduce the potential impact of any breach by setting strict lifetimes for behavioral data.&lt;/p&gt;
&lt;p&gt;Google has demonstrated the feasibility of this approach with their &lt;a href=&quot;https://www.blog.google/technology/safety-security/automatically-delete-data/&quot;&gt;recent announcement&lt;/a&gt; that users will be able to set their account to automatically delete location data after three or 18 months. This demonstrates that permanent retention of behavioral data is not critical to surveillance-based business models. Such limits should be enforced industrywide.&lt;/p&gt;
&lt;p&gt;Moving to a norm where behavioral data is kept briefly instead of forever will mark a major step forward in data security, both reducing the time data is potentially exposed to attackers, and reducing the total volume of data that must be kept safe.&lt;/p&gt;
&lt;p&gt;Time limits on behavioral data will also reduce consumers’ perception that they are making irrevocable privacy commitments every time they try a new product or service.&lt;/p&gt;

&lt;p&gt;The right to download is one of the most laudable features in the GDPR, and serves the important secondary purpose of educating the public about the extent of data collection.&lt;/p&gt;
&lt;p&gt;This right should, however, be expanded to include the right to download, and correct, all information that third-party data brokers have provided about a user, in a spirit similar to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fair_Credit_Reporting_Act&quot;&gt;Fair Credit Reporting Act&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Tech startups in the highly regulated areas of health, finance and banking should be required to compete on the same regulatory footing as established businesses in those areas. In particular, they should not be allowed to do an end run around existing data privacy laws by using machine learning and algorithmic inference.&lt;/p&gt;
&lt;p&gt;For example, the use of a machine learning algorithm should not allow a loan company to evade consumer protections against discrimination in fair lending laws.&lt;/p&gt;
&lt;p&gt;(For a fuller discussion of this point, see the &lt;a href=&quot;https://idlewords.com/talks/senate_testimony.2019.5.htm#addendum&quot;&gt;addendum on machine learning&lt;/a&gt; at the end of this document).&lt;/p&gt;

&lt;p&gt;While the above suggestions seek to impose limits and restrictions, there is an important way that privacy regulation can create new ground for innovation.&lt;/p&gt;
&lt;p&gt;What is missing from the regulatory landscape is a legal mechanism for making credible and binding promises to users about privacy practices.&lt;/p&gt;
&lt;p&gt;Today, internet startups in the U.S. who want to compete on privacy have no mechanism to signal their commitment to users other than making promises through their terms of service (which usually include a standard legal clause that they may change at any time).&lt;/p&gt;
&lt;p&gt;Except in the case of the most egregious violations, which sometimes attract the attention of the Federal Trade Commission, these terms of service carry little weight.&lt;/p&gt;
&lt;p&gt;As the owner of a company that markets itself to privacy-conscious people, I would derive enormous benefit from a legal framework that allowed me to make binding privacy promises (for example, a pledge that there is no third-party tracking on my website), and imposed stiff fines on my company if I violated these guarantees (including criminal liability in the case of outright fraud).&lt;/p&gt;
&lt;p&gt;Such a legal mechanism would not only enable competition around privacy-enhancing features, but it would also give future regulators a clearer idea of how much value consumers place on data privacy. It is possible that the tech giants are right, and people want services for free, no matter the privacy cost. It is also possible that people value privacy, and will pay extra for it, just like many people now pay a premium for organic fruit. The experiment is easy to run—but it requires a modest foundation in law.&lt;/p&gt;
&lt;p&gt;Academic research in computer science is full of fascinating ideas that could serve as the seed for business built around user privacy. Results in fields like &lt;a href=&quot;https://en.wikipedia.org/wiki/Homomorphic_encryption&quot;&gt;homomorphic encryption&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Differential_privacy&quot;&gt;differential privacy&lt;/a&gt;, &lt;a href=&quot;https://medium.com/dropoutlabs/privacy-preserving-machine-learning-2018-a-year-in-review-b6345a95ae0f&quot;&gt;privacy-preserving machine learning&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Zero-knowledge_proof&quot;&gt;zero-knowledge proofs&lt;/a&gt; all await a clever entrepreneur who can incorporate them into a useful product or service. It is very hard to compete against companies like Amazon or Facebook on price, but it is not hard to beat them on privacy. With a minimum of regulatory scaffolding, we might see a welcome new burst of innovation.&lt;/p&gt;

&lt;p&gt;The final, and paramount goal, of privacy regulation should be to preserve our liberty.&lt;/p&gt;
&lt;p&gt;There is no clearer warning of the danger of building up an infrastructure of surveillance than what is happening today in China’s Xinjiang Uygur Autonomous Region. Claiming to be concerned about the possible radicalization of a Muslim minority, Chinese authorities have imposed a regime of total surveillance over a population of twenty-five million people.&lt;/p&gt;
&lt;p&gt;As &lt;a href=&quot;https://www.hrw.org/report/2019/05/01/chinas-algorithms-repression/reverse-engineering-xinjiang-police-mass-surveillance&quot;&gt;recent reporting by Human Rights Watch&lt;/a&gt; has shown, a computer system called the Integrated Joint Operations Platform (IJOP) monitors the location and movement of all people in the province (based on phone data), as well as their gas and electricity consumption, which apps they use, where they worship, who they communicate with, and how they spend their money. This surveillance information is fed into machine learning models that can bin people into one of thirty-six suspect categories, bringing them to the closer attention of the police. Never before has a government had the technical means to implement this level of surveillance across an entire population. And they are doing it with the same off-the-shelf commercial technologies we use in America to get people to click on ads.&lt;/p&gt;
&lt;p&gt;The latent potential of the surveillance economy as a toolkit for despotism cannot be exaggerated. The monitoring tools we see in repressive regimes are not ‘dual use’ technologies—they are single use technologies, working as designed, except for a different master.&lt;/p&gt;
&lt;p&gt;For sixty years, we have called the threat of totalitarian surveillance ‘Orwellian’, but the word no longer fits the threat. The better word now may be ‘Californian’. A truly sophisticated system of social control, of the kind being pioneered in China, will not compel obedience, but nudge people towards it. Rather than censoring or punishing those who dissent, it will simply make sure their voices are not heard. It will reward complacent behavior, and sideline troublemakers. It’s even possible that, judiciously wielded, such a system of social control might enjoy wide public support in our own country.&lt;/p&gt;
&lt;p&gt;But I hope you will agree with me that such a future would be profoundly un-American.&lt;/p&gt;
&lt;p&gt;There is no deep reason that weds the commercial internet to a business model of blanket surveillance. The spirit of innovation is not dead in Silicon Valley, and there are other ways we can grow our digital economy that will maintain our lead in information technology, while also safeguarding our liberty. Just like the creation of the internet itself, the effort to put it on a safer foundation will require a combination of research, entrepreneurial drive and timely, enlightened regulation. But we did it before, and there’s no reason to think we can’t do it again.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Machine learning&lt;/a&gt; is a mathematical technique for training computer systems to make accurate predictions from a large corpus of training data, with a degree of accuracy that in some domains can mimic human cognition.&lt;/p&gt;
&lt;p&gt;For example, machine learning algorithms trained on a sufficiently large data set can learn to identify objects in photographs with a high degree of accuracy, transcribe spoken language to text, translate texts between languages, or flag anomalous behavior on a surveillance videotape.&lt;/p&gt;
&lt;p&gt;The mathematical techniques underpinning machine learning, like &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional neural networks&lt;/a&gt; (CNN), have been well-known since before the revolution in machine learning that took place beginning in 2012. What enabled the key breakthrough in machine learning was the arrival of truly large collections of data, along with concomitant computing power, allowing these techniques to finally demonstrate their full potential.&lt;/p&gt;
&lt;p&gt;It takes data sets of millions or billions of items, along with considerable computing power, to get adequate results from a machine learning algorithms Before the advent of the surveillance economy, we simply did not realize the power of these techniques when applied at scale.&lt;/p&gt;
&lt;p&gt;Because machine learning has a voracious appetite for data and computing power, it contributes both to the centralizing tendency that has consolidated the tech industry, and to the pressure companies face to maximize the collection of user data.&lt;/p&gt;
&lt;p&gt;Machine learning models poses some unique problems in privacy regulation because of the way they can obscure the links between the data used to train them and their ultimate behavior.&lt;/p&gt;
&lt;p&gt;A key feature of machine learning is that it occurs in separable phases. An initial training phase consists of running a learning algorithm on a large collection of labeled data (a time and computation-intensive process). This model can then be deployed in an exploitation phase, which requires far fewer resources.&lt;/p&gt;
&lt;p&gt;Once the training phase is complete, the data used to train the model is no longer required and can conceivably be thrown away.&lt;/p&gt;
&lt;p&gt;The two phases of training and exploitation can occur far away from each other both in space and time. The legal status of models trained on personal data under privacy laws like the GDPR, or whether data transfer laws apply to moving a trained model across jurisdictions, is not clear.&lt;/p&gt;
&lt;p&gt;Inspecting a trained model reveals nothing about the data that went into it. To a human inspecting it, the model consists of millions and millions of numeric weights that have no obvious meaning, or relationship to human categories of thought. One cannot examine an image recognition model, for example, and point to the numbers that encode ‘apple’.&lt;/p&gt;
&lt;p&gt;The training process behaves as a kind of one-way function. It is not possible to run a trained model backwards to reconstruct the input data; nor is it possible to “untrain” a model so that it will forget a specific part of its input.&lt;/p&gt;
&lt;p&gt;Machine learning algorithms are best understood as inference engines. They find structure and excel at making inferences from data that can sometimes be surprising even to people familiar with the technology. This ability to see patterns that humans don’t notice has led to interest in using machine learning algorithms in medical diagnosis, evaluating insurance risk, assigning credit scores, stock trading, and other fields that currently rely on expert human analysis.&lt;/p&gt;
&lt;p&gt;The opacity of machine learning models, combined with this capacity for inference, also make them an ideal technology for circumventing legal protections on data use. In this spirit, I have previously referred to machine learning as “money laundering for bias”. Whatever latent biases are in the training data, whether or not they are apparent to humans, and whether or not attempts are made to remove them from the data set, will be reflected in the behavior of the model.&lt;/p&gt;
&lt;p&gt;A final feature of machine learning is that it is curiously vulnerable to adversarial inputs. For example, an image classifier that correctly identifies a picture of a horse might reclassify the same image as an apple, sailboat or any other object of an attacker’s choosing if they can &lt;a href=&quot;https://arxiv.org/pdf/1710.08864.pdf&quot;&gt;manipulate even one pixel&lt;/a&gt; in the image. Changes in input data not noticeable to a human observer will be sufficient to persuade the model. &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0743731518309183&quot;&gt;Recent research suggests&lt;/a&gt; that this property is an inherent and ineradicable feature of any machine learning system that uses current approaches.&lt;/p&gt;
&lt;p&gt;In brief, machine learning is effective, has an enormous appetite for data, requires large computational resources, makes decisions that resist analysis, excels at finding latent structure in data, obscures the link between source data and outcomes, defies many human intuitions, and is readily fooled by a knowledgeable adversary.&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Sun, 12 May 2019 18:19:56 +0000</pubDate>
<dc:creator>aaronbrethorst</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://idlewords.com/talks/senate_testimony.2019.5.htm</dc:identifier>
</item>
<item>
<title>Openpilot – open-source self-driving agent</title>
<link>https://github.com/commaai/openpilot</link>
<guid isPermaLink="true" >https://github.com/commaai/openpilot</guid>
<description>&lt;div class=&quot;Box-body&quot;&gt;
&lt;article class=&quot;markdown-body entry-content p-5&quot; itemprop=&quot;text&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/commaai/openpilot#&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/6274a88ce2e4db4d69dece2c13e13bdc3c6de87f/68747470733a2f2f692e696d6775722e636f6d2f785932676448762e706e67&quot; alt=&quot;&quot; data-canonical-src=&quot;https://i.imgur.com/xY2gdHv.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://github.com/commaai/openpilot&quot;&gt;openpilot&lt;/a&gt; is an open source driving agent. Currently, it performs the functions of Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS) for selected Honda, Toyota, Acura, Lexus, Chevrolet, Hyundai, Kia. It's about on par with Tesla Autopilot and GM Super Cruise, and better than &lt;a href=&quot;http://www.thedrive.com/tech/5707/the-war-for-autonomous-driving-part-iii-us-vs-germany-vs-japan&quot; rel=&quot;nofollow&quot;&gt;all other manufacturers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The openpilot codebase has been written to be concise and to enable rapid prototyping. We look forward to your contributions - improving real vehicle automation has never been easier.&lt;/p&gt;

&lt;hr/&gt;&lt;h2&gt;Community&lt;/h2&gt;
&lt;p&gt;openpilot is developed by &lt;a href=&quot;https://comma.ai/&quot; rel=&quot;nofollow&quot;&gt;comma.ai&lt;/a&gt; and users like you.&lt;/p&gt;
&lt;p&gt;We have a &lt;a href=&quot;https://twitter.com/comma_ai&quot; rel=&quot;nofollow&quot;&gt;Twitter you should follow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, we have a several thousand people community on &lt;a href=&quot;https://discord.comma.ai&quot; rel=&quot;nofollow&quot;&gt;Discord&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Hardware&lt;/h2&gt;
&lt;p&gt;At the moment openpilot supports the &lt;a href=&quot;https://comma.ai/shop/products/eon-dashcam-devkit&quot; rel=&quot;nofollow&quot;&gt;EON Dashcam DevKit&lt;/a&gt;. A &lt;a href=&quot;https://shop.comma.ai/products/panda-obd-ii-dongle&quot; rel=&quot;nofollow&quot;&gt;panda&lt;/a&gt; and a &lt;a href=&quot;https://comma.ai/shop/products/giraffe/&quot; rel=&quot;nofollow&quot;&gt;giraffe&lt;/a&gt; are recommended tools to interface the EON with the car. We'd like to support other platforms as well.&lt;/p&gt;
&lt;p&gt;Install openpilot on a neo device by entering &lt;code&gt;https://openpilot.comma.ai&lt;/code&gt; during NEOS setup.&lt;/p&gt;
&lt;h2&gt;Supported Cars&lt;/h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Make&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Supported Package&lt;/th&gt;
&lt;th&gt;Lateral&lt;/th&gt;
&lt;th&gt;Longitudinal&lt;/th&gt;
&lt;th&gt;No Accel Below&lt;/th&gt;
&lt;th&gt;No Steer Below&lt;/th&gt;
&lt;th&gt;Giraffe&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Acura&lt;/td&gt;
&lt;td&gt;ILX 2016-17&lt;/td&gt;
&lt;td&gt;AcuraWatch Plus&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;25mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Acura&lt;/td&gt;
&lt;td&gt;RDX 2018&lt;/td&gt;
&lt;td&gt;AcuraWatch Plus&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Buick&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Regal 2018&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Chevrolet&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Malibu 2017&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Chevrolet&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Volt 2017-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Cadillac&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;ATS 2018&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica 2018&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;td&gt;FCA&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica Hybrid 2017-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;td&gt;FCA&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Chrysler&lt;/td&gt;
&lt;td&gt;Pacifica Hybrid 2019&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;39mph&lt;/td&gt;
&lt;td&gt;FCA&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GMC&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Acadia Denali 2018&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Holden&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;Astra 2017&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;7mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Accord 2018&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;3mph&lt;/td&gt;
&lt;td&gt;Bosch&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Sedan/Coupe 2016-18&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Sedan/Coupe 2019&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;2mph&lt;/td&gt;
&lt;td&gt;Bosch&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Civic Hatchback 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Bosch&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V 2015-16&lt;/td&gt;
&lt;td&gt;Touring&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V 2017-18&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Bosch&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;CR-V Hybrid 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Bosch&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Odyssey 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Inverted Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Passport 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Inverted Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Pilot 2016-18&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Pilot 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Inverted Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Ridgeline 2017-19&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Santa Fe 2019&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Elantra 2017&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;19mph&lt;/td&gt;
&lt;td&gt;34mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Hyundai&lt;/td&gt;
&lt;td&gt;Genesis 2018&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;19mph&lt;/td&gt;
&lt;td&gt;34mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Jeep&lt;/td&gt;
&lt;td&gt;Grand Cherokee 2017-18&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;9mph&lt;/td&gt;
&lt;td&gt;FCA&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Jeep&lt;/td&gt;
&lt;td&gt;Grand Cherokee 2019&lt;/td&gt;
&lt;td&gt;Adaptive Cruise&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;39mph&lt;/td&gt;
&lt;td&gt;FCA&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Optima 2019&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Sorento 2018&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Kia&lt;/td&gt;
&lt;td&gt;Stinger 2018&lt;/td&gt;
&lt;td&gt;SCC + LKAS&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Lexus&lt;/td&gt;
&lt;td&gt;RX Hybrid 2016-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Subaru&lt;/td&gt;
&lt;td&gt;Impreza 2019&lt;/td&gt;
&lt;td&gt;EyeSight&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Subaru&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Camry 2018&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;C-HR 2017-18&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Stock&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Corolla 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Highlander 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Highlander Hybrid 2018&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Prius Prime 2017-19&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 2016&lt;/td&gt;
&lt;td&gt;TSS-P&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;20mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Toyota&lt;/td&gt;
&lt;td&gt;Rav4 Hybrid 2017-18&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Toyota&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;a href=&quot;https://community.comma.ai/wiki/index.php/Comma_Pedal&quot; rel=&quot;nofollow&quot;&gt;Comma Pedal&lt;/a&gt; is used to provide stop-and-go capability to some of the openpilot-supported cars that don't currently support stop-and-go. Here is how to &lt;a href=&quot;https://medium.com/@jfrux/comma-pedal-building-with-macrofab-6328bea791e8&quot; rel=&quot;nofollow&quot;&gt;build a Comma Pedal&lt;/a&gt;. &lt;em&gt;&lt;strong&gt;NOTE: The Comma Pedal is not officially supported by &lt;a href=&quot;https://comma.ai&quot; rel=&quot;nofollow&quot;&gt;comma.ai&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;sup&gt;2&lt;/sup&gt;When disconnecting the Driver Support Unit (DSU), otherwise longitudinal control is stock ACC. For DSU locations, see &lt;a href=&quot;https://community.comma.ai/wiki/index.php/Toyota&quot; rel=&quot;nofollow&quot;&gt;Toyota Wiki page&lt;/a&gt;&lt;br/&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;a href=&quot;https://zoneos.com/volt/&quot; rel=&quot;nofollow&quot;&gt;GM installation guide&lt;/a&gt;.&lt;br/&gt;&lt;sup&gt;4&lt;/sup&gt;It needs an extra 120Ohm resistor (&lt;a href=&quot;https://i.imgur.com/CmdKtTP.jpg&quot; rel=&quot;nofollow&quot;&gt;pic1&lt;/a&gt;, &lt;a href=&quot;https://i.imgur.com/s2etUo6.jpg&quot; rel=&quot;nofollow&quot;&gt;pic2&lt;/a&gt;) on bus 3 and giraffe switches set to 01X1 (11X1 for stock LKAS), where X depends on if you have the &lt;a href=&quot;https://comma.ai/shop/products/power/&quot; rel=&quot;nofollow&quot;&gt;comma power&lt;/a&gt;.&lt;br/&gt;&lt;sup&gt;5&lt;/sup&gt;28mph for Camry 4CYL L, 4CYL LE and 4CYL SE which don't have Full-Speed Range Dynamic Radar Cruise Control.&lt;br/&gt;&lt;sup&gt;6&lt;/sup&gt;Open sourced &lt;a href=&quot;https://github.com/commaai/neo/tree/master/giraffe/hyundai&quot;&gt;Hyundai Giraffe&lt;/a&gt; is designed for the 2019 Sante Fe; pinout may differ for other Hyundais.&lt;br/&gt;&lt;sup&gt;7&lt;/sup&gt;Community built Giraffe, find more information &lt;a href=&quot;https://zoneos.com/shop/&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Community Maintained Cars&lt;/h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Make&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Supported Package&lt;/th&gt;
&lt;th&gt;Lateral&lt;/th&gt;
&lt;th&gt;Longitudinal&lt;/th&gt;
&lt;th&gt;No Accel Below&lt;/th&gt;
&lt;th&gt;No Steer Below&lt;/th&gt;
&lt;th&gt;Giraffe&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Honda&lt;/td&gt;
&lt;td&gt;Fit 2018&lt;/td&gt;
&lt;td&gt;Honda Sensing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;25mph&lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt;
&lt;td&gt;12mph&lt;/td&gt;
&lt;td&gt;Inverted Nidec&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Tesla&lt;/td&gt;
&lt;td&gt;Model S 2012-13&lt;/td&gt;
&lt;td&gt;All&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Not yet&lt;/td&gt;
&lt;td&gt;Not applicable&lt;/td&gt;
&lt;td&gt;0mph&lt;/td&gt;
&lt;td&gt;Custom&lt;sup&gt;8&lt;/sup&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/commaai/openpilot/pull/266&quot;&gt;[Honda Fit Pull Request]&lt;/a&gt;.&lt;br/&gt;&lt;a href=&quot;https://github.com/commaai/openpilot/pull/246&quot;&gt;[Tesla Model S Pull Request]&lt;/a&gt;&lt;br/&gt;&lt;sup&gt;8&lt;/sup&gt;Community built Giraffe, find more information here &lt;a href=&quot;https://github.com/jeankalud/neo/tree/tesla_giraffe/giraffe/tesla&quot;&gt;Community Tesla Giraffe&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Community Maintained Cars are not confirmed by comma.ai to meet our &lt;a href=&quot;https://github.com/commaai/openpilot/blob/devel/SAFETY.md&quot;&gt;safety model&lt;/a&gt;. Be extra cautious using them.&lt;/p&gt;
&lt;h2&gt;In Progress Cars&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;All TSS-P Toyota with Steering Assist and LSS-P Lexus with Steering Assist or Lane Keep Assist.
&lt;ul&gt;&lt;li&gt;Only remaining Toyota cars with no port yet are the Avalon and the Sienna.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;All Hyundai with SmartSense.&lt;/li&gt;
&lt;li&gt;All Kia with SCC and LKAS.&lt;/li&gt;
&lt;li&gt;All Chrysler, Jeep, Fiat with Adaptive Cruise Control and LaneSense.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;How can I add support for my car?&lt;/h2&gt;
&lt;p&gt;If your car has adaptive cruise control and lane keep assist, you are in luck. Using a &lt;a href=&quot;https://comma.ai/shop/products/panda-obd-ii-dongle/&quot; rel=&quot;nofollow&quot;&gt;panda&lt;/a&gt; and &lt;a href=&quot;https://community.comma.ai/cabana/&quot; rel=&quot;nofollow&quot;&gt;cabana&lt;/a&gt;, you can understand how to make your car drive by wire.&lt;/p&gt;
&lt;p&gt;We've written guides for &lt;a href=&quot;https://medium.com/@comma_ai/how-to-write-a-car-port-for-openpilot-7ce0785eda84&quot; rel=&quot;nofollow&quot;&gt;Brand&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6&quot; rel=&quot;nofollow&quot;&gt;Model&lt;/a&gt; ports. These guides might help you after you have the basics figured out.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;BMW, Audi, Volvo, and Mercedes all use &lt;a href=&quot;https://en.wikipedia.org/wiki/FlexRay&quot; rel=&quot;nofollow&quot;&gt;FlexRay&lt;/a&gt; and can be supported after &lt;a href=&quot;https://github.com/commaai/openpilot/pull/463&quot;&gt;FlexRay support&lt;/a&gt; is merged.&lt;/li&gt;
&lt;li&gt;We put time into a Ford port, but the steering has a 10 second cutout limitation that makes it unusable.&lt;/li&gt;
&lt;li&gt;The 2016-2017 Honda Accord uses a custom signaling protocol for steering that's unlikely to ever be upstreamed.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Directory structure&lt;/h2&gt;
&lt;pre&gt;
&lt;code&gt;.
├── apk                 # The apk files used for the UI
├── cereal              # The messaging spec used for all logs on EON
├── common              # Library like functionality we've developed here
├── installer/updater   # Manages auto-updates of openpilot
├── opendbc             # Files showing how to interpret data from cars
├── panda               # Code used to communicate on CAN and LIN
├── phonelibs           # Libraries used on EON
├── pyextra             # Libraries used on EON
└── selfdrive           # Code needed to drive the car
    ├── assets          # Fonts and images for UI
    ├── boardd          # Daemon to talk to the board
    ├── can             # Helpers for parsing CAN messages
    ├── car             # Car specific code to read states and control actuators
    ├── common          # Shared C/C++ code for the daemons
    ├── controls        # Perception, planning and controls
    ├── debug           # Tools to help you debug and do car ports
    ├── locationd       # Soon to be home of precise location
    ├── logcatd         # Android logcat as a service
    ├── loggerd         # Logger and uploader of car data
    ├── mapd            # Fetches map data and computes next global path
    ├── orbd            # Computes ORB features from frames
    ├── proclogd        # Logs information from proc
    ├── sensord         # IMU / GPS interface code
    ├── test            # Car simulator running code through virtual maneuvers
    ├── ui              # The UI
    └── visiond         # Vision pipeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To understand how the services interact, see &lt;code&gt;selfdrive/service_list.yaml&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;User Data / chffr Account / Crash Reporting&lt;/h2&gt;
&lt;p&gt;By default, openpilot creates an account and includes a client for chffr, our dashcam app. We use your data to train better models and improve openpilot for everyone.&lt;/p&gt;
&lt;p&gt;It's open source software, so you are free to disable it if you wish.&lt;/p&gt;
&lt;p&gt;It logs the road facing camera, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs. The user facing camera is only logged if you explicitly opt-in in settings. It does not log the microphone.&lt;/p&gt;
&lt;p&gt;By using it, you agree to &lt;a href=&quot;https://community.comma.ai/privacy.html&quot; rel=&quot;nofollow&quot;&gt;our privacy policy&lt;/a&gt;. You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma.ai. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma.ai for the use of this data.&lt;/p&gt;
&lt;h2&gt;Testing on PC&lt;/h2&gt;
&lt;p&gt;Check out &lt;a href=&quot;https://github.com/commaai/openpilot-tools&quot;&gt;openpilot-tools&lt;/a&gt;: lots of tools you can use to replay driving data, test and develop openpilot from your pc.&lt;/p&gt;
&lt;p&gt;Also, within openpilot there is a rudimentary infrastructure to run a basic simulation and generate a report of openpilot's behavior in different longitudinal control scenarios.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Requires working docker&lt;/span&gt;
./run_docker_tests.sh
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Contributing&lt;/h2&gt;
&lt;p&gt;We welcome both pull requests and issues on &lt;a href=&quot;http://github.com/commaai/openpilot&quot;&gt;github&lt;/a&gt;. Bug fixes and new car ports encouraged.&lt;/p&gt;
&lt;p&gt;We also have a &lt;a href=&quot;https://comma.ai/bounties.html&quot; rel=&quot;nofollow&quot;&gt;bounty program&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Want to get paid to work on openpilot? &lt;a href=&quot;https://comma.ai/jobs/&quot; rel=&quot;nofollow&quot;&gt;comma.ai is hiring&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Licensing&lt;/h2&gt;
&lt;p&gt;openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.&lt;/p&gt;
&lt;p&gt;Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT. YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS. NO WARRANTY EXPRESSED OR IMPLIED.&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/c93f9b3a1bbb98aee637825923b0d04c1de248e1/68747470733a2f2f64317162326e6235637a6e6174752e636c6f756466726f6e742e6e65742f73746172747570732f692f313036313135372d62633765396266336232343665636537333232653666666536353366366166382d6d656469756d5f6a70672e6a70673f6275737465723d31343538333633313330&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/c93f9b3a1bbb98aee637825923b0d04c1de248e1/68747470733a2f2f64317162326e6235637a6e6174752e636c6f756466726f6e742e6e65742f73746172747570732f692f313036313135372d62633765396266336232343665636537333232653666666536353366366166382d6d656469756d5f6a70672e6a70673f6275737465723d31343538333633313330&quot; width=&quot;75&quot; data-canonical-src=&quot;https://d1qb2nb5cznatu.cloudfront.net/startups/i/1061157-bc7e9bf3b246ece7322e6ffe653f6af8-medium_jpg.jpg?buster=1458363130&quot;/&gt;&lt;/a&gt; &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/5cda36b7447fb30b53fefb059ccd3b28e46aa7c8/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a433837456a7847654d50726b547556525657566734772e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/5cda36b7447fb30b53fefb059ccd3b28e46aa7c8/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a433837456a7847654d50726b547556525657566734772e706e67&quot; width=&quot;225&quot; data-canonical-src=&quot;https://cdn-images-1.medium.com/max/1600/1*C87EjxGeMPrkTuVRVWVg4w.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Sun, 12 May 2019 17:42:09 +0000</pubDate>
<dc:creator>boramalper</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/16128714?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>commaai/openpilot</og:title>
<og:url>https://github.com/commaai/openpilot</og:url>
<og:description>open source driving agent. Contribute to commaai/openpilot development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/commaai/openpilot</dc:identifier>
</item>
<item>
<title>Show HN: Koonchi – Convert Photo to Hand-Painted Painting by Artists from India</title>
<link>https://koonchi.com</link>
<guid isPermaLink="true" >https://koonchi.com</guid>
<description>&lt;p&gt;&lt;span&gt;Like the Royals of the world, here is a chance to create a timeless memory. We transform your photos into 100% hand-drawn, beautiful, paintings. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Koonchi has carefully curated a selected pool of artists from across India, who are remarkably talented.  &lt;/p&gt;
&lt;p&gt;Are you looking for customized and personalized gifts? Need something more eternal than photographs for the walls? We’ve the answer for you.&lt;/p&gt;
</description>
<pubDate>Sun, 12 May 2019 16:16:21 +0000</pubDate>
<dc:creator>OrganicQuote</dc:creator>
<og:url>https://koonchi.com/</og:url>
<og:title>Best Way to Convert Photo to 100% Hand-Drawn Painting</og:title>
<og:type>website</og:type>
<og:description>Convert photos to painting. Made by artists across India; 100% hand-painted artworks in oil, acrylic, miniature, pencil, ballpoint pen and many more mediums. A perfect gift for wedding, anniversary, birthday, christmas, valentines, diwali or any occasion. An essential piece of art for home interiors and office walls.</og:description>
<og:image>http://cdn.shopify.com/s/files/1/0029/6144/0881/files/Insta_Koonchi_2_1200x1200.jpg?v=1549189372</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://koonchi.com/</dc:identifier>
</item>
</channel>
</rss>