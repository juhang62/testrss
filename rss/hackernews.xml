<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Color blindness</title>
<link>https://commandcenter.blogspot.com/2020/09/color-blindness-is-inaccurate-term.html</link>
<guid isPermaLink="true" >https://commandcenter.blogspot.com/2020/09/color-blindness-is-inaccurate-term.html</guid>
<description>&lt;p&gt;&lt;span&gt;Color blindness is an inaccurate term. Most color-blind people can see color, they just don't see the same colors as everyone else.&lt;/span&gt;&lt;/p&gt;
&lt;span id=&quot;docs-internal-guid-dff89d18-7fff-456d-bfd8-1c99e13f78f7&quot;/&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span id=&quot;docs-internal-guid-dff89d18-7fff-456d-bfd8-1c99e13f78f7&quot;&gt;&lt;span&gt;There have been a number of articles written about how to improve graphs, charts, and other visual aids on computers to better serve color-blind people. That is a worthwhile endeavor, and the people writing them mean well, but I suspect very few of them are color-blind because the advice is often poor and sometimes wrong. The most common variety of color blindness is called red-green color blindness, or deuteranopia, and it affects about 6% of human males. As someone who has moderate deuteranopia, I'd like to explain what living with it is really like.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The answer may surprise you.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;I see red and green just fine. Maybe not as fine as you do, but just fine. I get by. I can drive a car and I stop when the light is red and go when the light is green. (Blue and yellow, by the way, I see the same as you. For a tiny fraction of people that is not the case, but that's not the condition I'm writing about.)&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;If I can see red and green, what then is red-green color blindness?&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;To answer that, we need to look at the genetics and design of the human vision system. I will only be writing about moderate deuteranopia, because that's what I have and I know what it is: I live with it. Maybe I can help you understand how that impairment—and it is an impairment, however mild—affects the way I see things, especially when people make charts for display on a computer.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;There's a lot to go through, but here is a summary. The brain interprets signals from the eye to determine color, but the eye doesn't see colors. There is no red receptor, no green receptor in the eye. The color-sensitive receptors in the eye, called &lt;em&gt;cones&lt;/em&gt;, don't work like that. Instead there are several different types of cones with broad but overlapping color response curves, and what the eye delivers to the brain is the&lt;/span&gt; &lt;span&gt;difference&lt;/span&gt; &lt;span&gt;between the signals from nearby cones with possibly different color response. Colors are what the brain makes from those signals.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;There are also monochromatic receptors in the eye, called rods, and lots of them, but we're ignoring them here. They are most important in low light. In bright light it's the color-sensitive cones that dominate.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;For most mammals, there are two color response curves for cones in the eye. They are called warm and cool, or yellow and blue. Dogs, for instance, see color, but from a smaller palette than we do. The color responses are determined, in effect, by pigments in front of the light receptors, filters if you will. We have this system in our eyes, but we also have another, and that second one is the central player in this discussion.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;We are mammals, primates, and we are members of the branch of primates called Old World monkeys. At some point our ancestors in Africa moved to the trees and started eating the fruit there. The old warm/cool color system is not great at spotting orange or red fruit in a green tree.  Evolution solved this problem by duplicating a pigment and mutating it to make a third one. This created three pigments in the monkey eye, and that allowed a new color dimension to arise, creating what we now think of as the red/green color axis. That dimension makes fruit easier to find in the jungle, granting a selective advantage to monkeys, like us, who possess it.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;It's not necessary to have this second, red/green color system to survive. Monkeys could find fruit before the new system evolved. So the red/green system favored monkeys who had it, but it wasn't &lt;em&gt;necessary&lt;/em&gt;, and evolutionary pressure hasn't yet perfected the system. It's also relatively new, so it's still evolving. As a result, not all humans have equivalent color vision.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The mechanism is a bit sloppy. The mutation is a &quot;stutter&quot; mutation, meaning that the pigment was created by duplicating the original warm pigment's DNA and then repeating some of its codon sequences. The quality of the new pigment—how much the pigment separates spectrally from the old warm pigment—is determined by how well the stutter mutation is preserved. No stutter, you get just the warm/cool dimension, a condition known as dichromacy that affects a small fraction of people, almost exclusively male (and all dogs). Full stutter, you get the normal human vision with yellow/blue and red/green dimensions. Partial stutter, and you get me, moderately red-green color-blind. Degrees of red-green color blindness arise according to how much stutter is in the chromosome.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Those pigments are encoded only on the X chromosome. That means that most males, being XY, get only one copy of the pigment genes, while most females, being XX, get two. If an XY male inherits a bad copy of the X he will be color-blind. An XX female, though, will be much less likely to get two bad copies. But some will get a good one&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;a bad one, one from the mother and one from the father, giving them&lt;/span&gt; &lt;span&gt;four&lt;/span&gt; &lt;span&gt;pigments. Such females are called tetrachromatic and have a richer color system than most of us, even than normal trichromats like you.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The key point about the X-residence of the pigment, though, is that men are much likelier than women to be red-green color-blind.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Here is a figure from an article by Denis Baylor in an essay collection called&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://www.amazon.com/Colour-Science-Darwin-College-Lectures/dp/0521496454&quot;&gt;Colour Art &amp;amp; Science,&lt;/a&gt; edited by Trevor Lamb and Janine Bourriau, an excellent resource&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;img height=&quot;629&quot; src=&quot;https://lh5.googleusercontent.com/_uUlaTOTL2VOjTaEK6VC1GQL3eJU1H3f_Sa6wMCzktahjUeA66egzEXBvA_9hE_JuF07MyYwF2SpHdv0ytjSQTecIExlZbv_qYs-pAIAAHv6QZv6oE3NiXTBNonoyQejE3NmZEIg&quot; width=&quot;312&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The top diagram shows the pigment spectra of a dichromat, what most mammals have. The bottom one shows the normal trichromat human pigment spectra. Note that two of the pigments are the same as in a dichromat, but there is a third, shifted slightly to the red. That is the Old World monkey mutation, making it possible to discriminate red. The diagram in the middle shows the spectra for someone with red-green color blindness. You can see that there are still three pigments, but the difference between the middle and longer-wave (redder) pigment is smaller.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;A deuteranope like me can still discriminate red and green, just not as well. Perhaps what I see is a bit like what you see when evening approaches and the color seems to drain from the landscape as the rods begin to take over. Or another analogy might be what happens when you turn the stereo's volume down: You can still hear all the instruments, but they don't stand out as well.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;It's worth emphasizing that there is no &quot;red&quot; or &quot;green&quot; or &quot;blue&quot; or &quot;yellow&quot; receptor in the eye. The optical pigments have very broad spectra. It's the&lt;/span&gt; &lt;span&gt;difference&lt;/span&gt; &lt;span&gt;in the response between two receptors that the vision system turns into color.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;In short, I still see red and green, just not as well as you do. But there's another important part of the human visual system that is relevant here, and it has a huge influence on how red-green color blindness affects the clarity of diagrams on slides and such.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;It has to do with edge detection. The signals from receptors in the eye are used not only to detect color, but also to detect edges. In fact since color is detected largely by differences of spectral response from nearby receptors, the edges are important because that's where the strongest difference lies. The color of a region, especially a small one, is largely determined at the edges.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Of course, all animals need some form of visual processing that identifies objects, and edge detection is part of that processing in mammals. But the edge detection circuitry is not uniformly deployed. In particular, there is very little high-contrast detection capability for cool colors. You can see this yourself in the following diagram, provided your monitor is set up properly. The small pure blue text on the pure black background is harder to read than even the slightly less saturated blue text, and much harder than the green or red. Make sure the image is no more than about 5cm across to see the effect properly, as the scale of the contrast signal matters:&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;img height=&quot;149&quot; src=&quot;https://lh3.googleusercontent.com/3927UoIgyalKdIZwQnTHHE2YKxoMHilBWZfnD6LXIIAwZefhHJWbIgyA043dknzyOGyl8LcTR0Jxqi16LpLX6siBdha8s_IhqNfMPSe6XuerqiY5ODymG6SVVG-9B-p_szSIk1LE&quot; width=&quot;192&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;In this image, the top line is pure computer green, the next is pure computer red, and the bottom is pure computer blue. In between is a sequence leading to ever purer blues towards the bottom. For me, and I believe for everyone, the bottom line is very hard to read.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Here is the same text field as above but with a white background:&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;img height=&quot;149&quot; src=&quot;https://lh3.googleusercontent.com/E3ibMjWZ7qUs0Gdx4u0jZYpFeTamQfe1ZQj8roPnw0dWxcGZWQp7q6BUuBcxWJ7bcg77pfCFQCwRoQuYcVubXahzbUja0vw4G98gDdq604Hk2XIl996hTxipRPo4kXeqHBcfhy_l=w195-h149&quot; width=&quot;192&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Notice that the blue text is now easy to read. That's because it's against white, which includes lots of light and all colors, so it's easy for the eye to build the difference signals and recover the edges. Essentially, it detects a change of color from the white to the blue. Across the boundary the level of blue changes, but so do the levels red and green. When the background is black, however, the eye depends on the blue alone—black has no color, no light to contribute a signal, no red, no green—and that is a challenge for the human eye.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Now here's some fun: double the size of the black-backgrounded image and the blue text becomes disproportionately more readable:&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;img height=&quot;298&quot; src=&quot;https://lh3.googleusercontent.com/XnnbY9BKZzLqkEW4P4rEGCsdmkIx6cHOMNshR9mZQRxjt7pijzbbjfYunfrJgNTvHD_LOIsplXnPuZtms45DAYAGvQwperTdtfYXZZ_J2nOTR1OEa7Z8qcZUyBMDhTpk6a6ZjlPn&quot; width=&quot;384&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Because the text is bigger, more receptors are involved and there is less dependence on edge detection, making it easier to read the text. As I said above, the scale of the contrast changes matters. If you use your browser to blow up the image further you'll see it becomes even easier to read the blue text.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;And that provides a hint about how red-green color blindness looks to people who have it.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;For red-green color-blind people, the major effect comes from the fact that edge detection is weaker in the red/green dimension, sort of like blue edge detection is for everyone. Because the pigments are closer together than in a person with regular vision, if the color difference in the red-green dimension is the only signal that an edge is there, it becomes hard to see the edge and therefore hard to see the color. &lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;In other words, the problem you have reading the blue text in the upper diagram is analogous to how much trouble a color-blind person has seeing detail in an image with only a mix of red and green. And the issue isn't between computer red versus computer green, which are quite easy to tell apart as they have very different spectra, but between more natural colors on the red/green dimension, colors that align with the naturally evolved pigments in the cones.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;In short, color detection when looking at small things, deciding what color an item is when it's so small that only the color difference signal at the edges can make the determination, is worse for color-blind people. Even though the colors are easy to distinguish for large objects, it's hard when they get small.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;In this next diagram I can easily tell that in the top row the left block is greenish and the right block is reddish, but in the bottom row that is a&lt;/span&gt; &lt;span&gt;much&lt;/span&gt; &lt;span&gt;harder distinction for me to make, and it gets even harder if I look from father away, further shrinking the apparent size of the small boxes. From across the room it's all but impossible, even though the colors of the upper boxes remain easy to identify.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;span&gt;&lt;img height=&quot;138&quot; src=&quot;https://lh5.googleusercontent.com/pd_IvLv0RnRM7vyM8FOmAaTI9Fk9dnwv89peRxQ0yHsAXm2iuOcGbdXVS2TIQfrw4IkLCkqeRgFjOKnMAwN2m26t51BA1dPNxooGcwcIk0U17p8ACg1nFM0NKqw0zPchsIKsccJ1&quot; width=&quot;192&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Remember when I said I could see red and green just fine? Well, I can see the colors just fine (more or less). But that is true only when the object is large enough that the color analysis isn't being done&lt;/span&gt; &lt;span&gt;only by edge detection&lt;/span&gt;&lt;span&gt;. Fields of color are easy, but lines and dots are very hard.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Here's another example. Some devices come with a tiny LED that indicates charging status by changing color: red for low battery, amber for medium, and green for a full charge. I have a lot of trouble discriminating the amber and green lights, but can solve this by holding the light very close to my eye so it occupies a larger part of the visual field. When the light looks bigger, I can tell what color it is.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Another consequence of all this is that I see very little color in the stars. That makes me sad.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Remember this is about color, just color. It's easy to distinguish two items if their colors are close but their intensities, for example, are different. A bright red next to a dull green is easy to spot, even if the same red dulled down to the level of the green would not be. Those squares above are at roughly equal saturations and intensities. If not, it would be easier to tell which is red and which is green.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;To return to the reason for writing this article, red/green color blindness affects legibility. The way the human vision system works, and the way it sometimes doesn't work so well, implies there are things to consider when designing an information display that you want to be clearly understood.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;First, choose colors that can be easily distinguished. If possible, keep them far apart on the spectrum. If not, differentiate them some other way, such as by intensity or saturation.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Second, use other cues if possible. Color is complex, so if you can add another component to a line on a graph, such as a dashed versus dotted pattern, or even good labeling, that helps a lot.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Third, edge detection is key to comprehension but can be tricky. Avoid difficult situations such as pure blue text on a black background. Avoid tiny text.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;Fourth, size matters. Don't use the thinnest possible line. A fatter one might work just as well for the diagram but be much easier to see and to identify by color.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;And to introduce one last topic, some people, like me, have old eyes, and old eyes have much more trouble with scattered light and what that does to contrast. Although dark mode is very popular these days, bright text on a black background scatters in a way that makes it hard to read. The letters have halos around them that can be confusing. Black text on a white background works well because the scatter is uniform and doesn't make halos. It's fortunate that paper is white and ink is black, because that works well for all ages.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The most important lesson is to not assume you know how something appears to a color-blind person, or to anyone else for that matter. If possible, ask someone you know who has eyes different from yours to assess your design and make sure it's legible. The world is full of people with vision problems of all kinds. If only the people who used amber LEDs to indicate charge had realized that.&lt;/span&gt;&lt;/p&gt;
&lt;br/&gt;
</description>
<pubDate>Sat, 03 Oct 2020 15:02:00 +0000</pubDate>
<dc:creator>bumbledraven</dc:creator>
<og:url>https://commandcenter.blogspot.com/2020/09/color-blindness-is-inaccurate-term.html</og:url>
<og:title>Color blindness</og:title>
<og:description>Color blindness is an inaccurate term. Most color-blind people can see color, they just don't see the same colors as everyone else. There ha...</og:description>
<og:image>https://lh5.googleusercontent.com/_uUlaTOTL2VOjTaEK6VC1GQL3eJU1H3f_Sa6wMCzktahjUeA66egzEXBvA_9hE_JuF07MyYwF2SpHdv0ytjSQTecIExlZbv_qYs-pAIAAHv6QZv6oE3NiXTBNonoyQejE3NmZEIg=w1200-h630-p-k-no-nu</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://commandcenter.blogspot.com/2020/09/color-blindness-is-inaccurate-term.html</dc:identifier>
</item>
<item>
<title>Show HN: Igel – A CLI tool to run machine learning without writing code</title>
<link>https://github.com/nidhaloff/igel</link>
<guid isPermaLink="true" >https://github.com/nidhaloff/igel</guid>
<description>&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/logo.jpg&quot;&gt;&lt;img alt=&quot;igel-icon&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/logo.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


&lt;a href=&quot;https://pypi.python.org/pypi/igel&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://camo.githubusercontent.com/d4825df598de43d0730c9d81fb4c3e2131a1d74e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6967656c3f636f6c6f723d677265656e&quot; data-canonical-src=&quot;https://img.shields.io/pypi/v/igel?color=green&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://travis-ci.com/nidhaloff/igel&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/2d174b4eff979226c70a1191611a0cd3e04a0d62/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f6e696468616c6f66662f6967656c2e737667&quot; data-canonical-src=&quot;https://img.shields.io/travis/nidhaloff/igel.svg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/igel&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;https://pepy.tech/badge/igel&quot; src=&quot;https://camo.githubusercontent.com/8adeae354da99ea9cfea49f5ae0b8b49e9b808ea/68747470733a2f2f706570792e746563682f62616467652f6967656c&quot; data-canonical-src=&quot;https://pepy.tech/badge/igel&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/igel/month&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;https://pepy.tech/badge/igel/month&quot; src=&quot;https://camo.githubusercontent.com/aea6697987aac0f1cdfb82632eaeaef5e8a2e1e5/68747470733a2f2f706570792e746563682f62616467652f6967656c2f6d6f6e7468&quot; data-canonical-src=&quot;https://pepy.tech/badge/igel/month&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://igel.readthedocs.io/en/latest/?badge=latest&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;Documentation Status&quot; src=&quot;https://camo.githubusercontent.com/42f52bcb75f5c47b7fcc0829eef0ed4af3fce454/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6967656c2f62616467652f3f76657273696f6e3d6c6174657374&quot; data-canonical-src=&quot;https://readthedocs.org/projects/igel/badge/?version=latest&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.python.org/pypi/igel&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;PyPI - Wheel&quot; src=&quot;https://camo.githubusercontent.com/0a1ecacb6be2f19a450762192f1690a0326d4cd2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f6967656c&quot; data-canonical-src=&quot;https://img.shields.io/pypi/wheel/igel&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.python.org/pypi/igel&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;PyPI - Status&quot; src=&quot;https://camo.githubusercontent.com/e0af9715fa8d46ebb3de7b6e1ec3c98ca36ca0c2/68747470733a2f2f696d672e736869656c64732e696f2f707970692f7374617475732f6967656c&quot; data-canonical-src=&quot;https://img.shields.io/pypi/status/igel&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/NidhalBaccouri&quot; rel=&quot;nofollow&quot;&gt;&lt;img alt=&quot;Twitter URL&quot; src=&quot;https://camo.githubusercontent.com/573f46b7537e12e00befbbafd6f3531e1a1540c0/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c3f75726c3d6874747073253341253246253246747769747465722e636f6d2532464e696468616c426163636f757269&quot; data-canonical-src=&quot;https://img.shields.io/twitter/url?url=https%3A%2F%2Ftwitter.com%2FNidhalBaccouri&quot;/&gt;&lt;/a&gt;

&lt;p&gt;A machine learning tool that allows you to train/fit, test and use models &lt;strong&gt;without writing code&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;Motivation &amp;amp; Goal&lt;/h2&gt;
&lt;p&gt;The goal of the project is to provide machine learning for &lt;strong&gt;everyone&lt;/strong&gt;, both technical and non technical users.&lt;/p&gt;
&lt;p&gt;I needed a tool sometimes, which I can use to fast create a machine learning prototype. Whether to build some proof of concept or create a fast draft model to prove a point. I find myself often stuck at writing boilerplate code and/or thinking too much of how to start this.&lt;/p&gt;
&lt;p&gt;Therefore, I decided to create &lt;strong&gt;igel&lt;/strong&gt;. Hopefully, it will make it easier for technical and non technical users to build machine learning models.&lt;/p&gt;

&lt;h2&gt;Features&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Supports all state of the art machine learning models (even preview models)&lt;/li&gt;
&lt;li&gt;Supports different data preprocessing methods&lt;/li&gt;
&lt;li&gt;Provides flexibility and data control while writing configurations&lt;/li&gt;
&lt;li&gt;Supports cross validation&lt;/li&gt;
&lt;li&gt;Supports yaml and json format&lt;/li&gt;
&lt;li&gt;Supports different sklearn metrics for regression, classification and clustering&lt;/li&gt;
&lt;li&gt;Supports multi-output/multi-target regression and classification&lt;/li&gt;
&lt;li&gt;Supports multi-processing for parallel model construction&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;igel is built on top of scikit-learn. It provides a simple way to use machine learning without writing a &lt;strong&gt;single line of code&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;All you need is a &lt;strong&gt;yaml&lt;/strong&gt; (or &lt;strong&gt;json&lt;/strong&gt;) file, where you need to describe what you are trying to do. That's it!&lt;/p&gt;
&lt;p&gt;Igel supports all sklearn's machine learning functionality, whether regression, classification or clustering.&lt;/p&gt;

&lt;h2&gt;Installation&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;The easiest way is to install igel using &lt;a href=&quot;https://packaging.python.org/guides/tool-recommendations/&quot; rel=&quot;nofollow&quot;&gt;pip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;highlight highlight-text-shell-session&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;pip install -U igel&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;Check the docs for other ways to install igel from source&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Models&lt;/h2&gt;
&lt;p&gt;Igel's supported models:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c1&quot;&gt;+--------------------+----------------------------+-------------------------+&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|      regression    |        classification      |        clustering       |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;+--------------------+----------------------------+-------------------------+&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|   LinearRegression |         LogisticRegression |                  KMeans |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|              Lasso |                      Ridge |     AffinityPropagation |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|          LassoLars |               DecisionTree |                   Birch |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;| BayesianRegression |                  ExtraTree | AgglomerativeClustering |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|    HuberRegression |               RandomForest |    FeatureAgglomeration |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|              Ridge |                 ExtraTrees |                  DBSCAN |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|  PoissonRegression |                        SVM |         MiniBatchKMeans |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|      ARDRegression |                  LinearSVM |    SpectralBiclustering |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|  TweedieRegression |                      NuSVM |    SpectralCoclustering |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;| TheilSenRegression |            NearestNeighbor |      SpectralClustering |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|    GammaRegression |              NeuralNetwork |               MeanShift |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|   RANSACRegression | PassiveAgressiveClassifier |                  OPTICS |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|       DecisionTree |                 Perceptron |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|          ExtraTree |               BernoulliRBM |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|       RandomForest |           BoltzmannMachine |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|         ExtraTrees |       CalibratedClassifier |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|                SVM |                   Adaboost |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|          LinearSVM |                    Bagging |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|              NuSVM |           GradientBoosting |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|    NearestNeighbor |        BernoulliNaiveBayes |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|      NeuralNetwork |      CategoricalNaiveBayes |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|         ElasticNet |       ComplementNaiveBayes |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|       BernoulliRBM |         GaussianNaiveBayes |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|   BoltzmannMachine |      MultinomialNaiveBayes |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|           Adaboost |                       ---- |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|            Bagging |                       ---- |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;|   GradientBoosting |                       ---- |                    ---- |&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;+--------------------+----------------------------+-------------------------+&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Quick Start&lt;/h2&gt;
&lt;p&gt;you can run this command to get instruction on how to use the model:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel --help&lt;/span&gt;

# &lt;span class=&quot;pl-s1&quot;&gt;or just&lt;/span&gt;

$ &lt;span class=&quot;pl-s1&quot;&gt;igel -h&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;Take some time and read the output of help command. You ll save time later if you understand how to use igel.&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-help.gif&quot;&gt;&lt;img alt=&quot;assets/igel-help.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-help.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;First step is to provide a yaml file (you can also use json if you want). You can do this manually by creating a .yaml file and editing it yourself. However, if you are lazy (and you probably are, like me :D), you can use the igel init command to get started fast:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;igel init &amp;lt;args&amp;gt;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;possible optional args are: (notice that these args are optional, so you can also just run igel init if you want)&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;-type: regression, classification or clustering&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;-model: model you want to use&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;-target: target you want to predict&lt;/span&gt;


&lt;span class=&quot;pl-c1&quot;&gt;Example:&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;If I want to use neural networks to classify whether someone is sick or not using the indian-diabetes dataset,&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;then I would use this command to initliaze a yaml file:&lt;/span&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel init -type &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;classification&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt; -model &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;NeuralNetwork&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt; -target &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;sick&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel init&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;After runnig the command, an igel.yaml file will be created for you in the current working directory. You can check it out and modify it if you want to, otherwise you can also create everything from scratch.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-init.gif&quot;&gt;&lt;img alt=&quot;assets/igel-init.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-init.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;div class=&quot;highlight highlight-source-yaml&quot; readability=&quot;18&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; model definition&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;model&lt;/span&gt;:
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; in the type field, you can write the type of problem you want to solve. Whether regression, classification or clustering&lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Then, provide the algorithm you want to use on the data. Here I'm using the random forest algorithm&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;classification&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;algorithm&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;RandomForest     &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; make sure you write the name of the algorithm in pascal case&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;arguments&lt;/span&gt;:
        &lt;span class=&quot;pl-ent&quot;&gt;n_estimators&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;100&lt;/span&gt;   &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; here, I set the number of estimators (or trees) to 100&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;max_depth&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;30&lt;/span&gt;       &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; set the max_depth of the tree&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; target you want to predict&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Here, as an example, I'm using the famous indians-diabetes dataset, where I want to predict whether someone have diabetes or not.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Depending on your data, you need to provide the target(s) you want to predict here&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;:
    - &lt;span class=&quot;pl-s&quot;&gt;sick&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In the example above, I'm using random forest to classify whether someone have diabetes or not depending on some features in the dataset I used this &lt;a href=&quot;https://www.kaggle.com/uciml/pima-indians-diabetes-database&quot; rel=&quot;nofollow&quot;&gt;indian-diabetes dataset&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The expected way to use igel is from terminal (igel CLI):&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Run this command in terminal to fit/train a model, where you provide the &lt;strong&gt;path to your dataset&lt;/strong&gt; and the &lt;strong&gt;path to the yaml file&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel fit --data_path &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_csv_dataset.csv&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; --yaml_file &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_yaml_file.yaml&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

# &lt;span class=&quot;pl-s1&quot;&gt;or shorter&lt;/span&gt;

$ &lt;span class=&quot;pl-s1&quot;&gt;igel fit -dp &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_csv_dataset.csv&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; -yml &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_yaml_file.yaml&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;That's it. Your &quot;trained&quot; model can be now found in the model_results folder&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;(automatically created for you in your current working directory).&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;Furthermore, a description can be found in the description.json file inside the model_results folder.&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-fit.gif&quot;&gt;&lt;img alt=&quot;assets/igel-fit.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-fit.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;You can then evaluate the trained/pre-fitted model:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel evaluate -dp &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_evaluation_dataset.csv&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;This will automatically generate an evaluation.json file in the current directory, where all evaluation results are stored&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-eval.gif&quot;&gt;&lt;img alt=&quot;assets/igel-eval.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-eval.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;Finally, you can use the trained/pre-fitted model to make predictions if you are happy with the evaluation results:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel predict -dp &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;path_to_your_test_dataset.csv&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;This will generate a predictions.csv file in your current directory, where all predictions are stored in a csv file&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-pred.gif&quot;&gt;&lt;img alt=&quot;assets/igel-pred.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-pred.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-predict.gif&quot;&gt;&lt;img alt=&quot;assets/igel-predict.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-predict.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;You can combine the train, evaluate and predict phases using one single command called experiment:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel experiment -DP &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;path_to_train_data path_to_eval_data path_to_test_data&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt; -yml &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;path_to_yaml_file&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;This will run fit using train_data, evaluate using eval_data and further generate predictions using the test_data&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/nidhaloff/igel/blob/master/assets/igel-experiment.gif&quot;&gt;&lt;img alt=&quot;assets/igel-experiment.gif&quot; src=&quot;https://github.com/nidhaloff/igel/raw/master/assets/igel-experiment.gif&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Alternatively, you can also write code if you want to:&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;highlight highlight-source-python&quot; readability=&quot;12&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;pl-s1&quot;&gt;igel&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;pl-v&quot;&gt;Igel&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;# provide the arguments in a dictionary&lt;/span&gt;
&lt;span class=&quot;pl-s1&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;=&lt;/span&gt; {
        &lt;span class=&quot;pl-s&quot;&gt;'cmd'&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;'fit'&lt;/span&gt;,    &lt;span class=&quot;pl-c&quot;&gt;# provide the command you want to use. whether fit, evaluate or predict&lt;/span&gt;
        &lt;span class=&quot;pl-s&quot;&gt;'data_path'&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;'path_to_your_dataset'&lt;/span&gt;,
        &lt;span class=&quot;pl-s&quot;&gt;'yaml_path'&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;'path_to_your_yaml_file'&lt;/span&gt;
}

&lt;span class=&quot;pl-v&quot;&gt;Igel&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;params&lt;/span&gt;)
&lt;span class=&quot;pl-s&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;pl-s&quot;&gt;check the examples folder for more&lt;/span&gt;
&lt;span class=&quot;pl-s&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;The main goal of igel is to provide you with a way to train/fit, evaluate and use models without writing code. Instead, all you need is to provide/describe what you want to do in a simple yaml file.&lt;/p&gt;
&lt;p&gt;Basically, you provide description or rather configurations in the yaml file as key value pairs. Here is an overview of all supported configurations (for now):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-yaml&quot; readability=&quot;47&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; dataset operations&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;dataset&lt;/span&gt;:
    &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;csv&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;read_data_options&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; options you want to supply for reading your data&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;sep&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Delimiter to use.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;delimiter&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Alias for sep.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;header&lt;/span&gt;:     &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Row number(s) to use as the column names, and the start of the data.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;names&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; List of column names to use&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;index_col&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Column(s) to use as the row labels of the DataFrame,&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;usecols&lt;/span&gt;:    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Return a subset of the columns&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;squeeze&lt;/span&gt;:    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If the parsed data only contains one column then return a Series.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;prefix&lt;/span&gt;:     &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Prefix to add to column numbers when no header, e.g. ‘X’ for X0, X1, …&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;mangle_dupe_cols&lt;/span&gt;:   &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Duplicate columns will be specified as ‘X’, ‘X.1’, …’X.N’, rather than ‘X’…’X’. Passing in False will cause data to be overwritten if there are duplicate names in the columns.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;dtype&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Data type for data or columns&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;engine&lt;/span&gt;:     &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Parser engine to use. The C engine is faster while the python engine is currently more feature-complete.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;converters&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Dict of functions for converting values in certain columns. Keys can either be integers or column labels.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;true_values&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Values to consider as True.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;false_values&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Values to consider as False.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;skipinitialspace&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Skip spaces after delimiter.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;skiprows&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;skipfooter&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Number of lines at bottom of file to skip&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;nrows&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Number of rows of file to read. Useful for reading pieces of large files.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;na_values&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Additional strings to recognize as NA/NaN.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;keep_default_na&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Whether or not to include the default NaN values when parsing the data.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;na_filter&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Detect missing value markers (empty strings and the value of na_values). In data without any NAs, passing na_filter=False can improve the performance of reading a large file.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;verbose&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Indicate number of NA values placed in non-numeric columns.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;skip_blank_lines&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If True, skip over blank lines rather than interpreting as NaN values.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;parse_dates&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; try parsing the dates&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;infer_datetime_format&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in the columns, and if it can be inferred, switch to a faster method of parsing them.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;keep_date_col&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If True and parse_dates specifies combining multiple columns then keep the original columns.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;dayfirst&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; DD/MM format dates, international and European format.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;cache_dates&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If True, use a cache of unique, converted dates to apply the datetime conversion.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;thousands&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; the thousands operator&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;decimal&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Character to recognize as decimal point (e.g. use ‘,’ for European data).&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;lineterminator&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Character to break file into lines.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;escapechar&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; One-character string used to escape other characters.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;comment&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Indicates remainder of line should not be parsed. If found at the beginning of a line, the line will be ignored altogether. This parameter must be a single character.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;encoding&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Encoding to use for UTF when reading/writing (ex. ‘utf-8’).&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;dialect&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If provided, this parameter will override values (default or not) for the following parameters: delimiter, doublequote, escapechar, skipinitialspace, quotechar, and quoting&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;delim_whitespace&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Specifies whether or not whitespace (e.g. ' ' or '    ') will be used as the sep&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;low_memory&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Internally process the file in chunks, resulting in lower memory use while parsing, but possibly mixed type inference.&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;memory_map&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If a filepath is provided for filepath_or_buffer, map the file object directly onto memory and access the data directly from there. Using this option can improve performance because there is no longer any I/O overhead.&lt;/span&gt;


    &lt;span class=&quot;pl-ent&quot;&gt;split&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; split options&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;test_size&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; 0.2 means 20% for the test data, so 80% are automatically for training&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;shuffle&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;True   &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; whether to shuffle the data before/while splitting&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;stratify&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;None  &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If not None, data is split in a stratified fashion, using this as the class labels.&lt;/span&gt;

    &lt;span class=&quot;pl-ent&quot;&gt;preprocess&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; preprocessing options&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;missing_values&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;mean    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; other possible values: [drop, median, most_frequent, constant] check the docs for more&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;encoding&lt;/span&gt;:
            &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;oneHotEncoding  &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; other possible values: [labelEncoding]&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;scale&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; scaling options&lt;/span&gt;
            &lt;span class=&quot;pl-ent&quot;&gt;method&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;standard    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; standardization will scale values to have a 0 mean and 1 standard deviation  | you can also try minmax&lt;/span&gt;
            &lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;inputs  &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; scale inputs. | other possible values: [outputs, all] # if you choose all then all values in the dataset will be scaled&lt;/span&gt;


&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; model definition&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;model&lt;/span&gt;:
    &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;classification    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; type of the problem you want to solve. | possible values: [regression, classification, clustering]&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;algorithm&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;NeuralNetwork    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; which algorithm you want to use. | type igel algorithms in the Terminal to know more&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;arguments&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;default          &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; model arguments: you can check the available arguments for each model by running igel help in your terminal&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;use_cv_estimator&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;false     &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; if this is true, the CV class of the specific model will be used if it is supported&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;cross_validate&lt;/span&gt;:
        &lt;span class=&quot;pl-ent&quot;&gt;cv&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; number of kfold (default 5)&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;n_jobs&lt;/span&gt;:   &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; The number of CPUs to use to do the computation (default None)&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;verbose&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; The verbosity level. (default 0)&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; target you want to predict&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;:
    - &lt;span class=&quot;pl-s&quot;&gt;put the target you want to predict here&lt;/span&gt;
    - &lt;span class=&quot;pl-s&quot;&gt;you can assign many target if you are making a multioutput prediction&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;E2E Example&lt;/h2&gt;
&lt;p&gt;A complete end to end solution is provided in this section to prove the capabilities of &lt;strong&gt;igel&lt;/strong&gt;. As explained previously, you need to create a yaml configuration file. Here is an end to end example for predicting whether someone have diabetes or not using the &lt;strong&gt;decision tree&lt;/strong&gt; algorithm. The dataset can be found in the examples folder.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Fit/Train a model&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;highlight highlight-source-yaml&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-ent&quot;&gt;model&lt;/span&gt;:
    &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;classification&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;algorithm&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;DecisionTree&lt;/span&gt;

&lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;:
    - &lt;span class=&quot;pl-s&quot;&gt;sick&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel fit -dp path_to_the_dataset -yml path_to_the_yaml_file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it, igel will now fit the model for you and save it in a model_results folder in your current directory.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Evaluate the model&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Evaluate the pre-fitted model. Igel will load the pre-fitted model from the model_results directory and evaluate it for you. You just need to run the evaluate command and provide the path to your evaluation data.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel evaluate -dp path_to_the_evaluation_dataset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! Igel will evaluate the model and store statistics/results in an &lt;strong&gt;evaluation.json&lt;/strong&gt; file inside the model_results folder&lt;/p&gt;
&lt;p&gt;Use the pre-fitted model to predict on new data. This is done automatically by igel, you just need to provide the path to your data that you want to use prediction on.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel predict -dp path_to_the_new_dataset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! Igel will use the pre-fitted model to make predictions and save it in a &lt;strong&gt;predictions.csv&lt;/strong&gt; file inside the model_results folder&lt;/p&gt;

&lt;h2&gt;Advanced Usage&lt;/h2&gt;
&lt;p&gt;You can also carry out some preprocessing methods or other operations by providing them in the yaml file. Here is an example, where the data is split to 80% for training and 20% for validation/testing. Also, the data are shuffled while splitting.&lt;/p&gt;
&lt;p&gt;Furthermore, the data are preprocessed by replacing missing values with the mean ( you can also use median, mode etc..). check &lt;a href=&quot;https://www.kaggle.com/uciml/pima-indians-diabetes-database&quot; rel=&quot;nofollow&quot;&gt;this link&lt;/a&gt; for more information&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-yaml&quot; readability=&quot;14&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; dataset operations&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;dataset&lt;/span&gt;:
    &lt;span class=&quot;pl-ent&quot;&gt;split&lt;/span&gt;:
        &lt;span class=&quot;pl-ent&quot;&gt;test_size&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;0.2&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;shuffle&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;stratify&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;default&lt;/span&gt;

    &lt;span class=&quot;pl-ent&quot;&gt;preprocess&lt;/span&gt;: &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; preprocessing options&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;missing_values&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;mean    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; other possible values: [drop, median, most_frequent, constant] check the docs for more&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;encoding&lt;/span&gt;:
            &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;oneHotEncoding  &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; other possible values: [labelEncoding]&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;scale&lt;/span&gt;:  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; scaling options&lt;/span&gt;
            &lt;span class=&quot;pl-ent&quot;&gt;method&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;standard    &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; standardization will scale values to have a 0 mean and 1 standard deviation  | you can also try minmax&lt;/span&gt;
            &lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;inputs  &lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; scale inputs. | other possible values: [outputs, all] # if you choose all then all values in the dataset will be scaled&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; model definition&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;model&lt;/span&gt;:
    &lt;span class=&quot;pl-ent&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;classification&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;algorithm&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;RandomForest&lt;/span&gt;
    &lt;span class=&quot;pl-ent&quot;&gt;arguments&lt;/span&gt;:
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; notice that this is the available args for the random forest model. check different available args for all supported models by running igel help&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;n_estimators&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;100&lt;/span&gt;
        &lt;span class=&quot;pl-ent&quot;&gt;max_depth&lt;/span&gt;: &lt;span class=&quot;pl-c1&quot;&gt;20&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; target you want to predict&lt;/span&gt;
&lt;span class=&quot;pl-ent&quot;&gt;target&lt;/span&gt;:
    - &lt;span class=&quot;pl-s&quot;&gt;sick&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, you can fit the model by running the igel command as shown in the other examples&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel fit -dp path_to_the_dataset -yml path_to_the_yaml_file&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For evaluation&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel evaluate -dp path_to_the_evaluation_dataset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For production&lt;/p&gt;
&lt;div class=&quot;highlight highlight-text-shell-session&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
$ &lt;span class=&quot;pl-s1&quot;&gt;igel predict -dp path_to_the_new_dataset&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Examples&lt;/h2&gt;
&lt;p&gt;In the examples folder in the repository, you will find a data folder,where the famous indian-diabetes, iris dataset and the linnerud (from sklearn) datasets are stored. Furthermore, there are end to end examples inside each folder, where there are scripts and yaml files that will help you get started.&lt;/p&gt;
&lt;p&gt;The indian-diabetes-example folder contains two examples to help you get started:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The first example is using a &lt;strong&gt;neural network&lt;/strong&gt;, where the configurations are stored in the neural-network.yaml file&lt;/li&gt;
&lt;li&gt;The second example is using a &lt;strong&gt;random forest&lt;/strong&gt;, where the configurations are stored in the random-forest.yaml file&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The iris-example folder contains a &lt;strong&gt;logistic regression&lt;/strong&gt; example, where some preprocessing (one hot encoding) is conducted on the target column to show you more the capabilities of igel.&lt;/p&gt;
&lt;p&gt;Furthermore, the multioutput-example contains a &lt;strong&gt;multioutput regression&lt;/strong&gt; example. Finally, the cv-example contains an example using the Ridge classifier using cross validation.&lt;/p&gt;
&lt;p&gt;I suggest you play around with the examples and igel cli. However, you can also directly execute the fit.py, evaluate.py and predict.py if you want to.&lt;/p&gt;

&lt;h2&gt;Links&lt;/h2&gt;

&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;You think this project is useful and you want to bring new ideas, new features, bug fixes, extend the docs?&lt;/p&gt;
&lt;p&gt;Contributions are always welcome. Make sure you read &lt;a href=&quot;https://igel.readthedocs.io/en/latest/contributing.html&quot; rel=&quot;nofollow&quot;&gt;the guidelines&lt;/a&gt; first&lt;/p&gt;

&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;MIT license&lt;/p&gt;
</description>
<pubDate>Sat, 03 Oct 2020 12:23:48 +0000</pubDate>
<dc:creator>nidhaloff</dc:creator>
<og:image>https://repository-images.githubusercontent.com/290879784/30f98100-f3a6-11ea-8d5c-a012fb399818</og:image>
<og:type>object</og:type>
<og:title>nidhaloff/igel</og:title>
<og:url>https://github.com/nidhaloff/igel</og:url>
<og:description>a machine learning tool that allows to train, test and use models without writing code - nidhaloff/igel</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/nidhaloff/igel</dc:identifier>
</item>
<item>
<title>Modi government to set up app store alternative to Google Play, Apple App Store</title>
<link>https://www.deccanherald.com/specials/aatmanirbhar-bharat-modi-govt-to-set-up-app-store-alternative-to-google-play-apple-app-store-895755.html?&amp;ampcf=1</link>
<guid isPermaLink="true" >https://www.deccanherald.com/specials/aatmanirbhar-bharat-modi-govt-to-set-up-app-store-alternative-to-google-play-apple-app-store-895755.html?&amp;ampcf=1</guid>
<description>&lt;p&gt;India government is reportedly planning to develop its own App Store to end the duopoly of the mobile ecosystem by Google and Apple.&lt;/p&gt;
&lt;p&gt;In recent weeks, several start-ups and app-makers have been voicing concern over Google's policy, which made it mandatory for all developers to give 30% commission for all billings on apps registered on the Play store. Even some popular apps such as Zomato and Swiggy were also sent a notice for violation of store policy. Even Paytm was suspended for several hours and only after complying with Google's policy, was restored.&lt;/p&gt;
&lt;p&gt;Apparently, affected apps mostly made by Indian firms planned to cash-in on the Indian Premier League cricket tournament and offered in-app gaming features to increase traffic and eventual monetary transactions. But, it was deemed a violation and Google termed them akin to betting and gambling, and were asked to shut the activities.&lt;/p&gt;
&lt;p&gt;This week several prominent app creators including Paytm CEO Vijay Shekhar Sharma and PhonePe head Sameer Nigam called the latest Play store guidelines as 'unfair and harsh'. They are talks of familiar minded people forming an association to urge the government to help them and take on Google.&lt;/p&gt;
&lt;p&gt;Prime Minister Narendra Modi, earlier in the year, in light of the Covid-19 pandemic, had called for the Aatmanirbhar Bharat initiative to become self-reliant. Now, the Modi government has asked the Centre for Development of Advanced Computing (C-DAC) to create a secured App Store for Indian apps, reported &lt;a href=&quot;https://www.timesnownews.com/business-economy/industry/article/modi-govt-planning-to-launch-its-own-app-store-as-an-alternative-to-google-apples-app-stores/660806&quot; target=&quot;_blank&quot;&gt;ET Now&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Google's Android and Apple iOS have 97% and 2% market share respectively in the Indian smartphone market. Many opine they have too much control and the 30% commission for all in-app purchases on their ecosystem, is a bit too autocratic in nature.&lt;/p&gt;
&lt;p&gt;The government's existing Mobile Seva App Store is expected to be refurbished to create the Indian App Store and all the applications in it, will not be charged for gatekeeping. &lt;/p&gt;
&lt;p&gt;Also, talks within the government indicate that it may ask Google and Apple to pre-install Indian App Store in all phones sold in India in near future. This particular request is likely to face counter from the American companies.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Get the latest news on new launches, gadget reviews, apps, cybersecurity, and more on personal technology only on &lt;strong&gt;&lt;a href=&quot;https://www.deccanherald.com/tag/dh-tech?_ga=2.210580691.73733284.1595225125-1706599323.1592232366&quot; target=&quot;_blank&quot;&gt;DH Tech&lt;/a&gt;.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 03 Oct 2020 12:19:54 +0000</pubDate>
<dc:creator>superasn</dc:creator>
<og:title>Aatmanirbhar Bharat: Modi govt to set-up App Store alternative to Google Play, Apple App Store</og:title>
<og:type>article</og:type>
<og:url>https://www.deccanherald.com/specials/aatmanirbhar-bharat-modi-govt-to-set-up-app-store-alternative-to-google-play-apple-app-store-895755.html</og:url>
<og:description>India government is reportedly planning to develop its own App Store to end the duopoly of the mobile ecosystem by Google and Apple. In recent weeks, several start-ups and app-makers have been voicing concern over Google's policy, which made it mandatory for all developers to give 30% commission for all billings on apps registered on the Play store. Even some popular apps such as Zomato and Swiggy were also sent a notice for violation of store policy. Even Paytm was suspended for several hours and only after complying with Google's policy, was restored.</og:description>
<og:image>https://www.deccanherald.com/sites/dh/files/articleimages/2020/10/01/app-store-895755-1601550229.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.deccanherald.com/specials/aatmanirbhar-bharat-modi-govt-to-set-up-app-store-alternative-to-google-play-apple-app-store-895755.html?&amp;ampcf=1</dc:identifier>
</item>
<item>
<title>Rust Starter Kit 2020</title>
<link>https://wiki.alopex.li/RustStarterKit2020</link>
<guid isPermaLink="true" >https://wiki.alopex.li/RustStarterKit2020</guid>
<description>&lt;p&gt;People were arguing about Rust’s std lib recently, so I went through the &lt;code&gt;Cargo.toml&lt;/code&gt; of all the Rust projects I’ve written since 2015 and picked out the choice tools that get used over and over again. Up to date as of October 2020.&lt;/p&gt;
&lt;p&gt;Also see &lt;a href=&quot;https://wiki.alopex.li/RustCrates&quot; class=&quot;uri&quot; title=&quot;Go to wiki page&quot;&gt;RustCrates&lt;/a&gt;, though that’s old. There’s also &lt;a href=&quot;https://christine.website/blog/rust-crates-go-stdlib-2020-09-27&quot;&gt;this&lt;/a&gt;, which is narrower but deeper, and &lt;a href=&quot;https://github.com/rust-unofficial/awesome-rust&quot;&gt;awesome-rust&lt;/a&gt;, which is shallower and broader, and the various &lt;a href=&quot;https://www.arewewebyet.org/&quot;&gt;more&lt;/a&gt; &lt;a href=&quot;https://arewegameyet.rs/&quot;&gt;specific&lt;/a&gt; &lt;a href=&quot;https://areweasyncyet.rs/&quot;&gt;websites&lt;/a&gt; &lt;a href=&quot;https://www.areweguiyet.com/&quot;&gt;for various&lt;/a&gt; &lt;a href=&quot;https://areweideyet.com/&quot;&gt;topics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I need to set up a new Rust dev environment, what do I install?&lt;/p&gt;
&lt;h2 id=&quot;linting-clippy&quot;&gt;Linting – &lt;code&gt;clippy&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The one, the only, the great Rust style and correctness linter. Want to learn how to write “idiomatic” Rust, or just learn more about handy little corners of the language and library? Run &lt;code&gt;clippy&lt;/code&gt; regularly. It’s distributed with the compiler via &lt;code&gt;rustup&lt;/code&gt; now, so you have no excuse not to.&lt;/p&gt;
&lt;h2 id=&quot;build-cache-sccache&quot;&gt;Build cache – &lt;code&gt;sccache&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Or, “how to make a full rebuild 70% faster”. &lt;code&gt;sccache&lt;/code&gt; is a build artifact cache similar to &lt;code&gt;icecream&lt;/code&gt; or &lt;code&gt;ccache&lt;/code&gt;, except it’s actually trivial to just use. &lt;code&gt;cargo install sccache&lt;/code&gt;, add a single line in a home dir config file, and you’re ready to go. Pretty much handles most crate and compiler versioning issues for you, so it Just Works if you update crates or install a new version of &lt;code&gt;rustc&lt;/code&gt; or something. I think I’ve had to force-clear the cache due to some build weirdness a grand total of once. Looks like it has enough features to use in a professional context as well, at least on a small-to-medium scale.&lt;/p&gt;
&lt;h2 id=&quot;dependency-viewer-cargo-tree&quot;&gt;Dependency viewer – &lt;code&gt;cargo-tree&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The best way to view what dependencies you are using, and what dependencies they are using, and so on. Best way to start cracking down on flabby dependencies.&lt;/p&gt;
&lt;h2 id=&quot;benchmarking-criterion&quot;&gt;Benchmarking – &lt;code&gt;criterion&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Basically the best benchmark system out there. Incredibly simple to use, informative, and statistically sound. Doesn’t really do profiling, but it’s a good start for understanding your program’s performance, and better for proving that your implementation of X is faster than someone else’s.&lt;/p&gt;
&lt;h2 id=&quot;other-things&quot;&gt;Other things&lt;/h2&gt;
&lt;p&gt;Stuff that is less general purpose but occasionally very useful for the meta-programming process of choosing libraries, evaluating them, etc.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;cargo-geiger&lt;/code&gt; – Measures how much unsafe code is in a codebase, and its dependencies&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cargo-crev&lt;/code&gt; – A &lt;a href=&quot;https://wiki.alopex.li/ActuallyUsingCrev&quot;&gt;very neat tool&lt;/a&gt; for authoring and verifying distributed code reviews.&lt;/li&gt;
&lt;li&gt;Various tools maintained by &lt;a href=&quot;https://github.com/EmbarkStudios/rust-ecosystem&quot;&gt;Embark Studios&lt;/a&gt;, useful for production/company purposes like checking licenses, pinning specific versions of crates, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The cool stuff Real Computer Scientists write about.&lt;/p&gt;
&lt;h2 id=&quot;hashing&quot;&gt;Hashing&lt;/h2&gt;
&lt;p&gt;No specific crates here. There’s no single crate that provides All The Hash Algorithms, just lots of little ones that generally provide a single algorithm each. Just type the name of the algorithm you want into &lt;code&gt;crates.io&lt;/code&gt; and you’ll get at least a couple options, choose the one with 8 million downloads or whatever. &lt;code&gt;sha2&lt;/code&gt;, &lt;code&gt;md5&lt;/code&gt;, &lt;code&gt;crc&lt;/code&gt;, etc. Lots of them are written by the Rust core team.&lt;/p&gt;
&lt;h2 id=&quot;compression&quot;&gt;Compression&lt;/h2&gt;
&lt;p&gt;Same as the hashing category. Type &lt;code&gt;zip&lt;/code&gt; or &lt;code&gt;bzip2&lt;/code&gt; or whatever into crates.io and you’ll get what you need. &lt;code&gt;flate2&lt;/code&gt; might be the one crate that’s not quite trivial to find. Again, many of them are written by the Rust core team.&lt;/p&gt;
&lt;h2 id=&quot;encryption&quot;&gt;Encryption&lt;/h2&gt;
&lt;p&gt;I have little actual experience or authority on this topic, so I’m going to punt on this one.&lt;/p&gt;
&lt;h2 id=&quot;pseudorandom-number-generator&quot;&gt;Pseudorandom number generator&lt;/h2&gt;
&lt;p&gt;Use &lt;code&gt;oorandom&lt;/code&gt;. (Disclaimer, I wrote &lt;code&gt;oorandom&lt;/code&gt;, but people besides me seem to like it.) More usually you’ll see the &lt;code&gt;rand&lt;/code&gt; crate in use. If you’re doing Real Science and need to generate &lt;a href=&quot;https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.random.html&quot;&gt;fancy probabilities&lt;/a&gt;, then &lt;code&gt;rand&lt;/code&gt; is the right tool, but most people aren’t doing that. Otherwise &lt;code&gt;rand&lt;/code&gt; is complicated and has lots of features, while &lt;code&gt;oorandom&lt;/code&gt; is very simple and has about two features, and I expect 80% of code to use at least one of them. &lt;code&gt;rand&lt;/code&gt; has had several major breaking changes in its history that the rest of the ecosystem still hasn’t caught up with, while I intend &lt;code&gt;oorandom&lt;/code&gt;’s API to change maybe twice in my lifetime. (Its version number, while obeying semver, is mostly a joke.)&lt;/p&gt;
&lt;p&gt;There’s other lightweight PRNG crates that are just fine; see &lt;code&gt;oorandom&lt;/code&gt;’s readme for a list of some others and choose one you like. Whatever you choose, use the &lt;code&gt;getrandom&lt;/code&gt; crate to produce Real Random Seeds for it.&lt;/p&gt;

&lt;p&gt;“I just need to solve this ooooooone common problem, but it needs to be solved WELL…”&lt;/p&gt;
&lt;h2 id=&quot;logging-log&quot;&gt;Logging – &lt;code&gt;log&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Need to output log messages in your code? Why, use the &lt;code&gt;log&lt;/code&gt; crate. Where do the log messages go? &lt;code&gt;log&lt;/code&gt; provides only an interface, and that interface compiles to nothing if it isn’t used. You can write your own system for it to actually output the logs to, which is pretty easy, or use one of the small plethora of crates for it. My preferred one is &lt;code&gt;pretty_env_logger&lt;/code&gt;, but &lt;code&gt;fern&lt;/code&gt;, &lt;code&gt;slog&lt;/code&gt; and others are all good too.&lt;/p&gt;
&lt;h2 id=&quot;parallel-data-crunching-rayon&quot;&gt;Parallel data crunching – &lt;code&gt;rayon&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Ever have some computation where you have a big list of STUFF and want to process it in parallel, farming out jobs to as many threads as you have CPU’s? That’s what &lt;code&gt;rayon&lt;/code&gt; does, and it does it really, really well. You still &lt;a href=&quot;https://aspenuwu.me/posts/rust-optimization.html&quot;&gt;have to know what you’re doing&lt;/a&gt;, but changing a single &lt;code&gt;.iter()&lt;/code&gt; into &lt;code&gt;.par_iter()&lt;/code&gt; and watching your CPU-bound data-crunching run 8x faster is pretty magical. Now your CPU can help keep you warm this winter!&lt;/p&gt;
&lt;p&gt;Please never use it in a library. It’s rude to spawn threads in library code, unless that’s specifically what the library is for.&lt;/p&gt;
&lt;h2 id=&quot;regexes-regex&quot;&gt;Regexes – &lt;code&gt;regex&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;To quote the inestimable &lt;a href=&quot;https://www.jwz.org/blog/&quot;&gt;jwz&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On the other hand, &lt;a href=&quot;https://xkcd.com/208/&quot;&gt;somebody’s gotta save the day&lt;/a&gt;. So, use the &lt;code&gt;regex&lt;/code&gt; crate. Also use &lt;a href=&quot;https://crates.io/crates/ripgrep&quot;&gt;anything else&lt;/a&gt; &lt;a href=&quot;https://crates.io/crates/xsv&quot;&gt;written by BurntSushi&lt;/a&gt;. BurntSushi is a paragon of Rust program design, and also just a great &lt;del&gt;human being&lt;/del&gt; charred cuisine in general.&lt;/p&gt;
&lt;h2 id=&quot;threadsafe-globals-lazy_static&quot;&gt;Threadsafe globals – &lt;code&gt;lazy_static&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;“I know globals are evil,” you say, “but I just need one. I’ll only use it for good, I promise.” &lt;code&gt;lazy_static&lt;/code&gt; has your back.&lt;/p&gt;
&lt;p&gt;May eventually be superseded by &lt;code&gt;once_cell&lt;/code&gt;, which looks like its &lt;a href=&quot;https://github.com/rust-lang/rfcs/pull/2788&quot;&gt;headed for inclusion into &lt;code&gt;std&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;serializationdeserialization-serde&quot;&gt;Serialization/deserialization – &lt;code&gt;serde&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Ever have a struct and just wanted to turn it into JSON, CBOR, XML, or some other engine of woe and devastation designed to be written to an I/O stream? Or had a blob of random JSON and wanted to just stuff it into a struct matching it? Sure you have. &lt;code&gt;serde&lt;/code&gt; lets you do this with a single &lt;code&gt;#[derive]&lt;/code&gt;. &lt;code&gt;serde&lt;/code&gt; is without a doubt one of Rust’s killer libraries. It is better than any other serialization system I have ever used.&lt;/p&gt;
&lt;p&gt;What data formats does it support? Anything; the actual reading and writing is done via plugin library. There’s a &lt;a href=&quot;https://serde.rs/#data-formats&quot;&gt;wide selection of them&lt;/a&gt;, of varying quality, and writing your own is a little tedious but not terribly difficult.&lt;/p&gt;
&lt;h2 id=&quot;error-handling&quot;&gt;Error handling&lt;/h2&gt;
&lt;p&gt;This spot deliberately left blank.&lt;/p&gt;
&lt;p&gt;Rust’s &lt;code&gt;Result&amp;lt;T,E&amp;gt;&lt;/code&gt; type is one of the best setups for lightweight, transparent error handling I’ve seen, but it doesn’t do everything. How do you easily write your own error type without a bunch of boilerplate? What if you have multiple different error types from different libraries you want to coalesce together? How do you collect a backtrace of every function an &lt;code&gt;Err&lt;/code&gt; is returned through, so you can find the root cause of where it came from? Can we do all this without allocating anything unnecessarily? And so on.&lt;/p&gt;
&lt;p&gt;There have been various crates to try to solve these problems. First in 2015 there was &lt;code&gt;error_chain&lt;/code&gt;, which was complicated and not very convenient. Then in 2017 there was &lt;code&gt;failure&lt;/code&gt;, which was simpler but not very flexible, and which took an irritatingly long time to compile. Then in 2019 there was &lt;code&gt;anyhow&lt;/code&gt;, which was about the time I stopped paying attention. Now apparently the new kid on the block is &lt;code&gt;eyre&lt;/code&gt;, and I’m sure that in another year or two there will be something else.&lt;/p&gt;
&lt;p&gt;So, I just write the boilerplate and make my errors descriptive enough I don’t need a backtrace. When I want to get fancy I implement the built-in &lt;a href=&quot;https://doc.rust-lang.org/std/error/trait.Error.html&quot;&gt;&lt;code&gt;Error&lt;/code&gt;&lt;/a&gt; trait, which used to be kinda useless but is now more helpful. And in another five years it’ll still work just fine.&lt;/p&gt;
&lt;h2 id=&quot;byte-mucking-bytemuck&quot;&gt;Byte mucking – &lt;code&gt;bytemuck&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;For the rare occasions you need to turn a structure into arbitrary &lt;code&gt;&amp;amp;[u8]&lt;/code&gt; or back. Doing this using unsafe pointers is quite easy, and also makes it very easy to screw up horribly with Undefined Behavior galore. (Did you know that changing the value of padding bytes in a struct in UB? You do now.) &lt;code&gt;bytemuck&lt;/code&gt; lets you muck around with bytes a little more responsibly.&lt;/p&gt;
&lt;h2 id=&quot;human-dates-and-times-chrono&quot;&gt;Human dates and times – &lt;code&gt;chrono&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Rust’s &lt;code&gt;std::time&lt;/code&gt; doesn’t really handle calendar or wall-clock times, just arbitrary, monotonic &lt;code&gt;Instant&lt;/code&gt;’s and measurable &lt;code&gt;Duration&lt;/code&gt;’s between them. Nice, pure, computationally-robust time measurement. For all the nasty human calendar and timezone stuff, you use &lt;code&gt;chrono&lt;/code&gt;. (And maybe &lt;code&gt;humantime&lt;/code&gt;, but I personally reach for &lt;code&gt;chrono&lt;/code&gt; first, just out of habit.)&lt;/p&gt;
&lt;h2 id=&quot;bit-flags-bitflags&quot;&gt;Bit flags – &lt;code&gt;bitflags&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Defining type-safe bit-masks in a reasonably convenient way. Not always worth the trouble, but sometimes pretty convenient.&lt;/p&gt;

&lt;p&gt;“I have to create or read a…”&lt;/p&gt;
&lt;h2 id=&quot;pngjpeggifetc-image&quot;&gt;PNG/JPEG/GIF/etc – &lt;code&gt;image&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;General-purpose loading and saving and images, which can handle a lot of formats. Can do some amount of image manipulation as well, such as cropping, smoothing, etc. but that will hopefully be pulled out into its own library at some point soon.&lt;/p&gt;
&lt;h2 id=&quot;small-data-things-uuid-base64-csv-semver&quot;&gt;Small data THINGS – &lt;code&gt;uuid&lt;/code&gt;, &lt;code&gt;base64&lt;/code&gt;, &lt;code&gt;csv&lt;/code&gt;, &lt;code&gt;semver&lt;/code&gt;…&lt;/h2&gt;
&lt;p&gt;Exactly what it says on the tin.&lt;/p&gt;

&lt;p&gt;Not aware of any great encoders, but there’s plenty of &lt;em&gt;decoders&lt;/em&gt; for common audio formats. &lt;code&gt;lewton&lt;/code&gt; for Ogg Vorbis, &lt;code&gt;hound&lt;/code&gt; for .wav, &lt;code&gt;minimp3&lt;/code&gt; for MP3, &lt;code&gt;claxon&lt;/code&gt; for FLAC. Video, I haven’t used enough to have an opinion on.&lt;/p&gt;
&lt;h2 id=&quot;config-files-toml&quot;&gt;Config files – &lt;code&gt;toml&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;For all your config file format needs. Works with &lt;code&gt;serde&lt;/code&gt;, naturally.&lt;/p&gt;
&lt;h2 id=&quot;markdown-pulldown-cmark&quot;&gt;Markdown – &lt;code&gt;pulldown-cmark&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There’s several good Markdown readers and writers, &lt;code&gt;pulldown-cmark&lt;/code&gt; is my favorite. It supports CommonMark, it’s simple to use, and it’s pure Rust.&lt;/p&gt;
&lt;h2 id=&quot;templating-askama&quot;&gt;Templating – &lt;code&gt;askama&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;There’s several quite good text templating engines, but &lt;code&gt;askama&lt;/code&gt; IMO rises above them all by compiling your templates into Rust code and type-checking your templates at compile time. Sometimes this isn’t what you want, but it is a great feature surprisingly often. This also makes it super fast, for when you really need to generate 150,000 HTML pages in a couple minutes. For when I need templates that can be altered at runtime, on the other hand, &lt;code&gt;tera&lt;/code&gt; is my default pick.&lt;/p&gt;
&lt;h2 id=&quot;lexingparsing&quot;&gt;Lexing/parsing&lt;/h2&gt;
&lt;p&gt;For lexing, I have never seen anything better than &lt;code&gt;logos&lt;/code&gt;. Derive a trait on your token type, add a few annotations, and you have a lexer.&lt;/p&gt;
&lt;p&gt;There are many good parser libraries out there in different styles: &lt;code&gt;nom&lt;/code&gt;, &lt;code&gt;pom&lt;/code&gt;, &lt;code&gt;lalrpop&lt;/code&gt;, &lt;code&gt;pest&lt;/code&gt;, &lt;code&gt;combine&lt;/code&gt;… They’re mostly quite good, in my experience, but in practice I tend to either use Known Formats like CBOR or TOML, or I write a parser by hand. Play around and find what you like.&lt;/p&gt;

&lt;p&gt;Rust’s standard library includes most data structures you need: hash map, ring buffer, all the good things. Can’t include everything though, so here’s a few things to fill in the cracks.&lt;/p&gt;
&lt;h2 id=&quot;immutable-data-structures-im&quot;&gt;Immutable data structures – &lt;code&gt;im&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Fast immutable data structures. Rust is a little weird, ’cause the borrowing and ownership actually makes immutable data structures both difficult and usually unnecessary – things can’t be mutated without you knowing it, because the borrow checker won’t let you. This makes trying to write code in a purely functional style feel weird. However, there’s some situations where immutable data structures are actually really nice – the one I ran into was passing shared data structures around in an Erlang-like multithreaded actor system. For those cases, &lt;code&gt;im&lt;/code&gt; is the way to go.&lt;/p&gt;
&lt;h2 id=&quot;generational-map-genmap&quot;&gt;Generational map – &lt;code&gt;genmap&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;What I call a “generational map”; I’ve also seen it called a slot map, handle map, and some other things I can’t remember. This is very much not a new structure, but I haven’t seen it often outside of video game code. Like &lt;code&gt;oorandom&lt;/code&gt;, this particular crate is my own implementation, but there’s several others that are good for different use cases; see the crate’s README file for links to some others.&lt;/p&gt;
&lt;p&gt;So what does a generational map do? It’s basically an array where you insert things and get an opaque handle back, which is actually just the array index. This is a quite useful pattern in Rust code when you have a collection of things with complicated dynamically-checked lifetimes, but using an &lt;code&gt;Rc&lt;/code&gt; for them feels icky. The improvement over just using plain array indices is that a generational map also has a “generation” count also stored in each handle, and the map’s generation counter increments each time you remove an object from the array. So when you remove an item, you can put a new one in the same slot in the array and reuse that storage, but the new object’s handle will have a different generation counter. So if you try to look up an object with an old handle that used to refer to something else, instead of silently getting the wrong item, you get a runtime error. It’s kind of the inverse of an &lt;code&gt;Rc&lt;/code&gt;, which promises that an object will never be freed until there are no more pointers referring to it. A generational map lets you free an object, but trying to use a dangling pointer will be a safe runtime error.&lt;/p&gt;
&lt;h2 id=&quot;stable-hashtables-fnv&quot;&gt;Stable hashtables – &lt;code&gt;fnv&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;fnv&lt;/code&gt; is a fast, stable hash algorithm, and the crate includes simple wrappers for using it with Rust’s standard &lt;code&gt;HashMap&lt;/code&gt; and &lt;code&gt;HashSet&lt;/code&gt;. Rust’s default &lt;code&gt;HashMap&lt;/code&gt; is not “stable”, which is to say, it contains an element of randomness. If you make two different &lt;code&gt;HashMap&lt;/code&gt;’s and insert the exact same values into them in the same order, they will be stored in a different order internally, which prevents &lt;a href=&quot;https://en.wikipedia.org/wiki/Collision_attack#Usage_in_DoS_attacks&quot;&gt;Hash DoS&lt;/a&gt; attacks. This is the sane default you want, but sometimes you want a stable hash map (so giving it the same contents results in the same ordering) or you need slightly higher performance. The &lt;code&gt;fnv&lt;/code&gt; crate gives you both. The main time I wanted it was writing a small compiler where I wanted to have reproducible builds.&lt;/p&gt;
&lt;h2 id=&quot;multithreaded-hashmap-dashmap&quot;&gt;Multithreaded hashmap – &lt;code&gt;dashmap&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I’ve never actually used this but always wanted to find an excuse to. Basically a replacement for &lt;code&gt;Mutex&amp;lt;HashMap&amp;lt;K,V&amp;gt;&amp;gt;&lt;/code&gt; that breaks the hashmap into several portions, each with its own lock, increasing performance and reducing contention. There’s a few similar crates out there, some of which are lockless, but like I said I’ve never had an excuse to use &lt;code&gt;dashmap&lt;/code&gt;, so I haven’t explored alternatives either.&lt;/p&gt;
&lt;h2 id=&quot;thread-pool&quot;&gt;Thread pool – ???&lt;/h2&gt;
&lt;p&gt;I haven’t done it often enough to have specific recommendations, but if you’re just feeding data to a fixed number of worker threads, using a thread pool of some kind is generally simpler and more convenient than herding threads around yourself. There’s also various scoped threadpools, which make sure that the threads never outlive the current program scope. This lets you feed references to local data into the threads, instead of forcing you to clone or &lt;code&gt;Arc&lt;/code&gt; them.&lt;/p&gt;

&lt;p&gt;“I need something fancier than simple network sockets but I’m not writing an HTTP service.”&lt;/p&gt;
&lt;h2 id=&quot;http-client-reqwest&quot;&gt;HTTP client – &lt;code&gt;reqwest&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Need to fetch something via HTTP? &lt;code&gt;reqwest&lt;/code&gt; is the way to do it. Not the most lightweight of things, but it’s fast, robust, and can do about anything.&lt;/p&gt;
&lt;p&gt;Lightweight alternative to try out: &lt;code&gt;ureq&lt;/code&gt;. Haven’t used it personally yet, though.&lt;/p&gt;

&lt;p&gt;“Okay, I’m making an interactive program, so it needs to be able to…”&lt;/p&gt;
&lt;h2 id=&quot;command-line-parsing-structopt&quot;&gt;Command line parsing – &lt;code&gt;structopt&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Like &lt;code&gt;serde&lt;/code&gt;, I consider this one of Rust’s killer libraries. No other language has anything nearly as good. You define a Rust struct and throw some annotations onto it, and you now have a command line parser. It’s amazing. Its one downside is it’s relatively slow to compile and doesn’t produce the tiniest code, though it’s not too bad. Lightweight alternatives: &lt;code&gt;argh&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&quot;env-file-loading-dotenv&quot;&gt;Env file loading – &lt;code&gt;dotenv&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Load options from environment variables or &lt;code&gt;.env&lt;/code&gt; files in the local directory. This is generally what server systems do for configuration so they don’t have to store usernames and passwords in config files that can get accidentally checked into version control. Very handy pattern if you’re making something that needs to touch a remote database or web service. It’s trivial to implement on your own, but it’s already been implemented once, so why redo the work?&lt;/p&gt;
&lt;h2 id=&quot;cross-platform-config-file-locations-directories&quot;&gt;Cross-platform config file locations – &lt;code&gt;directories&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Duality_(mathematics)&quot;&gt;dual&lt;/a&gt; of &lt;code&gt;dotenv&lt;/code&gt;, it gives you an easy, cross-platform way to access config files in the Right Place on whatever platform you’re using.&lt;/p&gt;
&lt;h2 id=&quot;interactive-cli-rustyline&quot;&gt;Interactive CLI – &lt;code&gt;rustyline&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Rust equivalent of the &lt;code&gt;readline&lt;/code&gt; library. &lt;code&gt;readline&lt;/code&gt; (or its descendent &lt;code&gt;linenoise&lt;/code&gt;) is what &lt;code&gt;bash&lt;/code&gt; and just about every other command line program in existence uses to give you an editable terminal that lets you backspace, jump to beginning/end of lines, search backwards through a buffer, and so on. &lt;code&gt;rustyline&lt;/code&gt; does the same thing without needing to bind to a C library.&lt;/p&gt;
&lt;h2 id=&quot;progress-bar-pbr&quot;&gt;Progress bar – &lt;code&gt;pbr&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Displays a progress bar in the console, with simple configurable styles. It’s amazingly handy.&lt;/p&gt;

&lt;p&gt;I am really not the person to ask here, and what knowledge I do have is &lt;a href=&quot;https://wiki.alopex.li/AnOpinionatedGuideToRustWebServers&quot;&gt;pretty out of date&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Again, I am the wrong person to ask, but in my &lt;em&gt;modest&lt;/em&gt; experience… &lt;code&gt;diesel&lt;/code&gt; is a very good, database-independent query builder. Database-independent query builders/ORM’s/etc might also be a bit of a white elephant. Usually life gets simpler when I just find a crate that interfaces with Postgres or SQLite directly, and I end up making better systems when I do it in terms of SQL directly. &lt;code&gt;diesel&lt;/code&gt;’s migrations are pretty nice, though. I dunno.&lt;/p&gt;

&lt;p&gt;Because every positive is incomplete without its negative. These are things that are popular, and are definitely not bad code or bad designs, but which I will personally choose not to use without a good reason. There are good reasons to use them, but most of the time if I see these in a project’s dependency tree I wince a little and expect excess complexity, two minute compiles, or inconvenient breaking changes within 6-12 months.&lt;/p&gt;
&lt;p&gt;Again, these are from my personal experience and priorities. Your needs are not necessarily mine.&lt;/p&gt;
&lt;h2 id=&quot;ring&quot;&gt;&lt;code&gt;ring&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;A very correct, very complete, very rigorous encryption library. Due to the level of rigor involved, and the &lt;del&gt;analness&lt;/del&gt; personality of the people required to maintain that level of rigor, they have an &lt;a href=&quot;https://github.com/briansmith/ring#versioning--stability&quot;&gt;Interesting policy regarding releases&lt;/a&gt;. Plus, for technical reasons, you can’t have multiple versions of &lt;code&gt;ring&lt;/code&gt; linked into a single crate like you can with most crates – there’s some asm embedded in there, and the symbols in the asm code don’t get mangled by &lt;code&gt;rustc&lt;/code&gt; and so can clash. So if you write a program that uses &lt;code&gt;ring&lt;/code&gt;, and it has a dependency that uses &lt;code&gt;ring&lt;/code&gt;, they must always use the exact same version of it or else your program will not compile, and the &lt;code&gt;ring&lt;/code&gt; developers will shrug and say “that’s your problem, we aren’t going to make life easier for you”. It used to be that all but the most current versions of &lt;code&gt;ring&lt;/code&gt; were deliberately yanked by the developers – marked “do not use” on crates.io, which made it still available if you really wanted but &lt;code&gt;cargo&lt;/code&gt; would refuse to compile it. That just made the problem less tractable, and &lt;a href=&quot;https://crates.io/crates/isildur&quot;&gt;really irked me once upon a time&lt;/a&gt;, though fortunately they seem to have given up on this madness more recently.&lt;/p&gt;
&lt;p&gt;What this adds up to is, if you use &lt;code&gt;ring&lt;/code&gt;, then having your application compile and function from one day to the next is &lt;strong&gt;always&lt;/strong&gt; less important than &lt;code&gt;ring&lt;/code&gt; working correctly, for the value of “correctly” defined by the library devs, and you will get exactly zero warning or sympathy when someone else decides to break your code forever. This is a perfectly reasonable and robust approach to the problem of dealing with security in human-built tools, and I hate it. Best to just move on and deal with other things.&lt;/p&gt;
&lt;h2 id=&quot;tokio&quot;&gt;&lt;code&gt;tokio&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Tokio is a multi-threaded runtime for fast async I/O, particularly for network servers. It is also a ton of work to use, a ton of compile time, and a ton of learning to do. If you need it, it’s fantastic, but if you only kindasorta need it, I find it less work to pass it by. I’ll spawn a thread per connection, I ain’t scared! On the other hand, if you already use &lt;code&gt;tokio&lt;/code&gt; all the time and you’re already on top of the learning curve, then it’s probably pretty easy to just whip it out whenever you need something I/O-ish. Lightweight alternatives to try out sometime: &lt;a href=&quot;https://crates.io/crates/smol&quot;&gt;&lt;code&gt;smol&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://crates.io/crates/popol&quot;&gt;&lt;code&gt;popol&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;proc-macro2syn&quot;&gt;&lt;code&gt;proc-macro2&lt;/code&gt;/&lt;code&gt;syn&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;These are the foundational crates of Rust’s procedural macro system. They are kindasorta part of the compiler, but not actually distributed as part of the compiler for Reasons I haven’t bothered finding out. Procedural macros are what makes it possible to have awesome metaprogramming tools like &lt;code&gt;serde&lt;/code&gt;, &lt;code&gt;structopt&lt;/code&gt; and &lt;code&gt;logos&lt;/code&gt;, and these crates are the glue that binds it together. They are also &lt;em&gt;slow&lt;/em&gt; to compile, no two ways about it, and generating procedural macros is also &lt;em&gt;slow&lt;/em&gt;. So if you &lt;em&gt;need&lt;/em&gt; to make a tool that does awesome metaprogramming, or you need to use a tool that does awesome metaprogramming, then go ahead and use these. But IMO it’s not worth the extra twenty seconds of compile time for a small convenience crate like &lt;a href=&quot;https://github.com/JelteF/derive_more&quot;&gt;&lt;code&gt;derive_more&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;num&quot;&gt;&lt;code&gt;num&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;A crate containing lots of numerical traits for compile-time metaprogramming, like &lt;code&gt;Integer&lt;/code&gt;, &lt;code&gt;Unsigned&lt;/code&gt;, etc. That’s cool, but it’s a large, complicated, slow-compiling dependency that evolves breaking changes &lt;em&gt;just&lt;/em&gt; fast enough to be irritating. I’ve written and used a fair amount of math code of various types, and I’ve never actually wanted or needed &lt;code&gt;num&lt;/code&gt;’s metaprogramming traits, but those are the part that infiltrate the dependency trees of everything ever. On the flip side though, &lt;code&gt;num&lt;/code&gt; also provides quite nice implementations for fancier numbers like ratios, complex numbers, bigint’s etc, so it’s totally worth using for those.&lt;/p&gt;
&lt;h2 id=&quot;crossbeam&quot;&gt;&lt;code&gt;crossbeam&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Additional, occasionally faster concurrency primitives than what the standard library provides. Multiple-producer-multiple-consumer queues, exponential backoff timers, fancy atomic things, huzzah! Great stuff, but also 95% of the time you aren’t gonna need it. If you profile your program and discover you really need to be spending 3% of your time on &lt;code&gt;mpsc::send()&lt;/code&gt; instead of 5%, then you can always just drop &lt;code&gt;crossbeam&lt;/code&gt; in. But until then, if you can do your stuff with just what’s already in &lt;code&gt;std&lt;/code&gt;, please do.&lt;/p&gt;
&lt;h2 id=&quot;parking_lot&quot;&gt;&lt;code&gt;parking_lot&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;More compact and efficient implementations of the standard synchronization primitives. Which, like &lt;code&gt;crossbeam&lt;/code&gt;, you aren’t going to need. If benchmarks show that Rust’s standard synchronization primitives are too slow for your application, then go for it, but the time it takes to lock or unlock a &lt;code&gt;Mutex&lt;/code&gt; has never been a bottleneck for me. Okay, it has once, but I switched to using &lt;code&gt;rayon&lt;/code&gt; and it did everything I needed better.&lt;/p&gt;
&lt;p&gt;Also, if &lt;code&gt;parking_lot&lt;/code&gt;’s implementation of this stuff was unilaterally better it’d be in &lt;code&gt;std&lt;/code&gt; already, so I wonder what the catch is. – Update: Looks like the whole story is &lt;a href=&quot;https://github.com/rust-lang/rust/pull/56410&quot;&gt;here&lt;/a&gt;, and a more actionable summary is &lt;a href=&quot;https://github.com/faern/parking_lot/pull/1&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
<pubDate>Sat, 03 Oct 2020 12:04:28 +0000</pubDate>
<dc:creator>psxuaw</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://wiki.alopex.li/RustStarterKit2020</dc:identifier>
</item>
<item>
<title>Show HN: I made a Computer Vision addon for Blender</title>
<link>https://github.com/Cartucho/vision_blender</link>
<guid isPermaLink="true" >https://github.com/Cartucho/vision_blender</guid>
<description>&lt;p&gt;&lt;a href=&quot;https://github.com/Cartucho/vision_blender&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/32d96b8bd21f9bb33ca926599701375d730bff0e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f436172747563686f2f766973696f6e5f626c656e6465722e7376673f7374796c653d736f6369616c266c6162656c3d5374617273&quot; alt=&quot;GitHub stars&quot; data-canonical-src=&quot;https://img.shields.io/github/stars/Cartucho/vision_blender.svg?style=social&amp;amp;label=Stars&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A Blender user-interface to generate synthetic ground truth data (benchmarks) for Computer Vision applications.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://user-images.githubusercontent.com/15831541/94527156-7b944d80-022e-11eb-85bd-0b387fd519fb.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/15831541/94527156-7b944d80-022e-11eb-85bd-0b387fd519fb.png&quot; width=&quot;100%&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://user-images.githubusercontent.com/15831541/94527180-8353f200-022e-11eb-9bf5-5ebd6102bc9f.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/15831541/94527180-8353f200-022e-11eb-9bf5-5ebd6102bc9f.png&quot; width=&quot;100%&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;To install the addon simply go to &lt;code&gt;Edit &amp;gt; Preferences &amp;gt; Add-on tab &amp;gt; Install an add-on&lt;/code&gt; , then select the file &lt;code&gt;path/to/vision_blender/addon_ground_truth_generation.py&lt;/code&gt; and click &lt;code&gt;Install Add-on&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally you have to enable the add-on; Search &lt;code&gt;VisionBlender&lt;/code&gt; and tick the check-box. You should now be able to find the &lt;code&gt;VisionBlender UI&lt;/code&gt; in th bottom of the &lt;code&gt;Output Properties&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;How to generate ground truth data?&lt;/h2&gt;
&lt;p&gt;Simply tick the boxes of what you want to save as ground truth in the &lt;code&gt;VisionBlender UI&lt;/code&gt;. Then start rendering and the outputs will be generated automatically. To render you click &lt;code&gt;Render &amp;gt; Render Image&lt;/code&gt; or &lt;code&gt;Render &amp;gt; Render Animation...&lt;/code&gt;, alternatively you can click &lt;code&gt;F12&lt;/code&gt; for image and &lt;code&gt;Ctrl F12&lt;/code&gt; for animation.&lt;/p&gt;
&lt;p&gt;You can change the output path in &lt;code&gt;Output Properties &amp;gt; Output &amp;gt; Output Path&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note: &lt;code&gt;Segmentation masks&lt;/code&gt; and &lt;code&gt;Optical flow&lt;/code&gt; are only available in Cycles.&lt;/p&gt;
&lt;h4&gt;Segmentation masks&lt;/h4&gt;
&lt;p&gt;To set-up the segmentation masks you need to choose a pass index for each object: &lt;code&gt;Object Properties &amp;gt; Relations &amp;gt; Pass Index&lt;/code&gt;&lt;/p&gt;
&lt;h4&gt;Optical flow&lt;/h4&gt;
&lt;p&gt;You will only have optical flow if the camera or the objects are moving during an animation.&lt;/p&gt;
&lt;h3&gt;How to read the data after generating it?&lt;/h3&gt;
&lt;p&gt;You simply have to load the numpy arrays from thr &lt;code&gt;.npz&lt;/code&gt; files. Go to the &lt;code&gt;vision_blender/samples&lt;/code&gt; and have a look at the example there!&lt;/p&gt;
</description>
<pubDate>Sat, 03 Oct 2020 11:24:50 +0000</pubDate>
<dc:creator>morroida</dc:creator>
<og:image>https://avatars3.githubusercontent.com/u/15831541?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>Cartucho/vision_blender</og:title>
<og:url>https://github.com/Cartucho/vision_blender</og:url>
<og:description>A Blender addon for generating synthetic ground truth data for Computer Vision applications - Cartucho/vision_blender</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/Cartucho/vision_blender</dc:identifier>
</item>
<item>
<title>Adults with ADHD show decreased function in brain dopamine reward pathway (2010)</title>
<link>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/</link>
<guid isPermaLink="true" >https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/</guid>
<description>&lt;div id=&quot;idm140161746353760&quot; lang=&quot;en&quot; xml:lang=&quot;en&quot; readability=&quot;19.5&quot; xml:lang=&quot;en&quot;&gt;
&lt;h2 class=&quot;head no_bottom_margin&quot; id=&quot;idm140161746353760title&quot;&gt;Abstract&lt;/h2&gt;
&lt;div readability=&quot;20&quot;&gt;
&lt;p id=&quot;P1&quot; class=&quot;p p-first-last&quot;&gt;ADHD is typically characterized as a disorder of inattention and hyperactivity/impulsivity but there is increasing evidence of deficits in motivation. Using PET we showed decreased function in the brain dopamine reward pathway in adults with ADHD, which we hypothesized could underlie the motivation deficits in this disorder. To evaluate this hypothesis we performed secondary analyses to assess the correlation between the PET measures of dopamine D2/D3 receptor and dopamine transporter availability (obtained with [&lt;sup&gt;11&lt;/sup&gt;C]raclopride and [&lt;sup&gt;11&lt;/sup&gt;C]cocaine, respectively) in the dopamine reward pathway (midbrain and nucleus accumbens), and a surrogate measures of trait motivation (assessed using the Achievement scale on the Multidimensional Personality Questionnaire or MPQ) in 45 ADHD participants and 41 controls. The Achievement scale was lower in ADHD participants than in controls (11±5 vs 14±3, p&amp;lt;0.001) and was significantly correlated with D2/D3 receptors (accumbens: r=0.39, p&amp;lt;0.008; midbrain: r=0.41, p&amp;lt;0.005) and transporters (accumbens: r=0.35, p &amp;lt; 0.02) in ADHD participants, but not in controls. ADHD participants also had lower values in the Constraint factor and higher values in the Negative Emotionality factor of the MPQ but did not differ in the Positive Emotionality factor - and none of these were correlated with the dopamine measures. In ADHD participants scores in the Achievement scale were also negatively correlated with symptoms of inattention (CAARS A, E and SWAN-I). These findings provide evidence that disruption of the dopamine reward pathway is associated with motivation deficits in ADHD adults, which may contribute to attention deficits and supports the use of therapeutic interventions to enhance motivation in ADHD.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong class=&quot;kwd-title&quot;&gt;Keywords:&lt;/strong&gt; &lt;span class=&quot;kwd-text&quot;&gt;psychiatric disorder, brain imaging, PET, attention, catecholamines, personality&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;&lt;div id=&quot;idm140161751921984&quot; readability=&quot;43.686721253115&quot;&gt;
&lt;p id=&quot;P2&quot; class=&quot;p p-first&quot;&gt;Attention Deficit Hyperactivity Disorder (ADHD) is the most recognized and treated psychiatric disorder of childhood&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R1&quot; rid=&quot;R1&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189005&quot; name=&quot;__tag_513189005&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; and it is increasingly recognized and treated in adults&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R2&quot; rid=&quot;R2&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188984&quot; name=&quot;__tag_513188984&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. ADHD is characterized by symptoms of inattention and/or hyperactivity/impulsivity that produce impairment across cognitive, behavioral and interpersonal domains of function&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R1&quot; rid=&quot;R1&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188982&quot; name=&quot;__tag_513188982&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. However, the hypothesis that there is a dysfunction in reward and motivation was proposed over 2 decades ago&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R3&quot; rid=&quot;R3&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189009&quot; name=&quot;__tag_513189009&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; and there is increasing evidence that this plays a role in ADHD&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R3&quot; rid=&quot;R3&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189016&quot; name=&quot;__tag_513189016&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R4&quot; rid=&quot;R4&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188996&quot; name=&quot;__tag_513188996&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. For example, children with ADHD require stronger incentives to modify their behavior than those without ADHD&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R5&quot; rid=&quot;R5&quot; class=&quot; bibr popnode&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;; they also show a failure to delay gratification, have impaired responses to partial schedules of reinforcement, and preference for small immediate rewards over larger delayed rewards&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R6&quot; rid=&quot;R6&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188985&quot; name=&quot;__tag_513188985&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R7&quot; rid=&quot;R7&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188981&quot; name=&quot;__tag_513188981&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P3&quot;&gt;The mesoaccumbens dopamine (DA) pathway, which projects from the ventral tegmental area (VTA) in midbrain to the nucleus accumbens (NAcc) in the ventral striatum, is critically involved in reward and motivation&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R8&quot; rid=&quot;R8&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189004&quot; name=&quot;__tag_513189004&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, and has been hypothesized to underlie the reward and motivational deficits observed in ADHD&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R6&quot; rid=&quot;R6&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188993&quot; name=&quot;__tag_513188993&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R9&quot; rid=&quot;R9&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188987&quot; name=&quot;__tag_513188987&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;. Indeed, using positron emission tomography (PET) we showed lower than normal availability of DA D2/D3 receptors (measured with [&lt;sup&gt;11&lt;/sup&gt;C]raclopride) and of DA transporters (measured with [&lt;sup&gt;11&lt;/sup&gt;C]cocaine) in the midbrain and in NAcc in drug naïve ADHD participants compared to non-ADHD control subjects&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R10&quot; rid=&quot;R10&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188999&quot; name=&quot;__tag_513188999&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. Here we report on secondary analyses that were performed on a subset of subjects in whom we had collected personality measures, including trait measures of motivation, to test the hypothesis that disruption of the DA reward pathway is associated with the motivation deficit in ADHD.&lt;/p&gt;
&lt;p id=&quot;P4&quot; class=&quot;p&quot;&gt;For this purpose we measured the correlations between our PET measures of D2/D3 receptors and DA transporters (DAT) in the DA reward pathway (midbrain and NAcc) and questionnaire measures of trait motivation in drug-naïve ADHD participants and group-matched controls. DA D2/D3 receptor availability was measured with [&lt;sup&gt;11&lt;/sup&gt;C]raclopride&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R11&quot; rid=&quot;R11&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189003&quot; name=&quot;__tag_513189003&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; and DAT availability was measured with [&lt;sup&gt;11&lt;/sup&gt;C]cocaine&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R12&quot; rid=&quot;R12&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189013&quot; name=&quot;__tag_513189013&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;. The Multidimensional Personality Questionnaire (MPQ)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R13&quot; rid=&quot;R13&quot; class=&quot; bibr popnode&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, obtained in 45 ADHD participants (never medicated) and 41 controls, was used to obtain scores on the Achievement scale, which was used as a surrogate trait measure of motivation. The Achievement scale of the MPQ, evaluates a motivational disposition that comprises social dominance, enthusiasm, energy, assertiveness, ambitiousness, and achievement striving. We hypothesized that the deficits in the DA reward pathway in ADHD participants would be associated with lower scores in the Achievement scale (surrogate trait measure of motivation), and that lower scores in this surrogate measure of motivation would predict more severe ADHD symptoms.&lt;/p&gt;
&lt;/div&gt;&lt;div id=&quot;S1&quot; readability=&quot;47.677933915686&quot;&gt;
&lt;h2 class=&quot;head no_bottom_margin&quot; id=&quot;S1title&quot;&gt;Methods&lt;/h2&gt;
&lt;div id=&quot;S2&quot; class=&quot;sec sec-first&quot; readability=&quot;22.92264573991&quot;&gt;
&lt;h3 id=&quot;S2title&quot;&gt;Participants&lt;/h3&gt;
&lt;p id=&quot;P5&quot; class=&quot;p p-first-last&quot;&gt;ADHD participants were recruited from the ADHD programs at Duke University, Mount Sinai Medical Center and UC Irvine, and controls were recruited from advertisements in local newspapers at Brookhaven National Laboratory. The study included 45 never medicated ADHD participants (23 males; 32 ±8 years of age; 16 ±2 years of education; BMI 26 ±5) and 41 healthy controls (28 males, 31 ±5 years of age; 15 ±2 years of education; BMI 25 ±3) from an imaging study that measured group differences in DA D2/D3 receptor and DAT availability&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R10&quot; rid=&quot;R10&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188989&quot; name=&quot;__tag_513188989&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. Details on subject inclusion and exclusion criteria have been described&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R10&quot; rid=&quot;R10&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188997&quot; name=&quot;__tag_513188997&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. Briefly, inclusion criteria for ADHD participants were: DSM-IV diagnostic criteria for ADHD, presence of at least 6 of 9 inattention symptoms (ascertained with semi-structured psychiatric interviews), evidence that some symptoms of ADHD started during childhood (before age seven) and a Clinical Global Impressions Severity scale (CGI-severity)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R14&quot; rid=&quot;R14&quot; class=&quot; bibr popnode&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; rating of 4 or greater. Exclusion criteria were past or present history of substance abuse (other than nicotine; 3 ADHD participants and 1 control were active smokers) or a positive urine drug screen during assessment, prior or current treatment with psychotropic medications (including stimulants), psychiatric co-morbidities (axis I or II diagnosis other than ADHD), neurological disease, medical conditions that may alter cerebral function (i.e., cardiovascular, endocrinological, oncological or autoimmune diseases) or head trauma with loss of consciousness (greater than 30 minutes). Controls met the same exclusion criteria but not the inclusion criteria for diagnosis of ADHD and were excluded if they described symptoms of inattention or hyperactivity that interfered with their everyday activities.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S3&quot; class=&quot;sec&quot; readability=&quot;21.680520781172&quot;&gt;
&lt;h3 id=&quot;S3title&quot;&gt;Clinical Scales&lt;/h3&gt;
&lt;p id=&quot;P6&quot; class=&quot;p p-first-last&quot;&gt;In addition to the categorical assessment of symptom severity by interview and questionnaires, we assessed the underlying traits of the two DSM-IV domains of attention and activity/reflectivity using the Strengths and Weaknesses of ADHD-symptoms and Normal-behavior (SWAN). The SWAN uses a 7-point scale (-3 to +3) to represent the full range of these dimensions in the population, with average behavior as a reference point (zero)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R15&quot; rid=&quot;R15&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188986&quot; name=&quot;__tag_513188986&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;. The SWAN scores traits that are below average and represent severity of psychopathology on a positive scale (from 1 to 3), and scores traits above average on a negative scale (from -1 to -3). The SWAN was completed in 37 of the controls and 39 of the ADHD participants. We also obtained standard assessments of symptom severity with the Conners Adult ADHD Rating Scale (CAARS) long version&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R16&quot; rid=&quot;R16&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188998&quot; name=&quot;__tag_513188998&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;. The CAARS provides self-assessment of severity of ADHD symptoms on a 4-point scale (Not at All = 0, Just a Little = 1, Pretty Much = 2, and Very Much = 3). Eight subscales are provided: A. Inattention/Memory problems, B. Hyperactivity/Restlessness, C. Impulsivity/Emotional lability, D. Problems with self-concept, E. DSM-IV Inattentive symptoms, F. DSM-IV Hyperactive-impulsive symptoms, G. DSM-IV Symptom total. The CAARS was completed in 36 of the controls and 43 of the ADHD participants. The scores on these clinical scales are shown in .&lt;/p&gt;
&lt;div class=&quot;table-wrap table anchored whole_rhythm&quot; id=&quot;T1&quot; readability=&quot;6.2331691297209&quot;&gt;
&lt;h3&gt;Table 1&lt;/h3&gt;
&lt;div class=&quot;caption&quot; readability=&quot;8&quot;&gt;
&lt;p id=&quot;__p4&quot;&gt;Scores on the clinical scales in Controls and in ADHD participants and number of subjects for whom measures were obtained for each group. Measures correspond to means and standard deviations.&lt;/p&gt;
&lt;/div&gt;
&lt;div data-largeobj=&quot;&quot; data-largeobj-link-rid=&quot;largeobj_idm140161784419504&quot; class=&quot;xtable&quot;&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;none&quot; class=&quot;rendered small default_table&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Controls (n=36)&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ADHD (n=43)&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Motivation ADHD&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;CAARS&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;A Inattention&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5 ±4&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25 ±6&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-0.43, p&amp;lt;0.005&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;B Hyperactivity&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;7 ±4&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;22 ±7&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NS&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;C Impulsivity&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;5 ±3&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20 ±7&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NS&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D Self-concept&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 ±2&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;9 ±4&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NS&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;E DSM Inattentive&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 ±3&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;20 ±4&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-0.43, p&amp;lt;0.005&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;F DSM Hyperactive&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;3 ±3&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15 ±5&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;G Total symptoms&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;6 ±5&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;35 ±6&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;(n=37)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;(n=39)&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;SWAN&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inattention&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-1.5 ±1&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;1.5 ±1&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.44, p&amp;lt;0.005&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hyperactivity&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;-1.2 ±1&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.4 ±1&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;S4&quot; class=&quot;sec&quot; readability=&quot;37.773674806432&quot;&gt;
&lt;h3 id=&quot;S4title&quot;&gt;Personality measures&lt;/h3&gt;
&lt;p id=&quot;P7&quot; class=&quot;p p-first-last&quot;&gt;The Multidimensional Personality Questionnaire (MPQ)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R13&quot; rid=&quot;R13&quot; class=&quot; bibr popnode&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R17&quot; rid=&quot;R17&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188990&quot; name=&quot;__tag_513188990&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;, which has 276 true-false items that score 3 broad factors (Positive Emotionality, Negative Emotionality, and Constraint) and 11 primary trait dimensions, was administered. The trait dimension of Achievement (hard working, driven, tenacious, perfectionistic, enthusiastic) was used as a surrogate trait measure of motivation. The Achievement scale consists of items such as “works hard, drives self, enjoys working hard, welcomes difficult and demanding tasks, persists where others give up, is ambitious, puts work and accomplishments before many other things, sets high standards, is a perfectionist” versus “does not like to work harder than is strictly necessary, avoids very demanding projects, sees no point in persisting when success seems unlikely, is not terribly ambitious or a perfectionist”. The MPQ Achievement scale has been shown to correlate with leadership role occupancy among a large sample (N=238) of identical male twins&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R18&quot; rid=&quot;R18&quot; class=&quot; bibr popnode&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;, consistent with prior meta-analyses studies&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R19&quot; rid=&quot;R19&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189006&quot; name=&quot;__tag_513189006&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;. The activation and sustainment of achievement motivation has been conceptualized to be accomplished by central representations of delayed rewards&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R20&quot; rid=&quot;R20&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189007&quot; name=&quot;__tag_513189007&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;. To assess if the correlations with the DA measures were specific for the surrogate trait measure of motivation, we also inspected correlations with the broad trait measures of Positive Emotionality (combination of scores on well-being, social potency, achievement), Negative Emotionality (combination of scores for stress reaction, alienation, and aggression) and Constraint (combination of scores for self-control, harm avoidance and traditionalism).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S5&quot; class=&quot;sec&quot; readability=&quot;10&quot;&gt;
&lt;h3 id=&quot;S5title&quot;&gt;PET Scans&lt;/h3&gt;
&lt;p id=&quot;P8&quot; class=&quot;p p-first-last&quot;&gt;We used a Siemens HR+ tomograph (resolution 4.5×4.5×4.5 mm full width half-maximum). Dynamic scans were started immediately after injection of 4-10 mCi of [&lt;sup&gt;11&lt;/sup&gt;C]raclopride (specific activity 0.5-1.5 Ci/μM at end of bombardment) and after injection of 4-8 mCi of [&lt;sup&gt;11&lt;/sup&gt;C]cocaine (specific activity &amp;gt; 0. 53 Ci/μmol at end of bombardment) and were obtained for a total of 60 minutes as described (12). Arterial blood was obtained to measure the concentration of unchanged [&lt;sup&gt;11&lt;/sup&gt;C]raclopride and [&lt;sup&gt;11&lt;/sup&gt;C]cocaine in plasma.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S6&quot; class=&quot;sec sec-last&quot; readability=&quot;17.924409448819&quot;&gt;
&lt;h3 id=&quot;S6title&quot;&gt;Image analysis and statistics&lt;/h3&gt;
&lt;p id=&quot;P9&quot; class=&quot;p p-first-last&quot;&gt;We manually obtained regions of interest (ROI) in ventral striatum (in the location of the NAcc), midbrain and cerebellum&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R11&quot; rid=&quot;R11&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189001&quot; name=&quot;__tag_513189001&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;. The carbon-11 concentration in each ROI was used to generate time activity curves for [&lt;sup&gt;11&lt;/sup&gt;C]cocaine and for [&lt;sup&gt;11&lt;/sup&gt;C]raclopride as previously described&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R11&quot; rid=&quot;R11&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188992&quot; name=&quot;__tag_513188992&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;sup&gt;,&lt;/sup&gt;&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R12&quot; rid=&quot;R12&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188983&quot; name=&quot;__tag_513188983&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;. The time activity curves for tissue concentration and unchanged tracer in plasma were used to calculate the distribution volumes (DV) using a graphical analysis technique for reversible systems&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R21&quot; rid=&quot;R21&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189008&quot; name=&quot;__tag_513189008&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; to estimate the equilibrium ratio of tissue concentration to plasma concentration in NAcc, midbrain and cerebellar regions. The ratios of DV in the accumbens and midbrain regions to that in cerebellum correspond to (Bmax/Kd) +1 and were used as measures of D2/D3 receptor and DAT availability. Pearson product moment correlations were used to assess the association between our trait measure of motivation and D2/D3 receptor and DAT availability in the midbrain and NAcc regions. These correlations were calculated first for all participants and then separately for each group of participants (i.e., for the ADHD and Control group separately). We also measured the correlation between the Achievement scale (surrogate trait measure of motivation) and the symptom ratings of inattention and hyperactivity in the ADHD participants using the CAARS and the dimensions of attention and activity/reflectivity from the SWAN. Significance for the a priori hypothesis (i.e., association of DA measures with surrogate trait measures of motivation, and association of surrogate trait measures of motivation with ADHD symptoms) was set at p &amp;lt; 0.05. The significance level for the correlations of the DA measures and scores on the 3 broad traits of the MPQ (Positive Emotionality, Negative Emotionality and Constraint factors) was set at p &amp;lt; 0.008 (i.e., Bonferroni correction for 3 personality and 2 DA measures).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div id=&quot;S7&quot; readability=&quot;29.391480730223&quot;&gt;
&lt;h2 class=&quot;head no_bottom_margin&quot; id=&quot;S7title&quot;&gt;Results&lt;/h2&gt;
&lt;div id=&quot;S8&quot; class=&quot;sec sec-first&quot; readability=&quot;14.634502923977&quot;&gt;
&lt;h3 id=&quot;S8title&quot;&gt;Personality measures&lt;/h3&gt;
&lt;p id=&quot;P10&quot; class=&quot;p p-first-last&quot;&gt;The scores on the Achievement scale (surrogate trait measure of motivation) was significantly lower in ADHD participants than in controls (11±5 vs 15±3; t = 3.5, p &amp;lt;0.001). Compared to control participants, the ADHD participants also showed lower scores for Constraint (t= 5.2, p &amp;lt; 0.0001) and a trend for higher scores on Negative Emotionality (t = 2.1, p &amp;lt; 0.05), but the 2 groups did not differ on scores of Positive Emotionality ().&lt;/p&gt;
&lt;div class=&quot;table-wrap table anchored whole_rhythm&quot; id=&quot;T2&quot; readability=&quot;7.1687279151943&quot;&gt;
&lt;h3&gt;Table 2&lt;/h3&gt;
&lt;div class=&quot;caption&quot; readability=&quot;10&quot;&gt;
&lt;p id=&quot;__p5&quot;&gt;Scores on the Achievement Scale (surrogate trait measure of motivation) and on the Positive and Negative Emotionality and Constraint factors from the Multidimensional Personality Questionnaire (MPQ) in controls and ADHD participants. Comparisons correspond to independent t tests (two tail). Measures correspond to mean and standard deviations.&lt;/p&gt;
&lt;/div&gt;
&lt;div data-largeobj=&quot;&quot; data-largeobj-link-rid=&quot;largeobj_idm140161752188096&quot; class=&quot;xtable&quot;&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot; class=&quot;rendered small default_table&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Controls (n=41)&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ADHD (n=45)&lt;/th&gt;
&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;p&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Achievement Scale&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;15 ±3&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;11 ±5&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0003&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Positive Emotionality&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52 ±10&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48 ±14&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.13&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Negative Emotionality&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;12 ±9&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;16 ±11&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.04&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Constraint&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;52 ±10&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;42 ±12&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.0001&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;S9&quot; class=&quot;sec&quot; readability=&quot;8.7859181731684&quot;&gt;
&lt;h3 id=&quot;S9title&quot;&gt;Dopamine measures&lt;/h3&gt;
&lt;p id=&quot;P11&quot; class=&quot;p p-first-last&quot;&gt;Compared to controls, the ADHD participants had significantly lower measures of D2/D3 receptor and of DAT availability in NAcc and midbrain regions (averaged for left and right regions) (see ).&lt;/p&gt;
&lt;div class=&quot;table-wrap table anchored whole_rhythm&quot; id=&quot;T3&quot; readability=&quot;7.2765196662694&quot;&gt;
&lt;h3&gt;Table 3&lt;/h3&gt;
&lt;div class=&quot;caption&quot; readability=&quot;10&quot;&gt;
&lt;p id=&quot;__p6&quot;&gt;Measures of DA D2/D3 receptor availability (Bmax/Kd) in ADHD participants and controls in the NAcc and midbrain regions and correlations with the scores on the Achievement Scale (surrogate trait measure of motivation) for analysis done with all subjects (ALL) and for analysis done with only ADHD participants (ADHD). The correlations with controls were not significant. Measures correspond to mean and standard deviation.&lt;/p&gt;
&lt;/div&gt;
&lt;div data-largeobj=&quot;&quot; data-largeobj-link-rid=&quot;largeobj_idm140161783168784&quot; class=&quot;xtable&quot;&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;none&quot; class=&quot;rendered small default_table&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;D2/D3 receptors&lt;/th&gt;
&lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Controls (n=41)&lt;/th&gt;
&lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ADHD (n=45)&lt;/th&gt;
&lt;th align=&quot;left&quot; valign=&quot;top&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;em&gt;Correlations with Achievement Scale&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;8&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NAcc region&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.65 ±0.27&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;2.53 ±0.22&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ALL r=0.27, p&amp;lt;0.01; ADHD: r=0.39, p&amp;lt;0.008&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Midbrain&lt;sup&gt;**&lt;/sup&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.28 ±0.12&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.20 ±0.17&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ALL r=0.36, p&amp;lt;0.0007; ADHD r=0.41, p&amp;lt;0.005&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;strong&gt;DAT&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;NAcc region&lt;sup&gt;*&lt;/sup&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.60 ±0.14&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.55 ±0.10&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ALL r=0.20, p&amp;lt;0.07; ADHD r=0.35, p&amp;lt;0.02&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Midbrain&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.14 ±0.09&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;0.10 ±0.09&lt;/td&gt;
&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ALL r=0.18, NS; ADHD R=0.17, NS&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;S10&quot; class=&quot;sec&quot; readability=&quot;29.555160142349&quot;&gt;
&lt;h3 id=&quot;S10title&quot;&gt;Correlation Between DA and Personality Measures&lt;/h3&gt;
&lt;p id=&quot;P12&quot; class=&quot;p p-first&quot;&gt;The correlation analysis between the Achievement scale (surrogate trait measure of motivation) and DA measures in NAcc (averaged left and right ROI) was significant for D2/D3 receptors (r=0.27, p&amp;lt;0.01) but not for DAT (r=0.20, p&amp;lt;0.07) when all subjects were included. Separate group analyses showed the correlation was significant for ADHD participants for both D2/D3 receptors (r=0.39, p&amp;lt;0.008) and DAT (r=0.35, p &amp;lt; 0.02), but not for either in the control participants (see ).&lt;/p&gt;
&lt;div class=&quot;fig iconblock whole_rhythm clearfix&quot; id=&quot;F1&quot; co-legend-rid=&quot;lgnd_F1&quot;&gt;
&lt;div data-largeobj=&quot;&quot; data-largeobj-link-rid=&quot;largeobj_idm140161750367872&quot; class=&quot;figure&quot;&gt;&lt;img class=&quot;fig-image&quot; alt=&quot;An external file that holds a picture, illustration, etc. Object name is nihms229585f1.jpg&quot; title=&quot;An external file that holds a picture, illustration, etc. Object name is nihms229585f1.jpg&quot; src=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/bin/nihms229585f1.jpg&quot; /&gt;&lt;/div&gt;

&lt;div class=&quot;icnblk_cntnt&quot; id=&quot;lgnd_F1&quot; readability=&quot;7.5&quot;&gt;

&lt;div class=&quot;caption&quot; readability=&quot;10&quot;&gt;
&lt;p id=&quot;__p2&quot;&gt;Scattegram showing the regression between the measures of DA D2/D3 receptor and of DAT availability in the NAcc and in the midbrain regions and Trait Motivations (MPQ Achievement scale, which was used as surrogate trait measure of motivation) in ADHD participants (circles) and in controls (x).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p id=&quot;P13&quot;&gt;The correlation analysis between the Achievement scale and the DA measures in midbrain (averaged left and right ROI) when all subjects were included was significant for D2/D3 receptors (r=0.36, p&amp;lt;0.0007) but not DAT (r=0.18, NS). Separate group analyses showed the correlation was significant for ADHD participants for D2/D3 receptor (r=0.41, p&amp;lt;0.005) but not DAT; in controls neither correlation was significant ().&lt;/p&gt;
&lt;p id=&quot;P14&quot; class=&quot;p p-last&quot;&gt;The correlations with the other personality measures when all subjects were included revealed a significant correlations between Positive Emotionality and D2/D3 receptor in midbrain (r=0.34, p&amp;lt;0.002) and separate group analysis showed that this correlation was significant in ADHD participants (r=0.41, p&amp;lt;0.005) but not in controls. The correlations with Negative Emotionality and Constraint factors were not significant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S11&quot; class=&quot;sec sec-last&quot; readability=&quot;15.653979238754&quot;&gt;
&lt;h3 id=&quot;S11title&quot;&gt;Correlation of Achievement (Surrogate Trait Measures of Motivation) and Clinical Symptoms&lt;/h3&gt;
&lt;p id=&quot;P15&quot; class=&quot;p p-first-last&quot;&gt;To assess if our surrogate trait measure of motivation contributed to symptoms in ADHD participants we also measured the correlation between the scores in the Achievement scale and ADHD symptoms, which was significant for the CAARS A (r=-0.43, p&amp;lt;0.005), CAARS E (r=-0.43, p&amp;lt;0.005) and the SWAN Attention dimension (r=-0.44, p &amp;lt; 0.005) (); the lower the scores in the Achievement scale the greater the inattention. In viewing the regression slopes in , note that the positive SWAN score in attention reflects greater symptoms whereas the negative values reflect the opposite of the symptom. The correlation between scores in the Achievement scale and ratings of hyperactivity were not significant. Correlations between the other personality measures and symptoms of inattention or hyperactivity were not significant.&lt;/p&gt;
&lt;div class=&quot;fig iconblock whole_rhythm clearfix&quot; id=&quot;F2&quot; co-legend-rid=&quot;lgnd_F2&quot;&gt;
&lt;div data-largeobj=&quot;&quot; data-largeobj-link-rid=&quot;largeobj_idm140161750365056&quot; class=&quot;figure&quot;&gt;&lt;img class=&quot;fig-image&quot; alt=&quot;An external file that holds a picture, illustration, etc. Object name is nihms229585f2.jpg&quot; title=&quot;An external file that holds a picture, illustration, etc. Object name is nihms229585f2.jpg&quot; src=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/bin/nihms229585f2.jpg&quot; /&gt;&lt;/div&gt;

&lt;div class=&quot;icnblk_cntnt&quot; id=&quot;lgnd_F2&quot; readability=&quot;8&quot;&gt;

&lt;div class=&quot;caption&quot; readability=&quot;11&quot;&gt;
&lt;p id=&quot;__p3&quot;&gt;Scattegram for the regression between Trait Motivations (MPQ Achievement scale, which was used as surrogate trait measure of motivation) and the measures of inattention (CAARS A, CAARS E and SWAN-I) in the ADHD participants.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div id=&quot;S12&quot; readability=&quot;70.255728177043&quot;&gt;
&lt;h2 class=&quot;head no_bottom_margin&quot; id=&quot;S12title&quot;&gt;Discussion&lt;/h2&gt;
&lt;p id=&quot;P16&quot; class=&quot;p p-first&quot;&gt;This study provides evidence of the predicted association between synaptic DA markers in regions of the mesoaccumbens DA pathway (NAcc and midbrain) and the trait of motivation in adults with ADHD. These are key brain regions for reward&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R22&quot; rid=&quot;R22&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189014&quot; name=&quot;__tag_513189014&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;, and the observed decreased availability of D2/D3 receptors and DAT could explain the decreased motivation in these patients.&lt;/p&gt;
&lt;p id=&quot;P17&quot;&gt;The correlation between our surrogate measure of motivation (Achievement scale) and symptoms of inattention also suggest that impaired motivation may contribute to severity of symptoms of inattention in ADHD. These findings are consistent with the clinical recognition that attentional deficits in individuals with ADHD are most evident in tasks that are boring, repetitive and considered uninteresting (i.e., tasks or assignments for which intrinsic motivation is low)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R23&quot; rid=&quot;R23&quot; class=&quot; bibr popnode&quot;&gt;23&lt;/a&gt;&lt;/sup&gt;. However, the correlational approach in our study does not allow us to assess which of these dimensions is more primary; the motivation deficit produces inattention as opposed to the attention deficit resulting in decreased motivation. Alternatively these two dimensions could have common neurobiological substrates (DA reward pathway) as well as unique features (i.e., noradrenergic prefrontal pathways for inattention).&lt;/p&gt;
&lt;p id=&quot;P18&quot;&gt;The observation of a deficit in the DA reward pathway is further evidence that deficits in motivation may be contributing to impairment in ADHD adults&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R6&quot; rid=&quot;R6&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188995&quot; name=&quot;__tag_513188995&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. This finding is also consistent with a recent fMRI study that reported decreased activation of the ventral striatum (the location of the NAcc) in adults with ADHD when compared to controls for both immediate and delayed rewards&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R24&quot; rid=&quot;R24&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189012&quot; name=&quot;__tag_513189012&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P19&quot; class=&quot;p&quot;&gt;We also showed a significant positive correlation between Positive Emotionality and D2/D3 receptor availability in midbrain in ADHD participants. Since the Achievement scale is part of the Positive Emotionality factor this correlation is likely to reflect the association between D2/D3 receptor availability and this surrogate trait measure of motivation.&lt;/p&gt;
&lt;div id=&quot;S13&quot; class=&quot;sec&quot; readability=&quot;21.947147147147&quot;&gt;
&lt;h3 id=&quot;S13title&quot;&gt;Clinical Implications of Findings&lt;/h3&gt;
&lt;p id=&quot;P20&quot; class=&quot;p p-first&quot;&gt;Our findings may have clinical relevance. They support the use of interventions to enhance the saliency of school and work tasks to improve motivation and performance. Indeed both motivational intervention and contingency management have been shown to improve performance in ADHD patients&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R25&quot; rid=&quot;R25&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189011&quot; name=&quot;__tag_513189011&quot;&gt;25&lt;/a&gt;&lt;/sup&gt;. For example, the use of intrinsically interesting activities (perhaps in areas where the individual shows talent and has successes) to reinforce mundane but necessary behaviors offers a therapeutic strategy to overcome a motivation deficit. Also methylphenidate, which is one of the most frequent pharmacological interventions for ADHD, has been shown to increase motivation and interest in a cognitive task in proportion to the drug-induced DA increases in striatum&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R26&quot; rid=&quot;R26&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189015&quot; name=&quot;__tag_513189015&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P21&quot; class=&quot;p p-last&quot;&gt;Decreased activity of the reward system in individuals with ADHD is likely to translate into problems in engaging in activities that are not inherently rewarding or reinforcing (and therefore may be described as less interesting or less motivating). By extrapolation to children, our observations in adults with ADHD could explain the reports of some parents that their child with ADHD can spend hours playing video games, but cannot focus attention on tasks at school, and the reports of children with ADHD that schoolwork is “boring”, which is commonly used to explain their lack of effort. The strong correlation between low scores in the surrogate trait measure of motivation and symptoms of inattention observed in this study suggests the need to consider the possibility of including “motivation or interest deficit” as part of the core pathology of ADHD.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S14&quot; class=&quot;sec&quot; readability=&quot;27.866920152091&quot;&gt;
&lt;h3 id=&quot;S14title&quot;&gt;Personality Measures and ADHD&lt;/h3&gt;
&lt;p id=&quot;P22&quot; class=&quot;p p-first&quot;&gt;Here we report lower scores on the Achievement scale (surrogate trait measure of motivation) in ADHD participants than in controls that was negatively associated with symptoms of inattention in ADHD. This is consistent with studies in children with ADHD in whom temperament measures of effortful control were linked to core symptoms of ADHD&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R27&quot; rid=&quot;R27&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188991&quot; name=&quot;__tag_513188991&quot;&gt;27&lt;/a&gt;&lt;/sup&gt;. Specifically, Martel and Nigg (2006) described two overall dimensions of temperament, effort control and reactive control, and suggested that inattention symptoms might be considered as extreme of the former (effort control) and hyperactive/impulsive symptoms as the extreme of the latter (reactive control)&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R28&quot; rid=&quot;R28&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_550447384&quot; name=&quot;__tag_550447384&quot;&gt;28&lt;/a&gt;&lt;/sup&gt;. In our study hyperactivity did not correlate with any of the personality measures. The difference between our findings and those in children with ADHD could reflect either the low occurrence of symptoms of hyperactivity in our adult ADHD participants, or indicate that the association of personality measures with ADHD symptoms is different in adults than in children with ADHD.&lt;/p&gt;
&lt;p id=&quot;P23&quot;&gt;We also found decreases in scores on the Constraint factor of the MPQ in ADHD participants. This factor is a combination of scores for self-control, harm avoidance and traditionalism, but the main difference was due to the lower scores on the self-control subscale in ADHD participants than in controls (data not shown). This is consistent with prior studies documenting poor self-control as one of the main behavioral characteristics that distinguishes adults with ADHD from controls&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R29&quot; rid=&quot;R29&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189018&quot; name=&quot;__tag_513189018&quot;&gt;29&lt;/a&gt;&lt;/sup&gt;. Moreover, impaired inhibition is considered a core symptom of ADHD&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R30&quot; rid=&quot;R30&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189017&quot; name=&quot;__tag_513189017&quot;&gt;30&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P24&quot; class=&quot;p p-last&quot;&gt;Using the Achievement scale we had previously documented in healthy controls an association with asymmetry in striatal measures of D2/D3 receptor availability such that greater scores on trait measures of motivation were associated with higher left relative to right receptor availability&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R31&quot; rid=&quot;R31&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513188994&quot; name=&quot;__tag_513188994&quot;&gt;31&lt;/a&gt;&lt;/sup&gt;. The current study did not corroborate this finding (data not shown), which may reflect the sensitivity of laterality measures to the position of the head in the field of view of the scanner.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;S15&quot; class=&quot;sec sec-last&quot; readability=&quot;45.854706253948&quot;&gt;
&lt;h3 id=&quot;S15title&quot;&gt;Limitations&lt;/h3&gt;
&lt;p id=&quot;P25&quot; class=&quot;p p-first&quot;&gt;[&lt;sup&gt;11&lt;/sup&gt;C]Raclopride measures are influenced by extracellular DA, so decreased binding could reflect low D2/D3 receptor levels or increased DA release&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R32&quot; rid=&quot;R32&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189002&quot; name=&quot;__tag_513189002&quot;&gt;32&lt;/a&gt;&lt;/sup&gt;. However, the latter is unlikely since we had previously reported that DA release in a subgroup of the ADHD participants reported in this study was lower than in controls&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R33&quot; rid=&quot;R33&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189000&quot; name=&quot;__tag_513189000&quot;&gt;33&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P26&quot;&gt;The relatively low affinity of [&lt;sup&gt;11&lt;/sup&gt;C]raclopride and [&lt;sup&gt;11&lt;/sup&gt;C]cocaine for their targets and the relatively poor spatial resolution of PET decreases the signal to error in measures done in small brain regions such as NAcc, or in regions with lower relative concentration of D2/D2 receptors or of DAT such as midbrain. Future studies using radiotracers with higher affinity and PET instruments with higher resolution will enable more accurate assessments.&lt;/p&gt;
&lt;p id=&quot;P27&quot;&gt;We used the MPQ Achievement scale as a surrogate trait measure of motivation, which in and of itself is a complex construct. However, we had previously shown that the Achievement scale correlated with the dorsolateral prefrontal cortical response to monetary reward, which was in turn correlated with reinforcement-driven reaction time in cocaine addicted individuals&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R34&quot; rid=&quot;R34&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_513189010&quot; name=&quot;__tag_513189010&quot;&gt;34&lt;/a&gt;&lt;/sup&gt;. In the current study it would have been useful to have also collected measures of objective task motivation. Such a measure would have enabled us to assess the relationship between sustained performance on a challenging task and the self-reported measure of motivation in ADHD and in control participants. In future studies we plan to include reaction time measures in response to tasks of varying difficulty and reward expectation, which primates studies have shown reflect behavioral markers of state motivation&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R35&quot; rid=&quot;R35&quot; class=&quot; bibr popnode tag_hotlink tag_tooltip&quot; id=&quot;__tag_810612641&quot; name=&quot;__tag_810612641&quot;&gt;35&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p id=&quot;P28&quot;&gt;Our findings show a significant correlation between DA measures in NAcc and midbrain and the MPQ Achievement scale, which we interpret to suggest an association between these two measures. However, correlations do not necessarily connote causality and further studies are required to address this. Nonetheless, our previous findings in healthy controls, in whom we showed that increases in striatal DA induced by methylphenidate were also associated with increases in the motivation to perform a cognitive task, provides evidence that DA is involved in motivation&lt;sup&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/#R23&quot; rid=&quot;R23&quot; class=&quot; bibr popnode&quot;&gt;23&lt;/a&gt;&lt;/sup&gt;. Further studies are also required to assess the directionality of the association between inattention and motivation deficit (i.e., poor motivation resulting in inattention vs. inattention resulting in poor motivation).&lt;/p&gt;
&lt;p id=&quot;P29&quot; class=&quot;p p-last&quot;&gt;In conclusion, these findings show that reductions in DA D2/D3 receptor and DAT availability in the DA reward pathway of ADHD participants are associated with low scores in the MPQ Achievement scale (surrogate trait measures of motivation). Moreover, the correlation between scores in the MPQ Achievement scale and symptoms of inattention in the ADHD participants suggests that deficits in motivation contribute to inattention in ADHD. Our findings and those from other studies reporting motivation deficits in ADHD strongly suggest that ADHD is a disorder not only of attention-deficit and hyperactivity/impulsivity but also of motivation-deficit, that appears to reflect an hypofunctional DA reward pathway.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
<pubDate>Sat, 03 Oct 2020 09:24:18 +0000</pubDate>
<dc:creator>rbanffy</dc:creator>
<og:title>Motivation Deficit in ADHD is Associated with Dysfunction of the Dopamine Reward Pathway</og:title>
<og:type>article</og:type>
<og:description>ADHD is typically characterized as a disorder of inattention and hyperactivity/impulsivity but there is increasing evidence of deficits in motivation. Using PET we showed decreased function in the brain dopamine reward pathway in adults with ADHD, which ...</og:description>
<og:url>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/</og:url>
<og:image>https://www.ncbi.nlm.nih.gov/corehtml/pmc/pmcgifs/pmc-logo-share.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3010326/</dc:identifier>
</item>
<item>
<title>The open source paradox</title>
<link>http://antirez.com/news/134</link>
<guid isPermaLink="true" >http://antirez.com/news/134</guid>
<description>&lt;section id=&quot;newslist&quot;&gt;&lt;article data-news-id=&quot;134&quot;&gt;
&lt;/article&gt;&lt;/section&gt;&lt;article class=&quot;comment&quot; data-comment-id=&quot;134-&quot; id=&quot;134-&quot; readability=&quot;19.55071955072&quot;&gt;&lt;span class=&quot;info&quot;&gt;&lt;span class=&quot;username&quot;&gt;&lt;a href=&quot;http://antirez.com/user/antirez&quot;&gt;antirez&lt;/a&gt;&lt;/span&gt; 15 hours ago. 71789 views.&lt;/span&gt;
&lt;pre&gt;
A new idea is insinuating in social networks and programming communities. It’s the proportionality between the money people give you for coding something, and the level of demand for quality they can claim to have about your work.

As somebody said, the best code is written when you are supposed to do something else [1]. Like a writer will do her best when writing that novel that, maybe, nobody will pay a single cent for, and not when doing copywriting work for a well known company, programmers are likely to spend more energies in their open source side projects than during office hours, while writing another piece of a project they feel stupid, boring, pointless. And, if the company is big enough, chances are it will be cancelled in six months anyway or retired one year after the big launch.

Open source is different, it’s an artifact, it’s a transposition in code of what you really want to do, of what you feel software should be, or just of all your fun and joy, or even anger you are feeling while coding. And you want it to rock, to be perfect, and you can’t sleep at night if there is a fucking heisenbug. So if a user of your software is addressing you because some part of your code sucks, and is willing to work with you to do something about it, and is very demanding, don’t think they are abusing you because they are not paying you. It’s not about money. You can ignore bugs if you want, and ignore their complains, you can do that since you don’t have a contract to do otherwise, but they are helping you, they care about the same thing you care: your software quality, grandiosity, perfection.

The real right you have, and often don’t exploit, is that you are the only one that can decide about the design of your software. So you are entitled to refuse a pull request, or a proposal to follow good practices, because you feel that what somebody is contributing does not fit in the big picture of what you are designing and building.

But if you recognize that somebody is talking you about something that is, really, a defect in your software, don’t do the error of reducing the interaction to a vile matter of money. You are doing work for free, they are risking their asses deploying what you wrote, you both care about quality.

EDIT: If you write OSS and you are upset about user demands, have you ever thought that maybe, at this point, your work is more similar to office work for some reason?

EDIT 2: A HN user asked the reasons for such title. The paradox is that the OSS writer cares and is often willing to fix code she writes for free, more than the other paid work she does.

[1] &quot;The best programs are the ones written when the programmer is supposed to be working on something else.&quot; - Melinda Varian. &lt;a rel=&quot;nofollow&quot; href=&quot;https://twitter.com/CodeWisdom/status/1309470447667421189&quot;&gt;https://twitter.com/CodeWisdom/status/1309470447667421189&lt;/a&gt;
&lt;/pre&gt;&lt;/article&gt;
&lt;noscript readability=&quot;1.15625&quot;&gt;
&lt;p&gt;Please enable JavaScript to view the &lt;a href=&quot;http://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/p&gt;
&lt;/noscript&gt; &lt;a href=&quot;http://disqus.com&quot; class=&quot;dsq-brlink&quot;&gt;blog comments powered by &lt;span class=&quot;logo-disqus&quot;&gt;Disqus&lt;/span&gt;&lt;/a&gt;</description>
<pubDate>Sat, 03 Oct 2020 09:14:48 +0000</pubDate>
<dc:creator>stargrave</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://antirez.com/news/134</dc:identifier>
</item>
<item>
<title>GHunt – An OSINT tool to extract information about a Google account</title>
<link>https://github.com/mxrch/GHunt</link>
<guid isPermaLink="true" >https://github.com/mxrch/GHunt</guid>
<description>&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/bcdd77ab953be946be023160eb4a55762558c230/68747470733a2f2f66696c65732e636174626f782e6d6f652f3861356e7a732e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/bcdd77ab953be946be023160eb4a55762558c230/68747470733a2f2f66696c65732e636174626f782e6d6f652f3861356e7a732e706e67&quot; alt=&quot;screenshot&quot; data-canonical-src=&quot;https://files.catbox.moe/8a5nzs.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;GHunt is an OSINT tool to extract a lot of informations of someone's Google Account email.&lt;/p&gt;
&lt;p&gt;It can currently extract :&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Owner's name&lt;/li&gt;
&lt;li&gt;Last time the profile was edited&lt;/li&gt;
&lt;li&gt;Google ID&lt;/li&gt;
&lt;li&gt;If the account is an Hangouts Bot&lt;/li&gt;
&lt;li&gt;Activated Google services (Youtube, Photos, Maps, News360, Hangouts, etc.)&lt;/li&gt;
&lt;li&gt;Possible Youtube channel&lt;/li&gt;
&lt;li&gt;Possible other usernames&lt;/li&gt;
&lt;li&gt;Public photos&lt;/li&gt;
&lt;li&gt;Phones models&lt;/li&gt;
&lt;li&gt;Phones firmwares&lt;/li&gt;
&lt;li&gt;Installed softwares&lt;/li&gt;
&lt;li&gt;Google Maps reviews&lt;/li&gt;
&lt;li&gt;Possible physical location&lt;/li&gt;
&lt;/ul&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/8b3224835ac46d071280908af544f31c22866d2b/68747470733a2f2f66696c65732e636174626f782e6d6f652f327a62317a392e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/8b3224835ac46d071280908af544f31c22866d2b/68747470733a2f2f66696c65732e636174626f782e6d6f652f327a62317a392e706e67&quot; data-canonical-src=&quot;https://files.catbox.moe/2zb1z9.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;⚠️ Warning&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;02/10/2020&lt;/strong&gt; : Since few days ago, Google return a 404 when we try to access someone's Google Photos public albums, we can only access it if we have a link of one of his albums.&lt;br/&gt;Either this is a bug and this will be fixed, either it's a protection that we need to find how to bypass.&lt;br/&gt;&lt;strong&gt;So, currently, the photos &amp;amp; metadata module will always return &quot;No albums&quot; even if there is one.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;02/10/2020&lt;/strong&gt; : I found a bypass, I'm working on the patch right now.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;03/10/2020&lt;/strong&gt; : Successfully bypassed. 🕺 (commit 01dc016)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Python 3.6.1+ would be ok. (I developed it with Python 3.8.1)&lt;/li&gt;
&lt;li&gt;These Python modules are required (we'll install them after):&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;geopy
httpx
selenium-wire
selenium
imagehash
pillow
python-dateutil
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2&gt;1. Chromedriver &amp;amp; Google Chrome&lt;/h2&gt;
&lt;p&gt;This project uses Selenium, so you'll need to download the chromedriver here : &lt;a href=&quot;https://chromedriver.chromium.org/downloads&quot; rel=&quot;nofollow&quot;&gt;https://chromedriver.chromium.org/downloads&lt;/a&gt;&lt;br/&gt;And put it in the GHunt folder. Be sure it's called &quot;chromedriver.exe&quot; or &quot;chromedriver&quot;.&lt;br/&gt;Also, be sure to have Google Chrome installed.&lt;/p&gt;
&lt;h2&gt;2. Requirements&lt;/h2&gt;
&lt;p&gt;In the GHunt folder, do this:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
python -m pip install -r requirements.txt
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Adapt the command with your operating system if needed.&lt;/p&gt;

&lt;p&gt;For the first usage and sometimes after, you'll need to check the validity of your cookies.&lt;br/&gt;To do this, launch &lt;code&gt;check_and_gen.py&lt;/code&gt;.&lt;br/&gt;If you don't have cookies stored (ex: first launch) it will ask you the 4 needed cookies, enter them and if they are valid, it will generate the Authentification token, and the Google Docs &amp;amp; Hangouts tokens.&lt;/p&gt;
&lt;p&gt;Then, you can run the tool like this :&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
python hunt.py myemail@gmail.com
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;⚠️ Every time you re-login to the Google Account used for the cookies, it will break their validity, so I highly suggest you to make an empty account just for that, or use an account where you never login.&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;Login to accounts.google.com&lt;/li&gt;
&lt;li&gt;Once connected, open the Dev Tools window and goes to the Storage tab (Shift + F9 on Firefox) (looks like it's called &quot;Application&quot; on Chrome)&lt;br/&gt;If you don't know how to open it, just right-click somewhere and &quot;Inspect Element&quot;&lt;/li&gt;
&lt;li&gt;Then you'll find every cookie you need, including the 4 ones.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://camo.githubusercontent.com/bc93e734aa73854233c34931797e5ce259e8669a/68747470733a2f2f66696c65732e636174626f782e6d6f652f396a793230302e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/bc93e734aa73854233c34931797e5ce259e8669a/68747470733a2f2f66696c65732e636174626f782e6d6f652f396a793230302e706e67&quot; alt=&quot;cookies&quot; data-canonical-src=&quot;https://files.catbox.moe/9jy200.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This tool is based on the Sector's researches on the Google IDs : &lt;a href=&quot;https://sector035.nl/articles/getting-a-grasp-on-google-ids&quot; rel=&quot;nofollow&quot;&gt;https://sector035.nl/articles/getting-a-grasp-on-google-ids&lt;/a&gt;&lt;br/&gt;And completed by my own researches.&lt;br/&gt;If I have the motivation to write a blog post about it, I'll add the link here !&lt;/p&gt;
</description>
<pubDate>Sat, 03 Oct 2020 07:53:06 +0000</pubDate>
<dc:creator>jennifer_lopez</dc:creator>
<og:image>https://repository-images.githubusercontent.com/300592907/8875a300-04b3-11eb-8348-40771294805b</og:image>
<og:type>object</og:type>
<og:title>mxrch/GHunt</og:title>
<og:url>https://github.com/mxrch/GHunt</og:url>
<og:description>🕵️‍♂️ Investigate Google Accounts with emails. . Contribute to mxrch/GHunt development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/mxrch/GHunt</dc:identifier>
</item>
<item>
<title>Hacktoberfest Is Now Opt-In</title>
<link>https://github.com/digitalocean/hacktoberfest/pull/596</link>
<guid isPermaLink="true" >https://github.com/digitalocean/hacktoberfest/pull/596</guid>
<description>
&lt;p&gt;Edit: just creating a new issue for the following in &lt;a class=&quot;issue-link js-issue-link&quot; data-error-text=&quot;Failed to load title&quot; data-id=&quot;714164354&quot; data-permission-text=&quot;Title is private&quot; data-url=&quot;https://github.com/digitalocean/hacktoberfest/issues/608&quot; data-hovercard-type=&quot;issue&quot; data-hovercard-url=&quot;/digitalocean/hacktoberfest/issues/608/hovercard&quot; href=&quot;https://github.com/digitalocean/hacktoberfest/issues/608&quot;&gt;#608&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;TL;DR Make searching hacktoberfest repos easier and show smaller repos more often.&lt;/p&gt;
&lt;p&gt;I'm mostly for this PR, but (the following is my proposal to distribute the spamming of PRs to become small PRs but in &quot;actual code form&quot; to smaller repos):&lt;/p&gt;
&lt;p&gt;I have an organization's repo with only 5 stars in it, and I'd love to see contributions to it. I think we just need to think back to what this entire event is for: encouraging people to contribute to open source. Right now, I think this PR 1. makes large repositories overrun with spam and 2. make smaller repositories who desperately need contributions left in the dust that is GitHub's tag search engine (which is great, but not for this event).&lt;/p&gt;
&lt;p&gt;Solutions via rules and code:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Some new rules that can be made in place is for repositories with greater than 2,000 stars to only accepts PRs from GitHub users who have created at least 100 contributions with an account age over 2 months. In my opinion, &lt;em&gt;most&lt;/em&gt; people (but not all e.g. I joined with programming knowledge and immediately contributed to a large repo) with the aforementioned criteria create meaningful PRs in those repositories.&lt;/li&gt;
&lt;li&gt;What about those smaller repositories? The ones with less than 2,000 stars? In my opinion, they need contributions from newcomers to git and/or programming in general. They need small PRs to catch edge cases or small new features as the repo is growing. Those are where most PRs should be geared towards since the larger ones already have a huge backing of PRs. &quot;This sounds like the event is for open source rather than for introducing devs to open source?&quot; To me, yes I agree, but also note that that way we can distribute the spam -- in a sense -- and the people are more encouraged to spam with small CODE changes not doc changes.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Code changes:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;I believe the way we can help with an endeavor to persuade spamming &lt;strong&gt;meaningful&lt;/strong&gt; PRs to small repositories would be to offer a survey. See which programming languages people are most familiar with, and show them repositories based on the above criteria.&lt;/li&gt;
&lt;li&gt;Additionally, the GitHub search engine can provide easier methods for searching repositories by finding hacktoberfest-labeled small repos that have recently been updated in the last 2 months. Order them based on the fuzzy searching of GitHub's search engine (per usual) and number of commits and currently open PRs. (I'm not entirely sure about the ordering, and obviously messing with the ordering at this point in time is difficult).&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Idk, in essence, I think there would be more good to this event if small repositories got more PRs to 1. distribute the spam, 2. make sure PRs are in &quot;actual code form,&quot; and 3. help the smaller repositories grow.&lt;/p&gt;
</description>
<pubDate>Sat, 03 Oct 2020 02:39:33 +0000</pubDate>
<dc:creator>gmemstr</dc:creator>
<og:image>https://repository-images.githubusercontent.com/200877850/a3998700-e181-11ea-86c2-0bff8811ded4</og:image>
<og:type>object</og:type>
<og:title>Require PRs be in a repo with hacktoberfest topic and be accepted by MattIPv4 · Pull Request #596 · digitalocean/hacktoberfest</og:title>
<og:url>https://github.com/digitalocean/hacktoberfest/pull/596</og:url>
<og:description>Description To reduce spam and make Hacktoberfest an opt-in event, only consider pull requests that are submitted in a repository that has &amp;#39;hacktoberfest&amp;#39; as a repository topic. Further, on...</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/digitalocean/hacktoberfest/pull/596</dc:identifier>
</item>
<item>
<title>Raycasting engine in Factorio 1.0 (unmodded) [video]</title>
<link>https://www.youtube.com/watch?v=28UzqVz1r24</link>
<guid isPermaLink="true" >https://www.youtube.com/watch?v=28UzqVz1r24</guid>
<description>&lt;div id=&quot;player&quot; class=&quot;skeleton flexy&quot;&gt;
      &lt;div id=&quot;player-wrap&quot;&gt;
        
            &lt;/div&gt;
  &lt;/div&gt;&lt;!-- end of chunk --&gt;&lt;ytd-app disable-upgrade=&quot;true&quot;&gt;&lt;ytd-masthead id=&quot;masthead&quot; slot=&quot;masthead&quot; class=&quot;shell  chunked&quot; disable-upgrade=&quot;true&quot;&gt;&lt;svg id=&quot;menu-icon&quot; class=&quot;external-icon&quot; preserveaspectratio=&quot;xMidYMid meet&quot;&gt;&lt;g id=&quot;menu&quot; class=&quot;yt-icons-ext&quot; viewbox=&quot;0 0 24 24&quot;&gt;&lt;path d=&quot;M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z&quot;/&gt;&lt;/g&gt;&lt;/svg&gt;&lt;div id=&quot;masthead-logo&quot; slot=&quot;masthead-logo&quot;&gt;  &lt;div&gt;
    &lt;svg id=&quot;yt-logo-svg&quot; class=&quot;external-icon&quot; viewbox=&quot;0 0 200 60&quot;&gt;&lt;g id=&quot;yt-logo&quot; viewbox=&quot;0 0 200 60&quot; preserveaspectratio=&quot;xMidYMid meet&quot;&gt;&lt;g&gt;&lt;path fill=&quot;#FF0000&quot; d=&quot;M63,14.87c-0.72-2.7-2.85-4.83-5.56-5.56C52.54,8,32.88,8,32.88,8S13.23,8,8.32,9.31&amp;#10;            c-2.7,0.72-4.83,2.85-5.56,5.56C1.45,19.77,1.45,30,1.45,30s0,10.23,1.31,15.13c0.72,2.7,2.85,4.83,5.56,5.56&amp;#10;            C13.23,52,32.88,52,32.88,52s19.66,0,24.56-1.31c2.7-0.72,4.83-2.85,5.56-5.56C64.31,40.23,64.31,30,64.31,30&amp;#10;            S64.31,19.77,63,14.87z&quot;/&gt;&lt;polygon fill=&quot;#FFFFFF&quot; points=&quot;26.6,39.43 42.93,30 26.6,20.57&quot;/&gt;&lt;/g&gt;&lt;g&gt;&lt;g id=&quot;youtube-paths&quot;&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M92.69,48.03c-1.24-0.84-2.13-2.14-2.65-3.91c-0.52-1.77-0.79-4.12-0.79-7.06v-4&amp;#10;              c0-2.97,0.3-5.35,0.9-7.15c0.6-1.8,1.54-3.11,2.81-3.93c1.27-0.82,2.94-1.24,5.01-1.24c2.04,0,3.67,0.42,4.9,1.26&amp;#10;              c1.23,0.84,2.13,2.15,2.7,3.93c0.57,1.78,0.85,4.16,0.85,7.12v4c0,2.94-0.28,5.3-0.83,7.08c-0.55,1.78-1.45,3.09-2.7,3.91&amp;#10;              c-1.24,0.82-2.93,1.24-5.06,1.24C95.65,49.29,93.93,48.87,92.69,48.03z M99.66,43.71c0.34-0.9,0.52-2.37,0.52-4.4v-8.59&amp;#10;              c0-1.98-0.17-3.42-0.52-4.34c-0.34-0.91-0.95-1.37-1.82-1.37c-0.84,0-1.43,0.46-1.78,1.37c-0.34,0.91-0.52,2.36-0.52,4.34v8.59&amp;#10;              c0,2.04,0.16,3.51,0.49,4.4c0.33,0.9,0.93,1.35,1.8,1.35C98.71,45.06,99.31,44.61,99.66,43.71z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M188.16,37.13v1.39c0,1.77,0.05,3.09,0.16,3.98c0.1,0.88,0.32,1.53,0.65,1.93&amp;#10;              c0.33,0.4,0.84,0.61,1.53,0.61c0.93,0,1.57-0.36,1.91-1.08c0.34-0.72,0.53-1.92,0.56-3.6l5.35,0.31&amp;#10;              c0.03,0.24,0.04,0.57,0.04,0.99c0,2.55-0.7,4.45-2.09,5.71c-1.39,1.26-3.36,1.89-5.91,1.89c-3.06,0-5.2-0.96-6.43-2.88&amp;#10;              c-1.23-1.92-1.84-4.88-1.84-8.9v-4.81c0-4.14,0.64-7.15,1.91-9.06c1.27-1.9,3.45-2.85,6.54-2.85c2.13,0,3.76,0.39,4.9,1.17&amp;#10;              c1.14,0.78,1.94,1.99,2.41,3.64c0.46,1.65,0.7,3.93,0.7,6.83v4.72H188.16z M188.95,25.53c-0.31,0.39-0.52,1.03-0.63,1.91&amp;#10;              c-0.11,0.88-0.16,2.23-0.16,4.02v1.98h4.54v-1.98c0-1.77-0.06-3.11-0.18-4.02c-0.12-0.91-0.34-1.56-0.65-1.93&amp;#10;              c-0.31-0.37-0.8-0.56-1.46-0.56C189.75,24.94,189.26,25.14,188.95,25.53z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M77.59,36.61l-7.06-25.49h6.16l2.47,11.55c0.63,2.85,1.09,5.27,1.39,7.28h0.18&amp;#10;              c0.21-1.44,0.67-3.85,1.39-7.24l2.56-11.6h6.16L83.7,36.61v12.23h-6.11V36.61z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M126.45,21.28v27.55h-4.85l-0.54-3.37h-0.13c-1.32,2.55-3.3,3.82-5.93,3.82c-1.83,0-3.18-0.6-4.05-1.8&amp;#10;              c-0.87-1.2-1.3-3.07-1.3-5.62V21.28h6.2v20.23c0,1.23,0.13,2.11,0.4,2.63c0.27,0.52,0.72,0.79,1.35,0.79&amp;#10;              c0.54,0,1.06-0.16,1.55-0.49c0.49-0.33,0.86-0.75,1.1-1.26V21.28H126.45z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M158.27,21.28v27.55h-4.85l-0.54-3.37h-0.13c-1.32,2.55-3.3,3.82-5.93,3.82c-1.83,0-3.18-0.6-4.05-1.8&amp;#10;              c-0.87-1.2-1.3-3.07-1.3-5.62V21.28h6.2v20.23c0,1.23,0.13,2.11,0.4,2.63c0.27,0.52,0.72,0.79,1.35,0.79&amp;#10;              c0.54,0,1.06-0.16,1.55-0.49c0.49-0.33,0.86-0.75,1.1-1.26V21.28H158.27z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M143.31,16.11h-6.16v32.72h-6.07V16.11h-6.16v-4.99h18.38V16.11z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M178.8,25.69c-0.38-1.74-0.98-3-1.82-3.78c-0.84-0.78-1.99-1.17-3.46-1.17c-1.14,0-2.2,0.32-3.19,0.97&amp;#10;              c-0.99,0.64-1.75,1.49-2.29,2.54h-0.05l0-14.52h-5.98v39.11h5.12l0.63-2.61h0.13c0.48,0.93,1.2,1.66,2.16,2.2&amp;#10;              c0.96,0.54,2.02,0.81,3.19,0.81c2.1,0,3.64-0.97,4.63-2.9c0.99-1.93,1.48-4.95,1.48-9.06v-4.36&amp;#10;              C179.36,29.84,179.17,27.43,178.8,25.69z M173.11,36.93c0,2.01-0.08,3.58-0.25,4.72c-0.16,1.14-0.44,1.95-0.83,2.43&amp;#10;              c-0.39,0.48-0.91,0.72-1.57,0.72c-0.51,0-0.98-0.12-1.42-0.36c-0.43-0.24-0.79-0.6-1.06-1.08V27.71&amp;#10;              c0.21-0.75,0.57-1.36,1.08-1.84c0.51-0.48,1.06-0.72,1.66-0.72c0.63,0,1.12,0.25,1.46,0.74c0.34,0.49,0.58,1.33,0.72,2.49&amp;#10;              c0.13,1.17,0.2,2.83,0.2,4.99V36.93z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;
  &lt;div&gt;
    &lt;svg id=&quot;yt-logo-red-svg&quot; class=&quot;external-icon&quot; viewbox=&quot;0 0 98 24&quot;&gt;&lt;g id=&quot;yt-logo-red&quot; viewbox=&quot;0 0 98 24&quot; preserveaspectratio=&quot;xMidYMid meet&quot;&gt;&lt;g&gt;&lt;path fill=&quot;#FF0000&quot; d=&quot;M28.4,5.12c-0.34-1.24-1.31-2.2-2.55-2.52C23.62,2,14.68,2,14.68,2S5.75,2,3.52,2.6&amp;#10;            C2.29,2.93,1.33,3.89,1,5.12C0.59,7.39,0.39,9.69,0.4,12c-0.01,2.31,0.19,4.61,0.6,6.88c0.33,1.23,1.29,2.19,2.52,2.52&amp;#10;            C5.75,22,14.68,22,14.68,22s8.93,0,11.16-0.6c1.24-0.32,2.22-1.28,2.56-2.52c0.41-2.27,0.61-4.57,0.6-6.88&amp;#10;            C29.01,9.69,28.81,7.39,28.4,5.12z&quot;/&gt;&lt;polygon fill=&quot;#FFFFFF&quot; points=&quot;11.83,16.29 19.25,12 11.83,7.71&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;youtube-red-paths&quot;&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M41.67,8.35V9c0,3.45-1.53,5.48-4.88,5.48h-0.51v6h-2.74V3.42h3.49C40.22,3.42,41.67,4.77,41.67,8.35z&amp;#10;            M38.79,8.6c0-2.49-0.45-3.09-2-3.09h-0.51v7h0.47c1.47,0,2-1.06,2-3.37L38.79,8.6z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M48.14,7.83L48,11.08c-1.17-0.24-2.13-0.08-2.6,0.69v8.78h-2.67V8h2.17l0.24,2.71h0.1c0.28-2,1.2-3,2.39-3&amp;#10;            C47.8,7.73,47.98,7.77,48.14,7.83z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M51.27,15.25v0.63c0,2.21,0.12,3,1.06,3s1.1-0.69,1.12-2.12l2.43,0.14c0.18,2.7-1.23,3.9-3.61,3.9&amp;#10;            c-2.9,0-3.76-1.9-3.76-5.35v-2.23c0-3.64,1-5.41,3.84-5.41s3.64,1.51,3.64,5.29v2.15H51.27z M51.27,12.67v0.9h2.06v-0.89&amp;#10;            c0-2.3-0.16-3-1-3s-1,0.67-1,3L51.27,12.67z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M70.02,11.1v9.46H67.2v-9.25c0-1-0.27-1.53-0.88-1.53c-0.54,0.02-1.02,0.34-1.25,0.82&amp;#10;            c0.01,0.17,0.01,0.34,0,0.51v9.46h-2.79v-9.26c0-1-0.27-1.53-0.88-1.53c-0.53,0.02-1,0.33-1.23,0.8v10H57.4V8h2.23l0.25,1.59l0,0&amp;#10;            c0.51-1.12,1.63-1.85,2.86-1.86c1.05-0.07,1.98,0.67,2.16,1.7c0.55-1.01,1.61-1.65,2.76-1.66C69.4,7.78,70.02,9,70.02,11.1z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M71.4,4.83c0-1.35,0.49-1.74,1.53-1.74s1.53,0.45,1.53,1.74s-0.47,1.74-1.53,1.74S71.4,6.22,71.4,4.83z&amp;#10;            M71.59,8h2.7v12.56h-2.7V8z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M83.5,8v12.56h-2.2L81.05,19h-0.06c-0.46,1.08-1.53,1.77-2.7,1.74c-1.67,0-2.43-1.06-2.43-3.37V8h2.82&amp;#10;            v9.19c0,1.1,0.23,1.55,0.8,1.55c0.52-0.02,0.98-0.33,1.2-0.8V8H83.5z&quot;/&gt;&lt;path fill=&quot;#282828&quot; d=&quot;M97.8,11.1v9.46h-2.82v-9.25c0-1-0.27-1.53-0.88-1.53c-0.54,0.02-1.02,0.34-1.25,0.82&amp;#10;            c0.01,0.17,0.01,0.34,0,0.51v9.46h-2.79v-9.26c0-1-0.27-1.53-0.88-1.53c-0.53,0.02-1,0.33-1.23,0.8v10h-2.81V8h2.26l0.24,1.59l0,0&amp;#10;            c0.51-1.12,1.63-1.85,2.86-1.86c1.04-0.07,1.97,0.64,2.17,1.66c0.55-0.99,1.6-1.61,2.73-1.62C97.15,7.78,97.8,9,97.8,11.1z&quot;/&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;
&lt;span id=&quot;country-code&quot;/&gt;&lt;/div&gt;&lt;div id=&quot;masthead-skeleton-icons&quot; slot=&quot;masthead-skeleton&quot;/&gt;&lt;/ytd-masthead&gt;&lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/about/&quot;&gt;About&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/about/press/&quot;&gt;Press&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/about/copyright/&quot;&gt;Copyright&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/t/contact_us&quot;&gt;Contact us&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/creators/&quot;&gt;Creators&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://www.youtube.com/ads/&quot;&gt;Advertise&lt;/a&gt;
  &lt;a slot=&quot;guide-links-primary&quot; href=&quot;https://developers.google.com/youtube&quot;&gt;Developers&lt;/a&gt;

      &lt;a slot=&quot;guide-links-secondary&quot; href=&quot;https://www.youtube.com/t/terms&quot;&gt;Terms&lt;/a&gt;
  &lt;a slot=&quot;guide-links-secondary&quot; href=&quot;https://www.google.com/intl/en/policies/privacy/&quot;&gt;Privacy&lt;/a&gt;
  &lt;a slot=&quot;guide-links-secondary&quot; href=&quot;https://www.youtube.com/about/policies/&quot;&gt;Policy &amp;amp; Safety&lt;/a&gt;
    &lt;a slot=&quot;guide-links-secondary&quot; href=&quot;https://www.youtube.com/howyoutubeworks?utm_campaign=ytgen&amp;amp;utm_source=ythp&amp;amp;utm_medium=LeftNav&amp;amp;utm_content=txt&amp;amp;u=https%3A%2F%2Fwww.youtube.com%2Fhowyoutubeworks%3Futm_source%3Dythp%26utm_medium%3DLeftNav%26utm_campaign%3Dytgen&quot;&gt;How YouTube works&lt;/a&gt;
  &lt;a slot=&quot;guide-links-secondary&quot; href=&quot;https://www.youtube.com/new&quot;&gt;Test new features&lt;/a&gt;

      

      &lt;div id=&quot;copyright&quot; slot=&quot;copyright&quot;&gt;
    &lt;p&gt;© 2020 Google LLC&lt;/p&gt;
  &lt;/div&gt;

  &lt;/ytd-app&gt;&lt;!-- end of chunk --&gt;&lt;div id=&quot;watch-page-skeleton&quot; class=&quot;watch-skeleton &quot;&gt;
    &lt;div id=&quot;container&quot;&gt;
      &lt;div id=&quot;related&quot;&gt;
        &lt;div class=&quot;autoplay skeleton-light-border-bottom&quot;&gt;
          
            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

        &lt;/div&gt;
            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

            &lt;div class=&quot;video-skeleton&quot;&gt;
    &lt;div class=&quot;video-details&quot;&gt;
      
      &lt;div class=&quot;details flex-1&quot;&gt;
        
        
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

      &lt;/div&gt;
      &lt;div id=&quot;info-container&quot;&gt;
        &lt;div id=&quot;primary-info&quot; class=&quot;skeleton-light-border-bottom&quot;&gt;
          
          &lt;div id=&quot;info&quot;&gt;
            
            
            &lt;div id=&quot;menu&quot;&gt;
                
                
                
                
                
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
        &lt;div id=&quot;secondary-info&quot; class=&quot;skeleton-light-border-bottom&quot;&gt;
          &lt;div id=&quot;top-row&quot;&gt;
            &lt;div id=&quot;video-owner&quot; class=&quot;flex-1&quot;&gt;
              
              &lt;div id=&quot;upload-info&quot; class=&quot;flex-1&quot;&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
            
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

    &lt;link rel=&quot;canonical&quot; href=&quot;https://www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;link rel=&quot;alternate&quot; media=&quot;handheld&quot; href=&quot;https://m.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;link rel=&quot;alternate&quot; media=&quot;only screen and (max-width: 640px)&quot; href=&quot;https://m.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;#ff0000&quot;/&gt;&lt;title&gt;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0 - YouTube&lt;/title&gt;&lt;meta name=&quot;title&quot; content=&quot;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0&quot;/&gt;&lt;meta name=&quot;description&quot; content=&quot;Version 2.0 of my ray casting engine build entirely inside factorio. No mods are needed to run it. more information and download: https://forums.factorio.com...&quot;/&gt;&lt;meta name=&quot;keywords&quot; content=&quot;factorio, arrow in my gluteus maximus, factorio 1.0, factorio computer, raycasting engine, 3d factorio, raycasting, arrowGMaximus, factorio gameplay, FactoRa...&quot;/&gt;&lt;link rel=&quot;shortlink&quot; href=&quot;https://youtu.be/28UzqVz1r24&quot;/&gt;&lt;link rel=&quot;alternate&quot; href=&quot;android-app://com.google.android.youtube/http/www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;link rel=&quot;alternate&quot; href=&quot;ios-app://544007664/vnd.youtube/www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;link rel=&quot;alternate&quot; type=&quot;application/json+oembed&quot; href=&quot;http://www.youtube.com/oembed?format=json&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D28UzqVz1r24&quot; title=&quot;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0&quot;/&gt;&lt;link rel=&quot;alternate&quot; type=&quot;text/xml+oembed&quot; href=&quot;http://www.youtube.com/oembed?format=xml&amp;amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D28UzqVz1r24&quot; title=&quot;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0&quot;/&gt;&lt;link rel=&quot;image_src&quot; href=&quot;https://i.ytimg.com/vi/28UzqVz1r24/maxresdefault.jpg&quot;/&gt;&lt;meta property=&quot;og:site_name&quot; content=&quot;YouTube&quot;/&gt;&lt;meta property=&quot;og:url&quot; content=&quot;https://www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;meta property=&quot;og:title&quot; content=&quot;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0&quot;/&gt;&lt;meta property=&quot;og:image&quot; content=&quot;https://i.ytimg.com/vi/28UzqVz1r24/maxresdefault.jpg&quot;/&gt;&lt;meta property=&quot;og:image:width&quot; content=&quot;1280&quot;/&gt;&lt;meta property=&quot;og:image:height&quot; content=&quot;720&quot;/&gt;&lt;meta property=&quot;og:description&quot; content=&quot;Version 2.0 of my ray casting engine build entirely inside factorio. No mods are needed to run it. more information and download: https://forums.factorio.com...&quot;/&gt;&lt;meta property=&quot;al:ios:app_store_id&quot; content=&quot;544007664&quot;/&gt;&lt;meta property=&quot;al:ios:app_name&quot; content=&quot;YouTube&quot;/&gt;&lt;meta property=&quot;al:ios:url&quot; content=&quot;vnd.youtube://www.youtube.com/watch?v=28UzqVz1r24&amp;amp;feature=applinks&quot;/&gt;&lt;meta property=&quot;al:android:url&quot; content=&quot;vnd.youtube://www.youtube.com/watch?v=28UzqVz1r24&amp;amp;feature=applinks&quot;/&gt;&lt;meta property=&quot;al:android:app_name&quot; content=&quot;YouTube&quot;/&gt;&lt;meta property=&quot;al:android:package&quot; content=&quot;com.google.android.youtube&quot;/&gt;&lt;meta property=&quot;al:web:url&quot; content=&quot;https://www.youtube.com/watch?v=28UzqVz1r24&amp;amp;feature=applinks&quot;/&gt;&lt;meta property=&quot;og:type&quot; content=&quot;video.other&quot;/&gt;&lt;meta property=&quot;og:video:url&quot; content=&quot;https://www.youtube.com/embed/28UzqVz1r24&quot;/&gt;&lt;meta property=&quot;og:video:secure_url&quot; content=&quot;https://www.youtube.com/embed/28UzqVz1r24&quot;/&gt;&lt;meta property=&quot;og:video:type&quot; content=&quot;text/html&quot;/&gt;&lt;meta property=&quot;og:video:width&quot; content=&quot;640&quot;/&gt;&lt;meta property=&quot;og:video:height&quot; content=&quot;360&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;arrow in my gluteus maximus&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;factorio 1.0&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;factorio computer&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;raycasting engine&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;3d factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;raycasting&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;arrowGMaximus&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;factorio gameplay&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;FactoRayO&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;3d&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;game engine&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;first person shooter&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;fps in factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;first person shooter in factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;doom in factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;wolfenstein in factorio&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;v2.0&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;FactoRayO v2.0&quot;/&gt;&lt;meta property=&quot;og:video:tag&quot; content=&quot;version two&quot;/&gt;&lt;meta property=&quot;fb:app_id&quot; content=&quot;87741124305&quot;/&gt;&lt;meta name=&quot;twitter:card&quot; content=&quot;player&quot;/&gt;&lt;meta name=&quot;twitter:site&quot; content=&quot;@youtube&quot;/&gt;&lt;meta name=&quot;twitter:url&quot; content=&quot;https://www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;Raycasting engine in Factorio 1.0 (unmodded) - Facto-RayO v2.0&quot;/&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;Version 2.0 of my ray casting engine build entirely inside factorio. No mods are needed to run it. more information and download: https://forums.factorio.com...&quot;/&gt;&lt;meta name=&quot;twitter:image&quot; content=&quot;https://i.ytimg.com/vi/28UzqVz1r24/maxresdefault.jpg&quot;/&gt;&lt;meta name=&quot;twitter:app:name:iphone&quot; content=&quot;YouTube&quot;/&gt;&lt;meta name=&quot;twitter:app:id:iphone&quot; content=&quot;544007664&quot;/&gt;&lt;meta name=&quot;twitter:app:name:ipad&quot; content=&quot;YouTube&quot;/&gt;&lt;meta name=&quot;twitter:app:id:ipad&quot; content=&quot;544007664&quot;/&gt;&lt;meta name=&quot;twitter:app:url:iphone&quot; content=&quot;vnd.youtube://www.youtube.com/watch?v=28UzqVz1r24&amp;amp;feature=applinks&quot;/&gt;&lt;meta name=&quot;twitter:app:url:ipad&quot; content=&quot;vnd.youtube://www.youtube.com/watch?v=28UzqVz1r24&amp;amp;feature=applinks&quot;/&gt;&lt;meta name=&quot;twitter:app:name:googleplay&quot; content=&quot;YouTube&quot;/&gt;&lt;meta name=&quot;twitter:app:id:googleplay&quot; content=&quot;com.google.android.youtube&quot;/&gt;&lt;meta name=&quot;twitter:app:url:googleplay&quot; content=&quot;https://www.youtube.com/watch?v=28UzqVz1r24&quot;/&gt;&lt;meta name=&quot;twitter:player&quot; content=&quot;https://www.youtube.com/embed/28UzqVz1r24&quot;/&gt;&lt;meta name=&quot;twitter:player:width&quot; content=&quot;640&quot;/&gt;&lt;meta name=&quot;twitter:player:height&quot; content=&quot;360&quot;/&gt;
  
  
  

      
</description>
<pubDate>Sat, 03 Oct 2020 00:17:38 +0000</pubDate>
<dc:creator>bufferoverflow</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.youtube.com/watch?v=28UzqVz1r24</dc:identifier>
</item>
</channel>
</rss>