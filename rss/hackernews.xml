<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>800M Email Addresses Leaked Online by Email Verification Service</title>
<link>https://securitydiscovery.com/800-million-emails-leaked-online-by-email-verification-service/</link>
<guid isPermaLink="true" >https://securitydiscovery.com/800-million-emails-leaked-online-by-email-verification-service/</guid>
<description>
&lt;div class=&quot;image-full&quot;&gt;&lt;img width=&quot;1502&quot; height=&quot;1008&quot; src=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM.png&quot; class=&quot;attachment-full size-full wp-post-image&quot; alt=&quot;&quot; srcset=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM.png 1502w, https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM-300x201.png 300w, https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM-768x515.png 768w, https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM-1024x687.png 1024w&quot; sizes=&quot;(max-width: 1502px) 100vw, 1502px&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;entry-content&quot; data-initials=&quot;&quot;&gt;
&lt;p&gt;&lt;span&gt;On February 25th, 2019, I discovered a non-password protected 150GB-sized MongoDB instance. This is perhaps the biggest and most comprehensive email database I have ever reported. Upon verification I was shocked at the massive number of emails that were publicly accessible for anyone with an internet connection. Some of data was much more detailed than just the email address and included personally identifiable information (PII).  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;This database contained four separate collections of data and combined was an astounding 808,539,939 records. The largest part of it was named ‘mailEmailDatabase’ – and inside it contained three folders:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Emailrecords (count: 798,171,891 records)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;emailWithPhone (count: 4,150,600 records)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;businessLeads (count: 6,217,358 records)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic3.jpg&quot;&gt;&lt;img class=&quot;aligncenter wp-image-350&quot; src=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic3.jpg&quot; alt=&quot;&quot; width=&quot;511&quot; height=&quot;411&quot; srcset=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic3.jpg 692w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic3-300x241.jpg 300w&quot; sizes=&quot;(max-width: 511px) 100vw, 511px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;‘Emailrecords’ was structured  to include zip / phone / address / gender / email / user IP / DOB:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic1.jpg&quot;&gt;&lt;img class=&quot;aligncenter wp-image-348&quot; src=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic1.jpg&quot; alt=&quot;&quot; width=&quot;665&quot; height=&quot;264&quot; srcset=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic1.jpg 825w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic1-300x119.jpg 300w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic1-768x305.jpg 768w&quot; sizes=&quot;(max-width: 665px) 100vw, 665px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;As part of the verification process I  cross-checked a random selection of records with Troy Hunt’s HaveIBeenPwned database. Based on the results, I came to conclusion that this is not just another ‘Collection’ of previously leaked sources but a completely unique set of data. Although, not all records contained the detailed profile information about the email owner, a large amount of records were very detailed. We are still talking about millions of records.  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;I started to analyze the content in an attempt to identify the owner and responsibly disclose it – even despite the fact that this started to look very much like a spam organization dataset.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In addition to the email databases this unprotected Mongo instance it also uncovered details on the possible owner of the database – a company named ‘Verifications.io’ – which offered the services of ‘Enterprise Email Validation’. Unfortunately, it appears that once emails were uploaded for verification they were also stored in plain text. Once I reported my discovery to Verifications.io the site was taken offline and is currently down at the time of this publication. &lt;a href=&quot;https://web.archive.org/web/20190227230352/https://verifications.io/&quot;&gt;Here is the archived version&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic2.jpg&quot;&gt;&lt;img class=&quot;aligncenter size-full wp-image-349&quot; src=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic2.jpg&quot; alt=&quot;&quot; width=&quot;1898&quot; height=&quot;845&quot; srcset=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic2.jpg 1898w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic2-300x134.jpg 300w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic2-768x342.jpg 768w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic2-1024x456.jpg 1024w&quot; sizes=&quot;(max-width: 1898px) 100vw, 1898px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;At this point I teamed up with Vinny Troya, owner of NightLion Security with whom I worked on other projects previously and who had a similar experience with &lt;a href=&quot;https://www.wired.com/story/exactis-database-leak-340-million-records/&quot;&gt;finding the Exactis database&lt;/a&gt;&lt;/span&gt; &lt;span&gt; . I also sent a data breach notification email to the company’s support (yes, I decided it is a right thing to do).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic4.jpg&quot;&gt;&lt;img class=&quot;aligncenter wp-image-352&quot; src=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic4.jpg&quot; alt=&quot;&quot; width=&quot;518&quot; height=&quot;409&quot; srcset=&quot;https://securitydiscovery.com/wp-content/uploads/2019/03/pic4.jpg 718w, https://securitydiscovery.com/wp-content/uploads/2019/03/pic4-300x236.jpg 300w&quot; sizes=&quot;(max-width: 518px) 100vw, 518px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;After researching more about Verifications.io online and comparing the information that was publicly available in the database we have come to the following conclusions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;How this all works&lt;/strong&gt;:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;span&gt;Someone uploads a list of email addresses that they want to validate.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Verifications.io has a list of mail servers and internal email accounts that they use to “validate” an email address.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;They do this by literally sending the people an email. If it does not bounce, the email is validated.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;If it bounces, they put it in a bounce list so they can easily validate later on.&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Here is the scenario:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“Mr. Threat Actor” has a list of 1000 companies that he wants to hack into. He has a bunch of potential users and passwords, but has no idea which ones are real. He could try to log in to a service or system using ALL of those accounts, but that type of brute force attack is very noisy and would likely be identified. Instead, he uploads all of his potential email addresses to a service like&lt;/span&gt; &lt;span&gt;verifications.io&lt;/span&gt;&lt;span&gt;. The email verification service then sends tens of thousands of emails to validate these users (some real, some not). Each one of the users on the list gets their own spam message saying “hi”. Then the threat actor gets a cleaned, verified, and valid list of users at these companies. Now he knows who works there and who does not, and he can start a more focused phishing or brute forcing campaign.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How do I know this?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The database(s) included email accounts they use for sending mail as well as hundreds of SMTP servers, email, spam traps, keywords to avoid, IP addresses to blacklist, and more. This is why I initially thought they were potentially engaged in spam related activities. It turns out that technically they actually are sending unwanted and unsolicited emails. This is the worst kind of spam because they send millions of completely worthless “hello” emails that no one can understand.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;As I mentioned they did act fast and the database was taken down the same day I sent notification email to the company’s support. Ironically, they did reply to my notification. In the response they identified that what I had discovered was public data and not client data, so why close the database and take the site offline if it indeed was “public”? In addition to the email profiles this database also had access details and a user list of (130 records), with names and credentials to access FTP server to upload / download email lists (hosted on the same IP with MongoDB). We can only speculate that this was not meant to be public data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;You can read more details about this discovery that was featured in: Wired – &lt;a href=&quot;https://www.wired.com/story/email-marketing-company-809-million-records-exposed-online/&quot;&gt;Click Here&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About author and security researcher:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Bob Diachenko has over 12 years experience working in corporate/product/internal communications with a strong focus on infosecurity, IT and technology. In the past Bob has worked with top tier media, government agencies, and law enforcement to help secure exposed data. Follow Bob on &lt;a href=&quot;https://twitter.com/MayhemDayOne&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Twitter&lt;/a&gt; and his blog on &lt;a href=&quot;https://www.linkedin.com/in/vdyachenko/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Linkedin&lt;/a&gt;, Email: bob@securitydiscovery.com&lt;/p&gt;

&lt;/div&gt;
&lt;footer class=&quot;entry-footer&quot;&gt;

&lt;/footer&gt;</description>
<pubDate>Thu, 07 Mar 2019 23:12:43 +0000</pubDate>
<dc:creator>tlrobinson</dc:creator>
<og:type>article</og:type>
<og:title>800+ Million Emails Leaked Online by Email Verification Service - Security Discovery</og:title>
<og:description>Our Biggest Data Breach Discovery of 2019 a massive 800 million emails leaked online. This data breach uncovered how an email verification service uses spam</og:description>
<og:url>https://securitydiscovery.com/800-million-emails-leaked-online-by-email-verification-service/</og:url>
<og:image>https://securitydiscovery.com/wp-content/uploads/2019/03/Screen-Shot-2019-03-06-at-3.58.36-PM.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://securitydiscovery.com/800-million-emails-leaked-online-by-email-verification-service/</dc:identifier>
</item>
<item>
<title>What&amp;#039;s the minimum number of words you&amp;#039;d need to define all other words? (2012)</title>
<link>https://www.reddit.com/r/AskReddit/comments/sxqt5/what_is_the_minimum_number_of_words_that_you/</link>
<guid isPermaLink="true" >https://www.reddit.com/r/AskReddit/comments/sxqt5/what_is_the_minimum_number_of_words_that_you/</guid>
<description>&lt;p class=&quot;s90z9tc-10 fHRkcP&quot;&gt;I realize that this question most likely doesn't have a specific answer, but it's an interesting thought experiment.&lt;/p&gt;
&lt;p class=&quot;s90z9tc-10 fHRkcP&quot;&gt;How many words are needed to form a basis that can be used to define all other words? Once a word that's not in the original basis has been defined, it can be used to define other words.&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 19:29:02 +0000</pubDate>
<dc:creator>devilcius</dc:creator>
<og:title>r/AskReddit - What is the minimum number of words that you would need to define all other words in the English language?</og:title>
<og:type>website</og:type>
<og:url>https://www.reddit.com/r/AskReddit/comments/sxqt5/what_is_the_minimum_number_of_words_that_you/</og:url>
<og:description>260 votes and 124 comments so far on Reddit</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reddit.com/r/AskReddit/comments/sxqt5/what_is_the_minimum_number_of_words_that_you/</dc:identifier>
</item>
<item>
<title>Airbnb to Acquire HotelTonight</title>
<link>https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/</link>
<guid isPermaLink="true" >https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/</guid>
<description>&lt;ul&gt;&lt;li&gt;&lt;em&gt;Continues to Build End-to-End Travel Platform&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Delivering on Commitment to Bring Magical Travel to Everyone&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span&gt;Airbnb’s mission is to create a world where anyone can belong anywhere. To advance our mission, we are reimagining travel by building an end-to-end travel platform that combines where you stay, what you do, and how you get there, all in one place. Today, we are building on this work by signing an agreement to acquire HotelTonight. HotelTonight is a hotel-booking service focused on making last-minute trips easy and fun, offering guests seamless, on-demand booking for boutique and independent hotels.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span&gt;A big part of building an end-to-end travel platform is serving every guest, whether they plan their trip a year or a day in advance.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Working with the incredible team at HotelTonight, we will offer guests an unparalleled last-minute travel experience that provides unique, memorable hospitality on every trip, on any schedule, at any time.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Brian Chesky, Airbnb Co-Founder, CEO and Head of Community&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;Airbnb has built an unrivaled global community of hosts and guests—we can connect travelers to more than 6 million places to stay and tens of thousands of incredible experiences around the world. Airbnb “is now the most searched-for accommodations brand” according to&lt;/span&gt; &lt;a href=&quot;http://www.hotelnewsnow.com/Articles/291386/Glamping-as-an-asset-class-Google-misspelling&quot;&gt;&lt;span&gt;one Google executive&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, and guests are using Airbnb for every kind of trip, including business and last-minute travel. Today, more than 400,000 companies are using Airbnb to help manage their travel, and same-day bookings are now growing 2x year over year. Welcoming more boutique hotels to our platform will help us deliver on our commitment to make Airbnb for everyone, providing guests the authentic, local experience they have come to expect on every trip.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span&gt;We started HotelTonight because we knew people wanted a better way to book an amazing hotel room on-demand, and we are excited to join forces with Airbnb to bring this service to guests around the world.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Together, HotelTonight and Airbnb can give guests more choices and the world’s best boutique and independent hotels a genuine partner to connect them with those guests.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Sam Shank, Co-founder &amp;amp; CEO of HotelTonight&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;The acquisition of HotelTonight:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Accelerates our work to build an end-to-end travel platform that serves everyone.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;We are making it easier for people who use Airbnb to find last-minute places to stay when Home hosts are often already booked. The availability of boutique hotels—in addition to private rooms and entire homes that are instantly bookable—helps ensure authentic, high-quality stays are available on-demand, especially at the last minute.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The&lt;/span&gt; &lt;span&gt;HotelTonight&lt;/span&gt; &lt;span&gt;team has developed deep relationships with boutique hotels and expertise in seamless, last-minute booking.&lt;/span&gt; &lt;span&gt;HotelTonight&lt;/span&gt; &lt;span&gt;is rooted in the same kinds of hospitality properties we have welcomed to the Airbnb community, and these boutique and independent hotels contribute the majority of&lt;/span&gt; &lt;span&gt;HotelTonight&lt;/span&gt;&lt;span&gt;’s business.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Meets enormous demand from—and for—boutique hotels.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Over the past few years, we’ve seen an increasing number of hoteliers take inspiration from our Home hosts, making their boutique hotels more unique with custom touches. Last year,&lt;/span&gt; &lt;a href=&quot;https://press.airbnb.com/airbnb-unveils-roadmap-to-bring-magical-travel-to-everyone/&quot;&gt;&lt;span&gt;we announced our intent&lt;/span&gt;&lt;/a&gt; &lt;span&gt;to&lt;/span&gt; &lt;a href=&quot;https://www.airbnb.com/b/hotels&quot;&gt;&lt;span&gt;make it easier for these boutique hotels to list on our platform&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. We’ve had an overwhelming response from hoteliers&lt;/span&gt; &lt;span&gt;with strong ties to their local communities who want to offer truly personal hospitality&lt;/span&gt;&lt;span&gt;: in 2018, we more than doubled the number of rooms available on Airbnb in properties that hosts categorized as boutique hotels, bed and breakfasts, and other hospitality venues like hostels and resorts.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;We’ve also seen Airbnb guests respond enthusiastically to this increased supply, booking three times as many nights with boutique hotels in 2018 compared with 2017.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Together with HotelTonight, we can offer places to stay for every type of traveler on all kinds of trips and make it easier for small businesses and hoteliers who take pride in providing authentic, unique experiences to tap into Airbnb’s truly global network of hosts and guests.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Supports Home hosts.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;As we have welcomed boutique hotels to Airbnb, we’ve been delighted to see the positive impact for our Home hosts. The acquisition of&lt;/span&gt; &lt;span&gt;HotelTonight&lt;/span&gt; &lt;span&gt;will accelerate that positive benefit. Boutique hotels are helping to bring new guests to Airbnb and making our community larger and stronger. Once these new guests come to Airbnb, they’re returning to book with our Home hosts: in fact, nearly 90 percent of guests who first used Airbnb to book a hotel room and returned to our platform for a second trip then booked a home.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We’re excited to continue our work to grow our community and make Airbnb for everyone. The HotelTonight app and website will continue to operate as they do today. Once the acquisition is complete, Sam Shank will report to Greg Greeley, Airbnb’s President of Homes, and lead our boutique hotel category.&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 18:43:04 +0000</pubDate>
<dc:creator>c_t_montgomery</dc:creator>
<og:type>article</og:type>
<og:title>Airbnb Signs Agreement to Acquire HotelTonight</og:title>
<og:description>To advance our mission, we are reimagining travel by building an end-to-end travel platform. Today, we are building on this work by signing an agreement to acquire HotelTonight.</og:description>
<og:url>https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/</og:url>
<og:image>https://press.airbnb.com/wp-content/uploads/sites/4/2019/03/PJMPHOTO18Q335-Society-Hotel-03568.jpg?fit=2000%2C1334</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://press.airbnb.com/airbnb-signs-agreement-to-acquire-hoteltonight/</dc:identifier>
</item>
<item>
<title>Scheduling in React</title>
<link>https://philippspiess.com/scheduling-in-react/</link>
<guid isPermaLink="true" >https://philippspiess.com/scheduling-in-react/</guid>
<description>&lt;p&gt;In modern applications, user interfaces often have to juggle multiple tasks at the same time. For example, a search component might need to respond to user input while providing auto completion results, and an interactive dashboard might need to update charts while loading data from the server and sending analytics data to a backend.&lt;/p&gt;
&lt;p&gt;All these parallel steps can lead to slow and unresponsive interfaces and unhappy users, so let’s learn how we can fix this.&lt;/p&gt;
&lt;h2&gt;Scheduling in User Interfaces&lt;/h2&gt;
&lt;p&gt;Our users expect immediate feedback. Whether they are clicking on a button to open a modal or adding text to an input field, they don’t want to wait before seeing some kind of confirmation. For example, the button could show a modal and the input field could display the key that was typed.&lt;/p&gt;
&lt;p&gt;To visualize what happens when this is not the case, let’s take a look at the demo application that Dan Abramov presented at his talk, &lt;a href=&quot;https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html&quot;&gt;Beyond React 16&lt;/a&gt;, at JSConf Iceland 2018.&lt;/p&gt;
&lt;p&gt;The application works like this: The more you type into the input below, the more detailed the charts below will get. Since both of the updates (the input element and the chart) run at the same time, the browser has to do so much computation that it will drop some frames. This leads to noticeable delays and a bad user experience:&lt;/p&gt;
&lt;p&gt;However, a version that prioritizes updating the input with new keystrokes will appear to the end user as though it’s running a lot faster. This is because users receive immediate feedback even though the same computation time is required:&lt;/p&gt;
&lt;p&gt;Unfortunately, current user interface architectures make it non-trivial to implement prioritization. One way to work around this problem is by &lt;a href=&quot;https://davidwalsh.name/javascript-debounce-function&quot;&gt;debouncing&lt;/a&gt; the chart update. The problem with this approach is that the charts still render synchronously when the debounced callback fires, which will again cause the user interface to be unresponsive for some time. We can do better!&lt;/p&gt;
&lt;h2&gt;Browser Event Loop&lt;/h2&gt;
&lt;p&gt;Before we learn more about how proper prioritizing of updates can be achieved, let’s dig deeper and understand why the browser has issues with these kind of user interactions.&lt;/p&gt;
&lt;p&gt;JavaScript code is executed in one thread, meaning that only one line of JavaScript can be run at any given time. The same thread is also responsible for other document lifecycles, like layout and paint.&lt;sup id=&quot;fnref-1&quot;/&gt; This means that whenever JavaScript code runs, the browser is blocked from doing anything else.&lt;/p&gt;
&lt;p&gt;To keep the user interface responsive, we only have a very short timeframe before we need to be able to receive the next input events. At the Chrome Dev Summit 2018, Shubhie Panicker and Jason Miller gave a talk, &lt;a href=&quot;https://developer.chrome.com/devsummit/schedule/scheduling-on-off-main-thread&quot;&gt;A Quest to Guarantee Responsiveness&lt;/a&gt;. During the talk, they showed the following visualization of the browser’s run loop, in which we can see that we only have 16ms (on a typical 60Hz screen) before the next frame is drawn and the next event needs to be processed:&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;https://philippspiess.com/static/805b72e5fe22f38f3f794de9668a14cc/5854f/event-loop-browser.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot;&gt; &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;The browser event loop starts by running input handlers. Then it runs animation frame callbacks, and it ends with document lifecycles (style, layout, paint). All of this should complete within one frame, which is approximately 16ms on a 60Hz display.&quot; title=&quot;&quot; src=&quot;https://philippspiess.com/static/805b72e5fe22f38f3f794de9668a14cc/26338/event-loop-browser.png&quot; srcset=&quot;/static/805b72e5fe22f38f3f794de9668a14cc/fd659/event-loop-browser.png 175w, /static/805b72e5fe22f38f3f794de9668a14cc/bbe02/event-loop-browser.png 350w, /static/805b72e5fe22f38f3f794de9668a14cc/26338/event-loop-browser.png 700w, /static/805b72e5fe22f38f3f794de9668a14cc/efbfc/event-loop-browser.png 1050w, /static/805b72e5fe22f38f3f794de9668a14cc/42736/event-loop-browser.png 1400w, /static/805b72e5fe22f38f3f794de9668a14cc/5854f/event-loop-browser.png 1692w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Most JavaScript frameworks (including the current version of React) will run updates synchronously. We can think of this behavior as a function &lt;code class=&quot;language-text&quot;&gt;render()&lt;/code&gt; that will only return once the DOM is updated. During this time, the main thread is blocked.&lt;/p&gt;
&lt;h2&gt;Problems with Current Solutions&lt;/h2&gt;
&lt;p&gt;With the information above, we can formulate two problems that we have to solve in order to get to more responsive user interfaces:&lt;/p&gt;
&lt;ol readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Long-running tasks cause frame drops.&lt;/strong&gt; We need to make sure all of our tasks are small and can be completed within a couple of milliseconds so that we can run them within one frame.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Different tasks have different priorities.&lt;/strong&gt; In the example application above, we saw that prioritizing the user input leads to a better experience overall. To do this, we need a way to define the order and to schedule tasks accordingly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;Concurrent React and the Scheduler&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;⚠️ Warning: The following APIs are not yet stable and will change. I will do my best to keep this post updated.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To implement a properly scheduled user interface with React, we have to look into two upcoming React features:&lt;/p&gt;
&lt;ul readability=&quot;15.305555555556&quot;&gt;&lt;li readability=&quot;8.6966292134831&quot;&gt;
&lt;p&gt;&lt;strong&gt;Concurrent React (also known as Time Slicing).&lt;/strong&gt; With the help of the new &lt;a href=&quot;https://www.youtube.com/watch?v=ZCuYPiUIONs&quot;&gt;Fiber architecture&lt;/a&gt; rewrite that was released with React 16, React can now pause during rendering and yield&lt;sup id=&quot;fnref-2&quot;/&gt; to the main thread.&lt;/p&gt;
&lt;p&gt;We will hear more about Concurrent React in the future. For now it is important to understand that when this mode is enabled, React will split the synchronous rendering of our React components into pieces that are run over multiple frames.&lt;/p&gt;
&lt;p&gt;➡️ With this feature, we’re able to split long-running rendering tasks into small chunks.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;21.870359457867&quot;&gt;
&lt;p&gt;&lt;strong&gt;Scheduler.&lt;/strong&gt; The general purpose cooperative main thread scheduler is developed by the React Core team and makes it possible to register callbacks with different priority levels in the browser.&lt;/p&gt;
&lt;p&gt;At the time of writing this article, the priority levels are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Immediate&lt;/code&gt; for tasks that need to run synchronously.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;UserBlocking&lt;/code&gt; (250ms timeout) for tasks that should run as the result of a user interaction (e.g. a button click).&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Normal&lt;/code&gt; (5s timeout) for updates that don’t have to feel instantaneous.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Low&lt;/code&gt; (10s timeout) for tasks that can be deferred but must still complete eventually (e.g. an analytics notification).&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;Idle&lt;/code&gt; (no timeout) for tasks that do not have to run at all (e.g. hidden offscreen content).&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The timeouts for each priority level are necessary to make sure that lower priority work still runs even if we have so much higher priority work to do that the higher priority work could run continuously. In scheduling algorithms, this problem is referred to as &lt;a href=&quot;https://en.wikipedia.org/wiki/Starvation_(computer_science)&quot;&gt;starvation&lt;/a&gt;. The timeouts give us the guarantee that every scheduled task will eventually run. For example, we won’t miss a single analytics notification, even if we have ongoing animations in our app.&lt;/p&gt;
&lt;p&gt;Under the hood, the Scheduler will store all registered callbacks in a list ordered by the expiration time (which is the time at which the callback was registered plus the timeout of the priority level). Then, the Scheduler will itself register a callback that is run after the next frame is drawn by the browser.&lt;sup id=&quot;fnref-3&quot;/&gt; Within this callback, the Scheduler will execute as many of the registered callbacks as possible until it’s time to render the next frame.&lt;/p&gt;
&lt;p&gt;➡️ With this feature, we can schedule tasks with different priorities.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Scheduling in Action&lt;/h2&gt;
&lt;p&gt;Let’s see how we can use these features to make an app feel a lot more responsive. To do this, we’ll take a look at &lt;a href=&quot;https://github.com/philipp-spiess/scheduletron3000&quot;&gt;ScheduleTron 3000&lt;/a&gt;, an app I built that allows users to highlight a search term in a list of names. Let’s take a look at the initial implementation first:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;js&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;


&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;App&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;searchValue&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; setSearchValue&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; React&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;useState&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;handleChange&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token function&quot;&gt;setSearchValue&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;div&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;SearchBox onChange&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;handleChange&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;NameList searchValue&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;searchValue&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;div&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;






&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;SearchBox&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;props&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;inputValue&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; setInputValue&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; React&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;useState&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;handleChange&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;event&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; value &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; event&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;target&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;token function&quot;&gt;setInputValue&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    props&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;onChange&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;token function&quot;&gt;sendAnalyticsNotification&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;input
      type&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;text&quot;&lt;/span&gt;
      value&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;inputValue&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
      onChange&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;handleChange&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;

ReactDOM&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;App &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; container&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;ℹ️ This example uses &lt;a href=&quot;https://reactjs.org/docs/hooks-intro.html&quot;&gt;React Hooks&lt;/a&gt;. If you’re not familiar with this new React feature, take a look at the &lt;a href=&quot;https://codesandbox.io/s/j3zrqpzkr5&quot;&gt;CodeSandbox code&lt;/a&gt;. Additionally, you might wonder why we use two different state variables for this example. We’ll find out why this is needed in a bit.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Try it out! Type a name (e.g. “Ada Stewart”) in the search box below and see how it works:&lt;/p&gt;

&lt;p&gt;You might notice that the interface is not very responsive. To amplify the issue, I artificially slowed down the rendering time of the names list. And since this list is big, it has a significant impact on the application’s performance. This is not good 😰.&lt;/p&gt;
&lt;p&gt;Our users expect immediate feedback, but the app is unresponsive for quite some time after a keystroke. To understand what’s going on, let’s take a look at the DevTools’ Performance tab. Here’s a screenshot of a recording I made while I type the name “Ada” into the search box:&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;https://philippspiess.com/static/d8e525b8fc31fdba90634f8577da8301/3e72c/devtools-sync.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot;&gt; &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Screenshot of Chrome DevTools that shows that the three keypress events take 733ms to render.&quot; title=&quot;&quot; src=&quot;https://philippspiess.com/static/d8e525b8fc31fdba90634f8577da8301/26338/devtools-sync.png&quot; srcset=&quot;/static/d8e525b8fc31fdba90634f8577da8301/fd659/devtools-sync.png 175w, /static/d8e525b8fc31fdba90634f8577da8301/bbe02/devtools-sync.png 350w, /static/d8e525b8fc31fdba90634f8577da8301/26338/devtools-sync.png 700w, /static/d8e525b8fc31fdba90634f8577da8301/efbfc/devtools-sync.png 1050w, /static/d8e525b8fc31fdba90634f8577da8301/42736/devtools-sync.png 1400w, /static/d8e525b8fc31fdba90634f8577da8301/8b47e/devtools-sync.png 2100w, /static/d8e525b8fc31fdba90634f8577da8301/3e72c/devtools-sync.png 2576w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can see there are a lot of red triangles, which is usually not a good sign. For every keystroke, we see a &lt;code class=&quot;language-text&quot;&gt;keypress&lt;/code&gt; event being fired. All three events run within one frame,&lt;sup id=&quot;fnref-5&quot;/&gt; which causes the duration of the frame to extend to &lt;strong&gt;733ms&lt;/strong&gt;. That’s way above our average frame budget of 16ms.&lt;/p&gt;
&lt;p&gt;Inside this &lt;code class=&quot;language-text&quot;&gt;keypress&lt;/code&gt; event, our React code will be called, which causes the input value and the search value to update and then send the analytics notification. In turn, the updated state values will cause the app to rerender down to every individual name. That’s quite a lot of work that we have to do, and with a naive approach, it would block the main thread!&lt;/p&gt;
&lt;p&gt;The first step toward improving the status quo is to enable the unstable Concurrent Mode. This can be done by wrapping a part of our React tree with the &lt;code class=&quot;language-text&quot;&gt;&amp;lt;React.unstable_ConcurrentMode&amp;gt;&lt;/code&gt; component, like this&lt;sup id=&quot;fnref-4&quot;/&gt;:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;diff&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;language-diff&quot;&gt;
&lt;code class=&quot;language-diff&quot;&gt;&lt;span class=&quot;token deleted&quot;&gt;- ReactDOM.render(&amp;lt;App /&amp;gt;, container);&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+ ReactDOM.render(&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+  &amp;lt;React.unstable_ConcurrentMode&amp;gt;&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+    &amp;lt;App /&amp;gt;&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+  &amp;lt;/React.unstable_ConcurrentMode&amp;gt;,&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+  rootElement&lt;/span&gt;
&lt;span class=&quot;token inserted&quot;&gt;+ );&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, enabling Concurrent Mode alone will not change the experience in our case. React will still receive both state updates at the same time, so there’s no way of knowing which is less important.&lt;/p&gt;
&lt;p&gt;We instead want to set the input value first so that we only need to update the search box in the beginning. Updates to the search value and our analytics notification should happen later. To do this, we’re using an API exposed by the Scheduler package (which can be installed with &lt;code class=&quot;language-text&quot;&gt;npm i scheduler&lt;/code&gt;) to enqueue a lower priority callback:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight has-highlighted-lines&quot; data-language=&quot;js&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; unstable_next &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;scheduler&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;SearchBox&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;props&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;inputValue&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; setInputValue&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; React&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;useState&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

  &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;handleChange&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;event&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;const&lt;/span&gt; value &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; event&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;target&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;token function&quot;&gt;setInputValue&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;    &lt;span class=&quot;token function&quot;&gt;unstable_next&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;      props&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;onChange&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;      &lt;span class=&quot;token function&quot;&gt;sendAnalyticsNotification&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;    &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;gatsby-highlight-code-line&quot;&gt;  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;
  &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;input type&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;text&quot;&lt;/span&gt; value&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;inputValue&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; onChange&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;handleChange&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Inside the API we’re using, &lt;code class=&quot;language-text&quot;&gt;unstable_next()&lt;/code&gt;, all React updates will be scheduled with the &lt;code class=&quot;language-text&quot;&gt;Normal&lt;/code&gt; priority, which is lower then the default priority inside an &lt;code class=&quot;language-text&quot;&gt;onChange&lt;/code&gt; listener.&lt;/p&gt;
&lt;p&gt;Indeed, with this change, our input box already feels a lot more responsive, and frames no longer get dropped while we’re typing. Let’s take another look at the Performance tab together:&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;https://philippspiess.com/static/a523aeacdf07611d54568ba07f655d9d/50025/devtools-normal.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot;&gt; &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Screenshot of Chrome DevTools that shows that React breaks the rendering work down into small chunks. All frames can be drawn very quickly, although the analytics notifications are still sent in the middle of the rendering work.&quot; title=&quot;&quot; src=&quot;https://philippspiess.com/static/a523aeacdf07611d54568ba07f655d9d/26338/devtools-normal.png&quot; srcset=&quot;/static/a523aeacdf07611d54568ba07f655d9d/fd659/devtools-normal.png 175w, /static/a523aeacdf07611d54568ba07f655d9d/bbe02/devtools-normal.png 350w, /static/a523aeacdf07611d54568ba07f655d9d/26338/devtools-normal.png 700w, /static/a523aeacdf07611d54568ba07f655d9d/efbfc/devtools-normal.png 1050w, /static/a523aeacdf07611d54568ba07f655d9d/42736/devtools-normal.png 1400w, /static/a523aeacdf07611d54568ba07f655d9d/8b47e/devtools-normal.png 2100w, /static/a523aeacdf07611d54568ba07f655d9d/50025/devtools-normal.png 2580w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We see that the long-running tasks are now broken down into smaller ones that can be completed within a single frame. The red triangles that indicate frame drops are also gone.&lt;/p&gt;
&lt;p&gt;However, one thing that is still not ideal is that the analytics notification (highlighted in the above screenshot) is still executed with the rendering work. Since the users of our app do not see this task, we can schedule a callback with an even lower priority for that:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;js&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  unstable_LowPriority&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  unstable_runWithPriority&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
  unstable_scheduleCallback
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;scheduler&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;sendDeferredAnalyticsNotification&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;token function&quot;&gt;unstable_runWithPriority&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;unstable_LowPriority&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token function&quot;&gt;unstable_scheduleCallback&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;token function&quot;&gt;sendAnalyticsNotification&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;value&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we now use &lt;code class=&quot;language-text&quot;&gt;sendDeferredAnalyticsNotification()&lt;/code&gt; in our search box component and take another look at the Performance tab with this change and scroll toward the end, we’ll see that our analytics are now sent after all rendering work has completed, and so all the tasks in our app are perfectly scheduled:&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;https://philippspiess.com/static/e5ef87ea4a1dc1f0ef5c1a52776ca343/63a84/devtools-normal-and-low.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span class=&quot;gatsby-resp-image-wrapper&quot;&gt; &lt;img class=&quot;gatsby-resp-image-image&quot; alt=&quot;Screenshot of Chrome DevTools that show that React breaks the rendering work down into small chunks. Analytics are sent at the end after all rendering work has completed.&quot; title=&quot;&quot; src=&quot;https://philippspiess.com/static/e5ef87ea4a1dc1f0ef5c1a52776ca343/26338/devtools-normal-and-low.png&quot; srcset=&quot;/static/e5ef87ea4a1dc1f0ef5c1a52776ca343/fd659/devtools-normal-and-low.png 175w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/bbe02/devtools-normal-and-low.png 350w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/26338/devtools-normal-and-low.png 700w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/efbfc/devtools-normal-and-low.png 1050w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/42736/devtools-normal-and-low.png 1400w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/8b47e/devtools-normal-and-low.png 2100w, /static/e5ef87ea4a1dc1f0ef5c1a52776ca343/63a84/devtools-normal-and-low.png 2578w&quot; sizes=&quot;(max-width: 700px) 100vw, 700px&quot;/&gt;&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try it out:&lt;/p&gt;

&lt;h2&gt;Limitations of the Scheduler&lt;/h2&gt;
&lt;p&gt;With the Scheduler, it’s possible to control in what order callbacks are executed. It’s built deep into the latest React implementation and works out of the box with Concurrent mode.&lt;/p&gt;
&lt;p&gt;That said, there are two limitations of the Scheduler:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Resource Fighting.&lt;/strong&gt; The Scheduler tries to use all of the resources available. This causes issues if multiple instances of a scheduler run on the same thread and compete for resources. We need to ensure that all parts of our application will use the same instance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balancing user-defined tasks with browser work.&lt;/strong&gt; Since the Scheduler runs in the browser, it only has access to the APIs the browser exposes. Document lifecycles like rendering or garbage collection can interfere with the work in an uncontrollable way.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;To remove these limitations, the Google Chrome team is working together with React, Polymer, Ember, Google Maps, and the Web Standards Community to create a &lt;a href=&quot;https://github.com/spanicker/main-thread-scheduling&quot;&gt;Scheduling API in the browser&lt;/a&gt;. What an exciting time!&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Concurrent React and the Scheduler allow us to implement scheduling of tasks in our applications which will allow us to create highly responsive user interfaces.&lt;/p&gt;
&lt;p&gt;The official release for these features will likely happen in &lt;a href=&quot;https://reactjs.org/blog/2018/11/27/react-16-roadmap.html#react-16x-q2-2019-the-one-with-concurrent-mode&quot;&gt;Q2 2019&lt;/a&gt;. Until then, you can play around with the unstable APIs, but be aware that they will change.&lt;/p&gt;
&lt;p&gt;If you want to be among the first to know when these APIs change or when documentation for the new features is written, subscribe to &lt;a href=&quot;https://this-week-in-react.org&quot;&gt;This Week in React ⚛️&lt;/a&gt;.&lt;/p&gt;

</description>
<pubDate>Thu, 07 Mar 2019 17:32:33 +0000</pubDate>
<dc:creator>tosh</dc:creator>
<og:title>Scheduling in React</og:title>
<og:description>In modern applications, user interfaces often have to juggle multiple tasks at the same time. For example, a search component might need to respond to user…</og:description>
<og:type>website</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://philippspiess.com/scheduling-in-react/</dc:identifier>
</item>
<item>
<title>Notepad++ drops code signing for its releases</title>
<link>https://notepad-plus-plus.org/news/notepad-7.6.4-released.html</link>
<guid isPermaLink="true" >https://notepad-plus-plus.org/news/notepad-7.6.4-released.html</guid>
<description>&lt;ol id=&quot;breadcrumbs&quot;&gt;&lt;li class=&quot;first&quot;&gt;&lt;a href=&quot;https://notepad-plus-plus.org/&quot; title=&quot;Home&quot;&gt;Home&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;news/&quot; title=&quot;News&quot;&gt;News&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;news/notepad-7.6.4-released.html&quot; title=&quot;Notepad++ 7.6.4 released&quot; class=&quot;active&quot;&gt;Notepad++ 7.6.4 released&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;u&gt;06 Mar 2019 14:33:00&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;When you install Notepad++ version 7.6.4, You might notice there's no more blue-trusted UAC popup. Here's the explanation for the reason that we remove code signing from Notepad++ :&lt;/p&gt;
&lt;p&gt;3 years ago DigiCert donated a 3 years code signing certificate to the project, and every good thing has its end, the certificate has been expired since the beginning of this year.&lt;/p&gt;
&lt;p&gt;I was trying to purchase another certificate with reasonable price. However I cannot use &quot;Notepad++&quot; as CN to sign because Notepad++ doesn’t exist as company or organization. I wasted hours and hours for getting one suitable certificate instead of working on essential thing - Notepad++ project. I realize that code signing certificate is just an overpriced masturbating toy of FOSS authors - Notepad++ has done without certificate for more than 10 years, I don’t see why I should add the dependency now (and be an accomplice of this overpricing industry). I decide to do without it.&lt;/p&gt;
&lt;p&gt;It doesn’t mean there’s no more security in Notepad++, but it will be less flexible for sure:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;SHA256 hash of Installer and other packages will be provided for every release as usual. Too bad for ugly yellow-orange UAC popup while installation.&lt;/li&gt;
&lt;li&gt;Notepad++ will check the SHA256 of all the components (SciLexer.dll, GUP.exe and nppPluginList.dll) used by the program.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Markdown is supposed to work in v7.6.3, but the needed file isn't deployed correctly by the installer. The bug is fixed in this version. Additionally Markdown is available in every package from this release.&lt;/p&gt;
&lt;p&gt;European Commission's Free and Open Source Software Auditing Bug Bounty program is still in progress, few vulnerable issues and some crash bugs are identified and fixed in this release thanks to &lt;a href=&quot;https://www.hackerone.com/&quot; target=&quot;_blank&quot;&gt;HackerOne&lt;/a&gt; team's help.&lt;/p&gt;
&lt;p&gt;Download 7.6.4 here:&lt;br/&gt;&lt;a href=&quot;/download/v7.6.4.html&quot;&gt;&lt;img title=&quot;Notepad++ Download&quot; src=&quot;/assets/images/downloadLogo.png&quot; alt=&quot;Notepad++ Download&quot; width=&quot;268&quot; height=&quot;56&quot;/&gt;&lt;br/&gt;https://notepad-plus-plus.org/download/v7.6.4.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Auto-updater will be triggered in few days if there's no critical issue found.&lt;/p&gt;
&lt;p&gt;If you find any regression or critical bug, please report here:&lt;a href=&quot;https://notepad-plus-plus.org/community/topic/17239/notepad-v7-6-4-released&quot; target=&quot;_blank&quot;&gt;&lt;br/&gt;https://notepad-plus-plus.org/community/topic/17239/notepad-v7-6-4-released&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 16:27:03 +0000</pubDate>
<dc:creator>pmh</dc:creator>
<dc:language>en-AU</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://notepad-plus-plus.org/news/notepad-7.6.4-released.html</dc:identifier>
</item>
<item>
<title>Black Hole Propulsion as Technosignature</title>
<link>https://www.centauri-dreams.org/2019/03/06/black-hole-propulsion-as-technosignature/</link>
<guid isPermaLink="true" >https://www.centauri-dreams.org/2019/03/06/black-hole-propulsion-as-technosignature/</guid>
<description>&lt;p&gt;When he was considering white dwarfs and neutron stars in the context of what he called ‘gravitational machines,’ Freeman Dyson became intrigued by the fate of a neutron star binary. He calculated in his paper of the same name (citation below) that gradual loss of energy through gravitational radiation would bring the two neutron stars together, creating a gravitational wave event of the sort that has since been observed. Long before LIGO, Dyson was talking about gravitational wave detection instruments that could track the ‘gravitational flash.’&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.centauri-dreams.org/wp-content/uploads/2019/03/Colliding-Neutron-Stars-Produce-Gold.jpg&quot; alt=&quot;&quot; width=&quot;540&quot; height=&quot;303&quot; class=&quot;aligncenter size-full wp-image-41867&quot; srcset=&quot;https://www.centauri-dreams.org/wp-content/uploads/2019/03/Colliding-Neutron-Stars-Produce-Gold.jpg 540w, https://www.centauri-dreams.org/wp-content/uploads/2019/03/Colliding-Neutron-Stars-Produce-Gold-300x168.jpg 300w&quot; sizes=&quot;(max-width: 540px) 100vw, 540px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt;: Artist conception of the moment two neutron stars collide. Credit: LIGO / Caltech / MIT.&lt;/p&gt;
&lt;p&gt;Observables of this kind, if we could figure out how to do it (and we subsequently have) fascinated Dyson, who was in this era (early 1960s) working out his ideas on Dyson spheres and the capabilities of advanced civilizations. As to the problematic merger of neutron stars in a ‘machine,’ he naturally wondered whether astrophysical evidence of manipulations of these would flag the presence of such cultures, noting that “…it would be surprising if a technologically advanced species could not find a way to design a nonradiating gravitational machine, and so to exploit the much higher velocities which neutron stars in principle make possible.”&lt;/p&gt;
&lt;p&gt;He goes on in the conclusion to the “Gravitational Engines” paper to say this: “In any search for evidences of technologically advanced societies in the universe, an investigation of anomalously intense sources of gravitational radiation ought to be included.”&lt;/p&gt;
&lt;p&gt;Searching for unusual astrophysical activity is part of what would emerge as ‘Dysonian SETI,’ or in our current parlance, the search for ‘technosignatures.’ It’s no surprise that since he discusses using binary black holes as the venue for his laser-based gravity assist, David Kipping should also be thinking along these lines. If the number of black holes in the galaxy were large enough to support a network of transportation hubs using binary black holes, what would be the telltale sign of its presence? Or would it be observable in the first place?&lt;/p&gt;
&lt;p&gt;Remember the methodology: A spacecraft emits a beam of energy at a black hole that is moving towards it, choosing the angles so that the beam returns to the spacecraft (along the so-called ‘boomerang geodesic’). With the beam making the gravitational flyby rather than the spacecraft, the vehicle can nonetheless exploit the kinetic energy of the black hole for acceleration. Huge objects up to planetary size could be accelerated in such a way, assuming their mass is far smaller than the mass of the black hole. No fuel is spent aboard the spacecraft which, using stored energy from the beam, continues to accelerate up to terminal velocity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.centauri-dreams.org/wp-content/uploads/2019/03/fig1_large-702x336.jpg&quot; alt=&quot;&quot; width=&quot;540&quot; height=&quot;258&quot; class=&quot;aligncenter size-full wp-image-41866&quot; srcset=&quot;https://www.centauri-dreams.org/wp-content/uploads/2019/03/fig1_large-702x336.jpg 540w, https://www.centauri-dreams.org/wp-content/uploads/2019/03/fig1_large-702x336-300x143.jpg 300w&quot; sizes=&quot;(max-width: 540px) 100vw, 540px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt;: Simulated image of the two merging black holes detected by LIGO, viewed face-on. LIGO’s gravitational-wave detection is the first direct observation of such a merger. Credit: LIGO / AAS Nova.&lt;/p&gt;
&lt;p&gt;Kipping likes to talk about the process in terms of a mirror. Because light loops around the approaching black hole and returns to the spacecraft, the black hole exhibits mirror-like behavior. Thus on Earth, if we bounce a ping-pong ball off a mirror, the ball returns to us. But if the mirror is moving towards us quickly, the ball returns faster because it has picked up momentum from the mirror. Light acts the same way, but light cannot return faster than the speed of light. Instead, in gaining momentum from the black hole, the light blueshifts.&lt;/p&gt;
&lt;p&gt;We exploit the gain in energy, and we can envision a sufficiently advanced civilization doing the same. If it can reach a black hole binary, it has gained an essentially free source of energy for continued operations in moving objects to relativistic speeds. Operations like these at a black hole binary result in certain effects, so there is a whisper of an observable technosignature.&lt;/p&gt;
&lt;p&gt;I discussed the question with Kipping in a recent email exchange. One problem emerges at the outset, for as he writes: “The halo drive is a very efficient system by design and that’s bad news for technosignatures: there’s zero leakage with an idealized system.” But he goes on:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The major effect I considered in the paper is the impact on the black hole binary itself. During departure, one is stealing energy from the black binary, which causes the separation between the two dead stars to shrink slightly via the loss of gravitational potential energy. However, an arriving ship would cancel out this effect by depositing approximately the same energy back into the system during a deceleration maneuver. Despite this averaging effect, there is presumably some time delay between departures and arrivals, and during this interval the black hole binary is forced into a temporarily contracted state. Since the rate of binary merger via gravitational radiation is very sensitive to the binary separation, these short intervals will experience enhanced infall rates. And thus, overall, the binary will merge faster than one should expect naturally. It may be possible to thus search for elevated merger rates than that expected to occur naturally. In addition, if the highway system is not isotropic, certain directions are preferred over others, then the binary will be forced into an eccentric orbit which may also lead to an observational signature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Tricky business, this, for a black hole binary in this formulation can be used not only for acceleration but deceleration. The latter potentially undoes the distortions caused by the former, though Kipping believes elevated merger rates between the binary pair will persist. Our technosignature, then, could be an elevated binary merger rate and excess binary eccentricity.&lt;/p&gt;
&lt;p&gt;I was also interested in directionality — was the spacecraft limited in where it could go? I learned that the halo drive would be most effective when moving in a direction that lies along the plane of the binary orbit. Traveling out of this plane is possible, though it would involve using onboard propellant to attain the correct trajectory. The potential of reaching the speed of the black hole itself remains, but excess stored energy would then need to be applied to an onboard thruster to make the course adjustment. Kipping says he has not run the numbers on this yet, but from the work so far be believes that a spacecraft could work with angles as high as 20 degrees out of the plane of the binary orbit and still reach an acceleration equal to that of the black hole.&lt;/p&gt;
&lt;p&gt;For more on the halo drive, remember that Kipping has made available a video that you can access &lt;a href=&quot;https://youtu.be/rFqL9CkNxXw&quot;&gt;here&lt;/a&gt;. The other citations are Kipping, “The Halo Drive: Fuel-free Relativistic Propulsion of Large Masses via Recycled Boomerang Photons,” accepted at the &lt;em&gt;Journal of the British Interplanetary Society&lt;/em&gt; (&lt;a href=&quot;http://coolworlds.astro.columbia.edu/halodrive_preprint.pdf&quot;&gt;preprint&lt;/a&gt;); and Dyson, “Gravitational Machines,” in A.G.W. Cameron, ed., &lt;em&gt;Interstellar Communication&lt;/em&gt;, New York: Benjamin Press, 1963, Chapter 12.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.centauri-dreams.org/wp-content/uploads/2009/05/tzf_img_post.jpg&quot; alt=&quot;tzf_img_post&quot; title=&quot;tzf_img_post&quot; width=&quot;500&quot; height=&quot;124&quot; class=&quot;aligncenter size-full wp-image-7718&quot; srcset=&quot;https://www.centauri-dreams.org/wp-content/uploads/2009/05/tzf_img_post.jpg 500w, https://www.centauri-dreams.org/wp-content/uploads/2009/05/tzf_img_post-300x74.jpg 300w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 16:12:51 +0000</pubDate>
<dc:creator>elorant</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.centauri-dreams.org/2019/03/06/black-hole-propulsion-as-technosignature/</dc:identifier>
</item>
<item>
<title>PureOS is convergent</title>
<link>https://puri.sm/posts/converging-on-convergence-pureos-is-convergent-welcome-to-the-future/</link>
<guid isPermaLink="true" >https://puri.sm/posts/converging-on-convergence-pureos-is-convergent-welcome-to-the-future/</guid>
<description>&lt;h2&gt;Many Devices, One OS&lt;/h2&gt;
&lt;p&gt;The two big mobile OS vendors have been dreaming of convergence between laptop OS and mobile OS for a long time; dreaming of being able to make the same application code execute, and operate, both on mobile phones and laptops – adapting the applications to screen size and input devices.&lt;/p&gt;
&lt;p&gt;Purism is beating the duopoly to that dream, with PureOS: we are now announcing that Purism’s PureOS is convergent, and has laid the foundation for all future applications to run on both the Librem 5 phone and Librem laptops, from the same PureOS release.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-large wp-image-62779&quot; src=&quot;https://puri.sm/wp-content/uploads/2019/03/web-on-devices-1024x576.png&quot; alt=&quot;Purism has one convergent operating system, PureOS. Google has two separate ones, ChromeOS and Android; Apple has two separate ones too, macOS and iOS.&quot; srcset=&quot;https://puri.sm/wp-content/uploads/2019/03/web-on-devices-1024x576.png 1024w, https://puri.sm/wp-content/uploads/2019/03/web-on-devices-300x169.png 300w, https://puri.sm/wp-content/uploads/2019/03/web-on-devices-768x432.png 768w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Purism has one convergent operating system, PureOS. Google has two separate ones, ChromeOS and Android; Apple has two separate ones too, macOS and iOS.&lt;/p&gt;
&lt;h2&gt;What Is Convergence?&lt;/h2&gt;
&lt;p&gt;If you’ve ever had an app on your phone that you wanted on your laptop, you’ve wanted convergence. Convergence is a term used to describe the similar functioning of an app across different platforms. Many companies are eager to have their software be convergent, because it brings a consistent look and feel, as well as the exact same functionality for apps that run on your phone and your computer.&lt;/p&gt;
&lt;p&gt;Convergence can be really handy, since it allows you to use the apps you’re already familiar with, as well as the data that you’ve already synced. Convergence also brings plenty of of benefits to developers, such as writing your app once, testing it once and running it everywhere.&lt;/p&gt;
&lt;p&gt;Since this is the ideal dream, why don’t we have convergence already? Why can’t a person run the exact same app on a phone and laptop today? It turns out that this is really hard to do unless you have complete control of software source code and access to hardware itself. Even then, there is a catch; you need to compile software for both the phone’s CPU and the laptop CPU which are usually different architectures. This is a complex process that often reveals assumptions made in software development but it shows that to build a truly convergent device you need to design for convergence from the beginning.&lt;/p&gt;
&lt;p&gt;Reaching convergence is one more checked item on the list that Purism’s &lt;a href=&quot;https://puri.sm/posts/purism-origin-story/&quot;&gt;founder envisioned&lt;/a&gt; when he created the company, specifically to:&lt;/p&gt;
&lt;p&gt;“…manufacture a mobile phone from the schematics on up that would run that same ethical operating system.”&lt;/p&gt;
&lt;h2&gt;How We Got There&lt;/h2&gt;
&lt;p&gt;The right path to get us here was starting with the “&lt;a href=&quot;https://www.debian.org/&quot;&gt;universal operating system&lt;/a&gt;” as the foundation of PureOS, Purism’s operating system. Running on so many different CPU architectures is a huge benefit, because very often laptops need a power-hungry and fast CPU, while a phone needs a power-aware, battery-saving CPU. These CPUs are, consequently, designed differently for their different uses, and you often have to “port” or cross-compile software for it to work well on both CPUs. By basing PureOS on a solid, foundational operating system – one that has been solving this performance and run-everywhere problem for years – means there is a large set of packaged software that “just works” on many different types of CPUs.&lt;/p&gt;

&lt;blockquote readability=&quot;5.5&quot;&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;strong&gt;Purism’s PureOS showcasing adaptive convergent design in a Web Browser — notice as the window subtly resizes the buttons in the application’s header shift to the footer, for a mobile friendly interface.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The above example is already built into the &lt;a href=&quot;https://gitlab.gnome.org/GNOME/epiphany/blob/master/NEWS#L74&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;master branch&lt;/a&gt; of &lt;a href=&quot;https://wiki.gnome.org/Apps/Web/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GNOME Web&lt;/a&gt; as a class-based modification to the existing code, allowing it to easily adjust and adapt to the screen size and inputs of both mobile and desktop.&lt;/p&gt;
&lt;h2&gt;Adaptive Design&lt;/h2&gt;
&lt;p&gt;Multiple architectures are not enough to reach convergence however – as most people know by now, there are many important parts in getting true convergence. A good example of the problem in website design space: if you’ve ever gone to a website on your phone that had tiny text which scrolled off your phone screen, you know that a regular web page, designed for a desktop computer, isn’t always suitable for your smaller screen phone.&lt;/p&gt;
&lt;p&gt;Web designers now have toolboxes to design web pages, which they adjust for mobile or desktop in order to get easier readability and use. A similar, but far more complex practice, is required for software apps and defined by Purism as “adaptive design”. Purism is hard at work on creating adaptive &lt;a href=&quot;https://www.gnome.org/&quot;&gt;GNOME&lt;/a&gt; apps – and the community is joining this effort as well – apps that look great, and work great, both on a phone and on a laptop. Combining the work of the free software ecosystem with Purism’s contributions means we can target convergence for all our Librem hardware line: both the 13″ and 15″ laptops and the 5″ phone. This means we can get the most out of the ecosystem for the community: convergent apps will be easier to maintain, and therefore easier to secure. They will also be easier to build, enabling a vibrant community to build cool stuff that is free software and protects your privacy.&lt;/p&gt;

&lt;blockquote readability=&quot;8&quot;&gt;
&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p&gt;&lt;strong&gt;Purism’s PureOS showcasing adaptive convergent development in Discussions (Fractal) – a matrix chat program. As the window resizes, the column width dynamically changes to preserve a legible line width, until the sidebar and message view don’t fit at the same time. At this point the leaflet folds, only the message view is visible, and a back button is added to the header-bar, to allow navigation to the room list.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Building Convergence Features into Existing Apps&lt;/h2&gt;
&lt;p&gt;Developers can tap into convergence through the tools we actively use, contribute to, and develop directly in the ecosystem. We’ve created libhandy, a mobile and adaptive presentation library for GTK+ and GNOME, which is under active development. Packaged in PureOS and Debian already, you can also use it in flatpaks, simply by including it in your flatpak maniphest in Builder.&lt;/p&gt;

&lt;blockquote readability=&quot;5&quot;&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;strong&gt;Purism’s PureOS showcasing adaptive convergent development in Password Safe, an encrypted password storing application.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are excited to provide convergence well before any of the other mobile OS vendors. Let’s see how long it takes for them to catch-up. Thank you for your continued support!&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 14:09:44 +0000</pubDate>
<dc:creator>iBelieve</dc:creator>
<og:type>article</og:type>
<og:title>Converging on Convergence PureOS is Convergent, Welcome to the Future – Purism</og:title>
<og:description>Many Devices, One OS The two big mobile OS vendors have been dreaming of convergence between laptop OS and mobile OS for a long time; dreaming of being able to make the same application code execute, and operate, both on mobile phones and laptops – adapting the applications to screen size and input devices. Purism …</og:description>
<og:url>https://puri.sm/posts/converging-on-convergence-pureos-is-convergent-welcome-to-the-future/</og:url>
<og:image>https://puri.sm/wp-content/uploads/2019/02/aerial-photography-aerial-shot-aerial-view-1629803.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://puri.sm/posts/converging-on-convergence-pureos-is-convergent-welcome-to-the-future/</dc:identifier>
</item>
<item>
<title>The rise of wgpu</title>
<link>https://gfx-rs.github.io/2019/03/06/wgpu.html</link>
<guid isPermaLink="true" >https://gfx-rs.github.io/2019/03/06/wgpu.html</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;a href=&quot;https://github.com/gfx-rs/gfx&quot;&gt;gfx-rs&lt;/a&gt; is a Rust project aiming to make graphics programming more accessible and portable, focusing on exposing a universal Vulkan-like API targeting all platforms. Over the past 2 years we’ve put a lot of effort into making gfx-rs API compatible with Vulkan and wrapping it in the &lt;a href=&quot;https://github.com/gfx-rs/portability&quot;&gt;Vulkan Portability bindings&lt;/a&gt;. We optimized the Metal backend, nailed down hundreds of issues revealed by &lt;a href=&quot;https://github.com/KhronosGroup/VK-GL-CTS&quot;&gt;Vulkan Conformance Test Suite&lt;/a&gt;, contributed our expertize to the &lt;a href=&quot;https://github.com/KhronosGroup/Vulkan-Portability&quot;&gt;standardization process&lt;/a&gt;. What didn’t go entirely smoothly was the integration into Rust ecosystem: building libraries and applications on top of gfx’s hardware abstraction layer (hal) proved to be extremely challenging due to the low-levelness and unsafety of the API. There is a serious demand for an API that has the capabilities of Vulkan but is easier to work with.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The main community driving &lt;code class=&quot;highlighter-rouge&quot;&gt;gfx-hal&lt;/code&gt; adoption today is the Amethyst game engine, and they produced a nice helper library called &lt;a href=&quot;https://github.com/omni-viral/rendy&quot;&gt;Rendy&lt;/a&gt;. As they call it, a “collection of crates to build your own renderer”. The concept is reminiscent of LLVM (“build your own compiler”) and overall fits nicely into the Amethyst philosophy. The project has recently &lt;a href=&quot;https://community.amethyst-engine.org/t/rendy-is-released/459&quot;&gt;been released&lt;/a&gt;, and is worth checking out despite being experimental. Amethyst hasn’t fully switched to gfx-hal yet, Rendy is version 0.1.1, and gfx’s mileage may vary per backend. We’re having a lot of fun with it already, but that’s not all.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;webgpu&quot;&gt;&lt;span&gt;WebGPU&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;https://www.w3.org/community/gpu/&quot;&gt;WebGPU&lt;/a&gt; is a new graphics/compute API developed by the browser vendors (and Intel) within W3C:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;The goal is to design a new Web API that exposes these modern technologies in a performant, powerful and safe manner.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;Don’t be confused by the “Web” part here - both us (gfx-rs team) and Google are trying to make it feasible to run on native platforms as well. Typically, the Web as a platform has different priorities from native: a lot of focus is placed on the security and portability (in a wider and stronger sense). Coincidentally, these are qualities we are currently missing in the gfx-rs ecosystem: security means safety (in the Rust sense), and portability means that people can use it and run everywhere, without worrying about thousands of potential configurations at run-time, or diverging behavior between platforms due to timing differences or loosely defined behavior.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With these goals in mind, we’d like to announce our new project: &lt;a href=&quot;https://github.com/gfx-rs/wgpu&quot;&gt;wgpu-rs&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;wgpu-rs&quot;&gt;&lt;span&gt;wgpu-rs&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;wgpu-rs is aiming to be a safe, portable API which reduces the complexity of working with low-level graphics APIs. It has a layered implementation:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-native&lt;/code&gt; layer which exposes a C API. &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-native&lt;/code&gt; is implemented on top of gfx-hal which allows it to work with the same platforms that gfx-hal already supports. The idea is that &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-native&lt;/code&gt; will have a compatible C API with Google’s &lt;a href=&quot;https://dawn.googlesource.com/dawn&quot;&gt;Dawn&lt;/a&gt; implementation so they can be used interchangeably for applications compiled in other languages.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-rs&lt;/code&gt; layer which is an idiomatic safe Rust API. It uses &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-native&lt;/code&gt; internally on native targets. It will use Web APIs (from WASM host bindings) directly when targeting the Web in the future.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;Let’s dive into the mentioned qualities of the API:&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;simple&quot;&gt;&lt;span&gt;Simple&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;Writing straight Vulkan code for an application can easily challenge a user’s sanity. It’s a great API, it’s well documented, but still very complex, and it’s made with a focus on engines as opposed to direct users. You have to think about the memory allocations, pipeline stages, resource states and lifetimes, all the time. Just rendering a textured rectangle out there without triggering any validation warnings is an achievement.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;WebGPU automates some of the aspects of low-level graphics APIs which have high complexity but low return on investment. It still has the core pieces of the next-gen APIs, such as command buffers, render passes, pipeline states and layouts. Because the complexity is reduced, users will be able to direct more focus towards writing efficiently application code.&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;safe&quot;&gt;&lt;span&gt;Safe&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;WebGPU is designed from the ground up to have all the validation built-in. In our case, &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-native&lt;/code&gt; is responsible for sanitizing the inputs, capturing invalid usages, and reporting back to the user. This means that our &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-rs&lt;/code&gt; wrapper can be entirely safe in the Rust sense. We don’t even rely on Rust syntax and features for safety, since it’s handled at the lower level, so we can have as much type-safety as we want to feel comfortable, or as little as needed to be flexible.&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;portable&quot;&gt;&lt;span&gt;Portable&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;The code runs on a variety of platforms from a single source: Vulkan, Metal, D3D12/D3D11, and eventually the Web (when the browsers gain support for the API, which is also in our scope of work). And it’s expected to behave exactly the same, independent of the exposed hardware queues, image format capabilities, or memory types.&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;picking-up-the-flag&quot;&gt;&lt;span&gt;Picking up the flag&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;In a way, WebGPU is trying to accomplish all the goals we set for ourselves originally at the start of gfx-rs project. And that’s why we are so enthusiastic about it: it’s more modern, it’s more safe, it’s more portable, and there is a working group behind it with strong momentum which will eventually become widespread. Today we are releasing &lt;a href=&quot;https://crates.io/crates/wgpu/0.2.0&quot;&gt;wgpu-0.2&lt;/a&gt; for the community to try out. It’s still largely incomplete (e.g. no validation or error reporting yet), but it reached a point of being usable:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://gfx-rs.github.io/img/wgpu-shadow.png&quot; alt=&quot;WGPU shadow&quot;/&gt;&lt;img src=&quot;https://gfx-rs.github.io/img/wgpu-asteroid-giger.png&quot; alt=&quot;WGPU asteroid&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-rs&lt;/code&gt; is a heavy work in progress, has only been developed since September last year, and as such some assembly is still required. However, those who don’t mind some tinkering, will find a promising library with a bright future and a compelling compatibility proposition already. We are looking forward to see what the community is able to accomplish with it, and I myself (@kvark) have a number of projects using gfx pre-ll that have been waiting for this moment to upgrade. With &lt;code class=&quot;highlighter-rouge&quot;&gt;wgpu-rs&lt;/code&gt; we can finally deprecate the old gfx and have a solid recommendation for people getting started with graphics in Rust.&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 10:44:26 +0000</pubDate>
<dc:creator>jobstijl</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://gfx-rs.github.io/2019/03/06/wgpu.html</dc:identifier>
</item>
<item>
<title>ARM processors like A12X are nearing performance parity with desktop processors</title>
<link>https://reveried.com/article/arm-processors-nearing-performance-parity-with-x86</link>
<guid isPermaLink="true" >https://reveried.com/article/arm-processors-nearing-performance-parity-with-x86</guid>
<description>&lt;p&gt;Anandtech released their review of the iPhone XS a few months ago and despite a vague commitment to Android, my curiosity got the better of me: I read the whole thing. Most of the review didn't surprise, it had a typically great screen, battery life was solid and GPU performance had improved significantly. But several pages of it were dedicated to analysing and testing the micro-architecture of Apple’s new A12 chip, and these sections stood out in a big way. Reading the final sentence, I was somewhat shocked:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;em&gt;“Of course there’s compiler considerations and various frequency concerns to take into account, but still we’re now talking about very small margins until Apple’s mobile SoCs outperform the fastest desktop CPUs in terms of ST [single thread] performance.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Surely that’s not possible, I figured. Desktop processors were just faster. They always have been. Surely my enormous gaming setup, with an i7 4790k processor, 5 fans and far too much desk space could not be bested in peak CPU performance by a smartphone. Bullshit. So I ignored it and moved on - nothing to see here.&lt;/p&gt;
&lt;h2&gt;The 2018 iPad Pro.&lt;/h2&gt;
&lt;p&gt;A few months after this dismissal the iPad Pro 2018 was announced, and an interesting headline immediately hit the press: &lt;em&gt;“Apple's A12X reaches desktop CPU performance in benchmarks”&lt;/em&gt;. I saw posts with titles to this effect in various places - on tech websites, reddit, forums, in discussion threads, comment sections and more - and noticed a common reaction: many saw that the benchmark in question was Geekbench or something similar, and they rejected the central claim outright. The popular consensus was very clear: if you were to truly compare the performance in an objective fashion, a high-end Intel core i7, or even an i3, would obliterate the A12X.&lt;/p&gt;
&lt;p&gt;I wasn’t satisfied with this reaction for a bunch of reasons, even though it was precisely the reflexive rejection I employed after reading the iPhone XS review. Firstly, it struck me as odd that Anandtech, arguably the most thorough technology reviewers in tech journalism, would make such a bold claim about the A12 if it could be easily disproved. Secondly, as I looked through many iPad Pro reviews, I was consistently shocked at just how well it performed, not just in terms of fluidity, but &lt;a href=&quot;https://www.youtube.com/watch?v=QiPRnPXOvjc&quot;&gt;workload tasks&lt;/a&gt;. This was completely against what I understood to be true: that modern x86 architectures were inherently and far better than ARM-based chips, that ARM chips only had an efficiency advantage.&lt;/p&gt;
&lt;p&gt;But after a fair amount of research and deliberation, I came to a very different conclusion. There’s nuance in every corner, but after working through common arguments and misconceptions, such as that the RISC (ARM) instruction set architecture is inherently inferior to CISC (x86), that the benchmarks spruiking excellent ARM performance are inaccurate, that the performance gap is explained by different operating systems, and more, I’ve found that the central claim of these articles largely holds true: ARM processors, particularly those produced by Apple, have caught up to high-end x86 processors in most perspectives of performance. This article is the result of a deep-dive into the topic, intended somewhat to end the myth that ARM processors are behind in terms of performance. Be ready for a good, long read.&lt;/p&gt;
&lt;h2&gt;RISC v CISC&lt;/h2&gt;
&lt;p&gt;The first response I saw many employ against ARM parity was that x86 chips are based on the CISC Instruction Set Architecture (ISA) and are therefore inherently superior to mobile chips based on the RISC ISA. This argument centres around a postulation that complex desktop programs perform worse on ARM chips because they are RISC-based. Many discussions and upvotes seemed to end on this point.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/&quot;&gt;&lt;u&gt;CISC&lt;/u&gt;&lt;/a&gt; is an ISA which executes complex commands in as few lines as possible so to minimise reliance on cache and memory, while &lt;a href=&quot;https://cs.stanford.edu/people/eroberts/courses/soco/projects/risc/risccisc/&quot;&gt;&lt;u&gt;RISC&lt;/u&gt;&lt;/a&gt; organises commands into multiple simple instructions that can be each executed in a single CPU cycle. Here’s a &lt;a href=&quot;https://medium.com/@csoham358/a-beginners-guide-to-risc-and-cisc-architectures-fc9af424db3b&quot;&gt;&lt;u&gt;fantastic article&lt;/u&gt;&lt;/a&gt; breaking down covering the differences between the two. While they have different advantages, CISC was first prioritised not necessarily because it was inherently better, but because memory was expensive at the time and because RISC processors required a bit of software wizardry to get right (Stanford UNI). Once Windows cemented support for CISC, specifically x86, processors only, the industry and investment interest focused on CISC, and faster and faster x86 processors were demanded. A notable exception existed in PowerPC and the Mac, but soon enough Apple joined the CISC bandwagon in their Intel transition.&lt;/p&gt;
&lt;p&gt;In the meantime, ARM Holdings (keeping in mind that ARM stands for Advanced RISC Machine) started to develop chips for mobile devices where efficiency, not raw performance, was the initial focus. Alongside a competitive environment between Intel and AMD and a lack of demand for RISC chips in desktop systems, this created a vague industry momentum - RISC for mobile and CISC for desktop. At some point in time, this momentum turned into a popular assumption that the &lt;em&gt;reason&lt;/em&gt; the performance gap existed was the difference in ISA. Accordingly, RISC was meant to be inferior and naturally relegated to slow but efficient chips, while high-end Intel and AMD chips would forever reign supreme on the performance front. But this is a myth, and it hasn't borne out - in so many ways the lines of RISC and CISC have blurred, borrowing design ideas over decades of manufacturing. RISC stands for Reduced Instruction Set Chip, and yet most modern RISC processors have large, complex instruction sets that rival their &lt;strong&gt;C&lt;/strong&gt;omplex &lt;strong&gt;I&lt;/strong&gt;nstruction &lt;strong&gt;S&lt;/strong&gt;et (&lt;strong&gt;C&lt;/strong&gt;hip) counterparts. Today, micro-architecture, which is how an ISA is actually implemented, is far more important.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://research.cs.wisc.edu/vertical/papers/2013/hpca13-isa-power-struggles.pdf&quot;&gt;A study by the University of Wisconsin backs this up,&lt;/a&gt; stating that the performance difference between RISC and CISC is not due to their ISA, but the micro-architecture differences between products in the market, &lt;strong&gt;suggesting that either ISA has the potential to perform as well as one another.&lt;/strong&gt; In any case, 5 years ago x86 CISC chips had the developmental and micro-architectural advantage, so the ISA argument was hard to disprove.&lt;/p&gt;
&lt;p&gt;But some combination of the portable device boom, demand for fast and efficient server processors and a general trend towards more efficient devices created a firestorm of investment and R&amp;amp;D that saw ARM processors begin to improve radically. In the last decade Intel and AMD CPUs have made small overall performance improvements year on year, while ARM chips have made sizable leaps each generation. Granted, 10 years ago commercial ARM chips were absolutely obliterated by any half-decent desktop Intel or AMD processor, so these leaps were absolutely necessary for any chance at actually competing, but the changes have been so quick that the old truths about ARM inferiority haven’t had time to leave the computing vernacular.&lt;/p&gt;
&lt;p&gt;All this said, the original claim about against RISC still has an element of truth to it - Windows doesn’t fully support ARM or RISC based processors. When these chips have been used on Windows devices, such as in the case of the HP Envy X2, a 32-bit x86 emulator is employed to run x86 programs, and they perform terribly. To be clear, this doesn't indicate that ARM chips are slower, just that they aren’t natively supported by desktop OSes like Windows and MacOS. The key takeaway is that there’s no ISA battle anymore, the performance debate between RISC and CISC is moot, and gains are to be found in microarchitecture, not instruction set architecture. Improvements to cache implementation, instruction set width, chip size, branch prediction, and other architectural improvements are where the biggest IPC gains exist.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;“Benchmarks are imperfect, therefore useless.”&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The next issue I want to address are fallacies I've seen permeate discussions around ARM performance: that a benchmark is not at all useful because it is flawed or not objective in some way. I understand the reaction to a degree (I once wrote an article criticising camera benchmarks that reduce complex data into single scalar numbers), but I also believe that it’s possible to understand the shortcomings of a benchmark, not to approach it objectively, and to properly consider what it might indicate, rather than dismissing it out of hand.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Differences in Operating System&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;There's a popular but somewhat misguided belief that cross-platform benchmarks are not at all relevant because different operating systems like iOS and Windows skew the results too significantly. You've seen it before: &lt;em&gt;&quot;Yeah, it performs that well on iOS, but on a Windows machine an i5 would destroy it&quot;.&lt;/em&gt; On the surface this doesn't sound like a bad argument, since it's true in many instances, especially those benchmarks that use high-level OS APIs. Metal for example is a highly optimised low-level graphics API for iOS/MacOS only, so comparing its performance on an iPhone against OpenCL performance on a Windows machine would produce highly misleading results. But unless there's a bug or performance issue with the kernel or firmware with a tested device, the kind of compute tasks most cross-platform CPU benchmarks test are designed so that they’re not particularly bound to the platforms on which they run.&lt;/p&gt;
&lt;p&gt;To explain with a hypothetical: let’s say, using a native program, I ask an iPad Pro 2018 to calculate ten million digits of pi (a rudimentary way to measure one very particular facet of performance) and it takes 60 seconds, compared to a Windows desktop machine 90 seconds. In this scenario one of either two things are roughly true: the hardware in the iPad Pro is genuinely 33% faster at completing this task, or there's an inefficiency, bug or poor optimisation somewhere in the setup of that Windows device that is limiting its true performance in this task. It's the first scenario that is most often true. Again, this doesn’t mean the takeaway is that the iPad is 33% faster, but it does typically indicate some advantage in that particular facet of performance.&lt;/p&gt;
&lt;p&gt;There are examples, however, where the difference between OSes are much more significant: browser tests and JS performance. In so many of these cases, the A12X excels. NotebookCheck’s review of the iPad Pro shows it performing 16% - 50% better than the brand new Surface Pro i7 model in browser benchmarks. If we compare to a desktop processor by cross-referencing the data with &lt;a href=&quot;https://www.anandtech.com/show/11859/the-anandtech-coffee-lake-review-8700k-and-8400-initial-numbers/9&quot;&gt;Anandtech’s review of the i7-8700k&lt;/a&gt;, we can see that the iPad Pro either wins or is on par in all tested browser benchmarks, only falling behind by a decent margin in WebXPRT 15 (keeping in mind the desktop setup is using a GTX 1080).&lt;/p&gt;
&lt;p&gt;You’d be completely correct if you're thinking that there’s a decent room for error in these kinds of tests - they run on different browsers and operating systems after all, and one setup could be significantly more optimised than the other, but optimisation isn’t some magic bullet pulling performance out of thin-air - &lt;strong&gt;optimisation is something that is done to make the most out of the available hardware.&lt;/strong&gt; It's fair to criticise a performance benchmark when the device or software in question is clearly not correctly optimised or compiled, but it's somewhat silly to ignore a benchmark because a device is &lt;em&gt;too&lt;/em&gt; well optimised, as so often happens in response to positive ARM benchmarks. The fact that the browser performance of a device using the A12X is trading blows with a high-end desktop processor is impressive from whichever angle you look at it. Even in the worst case scenario - on the negative end of the high margin of error that browser benchmarks have - these tests indicate clearly that an ARM-powered chip is within punching distance of high-end x86 chips in browser performance, and that’s all that needs to be true to support the central claim.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Benchmarks don't represent actual workloads.&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Beyond the cross platform point, there's a tendency for some to disregard benchmarks that show ARM as competitive because they believe that either they don't accurately represent CPU workloads, that the compute tasks they test aren't relevant to actual performance, or because they don't actually indicate end-user experience. These are also sometimes true – after all, the final score in a benchmark like Geekbench is estimated and derived from various compute tasks and the usefulness, weighting and relevance of those tests are all subjective. Linus Torvalds famously ripped into Geekbench 3 for its ridiculous testing of crypto tasks, but this and various other issues were addressed in Geekbench 4, so the benchmark today isn't actually a bad indicator of generalised peak performance. It does not necessarily indicate sustained performance, but it does give a &lt;strong&gt;roughly,&lt;/strong&gt; &lt;em&gt;&lt;strong&gt;kinda&lt;/strong&gt;&lt;/em&gt; &lt;strong&gt;accurate&lt;/strong&gt; standardised way to compare peak potential. Once again, there can be an appreciable margin of error in these benchmarks, but it's not so significant to render them useless, able to be totally dismissed.&lt;/p&gt;
&lt;p&gt;In other words: while anyone using Geekbench as a be-all-and-end-all measure of performance is, put plainly, using it wrong, the score still has relevance. Geekbench is a thermometer that’s a little broken - sure, it might be off by a few fahrenheit most of the time, but if the temperature is reporting double what it was yesterday, you can bet it's gonna feel almost double as hot.  In my opinion, those who completely, utterly dismiss Geekbench as evidence are engaging in logic just as reductive as those who interpret it with objective authority - the truth lies somewhere in the middle, as it often does. All this said, I would not want to rest my ARM performance case on Geekbench alone, so let’s explore deeper.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;SPEC2006 it is.&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For the sake of seeking more objective and thorough data, let’s look at an industry standard benchmark, SPEC2006, an encompassing tool that according to the excellent writers at Anandtech, &lt;em&gt;“is much better [than Geekbench] as a representative benchmark that fully exhibits more details of a given microarchitecture”.&lt;/em&gt; SPEC2006 measures genuine real world tasks - compression, image recognition, compiling, spam filtering, game/chess AI, pathfinding and XML processing. The numbers aren’t arbitrary.&lt;/p&gt;
&lt;p&gt;I've picked the i7 6700k to compare the A12X with in this scenario. It’s the most recent and fastest i7 in the official SPEC2006 database, so in honesty, it's the only option. But since SPEC2006 is a single-core benchmark, core count changes in recent generations aren’t that relevant, and Intel haven’t improved IPC in a few generations, the results will still be quite relevant compared to using the 8700k. The iPad Pro results have been pulled from &lt;a href=&quot;https://www.anandtech.com/show/13661/the-2018-apple-ipad-pro-11-inch-review/4&quot;&gt;Anandtech’s review of the iPad Pro&lt;/a&gt;. The i7 results have been pulled directly from the &lt;a href=&quot;https://www.spec.org/cpu2006/results/res2015q4/cpu2006-20151019-37700.pdf&quot;&gt;SPEC2006 website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Compared to the top-of-the-line consumer desktop CPU from a few years ago, the A12X performs 20% worse on average in the various SPECint2006 benchmarks, a figure which almost sounds like I’ve produced evidence that doesn’t support my argument. But it achieves this result while being clocked 40% lower, while using a fraction of the power, and with absolutely zero active cooling. No one pound heatsink and fan or case fans – the processor sits inside a 5.9mm slate of silicon, metal and glass. It doesn’t matter how you slice or stretch the numbers – they are wildly impressive. The A12X is definitely winning on the IPC front, and absolutely obliterating on the performance per watt front. If you normalise for clock speed, the A12X would have performed &lt;strong&gt;significantly&lt;/strong&gt; better than the i7 overall.&lt;/p&gt;
&lt;p&gt;At first I found some of these numbers hard to believe, but they're hard to argue with. The A12X has reached parity on Geekbench scores, it’s approaching parity on a host of assorted system benchmarks, it’s approaching parity on SPEC2006, it’s at parity in just about every browser benchmark, and every indication shows that it has better IPC and better performance per watt than current high-end Intel and AMD processors. Even if Anandtech are over-exaggerating, if all these benchmarks are ARM-favoured and every supporting figure has a huge margin of error, the results are still absolutely remarkable. We’re talking about processors that are designed to live in your pocket with no cooling, not one designed to go toe-to-toe with desktops, but they're doing it anyway.&lt;/p&gt;
&lt;h5&gt;Aha! What about sustained performance?&lt;/h5&gt;
&lt;p&gt;It’s completely true that any decent Intel or AMD processor would defeat the A12X in intensive real-world tasks that last longer than a few minutes, like rendering a video or running a PC game (in theory), but this isn't really a rebuttal to the theme of this article, for one significant reason. Before I explain, let me clarify: the purpose of this article is not to suggest that one could take the A12X from an iPad Pro, put it in a desktop as-is and use it without any issues today, but rather to end the myth that ARM chips are behind performance-wise. With this in mind, the reason that the lack of sustained performance is not really an issue is simply because ARM processors have an absolutely enormous disadvantage that has nothing to do with microarchitecture or ISA: in practically every consumer device they are used in, they have no active cooling solutions.&lt;/p&gt;
&lt;p&gt;ARM chips on the market already run cooler and more efficiently than modern x86 chips, so a cooling unit as good as any generic desktop fan would not only solve the issue of sustained performance and throttling, but also likely increase the headroom for boost clocks, assuming assorted 7nm nodes are mainly limited by heat to reach higher clock speeds. With an IPC lead already, this change would allow good ARM chips like the A12X to properly compare in almost all performance scenarios.&lt;/p&gt;
&lt;p&gt;The fact that they're performing so well despite an enormous disadvantage is impressive in itself, but if Apple were to custom create an ARM chip designed to be used in a high-end desktop or laptop system with active cooling, they would likely produce the fastest consumer chip on the market. Apple modify each generation of their chip architecture and optimise it for each device category – the A12X for the iPad Pro being an enhanced tablet version of the A12 for the iPhone XS – any ARM CPU that Apple might stick in a laptop would be accordingly modified and enhanced – more Big cores, some tweaks that cater to desktop use-cases, higher base and boost clocks etc. To say we haven't seen the full potential of ARM yet would be an understatement.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;So how did this shift happen?&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;In a broad sense, ARM chips have caught up because of a significant demand in the market for fast, efficient chips in portable devices, and now servers. Because of this, ARM reference designs got better and better, and Apple began to double down in a big way. While Apple aren't the only company developing extremely fast chips, with the upcoming Qualcomm Snapdragon 8CX expected to perform rather well, their custom fabrications are particularly... insane? The A12X has more execution ports than any other consumer processor Anandtech have seen (in a desktop, laptop, phone, tablet or otherwise):&lt;/p&gt;
&lt;blockquote readability=&quot;15&quot;&gt;
&lt;p&gt;&lt;em&gt;Monsoon (A11) and Vortex (A12) are extremely wide machines – with 6 integer execution pipelines among which two are complex units, two load/store units, two branch ports, and three FP/vector pipelines this gives an estimated 13 execution ports, far wider than ARM's upcoming Cortex A76 and also wider than Samsung’s M3. In fact, assuming we're not looking at an atypical shared port situation, Apple’s microarchitecture seems to far surpass anything else in terms of width, including desktop CPUs.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This brings us back to the earlier point in the RISC v CISC discussion: performance is coming down to micro-architecture, not RISC or CISC, not ARM or x86. With this in mind, ARM's desirability is only likely to further improve from here. Due in part to a stagnant desktop CPU industry, with Intel not facing significant competition from AMD up until just recently, x86 development has been, plainly put, dull. Performance and feature improvements have been minor besides core count increases. The opposite has been true in the mobile and tablet industry, and there's no sign of any slowing down.&lt;/p&gt;
&lt;p&gt;Even if Intel manages to drastically improve IPC and efficiency in the shift to 7nm, they would be still be counting on ARM not making further strides ahead given the sheer levels of research, development and demand being funnelled from the smartphone, server, tablet, and soon laptop industry.&lt;/p&gt;
&lt;h3&gt;If all this is true, why is x86 still king?&lt;/h3&gt;
&lt;p&gt;Why has nothing changed? Why is x86 relegated to desktop, and why aren’t people talking about this shift?&lt;/p&gt;
&lt;p&gt;One answer to these questions is simple: only in the last 3 years has ARM started to become truly competitive, and &lt;strong&gt;it takes far longer than 5 years to trigger widespread adoption given just how entrenched Windows and x86 software is&lt;/strong&gt;. While integrated mobile and tablet chips are currently excellent, to create an similar desktop chip with modular parts, a lot of work still has to be done.&lt;/p&gt;
&lt;p&gt;The other answer is more complex: things &lt;em&gt;are&lt;/em&gt; changing, behind the scenes. There’s already evidence of Apple’s shift to ARM - though it generated very little press coverage or discussion, the &lt;strong&gt;latest Macbook Pros are using Apple’s proprietary ARM-based T2 chip, built on their A11 iPhone chip, to handle H.265 video decoding&lt;/strong&gt;. I imagine that each generation of Macbook processors will offload more and more tasks to the ARM co-processor, until eventually, some generation completes the transition, and the co-processor becomes the processor.&lt;/p&gt;
&lt;p&gt;And this is great news for all of us consumers: &lt;strong&gt;What better way to light a fire under the desktop and tablet CPU industry than the threat of the most lucrative hardware company in the world shifting to a competing architecture.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One day soon, we might be able to buy Macbooks with near complete vertical integration from Apple, and despite the possibility of disruption and compatibility issues, I imagine these devices might have some pretty great advantages: industry leading performance and great battery life. For now, let's wait to see what the A13X looks like - if the last few years indicate anything, we're in for a chip that could foreshadow the biggest change in the dynamic of the processor industry in a long, long time.&lt;/p&gt;
&lt;h6&gt;This was my first new article in years, so please let me know what you thought!&lt;/h6&gt;
</description>
<pubDate>Thu, 07 Mar 2019 04:42:03 +0000</pubDate>
<dc:creator>kristianp</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://reveried.com/article/arm-processors-nearing-performance-parity-with-x86</dc:identifier>
</item>
<item>
<title>Tesla – Introducing V3 Supercharging</title>
<link>https://www.tesla.com/blog/introducing-v3-supercharging</link>
<guid isPermaLink="true" >https://www.tesla.com/blog/introducing-v3-supercharging</guid>
<description>&lt;p&gt;Tesla has more than 12,000 Superchargers across North America, Europe, and Asia and our network continues to grow daily: more than 99% of the U.S. population is covered by the network, and we anticipate similar coverage in Europe by the end of 2019. Recently, we passed 90% population coverage in China and are growing that number quickly. However, in order to drive continued electric vehicle adoption and further accelerate the world’s transition to sustainable energy, charging needs to be even faster, and the number of vehicles able to charge at a location in a day needs to be significantly higher. Today, we’re unveiling V3 Supercharging, the next step in the growth of Tesla’s Supercharger network. V3, which is born from our experience building the &lt;a href=&quot;https://www.tesla.com/blog/tesla-powerpack-enable-large-scale-sustainable-energy-south-australia&quot;&gt;world’s largest grid-connected batteries&lt;/a&gt;, enables our vehicles to charge faster than any other electric vehicle on the market today.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Faster Charging, No More Power Sharing&lt;/strong&gt;&lt;br/&gt;V3 is a completely new architecture for Supercharging. A new 1MW power cabinet with a similar design to our &lt;a href=&quot;https://www.tesla.com/powerpack&quot;&gt;utility-scale products&lt;/a&gt; supports peak rates of up to 250kW per car. At this rate, a Model 3 Long Range operating at peak efficiency can recover up to 75 miles of charge in 5 minutes and charge at rates of up to 1,000 miles per hour. Combined with other improvements we’re announcing today, V3 Supercharging will ultimately cut the amount of time customers spend charging by an average of 50%, as modeled on our fleet data.&lt;/p&gt;
&lt;iframe src=&quot;https://player.vimeo.com/video/321899093&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;
&lt;p&gt;Supercharger stations with V3’s new power electronics are designed to enable any owner to charge at the full power their battery can take – no more splitting power with a vehicle in the stall next to you. With these significant technical improvements, we anticipate the typical charging time at a V3 Supercharger will drop to around 15 minutes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On-Route Battery Warmup&lt;/strong&gt;&lt;br/&gt;New Supercharging infrastructure isn’t the only way we are improving our customers’ charging experience. Beginning this week, Tesla is rolling out a new feature called On-Route Battery Warmup. Now, whenever you navigate to a Supercharger station, your vehicle will intelligently heat the battery to ensure you arrive at the optimal temperature to charge, reducing average charge times for owners by 25%.&lt;/p&gt;
&lt;p&gt;This combination of higher peak power with V3, dedicated vehicle power allocation across Supercharger sites, and On-Route Battery Warmup enables customers to charge in half the time and Tesla to serve more than twice the number of customers per hour. Additionally, we are also unlocking 145kW charge rates for our 12,000+ V2 Superchargers over the coming weeks.&lt;/p&gt;
&lt;img src=&quot;https://www.tesla.com/sites/default/files/images/blogs/average-time-charging.jpg&quot;/&gt;&lt;p&gt;With Model 3 now shipping globally in high volumes and Model Y on the way, V3 Supercharging enables us to deliver the fastest production charging experience at an unprecedented scale compared to other electric vehicle manufacturers. By increasing the number of vehicles we’re able to charge at each Supercharger in a day, the investment we’re making in our network will go significantly further with every V3 station deployed. Paired with other savings, these efficiencies will translate to an increased pace of investment for Superchargers moving forward, with a continued focus on getting to 100% ownership coverage across all regions we operate. With thousands of new Superchargers coming online in 2019, the launch of V3, and other changes we’re making to improve throughput, the Supercharger network will be able to serve more than 2x more vehicles per day at the end of 2019 compared with today – easily keeping pace with our 2019 fleet growth.&lt;/p&gt;
&lt;p&gt;Beginning today, we’re opening the first public beta site in the Bay Area, which will incrementally be made available to owners in Tesla’s Early Access Program. We’re launching V3 Supercharging for Model 3, our highest volume vehicle, and we’ll continue to expand access as we review and assess the results of millions of charging events. We will increase Model S and X charging speeds via software updates in the coming months. V3 Supercharging will roll out to the wider fleet in an over the air firmware update to all owners in Q2 as more V3 Superchargers come online. Our first non-beta V3 Supercharger site will break ground next month, with North American sites ramping in Q2 and Q3 before coming to Europe and Asia-Pacific in Q4.&lt;/p&gt;
</description>
<pubDate>Thu, 07 Mar 2019 04:06:04 +0000</pubDate>
<dc:creator>arturogarrido</dc:creator>
<og:title>Introducing V3 Supercharging</og:title>
<og:description>Tesla has more than 12,000 Superchargers across North America, Europe, and Asia and our network continues to grow daily: more than 99% of the U.S. population is covered by the network, and we anticipate similar coverage in Europe by the end of 2019. Recently, we passed 90% population coverage in China and are growing that number quickly. However, in order to drive continued electric vehicle adoption and further accelerate the world’s transition to sustainable energy, charging needs to be even faster, and the number of vehicles able to charge at a location in a day needs to be significantly higher. Today, we’re unveiling V3 Supercharging, the next step in the growth of Tesla’s Supercharger network. V3, which is born from our experience building the</og:description>
<og:image>https://www.tesla.com/sites/default/files/blog_images/tesla_announcement_social.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.tesla.com/blog/introducing-v3-supercharging</dc:identifier>
</item>
</channel>
</rss>