<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Autopilot on Cars for $999</title>
<link>http://comma.ai</link>
<guid isPermaLink="true" >http://comma.ai</guid>
<description>&lt;div readability=&quot;10&quot;&gt;
&lt;h3 class=&quot;xu-mt-0&quot;&gt;openpilot&lt;/h3&gt;
&lt;p class=&quot;xu-mt-0&quot;&gt;openpilot is open source software built to improve upon the existing driver assistance in most new cars on the road today. Tesla Autopilot like functionality for your Toyota, Honda, and more.&lt;/p&gt;
&lt;p&gt;&lt;button class=&quot;x-button x-button--medium x-button--transparent x-button--bright-blue x-button--arrow&quot;&gt;&lt;span class=&quot;xt-color--blue&quot;&gt;See openpilot in action&lt;/span&gt;&lt;/button&gt;&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;12.428571428571&quot;&gt;
&lt;h3 class=&quot;xu-mt-0&quot;&gt;Works with the push of a button.&lt;/h3&gt;
&lt;p class=&quot;xu-mt-0&quot;&gt;openpilot is simple to use. It enables your car to steer, accelerate, and brake automatically within its lane. Drive to a highway, press the cruise control SET button, and openpilot will engage. To disengage, use either pedal.&lt;/p&gt;

&lt;/div&gt;</description>
<pubDate>Mon, 15 Feb 2021 20:01:39 +0000</pubDate>
<dc:creator>cbracketdash</dc:creator>
<og:title>comma.ai – Introducing openpilot</og:title>
<og:description>Make driving chill. Retrofit your car with a comma two. comma.ai is building the Android for cars. We have an open source driving agent that runs on most modern cars.</og:description>
<og:image>http://comma.ai/meta-cover.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://comma.ai/</dc:identifier>
</item>
<item>
<title>Mining Bitcoin with pencil and paper: 0.67 hashes per day (2014)</title>
<link>http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html</link>
<guid isPermaLink="true" >http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html</guid>
<description>

I decided to see how practical it would be to mine Bitcoin with pencil and paper. It turns out that the SHA-256 algorithm used for mining is pretty simple and can in fact be done by hand. Not surprisingly, the process is extremely slow compared to hardware mining and is entirely impractical. But performing the algorithm manually is a good way to understand exactly how it works.
&lt;p&gt;&lt;a href=&quot;http://static.righto.com/images/bitcoin/mine_pencil_1-s800.png&quot;&gt;&lt;img alt=&quot;A pencil-and-paper round of SHA-256&quot; class=&quot;hilite2&quot; src=&quot;http://static.righto.com/images/bitcoin/mine_pencil_1-w500.png&quot; title=&quot;A pencil-and-paper round of SHA-256&quot; width=&quot;500&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;div class=&quot;cite&quot;&gt;A pencil-and-paper round of SHA-256&lt;/div&gt;
&lt;h2&gt;The mining process&lt;/h2&gt;
Bitcoin mining is a key part of the security of the Bitcoin system. The idea is that Bitcoin miners group a bunch of Bitcoin transactions into a block, then repeatedly perform a cryptographic operation called hashing zillions of times until someone finds a special extremely rare hash value. At this point, the block has been mined and becomes part of the Bitcoin block chain. The hashing task itself doesn't accomplish anything useful in itself, but because finding a successful block is so difficult, it ensures that no individual has the resources to take over the Bitcoin system. For more details on mining, see my &lt;a href=&quot;http://www.righto.com/2014/02/bitcoin-mining-hard-way-algorithms.html&quot;&gt;Bitcoin mining article&lt;/a&gt;.
&lt;p&gt;A cryptographic hash function takes a block of input data and creates a smaller, unpredictable output. The hash function is designed so there's no &quot;short cut&quot; to get the desired output - you just have to keep hashing blocks until you find one by brute force that works. For Bitcoin, the hash function is a function called &lt;a href=&quot;https://en.wikipedia.org/wiki/SHA-2&quot;&gt;SHA-256&lt;/a&gt;. To provide additional security, Bitcoin applies the SHA-256 function twice, a process known as double-SHA-256.&lt;/p&gt;
&lt;p&gt;In Bitcoin, a successful hash is one that starts with enough zeros.&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref1&quot;&gt;[1]&lt;/a&gt; Just as it is rare to find a phone number or license plate ending in multiple zeros, it is rare to find a hash starting with multiple zeros. But Bitcoin is exponentially harder. Currently, a successful hash must start with approximately 17 zeros, so only one out of 1.4x10&lt;sup&gt;20&lt;/sup&gt; hashes will be successful. In other words, finding a successful hash is harder than finding a particular grain of sand out of &lt;a href=&quot;http://www.npr.org/blogs/krulwich/2012/09/17/161096233/which-is-greater-the-number-of-sand-grains-on-earth-or-stars-in-the-sky&quot;&gt;all the grains of sand on Earth&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following diagram shows a &lt;a href=&quot;https://blockexplorer.com/b/286819&quot;&gt;block&lt;/a&gt; in the Bitcoin blockchain along with its hash. The yellow bytes are hashed to generate the block hash. In this case, the resulting hash starts with enough zeros so mining was successful. However, the hash will almost always be unsuccessful. In that case, the miner changes the nonce value or other block contents and tries again.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://static.righto.com/images/bitcoin/block_diagram_ghash.png&quot;&gt;&lt;img alt=&quot;Structure of a Bitcoin block&quot; class=&quot;hilite2&quot; src=&quot;http://static.righto.com/images/bitcoin/block_diagram_ghash-w550.png&quot; title=&quot;Structure of a Bitcoin block&quot; width=&quot;500&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cite&quot;&gt;Structure of a Bitcoin block&lt;/div&gt;
&lt;h2&gt;The SHA-256 hash algorithm used by Bitcoin&lt;/h2&gt;
The SHA-256 hash algorithm takes input blocks of 512 bits (i.e. 64 bytes), combines the data cryptographically, and generates a 256-bit (32 byte) output. The SHA-256 algorithm consists of a relatively simple round repeated 64 times. The diagram below shows one round, which takes eight 4-byte inputs, A through H, performs a few operations, and generates new values of A through H.
&lt;p&gt;&lt;a href=&quot;http://static.righto.com/images/bitcoin/SHA_2.svg.png&quot;&gt;&lt;img alt=&quot;SHA-256 round, from Wikipedia&quot; class=&quot;hilite2&quot; src=&quot;http://static.righto.com/images/bitcoin/SHA_2.svg-w400.png&quot; title=&quot;SHA-256 round, from Wikipedia&quot; width=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cite&quot;&gt;One round of the SHA-256 algorithm showing the 8 input blocks A-H, the processing steps, and the new blocks. &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:SHA-2.svg#mediaviewer/File:SHA-2.svg&quot;&gt;Diagram&lt;/a&gt; created by kockmeyer, &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0/&quot;&gt;CC BY-SA 3.0&lt;/a&gt;.&lt;/div&gt;
&lt;p&gt;The blue boxes mix up the values in non-linear ways that are hard to analyze cryptographically. Since the algorithm uses several different functions, discovering an attack is harder. (If you could figure out a mathematical shortcut to generate successful hashes, you could take over Bitcoin mining.)&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Ma&lt;/em&gt; majority box looks at the bits of A, B, and C. For each position, if the majority of the bits are 0, it outputs 0. Otherwise it outputs 1. That is, for each position in A, B, and C, look at the number of 1 bits. If it is zero or one, output 0. If it is two or three, output 1.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Σ0&lt;/em&gt; box rotates the bits of A to form three rotated versions, and then sums them together modulo 2. In other words, if the number of 1 bits is odd, the sum is 1; otherwise, it is 0. The three values in the sum are A rotated right by 2 bits, 13 bits, and 22 bits.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Ch&lt;/em&gt; &quot;choose&quot; box chooses output bits based on the value of input E. If a bit of E is 1, the output bit is the corresponding bit of F. If a bit of E is 0, the output bit is the corresponding bit of G. In this way, the bits of F and G are shuffled together based on the value of E.&lt;/p&gt;
&lt;p&gt;The next box &lt;em&gt;Σ1&lt;/em&gt; rotates and sums the bits of E, similar to &lt;em&gt;Σ0&lt;/em&gt; except the shifts are 6, 11, and 25 bits.&lt;/p&gt;
&lt;p&gt;The red boxes perform 32-bit addition, generating new values for A and E. The input &lt;em&gt;W&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; is based on the input data, slightly processed. (This is where the input block gets fed into the algorithm.) The input &lt;em&gt;K&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; is a constant defined for each round.&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref2&quot;&gt;[2]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As can be seen from the diagram above, only A and E are changed in a round. The other values pass through unchanged, with the old A value becoming the new B value, the old B value becoming the new C value and so forth. Although each round of SHA-256 doesn't change the data much, after 64 rounds the input data will be completely scrambled.&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref3&quot;&gt;[3]&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Manual mining&lt;/h2&gt;
The video below shows how the SHA-256 hashing steps described above can be performed with pencil and paper. I perform the first round of hashing to mine a block. Completing this round took me 16 minutes, 45 seconds.
&lt;p&gt;&lt;iframe width=&quot;480&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/y3dqhixzGVo&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;To explain what's on the paper: I've written each block A through H in hex on a separate row and put the binary value below. The &lt;em&gt;maj&lt;/em&gt; operation appears below C, and the shifts and &lt;em&gt;Σ0&lt;/em&gt; appear above row A. Likewise, the &lt;em&gt;choose&lt;/em&gt; operation appears below G, and the shifts and &lt;em&gt;Σ1&lt;/em&gt; above E. In the lower right, a bunch of terms are added together, corresponding to the first three red sum boxes. In the upper right, this sum is used to generate the new A value, and in the middle right, this sum is used to generate the new E value. These steps all correspond to the diagram and discussion above.&lt;/p&gt;
&lt;p&gt;I also manually performed another hash round, the last round to finish hashing the Bitcoin block. In the image below, the hash result is highlighted in yellow. The zeroes in this hash show that it is a successful hash. Note that the zeroes are at the end of the hash. The reason is that Bitcoin inconveniently reverses all the bytes generated by SHA-256.&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref4&quot;&gt;[4]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://static.righto.com/images/bitcoin/mine_pencil_2.png&quot;&gt;&lt;img alt=&quot;Last pencil-and-paper round of SHA-256, showing a successfully-mined Bitcoin block.&quot; class=&quot;hilite2&quot; src=&quot;http://static.righto.com/images/bitcoin/mine_pencil_2-w500.png&quot; title=&quot;Last pencil-and-paper round of SHA-256, showing a successfully-mined Bitcoin block.&quot; width=&quot;500&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cite&quot;&gt;Last pencil-and-paper round of SHA-256, showing a successfully-mined Bitcoin block.&lt;/div&gt;
&lt;h2&gt;What this means for mining hardware&lt;/h2&gt;
Each step of SHA-256 is very easy to implement in digital logic - simple Boolean operations and 32-bit addition. (If you've studied electronics, you can probably visualize the circuits already.) For this reason, custom ASIC chips can implement the SHA-256 algorithm very efficiently in hardware, putting hundreds of rounds on a chip in parallel. The image below shows a mining chip that runs at 2-3 billion hashes/second; &lt;a href=&quot;http://zeptobars.ru/en/read/bitfury-bitcoin-mining-chip&quot;&gt;Zeptobars&lt;/a&gt; has more photos.
&lt;p&gt;&lt;a href=&quot;http://zeptobars.ru/en/read/bitfury-bitcoin-mining-chip&quot;&gt;&lt;img alt=&quot;The silicon die inside a Bitfury ASIC chip. This chip mines Bitcoin at 2-3 Ghash/second. Image from http://zeptobars.ru/en/read/bitfury-bitcoin-mining-chip (CC BY 3.0 license)&quot; class=&quot;hilite2&quot; src=&quot;http://static.righto.com/images/bitcoin/bitfury_die-s350.jpg&quot; title=&quot;The silicon die inside a Bitfury ASIC chip. This chip mines Bitcoin at 2-3 Ghash/second. Image from http://zeptobars.ru/en/read/bitfury-bitcoin-mining-chip (CC BY 3.0 license)&quot; width=&quot;350&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;cite&quot;&gt;The silicon die inside a Bitfury ASIC chip. This chip mines Bitcoin at 2-3 Ghash/second. Image from &lt;a href=&quot;http://zeptobars.ru/en/read/bitfury-bitcoin-mining-chip&quot;&gt;Zeptobars&lt;/a&gt;. (&lt;a href=&quot;https://creativecommons.org/licenses/by/3.0/&quot;&gt;CC BY 3.0&lt;/a&gt;)&lt;/div&gt;
&lt;p&gt;In contrast, Litecoin, Dogecoin, and similar altcoins use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Scrypt&quot;&gt;scrypt&lt;/a&gt; hash algorithm, which is intentionally designed to be difficult to implement in hardware. It stores 1024 different hash values into memory, and then combines them in unpredictable ways to get the final result. As a result, much more circuitry and memory is required for scrypt than for SHA-256 hashes. You can see the impact by looking at &lt;a href=&quot;http://www.topbitcoinmininghardware.com/&quot;&gt;mining hardware&lt;/a&gt;, which is thousands of times slower for scrypt (Litecoin, etc) than for SHA-256 (Bitcoin).&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
The SHA-256 algorithm is surprisingly simple, easy enough to do by hand. (The elliptic curve algorithm for signing Bitcoin transactions would be very painful to do by hand since it has lots of multiplication of 32-byte integers.) Doing one round of SHA-256 by hand took me 16 minutes, 45 seconds. At this rate, hashing a full Bitcoin block (128 rounds)&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref3&quot;&gt;[3]&lt;/a&gt; would take 1.49 days, for a hash rate of 0.67 hashes per day (although I would probably get faster with practice). In comparison, current Bitcoin mining hardware does several terahashes per second, about a quintillion times faster than my manual hashing. Needless to say, manual Bitcoin mining is not at all practical.&lt;a class=&quot;ref&quot; href=&quot;http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html#ref5&quot;&gt;[5]&lt;/a&gt;
&lt;p&gt;A Reddit reader &lt;a href=&quot;https://www.reddit.com/r/Bitcoin/comments/2hpegd/mining_bitcoin_with_pencil_and_paper_067_hashes/ckurfv4&quot;&gt;asked&lt;/a&gt; about my energy consumption. There's not much physical exertion, so assuming a resting metabolic rate of 1500kcal/day, manual hashing works out to almost 10 megajoules/hash. A typical energy consumption for mining hardware is 1000 megahashes/joule. So I'm less energy efficient by a factor of 10^16, or 10 quadrillion. The next question is the energy cost. A cheap source of food energy is &lt;a href=&quot;http://www.mymoneyblog.com/what-does-200-calories-cost-the-economics-of-obesity.html&quot;&gt;donuts&lt;/a&gt; at $0.23 for 200 kcalories. Electricity here is $0.15/kilowatt-hour, which is cheaper by a factor of 6.7 - closer than I expected. Thus my energy cost per hash is about 67 quadrillion times that of mining hardware. It's clear I'm not going to make my fortune off manual mining, and I haven't even included the cost of all the paper and pencils I'll need.&lt;/p&gt;
&lt;p&gt;2017 edit: My Bitcoin mining on paper system is part of the book &lt;a href=&quot;https://amzn.to/2kGdqBA&quot;&gt;The Objects That Power the Global Economy&lt;/a&gt;, so take a look.&lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&quot;https://twitter.com/kenshirriff&quot;&gt;Twitter&lt;/a&gt; to find out about my latest blog posts.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
[1] It's not exactly the number of zeros at the start of the hash that matters. To be precise, the hash must be less than a particular value that depends on the current Bitcoin &lt;a href=&quot;https://en.bitcoin.it/wiki/Difficulty&quot;&gt;difficulty level&lt;/a&gt;.
&lt;p&gt;[2] The source of the constants used in SHA-256 is interesting. The NSA designed the SHA-256 algorithm and picked the values for these constants, so how do you know they didn't pick special values that let them break the hash? To avoid suspicion, the initial hash values come from the square roots of the first 8 primes, and the &lt;em&gt;K&lt;sub&gt;t&lt;/sub&gt;&lt;/em&gt; values come from the cube roots of the first 64 primes. Since these constants come from a simple formula, you can trust that the NSA didn't do anything shady (at least with the constants).&lt;/p&gt;
&lt;p&gt;[3] Unfortunately the SHA-256 hash works on a block of 512 bits, but the Bitcoin block header is more than 512 bits. Thus, a second set of 64 SHA-256 hash rounds is required on the second half of the Bitcoin block. Next, Bitcoin uses &lt;em&gt;double-SHA-256&lt;/em&gt;, so a second application of SHA-256 (64 rounds) is done to the result. Adding this up, hashing an arbitrary Bitcoin block takes 192 rounds in total. However there is a shortcut. Mining involves hashing the same block over and over, just changing the &lt;em&gt;nonce&lt;/em&gt; which appears in the second half of the block. Thus, mining can reuse the result of hashing the first 512 bits, and hashing a Bitcoin block typically only requires 128 rounds.&lt;/p&gt;
&lt;p&gt;[4] Obviously I didn't just have incredible good fortune to end up with a successful hash. I started the hashing process with a block that had already been successfully mined. In particular I used the one displayed earlier in this article, &lt;a href=&quot;https://blockexplorer.com/b/286819&quot;&gt;#286819&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[5] Another problem with manual mining is new blocks are mined about every 10 minutes, so even if I did succeed in mining a block, it would be totally obsolete (orphaned) by the time I finished.&lt;/p&gt;

</description>
<pubDate>Mon, 15 Feb 2021 19:12:51 +0000</pubDate>
<dc:creator>tusharchoudhary</dc:creator>
<og:url>http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html</og:url>
<og:title>Mining Bitcoin with pencil and paper: 0.67 hashes per day</og:title>
<og:description>This article is now available in Japanese: 紙と鉛筆でビットコインをマイニング：1日に0.67ハッシュ and Russian: Майним Bitcoin с помощью бумаги и ручки . I decided...</og:description>
<og:image>https://lh6.googleusercontent.com/proxy/TLCStrwNoSkl9odtjecKu2Imfrd3s159QSyZ-AE-tuDYuHgvSb44mB7u03DsOdNC4eyt1EDgYspm6HUDbxzsgxEUkvL6aUNr5apSNwEwRbxxtw=w1200-h630-p-k-no-nu</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.righto.com/2014/09/mining-bitcoin-with-pencil-and-paper.html</dc:identifier>
</item>
<item>
<title>YouTubers have to declare ads. Why doesn&amp;#039;t anyone else? [video]</title>
<link>https://www.youtube.com/watch?v=L-x8DYTOv7w</link>
<guid isPermaLink="true" >https://www.youtube.com/watch?v=L-x8DYTOv7w</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.youtube.com/watch?v=L-x8DYTOv7w&quot;&gt;https://www.youtube.com/watch?v=L-x8DYTOv7w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=26144528&quot;&gt;https://news.ycombinator.com/item?id=26144528&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 276&lt;/p&gt;
&lt;p&gt;# Comments: 77&lt;/p&gt;
</description>
<pubDate>Mon, 15 Feb 2021 17:01:56 +0000</pubDate>
<dc:creator>zinekeller</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.youtube.com/watch?v=L-x8DYTOv7w</dc:identifier>
</item>
<item>
<title>Launch HN: Noya (YC W21) – Direct air capture of CO2 using cooling towers</title>
<link>https://news.ycombinator.com/item?id=26144432</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=26144432</guid>
<description>Hello HN!
&lt;p&gt;I'm Josh, one of the co-founders of Noya (&lt;a href=&quot;https://noyalabs.com&quot; rel=&quot;nofollow&quot;&gt;https://noyalabs.com&lt;/a&gt;). Noya is designing a cheaper process to capture CO2 directly from the atmosphere. We do this by retrofitting industrial cooling towers owned and operated by other companies to perform carbon capture. We then sell the captured CO2 to companies that need it, and pay a piece of the proceeds to the companies that own the cooling towers.&lt;/p&gt;&lt;p&gt;As the wildfires in California became worse and worse, my co-founder (and roommate at the time) Daniel and I became increasingly concerned that we weren't doing enough to be a part of the solution. The more that climate catastrophes became the norm, the more we became obsessed with one seemingly-simple question:&lt;/p&gt;
&lt;p&gt;If climate change is caused by having too much CO2 in the sky... can't we just reverse it by yanking CO2 out of the sky?&lt;/p&gt;
&lt;p&gt;Humans have known how to scrub CO2 out of gas mixtures for almost a century [1]; but, we haven't been able to widely apply this type of tech to scrubbing CO2 from the air because of its high cost. For example, one popular direct air capture project is estimated to capture 1M tons of CO2/year [2], but has an estimated equipment cost of $700M and all-in costs of ~$1.1B [3]. The single largest component of this cost is in the piece of equipment called the air contactor — the big wall of fans you see in the image linked above — which clocks in at $212M by itself. Yet fundamentally, all that air contactors do is put air into contact with something that captures CO2, whether it's an aqueous capture solution or some sort of solid sorbent.&lt;/p&gt;
&lt;p&gt;These costs felt astronomical to Daniel and I, so we set out with the singular focus to reduce the costs of carbon capture by reducing the costs of the air contactor. But no matter how we thought about it, we couldn’t get around the fact that to capture meaningful amounts of CO2, you need to move massive amounts of air since CO2 is very dilute in the atmosphere (0.04% by volume). Looking at the existing solutions, we began to understand why it makes sense to build something equally massive: so you can go after economies of scale.&lt;/p&gt;
&lt;p&gt;As Daniel and I were feeling stuck late one night, he got a call from his dad. They started talking about the refrigeration facility Daniel’s dad runs in Venezuela (where Daniel's from), and they started talking about the cooling towers at the facility. Cooling towers move air and water into contact with each other to provide cooling to industrial processes (descriptive video: &lt;a href=&quot;https://www.youtube.com/watch?v=pXaK8_F8dn0&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=pXaK8_F8dn0&lt;/a&gt;). As Daniel listened to his dad, Daniel realized that if we could just add the blend of CO2-absorbing chemicals we had been developing into the water his dad’s cooling tower used, we could use it as an air contactor and achieve CO2 capture at the same time the cooling tower was cooling its processes. This eliminates the need to build millions of dollars worth of dedicated equipment to pluck CO2 from the sky.&lt;/p&gt;
&lt;p&gt;Our cooling-tower-based carbon capture process works as follows: we add our chemical carbon capture blend into a cooling tower's water, we connect the tower to some pieces of downstream processing equipment to regenerate the captured CO2, and then we pressurize the CO2 into cylinders for sale as &quot;reclaimed CO2&quot; to companies that need it. All of this is installed onto a cooling tower that another company already owns and operates. In exchange for letting us install this process on their towers, we will cover the cost of installation, and the companies will get a piece of the revenue generated through the sale of their CO2.&lt;/p&gt;
&lt;p&gt;We’re well on our way towards making this process a reality. We’ve partnered with a local farm to install our process in their cooling towers, and we've just produced CO2 using our industrial-scale prototype.&lt;/p&gt;
&lt;p&gt;We're excited for the opportunity to reverse climate change and ensure we have a future on this planet that is good. Please let us know what questions, concerns, or feedback you have about what we're building - I’ll be here all day!&lt;/p&gt;
&lt;p&gt;[1]: &lt;a href=&quot;https://science.sciencemag.org/content/325/5948/1652&quot; rel=&quot;nofollow&quot;&gt;https://science.sciencemag.org/content/325/5948/1652&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]: &lt;a href=&quot;https://blogs-images.forbes.com/jamesconca/files/2019/10/1-air-contactor.jpg&quot; rel=&quot;nofollow&quot;&gt;https://blogs-images.forbes.com/jamesconca/files/2019/10/1-a...&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]: &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S2542435118302253&quot; rel=&quot;nofollow&quot;&gt;https://www.sciencedirect.com/science/article/pii/S254243511...&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 15 Feb 2021 16:53:24 +0000</pubDate>
<dc:creator>jsantos511</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=26144432</dc:identifier>
</item>
<item>
<title>Faster JavaScript Calls</title>
<link>https://v8.dev/blog/adaptor-frame</link>
<guid isPermaLink="true" >https://v8.dev/blog/adaptor-frame</guid>
<description>&lt;p&gt;JavaScript allows calling a function with a different number of arguments than the expected number of parameters, i.e., one can pass fewer or more arguments than the declared formal parameters. The former case is called under-application and the latter is called over-application.&lt;/p&gt;
&lt;p&gt;In the under-application case, the remaining parameters get assigned the undefined value. In the over-application case, the remaining arguments can be accessed by using the rest parameter and the &lt;code&gt;arguments&lt;/code&gt; property, or they are simply superfluous and they can be ignored. Many Web/NodeJS frameworks nowadays use this JS feature to accept optional parameters and create a more flexible API.&lt;/p&gt;
&lt;p&gt;Until recently, V8 had a special machinery to deal with arguments size mismatch: the arguments adaptor frame. Unfortunately, argument adaption comes at a performance cost, but is commonly needed in modern front-end and middleware frameworks. It turns out that, with a clever trick, we can remove this extra frame, simplify the V8 codebase and get rid of almost the entire overhead.&lt;/p&gt;
&lt;p&gt;We can calculate the performance impact of removing the arguments adaptor frame through a micro-benchmark.&lt;/p&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;console&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token parameter&quot;&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; z&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token keyword&quot;&gt;let&lt;/span&gt; i &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; i &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;  &lt;span class=&quot;token constant&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; i&lt;span class=&quot;token operator&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token function&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;br/&gt;console&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;timeEnd&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;img alt=&quot;&quot; height=&quot;371&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/v8-release-89/perf.svg&quot; width=&quot;600&quot;/&gt;Performance impact of removing the arguments adaptor frame, as measured through a micro-benchmark.
&lt;p&gt;The graph shows that there is no overhead anymore when running on &lt;a href=&quot;https://v8.dev/blog/jitless&quot;&gt;JIT-less mode&lt;/a&gt; (Ignition) with a 11.2% performance improvement. When using &lt;a href=&quot;https://v8.dev/docs/turbofan&quot;&gt;TurboFan&lt;/a&gt;, we get up to 40% speedup.&lt;/p&gt;
&lt;p&gt;This microbenchmark was naturally designed to maximise the impact of the arguments adaptor frame. We have however seen a considerable improvement in many benchmarks, such as in &lt;a href=&quot;https://chromium.googlesource.com/v8/v8/+/b7aa85fe00c521a704ca83cc8789354e86482a60/test/js-perf-test/JSTests.json&quot;&gt;our internal JSTests/Array benchmark&lt;/a&gt; (7%) and in &lt;a href=&quot;https://github.com/chromium/octane&quot;&gt;Octane2&lt;/a&gt; (4.6% in Richards and 6.1% in EarleyBoyer).&lt;/p&gt;
&lt;h2 id=&quot;tl%3Bdr%3A-reverse-the-arguments&quot;&gt;TL;DR: Reverse the arguments &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#tl%3Bdr%3A-reverse-the-arguments&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The whole point of this project was to remove the arguments adaptor frame, which offers a consistent interface to the callee when accessing its arguments in the stack. In order to do that, we needed to reverse the arguments in the stack and added a new slot in the callee frame containing the actual argument count. The figure below shows the example of a typical frame before and after the change.&lt;img alt=&quot;&quot; height=&quot;624&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/frame-diff.svg&quot; width=&quot;639&quot;/&gt;A typical JavaScript stack frame before and after removing the arguments adaptor frame.&lt;/p&gt;
&lt;h2 id=&quot;making-javascript-calls-faster&quot;&gt;Making JavaScript calls faster &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#making-javascript-calls-faster&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;To appreciate what we have done to make calls faster, let’s see how V8 performs a call and how the arguments adaptor frame works.&lt;/p&gt;
&lt;p&gt;What happens inside V8 when we invoke a function call in JS? Let’s suppose the following JS script:&lt;/p&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token parameter&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; x &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;img alt=&quot;&quot; height=&quot;391&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/flow.svg&quot; width=&quot;1193&quot;/&gt;Flow of execution inside V8 during a function call.
&lt;h2 id=&quot;ignition&quot;&gt;Ignition &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#ignition&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;V8 is a multi-tier VM. Its first tier is called &lt;a href=&quot;https://v8.dev/docs/ignition&quot;&gt;Ignition&lt;/a&gt;, it is a bytecode stack machine with an accumulator register. V8 starts by compiling the code to &lt;a href=&quot;https://medium.com/dailyjs/understanding-v8s-bytecode-317d46c94775&quot;&gt;Ignition bytecodes&lt;/a&gt;. The above call is compiled to the following:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;0d              LdaUndefined              ;; Load undefined into the accumulator
26 f9           Star r2                   ;; Store it in register r2
13 01 00        LdaGlobal [1]             ;; Load global pointed by const 1 (add42)
26 fa           Star r1                   ;; Store it in register r1
0c 03           LdaSmi [3]                ;; Load small integer 3 into the accumulator
26 f8           Star r3                   ;; Store it in register r3
5f fa f9 02     CallNoFeedback r1, r2-r3  ;; Invoke call
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first argument of a call is usually referred to as the receiver. The receiver is the &lt;code&gt;this&lt;/code&gt; object inside a JSFunction, and every JS function call must have one. The bytecode handler of &lt;code&gt;CallNoFeedback&lt;/code&gt; needs to call the object &lt;code&gt;r1&lt;/code&gt; with the arguments in the register list &lt;code&gt;r2-r3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we dive into the bytecode handler, note how registers are encoded in the bytecode. They are negative single byte integers: &lt;code&gt;r1&lt;/code&gt; is encoded as &lt;code&gt;fa&lt;/code&gt;, &lt;code&gt;r2&lt;/code&gt; as &lt;code&gt;f9&lt;/code&gt; and &lt;code&gt;r3&lt;/code&gt; as &lt;code&gt;f8&lt;/code&gt;. We can refer to any register ri as &lt;code&gt;fb - i&lt;/code&gt;, actually as we will see, the correct encoding is &lt;code&gt;- 2 - kFixedFrameHeaderSize - i&lt;/code&gt;. Register lists are encoded using the first register and the size of the list, so &lt;code&gt;r2-r3&lt;/code&gt; is &lt;code&gt;f9 02&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There are many bytecode call handlers in Ignition. You can see a list of them &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/interpreter/bytecodes.h;drc=3965dcd5cb1141c90f32706ac7c965dc5c1c55b3;l=184&quot;&gt;here&lt;/a&gt;. They vary slightly from each other. There are bytecodes optimized for calls with an &lt;code&gt;undefined&lt;/code&gt; receiver, for property calls, for calls with a fixed number of parameters or for generic calls. Here we analyze &lt;code&gt;CallNoFeedback&lt;/code&gt; which is a generic call in which we don’t accumulate feedback from the execution.&lt;/p&gt;
&lt;p&gt;The handler of this bytecode is quite simple. It is written in &lt;a href=&quot;https://v8.dev/docs/csa-builtins&quot;&gt;&lt;code&gt;CodeStubAssembler&lt;/code&gt;&lt;/a&gt;, you can check it out &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/interpreter/interpreter-generator.cc;drc=6cdb24a4ce9d4151035c1f133833137d2e2881d1;l=1467&quot;&gt;here&lt;/a&gt;. Essentially, it tailcalls to an architecture-dependent built-in &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/builtins/x64/builtins-x64.cc;drc=8665f09771c6b8220d6020fe9b1ad60a4b0b6591;l=1277&quot;&gt;&lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The built-in essentially pops the return address to a temporary register, pushes all the arguments (including the receiver) and pushes back the return address. At this point, we do not know if the callee is a callable object nor how many arguments the callee is expecting, i.e., its formal parameter count.&lt;img alt=&quot;&quot; height=&quot;191&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/normal-push.svg&quot; width=&quot;422&quot;/&gt;State of the frame after the execution of &lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt; built-in.&lt;/p&gt;
&lt;p&gt;Eventually the execution tailcalls to the built-in &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/builtins/x64/builtins-x64.cc;drc=8665f09771c6b8220d6020fe9b1ad60a4b0b6591;l=2256&quot;&gt;&lt;code&gt;Call&lt;/code&gt;&lt;/a&gt;. There, it checks if the target is a proper function, a constructor or any callable object. It also reads the &lt;code&gt;shared function info&lt;/code&gt; structure to get its formal parameter count.&lt;/p&gt;
&lt;p&gt;If the callee is a function object, it tailcalls to the built-in &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/builtins/x64/builtins-x64.cc;drc=8665f09771c6b8220d6020fe9b1ad60a4b0b6591;l=2038&quot;&gt;&lt;code&gt;CallFunction&lt;/code&gt;&lt;/a&gt;, where a bunch of checks happen, including if we have an &lt;code&gt;undefined&lt;/code&gt; object as receiver. If we have an &lt;code&gt;undefined&lt;/code&gt; or &lt;code&gt;null&lt;/code&gt; object as receiver, we should patch it to refer to the global proxy object, according to the &lt;a href=&quot;https://262.ecma-international.org/11.0/#sec-ordinarycallbindthis&quot;&gt;ECMA specification&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The execution then tailcalls to the built-in &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/codegen/x64/macro-assembler-x64.cc;drc=a723767935dec385818d1134ea729a4c3a3ddcfb;l=2781&quot;&gt;&lt;code&gt;InvokeFunctionCode&lt;/code&gt;&lt;/a&gt;, which will in the absence of arguments mismatch just call whatever is being pointed by the field &lt;code&gt;Code&lt;/code&gt; in the callee object. This could either be an optimized function or the built-in &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/builtins/x64/builtins-x64.cc;drc=8665f09771c6b8220d6020fe9b1ad60a4b0b6591;l=1037&quot;&gt;&lt;code&gt;InterpreterEntryTrampoline&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If we assume we’re calling a function that hasn’t been optimized yet, the Ignition trampoline will set up an &lt;code&gt;IntepreterFrame&lt;/code&gt;. You can see a brief summary of the frame types in V8 &lt;a href=&quot;https://source.chromium.org/chromium/chromium/src/+/master:v8/src/execution/frame-constants.h;drc=574ac5d62686c3de8d782dc798337ce1355dc066;l=14&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Without going into too much detail of what happens next, we can see a snapshot of the interpreter frame during the callee execution.&lt;img alt=&quot;&quot; height=&quot;511&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/normal-frame.svg&quot; width=&quot;588&quot;/&gt;The &lt;code&gt;InterpreterFrame&lt;/code&gt; for the call &lt;code&gt;add42(3)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We see that we have a fixed number of slots in the frame: the return address, the previous frame pointer, the context, the current function object we’re executing, the bytecode array of this function and the offset of the current bytecode we’re executing. Finally, we have a list of registers dedicated to this function (you can think of them as function locals). The &lt;code&gt;add42&lt;/code&gt; function doesn’t actually have any registers, but the caller has a similar frame with 3 registers.&lt;/p&gt;
&lt;p&gt;As expected add42 is a simple function:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;25 02             Ldar a0          ;; Load the first argument to the accumulator
40 2a 00          AddSmi [42]      ;; Add 42 to it
ab                Return           ;; Return the accumulator
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note how we encode the argument in the &lt;code&gt;Ldar&lt;/code&gt; &lt;em&gt;(Load Accumulator Register)&lt;/em&gt; bytecode: argument &lt;code&gt;1&lt;/code&gt; (&lt;code&gt;a0&lt;/code&gt;) is encoded with the number &lt;code&gt;02&lt;/code&gt;. In fact, the encoding of any argument is simply &lt;code&gt;[ai] = 2 + parameter_count - i - 1&lt;/code&gt; and the receiver &lt;code&gt;[this] = 2 + parameter_count&lt;/code&gt;, or in this example &lt;code&gt;[this] = 3&lt;/code&gt;. The parameter count here does not include the receiver.&lt;/p&gt;
&lt;p&gt;We’re now able to understand why we encode registers and arguments this way. They simply denote an offset from the frame pointer. We can then treat argument/register load and store in the same way. The offset for the last argument from the frame pointer is &lt;code&gt;2&lt;/code&gt; (previous frame pointer and the return address). That explains the &lt;code&gt;2&lt;/code&gt; in the encoding. The fixed part of the interpreter frame is &lt;code&gt;6&lt;/code&gt; slots (&lt;code&gt;4&lt;/code&gt; from the frame pointer), so the register zero is located at offset &lt;code&gt;-5&lt;/code&gt;, i.e. &lt;code&gt;fb&lt;/code&gt;, register &lt;code&gt;1&lt;/code&gt; at &lt;code&gt;fa&lt;/code&gt;. Clever, right?&lt;/p&gt;
&lt;p&gt;Note however to be able to access the arguments, the function must know how many arguments are in the stack! The index &lt;code&gt;2&lt;/code&gt; points to the last argument regardless of how many arguments there are!&lt;/p&gt;
&lt;p&gt;The bytecode handler of &lt;code&gt;Return&lt;/code&gt; will finish by calling the built-in &lt;code&gt;LeaveInterpreterFrame&lt;/code&gt;. This built-in essentially reads the function object to get the parameter count from the frame, pops the current frame, recovers the frame pointer, saves the return address in a scratch register, pops the arguments according to the parameter count and jumps to the address in the scratch registers.&lt;/p&gt;
&lt;p&gt;All this flow is great! But what happens when we call a function with fewer or more arguments than its parameter count? The clever argument/register access will fail and how do we clean up the arguments at the end of the call?&lt;/p&gt;
&lt;h2 id=&quot;arguments-adaptor-frame&quot;&gt;Arguments adaptor frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#arguments-adaptor-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Let’s now call &lt;code&gt;add42&lt;/code&gt; with fewer and more arguments:&lt;/p&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The JS developers between us will know that in the first case, &lt;code&gt;x&lt;/code&gt; will be assigned &lt;code&gt;undefined&lt;/code&gt; and the function will return &lt;code&gt;undefined + 42 = NaN&lt;/code&gt;. In the second case, &lt;code&gt;x&lt;/code&gt; will be assigned &lt;code&gt;1&lt;/code&gt; and the function will return &lt;code&gt;43&lt;/code&gt;, the remaining arguments will be ignored. Note that the caller does not know if that will happen. Even if the caller checks the parameter count, the callee could use the rest parameter or the arguments object to access all the other arguments. Actually, the arguments object can even be accessed outside &lt;code&gt;add42&lt;/code&gt; in sloppy mode.&lt;/p&gt;
&lt;p&gt;If we follow the same steps as before, we will first call the built-in &lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt;. It will push the arguments to the stack like so:&lt;img alt=&quot;&quot; height=&quot;276&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/adaptor-push.svg&quot; width=&quot;679&quot;/&gt;State of the frames after the execution of &lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt; built-in.&lt;/p&gt;
&lt;p&gt;Continuing the same procedure as before, we check if the callee is a function object, get its parameter count and patch the receiver to the global proxy. Eventually we reach &lt;code&gt;InvokeFunctionCode&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here instead of jumping to the &lt;code&gt;Code&lt;/code&gt; in the callee object. We check that we have a mismatch between argument size and parameter count and jump to &lt;code&gt;ArgumentsAdaptorTrampoline&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In this built-in, we build an extra frame, the infamous arguments adaptor frame. Instead of explaining what happens inside the built-in, I will just present you the state of the frame before the built-in calls the callee’s &lt;code&gt;Code&lt;/code&gt;. Note that this is a proper &lt;code&gt;x64 call&lt;/code&gt; (not a &lt;code&gt;jmp&lt;/code&gt;) and after the execution of the callee we will return to the &lt;code&gt;ArgumentsAdaptorTrampoline&lt;/code&gt;. This is a contrast with &lt;code&gt;InvokeFunctionCode&lt;/code&gt; that tailcalls.&lt;img alt=&quot;&quot; height=&quot;665&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/adaptor-frames.svg&quot; width=&quot;767&quot;/&gt;Stack frames with arguments adaptation.&lt;/p&gt;
&lt;p&gt;You can see that we create another frame that copies all the arguments necessary in order to have precisely the parameter count of arguments on top of the callee frame. It creates an interface to the callee function, so that the latter does not need to know the number of arguments. The callee will always be able to access its parameters with the same calculation as before, that is, &lt;code&gt;[ai] = 2 + parameter_count - i - 1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;V8 has special built-ins that understand the adaptor frame whenever it needs to access the remaining arguments through the rest parameter or the arguments object. They will always need to check the adaptor frame type on top of the callee’s frame and then act accordingly.&lt;/p&gt;
&lt;p&gt;As you can see, we solve the argument/register access issue, but we create a lot of complexity. Every built-in that needs to access all the arguments will need to understand and check the existence of the adaptor frame. Not only that, we need to be careful to not access stale and old data. Consider the following changes to &lt;code&gt;add42&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token parameter&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;&lt;br/&gt;x &lt;span class=&quot;token operator&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The bytecode array now is:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;25 02             Ldar a0       ;; Load the first argument to the accumulator
40 2a 00          AddSmi [42]   ;; Add 42 to it
26 02             Star a0       ;; Store accumulator in the first argument slot
ab                Return        ;; Return the accumulator
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As you can see, we now modify &lt;code&gt;a0&lt;/code&gt;. So, in the case of a call &lt;code&gt;add42(1, 2, 3)&lt;/code&gt; the slot in the arguments adaptor frame will be modified, but the caller frame will still contain the number &lt;code&gt;1&lt;/code&gt;. We need to be careful that the arguments object is accessing the modified value instead of the stale one.&lt;/p&gt;
&lt;p&gt;Returning from the function is simple, albeit slow. Remember what &lt;code&gt;LeaveInterpreterFrame&lt;/code&gt; does? It basically pops the callee frame and the arguments up to the parameter count number. So when we return to the arguments adaptor stub, the stack looks like so:&lt;img alt=&quot;&quot; height=&quot;401&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/adaptor-frames-cleanup.svg&quot; width=&quot;765&quot;/&gt;State of the frames after the execution of the callee &lt;code&gt;add42&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We just need to pop the number of arguments, pop the adaptor frame, pop all the arguments according to the actual arguments count and return to the caller execution.&lt;/p&gt;
&lt;p&gt;TL;DR: the arguments adaptor machinery is not only complex, but costly.&lt;/p&gt;
&lt;h2 id=&quot;removing-the-arguments-adaptor-frame&quot;&gt;Removing the arguments adaptor frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#removing-the-arguments-adaptor-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Can we do better? Can we remove the adaptor frame? It turns out that we can indeed.&lt;/p&gt;
&lt;p&gt;Let’s review our requirements:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We need to be able to access the arguments and registers seamlessly like before. No checks can be done when accessing them. That would be too expensive.&lt;/li&gt;
&lt;li&gt;We need to be able to construct the rest parameter and the arguments object from the stack.&lt;/li&gt;
&lt;li&gt;We need to be able to easily clean up an unknown number of arguments when returning from a call.&lt;/li&gt;
&lt;li&gt;And, of course, we want to do that without an extra frame!&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;If we want to eliminate the extra frame, then we need to decide where to put the arguments: either in the callee frame or in the caller frame.&lt;/p&gt;
&lt;h3 id=&quot;arguments-in-the-callee-frame&quot;&gt;Arguments in the callee frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#arguments-in-the-callee-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let’s suppose we put the arguments in the callee frame. This seems actually a good idea, since whenever we pop the frame, we also pop all the arguments at once!&lt;/p&gt;
&lt;p&gt;The arguments would need to be located somewhere between the saved frame pointer and the end of the frame. It entails that the size of the frame will not be statically known. Accessing an argument will still be easy, it is a simple offset from the frame pointer. But accessing a register is now much more complicated, since it varies according to the number of the arguments.&lt;/p&gt;
&lt;p&gt;The stack pointer always points to the last register, we could then use it to access the registers without knowing the arguments count. This approach might actually work, but it has a major drawback. That would entail duplicating all the bytecodes that can access registers and arguments. We would need a &lt;code&gt;LdaArgument&lt;/code&gt; and a &lt;code&gt;LdaRegister&lt;/code&gt; instead of simply &lt;code&gt;Ldar&lt;/code&gt;. Of course, we could also check if we are accessing an argument or a register (positive or negative offsets), but that would require a check in every argument and register access. Clearly too expensive!&lt;/p&gt;
&lt;h3 id=&quot;arguments-in-the-caller-frame&quot;&gt;Arguments in the caller frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#arguments-in-the-caller-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Okay… what if we stick with the arguments in the caller frame?&lt;/p&gt;
&lt;p&gt;Remember how to calculate the offset of the argument &lt;code&gt;i&lt;/code&gt; in a frame: &lt;code&gt;[ai] = 2 + parameter_count - i - 1&lt;/code&gt;. If we have all arguments (not only the parameters), the offset will be &lt;code&gt;[ai] = 2 + argument_count - i - 1&lt;/code&gt;. That is, for every argument access, we would need to load the actual argument count.&lt;/p&gt;
&lt;p&gt;But what happens if we reverse the arguments? Now the offset can be simply calculated as &lt;code&gt;[ai] = 2 + i&lt;/code&gt;. We don’t need to know how many arguments are in the stack, but if we can guarantee that we'll always have at least the parameter count of arguments in the stack, then we can always use this scheme to calculate the offset.&lt;/p&gt;
&lt;p&gt;In other words, the number of arguments pushed in the stack will always be the maximum between the number of arguments and the formal parameter count, and it will be padded with undefined objects if needed.&lt;/p&gt;
&lt;p&gt;This has yet another bonus! The receiver is always located in the same offset for any JS function, just above the return address: &lt;code&gt;[this] = 2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is a clean solution for our requirement number &lt;code&gt;1&lt;/code&gt; and number &lt;code&gt;4&lt;/code&gt;. What about the other two requirements? How can we construct the rest parameter and the arguments object? And how to clean the arguments in the stack when returning to the caller? For that we are only missing the argument count. We will need to save it somewhere. The choice here is a bit arbitrary, as long as it is easy to access this information. Two basic choices are: to push it just after the receiver in the caller frame or as part of the callee frame in the fixed header part. We implemented the latter, since it coalesces the fixed header part of Interpreter and Optimized frames.&lt;/p&gt;
&lt;p&gt;If we run our example in V8 v8.9 we will see the following stack after &lt;code&gt;InterpreterArgsThenPush&lt;/code&gt; (note that the arguments are now reversed):&lt;img alt=&quot;&quot; height=&quot;276&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/no-adaptor-push.svg&quot; width=&quot;679&quot;/&gt;State of the frames after the execution of &lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt; built-in.&lt;/p&gt;
&lt;p&gt;All the execution follows a similar path until we reach InvokeFunctionCode. Here we massage the arguments in case of under-application, pushing as many undefined objects as needed. Note that we do not change anything in case of over-application. Finally we pass the number of arguments to callee’s &lt;code&gt;Code&lt;/code&gt; through a register. In the case of &lt;code&gt;x64&lt;/code&gt;, we use the register &lt;code&gt;rax&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If the callee hasn’t been optimized yet, we reach &lt;code&gt;InterpreterEntryTrampoline&lt;/code&gt;, which builds the following stack frame.&lt;img alt=&quot;&quot; height=&quot;626&quot; loading=&quot;lazy&quot; src=&quot;https://v8.dev/_img/adaptor-frame/no-adaptor-frames.svg&quot; width=&quot;679&quot;/&gt;Stack frames without arguments adaptors.&lt;/p&gt;
&lt;p&gt;The callee frame has an extra slot containing the number of arguments that can be used for constructing the rest parameter or the arguments object and to clean the arguments in the stack before returning to the caller.&lt;/p&gt;
&lt;p&gt;To return, we modify &lt;code&gt;LeaveInterpreterFrame&lt;/code&gt; to read the arguments count in the stack and pop out the maximum number between the argument count and the formal parameter count.&lt;/p&gt;
&lt;h2 id=&quot;turbofan&quot;&gt;TurboFan &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#turbofan&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;What about optimized code? Let’s change slightly our initial script to force V8 to compile it with TurboFan:&lt;/p&gt;
&lt;pre class=&quot;language-js&quot;&gt;
&lt;code class=&quot;language-js&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token parameter&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; x &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token keyword&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;callAdd42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;add42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;PrepareFunctionForOptimization&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;callAdd42&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token function&quot;&gt;callAdd42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token operator&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;token function&quot;&gt;OptimizeFunctionOnNextCall&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;callAdd42&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token function&quot;&gt;callAdd42&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;;&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here we use V8 intrinsics to force V8 to optimize the call, otherwise V8 would only optimize our little function if it becomes hot (used very often). We call it once before optimization to gather some type information that can be used to guide the compilation. Read more about TurboFan &lt;a href=&quot;https://v8.dev/docs/turbofan&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll show you here only the part of the generated code that is relevant to us.&lt;/p&gt;
&lt;pre class=&quot;language-nasm&quot;&gt;
&lt;code class=&quot;language-nasm&quot;&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rdi&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x1a8e082126ad&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; Load the function object &amp;lt;JSFunction add42&amp;gt;&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x6&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;;; Push SMI 3 as argument&lt;/span&gt;&lt;br/&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x1a8e082030d1&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; &amp;lt;JSGlobal Object&amp;gt;&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;;; Push receiver (the global proxy object)&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rax&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x1&lt;/span&gt;               &lt;span class=&quot;token comment&quot;&gt;;; Save the arguments count in rax&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token operator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token register variable&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0x17&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;token comment&quot;&gt;;; Load function object {Code} field in rcx&lt;/span&gt;&lt;br/&gt;call &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;;; Finally, call the code object!&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Although written in assembler, this code snippet should not be difficult to read if you follow my comments. Essentially, when compiling the call, TF needs to do all the work that was done in &lt;code&gt;InterpreterPushArgsThenCall&lt;/code&gt;, &lt;code&gt;Call&lt;/code&gt;, &lt;code&gt;CallFunction&lt;/code&gt; and &lt;code&gt;InvokeFunctionCall&lt;/code&gt; built-ins. Hopefully it has more static information to do that and emit less computer instructions.&lt;/p&gt;
&lt;h3 id=&quot;turbofan-with-the-arguments-adaptor-frame&quot;&gt;TurboFan with the arguments adaptor frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#turbofan-with-the-arguments-adaptor-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now, let’s see in the case of mismatching number of arguments and parameter count. Consider the call &lt;code&gt;add42(1, 2, 3)&lt;/code&gt;. This is compiled to:&lt;/p&gt;
&lt;pre class=&quot;language-nasm&quot;&gt;
&lt;code class=&quot;language-nasm&quot;&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rdi&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x4250820fff1&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; Load the function object &amp;lt;JSFunction add42&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token comment&quot;&gt;;; Push receiver and arguments SMIs 1, 2 and 3&lt;/span&gt;&lt;br/&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x42508080dd5&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; &amp;lt;JSGlobal Object&amp;gt;&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x2&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x4&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x6&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rax&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x3&lt;/span&gt;              &lt;span class=&quot;token comment&quot;&gt;;; Save the arguments count in rax&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rbx&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x1&lt;/span&gt;              &lt;span class=&quot;token comment&quot;&gt;;; Save the formal parameters count in rbx&lt;/span&gt;&lt;br/&gt;movq &lt;span class=&quot;token register variable&quot;&gt;r10&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x564ed7fdf840&lt;/span&gt;   &lt;span class=&quot;token comment&quot;&gt;;; &amp;lt;ArgumentsAdaptorTrampoline&amp;gt;&lt;/span&gt;&lt;br/&gt;call &lt;span class=&quot;token register variable&quot;&gt;r10&lt;/span&gt;                  &lt;span class=&quot;token comment&quot;&gt;;; Call the ArgumentsAdaptorTrampoline&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As you can see, it is not hard to add support to TF for argument and parameter count mismatch. Just call the arguments adaptor trampoline!&lt;/p&gt;
&lt;p&gt;This is however expensive. For every optimized call, we now need to enter in the arguments adaptor trampoline and massage the frame as in non-optimized code. That explains why the performance gain of removing the adaptor frame in optimized code is much larger than on Ignition.&lt;/p&gt;
&lt;p&gt;The generated code is however very simple. And returning from it is extremely easy (epilogue):&lt;/p&gt;
&lt;pre class=&quot;language-nasm&quot;&gt;
&lt;code class=&quot;language-nasm&quot;&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rsp&lt;/span&gt;,&lt;span class=&quot;token register variable&quot;&gt;rbp&lt;/span&gt;   &lt;span class=&quot;token comment&quot;&gt;;; Clean callee frame&lt;/span&gt;&lt;br/&gt;pop &lt;span class=&quot;token register variable&quot;&gt;rbp&lt;/span&gt;&lt;br/&gt;ret &lt;span class=&quot;token number&quot;&gt;0x8&lt;/span&gt;        &lt;span class=&quot;token comment&quot;&gt;;; Pops a single argument (the receiver)&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We pop our frame and emit a return instruction according to the parameter count. If we have a mismatch in the number of arguments and parameter count, the adaptor frame trampoline will deal with it.&lt;/p&gt;
&lt;h3 id=&quot;turbofan-without-the-arguments-adaptor-frame&quot;&gt;TurboFan without the arguments adaptor frame &lt;a href=&quot;https://v8.dev/blog/adaptor-frame#turbofan-without-the-arguments-adaptor-frame&quot; class=&quot;bookmark&quot;&gt;#&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The generated code is essentially the same as in a call with a matching number of arguments. Consider the call &lt;code&gt;add42(1, 2, 3)&lt;/code&gt;. This generates:&lt;/p&gt;
&lt;pre class=&quot;language-nasm&quot;&gt;
&lt;code class=&quot;language-nasm&quot;&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rdi&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x35ac082126ad&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; Load the function object &amp;lt;JSFunction add42&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token comment&quot;&gt;;; Push receiver and arguments 1, 2 and 3 (reversed)&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x6&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x4&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token number&quot;&gt;0x2&lt;/span&gt;&lt;br/&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x35ac082030d1&lt;/span&gt;    &lt;span class=&quot;token comment&quot;&gt;;; &amp;lt;JSGlobal Object&amp;gt;&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rax&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x3&lt;/span&gt;               &lt;span class=&quot;token comment&quot;&gt;;; Save the arguments count in rax&lt;/span&gt;&lt;br/&gt;movl &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token operator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token register variable&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0x17&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;token comment&quot;&gt;;; Load function object {Code} field in rcx&lt;/span&gt;&lt;br/&gt;call &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;;; Finally, call the code object!&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What about the epilogue of the function? We are not going back to the arguments adaptor trampoline anymore, so the epilogue is indeed a bit more complex than before.&lt;/p&gt;
&lt;pre class=&quot;language-nasm&quot;&gt;
&lt;code class=&quot;language-nasm&quot;&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token operator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token register variable&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0x18&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;token comment&quot;&gt;;; Load the argument count (from callee frame) to rcx&lt;/span&gt;&lt;br/&gt;movq &lt;span class=&quot;token register variable&quot;&gt;rsp&lt;/span&gt;,&lt;span class=&quot;token register variable&quot;&gt;rbp&lt;/span&gt;               &lt;span class=&quot;token comment&quot;&gt;;; Pop out callee frame&lt;/span&gt;&lt;br/&gt;pop &lt;span class=&quot;token register variable&quot;&gt;rbp&lt;/span&gt;&lt;br/&gt;cmpq &lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;,&lt;span class=&quot;token number&quot;&gt;0x0&lt;/span&gt;               &lt;span class=&quot;token comment&quot;&gt;;; Compare arguments count with formal parameter count&lt;/span&gt;&lt;br/&gt;jg &lt;span class=&quot;token number&quot;&gt;0x35ac000840c6&lt;/span&gt;  &lt;span class=&quot;token operator&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0x86&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&amp;gt;&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token comment&quot;&gt;;; If arguments count is smaller (or equal) than the formal parameter count:&lt;/span&gt;&lt;br/&gt;ret &lt;span class=&quot;token number&quot;&gt;0x8&lt;/span&gt;                    &lt;span class=&quot;token comment&quot;&gt;;; Return as usual (parameter count is statically known)&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;token comment&quot;&gt;;; If we have more arguments in the stack than formal parameters:&lt;/span&gt;&lt;br/&gt;pop &lt;span class=&quot;token register variable&quot;&gt;r10&lt;/span&gt;                    &lt;span class=&quot;token comment&quot;&gt;;; Save the return address&lt;/span&gt;&lt;br/&gt;leaq &lt;span class=&quot;token register variable&quot;&gt;rsp&lt;/span&gt;,&lt;span class=&quot;token operator&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token register variable&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token register variable&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0x8&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;]&lt;/span&gt;   &lt;span class=&quot;token comment&quot;&gt;;; Pop all arguments according to rcx&lt;/span&gt;&lt;br/&gt;push &lt;span class=&quot;token register variable&quot;&gt;r10&lt;/span&gt;                   &lt;span class=&quot;token comment&quot;&gt;;; Recover the return address&lt;/span&gt;&lt;br/&gt;retl&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;The arguments adaptor frame was an ad-hoc solution to calls with a mismatch number of arguments and formal parameters. It was a straightforward solution, but it came with high performance cost and added complexity to the codebase. The performance cost is nowadays exacerbated by many Web frameworks using this feature to create a more flexible API. The simple idea of reversing the arguments in the stack allowed a significant reduction in implementation complexity and removed almost the entire overhead for such calls.&lt;/p&gt;
</description>
<pubDate>Mon, 15 Feb 2021 15:33:57 +0000</pubDate>
<dc:creator>feross</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://v8.dev/blog/adaptor-frame</dc:identifier>
</item>
<item>
<title>Ask HN: Why are e-ink note-taking devices so expensive compared to iPads?</title>
<link>https://news.ycombinator.com/item?id=26143407</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=26143407</guid>
<description>&lt;tr readability=&quot;0.58823529411765&quot;&gt;&lt;td bgcolor=&quot;#FF6600&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr id=&quot;pagespace&quot; title=&quot;Ask HN: Why are e-ink note-taking devices so expensive compared to iPads?&quot;&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;9.9520451339915&quot;&gt;&lt;td&gt;
&lt;table class=&quot;fatitem&quot; border=&quot;0&quot; readability=&quot;6.6346967559944&quot;&gt;&lt;tr class=&quot;athing&quot; id=&quot;26143407&quot; readability=&quot;0&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;title&quot;/&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;title&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=26143407&quot; class=&quot;storylink&quot;&gt;Ask HN: Why are e-ink note-taking devices so expensive compared to iPads?&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.72&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td class=&quot;subtext&quot;&gt;&lt;span class=&quot;score&quot; id=&quot;score_26143407&quot;&gt;286 points&lt;/span&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=behnamoh&quot; class=&quot;hnuser&quot;&gt;behnamoh&lt;/a&gt; &lt;span class=&quot;age&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=26143407&quot;&gt;9 hours ago&lt;/a&gt;&lt;/span&gt; &lt;span id=&quot;unv_26143407&quot;/&gt; | &lt;a href=&quot;https://news.ycombinator.com/hide?id=26143407&amp;amp;goto=item%3Fid%3D26143407&quot;&gt;hide&lt;/a&gt; | &lt;a href=&quot;https://hn.algolia.com/?query=Ask%20HN%3A%20Why%20are%20e-ink%20note-taking%20devices%20so%20expensive%20compared%20to%20iPads%3F&amp;amp;sort=byDate&amp;amp;dateRange=all&amp;amp;type=story&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0&quot; class=&quot;hnpast&quot;&gt;past&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/fave?id=26143407&amp;amp;auth=2ede406587d7f8563faaf9e1ca8b79621348fb3d&quot;&gt;favorite&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/item?id=26143407&quot;&gt;225 comments&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17.5&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td readability=&quot;14&quot;&gt;Looking at decent e-ink devices to write notes on, I cannot understand why they're so expensive. For the price of a remarkable 2, for instance, you could buy an iPad which supports note-taking, and much more.
&lt;p&gt;I know there are some advantages to e-ink displays, but I don't think that's enough to justify the high price tag on these readers/note-taking devices.&lt;/p&gt;
&lt;p&gt;There's also hidden costs involved, such as buying new tips for the e-ink pencils.&lt;/p&gt;
&lt;p&gt;Has the e-ink industry reached a dead-end where patents are impeding progress, or are there other reasons involved?&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.71604938271605&quot;&gt;&lt;td&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;10&quot; width=&quot;0&quot;/&gt;&lt;br/&gt;&lt;center&gt;&lt;span class=&quot;yclinks&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/newsguidelines.html&quot;&gt;Guidelines&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/newsfaq.html&quot;&gt;FAQ&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/lists&quot;&gt;Lists&lt;/a&gt; | &lt;a href=&quot;https://github.com/HackerNews/API&quot;&gt;API&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/security.html&quot;&gt;Security&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/legal/&quot;&gt;Legal&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/apply/&quot;&gt;Apply to YC&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Contact&lt;/a&gt;&lt;/span&gt;
&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Mon, 15 Feb 2021 15:11:00 +0000</pubDate>
<dc:creator>behnamoh</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=26143407</dc:identifier>
</item>
<item>
<title>Running Nomad for a Home Server</title>
<link>https://mrkaran.dev/posts/home-server-nomad/</link>
<guid isPermaLink="true" >https://mrkaran.dev/posts/home-server-nomad/</guid>
<description>&lt;p&gt;It's been a long time since I've written a post on Hydra (my home server). I use Hydra as a testbed to learn new tools, workflows and it just gives me joy to self-host applications while learning something in return.&lt;/p&gt;&lt;h2 id=&quot;history&quot;&gt;History&lt;/h2&gt;
&lt;p&gt;A brief history of how &lt;a href=&quot;https://github.com/mr-karan/hydra&quot;&gt;Hydra's&lt;/a&gt; setup evolved over time:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mrkaran.dev/posts/home-server-setup/&quot;&gt;2019&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;A pretty minimal K3s setup deployed on 2 RPi4 nodes. I couldn't continue with this setup because:
&lt;ul&gt;&lt;li&gt;Some of the apps didn't have ARM-based image (this was 2019, pre M1 hype era).&lt;/li&gt;
&lt;li&gt;Didn't want to risk deploying persistent workloads on RPi.&lt;/li&gt;
&lt;li&gt;A lot of tooling to deploy workloads was missing (storing env variables for eg.).&lt;/li&gt;
&lt;li&gt;It was so boring to write YAML (that I also did at work). Didn't give me joy.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://mrkaran.dev/posts/home-server-updates/&quot;&gt;2020 First Half&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;RPi 2x Nodes + K3s + DO Droplet. Tailscale for networking.
&lt;ul&gt;&lt;li&gt;This was a considerable step up from the previous setup. I deployed a DO node and added &lt;a href=&quot;https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/&quot;&gt;Node Labels&lt;/a&gt; to deploy persistent workloads on DO Node only.&lt;/li&gt;
&lt;li&gt;I used my own tooling &lt;a href=&quot;https://github.com/mr-karan/kubekutr/&quot;&gt;Kubekutr&lt;/a&gt; + Kustomize which helped with version control of my configs.&lt;/li&gt;
&lt;li&gt;Took quite a bit of time to onboard new services. Got lazy, didn't host much apart from initial 3-4 applications.&lt;/li&gt;
&lt;li&gt;Writing long YAMLs. No joy.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;2020 Second Half:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Single node on DO. Terraform for deploying Docker containers.
&lt;ul&gt;&lt;li&gt;I believe the third iteration nailed it for me. I kept the setup super simple, used Terraform for deploying workloads as Docker containers.&lt;/li&gt;
&lt;li&gt;Used Terraform extensively for setting up the node, Cloudflare records, DO firewall rules.&lt;/li&gt;
&lt;li&gt;Time to onboard new services reduced from a couple of hours to a few minutes. This was a huge win for me. I deployed around 10-15 new services to try it out on the server directly.&lt;/li&gt;
&lt;li&gt;Writing HCL is actually a much better experience than YAML.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;why-nomad&quot;&gt;Why Nomad&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://mrkaran.dev/images/nomad-hydra.png&quot; alt=&quot;image&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Around a month back, &lt;a href=&quot;https://nadh.in/&quot;&gt;Kailash&lt;/a&gt; had asked about feedback on &lt;a href=&quot;https://www.nomadproject.io/&quot;&gt;Nomad&lt;/a&gt;. We at &lt;a href=&quot;https://zerodha.com/&quot;&gt;Zerodha&lt;/a&gt; (India's largest stock broker) are evaluating it to migrate our services to Nomad from Kubernetes (more on this later). It was almost 2 years since I last saw Nomad so it was definitely worth re-evaluating (esp since it hit 1.0 recently). I wanted to try out Nomad to answer a personal curiosity: &lt;em&gt;What does it do differently than Kubernetes?&lt;/em&gt; No better way than actually getting hands dirty, right?!&lt;/p&gt;
&lt;p&gt;After following the brief tutorials from the &lt;a href=&quot;https://learn.hashicorp.com/nomad&quot;&gt;official website&lt;/a&gt; I felt confident to try it for actual workloads. In my previous setup, I was hosting quite a few applications (Pihole, Gitea, Grafana etc) and thought it'll be a nice way to learn how Nomad works by deploying the same services in the Nomad cluster. And I came in with zero expectations, I already had a nice setup which was reliable and running for me. My experience with a local Nomad cluster was joyful, I was able to quickly go from 0-&amp;gt;1 in less than 30 minutes. This BTW is a strong sign of how easy Nomad is to get started with as compared to K8s. The sheer amount of different concepts you've to register in your mind before you can even deploy a single container in a K8s cluster is bizarre. Nomad takes the easy way out here and simplified the concepts for developers into just three things:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;job
  \_ group
        \_ task
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;Job: Job is a collection of different groups. Job is where the constraints for type of scheduler, update strategies and ACL is placed.&lt;/li&gt;
&lt;li&gt;Group: Group is a collection of different tasks. A group is always executed on the same Nomad client node. You'll want to use Groups for use-cases like a logging sidecar, reverse proxies etc.&lt;/li&gt;
&lt;li&gt;Task: Atomic unit of work. A task in Nomad can be running a container/binary/Java VM etc, defining the mount points, env variables, ports to be exposed etc.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If you're coming from K8s you can think of Task as a Pod and Group as a Replicaset. There's no equivalent to Job in K8s. BUT! The coolest part? You don't have to familiarise yourself with all different types of Replicasets (Deployments, Daemonsets, Statefulsets) and different ways of configuring them.&lt;/p&gt;
&lt;p&gt;Want to make a normal job as a periodic job in Nomad? Simply add the following block to your existing Job:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;periodic {
  cron = &quot;@daily&quot;
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You want to make a service run as a batch job (on all Nomad nodes -- the equivalent of Daemonset in K8s)? Simply make the following change to your existing job:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;-type=&quot;service&quot;
&lt;/span&gt;&lt;span&gt;+type=&quot;batch&quot;
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You see &lt;strong&gt;this&lt;/strong&gt; is what I mean by the focus on UX. There are many many such examples which will leave a nice smile on your face if you're coming from K8s background.&lt;/p&gt;
&lt;p&gt;I'd recommend reading &lt;a href=&quot;https://www.nomadproject.io/docs/internals/architecture&quot;&gt;Internal Architecture&lt;/a&gt; of Nomad if you want to understand this in-depth.&lt;/p&gt;
&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Tech stack for Hydra:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Tailscale VPN: Serves as a mesh layer between my laptop/mobile and DO server. Useful for exposing internal services.&lt;/li&gt;
&lt;li&gt;Caddy for reverse proxying and automatic SSL setup for all services. I run 2 instances of Caddy:
&lt;ul&gt;&lt;li&gt;Internal: Listens on Tailscale Network Interface. Reverse proxies all private services.&lt;/li&gt;
&lt;li&gt;Public: Listens on DO's Public IPv4 network interface. Reverse proxies all public-facing services.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Terraform: Primary component to have IaC (Infra as Code). Modules to manage:
&lt;ul&gt;&lt;li&gt;Cloudflare DNS Zone and Records&lt;/li&gt;
&lt;li&gt;DO Droplet, Firewall rules, SSH Keys, Floating IPs etc.&lt;/li&gt;
&lt;li&gt;Nomad Jobs. Used for running workloads after templating env variables, config files in Nomad job files.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;complexity-of-nomad-vs-kubernetes&quot;&gt;Complexity of Nomad vs Kubernetes&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/mrkaran_/status/1268762357355823104&quot;&gt;&lt;img src=&quot;https://mrkaran.dev/images/k8s-meme.jpeg&quot; alt=&quot;image&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Nomad shines because it follows the UNIX philosophy of &quot;Make each program do one thing well&quot;. To put simply, Nomad is &lt;em&gt;just&lt;/em&gt; a workload orchestrator. It only is concerned about things like Bin Packing, scheduling decisions.&lt;/p&gt;
&lt;p&gt;If you're running heterogeneous workloads, running a server (or a set of servers) quickly becomes expensive. Hence orchestrators tend to make sense in this context. They tend to save costs by making it efficient to run a vast variety of workloads. This is all an orchestrator has to do really.&lt;/p&gt;
&lt;p&gt;Nomad doesn't interfere in your DNS setup, Service Discovery, secrets management mechanisms and pretty much anything else. If you read some of the posts at &lt;a href=&quot;https://k8s.af/&quot;&gt;Kubernetes Failure Stories&lt;/a&gt;, the most common reason for outages is Networking (DNS, ndots etc). A lot of marketing around K8s never talks about these things.&lt;/p&gt;
&lt;p&gt;I always maintain &quot;Day 0 is easy, Day N is the real test of your skills&quot;. Anyone can deploy a workload to a K8s cluster, it's always the Day N operations which involve debugging networking drops, mysterious container restarts, proper resource allocations and other such complex issues that require real skills &lt;strong&gt;and&lt;/strong&gt; effort. It's not as easy as &lt;code&gt;kubectl apply -f&lt;/code&gt; and my primary gripe is with people who miss out on this in their &quot;marketing&quot; pitches (obvious!).&lt;/p&gt;
&lt;h2 id=&quot;when-to-use-nomad&quot;&gt;When to use Nomad&lt;/h2&gt;
&lt;p&gt;Nomad hits the sweet spot of being operationally easy and functional. Nomad is a great choice if you want to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Run not just containers but other forms of workloads.&lt;/li&gt;
&lt;li&gt;Increase developer productivity by making it easier to deploy/onboard new services.&lt;/li&gt;
&lt;li&gt;Consistent experience of deployment by testing the deployments locally.&lt;/li&gt;
&lt;li&gt;(Not joking) You are tired of running Helm charts or writing large YAML manifests. The config syntax for Nomad jobs is human friendly and easy to grasp.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Nomad is available as a single binary. If you want to try it locally, all you need is &lt;code&gt;sudo nomad agent -dev&lt;/code&gt; and you'll have a Nomad Server, Client running in dev mode along with a UI. This makes it easy for the developers to test out the deployments locally because there's very little configuration difference between this and production deployment. Not to forget it's super easy to self-host Nomad clusters. I'm yet to meet anyone who self hosts K8s clusters in production without a dedicated team babysitting it always.&lt;/p&gt;
&lt;p&gt;Once you eliminate the &quot;blackbox&quot; components from your stack, life becomes easier for everyone.&lt;/p&gt;
&lt;h2 id=&quot;when-to-not-use-nomad&quot;&gt;When to not use Nomad&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;If you're relying on custom controllers and operators. &lt;a href=&quot;https://kubernetes.io/docs/concepts/extend-kubernetes/operator/&quot;&gt;Operator Pattern&lt;/a&gt; is a new way of managing large complex distributed systems (like databases, job queues etc). There are a lot of community built operators which help in reducing the effort to run these services. However, all of these are tied deeply into the &quot;Kubernetes&quot; ecosystem. If you find yourself running any of such operators, it'll be tough (not impossible) to translate the same in Nomad ecosystem.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I &lt;em&gt;genuinely&lt;/em&gt; cannot think of any other reason to not use Nomad!&lt;/p&gt;
&lt;h2 id=&quot;practical-scenarios&quot;&gt;Practical Scenarios&lt;/h2&gt;
&lt;p&gt;Since I migrated a couple of workloads from my DO docker containers setup to Nomad, I'd demonstrate a few use cases which might be helpful if you want to start migrating your services to Nomad&lt;/p&gt;
&lt;h3 id=&quot;accessing-a-web-service-with-reverse-proxy&quot;&gt;Accessing a Web service with Reverse Proxy&lt;/h3&gt;
&lt;p&gt;Context: I'm running Caddy as a reverse proxy for all the services. Since we discussed earlier, Nomad &lt;strong&gt;only&lt;/strong&gt; is concerned about scheduling, so how exactly do you do Service Discovery? You need Consul (or something like Consul, Nomad has no hard restrictions) to register a service name with it's IP Address. Here's how you can do that:&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;.task&lt;/code&gt; section of your Nomad job spec, you need to register the service name with the port you're registering and additional tags as metadata (optional):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;service {
  name = &quot;gitea-web&quot;
  tags = [&quot;gitea&quot;, &quot;web&quot;]
  port = &quot;http&quot;
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Nomad's &lt;a href=&quot;https://www.nomadproject.io/docs/job-specification/template&quot;&gt;template&lt;/a&gt; uses &lt;code&gt;consul-template&lt;/code&gt; behind the scenes. This is a small utility which continuously watches for Consul/Vault keys and provides the ability to reload/restart your workloads if any of those keys change. It can also be used to &lt;em&gt;discover&lt;/em&gt; the address of the service registered in Consul. So here's an example of &lt;code&gt;Caddyfile&lt;/code&gt; using Consul Template functions to pull the IP address of the upstream &lt;code&gt;gitea-web&lt;/code&gt; service:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;git.mrkaran.dev {
    {{ range service &quot;gitea-web&quot; }}
    reverse_proxy {{ .Address }}:{{ .Port }}
    {{ end }}
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When a job is submitted to Nomad, a rendered template is mounted inside the container. You can define actions on what to do when the values change. For eg on a redeployment of Gitea container, the address will most likely change. We'd like Caddy to automatically restart with the new address configured in the Caddyfile in that case:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;template {
  data = &amp;lt;&amp;lt;EOF
${caddyfile_public}
EOF

  destination = &quot;configs/Caddyfile&quot; # Rendered template.

  change_mode = &quot;restart&quot;
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Using &lt;a href=&quot;https://www.nomadproject.io/docs/job-specification/template#change_mode&quot;&gt;&lt;code&gt;change_mode&lt;/code&gt;&lt;/a&gt; we can either send a &lt;code&gt;signal&lt;/code&gt; or restart the task altogether.&lt;/p&gt;
&lt;h3 id=&quot;binding-to-different-network-interfaces&quot;&gt;Binding to different network interfaces&lt;/h3&gt;
&lt;p&gt;I run a public instance of Gitea but I wanted to restrict the SSH access only to my Tailscale network. Nomad has an interesting feature &lt;a href=&quot;https://www.nomadproject.io/docs/job-specification/network#host_network&quot;&gt;&lt;code&gt;host_network&lt;/code&gt;&lt;/a&gt; which lets you bind different ports of a task on different network interfaces.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;network {
  port &quot;http&quot; {
    to = 3000
  }

  port &quot;ssh&quot; {
    to = 22

    # Need a static assignment for SSH ops.
    static = 4222

    # SSH port on the host only exposed to Tailscale IP.
    host_network = &quot;tailscale&quot;
  }
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;templating-env-variables&quot;&gt;Templating Env Variables&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This is &lt;strong&gt;not&lt;/strong&gt; recommended for production.&lt;/p&gt;
&lt;p&gt;Nomad doesn't have any templating functionalities, so all the config must be sourced from Consul and secrets should be sourced from Vault. However in the time constraint I had, I wanted to understand Nomad and Consul better and use Vault at a &lt;a href=&quot;https://github.com/mr-karan/hydra/blob/master/docs/SETUP.md#vault&quot;&gt;later stage&lt;/a&gt;. I needed a way to interpolate the env variables. This is where Terraform comes into picture:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;resource &quot;nomad_job&quot; &quot;app&quot; {
  jobspec = templatefile(&quot;${path.module}/conf/shynet.nomad&quot;, {
    shynet_django_secret_key   = var.shynet_django_secret_key,
    shynet_postgresql_password = var.shynet_postgresql_password
  })
  hcl2 {
    enabled = true
  }
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We can pass the variables from Terraform (which can be sourced by &lt;code&gt;TF_VAR_&lt;/code&gt; in your local env) to the Nomad job spec. Inside the job spec we can use &lt;code&gt;env&lt;/code&gt; to make it available to our task:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;env {
  DB_PASSWORD              = &quot;${shynet_postgresql_password}&quot;
  DJANGO_SECRET_KEY        = &quot;${shynet_django_secret_key}&quot;
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;running-a-backup-job-on-the-host&quot;&gt;Running a backup job on the host&lt;/h3&gt;
&lt;p&gt;I use &lt;code&gt;restic&lt;/code&gt; to take periodic backups of my server and upload to Backblaze B2. Since Nomad supports running tasks as a different isolated environment (&lt;code&gt;chroot&lt;/code&gt;) using &lt;code&gt;exec&lt;/code&gt; driver and even without isolation using &lt;code&gt;raw_exec&lt;/code&gt; driver, I wanted to give that a try. I've to resort using &lt;code&gt;raw_exec&lt;/code&gt; driver here because &lt;code&gt;/data&lt;/code&gt; file path on my host was not available to the chroot'ed environment.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span&gt;job &quot;restic&quot; {
  datacenters = [&quot;hydra&quot;]
  type        = &quot;batch&quot;

  periodic {
    cron             = &quot;0 3 * * *&quot;
    time_zone        = &quot;Asia/Kolkata&quot;
    prohibit_overlap = true
  }
  ...
  task &quot;backup&quot; {
          driver = &quot;raw_exec&quot;

          config {
                # Since `/data` is owned by `root`, restic needs to be spawned as `root`. 

                # `raw_exec` spawns the process with which `nomad` client is running (`root` i.e.).
                command = &quot;$${NOMAD_TASK_DIR}/restic_backup.sh&quot;
          }
  }
  ...
}
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You can follow the rest of the config &lt;a href=&quot;https://github.com/mr-karan/hydra/blob/master/terraform/modules/restic/conf/restic.nomad&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;scope-of-improvements&quot;&gt;Scope of Improvements&lt;/h2&gt;
&lt;p&gt;Nomad has been an absolute joy to work with. However, I've spotted a few rough edge cases which I believe one should be aware of:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;host_network&lt;/code&gt; property sometimes gets ignored when doing a modification to &lt;code&gt;service&lt;/code&gt;. I've opened an &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/10001&quot;&gt;issue&lt;/a&gt; upstream but looks like other people are facing similar behaviours &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/10016&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/9006&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;host_network&lt;/code&gt; as of present &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/8577&quot;&gt;cannot&lt;/a&gt; bind to a floating IP address (DigitalOcean/GCP etc). I've to resort to using my droplet's public IPv4 address for now.&lt;/li&gt;
&lt;li&gt;I tried using Consul Connect (service mesh with mTLS) but looks like again because of &lt;code&gt;host_network&lt;/code&gt;, I'm &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/9683&quot;&gt;unable&lt;/a&gt; to use it.&lt;/li&gt;
&lt;li&gt;Nomad CLI can definitely be &lt;a href=&quot;https://github.com/hashicorp/nomad/issues/9441&quot;&gt;improved&lt;/a&gt; for a much more consistent experience. I particularly missed using &lt;code&gt;kubectl&lt;/code&gt; when using &lt;code&gt;nomad&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;That apart, I ended up sending a &lt;a href=&quot;https://github.com/hashicorp/nomad/pull/10026&quot;&gt;PR&lt;/a&gt; to upstream addressing a CLI arg ordering issue.&lt;/p&gt;
&lt;h3 id=&quot;gotchas&quot;&gt;Gotchas:&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;On a Nomad server &lt;em&gt;already&lt;/em&gt; bootstrapped, if you try changing &lt;code&gt;server.bind_addr&lt;/code&gt;, it won't have any effect. I almost pulled my hair debugging this, ultimately deleting the &lt;code&gt;data_dir&lt;/code&gt; of the server resolved the issue for me.&lt;/li&gt;
&lt;li&gt;I'm running DB &lt;em&gt;and&lt;/em&gt; the App together as a single &quot;group&quot; in my setup configs. Don't do this in production. Whenever you restart the job, the group will restart both the containers. The side effect of this is pretty interesting: Since we use Consul to fetch the DB Host, the app may start before the DB boots up &lt;em&gt;and&lt;/em&gt; registers its new address with Consul. I will fix the dependency in a future version but since I'm running fewer workloads and there are automatic retries, it's okay enough for me to keep it like this.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nomad's community is pretty small compared to Kubernetes. However, the folks are super responsive on &lt;a href=&quot;https://gitter.im/hashicorp-nomad/Lobby&quot;&gt;Gitter&lt;/a&gt;, &lt;a href=&quot;https://discuss.hashicorp.com/&quot;&gt;Discourse&lt;/a&gt; and Github Issues. A few noteworthy mentions:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/the-maldridge&quot;&gt;@the-maldridge&lt;/a&gt; helped me with my doubts in Gitter.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/tgross&quot;&gt;@tgross&lt;/a&gt; who is super responsive on Github issues and does an excellent job at housekeeping the issues.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/shantanugadgil&quot;&gt;@shantanugadgil&lt;/a&gt; who is also pretty active in the community.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Nomad's ecosystem is still in its nascent stage and I believe there are a lot of contribution opportunities for folks interested in Golang, Ops, Distributed Systems to contribute to Nomad. The codebase of Nomad is approachable and there are quite a few key areas which can be contributed to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Docs: More examples, practical use cases.&lt;/li&gt;
&lt;li&gt;Nomad Job files: There are many helm charts available to follow best practices. Something similar in Nomad will definitely be interesting.&lt;/li&gt;
&lt;li&gt;Nomad Gotchas: Since K8s is widely used and has a much larger adoption, it's only natural that the failure stories of K8s are highlighted a lot. Nomad being a pretty smaller community, we need more debugging and &quot;things that went wrong&quot; reference materials. You learn more from failures than 101 setup guides :)&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;I think I'm &lt;em&gt;sold&lt;/em&gt; on Nomad. I've used Kubernetes in prod for 2 years but if you were to ask me to write a Deployment spec from scratch (without Googling/kubectl help) I won't be able to. After writing Nomad configs, I just can't think of the sheer amount of boilerplate that K8s requires to get an application running.&lt;/p&gt;
&lt;p&gt;Nomad is also a simpler piece to keep in your tech stack. Sometimes it's best to keep things simple when you don't really achieve any benefits from the complexity.&lt;/p&gt;
&lt;p&gt;Nomad offers &lt;em&gt;less&lt;/em&gt; than Kubernetes and it's a feature, not a bug.&lt;/p&gt;
&lt;p&gt;Fin!&lt;/p&gt;
</description>
<pubDate>Mon, 15 Feb 2021 12:15:32 +0000</pubDate>
<dc:creator>elliebike</dc:creator>
<og:title>Karan Sharma | Running Nomad for home server</og:title>
<og:type>website</og:type>
<og:url>https://mrkaran.dev/posts/home-server-nomad/</og:url>
<og:description>Setting up a single node Nomad and Consul server to deploy self hosted workloads.</og:description>
<og:image>https://mrkaran.dev/images/nomad-hydra.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://mrkaran.dev/posts/home-server-nomad/</dc:identifier>
</item>
<item>
<title>Spotify is letting employees work from anywhere while paying SF and NY salaries</title>
<link>https://www.businessinsider.com/spotify-unveils-new-remote-work-option-for-all-employees-2021-2</link>
<guid isPermaLink="true" >https://www.businessinsider.com/spotify-unveils-new-remote-work-option-for-all-employees-2021-2</guid>
<description>&lt;p&gt;On Friday, Spotify &lt;a href=&quot;https://newsroom.spotify.com/2021-02-12/distributed-first-is-the-future-of-work-at-spotify/&quot; data-analytics-module=&quot;body_link&quot;&gt;announced&lt;/a&gt; its new work-from-home program that lets its 6,550 global employees choose how they want to work at the company — either in an office, remotely, or at a coworking space that the company will pay a subscription for. &lt;/p&gt;
&lt;p&gt;Employees have to commit to an option for one year at a time, and have their manager's approval. But other than that, all employees need is their laptop and internet access.&lt;/p&gt;
&lt;p&gt;Executives from the streaming giant say this is the future of work. &lt;/p&gt;
&lt;p&gt;A number of major companies have shifted to more permanent remote-work policies since the coronavirus pandemic upended traditional work culture. At Facebook, employees who are &quot;experienced&quot; can work from home if their manager allows it, according to &lt;a href=&quot;https://www.cnet.com/news/the-new-work-from-home-policies-at-facebook-twitter-apple-and-more/&quot; data-analytics-module=&quot;body_link&quot;&gt;CNET&lt;/a&gt;. And at Twitter, CEO Jack Dorsey &lt;a href=&quot;https://www.buzzfeednews.com/article/alexkantrowitz/twitter-will-allow-employees-to-work-at-home-forever#:~:text=Twitter%20CEO%20Jack%20Dorsey%20emailed,the%20coronavirus%20pandemic%20lockdown%20passes.&amp;amp;text=%E2%80%9CPeople%20who%20were%20reticent%20to,that%20way%2C%E2%80%9D%20Christie%20said.&quot; data-analytics-module=&quot;body_link&quot;&gt;sent an email&lt;/a&gt; to employees saying that workers who don't have to work in an office to perform their job function can work remotely. &lt;a href=&quot;https://www.sfgate.com/local/article/Salesforce-announces-permanent-remote-work-for-15937901.php&quot; data-analytics-module=&quot;body_link&quot;&gt;Salesforce&lt;/a&gt; recently unveiled it's own work-from-anywhere policy for employees, too. &lt;/p&gt;


&lt;p&gt;Spotify will continue to pay at San-Francisco or New York salary rates, based on the type of job. This is in stark contrast to &lt;a href=&quot;https://www.cnbc.com/2020/05/21/zuckerberg-50percent-of-facebook-employees-could-be-working-remotely.html&quot; data-analytics-module=&quot;body_link&quot;&gt;Facebook&lt;/a&gt; and &lt;a href=&quot;https://seekingalpha.com/news/3613425-vmware-twitter-cut-pay-for-remote-workers-who-relocate-update&quot; data-analytics-module=&quot;body_link&quot;&gt;Twitter&lt;/a&gt;, who have both said they will cut employee salaries for those who choose to relocate away from their offices in the expensive San Francisco Bay Area.&lt;/p&gt;
&lt;p&gt;Spotify will also be redesigning many of its offices to accommodate employees' preferences for quiet spaces, shared-desk spaces, and lounging areas. &lt;/p&gt;
&lt;div class=&quot;lazy-holder&quot;&gt;&lt;img class=&quot;lazy-image&quot; src=&quot;data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201%201'%3E%3C/svg%3E&quot; data-content-type=&quot;image/jpeg&quot; data-srcs=&quot;{&amp;quot;https://i.insider.com/6026ada42edd0f001a8d587e&amp;quot;:{&amp;quot;contentType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;aspectRatioW&amp;quot;:2400,&amp;quot;aspectRatioH&amp;quot;:1843}}&quot; alt=&quot;Spotify Meeting Area&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://i.insider.com/6026ada42edd0f001a8d587e?width=600&amp;amp;format=jpeg&amp;amp;auto=webp&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;
&lt;span class=&quot;image-source-caption&quot;&gt; Spotify's Singapore lounge offers a glimpse at what more of the company's offices might look like, complete with new collaboration spaces and lounge areas. &lt;span class=&quot;image-source headline-regular&quot; data-e2e-name=&quot;image-source&quot;&gt;Spotify&lt;/span&gt;&lt;/span&gt;
&lt;p&gt;Spotify's Travis Robinson, head of diversity, inclusion, and belonging, said the move will promote work-life balance, employee happiness, and inclusion. &lt;/p&gt;
&lt;p&gt;For one, it's going to help the company attract talent regardless of geographic location. &lt;/p&gt;
&lt;div class=&quot;lazy-holder&quot;&gt;&lt;img class=&quot;lazy-image&quot; src=&quot;data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201%201'%3E%3C/svg%3E&quot; data-content-type=&quot;image/png&quot; data-srcs=&quot;{&amp;quot;https://i.insider.com/6026aa412edd0f001a8d585d&amp;quot;:{&amp;quot;contentType&amp;quot;:&amp;quot;image/png&amp;quot;,&amp;quot;aspectRatioW&amp;quot;:1506,&amp;quot;aspectRatioH&amp;quot;:1129}}&quot; alt=&quot;Travis Robinson, Spotify&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://i.insider.com/6026aa412edd0f001a8d585d?width=600&amp;amp;format=jpeg&amp;amp;auto=webp&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;
&lt;span class=&quot;image-source-caption&quot;&gt; Travis Robinson is the global head of diversity, inclusion and belonging at Spotify. &lt;span class=&quot;image-source headline-regular&quot; data-e2e-name=&quot;image-source&quot;&gt;Jonathan C. Ward, courtesy of Spotify&lt;/span&gt;&lt;/span&gt;
&lt;p&gt;&quot;Most of our offices are in large cities like New York, London and Stockholm, but we know that moving to or staying in these cities isn't always realistic – or attractive – to potential employees,&quot; Robinson said. &lt;/p&gt;


&lt;p&gt;He offered the example of an employee who needs to move back home to be a caregiver for an ailing parent.&lt;/p&gt;
&lt;p&gt;&quot;This is an opportunity to scrap the idea that big cities are the only places where meaningful work can happen because we know first-hand that isn't true. We want employees to come as they are, wherever they are and whatever their circumstances are,&quot; he said.  &lt;/p&gt;
&lt;p&gt;The new program will also promote pay equity. &lt;/p&gt;
&lt;p&gt;&quot;Black employees historically have been discriminated against when it comes to pay and growth opportunity, and it is likely the local market pay is lower than a comparable city with a large white population,&quot; Robinson said. &lt;/p&gt;


&lt;p&gt;In other words, local-market rates are no longer a disadvantage for people living in non-major markets. &lt;/p&gt;
&lt;p&gt;Of course, there will be challenges managing a hybrid workforce of in-person and virtual workers. But Robinson said the pandemic has given the streaming service experience in making sure virtual workers feel included in meetings, employee resource groups, and company events. &lt;/p&gt;
&lt;div class=&quot;lazy-holder&quot;&gt;&lt;img class=&quot;lazy-image&quot; src=&quot;data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201%201'%3E%3C/svg%3E&quot; data-content-type=&quot;image/jpeg&quot; data-srcs=&quot;{&amp;quot;https://i.insider.com/6026ae20b3c2a800183cc9bf&amp;quot;:{&amp;quot;contentType&amp;quot;:&amp;quot;image/jpeg&amp;quot;,&amp;quot;aspectRatioW&amp;quot;:4,&amp;quot;aspectRatioH&amp;quot;:3}}&quot; alt=&quot;Spotify Lounge&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://i.insider.com/6026ae20b3c2a800183cc9bf?width=600&amp;amp;format=jpeg&amp;amp;auto=webp&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;
&lt;span class=&quot;image-source-caption&quot;&gt; Spotify's executive team said it would poll employees on what type of in-office features employees want. &lt;span class=&quot;image-source headline-regular&quot; data-e2e-name=&quot;image-source&quot;&gt;Spotify&lt;/span&gt;&lt;/span&gt;
&lt;p&gt;According to Robinson, Spotify's new work setup will not only benefit its employees, but it's listeners, indirectly. &lt;/p&gt;
&lt;p&gt;&quot;With even more diverse experiences and perspectives, spread across additional communities, we have the opportunity to bring more stories to life, through original content and other curated audio experiences that resonate culturally,&quot; he said. &lt;/p&gt;
</description>
<pubDate>Mon, 15 Feb 2021 10:54:10 +0000</pubDate>
<dc:creator>ivanche</dc:creator>
<og:title>Spotify is letting employees work from anywhere — while still paying San Francisco and New York salaries</og:title>
<og:description>Unlike other tech giants, the steaming service said it won't cut worker's pay if they decide to move away from major cities.</og:description>
<og:type>article</og:type>
<og:url>https://www.businessinsider.com/spotify-unveils-new-remote-work-option-for-all-employees-2021-2</og:url>
<og:image>https://i.insider.com/6026a99a2edd0f001a8d5856?width=1200&amp;format=jpeg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.businessinsider.com/spotify-unveils-new-remote-work-option-for-all-employees-2021-2</dc:identifier>
</item>
<item>
<title>Obfuscated Tiny C Compiler</title>
<link>https://bellard.org/otcc/</link>
<guid isPermaLink="true" >https://bellard.org/otcc/</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;title&gt;OTCC : Obfuscated Tiny C Compiler&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;84.807859436992&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;h2&gt;What is it ?&lt;/h2&gt;
&lt;p&gt;The Obfuscated Tiny C Compiler (OTCC) is a very small C compiler I wrote in order to win the &lt;a href=&quot;http://www.ioccc.org&quot;&gt;International Obfuscated C Code Contest&lt;/a&gt; (IOCCC) in 2002.&lt;/p&gt;
&lt;p&gt;My goal was to write the smallest C compiler which is able to compile itself. I choose a subset of C which was general enough to write a small C compiler. Then I extended the C subset until I reached the maximum size authorized by the contest: 2048 bytes of C source excluding the ';', '{', '}' and space characters.&lt;/p&gt;
&lt;p&gt;I choose to generate i386 code. The original OTCC code could only run on i386 Linux because it relied on endianness and unaligned access. It generated the program in memory and launched it directly. External symbols were resolved with &lt;tt&gt;dlsym()&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;In order to have a portable version of OTCC, I made a variant called OTCCELF. It is only a little larger than OTCC, but it generates directly a &lt;em&gt;dynamically linked i386 ELF executable&lt;/em&gt; from a C source without relying on any binutils tools! OTCCELF was tested succesfully on i386 Linux and on Sparc Solaris.&lt;/p&gt;
&lt;p&gt;NOTE: My other project &lt;a href=&quot;https://bellard.org/tcc/&quot;&gt;TinyCC&lt;/a&gt; which is a fully featured ISOC99 C compiler was written by starting from the source code of OTCC !&lt;/p&gt;
&lt;h2&gt;Download&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Original OTCC version (runs &lt;em&gt;only&lt;/em&gt; on i386 Linux): &lt;a href=&quot;https://bellard.org/otcc/otcc.c&quot;&gt;otcc.c&lt;/a&gt; (link it with &lt;tt&gt;-ldl&lt;/tt&gt;).&lt;/li&gt;
&lt;li&gt;OTCC with i386 ELF output (should be portable): &lt;a href=&quot;https://bellard.org/otcc/otccelf.c&quot;&gt;otccelf.c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Example of C program that can be compiled: &lt;a href=&quot;https://bellard.org/otcc/otccex.c&quot;&gt;otccex.c&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;[New]&lt;/strong&gt; The non-obfuscated versions are finally available: &lt;a href=&quot;https://bellard.org/otcc/otccn.c&quot;&gt;otccn.c&lt;/a&gt; and &lt;a href=&quot;https://bellard.org/otcc/otccelfn.c&quot;&gt;otccelfn.c&lt;/a&gt;. These non-obfuscated versions do not self compile. They are provided for documentation purpose.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Compilation:&lt;/p&gt;
&lt;pre&gt;
gcc -O2 otcc.c -o otcc -ldl
gcc -O2 otccelf.c -o otccelf 
&lt;/pre&gt;
&lt;p&gt;Self-compilation:&lt;/p&gt;
&lt;pre&gt;
./otccelf otccelf.c otccelf1
&lt;/pre&gt;
&lt;p&gt;As a test, here are the executables generated by OTCCELF: &lt;a href=&quot;https://bellard.org/otcc/otcc1&quot;&gt;otcc1&lt;/a&gt;, &lt;a href=&quot;https://bellard.org/otcc/otccelf1&quot;&gt;otccelf1&lt;/a&gt;, &lt;a href=&quot;https://bellard.org/otcc/otccex1&quot;&gt;otccex1&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;C Subset Definition&lt;/h2&gt;
&lt;p&gt;Read joint example otccex.c to have an example of C program.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Expressions:
&lt;ul&gt;&lt;li&gt;binary operators, by decreasing priority order: '*' '/' '%', '+' '-', '&amp;gt;&amp;gt;' '&amp;lt;&amp;lt;', '&amp;lt;' '&amp;lt;=' '&amp;gt;' '&amp;gt;=', '==' '!=', '&amp;amp;', '^', '|', '=', '&amp;amp;&amp;amp;', '||'.&lt;/li&gt;
&lt;li&gt;'&amp;amp;&amp;amp;' and '||' have the same semantics as C : left to right evaluation and early exit.&lt;/li&gt;
&lt;li&gt;Parenthesis are supported.&lt;/li&gt;
&lt;li&gt;Unary operators: '&amp;amp;', '*' (pointer indirection), '-' (negation), '+', '!', '~', post fixed '++' and '--'.&lt;/li&gt;
&lt;li&gt;Pointer indirection ('*') only works with explicit cast to 'char *', 'int *' or 'int (*)()' (function pointer).&lt;/li&gt;
&lt;li&gt;'++', '--', and unary '&amp;amp;' can only be used with variable lvalue (left value).&lt;/li&gt;
&lt;li&gt;'=' can only be used with variable or '*' (pointer indirection) lvalue.&lt;/li&gt;
&lt;li&gt;Function calls are supported with standard i386 calling convention. Function pointers are supported with explicit cast. Functions can be used before being declared.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Types: only signed integer ('int') variables and functions can be declared. Variables cannot be initialized in declarations. Only old K&amp;amp;R function declarations are parsed (implicit integer return value and no types on arguments).&lt;/li&gt;
&lt;li&gt;Any function or variable from the libc can be used because OTCC uses the libc dynamic linker to resolve undefined symbols.&lt;/li&gt;
&lt;li&gt;Instructions: blocks ('{' '}') are supported as in C. 'if' and 'else' can be used for tests. The 'while' and 'for' C constructs are supported for loops. 'break' can be used to exit loops. 'return' is used for the return value of a function.&lt;/li&gt;
&lt;li&gt;Identifiers are parsed the same way as C. Local variables are handled, but there is no local name space (not a problem if different names are used for local and global variables).&lt;/li&gt;
&lt;li&gt;Numbers can be entered in decimal, hexadecimal ('0x' or '0X' prefix), or octal ('0' prefix).&lt;/li&gt;
&lt;li&gt;'#define' is supported without function like arguments. No macro recursion is tolerated. Other preprocessor directives are ignored.&lt;/li&gt;
&lt;li&gt;C Strings and C character constants are supported. Only '\n', '\&quot;', '\'' and '\\' escapes are recognized.&lt;/li&gt;
&lt;li&gt;C Comments can be used (but no C++ comments).&lt;/li&gt;
&lt;li&gt;No error is displayed if an incorrect program is given.&lt;/li&gt;
&lt;li&gt;Memory: the code, data, and symbol sizes are limited to 100KB (it can be changed in the source code).&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;OTCC Invocation&lt;/h2&gt;
&lt;p&gt;You can use OTCC by typing:&lt;/p&gt;
&lt;pre&gt;
otcc prog.c [args]...
&lt;/pre&gt;
&lt;p&gt;or by giving the C source to its standard input. 'args' are given to the 'main' function of prog.c (argv[0] is prog.c).&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;p&gt;An alternate syntax is to use it as a script interpreter: you can put&lt;/p&gt;
&lt;pre&gt;
#!/usr/local/bin/otcc
&lt;/pre&gt;
&lt;p&gt;at the beginning of your C source if you installed otcc at this place.&lt;/p&gt;
&lt;h2&gt;OTCCELF Invocation&lt;/h2&gt;
&lt;p&gt;You can use OTCCELF by typing:&lt;/p&gt;
&lt;pre&gt;
otccelf prog.c prog
chmod 755 prog
&lt;/pre&gt;
&lt;p&gt;'prog' is the name of the ELF file you want to generate.&lt;/p&gt;
&lt;p&gt;Note that even if the generated i386 code is not as good as GCC, the resulting ELF executables are much smaller for small sources. Try this program:&lt;/p&gt;
&lt;pre&gt;
#include &amp;lt;stdio.h&amp;gt;

main() 
{
    printf(&quot;Hello World\n&quot;);
    return 0;
}
&lt;/pre&gt;
&lt;p&gt;Results:&lt;/p&gt;
&lt;table border=&quot;1&quot;&gt;&lt;tr&gt;&lt;th&gt;Compiler&lt;/th&gt;
&lt;th&gt;Executable size (in bytes)&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;OTCCELF&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;424&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GCC (stripped)&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;em&gt;2448&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h2&gt;Links&lt;/h2&gt;
&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;The obfuscated OTCC and OTCCELF are public domain. The &lt;em&gt;non-obfuscated&lt;/em&gt; versions are released under a BSD-like license (read the license at the start of the source code).&lt;/p&gt;
&lt;hr /&gt;&lt;p&gt;This page is Copyright (c) 2002 Fabrice Bellard&lt;/p&gt;
&lt;hr /&gt;&lt;p&gt;Fabrice Bellard - &lt;a href=&quot;http://bellard.org/&quot;&gt;http://bellard.org/&lt;/a&gt; - &lt;a href=&quot;http://www.tinycc.org/&quot;&gt;http://www.tinycc.org/&lt;/a&gt;&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Mon, 15 Feb 2021 10:04:06 +0000</pubDate>
<dc:creator>ducktective</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://bellard.org/otcc/</dc:identifier>
</item>
<item>
<title>Why SELECT * is bad for SQL performance (2020)</title>
<link>https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/</link>
<guid isPermaLink="true" >https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;/&gt;&lt;link rel=&quot;apple-touch-icon&quot; sizes=&quot;180x180&quot; href=&quot;/apple-touch-icon.png&quot;/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;32x32&quot; href=&quot;/favicon-32x32.png&quot;/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image/png&quot; sizes=&quot;16x16&quot; href=&quot;/favicon-16x16.png&quot;/&gt;&lt;link rel=&quot;manifest&quot; href=&quot;/site.webmanifest&quot;/&gt;&lt;link rel=&quot;mask-icon&quot; href=&quot;/safari-pinned-tab.svg&quot; color=&quot;#5BBAD5&quot;/&gt;&lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#da532c&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;#ffffff&quot;/&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;Reasons why SELECT * is bad for SQL performance - Tanel Poder Consulting&quot;/&gt;&lt;meta property=&quot;og:title&quot; content=&quot;Reasons why SELECT * is bad for SQL performance - Tanel Poder Consulting&quot;/&gt;&lt;meta name=&quot;description&quot; content=&quot;Here&amp;amp;rsquo;s a list of reasons why SELECT * is bad for SQL performance, assuming that your application doesn&amp;amp;rsquo;t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application&amp;amp;rsquo;s data processing code suddenly break when a new column has been added or the column order has changed in a table? - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;Here&amp;amp;rsquo;s a list of reasons why SELECT * is bad for SQL performance, assuming that your application doesn&amp;amp;rsquo;t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application&amp;amp;rsquo;s data processing code suddenly break when a new column has been added or the column order has changed in a table? - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta itemprop=&quot;description&quot; content=&quot;Here&amp;amp;rsquo;s a list of reasons why SELECT * is bad for SQL performance, assuming that your application doesn&amp;amp;rsquo;t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application&amp;amp;rsquo;s data processing code suddenly break when a new column has been added or the column order has changed in a table? - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta property=&quot;og:description&quot; content=&quot;Here&amp;amp;rsquo;s a list of reasons why SELECT * is bad for SQL performance, assuming that your application doesn&amp;amp;rsquo;t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application&amp;amp;rsquo;s data processing code suddenly break when a new column has been added or the column order has changed in a table? - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp;amp; training.&quot;/&gt;&lt;meta content=&quot;oracle sql tuning, oracle sql performance&quot; name=&quot;keywords&quot;/&gt;&lt;title&gt;Reasons why SELECT * is bad for SQL performance | Tanel Poder Consulting&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;733.75487804878&quot;&gt;
&lt;nav&gt;&lt;hr/&gt;&lt;/nav&gt;&lt;p&gt;

&lt;h2 class=&quot;date&quot;&gt;2020-11-24&lt;/h2&gt;
&lt;/p&gt;
&lt;p&gt;Here’s a list of reasons why &lt;code&gt;SELECT *&lt;/code&gt; is bad for SQL performance, assuming that your application doesn’t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application’s data processing code suddenly break when a new column has been added or the column order has changed in a table?&lt;/p&gt;
&lt;p&gt;I’ll focus only on the SQL performance aspects in this article. I’m using examples based on Oracle, but most of this reasoning applies to other modern relational databases too.&lt;/p&gt;
&lt;h4 id=&quot;index&quot;&gt;Index&lt;/h4&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#increased-network-traffic&quot;&gt;Increased network traffic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#increased-cpu-usage-on-client-side&quot;&gt;Increased CPU usage on client side&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#some-query-plan-optimizations-not-possible&quot;&gt;Some query plan optimizations not possible&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#server-side-memory-usage&quot;&gt;Server-side memory usage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#increased-cpu-usage-on-server-side&quot;&gt;Increased CPU usage on server side&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#hard-parsingoptimization-takes-more-time&quot;&gt;Hard parsing/optimization takes more time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#cached-cursors-take-more-memory-in-shared-pool&quot;&gt;Cached cursors take more memory in shared pool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#lob-fetching&quot;&gt;LOB Fetching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h4 id=&quot;increased-network-traffic&quot;&gt;Increased network traffic&lt;/h4&gt;
&lt;p&gt;This is the most obvious effect - if you’re returning 800 columns instead of 8 columns from every row, you could end up sending 100x more bytes over the network for every query execution (your mileage may vary depending on the individual column lengths, of course). More network bytes means more network packets sent and depending on your RDBMS implementation, also more app-DB network roundtrips.&lt;/p&gt;
&lt;p&gt;Oracle can stream result data &lt;em&gt;of a single fetch call&lt;/em&gt; back to client in multiple consecutive SQL*Net packets sent out in a burst, without needing the client application to acknowledge every preceding packet first. The throughput of such bursts depends on TCP send buffer size and of course the network link bandwidth and latency. Read more about the &lt;a href=&quot;https://tanelpoder.com/2008/02/10/sqlnet-message-to-client-vs-sqlnet-more-data-to-client/&quot;&gt;SQL*Net more data to client&lt;/a&gt; wait event.&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SET AUTOT TRACE STAT
SQL&amp;gt; &lt;strong&gt;SELECT *&lt;/strong&gt; FROM soe_small.customers;

1699260 rows selected.

Elapsed: &lt;strong&gt;00:01:35.82&lt;/strong&gt;

Statistics
----------------------------------------------------------
          0  recursive calls
          0  db block gets
      45201  consistent gets
          0  physical reads
          0  redo size
  &lt;mark&gt;169926130  bytes sent via SQL*Net to client&lt;/mark&gt;
     187267  bytes received via SQL*Net from client
      16994  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
    1699260  rows processed
&lt;/pre&gt;
&lt;p&gt;It took around 1 min 35 seconds and 169 MB of data was sent from the database back to the client (roughly 100 bytes per row, on average). Interestingly, the rough row length estimate from data dictionary stats shows that an average row size ought to be 119 bytes (116 plus 3 bytes for the row header, lock byte &amp;amp; column count):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SQL&amp;gt; SELECT COUNT(*),SUM(avg_col_len) FROM dba_tab_columns 
     WHERE owner = 'SOE_SMALL' AND table_name = 'CUSTOMERS';

  COUNT(*) SUM(AVG_COL_LEN)
---------- ----------------
        16              116
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above table has only 16 columns, now let’s just select 3 columns that my application needs:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT customer_id, credit_limit, customer_since FROM soe_small.customers;

1699260 rows selected.

Elapsed: &lt;strong&gt;00:00:43.20&lt;/strong&gt;

Statistics
----------------------------------------------------------
          0  recursive calls
          0  db block gets
      45201  consistent gets
          0  physical reads
          0  redo size
   &lt;mark&gt;31883155  bytes sent via SQL*Net to client&lt;/mark&gt;
     187307  bytes received via SQL*Net from client
      16994  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
    1699260  rows processed
&lt;/pre&gt;
&lt;p&gt;So, selecting only 3 columns out of 16 has given me over 2x better query response time (1m 35sec vs 43 sec). The sqlplus &lt;em&gt;Elapsed&lt;/em&gt; metric includes the time it took to execute the query on the DB server &lt;em&gt;and&lt;/em&gt; to fetch all its records from to the client side, so the network latency, throughput and TCP send buffer configuration will affect it.&lt;/p&gt;
&lt;p&gt;Oracle can deduplicate repetitive field values within a result set of every fetch call, so if you need to fetch a lot of rows &amp;amp; columns and save network bandwidth (say, copying data from New York to Singapore over a database link), you could maximize this “compression” by ordering the query resultset by the most repetitive (least distinct values) columns that are also wide.&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT * FROM soe_small.customers 
     &lt;strong&gt;ORDER BY&lt;/strong&gt; customer_class,nls_territory,nls_language,cust_first_name;

1699260 rows selected.

Elapsed: 00:01:09.23

Statistics
----------------------------------------------------------
          0  recursive calls
          0  db block gets
      28478  consistent gets
          0  physical reads
          0  redo size
   &lt;mark&gt;65960489  bytes sent via SQL*Net to client&lt;/mark&gt;
     187334  bytes received via SQL*Net from client
      16994  SQL*Net roundtrips to/from client
          1  sorts (memory)
          0  sorts (disk)
    1699260  rows processed
&lt;/pre&gt;
&lt;p&gt;The test above is a &lt;code&gt;SELECT *&lt;/code&gt; again, sorted by a few VARCHAR2 columns that were 10-40 bytes (max) size, with lots of repetitive values. Only about 65 MB were sent by the server after its SQL*Net protocol-level deduplication. Note that the &lt;strong&gt;SQL*Net roundtrips to/from client&lt;/strong&gt; value is the same for all test runs above, this is because my fetch &lt;code&gt;arraysize&lt;/code&gt; has been set to 100 in my application. The arraysize controls how many fetch calls you end up sending over the network for data retrieval, every fetch after the 1st one requests arraysize-ful of rows to be returned regardless of how wide they are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;1699260 rows / arraysize 100 will need 16993 fetches + 1 initial single-row fetch = 16994 SQL*Net roundtrips&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;So the SQL*net roundtrips metric depends on the number of DB calls (number of fetches) sent over the network and the &lt;em&gt;bytes sent per roundtrip&lt;/em&gt; derived metric depends on both the number of rows a fetch asks for in a single DB call and also the width of these rows. The reality is slightly more complicated and depends on app client library’s behavior, but I’ll skip this part for brevity.&lt;/p&gt;
&lt;p&gt;Note that you could increase the arraysize further (from 100 to 1000 for example) and not only will you be doing less SQL*Net roundtrips (1700 instead of 16994), but the amount of bytes transferred will slightly shrink too, potentially due to better compression and slightly lower SQL*Net packet overhead. When transferring data over Oracle database links, you won’t need to increase arraysize in your client session as Oracle uses the maximum possible arraysize (~32767) for dblinks automatically.&lt;/p&gt;
&lt;h4 id=&quot;increased-cpu-usage-on-client-side&quot;&gt;Increased CPU usage on client side&lt;/h4&gt;
&lt;p&gt;The more rows you process on the client side - and the more columns (and wider columns) you have, the more CPU time it will take to process them. In my case, the &lt;em&gt;application think time&lt;/em&gt; is about extracting, formatting the records and writing them to an output file.&lt;/p&gt;
&lt;p&gt;I logged in to the Linux database server directly and am running sqlplus over a local pipe, to rule out any network/TCP overhead. The two scripts I’m running, are:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;selectstar.sql&lt;/code&gt;: Select all 16 columns:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SET ARRAYSIZE 100 TERMOUT OFF
SPOOL customers.txt
SELECT * FROM soe_small.customers;
SPOOL OFF
EXIT
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;selectsome.sql&lt;/code&gt;: Select 3 columns:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SET ARRAYSIZE 100 TERMOUT OFF
SPOOL customers.txt
SELECT customer_id, credit_limit, customer_since FROM soe_small.customers;
SPOOL OFF
EXIT
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So, let’s run &lt;code&gt;selectstar&lt;/code&gt; locally:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ time sqlplus -s system/oracle @selectstar

real   1m21.056s
user   1m3.053s
sys    0m15.736s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When adding user+sys CPU together, we get around 1m 19 seconds of CPU time, out of 1m 21s of total wall-clock elapsed time, meaning that sqlplus spent very little time sleeping, waiting for more results to arrive from the pipe. So my “application” spent 99% of its runtime in &lt;em&gt;application think time&lt;/em&gt; on the client side, burning CPU when processing the retrieved data.&lt;/p&gt;
&lt;p&gt;I confirmed this with my &lt;a href=&quot;https://tanelpoder.com/psnapper/&quot;&gt;pSnapper&lt;/a&gt; tool:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ sudo psn -G syscall,wchan -p sqlplus

Linux Process Snapper v0.18 by Tanel Poder [https://0x.tools]
Sampling /proc/syscall, stat, wchan for 5 seconds... finished.


=== Active Threads ===========================================================

 samples | avg_threads | comm      | state            | syscall   | wchan     
------------------------------------------------------------------------------
      95 |        0.95 | (sqlplus) | Running (ON CPU) | [running] | 0         
       2 |        0.02 | (sqlplus) | Running (ON CPU) | [running] | pipe_wait 
       2 |        0.02 | (sqlplus) | Running (ON CPU) | read      | 0         
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since practically all the time is spent on the client application side, there’s not much “tuning” that I can do on the database, adding indexes or increasing various database buffers won’t help as the database time is only 1% of my total runtime.&lt;/p&gt;
&lt;p&gt;But with application code changes, by fetching only the columns I need, I can drastically reduce the client processing / application think time:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ time sqlplus -s system/oracle @selectsome

real   0m4.047s
user   0m2.752s
sys    0m0.349s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Only 4 seconds total runtime, with about 3.1 seconds of it spent on CPU. Better performance, lower CPU usage!&lt;/p&gt;
&lt;p&gt;Of course your mileage may vary, depending on what kind of application you’re running and which DB client libraries you’re using. Nevertheless, when your table has 500+ columns (like many data warehouse tables tend to be like), the difference between a &lt;code&gt;SELECT *&lt;/code&gt; and &lt;code&gt;SELECT 10 columns...&lt;/code&gt; can be massive.&lt;/p&gt;
&lt;p&gt;By the way, starting from Oracle 12.2, you can use &lt;a href=&quot;https://blogs.oracle.com/opal/sqlplus-12201-adds-new-performance-features&quot;&gt;sqlplus -fast option&lt;/a&gt; to make sqlplus enable some performance options (arraysize, large output pagesize, etc):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ time sqlplus -fast -s system/oracle @selectstar

real    0m16.046s
user    0m11.851s
sys     0m1.718s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The select &lt;em&gt;star&lt;/em&gt; script now runs in only 16 seconds instead of 1 min 21 sec.&lt;/p&gt;
&lt;p&gt;And with printing the output directly to CSV, sqlplus can avoid some (column-aligned) formatting codepath, using even less CPU:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ time sqlplus -m &quot;csv on&quot; -fast -s system/oracle @selectstar

real    0m12.048s
user    0m10.144s
sys     0m0.447s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;The fast CSV unloader written by Oracle has finally arrived!&lt;/em&gt;&lt;/p&gt;
&lt;h4 id=&quot;some-query-plan-optimizations-not-possible&quot;&gt;Some query plan optimizations not possible&lt;/h4&gt;
&lt;p&gt;Oracle’s optimizer can transform your query structure into something different, but logically equivalent, if it thinks it’s good for performance. Some transformations open up additional optimization opportunities (more efficient data access paths), some even allow you to skip executing a part of your query.&lt;/p&gt;
&lt;p&gt;For example, if there happens to be an index that covers all required columns by the SQL, Oracle can do an index-only scan through the “skinny index” instead of the entire “fat” wide table. This &lt;em&gt;index fast full scan&lt;/em&gt; is not using index tree-walking, but more like a full table scan done through all the index blocks in their storage order (ignoring root &amp;amp; branch blocks).&lt;/p&gt;
&lt;p&gt;Here’s an example of the &lt;code&gt;select *&lt;/code&gt; vs &lt;code&gt;select col1, col2&lt;/code&gt; where col1,col2 happen to be in an index:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; @xi f2czqvfz3pj5w 0

SELECT * FROM soe_small.customers

---------------------------------------------------------------------------
| Id | Operation         | Name      | Starts | A-Rows | A-Time   | Reads |
---------------------------------------------------------------------------
|  0 | SELECT STATEMENT  |           |      1 |   1699K| 00:00.57 | 28475 |
|  1 |  &lt;mark&gt;TABLE ACCESS FULL&lt;/mark&gt;| CUSTOMERS |      1 |   1699K| &lt;mark&gt;00:00.57&lt;/mark&gt; | &lt;mark&gt;28475&lt;/mark&gt; |
---------------------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;The above &lt;code&gt;select *&lt;/code&gt; had to scan the table to get all its columns. Total runtime 0.57 seconds and 28475 blocks read. Now let’s just select a couple of columns that happen to be covered by a single multi-column index:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; @xi 9gwxhcvwngh96 0

SELECT customer_id, dob FROM soe_small.customers

---------------------------------------------------------------------------------------
| Id  | Operation            | Name              | Starts | A-Rows | A-Time   | Reads |
---------------------------------------------------------------------------------------
|   0 | SELECT STATEMENT     |                   |      1 |   1699K| 00:00.21 |  5915 |
|   1 |  &lt;mark&gt;INDEX FAST FULL SCAN&lt;/mark&gt;| IDX_CUSTOMER_DOB2 |      1 |   1699K| &lt;mark&gt;00:00.21&lt;/mark&gt; |  &lt;mark&gt;5915&lt;/mark&gt; |
---------------------------------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;The above query switched from &lt;code&gt;table access full&lt;/code&gt; to &lt;code&gt;index fast full scan&lt;/code&gt; and as a result had to read only 5915 index blocks and ran in 0.21 seconds instead of 0.57 seconds.&lt;/p&gt;
&lt;p&gt;Another, more sophisticated example is Oracle’s &lt;em&gt;join elimination&lt;/em&gt; transformation. It can help with large complex queries accessing views that use multiple joins under the hood, but I’ll show a microscopic test case here. The U (users) and O (objects) test tables have a foreign key constraint defined between them - o.owner points to u.username and the FK constraint enforces that for every object record in O table, there must be a corresponding user record in U table.&lt;/p&gt;
&lt;p&gt;So let’s run a two-table join in SQL:&lt;/p&gt;
&lt;pre&gt;
SELECT o.owner FROM &lt;strong&gt;u, o WHERE u.username = o.owner&lt;/strong&gt;

--------------------------------------------------------------
| Id  | Operation         | Name | Starts | A-Rows | Buffers |
--------------------------------------------------------------
|   0 | SELECT STATEMENT  |      |      1 |  61477 |    1346 |
|   1 |  TABLE ACCESS FULL| O    |      1 |  61477 |    1346 |
--------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;Wait, what? Only one table is actually accessed according to the execution plan above? This is Oracle’s &lt;a href=&quot;https://oracle-base.com/articles/misc/join-elimination&quot;&gt;Join Elimination&lt;/a&gt; transformation in action. This query can be satisfied by accessing just the child table from the parent-child relationship as we want records from O that have a corresponding record in U - and the foreign key constraint guarantees that to be true!&lt;/p&gt;
&lt;p&gt;It gets better - in the previous query we selected columns only from the child table O, let’s also add &lt;code&gt;U.username&lt;/code&gt; into the select list:&lt;/p&gt;
&lt;pre&gt;
SELECT o.owner&lt;strong&gt;,u.username&lt;/strong&gt; FROM u, o WHERE u.username = o.owner

Plan hash value: 3411128970

--------------------------------------------------------------
| Id  | Operation         | Name | Starts | A-Rows | Buffers |
--------------------------------------------------------------
|   0 | SELECT STATEMENT  |      |      1 |  61477 |    1346 |
|   1 |  TABLE ACCESS FULL| O    |      1 |  61477 |    1346 |
--------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;We &lt;em&gt;still&lt;/em&gt; don’t have to go to the table U despite selecting a column from it - it’s because this column is guaranteed to be exactly the same as &lt;code&gt;o.owner&lt;/code&gt; thanks to the &lt;code&gt;WHERE u.username = o.owner&lt;/code&gt; join condition. Oracle is smart enough to avoid doing the join as it knows it’s a logically valid shortcut.&lt;/p&gt;
&lt;p&gt;But now let’s select an additional &lt;em&gt;non-join column&lt;/em&gt; from the table U, I’m not even using SELECT * that would have the same effect:&lt;/p&gt;
&lt;pre&gt;
SELECT o.owner,u.username&lt;strong&gt;,u.created&lt;/strong&gt; FROM u, o WHERE u.username = o.owner

--------------------------------------------------------------------------
| Id  | Operation          | Name | Starts | A-Rows | Buffers | Used-Mem |
--------------------------------------------------------------------------
|   0 | SELECT STATEMENT   |      |      1 |  61477 |    1350 |          |
|*  1 |  &lt;mark&gt;HASH JOIN&lt;/mark&gt;         |      |      1 |  61477 |    1350 | 1557K (0)|
|   2 |   TABLE ACCESS FULL| U    |      1 |     51 |       3 |          |
|   3 |   TABLE ACCESS FULL| O    |      1 |  61477 |    1346 |          |
--------------------------------------------------------------------------

   1 - access(&lt;mark&gt;&quot;U&quot;.&quot;USERNAME&quot;=&quot;O&quot;.&quot;OWNER&quot;&lt;/mark&gt;)
&lt;/pre&gt;
&lt;p&gt;Now we see both tables accessed and joined as there are no valid shortcuts (optimizations) to take.&lt;/p&gt;
&lt;p&gt;You might say that this seems like a quite exotic optimization with little value in real life (how often do you &lt;em&gt;not&lt;/em&gt; need columns from the parent table and the parent table is indexed by its primary key anyway). In practice, with complex execution plans (tens of tables joined, with multiple subqueries, views, etc) it can be quite beneficial. Additionally, if the transformation phase can eliminate some tables from the join, it will be easier for the “physical optimizer” to figure out a good join order for the remaining tables.&lt;/p&gt;
&lt;h4 id=&quot;server-side-memory-usage&quot;&gt;Server-side memory usage&lt;/h4&gt;
&lt;p&gt;If you look into the hash join plan above, there’s a column called &lt;code&gt;Used-Mem&lt;/code&gt;. Buffering row sources like sort buffers for &lt;code&gt;order by&lt;/code&gt; or hashtables for hash joins, &lt;code&gt;distinct&lt;/code&gt; and &lt;code&gt;group by&lt;/code&gt; all need a memory scratch area (SQL cursor workarea) to operate. The more rows you process at once, the more memory you generally need. But also, the more &lt;em&gt;columns&lt;/em&gt; you buffer with each row, the more memory you’ll need!&lt;/p&gt;
&lt;p&gt;The simplest example is just an ORDER BY:&lt;/p&gt;
&lt;pre&gt;
SELECT * FROM soe_small.customers ORDER BY customer_since

Plan hash value: 2792773903

----------------------------------------------------------------------------------
| Id  | Operation          | Name      | Starts | A-Rows |   A-Time   | Used-Mem |
----------------------------------------------------------------------------------
|   0 | SELECT STATEMENT   |           |      1 |   1699K|00:00:02.31 |          |
|   1 |  SORT ORDER BY     |           |      1 |   1699K|00:00:02.31 |  232M (0)|
|   2 |   TABLE ACCESS FULL| CUSTOMERS |      1 |   1699K|00:00:00.24 |          |
----------------------------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;232 MB of memory was used for the sort above. The &lt;code&gt;(0)&lt;/code&gt; indicates a zero-pass operation, we didn’t have to spill any temporary results to disk, the whole sort fit in memory.&lt;/p&gt;
&lt;p&gt;And now select just 2 columns (and order by 3rd):&lt;/p&gt;
&lt;pre&gt;
SELECT customer_id,dob FROM soe_small.customers ORDER BY customer_since

Plan hash value: 2792773903

----------------------------------------------------------------------------------
| Id  | Operation          | Name      | Starts | A-Rows |   A-Time   | Used-Mem |
----------------------------------------------------------------------------------
|   0 | SELECT STATEMENT   |           |      1 |   1699K|00:00:00.59 |          |
|   1 |  SORT ORDER BY     |           |      1 |   1699K|00:00:00.59 |   67M (0)|
|   2 |   TABLE ACCESS FULL| CUSTOMERS |      1 |   1699K|00:00:00.13 |          |
----------------------------------------------------------------------------------
&lt;/pre&gt;
&lt;p&gt;Memory usage dropped from 232 MB to 67 MB. The query still had to scan through the entire Customers table and processed 1699k rows as before, but it ran 4x faster as it did’t spend so much CPU time on the sorting phase. Narrower records not only use less memory in the buffers, but also are also CPU cache-friendly and require moving less bytes around (&lt;a href=&quot;https://tanelpoder.com/2015/08/09/ram-is-the-new-disk-and-how-to-measure-its-performance-part-1/&quot;&gt;RAM access is slow&lt;/a&gt;).&lt;/p&gt;
&lt;blockquote readability=&quot;12&quot;&gt;
&lt;p&gt;Wide resultsets also increase memory usage (both on server and the client) due to sending/receiving arrays of records through the database network &amp;amp; client libraries (not even talking about TCP send/receive buffers here). When you retrieve 1000 records per fetch and each 1000-column record is 5 kB in size on average, we are talking about at least 5 MB of memory &lt;em&gt;per connection&lt;/em&gt; on the database side and at least 5 MB of memory per &lt;em&gt;open cursor&lt;/em&gt; on the application side. In practice the usage will be larger as the data structures for processing &amp;amp; packing (and holding) the results have some overhead. Nowadays with memory being relatively cheap, this is not that much of a problem, but I recall one large Oracle system from 15 years ago where the customer had to reduce arraysize as otherwise they ran out of server memory with their 80 000 database connections :-) Also if your application has some sort of a cursor leak (statement handle leak), the associated memory with arrays of unconsumed cursor result sets may build up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;increased-cpu-usage-on-server-side&quot;&gt;Increased CPU usage on server side&lt;/h4&gt;
&lt;p&gt;Naturally, if the higher level shortcuts and optimizations described above do not kick in, you will end up doing more work. More work means more CPU usage (and possibly disk I/O, etc).&lt;/p&gt;
&lt;p&gt;Leaving the structural SQL plan shape optimization aside, when extracting all 500 fields from records in data blocks instead of just 20 - and passing them through the execution plan tree, you’ll use more CPU cycles for doing that. With columnar storage layouts, you potentially end up doing more I/O too. For example, with Oracle’s traditional record-oriented (OLTP) block format, navigating to 500th column of a table, requires to jump through all previous 499 columns' header bytes (run-length encoding) to find where the last column starts. Now, if you actually &lt;em&gt;need&lt;/em&gt; to retrieve all 500 fields from all rows, &lt;code&gt;SELECT *&lt;/code&gt; will be efficient for the &lt;em&gt;task at hand&lt;/em&gt;, but if your applications only use a handful of columns from the resultset, you’d be unnecessarily burning lots of extra CPU time on the (expensive) database server.&lt;/p&gt;
&lt;p&gt;If your database engine happens to perform datatype conversion (from its internal number, date format to what the client expects) on the server side - and character set conversion of strings (if any), you’ll be burning more CPU on your DB server too. For example, Oracle can leave this work to client side - a “decimal” NUMBER or DATE datatype within Oracle client libraries has the same representation as in the database storage, but needs to be converted into something that’s native for the application on client side. This price has to be paid somewhere, if the client &amp;amp; server speak different datatype-languages.&lt;/p&gt;
&lt;p&gt;So if you’re selecting “only” 1M rows from the database into some analytics app, but you select all 500 columns of the table, you’ll end up with &lt;em&gt;half a billion&lt;/em&gt; datatype/character set conversion operations just for this one query and you’ll quickly realize that these operations aren’t cheap.&lt;/p&gt;
&lt;h4 id=&quot;hard-parsingoptimization-takes-more-time&quot;&gt;Hard parsing/optimization takes more time&lt;/h4&gt;
&lt;p&gt;There’s more! I created a wide table (1000 columns) using my &lt;a href=&quot;https://github.com/tanelpoder/tpt-oracle/blob/master/demos/create_wide_table.sql&quot;&gt;create_wide_table.sql&lt;/a&gt; script. It has 100 rows in it and histograms on each column. I am running a very simple, single table select query against it (the “testNNN” in comments is for forcing a new hard parse each time I run the query). In the first two tests, I’m running the select statement right after recreating the table &amp;amp; gathering stats (no other queries have been executed at this table):&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SET AUTOTRACE TRACE STAT

SQL&amp;gt; SELECT * FROM widetable /* test100 */;

100 rows selected.

Statistics
----------------------------------------------------------
       &lt;mark&gt;2004  recursive calls&lt;/mark&gt;
       5267  db block gets
       2458  consistent gets
          9  physical reads
    1110236  redo size
     361858  bytes sent via SQL*Net to client
        363  bytes received via SQL*Net from client
          2  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
        100  rows processed
&lt;/pre&gt;
&lt;p&gt;2004 recursive calls for &lt;code&gt;SELECT *&lt;/code&gt; (for data dictionary access, can be verified using SQL*Trace). I recreated the table again and ran just a two column select next:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT id,col1 FROM widetable /* test101 */;

100 rows selected.

Statistics
----------------------------------------------------------
          &lt;mark&gt;5  recursive calls&lt;/mark&gt;
         10  db block gets
         51  consistent gets
          0  physical reads
       2056  redo size
       1510  bytes sent via SQL*Net to client
        369  bytes received via SQL*Net from client
          2  SQL*Net roundtrips to/from client
          0  sorts (memory)
          0  sorts (disk)
        100  rows processed
&lt;/pre&gt;
&lt;p&gt;Only 5 recursive calls for the hard parse. See, asking Oracle to do more work (“please check, evaluate &amp;amp; extract 1000 columns instead of 2”) has performance consequences. Ok, this may not be a too big deal assuming that your shared pool is big enough to keep all the column (and their stats/histograms) info in dictionary cache, you wouldn’t have all these recursive SQLs with a nice warm cache. Let’s see how much &lt;em&gt;time&lt;/em&gt; the hard parse phase takes when everything’s nicely cached in dictionary cache. I’m using my &lt;a href=&quot;https://tanelpoder.com/snapper/&quot;&gt;Session Snapper&lt;/a&gt; in a separate Oracle session to report metrics from the hard parsing tests in another session (1136):&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT * FROM widetable /* test1 */;

SQL&amp;gt; @snapper stats,gather=t 5 1 1136
Sampling SID 1136 with interval 5 seconds, taking 1 snapshots...

-- Session Snapper v4.30 - by Tanel Poder ( https://tanelpoder.com/snapper )

-----------------------------------------------------------------------------
    SID, USERNAME  , TYPE, STATISTIC                          ,         DELTA
-----------------------------------------------------------------------------
   1136, SYSTEM    , TIME, hard parse elapsed time            ,         &lt;mark&gt;78158&lt;/mark&gt;
   1136, SYSTEM    , TIME, parse time elapsed                 ,         80912
   1136, SYSTEM    , TIME, PL/SQL execution elapsed time      ,           127
   1136, SYSTEM    , TIME, DB CPU                             ,         89580
   1136, SYSTEM    , TIME, sql execute elapsed time           ,          5659
   1136, SYSTEM    , TIME, DB time                            ,         89616

--  End of Stats snap 1, end=2020-11-24 19:31:49, seconds=5
&lt;/pre&gt;
&lt;p&gt;The hard parse/optimization/compilation phase took 78 milliseconds (all CPU time) for this very simple query that was selecting all 1000 columns, even with all the table metadata and column stats &amp;amp; histograms already cached. Oracle had to do analysis &amp;amp; typechecking for all 1000 columns. Now let’s run another query on the same table, selecting only 2 columns:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT id,col1 FROM widetable /* test2 */;

-----------------------------------------------------------------------------
    SID, USERNAME  , TYPE, STATISTIC                          ,         DELTA
-----------------------------------------------------------------------------
   1136, SYSTEM    , TIME, hard parse elapsed time            ,          &lt;mark&gt;1162&lt;/mark&gt;
   1136, SYSTEM    , TIME, parse time elapsed                 ,          1513
   1136, SYSTEM    , TIME, PL/SQL execution elapsed time      ,           110
   1136, SYSTEM    , TIME, DB CPU                             ,          2281
   1136, SYSTEM    , TIME, sql execute elapsed time           ,           376
   1136, SYSTEM    , TIME, DB time                            ,          2128
&lt;/pre&gt;
&lt;p&gt;The hard parse took just ~1 millisecond! The SQL is structurally identical, on the same exact table, with just less columns selected.&lt;/p&gt;
&lt;p&gt;Out of curiosity, what happens when we drop the histograms on all columns and do a &lt;code&gt;SELECT *&lt;/code&gt; again:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; EXEC DBMS_STATS.GATHER_TABLE_STATS(user,'WIDETABLE',method_opt=&amp;gt;'FOR ALL COLUMNS SIZE 1');

PL/SQL procedure successfully completed.


SQL&amp;gt; SELECT * FROM widetable /* test3 */;

-----------------------------------------------------------------------------
    SID, USERNAME  , TYPE, STATISTIC                          ,         DELTA
-----------------------------------------------------------------------------
   1136, SYSTEM    , TIME, hard parse elapsed time            ,         &lt;mark&gt;30018&lt;/mark&gt;
   1136, SYSTEM    , TIME, parse time elapsed                 ,         30547
   1136, SYSTEM    , TIME, PL/SQL execution elapsed time      ,           202
   1136, SYSTEM    , TIME, DB CPU                             ,         37899
   1136, SYSTEM    , TIME, sql execute elapsed time           ,          5770
   1136, SYSTEM    , TIME, DB time                            ,         37807
&lt;/pre&gt;
&lt;p&gt;Now, hard parsing takes 30 milliseconds for the 1000 column query, apparently it enumerates/maps histograms for all columns involved in the query, including the columns that are just projected (and not used in any filters or joins, where histograms are actually used for plan optimization).&lt;/p&gt;
&lt;h4 id=&quot;cached-cursors-take-more-memory-in-shared-pool&quot;&gt;Cached cursors take more memory in shared pool&lt;/h4&gt;
&lt;p&gt;And there’s more!&lt;/p&gt;
&lt;p&gt;Oracle caches compiled cursors in shared pool memory. Oracle is smart and includes only the required metadata (various opcodes, datatypes, rules) into a compiled cursor. Thus, a cached cursor using 1000 columns is going to be much bigger than a cursor using just 2 columns:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT sharable_mem, sql_id, child_number, sql_text FROM v$sql 
     WHERE sql_text LIKE 'SELECT % FROM widetable';

SHARABLE_MEM SQL_ID        CHILD_NUMBER SQL_TEXT
------------ ------------- ------------ -------------------------------------
       &lt;mark&gt;19470&lt;/mark&gt; b98yvssnnk13p            0 SELECT id,col1 FROM widetable
      &lt;mark&gt;886600&lt;/mark&gt; c4d3jr3fjfa3t            0 SELECT * FROM widetable
&lt;/pre&gt;
&lt;p&gt;The 2-column cursor takes 19 kB and the 1000-column one takes 886 kB of memory in shared pool!&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;Since around 10g, Oracle splits most large library cache object allocations into standardized extent sizes (4 kB) to reduce the effect of shared pool fragmentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s take a look &lt;em&gt;inside&lt;/em&gt; these cursors with my &lt;a href=&quot;https://github.com/tanelpoder/tpt-oracle/blob/master/sqlmem.sql&quot;&gt;sqlmem.sql&lt;/a&gt; script (&lt;code&gt;v$sql_shared_memory&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; @sqlmem c4d3jr3fjfa3t
Show shared pool memory usage of SQL statement with SQL_ID c4d3jr3fjfa3t

CHILD_NUMBER SHARABLE_MEM PERSISTENT_MEM RUNTIME_MEM
------------ ------------ -------------- -----------
           0       &lt;mark&gt;886600&lt;/mark&gt;         324792      219488


TOTAL_SIZE   AVG_SIZE     CHUNKS ALLOC_CL CHUNK_TYPE STRUCTURE            FUNCTION             CHUNK_COM            HEAP_ADDR
---------- ---------- ---------- -------- ---------- -------------------- -------------------- -------------------- ----------------
    272000        272       &lt;mark&gt;1000&lt;/mark&gt; freeabl           0 kccdef               qkxrMem              kccdef: qkxrMem      000000019FF49290
    128000        128       &lt;mark&gt;1000&lt;/mark&gt; freeabl           0 opn                  qkexrInitO           opn: qkexrInitO      000000019FF49290
    112568         56       &lt;mark&gt;2002&lt;/mark&gt; freeabl           0                      qosdInitExprCtx      qosdInitExprCtx      000000019FF49290
     96456         96       &lt;mark&gt;1000&lt;/mark&gt; freeabl           0                      qosdUpdateExprM      qosdUpdateExprM      000000019FF49290
     57320         57       &lt;mark&gt;1000&lt;/mark&gt; freeabl           0 idndef*[]            qkex                 idndef*[]: qkex      000000019FF49290
     48304         48       &lt;mark&gt;1000&lt;/mark&gt; freeabl           0 qeSel                qkxrXfor             qeSel: qkxrXfor      000000019FF49290
     40808         40       &lt;mark&gt;1005&lt;/mark&gt; freeabl           0 idndef               qcuAll               idndef : qcuAll      000000019FF49290
     40024      40024          1 freeabl           0 kafco                qkacol               kafco : qkacol       000000019FF49290
     37272        591         63 freeabl           0                      237.kggec            237.kggec            000000019FF49290
     16080       8040          2 freeabl           0 qeeRwo               qeeCrea              qeeRwo: qeeCrea      000000019FF49290
      8032       8032          1 freeabl           0 kggac                kggacCre             kggac: kggacCre      000000019FF49290
      8024       8024          1 freeabl           0 kksoff               opitca               kksoff : opitca      000000019FF49290
      3392         64         53 freeabl           0 kksol                kksnsg               kksol : kksnsg       000000019FF49290
      2880       2880          1 free              0                      free memory          free memory          000000019FF49290
      1152        576          2 freeabl           0                      16751.kgght          16751.kgght          000000019FF49290
      1040       1040          1 freeabl           0 ctxdef               kksLoadC             ctxdef:kksLoadC      000000019FF49290
       640        320          2 freeabl           0                      615.kggec            615.kggec            000000019FF49290
       624        624          1 recr           4095                      237.kggec            237.kggec            000000019FF49290
       472        472          1 freeabl           0 qertbs               qertbIAl             qertbs:qertbIAl      000000019FF49290
...

53 rows selected.
&lt;/pre&gt;
&lt;p&gt;The 1000-column &lt;code&gt;SELECT *&lt;/code&gt; cursor has plenty of internal allocations (allocated inside the &lt;em&gt;cursor heaps&lt;/em&gt;) where the count of internal chunks is 1000 or close to a multiple of 1000, so one (or two) for each column in the compiled cursor. These structures are needed for executing the plan (like what Oracle kernel’s C function needs to be called, when the field #3 needs to be passed up the execution plan tree). For example if column #77 happens to be a DATE and it’s later compared to a TIMESTAMP column #88 in a separate step of the plan, there would need to be an additional &lt;em&gt;opcode&lt;/em&gt; somewhere that instructs Oracle to execute an additional datatype conversion function for one of the columns at that plan step. An execution plan is a tree of such dynamically allocated structures and opcodes within them. Apparently, even a simple select from a single table without any further complexity, requires plenty of such internal allocations to be in place.&lt;/p&gt;
&lt;p&gt;Let’s look inside the 2-column cursor memory:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; @sqlmem b98yvssnnk13p
Show shared pool memory usage of SQL statement with SQL_ID b98yvssnnk13p

CHILD_NUMBER SHARABLE_MEM PERSISTENT_MEM RUNTIME_MEM
------------ ------------ -------------- -----------
           0        19470           7072        5560


TOTAL_SIZE   AVG_SIZE     CHUNKS ALLOC_CL CHUNK_TYPE STRUCTURE            FUNCTION             CHUNK_COM            HEAP_ADDR
---------- ---------- ---------- -------- ---------- -------------------- -------------------- -------------------- ----------------
      1640       1640          1 free              0                      free memory          free memory          00000001AF2B75D0
      1152        576          2 freeabl           0                      16751.kgght          16751.kgght          00000001AF2B75D0
      1040       1040          1 freeabl           0 ctxdef               kksLoadC             ctxdef:kksLoadC      00000001AF2B75D0
       640        320          2 freeabl           0                      615.kggec            615.kggec            00000001AF2B75D0
       624        624          1 recr           4095                      237.kggec            237.kggec            00000001AF2B75D0
       544        272          &lt;mark&gt;2&lt;/mark&gt; freeabl           0 kccdef               qkxrMem              kccdef: qkxrMem      00000001AF2B75D0
       472        472          1 freeabl           0 qertbs               qertbIAl             qertbs:qertbIAl      00000001AF2B75D0
       456        456          1 freeabl           0 opixpop              kctdef               opixpop:kctdef       00000001AF2B75D0
       456        456          1 freeabl           0 kctdef               qcdlgo               kctdef : qcdlgo      00000001AF2B75D0
       328         54          6 freeabl           0                      qosdInitExprCtx      qosdInitExprCtx      00000001AF2B75D0
       312        312          1 freeabl           0 pqctx                kkfdParal            pqctx:kkfdParal      00000001AF2B75D0
       296        296          1 freeabl           0                      unmdef in opipr      unmdef in opipr      00000001AF2B75D0
       256        128          2 freeabl           0 opn                  qkexrInitO           opn: qkexrInitO      00000001AF2B75D0
       256         42          6 freeabl           0 idndef               qcuAll               idndef : qcuAll      00000001AF2B75D0
       208         41          5 freeabl           0                      kggsmInitCompac      kggsmInitCompac      00000001AF2B75D0
       192         96          2 freeabl           0                      qosdUpdateExprM      qosdUpdateExprM      00000001AF2B75D0
       184        184          1 freeabl           0                      237.kggec            237.kggec            00000001AF2B75D0
...
&lt;/pre&gt;
&lt;p&gt;Indeed we don’t see thousands of internal allocation chunks anymore (only 2 &lt;code&gt;kccdef&lt;/code&gt;s for example, compared to previous 1000).&lt;/p&gt;
&lt;h4 id=&quot;lob-fetching&quot;&gt;LOB fetching&lt;/h4&gt;
&lt;p&gt;Ok, let’s try to finish this post with a bit mellower theme :-)&lt;/p&gt;
&lt;p&gt;When you select LOB columns from a table, your performance will drop quite a lot due to extra network roundtrips done fetching LOB items for each returned row &lt;em&gt;individually&lt;/em&gt;. Yes, you read that right, you can set your arraysize to &lt;code&gt;1000&lt;/code&gt;, but if you are selecting a LOB column from the result set, then for each arrayful (of 1000) rows, you will have to do 1000 &lt;em&gt;extra&lt;/em&gt; network roundtrips for fetching individual LOB values.&lt;/p&gt;
&lt;p&gt;I’ll create a table with 2 LOB columns in addition to “normal” columns:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SQL&amp;gt; CREATE TABLE tl (id INT, a VARCHAR2(100), b CLOB, c CLOB);

Table created.

SQL&amp;gt; INSERT INTO tl SELECT rownum, dummy, dummy, dummy FROM dual CONNECT BY LEVEL &amp;lt;= 1000;

1000 rows created.

SQL&amp;gt; COMMIT;

Commit complete.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Let’s only select the 2 normal columns first:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SET AUTOT TRACE STAT
SQL&amp;gt; SET TIMING ON

SQL&amp;gt; SELECT id, a FROM tl;

1000 rows selected.

Elapsed: &lt;mark&gt;00:00:00.04&lt;/mark&gt;

Statistics
----------------------------------------------------------
          0  recursive calls
          0  db block gets
         28  consistent gets
          0  physical reads
          0  redo size
      10149  bytes sent via SQL*Net to client
        441  bytes received via SQL*Net from client
         &lt;mark&gt;11  SQL*Net roundtrips to/from client&lt;/mark&gt;
          0  sorts (memory)
          0  sorts (disk)
       1000  rows processed
&lt;/pre&gt;
&lt;p&gt;Fetching 2 normal columns was very fast (0.04 seconds) and took only 11 SQL*Net roundtrips (with arraysize 100).&lt;/p&gt;
&lt;p&gt;Now let’s add one LOB column:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT id, a, b FROM tl;

1000 rows selected.

Elapsed: &lt;mark&gt;00:00:05.50&lt;/mark&gt;

Statistics
----------------------------------------------------------
         10  recursive calls
          5  db block gets
       2027  consistent gets
          0  physical reads
       1052  redo size
     421070  bytes sent via SQL*Net to client
     252345  bytes received via SQL*Net from client
       &lt;mark&gt;2002  SQL*Net roundtrips to/from client&lt;/mark&gt;
          0  sorts (memory)
          0  sorts (disk)
       1000  rows processed
&lt;/pre&gt;
&lt;p&gt;It took 5.5 seconds and 2002 SQL*Net roundtrips due to the “breaking” nature of LOB retrieval. By default, any row with a non-NULL LOB column is sent back immediately (just one row in the fetched array) and instead of the LOB column value, a &lt;em&gt;LOB locator&lt;/em&gt; is sent back, causing the client to issue a separate &lt;a href=&quot;https://tanelpoder.com/2011/03/20/lobread-sql-trace-entry-in-oracle-11-2/&quot;&gt;LOBREAD&lt;/a&gt; database call just to fetch the single LOB column value. And this gets worse when you’re selecting multiple LOB columns:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SELECT id, a, b, c FROM tl;

1000 rows selected.

Elapsed: &lt;mark&gt;00:00:09.28&lt;/mark&gt;

Statistics
----------------------------------------------------------
          6  recursive calls
          5  db block gets
       3026  consistent gets
          0  physical reads
        996  redo size
     740122  bytes sent via SQL*Net to client
     493348  bytes received via SQL*Net from client
       &lt;mark&gt;3002  SQL*Net roundtrips to/from client&lt;/mark&gt;
          0  sorts (memory)
          0  sorts (disk)
       1000  rows processed
&lt;/pre&gt;
&lt;p&gt;Now it takes over 9 seconds instead of previous 5.5 with just a single LOB column. We have ~3000 roundtrips, one for each row (because LOB item retrieval breaks the array fetching) and one two LOB item fetch roundtrips for each row.&lt;/p&gt;
&lt;p&gt;Starting from Oracle 12.2 (I think), there’s a parameter LOBPREFETCH in sqlplus that allows &lt;em&gt;“bundling”&lt;/em&gt; some amount of LOB data right into the &lt;em&gt;row fetch&lt;/em&gt; roundtrip. Oracle client libraries should allow bigger LOB prefetch values, but the limit in sqlplus is 32kB:&lt;/p&gt;
&lt;pre&gt;
SQL&amp;gt; SET LOBPREFETCH 32767
SQL&amp;gt; 
SQL&amp;gt; SELECT id, a, b, c FROM tl;

1000 rows selected.

Elapsed: 00:00:04.80

Statistics
----------------------------------------------------------
          0  recursive calls
          0  db block gets
       1005  consistent gets
          0  physical reads
          0  redo size
     366157  bytes sent via SQL*Net to client
      11756  bytes received via SQL*Net from client
       &lt;mark&gt;1002  SQL*Net roundtrips to/from client&lt;/mark&gt;
          0  sorts (memory)
          0  sorts (disk)
       1000  rows processed
&lt;/pre&gt;
&lt;p&gt;Now we are down to ~1000 roundtrips again, because my LOB values were small, both of them were bundled within each row’s fetch result. But Oracle still ended up fetching just one row at a time, despite my arraysize = 100 value.&lt;/p&gt;
&lt;p&gt;So, with LOB columns added thanks to a casual &lt;code&gt;SELECT *&lt;/code&gt;, your 40 millisecond query may end up taking over 9 seconds. And you won’t see much activity at the database at all, as most of the response time is spent in the SQL*Net roundtrips between the client and server. No index will make this faster, more CPUs won’t make this faster - fixing your application code will make this faster. This leads to the question of what if &lt;em&gt;want&lt;/em&gt; to pull in millions of LOB values into my app, but I’ll leave this to a separate blog entry!&lt;/p&gt;
&lt;h4 id=&quot;one-more-thing&quot;&gt;One more thing&lt;/h4&gt;
&lt;p&gt;Note that you can use &lt;code&gt;SELECT *&lt;/code&gt; in places like view definitions (or inline views) without a problem as long you do restrict the query to the columns you want somewhere in your SQL.&lt;/p&gt;
&lt;p&gt;For example, this query would not cause a problem despite seeing a &lt;code&gt;SELECT *&lt;/code&gt; somewhere within it:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SELECT
    id, a 
FROM (
    SELECT * FROM tl
)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Oracle is smart enough to propagate the projection from top level SELECT into the inline view and only get the two required columns from it.&lt;/p&gt;
&lt;p&gt;Or, this would also be fine:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SELECT * FROM (
    SELECT id, a FROM tl
)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The goal is not to avoid a &lt;code&gt;*&lt;/code&gt; in your &lt;em&gt;SQL text&lt;/em&gt;, but to select only the columns that you actually need.&lt;/p&gt;
&lt;h4 id=&quot;summary&quot;&gt;Summary&lt;/h4&gt;
&lt;p&gt;When I look at a performance problem (something is taking too much time), I think about how to &lt;em&gt;&lt;strong&gt;do it less&lt;/strong&gt;&lt;/em&gt;. The other option is to add more hardware (and there are no guarantees that it will help). One way to “do it less” is to make sure that you ask &lt;em&gt;exactly what you want&lt;/em&gt; from your database, no more, no less. Selecting only the columns you actually need is one part of that approach.&lt;/p&gt;
&lt;p&gt;Thanks for reading, I sense more blog entries coming! :-)&lt;/p&gt;
&lt;p&gt;Also, you can still sign up for my &lt;a href=&quot;https://tanelpoder.com/seminar&quot;&gt;Advanced Oracle SQL Tuning training&lt;/a&gt; starting on 30. Nov!&lt;/p&gt;
&lt;hr/&gt;&lt;div&gt;&lt;span&gt;&lt;a href=&quot;https://tanelpoder.com/about/&quot;&gt;&lt;img src=&quot;https://tanelpoder.com/files/images/tanelpoder_small.png&quot;/&gt;&lt;/a&gt;&lt;/span&gt;

&lt;/div&gt;
&lt;hr/&gt;&lt;footer&gt;&lt;hr/&gt;
© &lt;a href=&quot;https://tanelpoder.com/&quot;&gt;Tanel Põder&lt;/a&gt; 2007-2021&lt;/footer&gt;&lt;/body&gt;</description>
<pubDate>Mon, 15 Feb 2021 09:47:16 +0000</pubDate>
<dc:creator>wheresvic4</dc:creator>
<og:title>Reasons why SELECT * is bad for SQL performance - Tanel Poder Consulting</og:title>
<og:description>Here&amp;rsquo;s a list of reasons why SELECT * is bad for SQL performance, assuming that your application doesn&amp;rsquo;t actually need all the columns. When I write production code, I explicitly specify the columns of interest in the select-list (projection), not only for performance reasons, but also for application reliability reasons. For example, will your application&amp;rsquo;s data processing code suddenly break when a new column has been added or the column order has changed in a table? - Linux, Oracle, SQL performance tuning and troubleshooting - consulting &amp; training.</og:description>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tanelpoder.com/posts/reasons-why-select-star-is-bad-for-sql-performance/</dc:identifier>
</item>
</channel>
</rss>