<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>This Startup Does Not Exist</title>
<link>https://thisstartupdoesnotexist.com/</link>
<guid isPermaLink="true" >https://thisstartupdoesnotexist.com/</guid>
<description>&lt;p&gt;
		&lt;h2 class=&quot;theme-title&quot;&gt;Bovise is more than just Audio&lt;/h2&gt;
		&lt;h5 class=&quot;description&quot;&gt;Market, VR and Calling are just a few characteristic of Bovise. But we offer a lot more!&lt;/h5&gt;
	&lt;/p&gt;
	&lt;div class=&quot;theme-blog-content&quot;&gt;
		&lt;div class=&quot;theme-features-content container&quot;&gt;
			&lt;div class=&quot;row&quot;&gt;
				&lt;div class=&quot;col-xs-12 col-md-4 feature-box&quot;&gt;
					&lt;div class=&quot;theme-info&quot;&gt;
						
							&lt;h4 class=&quot;info-title&quot;&gt;Irresistibly&lt;/h4&gt;
							
					&lt;/div&gt;
				&lt;/div&gt;
				&lt;div class=&quot;col-xs-12 col-md-4 feature-box&quot;&gt;
					&lt;div class=&quot;theme-info&quot;&gt;
						
							&lt;h4 class=&quot;info-title&quot;&gt;Useful&lt;/h4&gt;
							
					&lt;/div&gt;
				&lt;/div&gt;
				&lt;div class=&quot;col-xs-12 col-md-4 feature-box&quot;&gt;
					&lt;div class=&quot;theme-info&quot;&gt;
						
							&lt;h4 class=&quot;info-title&quot;&gt;Bolstering&lt;/h4&gt;
							
					&lt;/div&gt;
				&lt;/div&gt;
			&lt;/div&gt;
		&lt;/div&gt;
	&lt;/div&gt;
</description>
<pubDate>Tue, 26 Feb 2019 12:53:47 +0000</pubDate>
<dc:creator>oschn</dc:creator>
<dc:language>de-DE</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://thisstartupdoesnotexist.com/</dc:identifier>
</item>
<item>
<title>Linux Desktop Setup</title>
<link>https://hookrace.net/blog/linux-desktop-setup/</link>
<guid isPermaLink="true" >https://hookrace.net/blog/linux-desktop-setup/</guid>
<description>&lt;span class=&quot;post-date&quot;&gt;2019-01-15 · &lt;a href=&quot;https://hookrace.net/blog/programming/&quot;&gt;Programming&lt;/a&gt; · &lt;a href=&quot;https://hookrace.net/blog/frugality/&quot;&gt;Frugality&lt;/a&gt;&lt;/span&gt;
&lt;p&gt;My software setup has been surprisingly constant over the last decade, after a few years of experimentation since I initially switched to Linux in 2006. It might be interesting to look back in another 10 years and see what changed. A quick overview of what’s running as I’m writing this post:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hookrace.net/public/linux-desktop/htop.png&quot;&gt;&lt;img src=&quot;https://hookrace.net/public/linux-desktop/htop_small.png&quot; alt=&quot;htop overview&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;My software priorities are, in no specific order:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Programs should run on my local system so that I’m in control of them, this excludes cloud solutions.&lt;/li&gt;
&lt;li&gt;Programs should run in the terminal, so that they can be used consistently from anywhere, including weak computers or a phone.&lt;/li&gt;
&lt;li&gt;Keyboard focused is nearly automatic by using terminal software. I prefer to use the mouse where it makes sense only, reaching for the mouse all the time during typing feels like a waste of time. Occasionally it took me an hour to notice that the mouse wasn’t even plugged in.&lt;/li&gt;
&lt;li&gt;Ideally use fast and efficient software, I don’t like hearing the fan and feeling the room heat up. I can also keep running older hardware for much longer, my 10 year old Thinkpad x200s is still fine for all the software I use.&lt;/li&gt;
&lt;li&gt;Be composable. I don’t want to do every step manually, instead automate more when it makes sense. This naturally favors the shell.&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;operating-systems&quot;&gt;Operating Systems&lt;/h2&gt;
&lt;p&gt;I had a hard start with Linux 12 years ago by removing Windows, armed with just the &lt;a href=&quot;https://gentoo.org/&quot;&gt;Gentoo Linux&lt;/a&gt; installation CD and a printed manual to get a functioning Linux system. It took me a few days of compiling and tinkering, but in the end I felt like I had learnt a lot.&lt;/p&gt;
&lt;p&gt;I haven’t looked back to Windows since then, but I switched to &lt;a href=&quot;https://www.archlinux.org/&quot;&gt;Arch Linux&lt;/a&gt; on my laptop after having the fan fail from the constant compilation stress. Later I also switched all my other computers and private servers to Arch Linux. As a rolling release distribution you get package upgrades all the time, but the most important breakages are nicely reported in the &lt;a href=&quot;https://www.archlinux.org/news/&quot;&gt;Arch Linux News&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One annoyance though is that Arch Linux removes the old kernel modules once you upgrade it. I usually notice that once I try plugging in a USB flash drive and the kernel fails to load the relevant module. Instead you’re supposed to reboot after each kernel upgrade. There are a few &lt;a href=&quot;https://www.reddit.com/r/archlinux/comments/4zrsc3/keep_your_system_fully_functional_after_a_kernel/&quot;&gt;hacks&lt;/a&gt; around to get around the problem, but I haven’t been bothered enough to actually use them.&lt;/p&gt;
&lt;p&gt;Similar problems happen with other programs, commonly Firefox, cron or Samba requiring a restart after an upgrade, but annoyingly not warning you that that’s the case. &lt;a href=&quot;https://www.suse.com/&quot;&gt;SUSE&lt;/a&gt;, which I use at work, nicely warns about such cases.&lt;/p&gt;
&lt;p&gt;For the &lt;a href=&quot;https://ddnet.tw/&quot;&gt;DDNet&lt;/a&gt; production servers I prefer &lt;a href=&quot;https://www.debian.org/&quot;&gt;Debian&lt;/a&gt; over Arch Linux, so that I have a lower chance of breakage on each upgrade. For my firewall and router I used &lt;a href=&quot;https://www.openbsd.org/&quot;&gt;OpenBSD&lt;/a&gt; for its clean system, documentation and great &lt;a href=&quot;https://www.openbsd.org/faq/pf/&quot;&gt;pf firewall&lt;/a&gt;, but right now I don’t have a need for a separate router anymore.&lt;/p&gt;
&lt;h2 id=&quot;window-manager&quot;&gt;Window Manager&lt;/h2&gt;
&lt;p&gt;Since I started out with Gentoo I quickly noticed the huge compile time of KDE, which made it a no-go for me. I looked around for more minimal solutions, and used &lt;a href=&quot;http://openbox.org/wiki/Main_Page&quot;&gt;Openbox&lt;/a&gt; and &lt;a href=&quot;http://fluxbox.org/&quot;&gt;Fluxbox&lt;/a&gt; initially. At some point I jumped on the tiling window manager train in order to be more keyboard-focused and picked up &lt;a href=&quot;https://dwm.suckless.org/&quot;&gt;dwm&lt;/a&gt; and &lt;a href=&quot;https://awesomewm.org/&quot;&gt;awesome&lt;/a&gt; close to their initial releases.&lt;/p&gt;
&lt;p&gt;In the end I settled on &lt;a href=&quot;https://xmonad.org/&quot;&gt;xmonad&lt;/a&gt; thanks to its flexibility, extendability and being written and configured in pure &lt;a href=&quot;https://www.haskell.org/&quot;&gt;Haskell&lt;/a&gt;, a great functional programming language. One example of this is that at home I run a single 40” 4K screen, but often split it up into four virtual screens, each displaying a workspace on which my windows are automatically arranged. Of course xmonad has a &lt;a href=&quot;http://hackage.haskell.org/package/xmonad-contrib-0.15/docs/XMonad-Layout-LayoutScreens.html&quot;&gt;module&lt;/a&gt; for that.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://robm.github.io/dzen/&quot;&gt;dzen&lt;/a&gt; and &lt;a href=&quot;https://github.com/brndnmtthws/conky&quot;&gt;conky&lt;/a&gt; function as a simple enough status bar for me. My entire conky config looks like this:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;out_to_console yes
update_interval 1
total_run_times 0

TEXT
${downspeed eth0} ${upspeed eth0} | $cpu% ${loadavg 1} ${loadavg 2} ${loadavg 3} $mem/$memmax | ${time %F %T}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And gets piped straight into dzen2 with &lt;code&gt;conky | dzen2 -fn '-xos4-terminus-medium-r-normal-*-12-*-*-*-*-*-*-*' -bg '#000000' -fg '#ffffff' -p -e '' -x 1000 -w 920 -xs 1 -ta r&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;One important feature for me is to make the terminal emit a beep sound once a job is done. This is done simply by adding a &lt;code&gt;\a&lt;/code&gt; character to the &lt;code&gt;PR_TITLEBAR&lt;/code&gt; variable in zsh, which is shown whenever a job is done. Of course I disable the actual beep sound by blacklisting the &lt;code&gt;pcspkr&lt;/code&gt; kernel module with &lt;code&gt;echo &quot;blacklist pcspkr&quot; &amp;gt; /etc/modprobe.d/nobeep.conf&lt;/code&gt;. Instead the sound gets turned into an urgency by urxvt’s &lt;code&gt;URxvt.urgentOnBell: true&lt;/code&gt; setting. Then xmonad has an urgency hook to capture this and I can automatically focus the currently urgent window with a key combination. In dzen I get the urgent windowspaces displayed with a nice and bright &lt;code&gt;#ff0000&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The final result in all its glory on my Laptop:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://hookrace.net/public/linux-desktop/laptop.png&quot;&gt;&lt;img src=&quot;https://hookrace.net/public/linux-desktop/laptop_small.png&quot; alt=&quot;Laptop screenshot&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I hear that &lt;a href=&quot;https://i3wm.org/&quot;&gt;i3&lt;/a&gt; has become quite popular in the last years, but it requires more manual window alignment instead of specifying automated methods to do it.&lt;/p&gt;
&lt;p&gt;I realize that there are also terminal multiplexers like &lt;a href=&quot;https://github.com/tmux/tmux/wiki&quot;&gt;tmux&lt;/a&gt;, but I still require a few graphical applications, so in the end I never used them productively.&lt;/p&gt;
&lt;h2 id=&quot;terminal-persistency&quot;&gt;Terminal Persistency&lt;/h2&gt;
&lt;p&gt;In order to keep terminals alive I use &lt;a href=&quot;http://dtach.sourceforge.net/&quot;&gt;dtach&lt;/a&gt;, which is just the detach feature of screen. In order to make every terminal on my computer detachable I wrote a &lt;a href=&quot;https://github.com/def-/tach/blob/master/tach&quot;&gt;small wrapper script&lt;/a&gt;. This means that even if I had to restart my X server I could keep all my terminals running just fine, both local and remote.&lt;/p&gt;
&lt;h2 id=&quot;shell--programming&quot;&gt;Shell &amp;amp; Programming&lt;/h2&gt;
&lt;p&gt;Instead of &lt;a href=&quot;https://www.gnu.org/software/bash/&quot;&gt;bash&lt;/a&gt; I use &lt;a href=&quot;http://www.zsh.org/&quot;&gt;zsh&lt;/a&gt; as my shell for its huge number of features.&lt;/p&gt;
&lt;p&gt;As a terminal emulator I found &lt;a href=&quot;http://software.schmorp.de/pkg/rxvt-unicode.html&quot;&gt;urxvt&lt;/a&gt; to be simple enough, support Unicode and 256 colors and has great performance. Another great feature is being able to run the urxvt client and daemon separately, so that even a large number of terminals barely takes up any memory (except for the scrollback buffer).&lt;/p&gt;
&lt;p&gt;There is only one font that looks absolutely clean and perfect to me: &lt;a href=&quot;http://terminus-font.sourceforge.net/&quot;&gt;Terminus&lt;/a&gt;. Since it’s a bitmap font everything is pixel perfect and renders extremely fast and at low CPU usage. In order to switch fonts on-demand in each terminal with &lt;code&gt;CTRL-WIN-[1-7]&lt;/code&gt; my ~/.Xdefaults contains:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;URxvt.font: -xos4-terminus-medium-r-normal-*-14-*-*-*-*-*-*-*
dzen2.font: -xos4-terminus-medium-r-normal-*-14-*-*-*-*-*-*-*

URxvt.keysym.C-M-1: command:\033]50;-xos4-terminus-medium-r-normal-*-12-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-2: command:\033]50;-xos4-terminus-medium-r-normal-*-14-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-3: command:\033]50;-xos4-terminus-medium-r-normal-*-18-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-4: command:\033]50;-xos4-terminus-medium-r-normal-*-22-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-5: command:\033]50;-xos4-terminus-medium-r-normal-*-24-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-6: command:\033]50;-xos4-terminus-medium-r-normal-*-28-*-*-*-*-*-*-*\007
URxvt.keysym.C-M-7: command:\033]50;-xos4-terminus-medium-r-normal-*-32-*-*-*-*-*-*-*\007

URxvt.keysym.C-M-n: command:\033]10;#ffffff\007\033]11;#000000\007\033]12;#ffffff\007\033]706;#00ffff\007\033]707;#ffff00\007
URxvt.keysym.C-M-b: command:\033]10;#000000\007\033]11;#ffffff\007\033]12;#000000\007\033]706;#0000ff\007\033]707;#ff0000\007
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For programming and writing I use &lt;a href=&quot;https://www.vim.org/&quot;&gt;Vim&lt;/a&gt; with syntax highlighting and &lt;a href=&quot;http://ctags.sourceforge.net/&quot;&gt;ctags&lt;/a&gt; for indexing, as well as a few terminal windows with grep, sed and the other usual suspects for search and manipulation. This is probably not at the same level of comfort as an IDE, but allows me more automation.&lt;/p&gt;
&lt;p&gt;One problem with Vim is that you get so used to its key mappings that you’ll want to use them everywhere.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://nim-lang.org/&quot;&gt;Nim&lt;/a&gt; do well as scripting languages where the shell is not powerful enough.&lt;/p&gt;
&lt;h2 id=&quot;system-monitoring&quot;&gt;System Monitoring&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://hisham.hm/htop/&quot;&gt;htop&lt;/a&gt; (look at the background of that site, it’s a live view of the server that’s hosting it) works great for getting a quick overview of what the software is currently doing. &lt;a href=&quot;http://lm-sensors.org/&quot;&gt;lm_sensors&lt;/a&gt; allows monitoring the hardware temperatures, fans and voltages. &lt;a href=&quot;https://01.org/powertop/&quot;&gt;powertop&lt;/a&gt; is a great little tool by Intel to find power savings. &lt;a href=&quot;https://dev.yorhel.nl/ncdu&quot;&gt;ncdu&lt;/a&gt; lets you analyze disk usage interactively.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nmap.org/&quot;&gt;nmap&lt;/a&gt;, iptraf-ng, &lt;a href=&quot;https://www.tcpdump.org/&quot;&gt;tcpdump&lt;/a&gt; and &lt;a href=&quot;https://www.wireshark.org/&quot;&gt;Wireshark&lt;/a&gt; are essential tools for analyzing network problems.&lt;/p&gt;
&lt;p&gt;There are of course many more great tools.&lt;/p&gt;
&lt;h2 id=&quot;mails--synchronization&quot;&gt;Mails &amp;amp; Synchronization&lt;/h2&gt;
&lt;p&gt;On my home server I have a &lt;a href=&quot;http://www.fetchmail.info/&quot;&gt;fetchmail&lt;/a&gt; daemon running for each email acccount that I have. Fetchmail just retrieves the incoming emails and invokes &lt;a href=&quot;http://www.procmail.org/&quot;&gt;procmail&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; i in /home/deen/.fetchmail/*&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;
  &lt;span class=&quot;nv&quot;&gt;FETCHMAILHOME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt; /usr/bin/fetchmail -m &lt;span class=&quot;s1&quot;&gt;'procmail -d %T'&lt;/span&gt; -d 60
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The configuration is as simple as it could be and waits for the server to inform us of fresh emails:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;poll imap.1und1.de protocol imap timeout 120 user &quot;dennis@felsin9.de&quot; password &quot;XXX&quot; folders INBOX keep ssl idle
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;My &lt;code&gt;.procmailrc&lt;/code&gt; config contains a few rules to backup all mails and sort them into the correct directories, for example based on the mailing list id or from field in the mail header:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;MAILDIR=/home/deen/shared/Maildir
LOGFILE=$HOME/.procmaillog
LOGABSTRACT=no
VERBOSE=off
FORMAIL=/usr/bin/formail
NL=&quot;
&quot;

:0wc
* ! ? test -d /media/mailarchive/`date +%Y`
| mkdir -p /media/mailarchive/`date +%Y`

# Make backups of all mail received in format YYYY/YYYY-MM
:0c
/media/mailarchive/`date +%Y`/`date +%Y-%m`

:0
* ^From: .*(.*@.*.kit.edu|.*@.*.uka.de|.*@.*.uni-karlsruhe.de)
$MAILDIR/.uni/

:0
* ^list-Id:.*lists.kit.edu
$MAILDIR/.uni-ml/

[...]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To send emails I use &lt;a href=&quot;https://marlam.de/msmtp/&quot;&gt;msmtp&lt;/a&gt;, which is also great to configure:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;account default
host smtp.1und1.de
tls on
tls_trust_file /etc/ssl/certs/ca-certificates.crt
auth on
from dennis@felsin9.de
user dennis@felsin9.de
password XXX

[...]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;But so far the emails are still on the server. My documents are all stored in a directory that I synchronize between all computers using &lt;a href=&quot;https://www.cis.upenn.edu/~bcpierce/unison/&quot;&gt;Unison&lt;/a&gt;. Think of Unison as a bidirectional interactive &lt;a href=&quot;https://rsync.samba.org/&quot;&gt;rsync&lt;/a&gt;. My emails are part of this documents directory and thus they end up on my desktop computers.&lt;/p&gt;
&lt;p&gt;This also means that while the emails reach my server immediately, I only fetch them on deman instead of getting instant notifications when an email comes in.&lt;/p&gt;
&lt;p&gt;From there I read the mails with &lt;a href=&quot;http://www.mutt.org/&quot;&gt;mutt&lt;/a&gt;, using the sidebar plugin to display my mail directories. The &lt;code&gt;/etc/mailcap&lt;/code&gt; file is essential to display non-plaintext mails containing HTML, Word or PDF:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;text/html;w3m -I %{charset} -T text/html; copiousoutput
application/msword; antiword %s; copiousoutput
application/pdf; pdftotext -layout /dev/stdin -; copiousoutput
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;news--communication&quot;&gt;News &amp;amp; Communication&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://newsboat.org/&quot;&gt;Newsboat&lt;/a&gt; is a nice little RSS/Atom feed reader in the terminal. I have it running on the server in a &lt;code&gt;tach&lt;/code&gt; session with about 150 feeds. Filtering feeds locally is also possible, for example:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;ignore-article &quot;https://forum.ddnet.tw/feed.php&quot; &quot;title =~ \&quot;Map Testing •\&quot; or title =~ \&quot;Old maps •\&quot; or title =~ \&quot;Map Bugs •\&quot; or title =~ \&quot;Archive •\&quot; or title =~ \&quot;Waiting for mapper •\&quot; or title =~ \&quot;Other mods •\&quot; or title =~ \&quot;Fixes •\&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;I use &lt;a href=&quot;https://irssi.org/&quot;&gt;Irssi&lt;/a&gt; the same way for communication via IRC.&lt;/p&gt;
&lt;h2 id=&quot;calendar&quot;&gt;Calendar&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.roaringpenguin.com/products/remind&quot;&gt;remind&lt;/a&gt; is a calendar that can be used from the command line. Setting new reminders is done by editing the &lt;code&gt;rem&lt;/code&gt; files:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# One time events
REM 2019-01-20 +90 Flight to China %b

# Recurring Holidays
REM 1 May +90 Holiday &quot;Tag der Arbeit&quot; %b
REM [trigger(easterdate(year(today()))-2)] +90 Holiday &quot;Karfreitag&quot; %b

# Time Change
REM Nov Sunday 1 --7 +90 Time Change (03:00 -&amp;gt; 02:00) %b
REM Apr Sunday 1 --7 +90 Time Change (02:00 -&amp;gt; 03:00) %b

# Birthdays
FSET birthday(x) &quot;'s &quot; + ord(year(trigdate())-x) + &quot; birthday is %b&quot;
REM 16 Apr +90 MSG Andreas[birthday(1994)]

# Sun
SET $LatDeg 49
SET $LatMin 19
SET $LatSec 49
SET $LongDeg -8
SET $LongMin -40
SET $LongSec -24

MSG Sun from [sunrise(trigdate())] to [sunset(trigdate())]
[...]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Unfortunately there is no Chinese Lunar calendar function in remind yet, so Chinese holidays can’t be calculated easily.&lt;/p&gt;
&lt;p&gt;I use two aliases for remind:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;rem -m -b1 -q -g
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;to see a list of the next events in chronological order and&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;rem -m -b1 -q -cuc12 -w$(($(tput cols)+1)) | sed -e &quot;s/\f//g&quot; | less
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;to show a calendar fitting just the width of my terminal:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://hookrace.net/public/linux-desktop/remcal.png&quot; alt=&quot;remcal&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;dictionary&quot;&gt;Dictionary&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tsdh/rdictcc&quot;&gt;rdictcc&lt;/a&gt; is a little known dictionary tool that uses the excellent dictionary files from &lt;a href=&quot;https://www.dict.cc/&quot;&gt;dict.cc&lt;/a&gt; and turns them into a local database:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ rdictcc rasch
====================[ A =&amp;gt; B ]====================
rasch:
    - apace
    - brisk [speedy]
    - cursory
    - in a timely manner
    - quick
    - quickly
    - rapid
    - rapidly
    - sharpish  [Br.]  [coll.]
    - speedily
    - speedy
    - swift
    - swiftly
rasch [gehen]:
    - smartly [quickly]
Rasch {n} [Zittergras-Segge]:
    - Alpine grass [Carex brizoides]
    - quaking grass sedge [Carex brizoides]
Rasch {m} [regional] [Putzrasch]:
    - scouring pad
====================[ B =&amp;gt; A ]====================
Rasch model:
    - Rasch-Modell {n}
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;writing-and-reading&quot;&gt;Writing and Reading&lt;/h2&gt;
&lt;p&gt;I have a simple todo file containing my tasks, that is basically always sitting open in a Vim session. For work I also use the todo file as a “done” file so that I can later check what tasks I finished on each day.&lt;/p&gt;
&lt;p&gt;For writing documents, letters and presentations I use &lt;a href=&quot;https://www.latex-project.org/&quot;&gt;LaTeX&lt;/a&gt; for its superior typesetting. A simple letter in German format can be set like this for example:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[paper = a4, fromalign = right]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;scrlttr2&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;german&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;eurosym&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[utf8]&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;inputenc&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\setlength&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\parskip&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;6pt&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\setlength&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\parindent&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;0pt&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\setkomavar&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;fromname&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;Dennis Felsing&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\setkomavar&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;fromaddress&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;Meine Str. 1&lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt;69181 Leimen&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\setkomavar&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;subject&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;Titel&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\setkomavar*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;enclseparator&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;Anlagen&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\makeatletter&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\@&lt;/span&gt;setplength&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;refvpos&lt;span class=&quot;nb&quot;&gt;}{&lt;/span&gt;89mm&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\makeatother&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;document&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\begin&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;letter&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;Herr Soundso&lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt;Deine Str. 2&lt;span class=&quot;k&quot;&gt;\\&lt;/span&gt;69121 Heidelberg&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\opening&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;Sehr geehrter Herr Soundso,&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;

Sie haben bei mir seit dem Bla Bla Bla.

Ich fordere Sie hiermit zu Bla Bla Bla auf.

&lt;span class=&quot;k&quot;&gt;\closing&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;Mit freundlichen Grüßen&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;letter&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\end&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;{&lt;/span&gt;document&lt;span class=&quot;nb&quot;&gt;}&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Further example documents and presentations can be found over at &lt;a href=&quot;http://felsin9.de/nnis/research/&quot;&gt;my private site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To read PDFs &lt;a href=&quot;https://pwmt.org/projects/zathura/&quot;&gt;Zathura&lt;/a&gt; is fast, has Vim-like controls and even supports two different PDF backends: Poppler and MuPDF. &lt;a href=&quot;https://wiki.gnome.org/Apps/Evince&quot;&gt;Evince&lt;/a&gt; on the other hand is more full-featured for the cases where I encounter documents that Zathura doesn’t like.&lt;/p&gt;
&lt;h2 id=&quot;graphical-editing&quot;&gt;Graphical Editing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.gimp.org/&quot;&gt;GIMP&lt;/a&gt; and &lt;a href=&quot;https://inkscape.org/&quot;&gt;Inkscape&lt;/a&gt; are easy choices for photo editing and interactive vector graphics respectively.&lt;/p&gt;
&lt;p&gt;In some cases &lt;a href=&quot;https://imagemagick.org/Usage/&quot;&gt;Imagemagick&lt;/a&gt; is good enough though and can be used straight from the command line and thus automated to edit images. Similarly &lt;a href=&quot;https://www.graphviz.org/&quot;&gt;Graphviz&lt;/a&gt; and &lt;a href=&quot;https://sourceforge.net/projects/pgf/&quot;&gt;TikZ&lt;/a&gt; can be used to draw graphs and other diagrams.&lt;/p&gt;
&lt;h2 id=&quot;web-browsing&quot;&gt;Web Browsing&lt;/h2&gt;
&lt;p&gt;As a web browser I’ve always used &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/new/&quot;&gt;Firefox&lt;/a&gt; for its extensibility and low resource usage compared to Chrome.&lt;/p&gt;
&lt;p&gt;Unfortunately the &lt;a href=&quot;https://github.com/5digits/dactyl&quot;&gt;Pentadactyl&lt;/a&gt; extension development stopped after Firefox switched to Chrome-style extensions entirely, so I don’t have satisfying Vim-like controls in my browser anymore.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mpv.io/&quot;&gt;mpv&lt;/a&gt; with hardware decoding allows watching videos at 5% CPU load using the &lt;code&gt;vo=gpu&lt;/code&gt; and &lt;code&gt;hwdec=vaapi&lt;/code&gt; config settings. &lt;code&gt;audio-channels=2&lt;/code&gt; in mpv seems to give me clearer downmixing to my stereo speakers / headphones than what PulseAudio does by default. A great little feature is exiting with &lt;code&gt;Shift-Q&lt;/code&gt; instead of just &lt;code&gt;Q&lt;/code&gt; to save the playback location. When watching with someone with another native tongue you can use &lt;code&gt;--secondary-sid=&lt;/code&gt; to show two subtitles at once, the primary at the bottom, the secondary at the top of the screen&lt;/p&gt;
&lt;p&gt;My wirelss mouse can easily be made into a remote control with mpv with a small &lt;code&gt;~/.config/mpv/input.conf&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;MOUSE_BTN5 run &quot;mixer&quot; &quot;pcm&quot; &quot;-2&quot;
MOUSE_BTN6 run &quot;mixer&quot; &quot;pcm&quot; &quot;+2&quot;
MOUSE_BTN1 cycle sub-visibility
MOUSE_BTN7 add chapter -1
MOUSE_BTN8 add chapter 1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://rg3.github.io/youtube-dl/&quot;&gt;youtube-dl&lt;/a&gt; works great for watching videos hosted online, best quality can be achieved with &lt;code&gt;-f bestvideo+bestaudio/best --all-subs --embed-subs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As a music player &lt;a href=&quot;http://moc.daper.net/&quot;&gt;MOC&lt;/a&gt; hasn’t been actively developed for a while, but it’s still a simple player that plays every format conceivable, including the strangest Chiptune formats. In the AUR there is a &lt;a href=&quot;https://aur.archlinux.org/packages/moc-pulse/&quot;&gt;patch&lt;/a&gt; adding PulseAudio support as well. Even with the CPU clocked down to 800 MHz MOC barely uses 1-2% of a single CPU core.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://hookrace.net/public/linux-desktop/moc.png&quot; alt=&quot;moc&quot; /&gt;&lt;/p&gt;
&lt;p&gt;My music collection sits on my home server so that I can access it from anywhere. It is mounted using &lt;a href=&quot;https://github.com/libfuse/sshfs&quot;&gt;SSHFS&lt;/a&gt; and automount in the &lt;code&gt;/etc/fstab/&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;root@server:/media/media  /mnt/media  fuse.sshfs noauto,x-systemd.automount,idmap=user,IdentityFile=/root/.ssh/id_rsa,allow_other,reconnect 0 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;cross-platform-building&quot;&gt;Cross-Platform Building&lt;/h2&gt;
&lt;p&gt;Linux is great to build packages for any major operating system except Linux itself! In the beginning I used &lt;a href=&quot;https://www.qemu.org/&quot;&gt;QEMU&lt;/a&gt; to with an old Debian, Windows and Mac OS X VM to build for these platforms.&lt;/p&gt;
&lt;p&gt;Nowadays I switched to using chroot for the old Debian distribution (for maximum Linux compatibility), &lt;a href=&quot;http://www.mingw.org/&quot;&gt;MinGW&lt;/a&gt; to cross-compile for Windows and &lt;a href=&quot;https://github.com/tpoechtrager/osxcross&quot;&gt;OSXCross&lt;/a&gt; to cross-compile for Mac OS X.&lt;/p&gt;
&lt;p&gt;The script used to &lt;a href=&quot;https://github.com/ddnet/ddnet-scripts/blob/master/ddnet-release.sh&quot;&gt;build DDNet&lt;/a&gt; as well as the &lt;a href=&quot;https://github.com/ddnet/ddnet-scripts/blob/master/ddnet-lib-update.sh&quot;&gt;instructions for updating library builds&lt;/a&gt; are based on this.&lt;/p&gt;
&lt;h2 id=&quot;backups&quot;&gt;Backups&lt;/h2&gt;
&lt;p&gt;As usual, we nearly forgot about backups. Even if this is the last chapter, it should not be an afterthought.&lt;/p&gt;
&lt;p&gt;I wrote &lt;a href=&quot;https://github.com/def-/rrb/blob/master/rrb&quot;&gt;rrb&lt;/a&gt; (reverse rsync backup) 10 years ago to wrap rsync so that I only need to give the backup server root SSH rights to the computers that it is backing up. Surprisingly rrb needed 0 changes in the last 10 years, even though I kept using it the entire time.&lt;/p&gt;
&lt;p&gt;The backups are stored straight on the filesystem. Incremental backups are implemented using hard links (&lt;code&gt;--link-dest&lt;/code&gt;). A simple &lt;a href=&quot;https://github.com/def-/rrb/blob/master/config.example&quot;&gt;config&lt;/a&gt; defines how long backups are kept, which defaults to:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;KEEP_RULES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=(&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
   &lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;7&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# One backup a day for the last 7 days&lt;/span&gt;
  &lt;span class=&quot;m&quot;&gt;31&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# 8 more backups for the last month&lt;/span&gt;
 &lt;span class=&quot;m&quot;&gt;365&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# 11 more backups for the last year&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1825&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# 4 more backups for the last 5 years&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since some of my computers don’t have a static IP / DNS entry and I still want to back them up using rrb I use a reverse SSH tunnel (as a systemd service) for them:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-cfg&quot; data-lang=&quot;cfg&quot;&gt;&lt;span class=&quot;k&quot;&gt;[Unit]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Reverse SSH Tunnel&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;network.target&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;[Service]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/usr/bin/ssh -N -R 27276:localhost:22 -o &quot;ExitOnForwardFailure yes&quot; server&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;KillMode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;process&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;always&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;[Install]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;multi-user.target&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now the server can reach the client through &lt;code&gt;ssh -p 27276 localhost&lt;/code&gt; while the tunnel is running to perform the backup, or in &lt;code&gt;.ssh/config&lt;/code&gt; format:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Host cr-remote
  HostName localhost
  Port 27276
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;While talking about SSH hacks, sometimes a server is not easily reachable thanks to some bad routing. In that case you can route the SSH connection through another server to get better routing, in this case going through the USA to reach my Chinese server which had not been reliably reachable from Germany for a few weeks:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Host chn.ddnet.tw
  ProxyCommand ssh -q usa.ddnet.tw nc -q0 chn.ddnet.tw 22
  Port 22
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;Thanks for reading my random collection of tools. I probably forgot many programs that I use so naturally every day that I don’t even think about them anymore. Let’s see how stable my software setup stays in the next years. If you have any questions, feel free to get in touch with me at &lt;a href=&quot;mailto:dennis@felsin9.de&quot;&gt;dennis@felsin9.de&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Comments on &lt;a href=&quot;https://news.ycombinator.com/item?id=19253072&quot;&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Tue, 26 Feb 2019 09:39:52 +0000</pubDate>
<dc:creator>def-</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://hookrace.net/blog/linux-desktop-setup/</dc:identifier>
</item>
<item>
<title>Universal Binaries Using WASM</title>
<link>https://github.com/wasmerio/wasmer</link>
<guid isPermaLink="true" >https://github.com/wasmerio/wasmer</guid>
<description>&lt;div class=&quot;Box-body p-6&quot;&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://wasmer.io&quot; rel=&quot;nofollow&quot;&gt;&lt;img width=&quot;400&quot; src=&quot;https://raw.githubusercontent.com/wasmerio/wasmer/master/logo.png&quot; alt=&quot;Wasmer logo&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://circleci.com/gh/wasmerio/wasmer/&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/d485b1e206b858beff9b7fa0c2117d9c7e69fc05/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f70726f6a6563742f6769746875622f7761736d6572696f2f7761736d65722f6d61737465722e737667&quot; alt=&quot;Build Status&quot; data-canonical-src=&quot;https://img.shields.io/circleci/project/github/wasmerio/wasmer/master.svg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/eb374d925576cf2f0cf67c18eaf31c3f5a257417/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7761736d6572696f2f7761736d65722e737667&quot; alt=&quot;License&quot; data-canonical-src=&quot;https://img.shields.io/github/license/wasmerio/wasmer.svg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://spectrum.chat/wasmer&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/3cc3d27f23a2c3948de24fc02c58bc576655d621/68747470733a2f2f77697468737065637472756d2e6769746875622e696f2f62616467652f62616467652e737667&quot; alt=&quot;Join the Wasmer Community&quot; data-canonical-src=&quot;https://withspectrum.github.io/badge/badge.svg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://wasmer.io/&quot; rel=&quot;nofollow&quot;&gt;Wasmer&lt;/a&gt; is a standalone JIT WebAssembly runtime, aiming to be fully compatible with Emscripten, Rust and Go.&lt;/p&gt;
&lt;p&gt;Install Wasmer with:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
curl https://get.wasmer.io -sSfL &lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt; sh
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NEW ✨&lt;/strong&gt;: You can now embed Wasmer in your Rust application, check our &lt;a href=&quot;https://github.com/wasmerio/wasmer-rust-example&quot;&gt;example repo&lt;/a&gt; to see how!&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Usage&lt;/h3&gt;
&lt;p&gt;Wasmer can execute both the standard binary format (&lt;code&gt;.wasm&lt;/code&gt;) and the text format defined by the WebAssembly reference interpreter (&lt;code&gt;.wat&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Once installed, you will be able to run any WebAssembly files (&lt;em&gt;including nginx and Lua!&lt;/em&gt;):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Run Lua&lt;/span&gt;
wasmer run examples/lua.wasm

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Run nginx&lt;/span&gt;
wasmer run examples/nginx/nginx.wasm -- -p examples/nginx -c nginx.conf
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Code Structure&lt;/h2&gt;
&lt;p&gt;Wasmer is structured into different directories:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/src&quot;&gt;&lt;code&gt;src&lt;/code&gt;&lt;/a&gt;: code related to the Wasmer executable itself&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/lib&quot;&gt;&lt;code&gt;lib&lt;/code&gt;&lt;/a&gt;: modularized libraries that Wasmer uses under the hood&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/examples&quot;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;: some useful examples to getting started with Wasmer&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Dependencies&lt;/h2&gt;
&lt;p&gt;Building Wasmer requires &lt;a href=&quot;https://rustup.rs/&quot; rel=&quot;nofollow&quot;&gt;rustup&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To build on Windows, download and run &lt;a href=&quot;https://win.rustup.rs/&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt; then follow the onscreen instructions.&lt;/p&gt;
&lt;p&gt;To build on other systems, run:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
curl https://sh.rustup.rs -sSf &lt;span class=&quot;pl-k&quot;&gt;|&lt;/span&gt; sh
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Other dependencies&lt;/h3&gt;
&lt;p&gt;Please select your operating system:&lt;/p&gt;
&lt;h4&gt;macOS&lt;/h4&gt;
&lt;p&gt;If you have &lt;a href=&quot;https://brew.sh/&quot; rel=&quot;nofollow&quot;&gt;Homebrew&lt;/a&gt; installed:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
brew install cmake
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Or, in case you have &lt;a href=&quot;https://www.macports.org/install.php&quot; rel=&quot;nofollow&quot;&gt;MacPorts&lt;/a&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
sudo port install cmake
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Debian-based Linuxes&lt;/h4&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
sudo apt install cmake
&lt;/pre&gt;&lt;/div&gt;
&lt;h4&gt;Windows (MSVC)&lt;/h4&gt;
&lt;p&gt;Windows support is &lt;em&gt;highly experimental&lt;/em&gt;. Only simple Wasm programs may be run, and no syscalls are allowed. This means nginx and Lua do not work on Windows. See &lt;a href=&quot;https://github.com/wasmerio/wasmer/issues/176&quot;&gt;this issue&lt;/a&gt; regarding Emscripten syscall polyfills for Windows.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Install &lt;a href=&quot;https://www.python.org/downloads/release/python-2714/&quot; rel=&quot;nofollow&quot;&gt;Python for Windows&lt;/a&gt;. The Windows x86-64 MSI installer is fine. Make sure to enable &quot;Add python.exe to Path&quot; during installation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;a href=&quot;https://git-scm.com/download/win&quot; rel=&quot;nofollow&quot;&gt;Git for Windows&lt;/a&gt;. Allow it to add &lt;code&gt;git.exe&lt;/code&gt; to your PATH (default settings for the installer are fine).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;a href=&quot;https://cmake.org/download/&quot; rel=&quot;nofollow&quot;&gt;CMake&lt;/a&gt;. Ensure CMake is in your PATH.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;Building&lt;/h2&gt;
&lt;p&gt;Wasmer is built with &lt;a href=&quot;https://crates.io/&quot; rel=&quot;nofollow&quot;&gt;Cargo&lt;/a&gt;, the Rust package manager.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; checkout code&lt;/span&gt;
git clone https://github.com/wasmerio/wasmer.git
&lt;span class=&quot;pl-c1&quot;&gt;cd&lt;/span&gt; wasmer

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; install tools&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; make sure that `python` is accessible.&lt;/span&gt;
cargo install --path &lt;span class=&quot;pl-c1&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Testing&lt;/h2&gt;
&lt;p&gt;Thanks to &lt;a href=&quot;https://github.com/wasmerio/wasmer/tree/master/lib/spectests/spectests&quot;&gt;spec tests&lt;/a&gt; we can ensure 100% compatibility with the WebAssembly spec test suite.&lt;/p&gt;
&lt;p&gt;Tests can be run with:&lt;/p&gt;

&lt;p&gt;If you need to regenerate the Rust tests from the spec tests you can run:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
make spectests
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also run integration tests with:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
make integration-tests
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Benchmarking&lt;/h2&gt;
&lt;p&gt;Benchmarks can be run with:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
cargo bench --all
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Wasmer is an open project guided by strong principles, aiming to be modular, flexible and fast. It is open to the community to help set its direction.&lt;/p&gt;
&lt;p&gt;Below are some of the goals of this project (in order of priority):&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;If you would like to know how Wasmer works under the hood, please see &lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/ARCHITECTURE.md&quot;&gt;ARCHITECTURE.md&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;Wasmer is primarily distributed under the terms of the &lt;a href=&quot;http://opensource.org/licenses/MIT&quot; rel=&quot;nofollow&quot;&gt;MIT license&lt;/a&gt; (&lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/LICENSE&quot;&gt;LICENSE&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/wasmerio/wasmer/blob/master/ATTRIBUTIONS.md&quot;&gt;ATTRIBUTIONS&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Tue, 26 Feb 2019 06:39:04 +0000</pubDate>
<dc:creator>grey-area</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/44205449?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>wasmerio/wasmer</og:title>
<og:url>https://github.com/wasmerio/wasmer</og:url>
<og:description>Universal Binaries Powered by WebAssembly. Contribute to wasmerio/wasmer development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/wasmerio/wasmer</dc:identifier>
</item>
<item>
<title>Humans who are not concentrating are not general intelligences</title>
<link>https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/</link>
<guid isPermaLink="true" >https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/</guid>
<description>&lt;p&gt;Recently, OpenAI came out with a &lt;a href=&quot;https://blog.openai.com/better-language-models/&quot;&gt;new language model&lt;/a&gt; that automatically synthesizes text, called GPT-2.&lt;/p&gt;
&lt;p&gt;It’s disturbingly good.  You can see some examples (cherry-picked, by their own admission) in OpenAI’s &lt;a href=&quot;https://blog.openai.com/better-language-models/&quot;&gt;post&lt;/a&gt; and in the related &lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;technical paper.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I’m not going to write about the machine learning here, but about the examples and what we can infer from them.&lt;/p&gt;
&lt;p&gt;The scary thing about GPT-2-generated text is that it flows very naturally if you’re just skimming, reading for writing style and key, evocative words.  The “unicorn” sample reads like a real science press release. The “theft of nuclear material” sample reads like a real news story. The “Miley Cyrus shoplifting” sample reads like a real post from a celebrity gossip site.  The “GPT-2” sample reads like a real OpenAI press release. The “Legolas and Gimli” sample reads like a real fantasy novel. The “Civil War homework assignment” reads like a real C-student’s paper.  The “JFK acceptance speech” reads like a real politician’s speech.  The “recycling” sample reads like a real right-wing screed.&lt;/p&gt;
&lt;p&gt;If I just skim, without focusing, they all look &lt;em&gt;totally normal. &lt;/em&gt;I would not have noticed they were machine-generated. I would not have noticed anything amiss about them at all.&lt;/p&gt;
&lt;p&gt;But if I read with focus, I notice that they don’t make a lot of logical sense.&lt;/p&gt;
&lt;p&gt;For instance, in the unicorn sample:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Wait a second, “Ovid” doesn’t refer to a “distinctive horn”, so why would naming them “Ovid’s Unicorn” be naming them after a distinctive horn?  Also, you just said they had &lt;em&gt;one &lt;/em&gt;horn, so why are you saying they have &lt;em&gt;four &lt;/em&gt;horns in the next sentence?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Wait, &lt;em&gt;unicorns &lt;/em&gt;originated from the interbreeding of humans and … unicorns?  That’s circular, isn’t it?&lt;/p&gt;
&lt;p&gt;Or, look at the GPT-2 sample:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We believe this project is the first step in the direction of developing large NLP systems without task-specific training data. That is, we are developing a machine language system in the generative style with no explicit rules for producing text.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Except the second sentence &lt;em&gt;isn’t &lt;/em&gt;a restatement of the first sentence — “task-specific training data” and “explicit rules for producing text” aren’t synonyms!  So saying “That is” doesn’t make sense.&lt;/p&gt;
&lt;p&gt;Or look at the LOTR sample:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Aragorn drew his sword, and the Battle of Fangorn was won. As they marched out through the thicket the morning mist cleared, and the day turned to dusk.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yeah, day doesn’t turn to dusk in the &lt;em&gt;morning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Or in the “resurrected JFK” sample:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(1) The brain of JFK was harvested and reconstructed via tissue sampling. There was no way that the tissue could be transported by air. (2) A sample was collected from the area around his upper chest and sent to the University of Maryland for analysis. A human brain at that point would be about one and a half cubic centimeters. The data were then analyzed along with material that was obtained from the original brain to produce a reconstruction; in layman’s terms, a “mesh” of brain tissue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;His brain tissue was harvested…from his chest?!  A human brain is one and a half cubic centimeters?!&lt;/p&gt;
&lt;p&gt;So, ok, this isn’t actually human-equivalent writing ability. OpenAI doesn’t claim it is, for what it’s worth — I’m not trying to diminish their accomplishment, that’s not the point of this post.  The point is, &lt;em&gt;if you skim text, you miss obvious absurdities&lt;/em&gt;.  The point is &lt;em&gt;OpenAI HAS achieved the ability to pass the Turing test against humans on autopilot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The point is, I know of a few people, acquaintances of mine, who, even when asked to try to find flaws, &lt;em&gt;could not detect anything weird or mistaken in the GPT-2-generated samples&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There are probably a lot of people who would be completely taken in by literal “fake news”, as in, computer-generated fake articles and blog posts.  This is pretty alarming.  Even more alarming: unless I make a conscious effort to read carefully, &lt;em&gt;I would be one of them&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Robin Hanson’s post &lt;a href=&quot;http://www.overcomingbias.com/2017/03/better-babblers.html&quot;&gt;Better Babblers&lt;/a&gt; is very relevant here.  He claims, and I don’t think he’s exaggerating, that a lot of human speech is simply generated by “low order correlations”, that is, generating sentences or paragraphs that are statistically likely to come after previous sentences or paragraphs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After eighteen years of being a professor, I’ve graded &lt;em&gt;many&lt;/em&gt; student essays. And while I usually try to teach a deep structure of concepts, what the median student actually learns seems to mostly be a set of low order correlations. They know what words to use, which words tend to go together, which combinations tend to have positive associations, and so on. But if you ask an exam question where the deep structure answer differs from answer you’d guess looking at low order correlations, most students usually give the wrong answer.&lt;/p&gt;
&lt;p&gt;Simple correlations also seem sufficient to capture most polite conversation talk, such as the weather is nice, how is your mother’s illness, and damn that other political party. Simple correlations are also most of what I see in inspirational TED talks, and when public intellectuals and talk show guests pontificate on topics they really don’t understand, such as quantum mechanics, consciousness, postmodernism, or the need always for more regulation everywhere. After all, media entertainers don’t need to understand deep structures any better than do their audiences.&lt;/p&gt;
&lt;p&gt;Let me call styles of talking (or music, etc.) that rely mostly on low order correlations “babbling”. Babbling isn’t meaningless, but to ignorant audiences it often appears to be based on a deeper understanding than is actually the case. When done well, babbling can be entertaining, comforting, titillating, or exciting. It just isn’t usually a good place to learn deep insight.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I used to half-joke that the &lt;a href=&quot;http://sebpearce.com/bullshit/&quot;&gt;New Age Bullshit Generator&lt;/a&gt; was &lt;em&gt;actually useful &lt;/em&gt;as a way to get myself to feel more optimistic. The truth is, it isn’t quite good enough to match the “aura” or “associations” of genuine, human-created inspirational text. GPT-2, though, &lt;em&gt;is&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I also suspect that the “lyrical” or “free-associational” function of poetry is adequately matched by GPT-2.  The &lt;a href=&quot;http://antinegationism.tumblr.com/post/182901133106/an-eternal-howl&quot;&gt;autocompletions of &lt;em&gt;Howl &lt;/em&gt;&lt;/a&gt;read a lot like Allen Ginsberg — they just don’t imply the same beliefs about the world.  (&lt;em&gt;Moloch whose heart is crying for justice!&lt;/em&gt; sounds rather positive.)&lt;/p&gt;
&lt;p&gt;I’ve noticed that I cannot tell, from casual conversation, whether someone is intelligent in the IQ sense.&lt;/p&gt;
&lt;p&gt;I’ve interviewed job applicants, and perceived them all as “bright and impressive”, but found that the vast majority of them could not solve a simple math problem.  The ones who could solve the problem didn’t appear any “brighter” in conversation than the ones who couldn’t.&lt;/p&gt;
&lt;p&gt;I’ve taught public school teachers, who were &lt;em&gt;incredibly &lt;/em&gt;bad at formal mathematical reasoning (I know, because I graded their tests), to the point that I had not realized humans could be that bad at math — but it had &lt;em&gt;no &lt;/em&gt;effect on how they came across in friendly conversation after hours. They didn’t seem “dopey” or “slow”, they were witty and engaging and warm.&lt;/p&gt;
&lt;p&gt;I’ve read the personal blogs of intellectually disabled people — people who, by definition, score poorly on IQ tests — and &lt;em&gt;they &lt;/em&gt;don’t read as any less funny or creative or relatable than anyone else.&lt;/p&gt;
&lt;p&gt;Whatever ability IQ tests and math tests measure, I believe that lacking that ability doesn’t have &lt;em&gt;any &lt;/em&gt;effect on one’s ability to make a good social impression or even to “seem smart” in conversation.&lt;/p&gt;
&lt;p&gt;If “human intelligence” is about reasoning ability, the capacity to detect whether arguments make sense, then you simply do not need human intelligence to create a linguistic style or aesthetic that can fool our pattern-recognition apparatus if we don’t concentrate on parsing content.&lt;/p&gt;
&lt;p&gt;I also noticed, upon reading GPT2 samples, just how often my brain slides from focused attention to just skimming. I read the paper’s sample about Spanish history with interest, and the GPT2-generated text was obviously absurd. My eyes glazed over during the sample about video games, since I don’t care about video games, and the machine-generated text looked totally unobjectionable to me. My brain is constantly making evaluations about what’s worth the trouble to focus on, and what’s ok to tune out. GPT2 is actually really useful as a *test* of one’s level of attention.&lt;/p&gt;
&lt;p&gt;This is related to my hypothesis in &lt;a href=&quot;https://srconstantin.wordpress.com/2017/10/10/distinctions-in-types-of-thought/&quot; rel=&quot;nofollow&quot;&gt;https://srconstantin.wordpress.com/2017/10/10/distinctions-in-types-of-thought/&lt;/a&gt; that effortless pattern-recognition is what machine learning can do today, while effortful attention, and explicit reasoning (which seems to be a subset of effortful attention) is generally beyond ML’s current capabilities.&lt;/p&gt;
&lt;p&gt;Beta waves in the brain are usually associated with focused concentration or active or anxious thought, while alpha waves are associated with the relaxed state of being awake but with closed eyes, before falling asleep, or while dreaming. Alpha waves sharply reduce after a subject makes a mistake and begins paying closer attention. I’d be interested to see whether ability to tell GPT2-generated text from human-generated text correlates with alpha waves vs. beta waves.&lt;/p&gt;
&lt;p&gt;The first-order effects of highly effective text-generators are scary. It will be incredibly easy and cheap to fool people, to manipulate social movements, etc. There’s a lot of opportunity for bad actors to take advantage of this.&lt;/p&gt;
&lt;p&gt;The second-order effects might well be good, though. If only conscious, focused logical thought can detect a bot, maybe some people will become more aware of when they’re thinking actively vs not, and will be able to flag when they’re not really focusing, and distinguish the impressions they absorb in a state of autopilot from “real learning”.&lt;/p&gt;
&lt;p&gt;The mental motion of “I didn’t really parse that paragraph, but sure, whatever, I’ll take the author’s word for it” is, in my introspective experience, absolutely identical to “I didn’t really parse that paragraph because it was bot-generated and didn’t make any sense so I couldn’t possibly have parsed it”, except that in the first case, I assume that the error lies with me rather than the text.  This is not a safe assumption in a post-GPT2 world. Instead of “default to humility” (assume that when you don’t understand a passage, the passage is true and you’re just missing something) the ideal mental action in a world full of bots is “default to null” (if you don’t understand a passage, assume you’re in the same epistemic state as if you’d never read it at all.)&lt;/p&gt;
&lt;p&gt;Maybe practice and experience with GPT2 will help people get better at doing “default to null”?&lt;/p&gt;

&lt;div class=&quot;wpcnt&quot;&gt;
&lt;div class=&quot;wpa wpmrec&quot;&gt;&lt;span class=&quot;wpa-about&quot;&gt;Advertisements&lt;/span&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-67143906-9297-5c7562834a0e3&quot; data-src=&quot;//widgets.wp.com/likes/index.html?ver=20180319#blog_id=67143906&amp;amp;post_id=9297&amp;amp;origin=srconstantin.wordpress.com&amp;amp;obj_id=67143906-9297-5c7562834a0e3&quot; data-name=&quot;like-post-frame-67143906-9297-5c7562834a0e3&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;
&lt;div class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
<pubDate>Tue, 26 Feb 2019 03:22:00 +0000</pubDate>
<dc:creator>jseliger</dc:creator>
<og:type>article</og:type>
<og:title>Humans Who Are Not Concentrating Are Not General Intelligences</og:title>
<og:url>https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/</og:url>
<og:description>Recently, OpenAI came out with a new language model that automatically synthesizes text, called GPT-2. It’s disturbingly good.  You can see some examples (cherry-picked, by their own admissio…</og:description>
<og:image>https://secure.gravatar.com/blavatar/e6f1f7f1406d0a73a37a43771ed072ba?s=200&amp;ts=1551196803</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/</dc:identifier>
</item>
<item>
<title>Go 1.12 Released</title>
<link>https://blog.golang.org/go1.12</link>
<guid isPermaLink="true" >https://blog.golang.org/go1.12</guid>
<description>&lt;h3 class=&quot;title&quot;&gt;&lt;a href=&quot;https://blog.golang.org/go1.12&quot;&gt;Go 1.12 is released&lt;/a&gt;&lt;/h3&gt;
&lt;p class=&quot;date&quot;&gt;25 February 2019&lt;/p&gt;
&lt;p&gt;Today the Go team is happy to announce the release of Go 1.12. You can get it from the &lt;a href=&quot;https://golang.org/dl/&quot; target=&quot;_blank&quot;&gt;download page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For details about the changes in Go 1.12, see the &lt;a href=&quot;https://golang.org/doc/go1.12&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Go 1.12 release notes&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some of the highlights include &lt;a href=&quot;https://golang.org/doc/go1.12#tls_1_3&quot; target=&quot;_blank&quot;&gt;opt-in support for TLS 1.3&lt;/a&gt;, &lt;a href=&quot;https://golang.org/doc/go1.12#modules&quot; target=&quot;_blank&quot;&gt;improved modules support&lt;/a&gt; (in preparation &lt;a href=&quot;https://blog.golang.org/modules2019&quot; target=&quot;_self&quot;&gt;for being the default in Go 1.13&lt;/a&gt;), support for &lt;code&gt;windows/arm&lt;/code&gt;, and &lt;a href=&quot;https://golang.org/doc/go1.12#darwin&quot; target=&quot;_blank&quot;&gt;improved macOS &amp;amp; iOS forwards compatibility.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As always, we also want to thank everyone who contributed to this release by writing code, filing bugs, providing feedback, and/or testing the betas and release candidates. Your contributions and diligence helped to ensure that Go 1.12 is as stable as possible. That said, if you do notice any problems, please &lt;a href=&quot;https://golang.org/issues/new&quot; target=&quot;_blank&quot;&gt;file an issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Enjoy the new release!&lt;/p&gt;
&lt;p class=&quot;author&quot;&gt;By Andrew Bonventre&lt;/p&gt;
</description>
<pubDate>Tue, 26 Feb 2019 01:13:10 +0000</pubDate>
<dc:creator>crawshaw</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.golang.org/go1.12</dc:identifier>
</item>
<item>
<title>Myths in Machine Learning Research</title>
<link>https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/</link>
<guid isPermaLink="true" >https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/</guid>
<description>&lt;time datetime=&quot;2019-02-16T00:00:00+00:00&quot; class=&quot;post-date&quot;&gt;16 Feb 2019&lt;/time&gt;&lt;p class=&quot;message&quot;&gt;&lt;strong&gt;tldr;&lt;/strong&gt; We present seven myths commonly believed to be true in machine learning research, circa Feb 2019. Also available on the &lt;a href=&quot;https://arxiv.org/pdf/1902.06789.pdf&quot;&gt;ArXiv&lt;/a&gt; in pdf form.&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-1&quot;&gt;Myth 1:&lt;/a&gt; TensorFlow is a Tensor manipulation library&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-2&quot;&gt;Myth 2:&lt;/a&gt; Image datasets are representative of real images found in the wild&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-3&quot;&gt;Myth 3:&lt;/a&gt; Machine Learning researchers do not use the test set for validation&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-4&quot;&gt;Myth 4:&lt;/a&gt; Every datapoint is used in training a neural network&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-5&quot;&gt;Myth 5:&lt;/a&gt; We need (batch) normalization to train very deep residual networks&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-6&quot;&gt;Myth 6:&lt;/a&gt; Attention &amp;gt; Convolution&lt;br/&gt;&lt;a href=&quot;https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/#myth-7&quot;&gt;Myth 7:&lt;/a&gt; Saliency maps are robust ways to interpret neural networks&lt;/p&gt;
&lt;h3 id=&quot;myth-1-tensorflow-is-a-tensor-manipulation-library&quot;&gt;Myth 1: TensorFlow is a Tensor manipulation library&lt;/h3&gt;
&lt;p&gt;It is actually a &lt;em&gt;Matrix&lt;/em&gt; manipulation library, and this difference is significant.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&quot;https://papers.nips.cc/paper/7540-computing-higher-order-derivatives-of-matrix-and-tensor-expressions.pdf&quot;&gt;Computing Higher Order Derivatives of Matrix and Tensor Expressions. Laue et al. NeurIPS 2018.&lt;/a&gt;, the authors demonstrate that their automatic differentiation library based on actual Tensor Calculus has significantly more compact expression trees. This is because Tensor Calculus uses index notation, which results in treating both the forward mode and the reverse mode in the same manner.&lt;/p&gt;
&lt;p&gt;By contrast, Matrix Calculus hides the indices for notational convenience, and this often results in overly complicated automatic differentiation expression trees.&lt;/p&gt;
&lt;p&gt;Consider the matrix multiplication . We have  for the forward mode and  for the reverse mode. To perform the multiplications correctly, we have to be careful about the order of multiplication and the use of transposes. Notationally, this is a point of confusion for the machine learning practitioner, but computationally, this is an overhead for the program.&lt;/p&gt;
&lt;p&gt;Here’s another example, which is decidedly less trivial: . We have  for the forward mode, and  for the reverse mode. In this case, it is clearly not possible to use the same expression tree for both modes, given that they are composed of different operations.&lt;/p&gt;
&lt;p&gt;In general, the way TensorFlow and other libraries (e.g. Mathematica, Maple, Sage, SimPy, ADOL-C, TAPENADE, TensorFlow, Theano, PyTorch, HIPS autograd) implement automatic differentiation results in different and inefficient expression trees for the forward and reverse mode. Tensor calculus conveniently avoids these problems by having commutativity in multiplication as a result of its index notation. (Please read the actual paper to learn more about how this works.)&lt;/p&gt;
&lt;p&gt;The authors tested their method of doing reverse-mode automatic differentiation, aka backpropagation, on three different problems and measured the amount of time it took to compute the Hessians.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_1_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The first problem involves optimizing a quadratic function like . The second problem solves for logistic regression, while the third problem solves for matrix factorization.&lt;/p&gt;
&lt;p&gt;On the CPU, their method was faster than popular automatic differentiation libraries like TensorFlow, Theano, PyTorch, and HIPS autograd by &lt;em&gt;two&lt;/em&gt; orders of magnitude.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_1_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;On the GPU, they observed an even greater speedup, outperforming these libraries by a factor of &lt;em&gt;three&lt;/em&gt; orders of magnitude.&lt;/p&gt;
&lt;h4 id=&quot;implication&quot;&gt;Implication:&lt;/h4&gt;
&lt;p&gt;Computing derivatives for quadratic or higher functions with current deep learning libraries is more expensive than it needs to be. This includes computing general fourth order tensors like the Hessian (e.g. in MAML and second-order Newton optimization). Fortunately, quadratic functions are not common in “deep learning.” But they are common in “classical” machine learning - dual of an SVM, least squares regression, LASSO, Gaussian Processes, etc.&lt;/p&gt;
&lt;h3 id=&quot;myth-2-image-datasets-are-representative-of-real-images-found-in-the-wild&quot;&gt;Myth 2: Image datasets are representative of real images found in the wild&lt;/h3&gt;
&lt;p&gt;We like to think that neural networks are now better than humans at the task of object recognition. This is not true. They might outperform humans on select image datasets like ImageNet, but given actual images found in the wild, they are most definitely not going to be better than a regular adult human at recognizing objects. This is because images found in current image datasets are not actually drawn from the same distribution as the set of all possible images naturally occurring in the wild.&lt;/p&gt;
&lt;p&gt;In an old paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/5995347&quot;&gt;Unbiased Look at Dataset Bias. Torralba and Efros. CVPR 2011.&lt;/a&gt;, the authors proposed to examine dataset bias in twelve popular image datasets by observing if it is possible to train a classifier to identify the dataset a given image is selected from.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_2_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The chance of getting it right by random is , while their lab members performed at .&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_2_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;They trained an SVM on HOG features, and found that their classifier performed at , way above chance. If the same experiment was repeated today with a state of the art CNN, we will probably see a further increase in classifier performance.&lt;/p&gt;
&lt;p&gt;If image datasets are truly representative of real images found in the wild, we ought to not be able to distinguish which dataset a given image originates from.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_2_2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;But there are biases in the data that make each dataset distinctive. For example, there are many race cars in the ImageNet dataset, which cannot be said to represent the “platonic” concept of a car in general.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_2_3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors further judged the value of a dataset by measuring how well a classifier trained on it performs on other datasets. By this metric, LabelMe and ImageNet are the least biased datasets, scoring  in a “basket of currencies.” The values are all less than one, which means that training on a different dataset always results in lower test performance. In an ideal world without dataset bias, some of these values should be above one.&lt;/p&gt;
&lt;p&gt;The authors pessimistically concluded:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;So, what is the value of current datasets when used to train algorithms that will be deployed in the real world? The answer that emerges can be summarized as: “better than nothing, but not by much”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;myth-3-machine-learning-researchers-do-not-use-the-test-set-for-validation&quot;&gt;Myth 3: Machine Learning researchers do not use the test set for validation&lt;/h3&gt;
&lt;p&gt;In Machine Learning 101, we are taught to split a dataset into training, validation, and test sets. The performance of a model trained on the training set and evaluated on the validation set helps the machine learning practitioner tune his model to maximize its performance in real world usage. The test set should be held out until the practitioner is done with the tuning so as to provide an unbiased estimate of the model’s actual performance in real world usage. If the practitioner “cheats” by using the test set in the training or validation process, he runs the risk of overfitting his model to biases inherent in the dataset that do not generalize beyond the dataset.&lt;/p&gt;
&lt;p&gt;In the hyper-competitive world of machine learning research, new algorithms and models are often evaluated using their performance on the test set. Thus, there is little reason for researchers to write or submit papers that propose methods with inferior test performance. This effectively means that the machine learning research community, as a whole, is using the test set for validation.&lt;/p&gt;
&lt;p&gt;What is the impact of this “cheating”?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_3_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors of &lt;a href=&quot;https://arxiv.org/pdf/1806.00451.pdf&quot;&gt;Do CIFAR-10 Classifiers Generalize to CIFAR-10? Recht et al. ArXiv 2018.&lt;/a&gt; investigated this by creating a new test set for CIFAR-10. They did this by parsing images from the Tiny Images repository, as was done in the original dataset collection process.&lt;/p&gt;
&lt;p&gt;They chose CIFAR-10 because it is one of the most widely used datasets in machine learning, being the second most popular dataset in NeurIPS 2017 (after MNIST). The dataset creation process for CIFAR-10 is also well-documented and transparent, with the large Tiny Images repository having sufficiently fine-grained labels that make it possible to replicate a new test set while minimizing distributional shift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_3_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;They found that across a wide range of different neural network models, there was a significant drop in accuracy () from the old test set to the new test set. However, the relative ranking of each model’s performance remained fairly stable.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_3_2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In general, the higher performing models experienced a smaller drop in accuracy compared to the lower performing models. This is heartening, because it indicates that the loss in generalization caused by the “cheating,” at least in the case of CIFAR-10, becomes more muted as the research community invents better machine learning models and methods.&lt;/p&gt;
&lt;h3 id=&quot;myth-4-every-datapoint-is-used-in-training-a-neural-network&quot;&gt;Myth 4: Every datapoint is used in training a neural network&lt;/h3&gt;
&lt;p&gt;Conventional wisdom says that &lt;a href=&quot;https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data&quot;&gt;data is the new oil&lt;/a&gt; and the more data we have, the better we can train our sample-inefficient and overparametrized deep learning models.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&quot;https://openreview.net/pdf?id=BJlxm30cKm&quot;&gt;An Empirical Study of Example Forgetting During Deep Neural Network Learning. Toneva et al. ICLR 2019.&lt;/a&gt;, the authors demonstrate significant redundancy in several common small image datasets. Shockingly,  of the datapoints in CIFAR-10 can be removed, without changing test accuracy by much.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_4_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;A forgetting event happens when the neural network makes a misclassification at time , having already made an accurate classification at time , where we consider the flow of time to be the number of SGD updates made to the network. To make the tracking of forgetting events tractable, the authors run their neural network over only the examples in the mini-batch every time an SGD update is made, rather than over every single example in the dataset. Examples that do not undergo a forgetting event are called &lt;em&gt;unforgettable&lt;/em&gt; examples.&lt;/p&gt;
&lt;p&gt;They find that  of MNIST,  of permutedMNIST,  of CIFAR-10, and  of CIFAR-100 comprise of unforgettable examples. This makes intuitive sense, since an increase in the diversity and complexity of an image dataset should cause the neural network to forget more examples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_4_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Forgettable examples seem to display more uncommon and peculiar features than unforgettable examples. The authors liken them to support vectors in SVM, because they seem to demarcate the contours of the decision boundary.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_4_2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Unforgettable examples, by contrast, encode mostly redundant information. If we sort the examples by their unforgettability, we can compress the dataset by removing the most unforgettable examples.&lt;/p&gt;
&lt;p&gt;On CIFAR-10,  of the dataset can be removed without affecting test accuracy, while a  removal causes a trivial  dip in test accuracy. If this  was selected by random instead of chosen by unforgettability, then its removal will result in a significant loss of around  in test accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_4_3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Similarly, on CIFAR-100,  of the dataset can be removed without affecting test accuracy.&lt;/p&gt;
&lt;p&gt;These findings show that there is significant data redundancy in neural network training, much like in SVM training where the non-support vectors can be taken away without affecting the decisions of the model.&lt;/p&gt;
&lt;h4 id=&quot;implication-1&quot;&gt;Implication:&lt;/h4&gt;
&lt;p&gt;If we can determine which examples are unforgettable before the start of training, then we can save space by removing those examples and save time by not training the neural network on them.&lt;/p&gt;
&lt;h3 id=&quot;myth-5-we-need-batch-normalization-to-train-very-deep-residual-networks&quot;&gt;Myth 5: We need (batch) normalization to train very deep residual networks&lt;/h3&gt;
&lt;p&gt;It had been believed for a very long time that “training a deep network to directly optimize only the supervised objective of interest (for example the log probability of correct classification) by gradient descent, starting from random initialized parameters, does not work very well.”&lt;sup id=&quot;fnref:fn-stacked&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Since then, a host of clever random initialization methods, activation functions, optimization techniques, and other architectural innovations like residual connections, has made it easier to train deep neural networks with gradient descent.&lt;/p&gt;
&lt;p&gt;But the real breakthrough came from the introduction of batch normalization (and other subsequent normalization techniques), which constrained the size of activations at every layer of a deep network to mitigate the vanishing and exploding gradients problem.&lt;/p&gt;
&lt;p&gt;In a recent paper &lt;a href=&quot;https://arxiv.org/pdf/1901.09321.pdf&quot;&gt;Fixup Initialization: Residual Learning Without Normalization. Zhang et al. ICLR 2019.&lt;/a&gt;, it was shown remarkably that it is actually possible to train a -layer deep network using vanilla SGD, without resorting to any normalization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_5_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors compared training a residual network at varying depths for one epoch on CIFAR-10, and found that while standard initialization methods failed for  layers, both Fixup and batch normalization succeeded for  layers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_5_4.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;They did a theoretical analysis to show that “the gradient norm of certain layers is in expectation lower bounded by a quantity that increases indefinitely with the network depth,” i.e. the exploding gradients problem.&lt;/p&gt;
&lt;p&gt;To prevent this, the key idea in Fixup is to scale the weights in the  layers for each of  residual branches by a factor that depends on  and .&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_5_3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Fixup enabled the training of a deep residual network with  layers on CIFAR-10 with a large learning rate, at comparable test performance with the same network architecture that had batch normalization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_5_2.png&quot; alt=&quot;&quot;/&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_5_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors also further showed comparable test results using a Fixup-ed network without any normalization on the ImageNet dataset and English-German machine translation tasks.&lt;/p&gt;
&lt;h3 id=&quot;myth-6-attention--convolution&quot;&gt;Myth 6: Attention &amp;gt; Convolution&lt;/h3&gt;
&lt;p&gt;There is an idea gaining currency in the machine learning community that attention mechanisms are a superior alternative to convolutions. &lt;sup id=&quot;fnref:fn-attention&quot;/&gt; Importantly, &lt;a href=&quot;(https://arxiv.org/abs/1706.03762)&quot;&gt;Vaswani et al.&lt;/a&gt; noted that “the computational cost of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer.”&lt;/p&gt;
&lt;p&gt;Even state-of-the-art GANS find self-attention superior to standard convolutions in its ability to model long-range, multi-scale dependencies.&lt;sup id=&quot;fnref:fn-sagan&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors of &lt;a href=&quot;https://openreview.net/pdf?id=SkVhlh09tX&quot;&gt;Pay Less Attention with Lightweight and Dynamic Convolutions. Wu et al. ICLR 2019.&lt;/a&gt; question the parameter efficiency and efficacy of self-attention in modelling long-range dependencies, and propose new variants of convolutions, partially inspired by self-attention, that are more parameter-efficient.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_6_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Lightweight&lt;/em&gt; convolutions are depthwise-separable, softmax-normalized across the temporal dimension, shares weights across the channel dimension, and re-uses the same weights at every time step (like RNNs). &lt;em&gt;Dynamic&lt;/em&gt; convolutions are lightweight convolutions that use different weights at every time step.&lt;/p&gt;
&lt;p&gt;These tricks make lightweight and dynamic convolutions several orders of magnitude more efficient that standard non-separable convolutions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_6_1.png&quot; alt=&quot;&quot;/&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_6_2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The authors show that these new convolutions match or exceed the self-attention baselines in machine translation, language modelling, and abstractive summarization tasks while using comparable or less number of parameters.&lt;/p&gt;
&lt;h3 id=&quot;myth-7-saliency-maps-are-robust-ways-to-interpret-neural-networks&quot;&gt;Myth 7: Saliency maps are robust ways to interpret neural networks&lt;/h3&gt;
&lt;p&gt;While neural networks are commonly believed to be black boxes, there have been many, many attempts made to interpret them. Saliency maps, or other similar methods that assign importance scores to features or training examples, are the most popular form of interpretation.&lt;/p&gt;
&lt;p&gt;It is tempting to be able to conclude that the reason why a given image is classified a certain way is due to particular parts of the image that are salient to the neural network’s decision in making the classification. There are several ways to compute this saliency map, often making use of a neural network’s activations on a given image and the gradients that flow through the network.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/pdf/1710.10547.pdf&quot;&gt;Interpretation of Neural Networks is Fragile. Ghorbani et al. AAAI 2019.&lt;/a&gt;, the authors show that they can introduce an imperceptible perturbation to a given image to distort its saliency map.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_0.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;A monarch butterfly is thus classified as a monarch butterfly, not on account of the patterns on its wings, but because of some unimportant green leaves in the background.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_1.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;High-dimensional images often lie close to the decision boundaries constructed by deep neural networks, hence their susceptibility to adversarial attacks. While adversarial attacks shift images past a decision boundary, adversarial interpretation attacks shift them along the contour of the decision boundary, while still remaining within the same decision territory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_2.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The basic method employed by the authors to do this is a modification of Goodfellow’s fast gradient sign method, which was one of the first efficient adversarial attacks introduced. This suggests that other more recent and sophisticated adversarial attacks can also be used to attack neural network interpretations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_3.png&quot; alt=&quot;&quot;/&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_4.png&quot; alt=&quot;&quot;/&gt;&lt;img src=&quot;https://crazyoscarchang.github.io/images/2019-02-16-seven-myths-in-machine-learning-research/myth_7_5.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h4 id=&quot;implication-2&quot;&gt;Implication:&lt;/h4&gt;
&lt;p&gt;As deep learning becomes more and more ubiquitous in high stakes applications like medical imaging, it is important to be careful of how we interpret decisions made by neural networks. For example, while it would be nice to have a CNN identify a spot on an MRI image as a malignant cancer-causing tumor, these results should not be trusted if they are based on fragile interpretation methods.&lt;/p&gt;

</description>
<pubDate>Mon, 25 Feb 2019 21:39:10 +0000</pubDate>
<dc:creator>crazyoscarchang</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://crazyoscarchang.github.io/2019/02/16/seven-myths-in-machine-learning-research/</dc:identifier>
</item>
<item>
<title>The Last POWER1 CPU on Mars Is Dead</title>
<link>https://www.talospace.com/2019/02/the-last-power1-on-mars-is-dead.html</link>
<guid isPermaLink="true" >https://www.talospace.com/2019/02/the-last-power1-on-mars-is-dead.html</guid>
<description>The &lt;a href=&quot;https://en.wikipedia.org/wiki/Opportunity_(rover)&quot;&gt;&lt;em&gt;Opportunity&lt;/em&gt; Rover&lt;/a&gt;, also known as the Mars Exploration Rover B (or MER-1), has finally been &lt;a href=&quot;https://www.apnews.com/868e731d260f4919b28218dcf345071f&quot;&gt;declared at end of mission today&lt;/a&gt; after 5,352 Mars solar days when NASA was not successfully able to re-establish contact. It had been apparently knocked off-line by a dust storm and was unable to restart either due to power loss or some other catastrophic failure. Originally intended for a 90 Mars solar day mission, its mission became almost 60 times longer than anticipated and it traveled nearly 30 miles on the surface in total. &lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_RAD6000&quot;&gt;&lt;em&gt;Spirit&lt;/em&gt;&lt;/a&gt;, or MER-2, its sister unit, had previously reached end of mission in 2010.
&lt;p&gt;And why would we report that here? Because &lt;em&gt;Opportunity&lt;/em&gt; and &lt;em&gt;Spirit&lt;/em&gt; were both in fact powered by the POWER1, or more accurately a 20MHz BAE RAD6000, a radiation-hardened version of the original IBM RISC Single Chip CPU and the indirect ancestor of the PowerPC 601. There are a lot of POWER chips in space, both &lt;a href=&quot;https://en.wikipedia.org/wiki/IBM_RAD6000&quot;&gt;with the original RAD6000&lt;/a&gt; and its successor the &lt;a href=&quot;https://en.wikipedia.org/wiki/RAD750&quot;&gt;RAD750&lt;/a&gt;, a radiation-hardened version of the PowerPC G3.&lt;/p&gt;&lt;p&gt;That's not the end of Power ISA chips on Mars, though: &lt;a href=&quot;https://en.wikipedia.org/wiki/Curiosity_(rover)&quot;&gt;&lt;em&gt;Curiosity&lt;/em&gt;&lt;/a&gt;, which is running a pair of RAD750s (one main and one backup, plus two SPARC accessory CPUs), is still in operation at 2,319 Mars solar days and ticking. There is also the &lt;a href=&quot;https://en.wikipedia.org/wiki/2001_Mars_Odyssey&quot;&gt;&lt;em&gt;2001 Mars Odyssey&lt;/em&gt;&lt;/a&gt; orbiter, which is still circling the planet with its own RAD6000 and is expected to have enough propellant to continue survey operations until 2025. &lt;em&gt;Curiosity&lt;/em&gt;'s design is likely to be reused for the &lt;a href=&quot;https://en.wikipedia.org/wiki/Mars_2020&quot;&gt;Mars 2020 rover&lt;/a&gt;, meaning possibly even more Power chips will be exploring space and doing science where it counts millions of miles from home.&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 19:53:10 +0000</pubDate>
<dc:creator>bellinom</dc:creator>
<og:url>https://www.talospace.com/2019/02/the-last-power1-on-mars-is-dead.html</og:url>
<og:title>The last POWER1 on Mars is dead</og:title>
<og:description>The Opportunity Rover , also known as the Mars Exploration Rover B (or MER-1), has finally been declared at end of mission today after 5,3...</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.talospace.com/2019/02/the-last-power1-on-mars-is-dead.html</dc:identifier>
</item>
<item>
<title>Show HN: Automatically synchronize subtitles with video</title>
<link>https://github.com/smacke/subsync</link>
<guid isPermaLink="true" >https://github.com/smacke/subsync</guid>
<description>&lt;div class=&quot;Box-body p-6&quot;&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;Language-agnostic automatic synchronization of subtitles to video, so that subtitles are aligned to the correct starting point within the video.&lt;/p&gt;
&lt;p&gt;The implementation for this project was started during HackIllinois 2019, for which it received an &lt;strong&gt;&lt;em&gt;Honorable Mention&lt;/em&gt;&lt;/strong&gt; (ranked in the top 5 projects, excluding projects that won company-specific prizes).&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Turn this:&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Into this:&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/smacke/subsync/blob/master/tearing-me-apart-wrong.gif&quot;&gt;&lt;img src=&quot;https://github.com/smacke/subsync/raw/master/tearing-me-apart-wrong.gif&quot; alt=&quot;&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/smacke/subsync/blob/master/tearing-me-apart-correct.gif&quot;&gt;&lt;img src=&quot;https://github.com/smacke/subsync/raw/master/tearing-me-apart-correct.gif&quot; alt=&quot;&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;First, make sure ffmpeg is installed. On MacOS, this looks like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;brew install ffmpeg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next, grab the script. It should work with both Python2 and Python3:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;pip install git+https://github.com/smacke/subsync
&lt;/code&gt;
&lt;/pre&gt;

&lt;pre&gt;
&lt;code&gt;subsync video.mp4 -i unsynchronized.srt &amp;gt; synchronized.srt
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;subsync video.mp4 -i unsynchronized.srt -o synchronized.srt
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Although it can usually work if all you have is the video file, there may be occasions where you have a correctly synchronized &quot;reference&quot; srt file in a language you are unfamiliar with, as well as an unsynchronized srt file in your native language. In this case, it will be faster to do the following:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;subsync reference.srt -i unsynchronized.srt -o synchronized.srt
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Whether to perform voice activity detection on the audio or to directly extract speech from an srt file is determined from the file extension.&lt;/p&gt;

&lt;p&gt;To demonstrate how one might use subsync seamlessly with real video software, we developed a prototype integration into the popular &lt;a href=&quot;https://www.videolan.org/vlc/index.html&quot; rel=&quot;nofollow&quot;&gt;VLC&lt;/a&gt; media player, which was demoed during the HackIllInois 2019 project expo. The resulting patch can be found in the file &lt;a href=&quot;https://github.com/smacke/subsync/raw/master/subsync-vlc.patch&quot;&gt;subsync-vlc.patch&lt;/a&gt;. Here are instructions for how to use it.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;First clone the 3.0 maintenance branch of VLC and checkout 3.0.6:&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;git clone git://git.videolan.org/vlc/vlc-3.0.git
cd vlc-3.0
git checkout 3.0.6
&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Next, apply the patch:&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;wget https://github.com/smacke/subsync/raw/master/subsync-vlc.patch
git apply subsync-vlc.patch
&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Follow the normal instructions on the &lt;a href=&quot;https://wiki.videolan.org/VLC_Developers_Corner/&quot; rel=&quot;nofollow&quot;&gt;VideoLAN wiki&lt;/a&gt; for building VLC from source. &lt;em&gt;Warning: this is not easy.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;You should now be able to autosynchronize subtitles using the hotkey &lt;code&gt;Ctrl+Shift+S&lt;/code&gt; (only enabled while subtitles are present).&lt;/p&gt;

&lt;p&gt;My experience is that &lt;code&gt;subsync&lt;/code&gt; usually finishes running in 20 to 30 seconds, depending on the length of the video. The most expensive step is actually extraction of raw audio. If you already have a correctly synchronized &quot;reference&quot; srt file (in which case the video is no longer necessary), &lt;code&gt;subsync&lt;/code&gt; typically runs in less than a second.&lt;/p&gt;

&lt;p&gt;The synchronization algorithm operates in 3 steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Discretize video and subtitles by time into 10ms windows.&lt;/li&gt;
&lt;li&gt;For each 10ms window, determine whether that window contains speech. This is trivial to do for subtitles (we just determine whether any subtitle is &quot;on&quot; during each time window); for video, use an off-the-shelf voice audio detector (VAD) like the one built into &lt;a href=&quot;https://webrtc.org/&quot; rel=&quot;nofollow&quot;&gt;webrtc&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Now we have two binary strings: one for the subtitles, and one for the video. Try to align these strings by matching 0's with 0's and 1's with 1's. We score these alignments as (# matching digits) - (# mismatched digits).&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The best-scoring alignment from step 3 determines how to offset the subtitles in time so that they are properly synced with the video. Because the binary strings are fairly long (millions of digits for video longer than an hour), the naive O(n^2) strategy for scoring all alignments is unacceptable. Instead, we use the fact that &quot;scoring all alignments&quot; is a convolution operation and can be implemented with the Fast Fourier Transform (FFT), bringing the complexity down to O(n log n).&lt;/p&gt;

&lt;p&gt;I have yet to find a case where the automatic synchronization has been off by more than ~1 second. If the reference is another subtitle file, I think it should work every time. I would expect that it could fail sometimes when the reference is a video since speech extraction is a noisier process, but I have not found any bad cases yet (currently tested on ~10 videos more than 30 minutes in length).&lt;/p&gt;

&lt;p&gt;The prototype VLC patch is very experimental -- it was developed under pressure and just barely works. I would love to see this project more robustly integrated with VLC, either directly in the VLC core, or as a plugin. If you or anyone you know has ideas for how to accomplish this, please let me know!&lt;/p&gt;
&lt;p&gt;Also: although I have not found cases where the synchronization algorithm fails, this does not mean they don't exist. It would be good to find these cases so as to better understand the algorithm's pain points.&lt;/p&gt;

&lt;p&gt;This project would not be possible without the following libraries:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.ffmpeg.org/&quot; rel=&quot;nofollow&quot;&gt;ffmpeg&lt;/a&gt; and the &lt;a href=&quot;https://github.com/kkroening/ffmpeg-python&quot;&gt;ffmpeg-python&lt;/a&gt; wrapper, for extracting raw audio from video&lt;/li&gt;
&lt;li&gt;VAD from &lt;a href=&quot;https://webrtc.org/&quot; rel=&quot;nofollow&quot;&gt;webrtc&lt;/a&gt; and the &lt;a href=&quot;https://github.com/wiseman/py-webrtcvad&quot;&gt;py-webrtcvad&lt;/a&gt; wrapper, for speech detection&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pypi.org/project/auditok/&quot; rel=&quot;nofollow&quot;&gt;auditok&lt;/a&gt;, for backup audio detection if webrtcvad misbehaves&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://pypi.org/project/srt/&quot; rel=&quot;nofollow&quot;&gt;srt&lt;/a&gt; for operating on &lt;a href=&quot;https://en.wikipedia.org/wiki/SubRip#SubRip_text_file_format&quot; rel=&quot;nofollow&quot;&gt;SRT files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.numpy.org/&quot; rel=&quot;nofollow&quot;&gt;numpy&lt;/a&gt; and, indirectly, &lt;a href=&quot;https://www.netlib.org/fftpack/&quot; rel=&quot;nofollow&quot;&gt;FFTPACK&lt;/a&gt;, which powers the FFT-based algorithm for fast scoring of alignments between subtitles (or subtitles and video).&lt;/li&gt;
&lt;li&gt;Other excellent Python libraries like &lt;a href=&quot;https://docs.python.org/3/library/argparse.html&quot; rel=&quot;nofollow&quot;&gt;argparse&lt;/a&gt; and &lt;a href=&quot;https://tqdm.github.io/&quot; rel=&quot;nofollow&quot;&gt;tqdm&lt;/a&gt;, not related to the core functionality, but which enable much better experiences for developers and users.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code in this project is &lt;a href=&quot;https://opensource.org/licenses/MIT&quot; rel=&quot;nofollow&quot;&gt;MIT Licensed&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Mon, 25 Feb 2019 19:46:16 +0000</pubDate>
<dc:creator>smacke</dc:creator>
<og:image>https://avatars2.githubusercontent.com/u/325653?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>smacke/subsync</og:title>
<og:url>https://github.com/smacke/subsync</og:url>
<og:description>Automagically synchronize subtitles with video. Contribute to smacke/subsync development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/smacke/subsync</dc:identifier>
</item>
<item>
<title>Starbucks&amp;#039; music is driving employees nuts</title>
<link>https://www.cbc.ca/radio/thecurrent/the-current-for-february-25-2019-1.5032165/starbucks-music-is-driving-employees-nuts-a-writer-says-it-s-a-workers-rights-issue-1.5028163</link>
<guid isPermaLink="true" >https://www.cbc.ca/radio/thecurrent/the-current-for-february-25-2019-1.5032165/starbucks-music-is-driving-employees-nuts-a-writer-says-it-s-a-workers-rights-issue-1.5028163</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;a href=&quot;https://www.cbc.ca/radio/thecurrent/the-current-for-february-25-2019-1.5032165/feb-25-2019-episode-transcript-1.5033098&quot;&gt;Read Story Transcript&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;You may not give a second thought to the tunes spinning on a constant loop at your favourite café or coffee shop, but one writer and podcaster who had to listen to repetitive music for years while working in bars and restaurants argues it's a serious workers' rights issue.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&quot;[It's] the same system that's used to … flood people out of, you know,&lt;/span&gt; &lt;a href=&quot;https://www.cbc.ca/news/world/judge-dismisses-waco-wrongful-death-lawsuit-1.235596&quot;&gt;the Branch Davidian in Waco&lt;/a&gt; or &lt;a href=&quot;https://www.cbc.ca/news/canada/british-columbia/skinny-puppy-says-its-music-used-for-torture-at-guantanamo-1.2524516&quot;&gt;was used on terror suspects in Guantanamo&lt;/a&gt; — they use the repetition of music,&quot; Adam Johnson told &lt;em&gt;&lt;a href=&quot;https://www.cbc.ca/radio/thecurrent&quot;&gt;The Current's&lt;/a&gt;&lt;/em&gt; Anna Maria Tremonti.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&quot;I'm not suggesting that working at Applebee's is the same as being at Guantanamo, but the principle's the same.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Earlier this year, irritated&lt;/span&gt; &lt;a href=&quot;http://www.grubstreet.com/2019/01/starbucks-playlists-hamilton-repeat.html&quot;&gt;Starbucks employees took to Reddit to rage&lt;/a&gt; about how they had to listen to the same songs from the Broadway hit musical &lt;em&gt;Hamilton&lt;/em&gt; on repeat while on the job. &lt;a href=&quot;https://www.reddit.com/r/starbucks/comments/ah2m1d/if_i_have_to_hear_hamilton_one_more_time_im/&quot;&gt;One user wrote&lt;/a&gt; that if they heard a &lt;em&gt;Hamilton&lt;/em&gt; song one more time, &quot;I'm getting a ladder and ripping out all of our speakers from the ceiling.&quot;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Johnson argues it wouldn't take years of research to understand that &quot;yes, playing the same music over and over again has a deleterious effect on one's mental well-being.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;
&lt;div class=&quot;placeholder&quot;&gt;

&lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://i.cbc.ca/1.5032206.1551100771!/fileImage/httpImage/image.jpeg_gen/derivatives/original_780/adam-johnson.jpeg&quot; class=&quot;loaded largeImage&quot;/&gt;&lt;/span&gt;&lt;/div&gt;
&lt;span&gt;&lt;span class=&quot;leadimage-caption&quot;&gt;Writer and podcaster Adam Johnson used to work in bars and restaurants, and argues repetitive music that gets on employees' nerves is a workers' rights issue. (Submitted by Adam Johnson)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;h2&gt;Snitch line for complaints?&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;As a solution, he suggested health inspectors could enforce better working conditions, or a tip line could be created for people to report poor working conditions, like repetitive music.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Another solution? Communication, says neuroscientist Jessica Grahn.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;She studies music, which science has shown to be one of the strongest influencers of mood, she said. It can calm &lt;a href=&quot;https://www.cbc.ca/news/canada/british-columbia/the-power-of-music-provides-comfort-to-those-with-dementia-1.4799775&quot;&gt;dementia patients&lt;/a&gt; struggling with depression or anger, or increase our endurance when we're working out.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, there are downsides to the power of music. Unlike how we can close our eyes to things we don't want to see, we can't close our ears to sound.&lt;/p&gt;
&lt;div&gt;
&lt;div class=&quot;placeholder&quot;&gt;

&lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;https://i.cbc.ca/1.5032209.1551100451!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/jessica-grahn.jpg&quot; class=&quot;loaded largeImage&quot;/&gt;&lt;/span&gt;&lt;/div&gt;
&lt;span&gt;&lt;span class=&quot;leadimage-caption&quot;&gt;Jessica Grahn is a cognitive neuroscientist and assistant professor in the department of psychology at Western University in London, Ont. (Submitted by Jessica Grahn)&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;h2&gt;Control makes a difference&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&quot;So it can be a very effective way of the external environment impinging, without our control, on our sensory processing,&quot; Grahn said.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&quot;Because we can't close our ears, it's very effective if somebody else has control of our sonic environment. We can do nothing about that, and that can be pretty debilitating.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Having control over one's environment can make a big difference, said Grahn, which is why she recommends employers and employees talk about why certain music is being played, or what they can do to switch things up.&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;

&lt;span class=&quot;media-caption&quot;&gt;Ryu Takahashi's composer friend couldn't stand the music at his favourite restaurant in New York. So Takahashi made the establishment a new one. He tells Anna Maria Tremonti how he did it. 1:17&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;&quot;I think when people have input or a sense of being listened to, that control actually makes a very big difference in their response to what they're listening to.&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;Click 'listen' near the top of this page to hear the full conversation.&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;&lt;span&gt;Written by Kirsten Fenn. Produced by Imogen Birchard. The audio contains: &lt;/span&gt;&lt;/em&gt;&lt;br/&gt;&lt;em&gt;&quot;Satisfied&quot; by Renée Elise Goldsberry / Hamilton — Original Broadway Cast Recording / Warner/Chappell Music Inc. / Atlantic (Warner Music Group). &lt;br/&gt;&quot;Young Folks&quot; by Peter Bjorn and John / Writer's Block / EMI Blackwood Music / Sony Music. &lt;br/&gt;&quot;Centerfield&quot; by John Fogerty / Centerfield / Wenaha Music / BMG Music&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 19:19:08 +0000</pubDate>
<dc:creator>colinprince</dc:creator>
<og:url>https://www.cbc.ca/radio/thecurrent/the-current-for-february-25-2019-1.5032165/starbucks-music-is-driving-employees-nuts-a-writer-says-it-s-a-workers-rights-issue-1.5028163</og:url>
<og:title>Starbucks' music is driving employees nuts. A writer says it's a workers' rights issue | CBC Radio</og:title>
<og:image>https://i.cbc.ca/1.4285403.1526347870!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_620/starbucks-results.jpg</og:image>
<og:description>Irritated Starbucks employees took to Reddit in a rage last month after being subjected to a constant loop of hits from the Broadway musical Hamilton. We ask whether the constant, repetitive music employees have to endure on the job — whether in restaurants, bars, or retail — should be a workers' rights issue, and what can be done to fix it.</og:description>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cbc.ca/radio/thecurrent/the-current-for-february-25-2019-1.5032165/starbucks-music-is-driving-employees-nuts-a-writer-says-it-s-a-workers-rights-issue-1.5028163</dc:identifier>
</item>
<item>
<title>Launch HN: OurWorldInData (YC W19 Nonprofit) – Data on World’s Largest Problems</title>
<link>https://news.ycombinator.com/item?id=19247821</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=19247821</guid>
<description>Hi HN!
&lt;p&gt;We’re Hannah, Esteban, Jaiden and Max, the founders of Our World in Data (&lt;a href=&quot;https://OurWorldInData.org&quot; rel=&quot;nofollow&quot;&gt;https://OurWorldInData.org&lt;/a&gt;). We’re a nonprofit in the YC W19 batch.&lt;/p&gt;&lt;p&gt;Our World in Data is a website that shows how and why global living conditions and the earth's environment are changing. Is the world becoming more violent? Is an end to poverty possible? It's hard to know because daily news focuses on negative single events, and misses long-lasting changes that reshape the world.&lt;/p&gt;
&lt;p&gt;We’re a group of researchers from the University of Oxford trying to solve this problem. We bring together data and research from many different sources often buried under jargon in static, outdated documents. We present a global perspective on living conditions and environmental change through interactive data visualizations and short explainers.&lt;/p&gt;
&lt;p&gt;Max started Our World in Data in 2013 whilst working as a researcher at the University of Oxford. The project was born from a frustration that we are so poorly informed about how the world is changing—we fail to notice the important developments shaping our world and are not aware what is possible for the future. It has now evolved into a full-time project with a small team of researchers and web developers (we’ll be looking for a new web developer this week!). We’re all driven by the same motivation: to make sure data and research on how the world is changing is free and accessible for everyone.&lt;/p&gt;
&lt;p&gt;We cover many topics, ranging from poverty to health, environment, energy, education, and violence. Our data and analysis are available at global, regional and country levels. And we try to provide the longest-term data we can, often going back many decades or centuries.&lt;/p&gt;
&lt;p&gt;We average more than 1M users per month; these range from policymakers to journalists, academics to school teachers. But we’ve also had some use cases that took us by surprise: To many readers it’s unexpected to see that the world has made substantial progress in important aspects and psychologists have recently told us that they use our website to help patients with depression and anxiety. We did not expect this use of our work at all and asked them for more details. One of them explained: “Facts can be a powerful weapon against fear, a gloomy worldview, learned helplessness. So I help clients find facts at Our World in Data.”&lt;/p&gt;
&lt;p&gt;We usually work remotely, because we are not all based in the same country—this is the first time that we were able to find a 3-month window of time to move to California and work together.&lt;/p&gt;
&lt;p&gt;We come from a university environment and applied to YC because we wanted learn from the startup and the technology world. The work at YC and the contact with the partners and other founders have definitely given us an entirely new perspective on how to work.&lt;/p&gt;
&lt;p&gt;We’re here at HN because we are sure we can learn a lot from the community here. We knew there had been HN threads on aspects of our work before – but after a recent search (&lt;a href=&quot;http://bit.ly/OWID-searches-on-HN&quot; rel=&quot;nofollow&quot;&gt;http://bit.ly/OWID-searches-on-HN&lt;/a&gt;) we had no idea there were so many. It’s amazing to see that these posts created such great discussion within the HN community.&lt;/p&gt;
&lt;p&gt;Our website is here: &lt;a href=&quot;https://OurWorldInData.org&quot; rel=&quot;nofollow&quot;&gt;https://OurWorldInData.org&lt;/a&gt;. We are a non-profit and all our work is entirely free; open access research (Creative Commons licensed) and open source code. If you’re interested in supporting this with a donation to us you can do so here: &lt;a href=&quot;https://OurWorldInData.org/Donate&quot; rel=&quot;nofollow&quot;&gt;https://OurWorldInData.org/Donate&lt;/a&gt;. Or if you have any other queries, you can reach out at hannah@ourworldindata.org.&lt;/p&gt;
&lt;p&gt;We would really appreciate any feedback you have on what we can do better. Thank you!&lt;/p&gt;
</description>
<pubDate>Mon, 25 Feb 2019 18:09:07 +0000</pubDate>
<dc:creator>Hannah_OWID</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=19247821</dc:identifier>
</item>
</channel>
</rss>