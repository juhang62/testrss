<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>You Promised Me Mars Colonies but I Got Facebook</title>
<link>https://www.linkedin.com/pulse/mars-colonies-counting-ads-edward-hsu</link>
<guid isPermaLink="true" >https://www.linkedin.com/pulse/mars-colonies-counting-ads-edward-hsu</guid>
<description>&lt;div id=&quot;&quot;&gt;&lt;p&gt;“You promised me Mars colonies, and instead, I got Facebook” said Buzz Aldrin in a MIT Technology Review article in 2012. PayPal co-founder Peter Thiel, added “We wanted flying cars — instead we got 140 characters.” In the article, titled “Why We Can’t Solve Big Problems,” Editor-in-chief Jason Pontin adds that critics of Silicon Valley feel “venture investing has shifted away from funding transformational companies” toward funding ”features, widgets, irrelevances.”&lt;/p&gt;
&lt;p&gt;Pontin feels we haven’t seen “big things” since the PC revolution due to three reasons. First, many of society’s big problems are bound by political policy, and ideology - not technology. Second, many of the remaining big problems are too complex and we haven't made the scientific and engineering progress required to address them. Lastly, economic forces - solving “big things” is expensive, and new innovations are often not cost competitive in the market. Tesla’s success in the automotive market is an exception.   &lt;/p&gt;
&lt;p&gt;But there is good evidence that the constraints of technological progress are about to change. To see why, we need to understand that tech innovations are dependent two types of computational problems.&lt;/p&gt;
&lt;h3&gt;Two Classes of Innovation&lt;/h3&gt;
&lt;p&gt;Today’s world does not look like the future many of us saw as children in movies, because there’s been asymmetrical progress between two types of innovation. The first, connectivity-driven innovations, were less anticipated by futurists decades ago. The second, applied science innovations, were eagerly expected. No one’s said predicting the future was easy. &lt;/p&gt;
&lt;div class=&quot;slate-resizable-image-embed slate-image-embed__resize-full-width&quot;&gt;&lt;img alt=&quot;No alt text provided for this image&quot; data-media-urn=&quot;&quot; data-li-src=&quot;https://media.licdn.com/dms/image/C5612AQFnKrLxtBw8kA/article-inline_image-shrink_1500_2232/0?e=1579737600&amp;amp;v=beta&amp;amp;t=O3eQKxflCyaxzEBusGrf_Hec-S8tDwWXmKIghDcsHn8&quot; src=&quot;https://:0/&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Modern connectivity-driven innovations started with computer networking, leading to e-commerce. By 2010, mobile computing devices, along with cloud and hyperscale infrastructures, made it possible to create seamless digital interactions that were personalized, and in context. Two examples are AirBnB and Uber. While valuable, AirBnB and Uber are essentially just-in-time &amp;amp; personalized brokers of something we’ve been doing for decades - renting houses and riding taxis. Same is true for social media like Facebook and Twitter, whose main product is better targeted advertisements.&lt;/p&gt;
&lt;div class=&quot;slate-resizable-image-embed slate-image-embed__resize-full-width&quot;&gt;&lt;img alt=&quot;No alt text provided for this image&quot; data-media-urn=&quot;&quot; data-li-src=&quot;https://media.licdn.com/dms/image/C5612AQF74toXdAYPzA/article-inline_image-shrink_1000_1488/0?e=1579737600&amp;amp;v=beta&amp;amp;t=mQlnu2NAqDPBU3IazStllT-4ScHZqMoKKlLnS5CbME0&quot; src=&quot;https://:0/&quot;/&gt;&lt;/div&gt;
&lt;p&gt;What happened on the applied science innovation front during this time? Not so much. While we’ve seen improvements on the quality and efficiency of physical products, it hasn’t changed how we live. No one has flown supersonic commercially for nearly twenty years. Electric cars and clean energy are barely starting to make a dent. No flying cars exist beyond early prototypes. It’s understandable why many are disappointed.&lt;/p&gt;
&lt;h3&gt;Uneven Progress&lt;/h3&gt;
&lt;p&gt;There are several reasons why connectivity-driven innovations have seen the spotlight while applied science innovations have been comparatively starved. &lt;/p&gt;
&lt;p&gt;Connectivity-driven innovation has several advantages. First is its faster innovation and monetization cycle. It’s much faster to build a mobile app and try to monetize than to advance scientific breakthroughs, push through regulatory hurdles, manufacture, and develop a product go-to-market. Because of this advantage, it sees more investment from VC’s (who often want returns in 10 years). This leads to more press and media coverage, and greater intake of talent. Lastly, computing requirements for connectivity-driven innovations are readily available utilities in the public cloud, where the 65% of workloads run. Just add software.&lt;/p&gt;
&lt;p&gt;On the other hand, applied science innovations not only require physical manufacturing, but solving problems in the outdated-ly named high performance computing (HPC) discipline. HPC hasn’t changed much in decades; its common operating model is closer to hugging servers than utility computing. Engineers are resigned to “dumb down their model” to complete their designs with limited computing power, or wait for compute time to run simulations. Project costs explode, time to market is delayed, while IT organizations take pride in the high utilization of expensive HPC hardware they bought three years ago. For anyone that’s spent time in enterprise cloud computing, it can seem downright crazy.&lt;/p&gt;
&lt;div class=&quot;slate-resizable-image-embed slate-image-embed__resize-full-width&quot;&gt;&lt;img alt=&quot;No alt text provided for this image&quot; data-media-urn=&quot;&quot; data-li-src=&quot;https://media.licdn.com/dms/image/C5612AQHYY89gqioEzw/article-inline_image-shrink_1500_2232/0?e=1579737600&amp;amp;v=beta&amp;amp;t=Ksx0UlbcSmvb054raTLpU464IvP7Ksn6KTVYXrPKaeY&quot; src=&quot;https://:0/&quot;/&gt;&lt;/div&gt;
&lt;h3&gt;Feeding the Applied Science Innovation Engine&lt;/h3&gt;
&lt;p&gt;But this is about to change - &lt;a href=&quot;https://www.linkedin.com/pulse/advancing-cloud-computings-final-frontier-edward-hsu/&quot; target=&quot;_blank&quot;&gt;HPC workloads are shifting to the cloud&lt;/a&gt;. This means for the computing part of the equation, applied science innovations will soon be on equal footing as its connectivity-driven cousins. Cloud providers are making specialized HPC infrastructures available on demand. Simulation software vendors are shifting their licensing models for utility consumption. And new platforms are bringing it all together as turnkey solutions.&lt;/p&gt;
&lt;p&gt;Today, mainstream enterprises like Nissan are using HPC in the cloud to build better cars with acoustic, airflow, structural, thermal, and crash simulation techniques. We’re also seeing a rapid expansion of VC-funded startups in the &lt;a href=&quot;https://techcrunch.com/2018/05/23/upstarts-emerge-to-chase-teslas-lead-in-electric-vehicles/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;electric car&lt;/a&gt;, &lt;a href=&quot;https://www.wsj.com/articles/aerospace-startups-see-stepped-up-role-at-paris-international-air-show-1497798699&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;aerospace&lt;/a&gt; and &lt;a href=&quot;https://spacenews.com/space-startup-investments-continued-to-rise-in-2018/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;space&lt;/a&gt; industries. For these startups, running simulations in the cloud is the default starting point.&lt;/p&gt;
&lt;p&gt;The agility of cloud computing is finally fueling the applied science innovation engine. So for those who’ve had their fill of social/mobile/local webapps and want clean energy, electric cars, and supersonic jets, there is reason to be optimistic.&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;30.145945945946&quot;&gt;
&lt;h3 class=&quot;ugc-post-bar__published_by&quot;&gt;Published By&lt;/h3&gt;
&lt;div data-impression-id=&quot;author_mini-profile_mini-card&quot; class=&quot;mini-card mini-profile mini-card--flat mini-profile--flat&quot;&gt;&lt;span class=&quot;ugc-post-bar__container&quot;&gt;&lt;img class=&quot;mini-card__image mini-profile__image mini-card__image--flat mini-profile__image--flat lazy-load&quot; data-delayed-url=&quot;https://media.licdn.com/dms/image/C5103AQFZxnAe56jHKg/profile-displayphoto-shrink_100_100/0?e=1579737600&amp;amp;v=beta&amp;amp;t=3ICptzZDX0dq483mjf2FMe5reZZZROGTKSl9-t_JjGc&quot; alt=&quot;Edward Hsu&quot;/&gt;&lt;/span&gt;
&lt;section class=&quot;mini-card__content mini-profile__content mini-card__content--flat mini-profile__content--flat&quot;&gt;&lt;h3 class=&quot;mini-card__title mini-profile__title mini-card__title--flat mini-profile__title--flat&quot;&gt;&lt;span class=&quot;ugc-post-bar__container&quot;&gt;&lt;a class=&quot;mini-card__title-link mini-profile__title-link mini-card__title-link--flat mini-profile__title-link--flat&quot; data-tracking-control-name=&quot;author_mini-profile_title&quot; data-tracking-will-navigate=&quot;&quot; href=&quot;https://www.linkedin.com/in/edwardihsu?trk=author_mini-profile_title&quot; aria-label=&quot;View profile for Edward Hsu&quot;&gt;Edward Hsu&lt;/a&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;h4 class=&quot;mini-card__subtitle mini-profile__subtitle mini-card__subtitle--flat mini-profile__subtitle--flat&quot;&gt;&lt;span class=&quot;ugc-post-bar__container&quot;&gt;Vice President of Product at Rescale - Hiring!&lt;/span&gt;&lt;/h4&gt;
&lt;/section&gt;&lt;/div&gt;
&lt;span class=&quot;ugc-post-bar__container&quot;&gt;&lt;a class=&quot;ugc-post-bar__follow-button&quot; data-tracking-will-navigate=&quot;&quot; data-tracking-control-name=&quot;ugc-post-bar__follow-button&quot; href=&quot;https://www.linkedin.com/uas/login?session_redirect=https%3A%2F%2Fwww%2Elinkedin%2Ecom%2Fpulse%2Fmars-colonies-counting-ads-edward-hsu&amp;amp;trk=ugc-post-bar__follow-button&quot;&gt;Follow&lt;/a&gt;&lt;/span&gt;
&lt;p class=&quot;ugc-post-bar__share-text&quot;&gt;In recent weeks I've formed a new perspective of how computing shapes our lives and how it's set to change.&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Sat, 16 Nov 2019 18:58:26 +0000</pubDate>
<dc:creator>rahimnathwani</dc:creator>
<og:title>You Promised Me Mars Colonies but I Got Facebook</og:title>
<og:description>“You promised me Mars colonies, and instead, I got Facebook” said Buzz Aldrin in a MIT Technology Review article in 2012. PayPal co-founder Peter Thiel, added “We wanted flying cars — instead we got 140 characters.</og:description>
<og:image>https://media.licdn.com/dms/image/C5612AQFa4X_SgiHxsw/article-cover_image-shrink_600_2000/0?e=1579737600&amp;v=beta&amp;t=tP5YXKt76EaBux34Y4JR-jE4xyn9IK74cHPGu4ISwYQ</og:image>
<og:url>https://www.linkedin.com/pulse/mars-colonies-counting-ads-edward-hsu</og:url>
<og:type>website</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.linkedin.com/pulse/mars-colonies-counting-ads-edward-hsu</dc:identifier>
</item>
<item>
<title>Infectious Executable Stacks</title>
<link>https://nullprogram.com/blog/2019/11/15/</link>
<guid isPermaLink="true" >https://nullprogram.com/blog/2019/11/15/</guid>
<description>&lt;time datetime=&quot;2019-11-15&quot;&gt;November 15, 2019&lt;/time&gt;&lt;p&gt;nullprogram.com/blog/2019/11/15/&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was discussed &lt;a href=&quot;https://news.ycombinator.com/item?id=21553882&quot;&gt;on Hacker News&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In software development there are many concepts that at first glance seem useful and sound, but, after considering the consequences of their implementation and use, are actually horrifying. Examples include &lt;a href=&quot;https://lwn.net/Articles/683118/&quot;&gt;thread cancellation&lt;/a&gt;, &lt;a href=&quot;https://nullprogram.com/blog/2019/10/27/&quot;&gt;variable length arrays&lt;/a&gt;, and &lt;a href=&quot;https://nullprogram.com/blog/2018/07/20/#strict-aliasing&quot;&gt;memory aliasing&lt;/a&gt;. GCC’s closure extension to C is another, and this little feature compromises the entire GNU toolchain.&lt;/p&gt;
&lt;h3 id=&quot;gnu-c-nested-functions&quot;&gt;GNU C nested functions&lt;/h3&gt;
&lt;p&gt;GCC has its own dialect of C called GNU C. One feature unique to GNU C is &lt;em&gt;nested functions&lt;/em&gt;, which allow C programs to define functions inside other functions:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;intsort1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;qsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The nested function above is straightforward and harmless. It’s nothing groundbreaking, and it is trivial for the compiler to implement. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt; function is really just a static function whose scope is limited to the containing function, no different than a local static variable.&lt;/p&gt;
&lt;p&gt;With one slight variation the nested function turns into a closure. This is where things get interesting:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;intsort2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;_Bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;qsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invert&lt;/code&gt; variable from the outer scope is accessed from the inner scope. This has &lt;a href=&quot;https://nullprogram.com/blog/2019/09/25/&quot;&gt;clean, proper closure semantics&lt;/a&gt; and works correctly just as you’d expect. It fits quite well with traditional C semantics. The closure itself is re-entrant and thread-safe. It’s automatically (read: stack) allocated, and so it’s automatically freed when the function returns, including when the stack is unwound via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;longjmp()&lt;/code&gt;. It’s a natural progression to support closures like this via nested functions. The eventual caller, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt;, doesn’t even know it’s calling a closure!&lt;/p&gt;
&lt;p&gt;While this seems so useful and easy, its implementation has serious consequences that, in general, outweigh its benefits. In fact, in order to make this work, the whole GNU toolchain has been specially rigged!&lt;/p&gt;
&lt;p&gt;How does it work? The function pointer, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt;, passed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt; must somehow be associated with its lexical environment, specifically the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invert&lt;/code&gt; variable. A static address won’t do. When I &lt;a href=&quot;https://nullprogram.com/blog/2017/01/08/&quot;&gt;implemented closures as a toy library&lt;/a&gt;, I talked about the function address for each closure instance somehow needing to be unique.&lt;/p&gt;
&lt;p&gt;GCC accomplishes this by constructing a trampoline on the stack. That trampoline has access to the local variables stored adjacent to it, also on the stack. GCC also generates a normal &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt; function, like the simple nested function before, that accepts &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;invert&lt;/code&gt; as an additional argument. The trampoline calls this function, passing the local variable as this additional argument.&lt;/p&gt;
&lt;h3 id=&quot;trampoline-illustration&quot;&gt;Trampoline illustration&lt;/h3&gt;
&lt;p&gt;To illustrate this, I’ve manually implemented &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;intsort2()&lt;/code&gt; below for x86-64 (&lt;a href=&quot;https://wiki.osdev.org/System_V_ABI&quot;&gt;System V ABI&lt;/a&gt;) without using GCC’s nested function extension:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;20.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;_Bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;intsort3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;_Bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;volatile&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// mov  edx, invert&lt;/span&gt;
        &lt;span class=&quot;mh&quot;&gt;0xba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x00&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x00&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0x00&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// mov  rax, cmp&lt;/span&gt;
        &lt;span class=&quot;mh&quot;&gt;0x48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xb8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// jmp  rax&lt;/span&gt;
        &lt;span class=&quot;mh&quot;&gt;0xff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xe0&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trampoline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;qsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nmemb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trampoline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here’s a complete example you can try yourself on nearly any x86-64 unix-like system: &lt;a href=&quot;https://nullprogram.com/download/trampoline.c&quot;&gt;&lt;strong&gt;trampoline.c&lt;/strong&gt;&lt;/a&gt;. It even works with Clang. The two notable systems where stack trampolines won’t work are &lt;a href=&quot;https://marc.info/?l=openbsd-cvs&amp;amp;m=149606868308439&amp;amp;w=2&quot;&gt;OpenBSD&lt;/a&gt; and &lt;a href=&quot;https://github.com/microsoft/WSL/issues/286&quot;&gt;WSL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(Note: The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volatile&lt;/code&gt; is necessary because C compilers rightfully do not see the contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt; as being consumed. Execution of the contents isn’t considered.)&lt;/p&gt;
&lt;p&gt;In case you hadn’t already caught it, there’s a catch. The linker needs to link a binary that asks the loader for an executable stack (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z execstack&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ cc -std=c99 -Os -Wl,-z,execstack trampoline.c
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;That’s because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf&lt;/code&gt; contains x86 code implementing the trampoline:&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invert&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;; assign third argument&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; store cmp address in RAX register&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;jmp&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;            &lt;span class=&quot;c&quot;&gt;; jump to cmp&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;(Note: The absolute jump through a 64-bit register is necessary because the trampoline on the stack and the jump target will be very far apart. Further, these days the program will likely be compiled as a Position Independent Executable (PIE), so &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt; &lt;a href=&quot;https://eli.thegreenplace.net/2012/01/03/understanding-the-x64-code-models&quot;&gt;might itself have an high address&lt;/a&gt; rather than load into the lowest 32 bits of the address space.)&lt;/p&gt;
&lt;p&gt;However, executable stacks were phased out ~15 years ago because it makes buffer overflows so much more dangerous! Attackers can inject and execute whatever code they like, typically &lt;em&gt;shellcode&lt;/em&gt;. That’s why we need this unusual linker option.&lt;/p&gt;
&lt;p&gt;You can see that the stack will be executable using our old friend, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;readelf&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ readelf -l a.out
...
  GNU_STACK  0x00000000 0x00000000 0x00000000
             0x00000000 0x00000000 RWE   0x10
...
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Note the “RWE” at the bottom right, meaning read-write-execute. This is a really bad sign in a real binary. Do any binaries installed on your system right now have an executable stack? &lt;a href=&quot;https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=944817&quot;&gt;I found one on mine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When compiling the original version using a nested function there’s no need for that special linker option. That’s because GCC saw that it would need an executable stack and used this option automatically.&lt;/p&gt;
&lt;p&gt;Or, more specifically, GCC &lt;em&gt;stopped&lt;/em&gt; requesting a non-executable stack in the object file it produced. For the GNU Binutils linker, &lt;strong&gt;the default is an executable stack.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;fail-open-design&quot;&gt;Fail open design&lt;/h3&gt;
&lt;p&gt;Since this is the default, the only way to get a non-executable stack is if &lt;em&gt;every&lt;/em&gt; object file input to the linker explicitly declares that it does not need an executable stack. To request a non-executable stack, an object file &lt;a href=&quot;https://www.airs.com/blog/archives/518&quot;&gt;must contain the (empty) section &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.note.GNU-stack&lt;/code&gt;&lt;/a&gt;. If even a single object file fails to do this, then the final program gets an executable stack.&lt;/p&gt;
&lt;p&gt;Not only does one contaminated object file infect the binary, everything dynamically linked with it &lt;em&gt;also&lt;/em&gt; gets an executable stack. Entire processes are infected! This occurs even via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dlopen()&lt;/code&gt;, where the stack is dynamically made executable to accomodate the new shared object.&lt;/p&gt;
&lt;p&gt;I’ve been bit myself. In &lt;a href=&quot;https://nullprogram.com/blog/2016/11/15/&quot;&gt;&lt;em&gt;Baking Data with Serialization&lt;/em&gt;&lt;/a&gt; I did it completely by accident, and I didn’t notice my mistake until three years later. The GNU linker outputs object files without the special note by default even though the object file only contains data.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ echo hello world &amp;gt;hello.txt
$ ld -r -b binary -o hello.o hello.txt
$ readelf -S hello.o | grep GNU-stack
$
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This is fixed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-z noexecstack&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;$ ld -r -b binary -z noexecstack -o hello.o hello.txt
$ readelf -S hello.o | grep GNU-stack
  [ 2] .note.GNU-stack  PROGBITS  00000000  0000004c
$
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This may happen any time you link object files not produced by GCC, such as output &lt;a href=&quot;https://nullprogram.com/blog/2015/04/19/&quot;&gt;from the NASM assembler&lt;/a&gt; or &lt;a href=&quot;https://nullprogram.com/blog/2016/11/17/&quot;&gt;hand-crafted object files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Nested C closures are super slick, but they’re just not worth the risk of an executable stack, and they’re certainly not worth an entire toolchain being fail open about it.&lt;/p&gt;
&lt;ol class=&quot;references print-only&quot;/&gt;&lt;nav class=&quot;no-print&quot;&gt;
&lt;/nav&gt;
</description>
<pubDate>Sat, 16 Nov 2019 17:47:23 +0000</pubDate>
<dc:creator>pantalaimon</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://nullprogram.com/blog/2019/11/15/</dc:identifier>
</item>
<item>
<title>Microsoft Advertisement Inc.</title>
<link>https://neil.computer/notes/microsoft-advertisement/</link>
<guid isPermaLink="true" >https://neil.computer/notes/microsoft-advertisement/</guid>
<description>&lt;p&gt;TLDR: Advertisement needs to end. It is insane to think that a piece of software, so fundamental to running computers, an operating system is selling user behavior to advertisers.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-9.png&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;I remember this logo. Classic sans-serif italics, with an odd artifact between &quot;o&quot; and &quot;s&quot;. I remember installing Command &amp;amp; Conquer on Windows 98 (and the bluescreen of death when I was invading the Brotherhood of Nod base). I remember the boot-up sound of Windows XP. The &lt;a href=&quot;https://en.wikipedia.org/wiki/Bliss_(image)&quot;&gt;Bliss&lt;/a&gt; desktop image is burned into memories of billions of people around the world. I remember neatly organizing desktop icons, I remember defragmentation tasks, I remember Solitaire; I remember the Microsoft that was run by Bill Gates and Steve Ballmer. It wasn't pretty, but in hindsight, I want old Microsoft back. I would have never said those words in 2002 or even in 2012.&lt;/p&gt;
&lt;p&gt;I just installed a new copy of Windows. Through some &lt;a href=&quot;https://community.spiceworks.com/topic/2198477-windows-10-decrapifier-1903&quot;&gt;decrapification&lt;/a&gt;, I was feeling pretty good. Skip through the usual Cortana bullshit to get through the very first boot, fresh, newly inaugurated desktop screen is waiting for a wallpaper change. There is no Windows equivalent of &lt;a href=&quot;https://www.obdev.at/products/littlesnitch/index.html&quot;&gt;Little Snitch&lt;/a&gt;, but &lt;a href=&quot;https://www.glasswire.com/&quot;&gt;Glasswire&lt;/a&gt; as good as it gets on Windows for selectively blocking network traffic.&lt;/p&gt;
&lt;p&gt;In about 15 mins, I was astounded by the amount of things Microsoft needs to phone home about.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Launching control panel (Settings)&lt;/li&gt;
&lt;li&gt;Opening Explorer&lt;/li&gt;
&lt;li&gt;Compatibility module when launching a legacy (Windows 7) app&lt;/li&gt;
&lt;li&gt;Office SDX helper&lt;/li&gt;
&lt;li&gt;Office Click-to-run client&lt;/li&gt;
&lt;li&gt;Office telemetry dashboard&lt;/li&gt;
&lt;li&gt;Update notification pipeline manager&lt;/li&gt;
&lt;li&gt;Speech Runtime Executable&lt;/li&gt;
&lt;li&gt;Office Langauge Preferences&lt;/li&gt;
&lt;/ul&gt;&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-1.png&quot; class=&quot;kg-image&quot;/&gt;Glasswire firewall blocking Microsoft Telemetry
&lt;p&gt;Here is a full list of Windows 10 tracking, telemetry and ads hosts: &lt;a href=&quot;https://github.com/StevenBlack/hosts/issues/155&quot;&gt;https://github.com/StevenBlack/hosts/issues/155&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;cortana&quot;&gt;Cortana&lt;/h2&gt;
&lt;p&gt;Why is Cortana needed on a PC? I've never met anyone in person, have heard of anyone on the internet or seen any evidence that Cortana is a useful feature of Windows. The push for installing &quot;AI Assistants&quot; by the big-4 is troublesome. Amazon is practically giving away their puck-sized Alexa devices for the hope of collecting data. Microsoft is pushing hard to get people to use Cortana; if they don't use, at least have the mic on for Cortana to listen and collect data.&lt;/p&gt;
&lt;p&gt;Cortana is so tightly integrated into Windows that it is not possible to completely get rid of it unless the user is OK with risking system stability and disabling Windows search altogether. Here is a recent &lt;a href=&quot;https://answers.microsoft.com/en-us/windows/forum/all/how-to-uninstall-cortana/4bab6bd8-fe67-47bd-ad4f-7fc5bd2ffdfc&quot;&gt;thread&lt;/a&gt; on Microsoft forums as well as StackOverflow: &lt;a href=&quot;https://superuser.com/questions/949569/can-i-completely-disable-cortana-on-windows-10&quot;&gt;https://superuser.com/questions/949569/can-i-completely-disable-cortana-on-windows-10&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Cortana is here to make your life easy, to solve your problems, and is pre-installed on every single Windows 10 PC (barring LTSC builds). Even Enterprise versions, I kid you not.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-2.png&quot; class=&quot;kg-image&quot; alt=&quot;Windows 10 Editions, notice Cortana is forced in every single edition from &amp;quot;Home&amp;quot; to &amp;quot;Enterprise&amp;quot;. No one asked for it.&quot;/&gt;Windows 10 Editions, notice Cortana is forced in every single edition from &quot;Home&quot; to &quot;Enterprise&quot;. No one asked for it.
&lt;p&gt;The fact of the matter is, Cortana and Microsoft telemetry is an important asset in their advertisement portfolio. After LinkedIn acquisition, Microsoft's focus towards advertisment has moved away from traditional ads on Bing to using rich, highly targetted and accurately tagged datasets from LinkedIn, and horrifyingly, Windows 10.&lt;/p&gt;
&lt;p&gt;I predict that Microsoft will offer a basic &quot;Home&quot; version of Windows for free. It will become so lucrative to be able to sell AI-analyzed behaviors such as mouse sensitivity, screentime, amortization of time amongst apps, detailed understanding of user behavior, webcam, speech and voice, tablet usage, location information, system information, etc. I could go on and on. Google has exploited user data in the most lucrative ways, but Microsoft has deeper integration, from Home PCs to SMB to Big corporations - and everything in the middle.&lt;/p&gt;
&lt;p&gt;They even proudly announce this on their advertisement page.&lt;/p&gt;
&lt;h2 id=&quot;unlock-marketing-superpowers-with-ai&quot;&gt;Unlock Marketing Superpowers with AI&lt;/h2&gt;
&lt;p&gt;Says, the &lt;a href=&quot;https://about.ads.microsoft.com/en-us&quot;&gt;headline&lt;/a&gt; with a couple of dudes snooping on animals, taking pictures and analyzing them while on a Safari.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-3.png&quot; class=&quot;kg-image&quot;/&gt;Front page - Microsoft Advertisement
&lt;p&gt;These stock pictures are creeping me out because they are completely and utterly irrelevant, they're smoke and mirrors to disguise the horrifying message that they're trying to convey.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-4.png&quot; class=&quot;kg-image&quot; alt=&quot;Benefits of Microsoft's advertisement network&quot;/&gt;Benefits of Microsoft's advertisement network
&lt;p&gt;And here is the juicy part that disguises Windows 10 telemetry under the name &quot;Microsoft Search Network&quot;.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-5.png&quot; class=&quot;kg-image&quot; alt=&quot;Microsoft Search Network&quot;/&gt;Microsoft Search Network
&lt;p&gt;&quot;Microsoft Search Network&quot; definition is vague enough, but notice the exclusivity of &quot;Desktop&quot; advertisement data. Microsoft does not have a mobile platform, however, the fact that they are offering desktop-exclusive data indicates that Bing is a negligible traffic source through Mobile searches.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-6.png&quot; class=&quot;kg-image&quot; alt=&quot;Microsoft Search Network definition&quot;/&gt;Microsoft Search Network definition
&lt;p&gt;Microsoft is also aggregating AOL Inc. advertisement data.&lt;/p&gt;
&lt;p&gt;I also found this recent &lt;a href=&quot;https://about.ads.microsoft.com/en-us/blog/post/november-2019/what-is-your-customer-profiling-really-doing-for-you&quot;&gt;article&lt;/a&gt; on the advertisement portal illuminating.&lt;/p&gt;
&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;At Microsoft Advertising, we take time to gather and study data and  trends to uncover ways to empower marketers everywhere to be more  successful. Below, we explore some ways top marketers are rethinking  customer profiling to deliver highly personalized, customer-centric  experiences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yes, I presume that if an ad-tech company has access to your PC, it can figure out where, what, how, which, why for anything and everything. The scope and possibilities of Microsoft's ad-tech is so vast, that they felt the need to publish an ebook: &lt;a href=&quot;https://advertiseonbing-blob.azureedge.net/blob/bingads/media/insight/ebook/2019/10-october/cdj-chapter-2/cdj_mini_ebook_us.pdf&quot;&gt;https://advertiseonbing-blob.azureedge.net/blob/bingads/media/insight/ebook/2019/10-october/cdj-chapter-2/cdj_mini_ebook_us.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I'll end with the most disgusting thing I can imagine an advertiser can take advantage of, accessibility and disabled individuals:&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-7.png&quot; class=&quot;kg-image&quot; alt=&quot;Microsoft's Accessibility Marketing eBook&quot;/&gt;Microsoft's Accessibility Marketing eBook
&lt;p&gt;&lt;a href=&quot;https://advertiseonbing-blob.azureedge.net/blob/bingads/media/library/insight/moder-marketing-is-accessible-marketing/accessibility-marketing.pdf&quot;&gt;https://advertiseonbing-blob.azureedge.net/blob/bingads/media/library/insight/moder-marketing-is-accessible-marketing/accessibility-marketing.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The whole thing is full of rotten ideas, terrible grammar and poorly written corporate bullshit, here is an excerpt:&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;The more people you reach the more you can serve, so accessibility is good business. Designing with accessibility in mind goes beyond compliance by providing more effective customer interactions, increased productivity and innovation. We are on a journey to understand the full impact that inclusion has on productivity and innovation, but one thing is clear: designing for inclusion means everyone can do and achieve more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What does the following even mean? On Page 6, they try to explain what all of this means...with more bullshit.&lt;/p&gt;
&lt;img src=&quot;https://neil.computer/content/images/2019/11/image-8.png&quot; class=&quot;kg-image&quot; alt=&quot;Microsoft's Accessibility Marketing eBook&quot;/&gt;Microsoft's Accessibility Marketing eBook
&lt;h2 id=&quot;push-vs-pull-advertisement-models&quot;&gt;Push vs. Pull Advertisement Models&lt;/h2&gt;
&lt;p&gt;I wish to see the world where advertisement is dead, businesses organically match with customers through a &quot;pull&quot; rather than &quot;push&quot; model. I've always enjoyed going to trade-shows and exhibitions where the entire advertisement model is &quot;pull&quot;. Customers willingly travel to these trade fairs with a purpose to connect with a potential supplier, get a demonstration of their products and meet with them in person. There is a presumed context for both, the supplier and the customer.&lt;/p&gt;
&lt;p&gt;Contrast it with an Instagram ad that keeps selling me noise-canceling headphones &lt;em&gt;after&lt;/em&gt; I've already purchased one or trying to sell me Geico Insurance while I am watching a lecture on YouTube. It is the wrong context. Yes, it is more common to bombard the audience with overwhelming number of ads to instill familiarity of the brandname even if the customer is not in the market for purchasing. As consumers become more aware of these tactics, it will be a matter of time when people will seek objective differentiating features of a product or service, as opposed to being tricked. I don't have an academic background in Advertisement, I am sure there are many models of advertisement and the process of pursuing a customer.&lt;/p&gt;
&lt;p&gt;The &quot;push&quot; advertisement model has created multi-billion dollar enterprises with ever-growing pervasive invasion into people's lives. It is time for new tech companies to come up with respectful, privacy-centric, accurate matching of suppliers to customers. Advertisement today is about shady tactics, store smells, psychological tricks, dark patterns, attention retainment, micro-transactions, deceptive wording, repetitive exposure; what happened to building great products so that the customers advertise it by word of mouth? What happened to companies that focus on quality, customer service and long-term prospects of building rapport with customers for their life-long loyalty? What happened to honesty and integrity in conducting business?&lt;/p&gt;
&lt;p&gt;Advertisement needs to end. It is insane to think that a piece of software, so fundamental to running computers, an operating system is selling user behavior to advertisers. This is only going to get worse with IoT devices, don't get me started on those things.&lt;/p&gt;
&lt;p&gt;Privacy is the biggest, one of the most important challenges in technology today. Democracy depends on it. And, Apple has lit this torch.&lt;/p&gt;
&lt;hr/&gt;&lt;pre&gt;
&lt;code&gt;CHANGE LOG:
Edit 1: Grammar edits
Edit 2: I didn't want to go on a tangent about Glasswire, but now it appears that I am condoning Glasswire. Glasswire is the least worse of network blockers on Windows. It is far from being as well developed as Little Snitch on Mac.
Edit 3: &quot;Safari&quot; -&amp;gt; &quot;a Safari&quot;&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Sat, 16 Nov 2019 07:47:44 +0000</pubDate>
<dc:creator>neilpanchal</dc:creator>
<og:type>article</og:type>
<og:title>Microsoft Advertisement Inc.</og:title>
<og:description>Advertisement needs to end. It is insane to think that a piece of software, so fundamental to running computers, an operating system is selling user behavior to advertisers.</og:description>
<og:url>https://neil.computer/notes/microsoft-advertisement/</og:url>
<og:image>https://neil.computer/content/images/2019/11/microsoft-old.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://neil.computer/notes/microsoft-advertisement/</dc:identifier>
</item>
<item>
<title>Supreme Court to Hear Google-Oracle Copyright Fight</title>
<link>https://www.axios.com/supreme-court-google-oracle-copyright-fight-c5de0f29-b6ff-436e-afe0-68f794a62ad6.html</link>
<guid isPermaLink="true" >https://www.axios.com/supreme-court-google-oracle-copyright-fight-c5de0f29-b6ff-436e-afe0-68f794a62ad6.html</guid>
<description>&lt;div&gt;
&lt;p class=&quot;StoryBody__paragraph--2-Doz&quot;&gt;The Supreme Court said Friday it will hear Google’s appeal in the long-running copyright dispute between the search company and Oracle.&lt;/p&gt;
&lt;p class=&quot;StoryBody__paragraph--2-Doz&quot;&gt;&lt;strong&gt;The big picture&lt;/strong&gt;: The two tech giants have been feuding for nearly a decade over whether Google illegally used parts of Oracle’s Java code for its Android software, with Oracle seeking billions of dollars in damages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;hidden-c5de0f29-b6ff-436e-afe0-68f794a62ad6&quot; class=&quot;&quot; aria-hidden=&quot;false&quot; aria-expanded=&quot;true&quot;&gt;
&lt;div&gt;
&lt;p class=&quot;StoryBody__paragraph--2-Doz&quot;&gt;&lt;strong&gt;Driving the news:&lt;/strong&gt; The Supreme Court granted Google’s petition to hear the case in&lt;br/&gt;a &lt;a href=&quot;https://www.supremecourt.gov/orders/courtorders/111519zr_8n59.pdf&quot; class=&quot;StoryBody__link--10w8x&quot;&gt;brief order&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;&lt;li class=&quot;StoryBody__item--1cHYD&quot;&gt;The &lt;a href=&quot;https://www.newsweek.com/2019/06/07/google-oracle-copyright-case-supreme-court-1433037.html&quot; class=&quot;StoryBody__link--10w8x&quot;&gt;case&lt;/a&gt; involves key issues surrounding permissible use for software development, with Google arguing it made fair use of the code and did not infringe copyright.&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;StoryBody__paragraph--2-Doz&quot;&gt;&lt;strong&gt;What they're saying:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote class=&quot;Quote__root--fHvPf Quote__plain--kY7a_&quot;&gt;
&lt;p&gt;“We welcome the Supreme Court’s decision to review the case and we hope that the court reaffirms the importance of software interoperability in American competitiveness. Developers should be able to create applications across platforms and not be locked into one company's software.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;cite class=&quot;QuoteAttribution__root--vrUkY&quot;&gt;— Kent Walker, Google senior vice president of global affairs, in a statement.&lt;/cite&gt;
&lt;blockquote class=&quot;Quote__root--fHvPf Quote__plain--kY7a_&quot;&gt;
&lt;p&gt;&quot;We are confident the Supreme Court will preserve long established copyright protections for original software and reject Google’s continuing efforts to avoid responsibility for copying Oracle’s innovations. We believe the Court will reject any reasoning that permits copying verbatim vast amounts of software code, used for the same purpose and same way as the original. That is not “transformative,” and certainly not fair use. We look forward to presenting our arguments, which have been embraced by the Solicitor General and the Federal Circuit. In the end, a finding that Google infringed Oracle’s original works will promote, not stifle, future innovation.&quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;cite class=&quot;QuoteAttribution__root--vrUkY&quot;&gt;— Deborah Hellinger, head of global corporate communications, Oracle&lt;/cite&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 15 Nov 2019 23:37:02 +0000</pubDate>
<dc:creator>nwrk</dc:creator>
<og:url>https://www.axios.com/supreme-court-google-oracle-copyright-fight-c5de0f29-b6ff-436e-afe0-68f794a62ad6.html</og:url>
<og:title>Google v. Oracle to go to Supreme Court</og:title>
<og:description>The two tech giants have been feuding for nearly a decade.</og:description>
<og:image>https://images.axios.com/7m5X5Rr9MDni9CW5alR8caKPiX0=/0x522:5936x3861/1920x1080/2019/11/15/1573847468229.jpg</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.axios.com/supreme-court-google-oracle-copyright-fight-c5de0f29-b6ff-436e-afe0-68f794a62ad6.html</dc:identifier>
</item>
<item>
<title>I Created a $60K/Month App That Collects In-Person Payments Through Stripe</title>
<link>https://www.starterstory.com/stripe-in-person-payments</link>
<guid isPermaLink="true" >https://www.starterstory.com/stripe-in-person-payments</guid>
<description>&lt;h3&gt;Hello! Who are you and what business did you start?&lt;/h3&gt;
&lt;p&gt;I’m Ryan Scherf, the founder of &lt;a href=&quot;http://payment.co&quot; target=&quot;_blank&quot;&gt;payment.co&lt;/a&gt; (&lt;a href=&quot;https://twitter.com/payment&quot; target=&quot;_blank&quot;&gt;@payment&lt;/a&gt;), an app built on top of the Stripe payment gateway for creating card-present charges. Payment is available on iOS and Android, and allows customers who have created online stores to easily collect payments in person, in the same account, with no barriers or card readers required.&lt;/p&gt;
&lt;p&gt;The app launched in January 2015, and since has grown to process over $70M in volume annually. The app collects a 1% service fee on every single charge.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/512/original/open-uri20191103-4-n8uyw3?1572745630&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;What's your backstory and how did you come up with the idea?&lt;/h3&gt;
&lt;p&gt;I went to school for Computer Science and was gated into being a developer. I mostly hated everything about development environments at the time (Java was the most popular at the time), and thus transitioned into a career of design. After working at several healthcare startups (these are abundant in Minneapolis, due to Unitedhealth Group being headquartered in the area), I quickly realized that working for people wasn’t a long term career path that I was willing to commit to.&lt;/p&gt;
&lt;p&gt;In 2013, I was approached by someone I had worked with previously on an invoicing app, and he had an idea for building an analytics app on top of a company in its infancy: Stripe. This was pre-huge valuations and funding rounds.&lt;/p&gt;
&lt;p&gt;At the time, I knew nothing about Stripe, nor payment gateways. I had 4 &amp;amp; 2-year-old boys and had just built a new house in the suburbs.&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;One of my biggest regrets is not focusing on my business earlier. We were making money, and I always treated it as just a hobby business.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our first app, Paid, launched in late 2013 and was the only analytics app for Stripe revenue. I won’t get into too many specifics about what happened afterward, but I did an &lt;a href=&quot;https://mixergy.com/interviews/ryan-scherf-payment-for-stripe/&quot; target=&quot;_blank&quot;&gt;interview with Mixergy&lt;/a&gt; in 2015 and explained the process.&lt;/p&gt;
&lt;p&gt;We were seeing significant traction, but we were unable to come up with a great way to monetize. As a designer and iOS dev combination, our one requirement for building apps is that we didn’t want to manage a backend server. Especially one calculating resource-intensive analytics data.&lt;/p&gt;
&lt;p&gt;Therefore, we decided to launch Payment; an app for collecting card-present payments. This was a gaping hole in the Stripe offering and didn’t require much validation because it was so obvious that it was missing. Stripe offered the ability to create direct charges via their API, however, they did not have an app. And due to the simplicity of their API, this feature would have been incredibly easy to include in their Dashboard app, yet they deliberately chose to leave it out.&lt;/p&gt;
&lt;h3&gt;Take us through the process of designing, prototyping, and manufacturing your first product.&lt;/h3&gt;
&lt;p&gt;Having worked with my partner on several projects and companies in the past, our working relationship was very natural. I focused on product design, and he focused on development.&lt;/p&gt;
&lt;p&gt;The first version of Payment was designed, built and tested over a span of 2 months. Development began in November 2014 and launched in January 2015.&lt;/p&gt;
&lt;p&gt;The only thing we needed the MVP to do was to allow for someone to create a charge which enabled us to collect a fee. This simple functionality wasn’t possible via an app (built on Stripe) at this time. No card readers, nothing fancy. Just processing a manual input credit card number, similar to what you’d do in an eCommerce experience.&lt;/p&gt;
&lt;h3&gt;Describe the process of launching the business.&lt;/h3&gt;
&lt;p&gt;We didn’t have any special tactics or tricks we used to launch. There wasn’t any hype, and nobody saw us coming. We chose the “build it and they will come” route because the hole was so immense, and the project was so small. We only focused on iOS to begin as the search results when searching for “Stripe” in the App Store had nothing to do with finance. We figured it would be relatively easy to own that keyword, and we did.&lt;/p&gt;
&lt;p&gt;From our previous app (Paid), we did have a user list of several thousand account emails that &lt;em&gt;might&lt;/em&gt; be interested in charging via an app, so we sent campaigns to them.&lt;/p&gt;
&lt;p&gt;In our first 15 days after launch, we made $184 from fees. I can remember how incredibly exciting this was, and how all of a sudden this app idea felt validated. Our previous app (Paid) relied on IAP’s. This app was different. All we needed to do was get more people using.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/513/original/open-uri20191103-4-eu5hi4?1572745630&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Because we bootstrapped everything and worked in our spare time, we had no costs, only time. We both had families, stable finances and an interest in working for ourselves. We let the app sit while we worked on some features, but essentially, it was all organic growth for 2015. We ended up making $38,000 that year, split 50/50.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/514/original/open-uri20191103-4-1eu6h1t?1572745631&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;Since launch, what has worked to attract and retain customers?&lt;/h3&gt;
&lt;p&gt;I’ve found that the best way to keep customers is by building what they want. That sounds cliche, but I’ve been super diligent in figuring out exactly what to build. I don’t keep a product roadmap, nor do I have an aspirational product I want to get to. My customers are people that run real businesses. And they run many types of businesses: from private jets to selling surfboards to donation collection at a church. There is no one size fits all for the product. The only exception to that is they want to be able to accept credit cards, in a variety of ways, very quickly. By focusing on just that user experience, we’ve been able to keep the app small and nimble, and dead simple to get started.&lt;/p&gt;
&lt;p&gt;You may be wondering how this is possible? Well, I do &lt;strong&gt;all&lt;/strong&gt; of the customer support. I don’t have a support team, nor do I have automation for answering the requests. I’ve added a few pieces to the app for the very common questions (like Where is my money?!), but other than that, I personally respond to every single support request. This allows me to keep my pulse on what’s working and what isn’t, and get ahead of the features that are being requested often.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/515/original/open-uri20191103-4-1jma451?1572745631&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In terms of growth, we’ve mostly focused on app stores (Apple and Google Play) to distribute and optimize the app. We’re in the process of launching an initiative to start funneling leads from the web as well but haven’t traditionally focused here.&lt;/p&gt;
&lt;p&gt;We position the app strongly as a preferred, verified Stripe Partner. In fact, our partnership with the Stripe team is deep enough that their account managers and support teams will refer their users to our apps to collect payments. Since we were first to the market, we mostly saw the competitors copy our descriptions verbatim. The most important term we can rank for at the moment is “stripe”, so we try to pack that term as much as possible (even though some competitors did it more).&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/516/original/open-uri20191103-4-zgmmlr?1572745631&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Since our app relies so heavily on Stripe accounts, we essentially buy paid ads for the keyword “stripe” on all of the relevant stores. Interestingly, we also rank #1 organically for the keyword “payment”, which is ahead of some really popular services like Venmo, Google Pay, PayPal, Zelle, etc. Although this gives us some downloads, most abandon as they have no idea what a Stripe account is or why they would need it.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/517/original/open-uri20191103-4-bgtwd3?1572745632&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;How are you doing today and what does the future look like?&lt;/h3&gt;
&lt;p&gt;In 2018, my partner and I parted ways as he was looking to focus on other things, and I was looking to go all-in on Payment. The app has been profitable since day 1, so there has never really been cash flow problems. At the time of writing this, the app is processing $6M per month in volume, which equates to $60,000 per month in revenue. Roughly 80% of that is profit after advertising, hosting, card reader inventory and other miscellaneous expenses. Charges are made all over the world.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/payment/status/1175110690706640896&quot; target=&quot;_blank&quot;&gt;embed:tweet&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I’m still the only true employee, though I have several contractors that assist in various capacities, to the tune of about 1 day per week. I still do the design, most of the smaller development, marketing, and business development.&lt;/p&gt;
&lt;p&gt;I’ve set a goal to be processing $10M/month in Stripe volume by Spring, which is a pretty big stretch goal. There’s product-market fit, a great organic and retention strategy, so focusing on growth seemed like the proper next step. Roughly speaking, the CMGR equates to 25% YoY growth.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;how-i-created-a-60k-month-app-that-collects-in-person-payments-through-stripe&quot; src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNkYAAAAAYAAjCB0C8AAAAASUVORK5CYII=&quot; data-src=&quot;//s3.amazonaws.com/www.starterstory.com/story_images/images/000/003/518/original/open-uri20191103-4-109n5qk?1572745632&quot; class=&quot;lazier&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;Through starting the business, have you learned anything particularly helpful or advantageous?&lt;/h3&gt;
&lt;p&gt;One of my biggest regrets is not focusing on my business earlier. We were making money, and I always treated it as just a hobby business. My partner also did the same. Had we taken 6 months and just focused on building it out, we could have drastically accelerated the path to where we are now. In addition, I would have parted ways with my unengaged partner sooner. It wasn’t his fault, we just wanted different things out of the business. When we finally realized it, it was too late.&lt;/p&gt;
&lt;p&gt;In addition, I’m not so naive that I don’t realize that there was a serious amount of luck involved with our success. As mentioned previously, Stripe was in its infancy, and we latched on and rode the wave for 5+ years.&lt;/p&gt;
&lt;h3&gt;What platform/tools do you use for your business?&lt;/h3&gt;
&lt;p&gt;The tools I check every day ironically have nothing to do with how much money I’m making. They’re all development and customer-focused:&lt;/p&gt;
&lt;h3&gt;What have been the most influential books, podcasts, or other resources?&lt;/h3&gt;
&lt;p&gt;I’ve never been a huge fan of the “this is how I did it” books. I don’t think that formula is repeatable for most situations. For instance, the just build it and ship it mentality is something you won’t find in any book -- as it doesn’t discuss testing, finding product-market fit, etc.&lt;/p&gt;
&lt;p&gt;I do however believe in the lean startup, and taking that even further, bootstrapping. People like Ruben Gamez and Rob Walling have been friends of mine for years, and they’ve launched several successful bootstrapped startups. Rob’s podcast &lt;a href=&quot;https://www.startupsfortherestofus.com/&quot; target=&quot;_blank&quot;&gt;Startups for the rest of us&lt;/a&gt; is especially great.&lt;/p&gt;
&lt;h3&gt;Advice for other entrepreneurs who want to get started or are just starting out?&lt;/h3&gt;
&lt;p&gt;I think the best thing anyone can do is to learn how to build it themselves. With the abundance of frameworks, it’s easier than ever to get something scrappy up and running. Don’t wait until you save up $10k to hire an engineer. It will never be enough money if you can’t do some things on your own.&lt;/p&gt;
&lt;p&gt;In addition, there isn’t any perfect timing. You don’t need to wait to quit your job, as something else will naturally come up. Jumping in and shipping something is the only way to get yourself motivated to free yourself from the constraints of working for people.&lt;/p&gt;
&lt;h3&gt;Are you looking to hire for certain positions right now?&lt;/h3&gt;
&lt;p&gt;I’m in the process of looking for someone to assist with growth. I’ve realized over the last few years that I can’t do everything, even though I want to, so I’ve been trying to identify my weak spots and find consultants for that.&lt;/p&gt;
&lt;p&gt;This person would handle iOS and Android store campaigns, SEO, Adwords and any other ways they think they could grow the user base while targeting Stripe users.&lt;/p&gt;
&lt;h3&gt;Where can we go to learn more?&lt;/h3&gt;

&lt;h2 class=&quot;question&quot;&gt;Want to start your own business?&lt;/h2&gt;
&lt;div class=&quot;answer&quot; readability=&quot;15.899204244032&quot;&gt;
&lt;p&gt;Hey! 👋I'm Pat Walls, the founder of Starter Story.&lt;/p&gt;
&lt;p&gt;We interview successful business owners and share the stories behind their business. By sharing these stories, we want to help others get started.&lt;/p&gt;
&lt;p&gt;If you liked this story, &lt;a class=&quot;external-link&quot; href=&quot;https://manage.kmail-lists.com/subscriptions/subscribe?a=Jchkiv&amp;amp;g=KBDbDN&quot; target=&quot;_blank&quot;&gt;join our mailing list&lt;/a&gt; for new interviews every Tuesday.&lt;/p&gt;
&lt;p&gt;Interested in sharing your own story? &lt;a class=&quot;external-link&quot; href=&quot;https://www.starterstory.com/share&quot;&gt;Find out how!&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;PROMOTED&lt;/strong&gt;&lt;/p&gt;
&lt;h2 class=&quot;question&quot;&gt;Are you ready to boost your revenue?&lt;/h2&gt;
&lt;div class=&quot;answer&quot; readability=&quot;15.090579710145&quot;&gt;
&lt;p&gt;Using &lt;a target=&quot;_blank&quot; href=&quot;https://bit.ly/2KX2SwI&quot;&gt;Klaviyo&lt;/a&gt; will open up a massive, untapped sales channel and bring you closer to your customers!&lt;/p&gt;
&lt;p&gt;We've interviewed many impressive businesses who swear by the results of the product, including &lt;a href=&quot;https://starterstory.com/stories/brumate&quot;&gt;Brumate&lt;/a&gt;, &lt;a href=&quot;https://starterstory.com/stories/brumate&quot;&gt;Beardbrand&lt;/a&gt;, and many more.&lt;/p&gt;
&lt;p&gt;Level up your email marketing with &lt;a target=&quot;_blank&quot; href=&quot;https://bit.ly/2KX2SwI&quot;&gt;Klaviyo&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 15 Nov 2019 22:35:50 +0000</pubDate>
<dc:creator>patwalls</dc:creator>
<og:title>How I Created A $60K/Month App That Collects In-Person Payments Through Stripe - Starter Story</og:title>
<og:description>I’m Ryan Scherf, the founder of payment.co (@payment), an app built on top of the Stripe payment gateway for creating card-present charges. Payment is available on iOS and Android, and allows customers who have created online stores to easily collect payments in person, in the same acco...</og:description>
<og:url>https://www.starterstory.com/stripe-in-person-payments</og:url>
<og:image>https://s3.amazonaws.com/www.starterstory.com/stories/social_shares/000/001/503/original/open-uri20191115-4-m7fc55?1573851958</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.starterstory.com/stripe-in-person-payments</dc:identifier>
</item>
<item>
<title>Google cancels TGIF weekly all-hands meetings</title>
<link>https://www.cnbc.com/2019/11/15/google-cancels-tgif-weekly-all-hands-meetings.html</link>
<guid isPermaLink="true" >https://www.cnbc.com/2019/11/15/google-cancels-tgif-weekly-all-hands-meetings.html</guid>
<description>&lt;div class=&quot;InlineImage-imageEmbed hasBkg&quot; id=&quot;ArticleBody-InlineImage-105834311&quot;&gt;
&lt;div class=&quot;InlineImage-wrapper&quot; readability=&quot;9&quot;&gt;

&lt;div readability=&quot;13&quot;&gt;
&lt;p&gt;Sundar Pichai, CEO of Google, speaks to the media before the opening of the Berlin representation of Google Germany in Berlin on January 22, 2019.&lt;/p&gt;
&lt;p&gt;Carsten Koall | Getty Images News | Getty Images&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;group&quot; readability=&quot;64.401693667158&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnbc.com/quotes/?symbol=GOOGL&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;Google&lt;/a&gt; is getting rid of one of its best-known workplace features: TGIF, its weekly all-hands meeting.&lt;/p&gt;
&lt;p&gt;The company confirmed to CNBC that it will instead hold monthly all-hands meetings that will be focused on business and strategy while holding separate town halls for &quot;workplace issues.&quot; An email announcing the change was previously reported by &lt;a href=&quot;https://www.theverge.com/2019/11/15/20966718/google-weekly-all-hands-tgif-staff-meeting-changes-ceo-sundar-pichai&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;_blank&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;The Verge&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Founders Larry Page and Sergey Brin started TGIFs in 1999 as a forum where employees could regularly express concerns and discuss topics open and freely with management. At that time, the company was small enough to fit in a meeting room, but the all-hands continued to grow as the employee base grew — until recently, that is. Page and Brin &lt;a href=&quot;https://www.cnet.com/news/elusive-google-co-founders-make-rare-appearance-at-town-hall-meeting/&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;_blank&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;stopped attending regularly in 2019&lt;/a&gt;. A company spokesperson said that the meetings had recently become a biweekly instead of weekly occurrence.&lt;/p&gt;
&lt;p&gt;The new model comes as the company cracks down on the open work culture that's long been part of its identity of holding free discussion. Employees have increasingly &lt;a href=&quot;https://www.cnbc.com/2018/11/01/google-employees-walk-out-in-protest-of-sexual-misconduct-handling.html&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;voiced their concerns&lt;/a&gt; about everything from the handling of sexual harassment to government hires and contracts. In recent months, employees have leaked meeting notes to the media, which have shown growing tension between executives and workers.&lt;/p&gt;
&lt;p&gt;During the summer, Google said it would &lt;a href=&quot;https://www.cnbc.com/2019/08/23/google-bans-political-discussion-on-internal-mailing-lists.html&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;ban&lt;/a&gt; political discussions from internal messaging forums. In late October, CEO &lt;a href=&quot;https://www.cnbc.com/sundar-pichai/&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;Sundar Pichai&lt;/a&gt; said, in a &lt;a href=&quot;https://www.washingtonpost.com/technology/2019/10/25/google-ceo-leaked-video-says-company-is-genuinely-struggling-with-employee-trust/&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;_blank&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;leaked video&lt;/a&gt;, that the company was &quot;genuinely struggling&quot; with employee trust.&lt;/p&gt;
&lt;p&gt;&quot;In other places -- like TGIF -- our scale is challenging us to evolve,&quot; Pichai said in a memo to employees this week. &quot;TGIF has traditionally provided a place to come together, share progress, and ask questions, but it's not working in its current form.&quot;&lt;/p&gt;
&lt;p&gt;In his note, Pichai alluded to recent leaks that employees have given to the media.&lt;/p&gt;

&lt;p&gt;&quot;We're unfortunately seeing a coordinated effort to share our conversations outside of the company after every TGIF,&quot; the note reportedly states. &quot;I know this is new information to many of you, and it has affected our ability to use TGIF as a forum for candid conversations on important topics.&lt;/p&gt;
&lt;p&gt;Pichai's note also said that only 25% of Google employees watch TGIF any given week, compared with 80% a decade ago.&lt;/p&gt;
&lt;p&gt;&quot;People come to TGIF with different expectations,&quot; the note said. &quot;Some people come to hear more about Google's product launches and business strategies, others come to hear answers on other topics. By splitting the difference every week, we're not serving either purpose very well.&quot;&lt;/p&gt;
&lt;p&gt;The note said that Google's TGIF team will set up &quot;small group discussions&quot; for feedback on the changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WATCH NOW:&lt;/strong&gt; &lt;a href=&quot;https://www.cnbc.com/video/2018/11/01/google-employees-stage-global-walkout-and-ask-for-accountability.html&quot; alt=&quot;&quot; class=&quot;&quot; rel=&quot;&quot; target=&quot;&quot; title=&quot;&quot; role=&quot;&quot; tabindex=&quot;&quot; data-type=&quot;&quot; aria-label=&quot;&quot;&gt;Google employees walk out to protest the handling of sexual harassment allegations&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;



</description>
<pubDate>Fri, 15 Nov 2019 19:02:04 +0000</pubDate>
<dc:creator>stygiansonic</dc:creator>
<og:type>article</og:type>
<og:title>Google will no longer hold weekly all-hands meetings amid growing workplace tensions</og:title>
<og:description>The company said it is scaling back the frequency of it's all-hands meetings, amid growing workplace issues and leaks.</og:description>
<og:url>https://www.cnbc.com/2019/11/15/google-cancels-tgif-weekly-all-hands-meetings.html</og:url>
<og:image>https://image.cnbcfm.com/api/v1/image/105834311-1554475541554google.jpg?v=1567535046</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cnbc.com/2019/11/15/google-cancels-tgif-weekly-all-hands-meetings.html</dc:identifier>
</item>
<item>
<title>Training algorithms on copyrighted data not illegal: US Supreme Court</title>
<link>https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf</link>
<guid isPermaLink="true" >https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf</guid>
<description>&lt;p id=&quot;ece3&quot; class=&quot;hb hc ef at hd b he jp hg jq hi jr hk js hm jt ho&quot;&gt;&lt;a href=&quot;https://arstechnica.com/tech-policy/2013/11/google-books-ruled-legal-in-massive-win-for-fair-use/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Google Books ruled legal in massive win for fair use (updated)&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ars_Technica&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Ars Technica&lt;/a&gt; Nov 14 2013.&lt;/p&gt;
&lt;p id=&quot;26a1&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.publishersweekly.com/pw/by-topic/digital/content-and-e-books/article/60006-google-wins-court-issues-a-ringing-endorsement-of-google-books.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Google Wins: Court Issues a Ringing Endorsement of Google Books&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Publishers_Weekly&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Publishers Weekly&lt;/a&gt;, Nov 14, 2013.&lt;/p&gt;
&lt;p id=&quot;4148&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://www.reuters.com/article/2015/10/16/us-google-books-idUSKCN0SA1S020151016&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;hu&quot;&gt;Google book-scanning project legal, says U.S. appeals court&lt;/em&gt;&lt;/a&gt;, Reuters, October 16, 2015.&lt;/p&gt;
&lt;p id=&quot;aeaf&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;“We trust that the Supreme Court will see fit to correct the Second Circuit’s reductive understanding of fair use….”, Authors Guild, Oct. 16, 2015, &lt;a href=&quot;https://www.authorsguild.org/industry-advocacy/2nd-circuit-leaves-authors-high-and-dry/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“2nd Circuit Leaves Authors High and Dry”&lt;/a&gt; (Press Release).&lt;/p&gt;
&lt;p id=&quot;8293&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Liptak, Adam (April 18, 2016). &lt;a href=&quot;https://www.nytimes.com/2016/04/19/technology/google-books-case.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Challenge to Google Books Is Declined by Supreme Court”&lt;/a&gt;. &lt;em class=&quot;hu&quot;&gt;New York Times&lt;/em&gt;. Retrieved April 18, 2016.&lt;/p&gt;
&lt;p id=&quot;d1b3&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;People’s Daily Online (August 15, 2005). &lt;a href=&quot;http://english.peopledaily.com.cn/200508/15/eng20050815_202595.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Google’s digital library suspended”&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;586c&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Siva Vaidhyanathan. &lt;a href=&quot;http://lawreview.law.ucdavis.edu/issues/40/3/copyright-creativity-catalogs/DavisVol40No3_Vaidhyanathan.pdf&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“The Googlization of Everything and the Future of Copyright”&lt;/a&gt;, &lt;em class=&quot;hu&quot;&gt;University of California Davis Law Review&lt;/em&gt;, volume 40 (March 2007), pp. 1207–1231, (pdf).&lt;/p&gt;
&lt;p id=&quot;5e03&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Robert B. Townsend, &lt;a href=&quot;http://www.historians.org/Perspectives/issues/2007/0709/0709vie1.cfm&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Google Books: Is It Good for History?&lt;/a&gt;, &lt;em class=&quot;hu&quot;&gt;Perspectives&lt;/em&gt; (September 2007).&lt;/p&gt;
&lt;p id=&quot;627a&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Copyright infringement suits against Google and their settlement: &lt;a href=&quot;http://www.google.com/intl/en/press/pressrel/20081027_booksearchagreement.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Copyright Accord Would Make Millions More Books Available Online”&lt;/a&gt;. Google Press Center. Retrieved November 22, 2008.&lt;/p&gt;
&lt;p id=&quot;a22e&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;em class=&quot;hu&quot;&gt;Authors Guild, Inc. v. Google, Inc.&lt;/em&gt;, 721 F.3d 132 (2d Cir. 2013).&lt;/p&gt;
&lt;p id=&quot;86de&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.thecrimson.com/article.aspx?ref=524989&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Google Online Book Deal at Risk”&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;ecd0&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://www.techcrunch.com/2009/02/11/google-book-settlement-site-is-up-paying-authors-60-per-scanned-book/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Google Book Settlement Site Is Up; Paying Authors $60 Per Scanned Book”&lt;/a&gt;, by Erick Schonfeld on February 11, 2009, at &lt;a href=&quot;https://en.wikipedia.org/wiki/TechCrunch&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;TechCrunch&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;a926&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.asja.org/google/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;American Society of Journalists and Authors&lt;/a&gt; &lt;a href=&quot;https://web.archive.org/web/20120225000524/http://www.asja.org/google/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Archived&lt;/a&gt; February 25, 2012, at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wayback_Machine&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Wayback Machine&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;ca1a&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Flood, Alison (January 22, 2010). &lt;a href=&quot;https://www.theguardian.com/books/2010/jan/22/ursula-le-guin-revolt-google-digital&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Ursula Le Guin leads revolt against Google digital book settlement”&lt;/a&gt;. &lt;em class=&quot;hu&quot;&gt;The Guardian&lt;/em&gt;. London.&lt;/p&gt;
&lt;p id=&quot;6bae&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://news.bbc.co.uk/2/hi/technology/8298674.stm&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;BBC: Google hits back at book critics&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;8b74&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20130910052712/http://www.openbookalliance.org/2009/10/pam-samuelson-on-google-books-its-not-a-library/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Openbookalliance.org”&lt;/a&gt;. Archived from &lt;a href=&quot;http://www.openbookalliance.org/2009/10/pam-samuelson-on-google-books-its-not-a-library/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;the original&lt;/a&gt; on 2013–09–10. Retrieved 2013–08–14.&lt;/p&gt;
&lt;p id=&quot;117d&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.huffingtonpost.com/pamela-samuelson/google-books-is-not-a-lib_b_317518.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Google Books Is Not a Library&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;75bc&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20100812015123/http://hlpronline.com/2010/05/the-case-for-book-privacy-parity-google-books-and-the-shift-from-offline-to-online-reading/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“The Case for Book Privacy Parity: Google Books and the Shift from Offline to Online Reading”&lt;/a&gt;. Harvard Law and Policy Review. May 16, 2010. Archived from &lt;a href=&quot;http://hlpronline.com/2010/05/the-case-for-book-privacy-parity-google-books-and-the-shift-from-offline-to-online-reading/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;the original&lt;/a&gt; on August 12, 2010. Retrieved September 8, 2010.&lt;/p&gt;
&lt;p id=&quot;ff62&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Pohl, R.D. (November 17, 2009). &lt;a href=&quot;http://nl.newsbank.com/nl-search/we/Archives?p_product=BN&amp;amp;p_theme=bn&amp;amp;p_action=search&amp;amp;p_maxdocs=200&amp;amp;p_topdoc=1&amp;amp;p_text_direct-0=12C0D444C11DDF98&amp;amp;p_field_direct-0=document_id&amp;amp;p_perpage=10&amp;amp;p_sort=YMD_date:D&amp;amp;s_trackval=GooglePM&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Google Books Settlement gets a makeover”&lt;/a&gt;. &lt;em class=&quot;hu&quot;&gt;The Buffalo News&lt;/em&gt;. Retrieved March 26, 2010.&lt;/p&gt;
&lt;p id=&quot;e3fd&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Hagey, Keach (March 17, 2010). &lt;a href=&quot;http://www.thenational.ae/business/media/understanding-the-google-publishing-settlement&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Understanding the Google publishing settlement”&lt;/a&gt;. &lt;em class=&quot;hu&quot;&gt;The National&lt;/em&gt;. Retrieved March 26, 2010.&lt;/p&gt;
&lt;p id=&quot;cc8b&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Siy, Sherwin (November 17, 2009). &lt;a href=&quot;https://web.archive.org/web/20100609220653/http://www.publicknowledge.org/node/2770&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“The New Google Book Settlement: First Impressions on Orphan Works”&lt;/a&gt;. &lt;em class=&quot;hu&quot;&gt;Public Knowledge&lt;/em&gt;. Archived from &lt;a href=&quot;http://www.publicknowledge.org/node/2770&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;the original&lt;/a&gt; on June 9, 2010. Retrieved March 26, 2010.&lt;/p&gt;
&lt;p id=&quot;d98c&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://1.usa.gov/elvHew&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Full text of Judge Chin’s ruling.&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;c536&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;Amir Efrati and Jeffrey A. Trachtenberg (March 23, 2011). &lt;a href=&quot;https://www.wsj.com/articles/SB10001424052748704461304576216923562033348&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Judge Rejects Google Books Settlement”&lt;/a&gt;. Wall Street Journal.&lt;/p&gt;
&lt;p id=&quot;dbb5&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.documentcloud.org/documents/834877-google-books-ruling-on-fair-use.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Opinion in Authors Guild v. Google”&lt;/a&gt;, Circuit Judge Chin, Case 1:05-cv-08136-DC Document 1088, November 14, 2013. Retrieved November 17, 2013.&lt;/p&gt;
&lt;p id=&quot;e502&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://www.forbes.com/sites/ericgoldman/2013/11/14/why-googles-fair-use-victory-in-google-books-suit-is-a-big-deal-and-why-it-isnt/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Why Google’s Fair Use Victory In Google Books Suit Is A Big Deal — And Why It Isn’t&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Forbes_magazine&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Forbes magazine&lt;/a&gt;, 11–14–2013&lt;/p&gt;
&lt;p id=&quot;0ee6&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://policynotes.arl.org/post/66992554073/google-books-decision-a-huge-victory-for-fair-use-and&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;ARL Policy Notes&lt;/a&gt; &lt;a href=&quot;https://web.archive.org/web/20131120091832/http://policynotes.arl.org/post/66992554073/google-books-decision-a-huge-victory-for-fair-use-and&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Archived&lt;/a&gt; November 20, 2013, at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wayback_Machine&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Wayback Machine&lt;/a&gt;, 11–14–2013&lt;/p&gt;
&lt;p id=&quot;7cd9&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://www.infodocket.com/2014/04/11/authors-guild-files-brief-in-google-books-appeal-says-congress-should-create-a-national-digital-library/&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;http://www.infodocket.com/2014/04/11/authors-guild-files-brief-in-google-books-appeal-says-congress-should-create-a-national-digital-library/&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;ab01&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;http://nysbar.com/blogs/EASL/2014/12/oral_argument_in_authors_guild.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Oral Argument in Authors Guild v. Google&lt;/a&gt; &lt;a href=&quot;https://web.archive.org/web/20150515040851/http://nysbar.com/blogs/EASL/2014/12/oral_argument_in_authors_guild.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Archived&lt;/a&gt; May 15, 2015, at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wayback_Machine&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Wayback Machine&lt;/a&gt;, 12–10–14&lt;/p&gt;
&lt;p id=&quot;5e3c&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://www.unitedstatescourts.org/federal/ca2/13-4829/230-0.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Slip opinion&lt;/a&gt; &lt;a href=&quot;https://web.archive.org/web/20170904052412/https://www.unitedstatescourts.org/federal/ca2/13-4829/230-0.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Archived&lt;/a&gt; 2017–09–04 at the &lt;a href=&quot;https://en.wikipedia.org/wiki/Wayback_Machine&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Wayback Machine&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;413c&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://web.archive.org/web/20170904052412/https://www.unitedstatescourts.org/federal/ca2/13-4829/230-0.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;“Slip op. at 46”&lt;/a&gt;. Archived from &lt;a href=&quot;https://www.unitedstatescourts.org/federal/ca2/13-4829/230-0.html&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;the original&lt;/a&gt; on 2017–09–04. Retrieved 2015–10–17.&lt;/p&gt;
&lt;p id=&quot;ff2a&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://www.authorsguild.org/wp-content/uploads/2015/12/Authors-Guild-v.-Google-Petition-w-Appendix.pdf&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;hu&quot;&gt;Authors Guild v. Google, Inc.&lt;/em&gt;&lt;/a&gt;, №15–849 (Dec. 31, 2013).&lt;/p&gt;
&lt;p id=&quot;718d&quot; class=&quot;hb hc ef at hd b he hf hg hh hi hj hk hl hm hn ho&quot;&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Campbell_v._Acuff-Rose_Music,_Inc.&quot; class=&quot;dc by hp hq hr hs&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;&lt;em class=&quot;hu&quot;&gt;Campbell v. Acuff-Rose Music, Inc.&lt;/em&gt;&lt;/a&gt;, 510 U.S. 569 (1994) (2 Live Crew “Pretty Woman” parody case).&lt;/p&gt;
</description>
<pubDate>Fri, 15 Nov 2019 18:30:34 +0000</pubDate>
<dc:creator>alok-g</dc:creator>
<og:type>article</og:type>
<og:title>The Most Important Supreme Court Decision For Data Science and Machine Learning</og:title>
<og:description>Training algorithms on copyrighted data is not illegal, according to the United States Supreme Court.</og:description>
<og:url>https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf</og:url>
<og:image>https://miro.medium.com/max/800/0*k8nIKvF2F6mnu9ZX.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf?gi=9899db2f3250</dc:identifier>
</item>
<item>
<title>Distilling knowledge from neural networks to build smaller and faster models</title>
<link>https://blog.floydhub.com/knowledge-distillation/</link>
<guid isPermaLink="true" >https://blog.floydhub.com/knowledge-distillation/</guid>
<description>&lt;p&gt;Not every smartphone owner carries around a high-end GPU and a power generator in their pockets. For most practical situations, we need compact models with small memory footprints and fast inference times.That said, you might have noticed that many recent advancements in Deep Learning are all about scaling up our models to gargantuan proportions. How can we take advantage of these new models while remaining under reasonable computing constraints?&lt;br/&gt;There are many ways to tackle this issue, but in this article we’ll focus on one in particular: knowledge distillation. Many ideas in this article are taken from the paper &lt;a href=&quot;https://arxiv.org/abs/1903.12136&quot;&gt;Distilling Task-Specific Knowledge from BERT into Simple Neural Networks&lt;/a&gt;, especially the choice of student model I will train, the choice of dataset and the data augmentation scheme.&lt;/p&gt;
&lt;h2 id=&quot;how-are-the-new-models-better-than-the-old-ones&quot;&gt;How are the new models better than the old ones?&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: I’m going to work with Natural Language Processing (&lt;strong&gt;NLP&lt;/strong&gt;) for this article. This doesn’t mean that the same technique and concepts don’t apply to other fields, but NLP is the most glaring example of the trends I will describe.&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;It’s not directly obvious why scaling up a model would improve its performance for a given target task.&lt;br/&gt;Let’s take language modeling and comprehension tasks as an example. The most prominent models right now are GPT-2,  BERT, XLNet, and T5, depending on the task. For the sake of simplicity, we’ll focus on the first two, as XLNet is an improved version of GPT-2. I’ll make a quick recap, but if you want to delve into the details you can check &lt;a href=&quot;https://blog.floydhub.com/gpt2/&quot;&gt;GPT-2&lt;/a&gt; and &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;BERT&lt;/a&gt;.&lt;/p&gt;
&lt;img src=&quot;https://lh4.googleusercontent.com/kA9KNcAnH_GaMDguaptOIs51AJmrl0bna-Y0OkSuVNMiWt-8x867iRXb0nlRLWo_p4xmqla70S90TGhrTAaMt1QXmOGUpf6zbirS07RhKr67S24F93F4IH95NbuEKS3yqzrKMsTa&quot; class=&quot;kg-image&quot; alt=&quot;asd &quot;/&gt;Illustration of the difference between a bidirectional language model, where information can travel both ways (left), and a conventional one, where information can only travel forwards in time. Source: &lt;a href=&quot;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&quot;&gt;Google AI blog&lt;/a&gt;
&lt;p&gt;BERT: State-of-the-art in language comprehension tasks like &lt;a href=&quot;https://gluebenchmark.com&quot;&gt;GLUE&lt;/a&gt;, after T5. It’s a model based on the &lt;a href=&quot;https://blog.floydhub.com/the-transformer-in-pytorch/&quot;&gt;Transformer&lt;/a&gt;. Network instances running on each word can access information from other words, a concept called &lt;em&gt;Attention&lt;/em&gt;. Information can be accessed both ways: words &lt;em&gt;before&lt;/em&gt;, and words &lt;em&gt;after&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;GPT-2: Contends state-of-the-art with XLNet on language generation tasks, depending on the length of the previous context. It also uses Attention, but each instance can only access information from the words before. This makes it possible for it to work in a generative context, where the following words are unknown at inference time, but hinders its performance at language comprehension compared to bidirectional models.&lt;/p&gt;
&lt;p&gt;T5: State-of-the-art model for many benchmarks, including GLUE. Result of an extensive study that analyzed the contribution of techniques from all over the field. Uses the original encoder-decoder formulation of the Transformer from the original paper &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;, coupled with a generalized training objective that can be adapted to many different downstream tasks. Every different task is formulated as a text generation task, so that the network can be trained on different tasks at the same time, with the same loss function. For example, sentence classification can be formulated as:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Input: &lt;em&gt;“sst-2 classify: This movie tries way too hard”&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Label: &lt;em&gt;“target: negative”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Now that I’ve introduced the topic, let’s get back to the matter of scaling things up.&lt;br/&gt;If you’re training a certain network architecture on &lt;a href=&quot;https://gluebenchmark.com&quot;&gt;GLUE&lt;/a&gt;’s tasks and you want to get the best possible performance, you can scale up the network, but eventually you’ll hit a ceiling. There will be a point where you just need more labeled examples or else your network will overfit on the training set.&lt;br/&gt;You can gather up more labels, but no matter how much you invest in gathering data you’ll get ever-diminishing returns.&lt;/p&gt;
&lt;img src=&quot;https://lh4.googleusercontent.com/PtV2hzpn2OqLONB3JxvbzLjJz6GURCVM7xqUJ9I4hgCZUof5ci11FUthQVo9bzbpJU3aivGYQ9jQ3Wj2KF4vQt9pVQzbVtpO058KaSc_39ztS7y0QSnPPwSYPessubzsRNTHGeJq&quot; class=&quot;kg-image&quot;/&gt;Illustration of the training objective used for BERT, called Masked Language Modeling, and the conventional language modeling objective used by GPT-2
&lt;p&gt;This dilemma was solved by training on language modeling, that is, predicting the next word in the sentence (GPT-2) or reconstructing masked tokens (BERT). This is done using unsupervised learning: you can construct labels for these training objectives from raw text.&lt;br/&gt;We can, therefore, scrape the internet for huge amounts of text and train the largest neural networks to date.&lt;br/&gt;The result is a neural network that can achieve SOTA results on a wide range of downstream tasks, like the GLUE benchmarks, with very little additional training. We refer to this second training stage as &lt;strong&gt;fine-tuning&lt;/strong&gt;. The drawback is that these models are so large that it’s not feasible to run them on user devices. Just the parameters of the largest released version of GPT-2 will occupy &lt;strong&gt;1.5 GB&lt;/strong&gt; of your storage! They also require too much time to run without a high-end GPU and will consume your phone’s battery in no time.&lt;/p&gt;&lt;p&gt;How can we make them smaller?&lt;/p&gt;
&lt;h3 id=&quot;sparse-predictions-are-more-informative-than-hard-labels&quot;&gt;Sparse predictions are more informative than hard labels&lt;/h3&gt;
&lt;img src=&quot;https://lh4.googleusercontent.com/ZoaRoo-dWpdIx7iCmbEICmFcOPJLuVC2fc_Pau6akiBbLG6ad-IczRXgKHhnMXDuCXJbmxRU8ucPUJXH18B-cLUTvWekxqQn3cJTaybv3RGK8_5U0lxL8ZOeT6UalyelYBFpxTiL&quot; class=&quot;kg-image&quot;/&gt;Illustration of the difference between hard encoded labels and sparse labels. With hard labels, all the probability is concentrated in the ground-truth value, while for sparse labels every word can be assigned its own probability.
&lt;p&gt;When language models are trained on raw data, hard labels are used. These labels are vectors with a 1 in the position corresponding to the true class, and 0s everywhere else. They represent a probability distribution where the corresponding word has a 100% probability of being the true class.&lt;br/&gt;This type of label can be made directly from raw text. After seeing a lot of examples, the network can act as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;maximum likelihood estimator&lt;/a&gt;, but it needs to be exposed to many examples before it can assign good probabilities, as the labels it sees are samples from a distribution with extremely high variance. Or, to say it in a more straightforward way, &lt;strong&gt;natural language is very unpredictable&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, in the sentence above, the network needs to see examples of different grammatical configurations that exist in English for using the verb “to jump”, in order to learn all the ways the verb can be used:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Jump &lt;strong&gt;over&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Jump &lt;strong&gt;to&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Jump &lt;strong&gt;the&lt;/strong&gt; fence&lt;/li&gt;
&lt;li&gt;Jump &lt;strong&gt;42&lt;/strong&gt; times&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If only we had a pretrained model that is extremely good at estimating probabilities for the next word and could encode all this information in the label… ;)&lt;/p&gt;
&lt;p&gt;If you’ve done transfer learning before, you can see where this is going.&lt;br/&gt;The big idea is:&lt;br/&gt;We can take a large pre-trained model like BERT, called the &lt;strong&gt;teacher&lt;/strong&gt;, fine-tune it on the target task if it differs from the pre-training task, use it to predict the probabilities for our data, then use the probabilities as “soft labels” for the target model, the &lt;strong&gt;student&lt;/strong&gt;. This way we can communicate the target distribution to the network with fewer examples!&lt;br/&gt;This also corresponds to training a student to reproduce the behavior of the teacher as accurately as possible, but with fewer parameters.&lt;/p&gt;
&lt;img src=&quot;https://lh5.googleusercontent.com/z1HgVNIrRl_izvFJAcdt43OYBR5v3KCbOVyWqmGs9ZJbSITpGoOayzuBTxXlA3OmbRFrRZGUvmbDvMZdBbJ8uBmjwMEVga2dgVYBOPns_SDMzGb-RgqLK6y7k-PLAEdhI8GzbAWR&quot; class=&quot;kg-image&quot;/&gt;Summary of the workflow for knowledge distillation. The &lt;em&gt;transfer set&lt;/em&gt; can be constructed in many ways: DistilBERT just uses the training set, while the code for this article generates it by data augmentation. refers to a parameter of the loss function, explained in a later section.
&lt;p&gt;The researchers at &lt;a href=&quot;https://github.com/huggingface&quot;&gt;huggingface&lt;/a&gt; took advantage of these last two ideas to train two networks, DistilBERT and DistilGPT-2, that maintain a performance very close to that of their teachers while being half the size.&lt;/p&gt;
&lt;p&gt;The networks were trained to output the same probabilities as their larger versions, but in order to reach these amazing results, they had to take advantage of some peculiarities of the network architectures:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The smaller models are composed of the same basic blocks as the teachers, but with fewer layers&lt;/li&gt;
&lt;li&gt;Transformer models make use of &lt;strong&gt;residual connections&lt;/strong&gt;: while for most models there is no guarantee that representations from different layers share any similarities, Transformer layers work by &lt;strong&gt;adding&lt;/strong&gt; to their input vectors. This means that outputs from different layers are very similar to the initial word embedding, each layer changing them as little as it’s needed to encode useful information in them.&lt;/li&gt;
&lt;/ul&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/xZfjHgbu9RcdCnnG26smU2sOlpfSgJ_iBp0BO2t59W2RFeC2CB4iokC-DkhZKfyLJ9MpzgBYgPck1g4vdapYokAUMmIa2KPD9MmJQHDhXSI7YxNDH0J6ZX9VXskgs6uz9tWH6EXh&quot; class=&quot;kg-image&quot;/&gt;Transformer layer, &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;. Both the &lt;em&gt;Multi-Head Attention&lt;/em&gt; layer and the &lt;em&gt;Feed Forward&lt;/em&gt; layers work by adding to the input information, not substituting it. This is represented in the scheme by an arrow leaving the main “flow” before each module and re-entering into the &lt;em&gt;Add &amp;amp; Norm&lt;/em&gt; module, which simply adds the module’s inputs and outputs and takes the layer norm.
&lt;p&gt;&lt;br/&gt;This allowed the researchers to initialize the layers of the distilled models to the same weights as some of the layers of the teacher model, or equivalently, &lt;strong&gt;pruning away the layers that weren’t carried over.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;potential-for-data-augmentation&quot;&gt;Potential for data augmentation&lt;/h3&gt;
&lt;p&gt;Most task-specific datasets are tiny compared to what it takes to train even a small NLP model to reach its optimal performance.&lt;/p&gt;
&lt;p&gt;This is also related to how unpredictable language is if we consider it as a random distribution from which we can take samples. You can define the idea of a “sample” in multiple ways:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;Uncovering&lt;/em&gt; &lt;em&gt;the next word&lt;/em&gt; in a sentence, knowing the words that come before it, or&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Uncovering the whole sentence&lt;/em&gt; at once&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Either way, there are so many possible, &lt;em&gt;valid&lt;/em&gt; or commonly occurring combinations of words that it makes it really hard to find a &lt;strong&gt;representative&lt;/strong&gt; subset; that is, a subset on which we can train a model and expect it to have seen enough examples to perform well on data coming from outside of that subset.&lt;/p&gt;
&lt;p&gt;Take these two phrases that could be found in a movie review:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;One of the most praised disappointments&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;em&gt;&lt;em&gt;One of the most praised movies&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;If you had to guess whether the review was positive or negative based on the fact that it contains this sentence, it’s most likely they would be negative and positive, respectively.&lt;/p&gt;
&lt;p&gt;This can’t be figured out by looking at singled out words: by itself, &lt;em&gt;praised&lt;/em&gt; has a positive meaning, but the fact that it’s followed by &lt;em&gt;disappointments&lt;/em&gt; reverses that effect.&lt;br/&gt;To learn this and uncountably many other language patterns, a model would need to be exposed to them during the training process.&lt;/p&gt;
&lt;p&gt;One thing we can do is apply data augmentation. The paper &lt;a href=&quot;https://arxiv.org/abs/1903.12136&quot;&gt;Distilling Task-Specific Knowledge from BERT into Simple Neural Networks&lt;/a&gt; applies augmentation in three ways:&lt;br/&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Randomly replacing a word with a masked token: &lt;em&gt;&amp;lt;mask&amp;gt;&lt;/em&gt; on BERT or &lt;em&gt;&amp;lt;unk&amp;gt;&lt;/em&gt; for other models;&lt;/li&gt;
&lt;li&gt;Randomly replacing a word with another word with the same Part-of-Speech tag, for example, a noun with another noun;&lt;/li&gt;
&lt;li&gt;Randomly masking a group of &lt;em&gt;n&lt;/em&gt; adjacent words or, as linguists like to call it, an&lt;br/&gt;&lt;em&gt;n-gram&lt;/em&gt;;&lt;br/&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;This is only feasible with the help of a teacher model, as very often these kinds of perturbations can change the meaning of the sentence completely, like in the example above, rendering the original label unreliable.&lt;/p&gt;
&lt;p&gt;A model like BERT has seen billions of sentences during its pre-training, so it’s much more likely to have seen examples of many different patterns and it can supply labels for the new examples.&lt;/p&gt;
&lt;div id=&quot;fh-banner&quot;&gt;
&lt;h3 id=&quot;fh-banner-main-text&quot;&gt;Ready to build, train, and deploy AI?&lt;/h3&gt;
&lt;h4 id=&quot;fh-banner-sub-text&quot;&gt;Get started with FloydHub's collaborative AI platform for free&lt;/h4&gt;

&lt;h6 id=&quot;fh-banner-button-link-text&quot;&gt;&lt;a href=&quot;https://www.floydhub.com/?utm_source=blog&amp;amp;utm_medium=knowledge-distillation&amp;amp;utm_campaign=try_floydhub_for_free&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; id=&quot;fh-banner-button-link&quot;&gt;Try FloydHub for free&lt;/a&gt;&lt;/h6&gt;
&lt;/div&gt;
&lt;h3 id=&quot;setting-up-the-training-process-for-knowledge-distillation&quot;&gt;Setting up the training process for knowledge distillation&lt;/h3&gt;
&lt;p&gt;I will use a variety of libraries: Pytorch, Torchtext, huggingface’s &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers&lt;/a&gt;, &lt;a href=&quot;https://spacy.io/&quot;&gt;spacy&lt;/a&gt; and of course, good old Numpy.&lt;/p&gt;&lt;p&gt;I will train a tiny model on SST-2, &lt;a href=&quot;https://gluebenchmark.com/tasks&quot;&gt;Stanford Sentiment Penn Treebank&lt;/a&gt; task.&lt;br/&gt;Each sentence in the dataset is a movie review, and the number “2” indicates that there are only 2 possible classes for each review: positive or negative.&lt;br/&gt;The model is a straightforward BiLSTM:&lt;br/&gt;A 1-layer &lt;a href=&quot;https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/&quot;&gt;LSTM&lt;/a&gt; reads the sentence forwards, another one reads it backward. The final outputs from both LSTMs are concatenated and fed through a classifier with 1 hidden layer.&lt;br/&gt;I will use Facebook AI’s pretrained &lt;em&gt;&lt;strong&gt;fasttext&lt;/strong&gt;&lt;/em&gt; word embeddings with dimension 300.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;I will apply the data augmentation scheme from the paper to the training set and use&lt;br/&gt;&lt;strong&gt;bert-large-uncased&lt;/strong&gt;, fine-tuned on SST-2, to produce soft labels on the augmented dataset.&lt;br/&gt;I will compare this approach to training the BiLSTM on the original dataset with hard labels.&lt;/p&gt;
&lt;img src=&quot;https://lh4.googleusercontent.com/_Um8DZIkqzdWxUq13cOW2TBsydss9IPpJWWwV9E6bxVgZDlGnDYLGt2yS-gcJXkIWetjEVDwYxbfJ4GXVUw8Ix2UiQ5zSolV7FJ4MidQ-mI5L21cvSCfpYK7WtgRIyv2-WL057l9&quot; class=&quot;kg-image&quot;/&gt;Special case of the knowledge distillation workflow: transfer set constructed from applying data augmentation to the training set.
&lt;p&gt;Generally, knowledge distillation is done by blending two loss functions, choosing a value of \(\alpha\) between 0 and 1:&lt;br/&gt;\[L = (1 - \alpha)L_H + \alpha L_{KL}\]&lt;br/&gt;Where \(L_H\) is the cross-entropy loss from the hard labels and \(L_{KL}\) is the Kullback–Leibler divergence loss from the teacher labels.&lt;br/&gt;In our case, we can’t trust the original hard labels due to the aggressive perturbations to the data.  The exact rules for data augmentation are described below, but take this example:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Original:&lt;/strong&gt;&lt;br/&gt;Sentence: the &lt;strong&gt;worst&lt;/strong&gt; thing I’ve ever laid my eyes on&lt;br/&gt;Label: negative&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generated:&lt;/strong&gt;&lt;br/&gt;Sentence: the &lt;strong&gt;best&lt;/strong&gt; thing I’ve ever laid my eyes on&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Since we can’t expect the ground-truth labels to be correct after the algorithm is applied, we’ll set \(\alpha := 1\).&lt;br/&gt;Just to remind you, cross-entropy is defined as:&lt;br/&gt;\[H(p, q) = \sum_{x \in C} p(x)log(q_{\theta}(x))\]&lt;br/&gt;Where \(p(x)\) is the target distribution, that is, 1 for the ground-truth class and 0 for all others. \(q_{\theta}(x)\) is the distribution output by the model, parametrized by its parameters \(\theta\), and \(C\) is the set of all classes.&lt;/p&gt;
&lt;p&gt;Kullback–Leibler divergence is a metric of the difference between two probability distributions, defined similarly to the cross-entropy as:&lt;br/&gt;\[D_{KL}(p_{\phi}, q_{\theta}) = \sum_{x \in C} p_{\phi}(x)log(\frac{q_{\theta}(x)}{p_{\phi}(x)}) = H(p_{\phi}, q_{\theta}) - H(p_{\phi})\]&lt;br/&gt;In this case, \(p_{\phi}(x)\) and \(q_{\theta}(x)\) are the probability distributions given by the teacher and the student. It’s also called &lt;em&gt;relative entropy&lt;/em&gt;, as it’s equal to the cross-entropy of&lt;br/&gt;\(p_{\phi}\) and \(q_{\theta}\) minus \(p_{\phi}\)'s own entropy.&lt;/p&gt;
&lt;p&gt;The most common way for neural networks to output probabilities over classes is to apply a &lt;strong&gt;softmax&lt;/strong&gt; function to the output of the last layer. That is:&lt;br/&gt;\[q_{\theta}(x_j) = \frac{exp(z_j / T)}{\sum\limits_{k}^C exp(z_k / T)}\]&lt;br/&gt;Where \(z_j\) is the &lt;strong&gt;raw score&lt;/strong&gt; given by the network to class \(j\) and \(T\) is a hyperparameter called &lt;strong&gt;temperature&lt;/strong&gt;, set to 1 in the majority of networks, as its value is not important most of the time.&lt;/p&gt;
&lt;p&gt;However, as explained by the paper &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;, as \(T\) becomes larger the Kullback–Leibler divergence becomes more and more similar to applying MSE Loss to the raw scores. MSE Loss tends to be more common for training small networks since, among a variety of reasons, it doesn’t have hyper-parameters. That is, we don’t need to pick a value for \(T\).&lt;br/&gt;These metrics are taken for each token in the sentence. To get the values of the loss functions we average them over the sequence of tokens.&lt;/p&gt;
&lt;h3 id=&quot;the-code&quot;&gt;The code&lt;/h3&gt;
&lt;p&gt;I will leave out the boring parts like logging code for the article, but you can find the full code at &lt;a href=&quot;https://github.com/tacchinotacchi/distil-bilstm&quot;&gt;github.com/tacchinotacchi/distil-bilstm&lt;/a&gt;. With a single click of the below button you can get the same results.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://floydhub.com/run?template=https://github.com/tacchinotacchi/distil-bilstm&quot;&gt;&lt;img src=&quot;https://static.floydhub.com/button/button.svg&quot; alt=&quot;Run on FloydHub&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will go through 5 main steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#loading-data&quot;&gt;Loading the data from tsv files&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#data-augmentation&quot;&gt;Data augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#bi-lstm&quot;&gt;Defining the BiLSTM model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#training-loop&quot;&gt;Defining the training loop&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#launching&quot;&gt;Launching the training loop&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;loading-data&quot;&gt;Loading the data from tsv files&lt;/h2&gt;
&lt;p&gt;There are two formats we need to be able to load. The first one is the format used by the SST-2 dataset:&lt;/p&gt;
&lt;pre&gt;
&lt;span&gt;sentence&lt;/span&gt;	&lt;span&gt;label&lt;/span&gt;&lt;br/&gt;&lt;span&gt;contains no wit , only labored gags&lt;/span&gt;	&lt;span&gt;0&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;With the sentence, a tab separator and the id of the class to which the sentence belongs: 0 for negative, 1 for positive. Loading this format is relatively straightforward in torchtext. First, we create two &lt;code&gt;torchtext.data.Field&lt;/code&gt; objects:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;text_field = data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=batch_first)
label_field_class = data.Field(sequential=False, use_vocab=False, dtype=torch.long)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Let’s take the parameters apart.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;sequential&lt;/code&gt; is to be set &lt;code&gt;True&lt;/code&gt; for fields that consist of variable length sequences, like text.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tokenize&lt;/code&gt; is the function that torchtext should use to tokenize this field of each example. That is, &lt;code&gt;BertTokenizer.tokenize&lt;/code&gt; or &lt;code&gt;spacy_en.tokenize&lt;/code&gt; depending on which model is being used.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lower&lt;/code&gt; indicates that the text should be made lowercase.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;include_lengths&lt;/code&gt; indicates that later, when grouping the examples in batches, we want torchtext to provide a tensor with the length of each sequence in the batch. This is used to provide sequence lengths and attention masks to the BiLSTM and BERT models, respectively.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;batch_first&lt;/code&gt; indicates which dimension in the returned tensor should correspond to elements in the batch, and which should correspond to steps in the sequence. Values of &lt;code&gt;True&lt;/code&gt; and &lt;code&gt;False&lt;/code&gt; correspond to dimensions &lt;em&gt;&lt;em&gt;(batch_size, seq_length)&lt;/em&gt;&lt;/em&gt; or &lt;em&gt;&lt;em&gt;(seq_length, batch_size)&lt;/em&gt;&lt;/em&gt;, respectively. To be set &lt;code&gt;True&lt;/code&gt; for BERT and &lt;code&gt;False&lt;/code&gt; for BiLSTMClassifier.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The second format of data we need to load is the one used by the augmented dataset:&lt;/p&gt;
&lt;pre&gt;
&lt;span&gt;sentence&lt;/span&gt;	&lt;span&gt;label&lt;/span&gt;&lt;br/&gt;&lt;span&gt;contains no wit , only labored gags&lt;/span&gt;	&lt;span&gt;3.017604 -2.543920&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Where fields are separated by tab characters, and words or scores are separated by whitespace characters.&lt;br/&gt;Here, there are two values for each label: the score given by BERT to the negative class, and the score for the positive class. Loading this is more complicated, as torchtext does not directly support sequences of floats. However, we can load this format by specifying &lt;code&gt;sequential=False&lt;/code&gt; and specifying a function to be executed before the data is turned into a tensor with the argument &lt;code&gt;preprocessing&lt;/code&gt;. I specify a lambda function that splits the string into a list of two floats. After that, torchtext passes the list to &lt;code&gt;torch.tensor&lt;/code&gt;, which turns it into a tensor of dimension &lt;em&gt;(2,)&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;label_field_scores = data.Field(sequential=False, batch_first=True, use_vocab=False,
        preprocessing=lambda x: [float(n) for n in x.split(&quot; &quot;)], dtype=torch.float32)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We can then make a list of field objects next to the name we want them to have, and pass it to the initializer of &lt;em&gt;&lt;code&gt;torchtext.data.TabularDataset&lt;/code&gt;&lt;/em&gt; to load each set in one call:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;fields_train = [(&quot;text&quot;, text_field), (&quot;label&quot;, label_field_train)]
train_dataset = data.TabularDataset(
    path=os.path.join(data_dir, &quot;train.tsv&quot;),
    format=&quot;tsv&quot;,  skip_header=True,
    fields=fields_train
)

fields_valid = [(&quot;text&quot;, text_field), (&quot;label&quot;, label_field_valid)]
valid_dataset = data.TabularDataset(
    path=os.path.join(data_dir, &quot;dev.tsv&quot;),
    format=&quot;tsv&quot;, skip_header=True,
    fields=fields_valid
)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It would be possible to load both with only one call to &lt;code&gt;torchtext.data.TabularDataset.splits&lt;/code&gt;, but only when both the training and validation set use the same fields. I avoid using &lt;em&gt;splits&lt;/em&gt; since this condition is not met when the augmented dataset is used for training.&lt;/p&gt;
&lt;p&gt;We also need to specify a &lt;code&gt;torchtext.data.Vocab&lt;/code&gt; for text fields, so that each word can be converted to a corresponding numerical id. In the case of the BiLSTM model, we build one based on fasttext embeddings and the training set:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;vectors = pretrained_aliases[&quot;fasttext.en.300d&quot;](cache=&quot;.cache/&quot;)
text_field.build_vocab(train_dataset, vectors=vectors)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the case of BERT, I encountered two problems:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The &lt;em&gt;transformers&lt;/em&gt; library saves BERT’s vocabulary as a Python dictionary in &lt;code&gt;bert_tokenizer.vocab&lt;/code&gt;. However, there’s no way to initialize &lt;code&gt;torchtext.data.Vocab&lt;/code&gt; with a Python dictionary;&lt;/li&gt;
&lt;li&gt;A few tokens need to be swapped out in order to make BERT work with torchtext. For example, &lt;em&gt;[UNK]&lt;/em&gt; needs to be saved as &lt;em&gt;&amp;lt;unk&amp;gt;&lt;/em&gt;. I implemented the class &lt;code&gt;BertVocab&lt;/code&gt; to handle all of this. It implements the same interface as &lt;code&gt;torchtext.data.Vocab&lt;/code&gt;, so we can just assign it to &lt;code&gt;text_field.vocab&lt;/code&gt; and torchtext will know how to use it.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;text_field.vocab = BertVocab(tokenizer.vocab)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To handle the different cases, I wrote function &lt;code&gt;load_data&lt;/code&gt; in utils.py that knows which files to load and how based on the arguments passed to it. It’s called like this:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# For BiLSTMClassifier
datasets = load_data(data_dir, utils.spacy_tokenizer, augmented=args.augmented)
# For BERT
datasets = load_data(data_dir, bert_tokenizer, bert_vocab=bert_tokenizer.vocab, batch_first=True)

train_dataset, valid_dataset, text_field = datasets&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To generate tensors of batches, we create a &lt;code&gt;torchtext.data.BucketIterator&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;train_it = data.BucketIterator(train_dataset, self.batch_size, train=True, shuffle=True, device=self.device)
val_it = data.BucketIterator(val_dataset, self.batch_size, train=False, sort_key=lambda x: len(x.text), device=self.device)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the class that handles creating the batches, shuffling the dataset, padding, etc.&lt;br/&gt;We can iterate over this object to generate batches:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;example_original_it = data.BucketIterator(original_dataset, 2) 
example_augmented_it = data.BucketIterator(augmented_dataset, 2) 
for original_batch, augmented_batch in zip(example_original_it, example_augmented_it: 
    print(original_batch)
    print(augmented_batch) 
    break&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot;&gt;Output:
[torchtext.data.batch.Batch of size 2]
        [.text]:('[torch.LongTensor of size 21x2]', '[torch.LongTensor of size 2]')
        [.label]:[torch.LongTensor of size 2]

[torchtext.data.batch.Batch of size 2]
        [.text]:('[torch.LongTensor of size 17x2]', '[torch.LongTensor of size 2]')
        [.label]:[torch.FloatTensor of size 2x2]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;text&lt;/em&gt;, &lt;em&gt;label&lt;/em&gt; for the original dataset:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot;&gt;(tensor([[   46,     5],
        [  344,   239],
        ...
        [   17,     1],
        [   10,     1]]), tensor([21, 15])) tensor([0, 1])&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;text&lt;/em&gt; is a tuple with the word ids and sentence lengths, &lt;em&gt;label&lt;/em&gt; contains the class for each sentence.&lt;br/&gt;&lt;em&gt;label&lt;/em&gt; for the augmented dataset:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot;&gt;tensor([[ 1.5854, -0.8519], [-1.6052,  1.4790]])&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here, &lt;em&gt;label&lt;/em&gt; contains the scores for each class.&lt;/p&gt;
&lt;h2 id=&quot;data-augmentation&quot;&gt;Data augmentation&lt;/h2&gt;
&lt;p&gt;In this section, I’ll draw an outline of the procedure I used for data augmentation. As mentioned before, this is taken directly from the paper.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Load original tsv file
input_tsv = load_tsv(args.input)
sentences = [spacy_en(text) for text, _ in tqdm(input_tsv, desc=&quot;Loading dataset&quot;)]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;br/&gt;First, the input file, &lt;em&gt;train.tsv&lt;/em&gt; in the main folder of the SST-2 dataset, is loaded into a simple list of lists. Then the sentences are run through spacy’s &lt;em&gt;“en”&lt;/em&gt; pipeline, which returns the processed text.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Build lists of words’ indexes by POS tag
pos_dict = build_pos_dict(sentences)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The second step is to build a dictionary of Part-of-Speech tags. The dictionary will contain, for each tag that was encountered, a list of words that have been assigned that tag in the corpus.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# First 10 words classified as verbs 
print(pos_dict[&quot;VERB&quot;][:10])

['hide',
 'contains',
 'labored',
 'loves',
 'communicates',
 'remains',
 'remain',
 'clichés',
 'could',
 'dredge']&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next comes the generation step:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Generate augmented samples
sentences = augmentation(sentences, pos_dict)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;augmentation&lt;/code&gt; is a simple function that runs &lt;code&gt;make_sample&lt;/code&gt; in a loop and checks for duplicates. The latter function is more interesting:&lt;br/&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def make_sample(input_sentence, pos_dict, p_mask=0.1, p_pos=0.1, p_ng=0.25, max_ng=5):
    sentence = []
    for word in input_sentence:
        # Apply single token masking or POS-guided replacement
        u = np.random.uniform()
        if u &amp;lt; p_mask:
            sentence.append(mask_token)
        elif u &amp;lt; (p_mask + p_pos):
            same_pos = pos_dict[word.pos]
            # Pick from list of words with same POS tag
            sentence.append(np.random.choice(same_pos))
        else:
            sentence.append(word.text.lower())
    # Apply n-gram sampling
    if len(sentence) &amp;gt; 2 and np.random.uniform() &amp;lt; p_ng:
        n = min(np.random.choice(range(1, 5+1)), len(sentence) - 1)
        start = np.random.choice(len(sentence) - n)
        for idx in range(start, start + n):
            sentence[idx] = mask_token
    return sentence&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As you can see, either single token masking &lt;strong&gt;or&lt;/strong&gt; POS-guided replacement is applied, with probabilities &lt;code&gt;p_mask&lt;/code&gt; and &lt;code&gt;p_pos&lt;/code&gt;. Then, with probability &lt;code&gt;p_ng&lt;/code&gt;, a value &lt;code&gt;n&lt;/code&gt; is sampled from the distribution &lt;code&gt;[1, 2, 3, 4, 5]&lt;/code&gt;, and a group of &lt;code&gt;n&lt;/code&gt; adjacent words is sampled for masking.&lt;/p&gt;
&lt;img src=&quot;https://lh5.googleusercontent.com/fotNbQyTqcj92C3YD7pQ6gjpWbg0pnY05H71JMLZLGAm6CJOYQfgRDwslDQcdpTQLud2I14Tv0UmUgkcuPyxtIPvxkK7MqMWETAAgL2esN_ggHJincrO37Qabxl3cv84H7b6bVt7&quot; class=&quot;kg-image&quot;/&gt;Augmentation pipeline
&lt;p&gt;Here’s an example of a sentence from the original dataset and a corresponding sample:&lt;br/&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Original:&lt;/strong&gt; contains no wit , only labored gags&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Generated:&lt;/strong&gt; contains no wit , only &amp;lt;mask&amp;gt; appetite&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Load teacher model
model = BertForSequenceClassification.from_pretrained(args.model).to(device)
tokenizer = BertTokenizer.from_pretrained(args.model, do_lower_case=True)

# Assign labels with teacher
teacher_field = data.Field(sequential=True, tokenize=tokenizer.tokenize, lower=True, include_lengths=True, batch_first=True)
fields = [(&quot;text&quot;, teacher_field)]
examples = [
    data.Example.fromlist([&quot; &quot;.join(words)], fields) for words in sentences
]
augmented_dataset = data.Dataset(examples, fields)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now that the new sentences have been generated, it’s time to assign them labels.&lt;br/&gt;As far as I know, there’s no one-liner for loading a dataset from a list of sentences in torchtext. Instances of &lt;code&gt;torch.data.Example&lt;/code&gt; have to be constructed first, each corresponding to a sentence.&lt;/p&gt;
&lt;p&gt;Each &lt;code&gt;torchtext.data.Example&lt;/code&gt; is initialized with a sentence and &lt;code&gt;teacher_field&lt;/code&gt;, which contains the necessary information about what type of data the example contains.The list of examples can then be used to initialize a &lt;code&gt;torchtext.data.Dataset&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next, we need some code for calling the model. I use this class for training, evaluation and inference:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def __init__(self, model, device,
    loss=&quot;cross_entropy&quot;,
    train_dataset=None,
    temperature=1.0,
    val_dataset=None, val_interval=1,
    checkpt_callback=None, checkpt_interval=1,
    max_grad_norm=1.0, batch_size=64, gradient_accumulation_steps=1,
    lr=5e-5, weight_decay=0.0):&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For now we only need to do inference, so we can ignore all the keyword arguments.&lt;br/&gt;For reasons that will be explained later, I defined two sub-classes of &lt;code&gt;Trainer&lt;/code&gt;: &lt;code&gt;LSTMTrainer&lt;/code&gt; and &lt;code&gt;BertTrainer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To generate the new labels with BERT, &lt;code&gt;BertTrainer&lt;/code&gt;’s &lt;code&gt;infer&lt;/code&gt; method is called:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;new_labels = BertTrainer(model, device).infer(augmented_dataset)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This returns the raw class scores output by BERT in a numpy array. The only thing left to do is to write them into an output .tsv file:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Write to file
with open(args.output, &quot;w&quot;) as f:
    f.write(&quot;sentence\tscores\n&quot;)
    for sentence, rating in zip(sentences, new_labels):
        text = &quot; &quot;.join(sentence)
        f.write(&quot;%s\t%.6f %.6f\n&quot; % (text, *rating))&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The output file will look like this:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-bash&quot;&gt;sentence scores
hide new secretions from the parental units     0.514509 0.009824
hide new secretions from either &amp;lt;mask&amp;gt; units      1.619976 -0.642274
provide &amp;lt;mask&amp;gt; secretions from the heartache units        1.875626 -0.908450
&amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; secretions &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; &amp;lt;mask&amp;gt; units       0.879091 -0.349838
hide new &amp;lt;mask&amp;gt; from half parental units  1.410404 -0.473437
hide &amp;lt;mask&amp;gt; secretions from some &amp;lt;mask&amp;gt; units       2.397400 -1.622049
hide new secretions from half parental units    1.644318 -0.587317
hide new &amp;lt;mask&amp;gt; featherweight the parental units  0.108670 0.199454
...&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;bi-lstm&quot;&gt;Defining the BiLSTM model&lt;/h2&gt;
&lt;p&gt;The code for the BiLSTM network is encapsulated within the class BiLSTMClassifier:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;class BiLSTMClassifier(nn.Module):
    def __init__(self, num_classes, vocab_size, embed_size, lstm_hidden_size, classif_hidden_size, lstm_layers=1, dropout_rate=0.0, use_multichannel_embedding=False):
        super().__init__()
        self.vocab_size = vocab_size
        self.lstm_hidden_size = lstm_hidden_size
        self.use_multichannel_embedding = use_multichannel_embedding
        if self.use_multichannel_embedding:
            self.embedding = MultiChannelEmbedding(self.vocab_size, embed_size, dropout_rate=dropout_rate)
            self.embed_size = len(self.embedding.filters) * self.embedding.filters_size
        else:
            self.embedding = nn.Embedding(self.vocab_size, embed_size)
            self.embed_size = embed_size
        self.lstm = nn.LSTM(self.embed_size, self.lstm_hidden_size, lstm_layers, bidirectional=True, dropout=dropout_rate)
        self.classifier = nn.Sequential(
            nn.Linear(lstm_hidden_size*2, classif_hidden_size),
            nn.ReLU(inplace=True),
            nn.Dropout(p=dropout_rate),
            nn.Linear(classif_hidden_size, num_classes)
        )&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The network is composed of three main parts:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Word embeddings. To mimic the paper more accurately I implemented an option to use &lt;em&gt;multichannel embeddings&lt;/em&gt;, implemented in the class &lt;code&gt;MultiChannelEmbedding&lt;/code&gt;. However, in my tests, I haven’t found them to measurably improve the performance, so I default to the “plain” &lt;code&gt;torch.nn.Embedding&lt;/code&gt; layer.&lt;/li&gt;
&lt;li&gt;One or more LSTM layers.&lt;/li&gt;
&lt;li&gt;A simple, non-linear classifier with one hidden layer&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;For this article, I define the network to have 1 LSTM layer with hidden size 300 and the classifier’s hidden layer to have size 400. I set the dropout rate to 0.15.&lt;br/&gt;When using the &lt;a href=&quot;https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM&quot;&gt;LSTM&lt;/a&gt; layer from &lt;code&gt;torch.nn&lt;/code&gt; with variable length sequences, some adjustments have to be applied to the input first. Let me walk you through the forward function:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Sort batch
seq_size, batch_size = seq.size(0), seq.size(1)
length_perm = (-length).argsort()
length_perm_inv = length_perm.argsort()
seq = torch.gather(seq, 1, length_perm[None, :].expand(seq_size, batch_size))
length = torch.gather(length, 0, length_perm)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;First, the sequences in the batch have to be sorted by length. I know it looks complicated, so let’s break it down:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;length_perm = (-length).argsort()
length_perm_inv = length_perm.argsort()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first line returns a permutation of &lt;em&gt;length&lt;/em&gt;, as a list of indexes to the tensor, that sorts the sequences in increasing order. The second line creates a permutation that reverses the first one.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;seq = torch.gather(seq, 1, length_perm[None, :].expand(seq_size, batch_size))
length = torch.gather(length, 0, length_perm)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Applies the permutation to the two input tensors.The expression &lt;code&gt;length_perm[None, :].expand(seq_size, batch_size)&lt;/code&gt;  is meant to make sure that the size of tensor &lt;code&gt;length_perm&lt;/code&gt; matches that of &lt;code&gt;seq&lt;/code&gt; at dimension 0. Refer to &lt;a href=&quot;https://pytorch.org/docs/stable/torch.html#torch.gather&quot;&gt;torch.gather&lt;/a&gt; for the details, but the important thing is that now &lt;code&gt;seq&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt; are sorted by the length of each sequence.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Pack sequence
seq = self.embedding(seq)
seq = pack_padded_sequence(seq, length)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;pack_padded_sequence&lt;/code&gt; is imported from &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html&quot;&gt;torch.nn.utils.rnn&lt;/a&gt;, and it returns an instance of &lt;code&gt;PackedSequence&lt;/code&gt;, that is, an array where the sequences are packed in a way that makes it possible for the LSTM to run more efficiently, without the padding tokens.&lt;/p&gt;
&lt;img src=&quot;https://lh4.googleusercontent.com/4RVI_pKbQYYmedG7H9bSCvsmImReuVrAHgm0UXb7P0MSpfPhZm8M7WivaXdK5a6dklCAkOc-V6EzkVbK92_Si8bdlvlM6vZFdTFXKi8wpEZN0bJ-PVRUDMqIeizgzmB7hX_es_bW&quot; class=&quot;kg-image&quot;/&gt;The structure of the packed sequence allows the LSTM implementation to first process all the first tokens in a &lt;em&gt;memory-efficient&lt;/em&gt; way, then all the second tokens, and so on…
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Send through LSTM
features, hidden_states = self.lstm(seq)
# Unpack sequence
features = pad_packed_sequence(features)[0]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next, the LSTM processes the sentences, and the resulting features are turned back into a padded sequence with &lt;a href=&quot;https://pytorch.org/docs/stable/_modules/torch/nn/utils/rnn.html&quot;&gt;torch.nn.utils.rnn.pad_packed_sequence&lt;/a&gt;.&lt;br/&gt;Since we’re using a bidirectional LSTM, it returns an output of shape \((N_{seq}, N_{batch}, 2*hidden\_size)\), where the last dimension contains the features from the &lt;em&gt;forward-reading&lt;/em&gt; LSTM layer concatenated with those from the &lt;em&gt;backward-reading&lt;/em&gt; LSTM layer.&lt;/p&gt;
&lt;p&gt;Next steps are to:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Separate two outputs by reshaping the tensor to \((N_{seq}, N_{batch}, 2, hidden\_size)\)&lt;/li&gt;
&lt;li&gt;Gather the features corresponding to the last word in each sequence and take only the forward features&lt;/li&gt;
&lt;li&gt;Take the backward features for the first word of each sequence&lt;/li&gt;
&lt;li&gt;Concatenate&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Index to get forward and backward features and concatenate
# Gather the last word for each sequence
last_indexes = (length - 1)[None, :, None, None].expand((1, batch_size, 2, features.size(-1)))
forward_features = torch.gather(features, 0, last_indexes)
# Squeeze seq dimension, take forward features
forward_features = forward_features[0, :, 0]
# Take the first word, backward features
backward_features = features[0, :, 1]
features = torch.cat((forward_features, backward_features), -1)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Finally, the features are sent through the classifier to get the raw probability scores:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Send through the classifier
logits = self.classifier(features)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The last thing to do before returning the result is to apply &lt;code&gt;length_perm_inv&lt;/code&gt;. If you remember, the sequences in the batch are not in the same order as they were when &lt;em&gt;BiLSTMClassifier&lt;/em&gt; was called. The original order needs to be restored for the scores to make sense outside of this function:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Invert batch permutation
logits = torch.gather(logits, 0, length_perm_inv[:, None].expand((batch_size, logits.size(-1))))
return logits, hidden_states&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;training-loop&quot;&gt;Defining the training loop&lt;/h2&gt;
&lt;p&gt;I will now give an overview of the training loop. I will try to focus on the fun parts and leave out the unnecessary ones, like logging to Tensorboard and conditioning on options.&lt;br/&gt;The training loop is managed by the class &lt;code&gt;Trainer&lt;/code&gt;, which is initialized like so:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;trainer = LSTMTrainer(model, &quot;mse&quot; if args.augmented else &quot;cross_entropy&quot;, device, train_dataset=train_dataset, val_dataset=valid_dataset, val_interval=250, checkpt_callback=lambda m, step: save_bilstm(m, os.path.join(args.output_dir, &quot;checkpt_%d&quot; % step)), checkpt_interval=250, batch_size=args.batch_size, lr=args.lr)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The parameters are, in order:&lt;br/&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The model to be trained;&lt;/li&gt;
&lt;li&gt;The loss function to be used (more on that later);&lt;/li&gt;
&lt;li&gt;The device on which to execute the computations;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;train_dataset&lt;/code&gt;, the training dataset, as an instance of &lt;code&gt;torchtext.data.Dataset&lt;/code&gt;&lt;em&gt;;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;val_dataset&lt;/code&gt;, the validation dataset;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;val_interval&lt;/code&gt;&lt;em&gt;,&lt;/em&gt; how many gradient steps to execute between evaluations on the validation dataset;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpt_callback&lt;/code&gt;, the function to call in order to save a checkpoint of the model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;checkpt_interval&lt;/code&gt;, how many steps to execute between checkpoints&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The method for training the model is &lt;code&gt;Trainer.train&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def train(self, epochs=1, schedule=None, **kwargs):&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;schedule&lt;/code&gt; is the learning rate schedule, that can be set to &lt;code&gt;None&lt;/code&gt; for fixed learning rate, &lt;em&gt;&quot;warmup&quot;&lt;/em&gt; for a linear warmup schedule and &lt;em&gt;“cyclic”&lt;/em&gt; for cyclic learning rate. &lt;code&gt;kwargs&lt;/code&gt; captures settings specific to each schedule.&lt;br/&gt;All the models for this article are trained with a &lt;em&gt;warmup&lt;/em&gt; schedule, with the warmup lasting between 50 and 200 gradient steps depending on which value gives the best performance.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;warmup_steps = kwargs[&quot;warmup_steps&quot;]
self.scheduler = torch.optim.lr_scheduler.CyclicLR(self.optimizer, base_lr=self.lr/100, max_lr=self.lr, step_size_up=max(1, warmup_steps), step_size_down=(total_steps - warmup_steps), cycle_momentum=False)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When the &lt;em&gt;warmup&lt;/em&gt; schedule is selected we create a &lt;code&gt;torch.optim.lr_scheduler.CyclicLR&lt;/code&gt; and set it to increase the learning rate linearly for &lt;code&gt;warmup_steps&lt;/code&gt;, then decrease it over the rest of the training.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;for epoch in trange(epochs, desc=&quot;Training&quot;):
    for batch in tqdm(self.train_it, desc=&quot;Epoch %d&quot; % epoch, total=len(self.train_it)):
        self.train_step(batch)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For every epoch, we iterate over &lt;code&gt;self.train_it&lt;/code&gt;. &lt;code&gt;self.train_it&lt;/code&gt; is created in &lt;code&gt;Trainer.__init__&lt;/code&gt; with&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;data.BucketIterator(self.train_dataset, self.batch_size, train=True, sort_key=lambda x: len(x), device=self.device)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;train_step&lt;/code&gt; contains all the usual stuff:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def train_step(self, batch):
    self.model.train()
    batch, label, curr_batch_size = self.process_batch(batch)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;First, &lt;code&gt;Trainer.process_batch&lt;/code&gt; is called. This is the only function that works differently in &lt;code&gt;LSTMTrainer&lt;/code&gt; and &lt;code&gt;BertTrainer&lt;/code&gt;: For BERT, we need to compute the attention masks based on the length of each sequence in the batch. &lt;code&gt;process_batch&lt;/code&gt; returns a tuple of three values:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;A dictionary with the keyword arguments required to call the model in question. For example, in &lt;code&gt;BertTrainer&lt;/code&gt;, its content is &lt;code&gt;{“input_ids”: ..., “attention_mask”: ...}&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The labels&lt;/li&gt;
&lt;li&gt;The size of this batch, which is calculated differently based on the model. This is because BERT, unlike the BiLSTM model, uses &lt;code&gt;batch_first=True&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;s_logits = self.model(**batch)[0] # **kwargs syntax expands a dictionary into keyword arguments
loss = self.get_loss(s_logits, label, curr_batch_size)
# Compute the gradients
loss.backward()
# Apply gradient clipping
nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
self.optimizer.step()
if self.scheduler is not None:
    # Advance learning rate schedule
    self.scheduler.step()
# Clear the gradients for the next step
self.model.zero_grad()
# Save stats to tensorboard
# ...
# Every val_interval steps, evaluate and log stats to tensorboard
if self.val_dataset is not None and (self.global_step + 1) % self.val_interval == 0:
    results = self.evaluate()
    # Log to tensorboard...
# Save checkpoint every checkpt_interval steps...&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;loss&lt;/code&gt; is calculated in &lt;code&gt;Trainer.get_loss&lt;/code&gt; based on the loss function selected when creating the &lt;code&gt;Trainer&lt;/code&gt;. The options are &lt;em&gt;mse&lt;/em&gt;, &lt;em&gt;cross_entropy&lt;/em&gt; and &lt;em&gt;kl_div&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def get_loss(self, model_output, label, curr_batch_size):
    if self.loss_option in [&quot;cross_entropy&quot;, &quot;mse&quot;]:
        loss = self.loss_function(
            model_output,
            label
        ) / curr_batch_size # Mean over batch
    elif self.loss_option == &quot;kl_div&quot;:
        # KL Divergence loss needs special care
        # It expects log probabilities for the model's output, and probabilities for the label
        loss = self.loss_function(
            F.log_softmax(model_output / self.temperature, dim=-1),
            F.softmax(label / self.temperature, dim=-1)
        ) / (self.temperature * self.temperature) / curr_batch_size
    return loss&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When using &lt;em&gt;cross_entropy&lt;/em&gt; or &lt;em&gt;mse&lt;/em&gt;, the loss function is called in the same way, then averaged over the examples in the batch. On the other hand, when using &lt;em&gt;kl_div&lt;/em&gt;, &lt;code&gt;torch.nn.KLDivLoss&lt;/code&gt; expects log probabilities as the first argument and probabilities as the second. We also scale the scores by &lt;em&gt;temperature&lt;/em&gt;, then divide the loss by &lt;code&gt;(self.temperature * self.temperature)&lt;/code&gt; in order to &lt;a href=&quot;https://blog.floydhub.com/knowledge-distillation/#references&quot;&gt;maintain approximately the same scale of loss values as the cross-entropy&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to evaluate the model on the validation dataset, &lt;code&gt;Trainer.evaluate&lt;/code&gt; is called.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;def evaluate(self):
    self.model.eval()
    val_loss = val_accuracy = 0.0
    loss_func = nn.CrossEntropyLoss(reduction=&quot;sum&quot;)
    for batch in tqdm(self.val_it, desc=&quot;Evaluation&quot;, total=len(self.val_it)):
        with torch.no_grad():
           batch, label, _ = self.process_batch(batch)
           output = self.model(**batch)[0]
           loss = loss_func(output, label)
           val_loss += loss.item()
           val_accuracy += (output.argmax(dim=-1) == label).sum().item()
    val_loss /= len(self.val_dataset)
    val_accuracy /= len(self.val_dataset)
    return {
        &quot;loss&quot;: val_loss,
        &quot;perplexity&quot;: np.exp(val_loss),
        &quot;accuracy&quot;: val_accuracy
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;launching&quot;&gt;Launching the training loop&lt;/h2&gt;
&lt;p&gt;Putting all the pieces together, we can start the training.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;train_bilstm.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;train_dataset, valid_dataset, text_field = load_data(args.data_dir, spacy_tokenizer, augmented=args.augmented)
vocab = text_field.vocab

model = BiLSTMClassifier(2, len(vocab.itos), vocab.vectors.shape[-1],
        lstm_hidden_size=300, classif_hidden_size=400, dropout_rate=0.15).to(device)
# Initialize word embeddings to fasttext
model.init_embedding(vocab.vectors.to(device))
    
trainer = LSTMTrainer(model, device,
    loss=&quot;mse&quot; if args.augmented or args.use_teacher else &quot;cross_entropy&quot;,
    train_dataset=train_dataset,
    val_dataset=valid_dataset, val_interval=250,
    checkpt_interval=args.checkpoint_interval, checkpt_callback=lambda m, step: save_bilstm(m, os.path.join(args.output_dir, &quot;checkpt_%d&quot; % step)),
    batch_size=args.batch_size, gradient_accumulation_steps=args.gradient_accumulation_steps,
    lr=args.lr)
    
if args.do_train:
    trainer.train(args.epochs, schedule=args.lr_schedule,
        warmup_steps=args.warmup_steps, epochs_per_cycle=args.epochs_per_cycle)

print(&quot;Evaluating model:&quot;)
print(trainer.evaluate())&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;train_bert.py&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-python&quot;&gt;# Load model from transformers library
bert_config = BertConfig.from_pretrained(&quot;bert-large-uncased&quot;, cache_dir=args.cache_dir)
bert_model = BertForSequenceClassification.from_pretrained(&quot;bert-large-uncased&quot;, config=bert_config, cache_dir=args.cache_dir).to(device)
bert_tokenizer = BertTokenizer.from_pretrained(&quot;bert-large-uncased&quot;, do_lower_case=True, cache_dir=args.cache_dir)
train_dataset, valid_dataset, _ = load_data(args.data_dir, bert_tokenizer.tokenize,
        vocab=BertVocab(bert_tokenizer.vocab), batch_first=True)

train_dataset, valid_dataset, vocab = load_data(args.data_dir, bert_tokenizer.tokenize, bert_vocab=bert_tokenizer.vocab, batch_first=True)
    
trainer = BertTrainer(bert_model, device,
    loss=&quot;cross_entropy&quot;,
    train_dataset=train_dataset,
    val_dataset=valid_dataset, val_interval=250,
    checkpt_callback=lambda m, step: save_bert(m, bert_tokenizer, bert_config, os.path.join(args.output_dir, &quot;checkpt_%d&quot; % step)),
    checkpt_interval=args.checkpoint_interval,
    batch_size=args.batch_size, gradient_accumulation_steps=args.gradient_accumulation_steps,
    lr=args.lr)

print(&quot;Evaluating model:&quot;)
print(trainer.evaluate())&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;the-results&quot;&gt;The results&lt;/h2&gt;
&lt;img src=&quot;https://blog.floydhub.com/content/images/2019/11/performance_table-5.png&quot; class=&quot;kg-image&quot;/&gt;*Inference time for the SST-2 validation set measured on a CPU with batch size 1. **Accuracy averaged across 20 runs, plus or minus one standard deviation.
&lt;p&gt;As you can see from the table, the BiLSTM model trained on the augmented dataset attains a relatively competitive performance with bert-large-uncased, while taking about &lt;strong&gt;65 times less&lt;/strong&gt; &lt;strong&gt;computation&lt;/strong&gt; to run inference on a single sentence, and being about &lt;strong&gt;57&lt;/strong&gt; &lt;strong&gt;times lighter&lt;/strong&gt; when it comes to the number of trainable parameters.&lt;/p&gt;
&lt;p&gt;As an ablation study, I also tried to train the BiLSTM model on the BERT’s labels without applying data augmentation. I found that the improvement given only by training on the teacher's scores is not substantial, compared to when we include the data augmentation procedure.&lt;/p&gt;
&lt;p&gt;My hypothesis is that, for a tiny and structurally different student model like &lt;em&gt;BiLSTMClassifier&lt;/em&gt;, knowledge distillation does not bring the same advantages as it does for huggingface’s &lt;em&gt;DistilBERT&lt;/em&gt;. While it does seem to offer a slightly better training objective, most of the improvement in the student’s performance can be attributed to how the teacher made it possible to assign reasonably accurate labels to a very perturbed dataset, and the &lt;strong&gt;sheer size of the datasets that can be generated this way&lt;/strong&gt;. For example, in the paper this article is inspired by, as well as my tests, we produced &lt;strong&gt;20&lt;/strong&gt; augmented examples for every example in SST-2.&lt;/p&gt;
&lt;h2 id=&quot;lessons-learned&quot;&gt;Lessons learned&lt;/h2&gt;
&lt;p&gt;Trying to reproduce the results from &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt; taught me many valuable lessons about conducting research on NLP. Here are the most important ones that would have saved me many hours if only I knew them when I started:&lt;/p&gt;
&lt;h3 id=&quot;use-well-established-libraries&quot;&gt;Use well-established libraries&lt;/h3&gt;
&lt;p&gt;Always try to use well-established techniques, libraries, and resources over writing your own code and trying to come up with your custom solutions, unless you need to do something really unusual.&lt;/p&gt;
&lt;p&gt;On my first try, I used an outdated tokenizer library and downloaded word2vec embeddings pre-trained on Google News from &lt;em&gt;some Drive folder&lt;/em&gt;, in an effort to use the same exact embeddings as the paper.&lt;/p&gt;
&lt;p&gt;However, when using uncommon or outdated libraries and resources, it’s difficult to reproduce someone else’s results. In fact, your results will vary wildly based on unpredictable factors, like the exact version of a library. This can hinder you from reproducing their results, especially in the case when the code for the paper wasn’t published and you base your code only on the details that were explicitly mentioned.&lt;/p&gt;&lt;p&gt;In practice, it turned out to be a better decision to use fasttext embeddings, which are not the same as in the paper, just because they’re well integrated with Torchtext. The simple fact that Torchtext includes a link to pretrained fasttext embeddings makes it more likely they were tested by the maintainers of the library.&lt;/p&gt;
&lt;p&gt;Well-maintained resources give much more reliable results, and the fewer lines of code you have to write, the less you risk writing bugs.&lt;/p&gt;
&lt;p&gt;Since I have a bad habit of writing my own algorithms for everything, I’m also glad I looked into using the spacy NLP library for Part-of-Speech guided word replacement instead of writing custom code, as it only took a few lines. If I had tried to write it on my own, my guess is that it would’ve taken days and I would’ve come up with a very suboptimal solution.&lt;/p&gt;
&lt;h3 id=&quot;double-check-the-paper-then-check-it-again&quot;&gt;Double-check the paper, then check it again&lt;/h3&gt;
&lt;p&gt;On my first try, I have to admit I read the paper in question very sloppily. I spent many days testing and debugging my code, trying to figure out why I wasn’t getting any performance gains with the distillation training objective. It turned out I had skimmed over the authors’ data augmentation scheme. After I contacted the authors of the paper, they kindly mentioned that as a possible problem in my implementation. I spent only one afternoon in a caffeine-and-heavy-metal-induced coding trance replicating the scheme and it made &lt;strong&gt;all&lt;/strong&gt; the difference.&lt;/p&gt;
&lt;h3 id=&quot;don-t-spend-too-much-time-tuning-hyper-parameters&quot;&gt;Don’t spend too much time &lt;a href=&quot;https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/&quot;&gt;tuning hyper-parameters&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Unless that’s the object of your study.&lt;br/&gt;If you’re using a network architecture someone else invented, which most of the time you should, chances are you’ll be able to change the hyper-parameters quite a bit while only getting slight variations in performance. Most often, your time is better spent engineering the data and trying to come up with the best way to formulate the problem that fits your needs.&lt;br/&gt;This is not to say you shouldn’t tune them, but try to leave it for the end when everything else is decided. Your problem has high enough dimensionality as it is.&lt;/p&gt;
&lt;hr/&gt;&lt;h3 id=&quot;about-alex-amadori&quot;&gt;&lt;strong&gt;About Alex Amadori&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Alex is a student at &lt;a href=&quot;http://42.fr&quot;&gt;42&lt;/a&gt;, Paris. He’s interested in exploring new Deep Learning techniques; in particular, he is fascinated by the problems of reasoning and Meta-Learning, and always striving to gain a better understanding of the challenges involved so he may contribute to the research. You can connect with Alex on &lt;a href=&quot;https://www.linkedin.com/in/alex-amadori-141203185/&quot;&gt;LinkedIn&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/tacchinotacchi&quot;&gt;Twitter&lt;/a&gt; and &lt;a href=&quot;https://github.com/tacchinotacchi&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;Main source:&lt;/p&gt;
</description>
<pubDate>Fri, 15 Nov 2019 18:23:58 +0000</pubDate>
<dc:creator>alexamadoriml</dc:creator>
<og:type>article</og:type>
<og:title>Distilling knowledge from Neural Networks to build smaller and faster models</og:title>
<og:description>This article discusses GPT-2 and BERT models, as well using knowledge distillation to create highly accurate models with fewer parameters than their teachers</og:description>
<og:url>https://blog.floydhub.com/knowledge-distillation/</og:url>
<og:image>https://blog.floydhub.com/content/images/2019/11/teacherclass.jpeg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.floydhub.com/knowledge-distillation/</dc:identifier>
</item>
<item>
<title> Jimmy Wales has quietly launched a Facebook rival</title>
<link>https://www.zdnet.com/article/wikipedias-jimmy-wales-has-quietly-launched-a-facebook-rival-social-network/</link>
<guid isPermaLink="true" >https://www.zdnet.com/article/wikipedias-jimmy-wales-has-quietly-launched-a-facebook-rival-social-network/</guid>
<description>&lt;p&gt;Wikipedia co-founder Jimmy Wales has quietly rolled out a new social network that is intended to get right what Facebook and Twitter have so far been getting wrong. &lt;/p&gt;
&lt;p&gt;The new social network, WT:Social, which Wales &lt;a href=&quot;https://twitter.com/jimmy_wales/status/1192160194693718017&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; data-component=&quot;externalLink&quot;&gt;announced had 25,000 members on November 6&lt;/a&gt;, now has about 78,000 members who are at least intrigued by the idea of a social network that combats fake news. &lt;/p&gt;
&lt;p&gt;The site goes against the ad-funded models normalized by Google, Facebook, and Twitter, instead asking users to pay a subscription fee to access information and communicate on the site. &lt;/p&gt;
&lt;p&gt;It costs $12.99 a month or $100 a year in the US, or €12 a month or €90 a year in Europe. It's £10 and £80 in the UK. In other words, about the same as a Netflix or Spotify subscription. However, questions remain over what content WT:Social can provide that users would be willing to part with money for. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SEE:&lt;/strong&gt; &lt;a href=&quot;http://www.zdnet.com/topic/digital-transformation-a-cxos-guide/&quot;&gt;&lt;strong&gt;Digital transformation: A CXO's guide&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(ZDNet special report) |&lt;/strong&gt; &lt;a href=&quot;https://www.techrepublic.com/resource-library/whitepapers/digital-transformation-a-cxo-s-guide/?ftag=CMG-01-10aaa1b&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; data-component=&quot;externalLink&quot;&gt;&lt;strong&gt;Download the report as a PDF&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(TechRepublic)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The social network is a reboot of WikiTribune, which &lt;a href=&quot;https://en.wikipedia.org/wiki/WikiTribune&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; data-component=&quot;externalLink&quot;&gt;launched in 2017 as a news-sharing service but didn't gain traction&lt;/a&gt; and ended up with Wales laying off several journalists it had hired. WikiTribune was launched off the back of a crowdfunding campaign.   &lt;/p&gt;
&lt;p&gt;Wales &lt;a href=&quot;https://www.ft.com/content/9956ff9c-0622-11ea-a984-fbbacad9e7dd&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer nofollow&quot; data-component=&quot;externalLink&quot;&gt;spoke with the Financial Times&lt;/a&gt; about the new project and the site's jarring proposition for consumers, or rather users, who expect social networks to be free. LinkedIn could be an exception here. &lt;/p&gt;
&lt;section class=&quot;sharethrough-top&quot; data-component=&quot;medusaContentRecommendation&quot; data-medusa-content-recommendation-options=&quot;{&amp;quot;promo&amp;quot;:&amp;quot;promo_zd_recommendation_sharethrough_top_in_article_desktop&amp;quot;,&amp;quot;spot&amp;quot;:&amp;quot;dfp-in-article&amp;quot;}&quot;&gt;
&lt;/section&gt;&lt;p&gt;&quot;The business model of social-media companies, of pure advertising, is problematic,&quot; Wales told the publication.&lt;/p&gt;
&lt;p&gt;&quot;It turns out the huge winner is low-quality content,&quot; he added.  &lt;/p&gt;
&lt;p&gt;Wales hopes WT:Social can attract 50 million or even half a billion users and points to the success of Netflix and Spotify in attracting people who are willing to pay for &quot;meaningful&quot; content. &lt;/p&gt;
&lt;p&gt;Signing up is free, but WT:Social has established a wait-list that paying donors can bypass. So far, just 200 people have paid to skip the waiting list. &lt;/p&gt;
&lt;p&gt;He doesn't expect the social network to be profitable but reckons it can be sustainable with its current barebones staff. &lt;/p&gt;
&lt;p&gt;Wales didn't go down the crowdfunding route this time and says he wants to &quot;keep a tight rein on the costs&quot;. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SEE:&lt;/strong&gt; &lt;a href=&quot;https://www.zdnet.com/article/most-americans-cant-recognize-2fa-https-or-private-browsing/&quot;&gt;&lt;strong&gt;Most Americans can't recognize 2FA, HTTPS, or private browsing&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Instead of the global news focus of WikiTribune, WT:Social aims to build smaller self-sustaining communities built around niche subjects.&lt;/p&gt;
&lt;p&gt;And rather than hire moderators to monitor fake news as Facebook has, Wales' social network would rely on a community of users to enforce standards. Since all content on the platform can be edited or deleted by other users, he believes there's a good incentive for good behavior by users.&lt;/p&gt;
&lt;span class=&quot;img aspect-set&quot;&gt;&lt;img src=&quot;https://zdnet2.cbsistatic.com/hub/i/2019/11/15/d614f502-676b-4c35-8140-dcd4af363a7b/6f4eada0c0a8b41f9f65546f58230b5b/jimmy-wales1.jpg&quot; class=&quot;&quot; alt=&quot;jimmy-wales1.jpg&quot;/&gt;&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;caption&quot;&gt;Jimmy Wales: The business model of social-media companies, of pure advertising, is problematic.  &lt;/span&gt;&lt;/p&gt;
&lt;span class=&quot;credit&quot;&gt;Image: Joi Ito/Wikimedia Commons&lt;/span&gt;


</description>
<pubDate>Fri, 15 Nov 2019 17:22:53 +0000</pubDate>
<dc:creator>cpeterso</dc:creator>
<og:type>article</og:type>
<og:url>https://www.zdnet.com/article/wikipedias-jimmy-wales-has-quietly-launched-a-facebook-rival-social-network/</og:url>
<og:title>Wikipedia's Jimmy Wales has quietly launched a Facebook rival social network | ZDNet</og:title>
<og:description>Would you pay to join a social network? Wikipedia co-founder Jimmy Wales thinks up to 500 million people could.</og:description>
<og:image>https://zdnet3.cbsistatic.com/hub/i/r/2019/11/15/a6aba057-75e0-4c21-9a7a-bf698ea5f62e/thumbnail/770x578/c64f2274f8e8684164e42e9568428181/jimmy-wales2.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.zdnet.com/article/wikipedias-jimmy-wales-has-quietly-launched-a-facebook-rival-social-network/</dc:identifier>
</item>
<item>
<title>Interstellar space even weirder than expected, NASA probe reveals</title>
<link>https://www.nationalgeographic.com/science/2019/11/interstellar-space-weirder-than-expected-nasa-voyager-2-reveals/</link>
<guid isPermaLink="true" >https://www.nationalgeographic.com/science/2019/11/interstellar-space-weirder-than-expected-nasa-voyager-2-reveals/</guid>
<description>&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;&gt;In the blackness of space billions of miles from home, NASA’s Voyager 2 marked a milestone of exploration, becoming just the second spacecraft ever to enter interstellar space in November 2018. Now, a day before the anniversary of that celestial exit, scientists have revealed what Voyager 2 saw as it crossed the threshold—and it’s giving humans new insight into some of the big mysteries of our solar system.&lt;/span&gt;&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;&gt;The findings, spread across five studies &lt;a href=&quot;https://www.nature.com/articles/s41550-019-0928-3&quot;&gt;published&lt;/a&gt; &lt;a href=&quot;https://www.nature.com/articles/s41550-019-0929-2&quot;&gt;today&lt;/a&gt; &lt;a href=&quot;https://www.nature.com/articles/s41550-019-0918-5&quot;&gt;in&lt;/a&gt; &lt;em&gt;&lt;a href=&quot;https://www.nature.com/articles/s41550-019-0920-y&quot;&gt;Nature&lt;/a&gt; &lt;a href=&quot;https://www.nature.com/articles/s41550-019-0927-4&quot;&gt;Astronomy&lt;/a&gt;&lt;/em&gt;, mark the first time that a spacecraft has directly sampled the electrically charged hazes, or plasmas, that fill both interstellar space and the solar system’s farthest outskirts. It’s another first for the spacecraft, which was launched in 1977 and performed the first—and only—flybys of the ice giant planets Uranus and Neptune. (&lt;a href=&quot;https://www.nationalgeographic.com/magazine/2017/08/explore-space-voyager-spacecraft-turns-40/&quot;&gt;Find out more about the Voyager probes’ “grand tour”—and why it almost didn’t happen&lt;/a&gt;.)&lt;/span&gt;&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;/&gt;&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;&gt;Voyager 2’s charge into interstellar space follows that of sibling Voyager 1, which accomplished the same feat in 2012. The two spacecrafts’ data have many features in common, such as the overall density of the particles they’ve encountered in interstellar space. But intriguingly, the twin craft also saw some key differences on their way out—raising new questions about our sun’s movement through the galaxy.&lt;/span&gt;&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;&gt;“This has really been a wonderful journey,” Voyager project scientist &lt;a href=&quot;https://directory.caltech.edu/personnel/ecs&quot;&gt;Ed Stone&lt;/a&gt;, a physicist at Caltech, said in a press briefing last week.&lt;/span&gt;&lt;span itemprop=&quot;articleBody&quot; class=&quot;clearfix&quot;&gt;“It’s just really exciting that humankind is interstellar,” adds physicist &lt;a href=&quot;https://spacephysics.princeton.edu/people/jamie-s-rankin-phd&quot;&gt;Jamie Rankin&lt;/a&gt;, a postdoctoral researcher at Princeton University who wasn’t involved with the studies. “We have been interstellar travelers since Voyager 1 crossed, but now, Voyager 2’s cross is even more exciting, because we can now compare two very different locations ... in the interstellar medium.”&lt;/span&gt;</description>
<pubDate>Fri, 15 Nov 2019 17:09:10 +0000</pubDate>
<dc:creator>el_duderino</dc:creator>
<og:title>Interstellar space even weirder than expected, NASA probe reveals</og:title>
<og:description>The spacecraft is just the second ever to venture beyond the boundary that separates us from the rest of the galaxy.</og:description>
<og:url>https://www.nationalgeographic.com/science/2019/11/interstellar-space-weirder-than-expected-nasa-voyager-2-reveals/</og:url>
<og:type>article</og:type>
<og:image>https://www.nationalgeographic.com/content/dam/science/2019/11/04/voyager/01_voyager_pia22835_hires.ngsversion.1572965622896.adapt.1900.1.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nationalgeographic.com/science/2019/11/interstellar-space-weirder-than-expected-nasa-voyager-2-reveals/</dc:identifier>
</item>
</channel>
</rss>