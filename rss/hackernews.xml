<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Instacart paying 80 cents an hour because worker received a large tip</title>
<link>https://www.workingwa.org/instacart-eighty-cents</link>
<guid isPermaLink="true" >https://www.workingwa.org/instacart-eighty-cents</guid>
<description>&lt;p&gt;In case that email is as confusing to you as it looks to us, let us help you parse it: what Brie is saying is that &lt;strong&gt;because the customer left a high tip on this particular job, the “batch payment” — the amount Instacart actually pays to the worker — was just 80 cents,&lt;/strong&gt; including a mileage reimbursement.&lt;/p&gt;
&lt;p&gt;In other words, Instacart is now confirming &lt;a href=&quot;http://www.workingwa.org/22cents&quot; target=&quot;_blank&quot;&gt;what workers have been saying since the change in pay structure&lt;/a&gt;: that &lt;strong&gt;the company is actually using customers’ tips to pay workers’ wages&lt;/strong&gt;. When a customer tips up-front, it doesn't mean extra money for the worker. Instacart just pays the worker less to make up for it.&lt;/p&gt;
&lt;p&gt;That's right: the customer's tip doesn't get added to the worker's check — it just gets deducted from what Instacart pays. In other words, &lt;strong&gt;up-front tips go to Instacart, not to the worker.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Even Instacart seems to know how messed up it is to pay workers less when they get tipped more — which is why &lt;strong&gt;they’ve denied the practice when speaking to reporters at&lt;/strong&gt; &lt;a href=&quot;https://www.businessinsider.com/instacart-shoppers-threaten-boycott-claim-lower-pay-2018-12&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Business Insider&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;&amp;amp; the&lt;/strong&gt; &lt;a href=&quot;https://www.miamiherald.com/site-services/new-newsletters/business-news/article224072360.html&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Miami Herald&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, it seems like Instacart has been hearing us speaking out about the issue…and decided to double down by finally &lt;strong&gt;admitting that they’re taking workers’ tips&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Instacart knows it’s wrong. Workers know it’s wrong. We know it’s wrong.&lt;/p&gt;
&lt;p&gt;Let’s not let them get away with it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Think a company paying workers 80 cents for more than an hour of work needs to be held accountable?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Want to tell Instacart you don’t think taking workers’ tips is more “accurate” or treats them as “valuable”?&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;https://actionnetwork.org/petitions/instacart-heres-our-22-cents-no-more-tip-theft-low-pay-and-black-box-pay-algorithms&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Click here to sign on to our demands for Instacart&lt;/strong&gt;&lt;/a&gt; — we’ll be in touch to tell you how you can fight back.&lt;/h3&gt;
&lt;p&gt;And if you’re a customer on Instacart, &lt;a href=&quot;http://www.workingwa.org/22cents&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;click here to learn how you can work around Instacart taking your tips&lt;/strong&gt;&lt;/a&gt; by tipping just 22 cents up-front and changing it later.&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 21:10:18 +0000</pubDate>
<dc:creator>timebomb0</dc:creator>
<og:title>earn eighty cents an hour by delivering groceries with instacart! — Working Washington</og:title>
<og:url>http://www.workingwa.org/instacart-eighty-cents/</og:url>
<og:type>website</og:type>
<og:description>Yep, that’s right. Eighty cents. That’s what Instacart actually paid Tom, an Instacart worker, for 69 minutes of his time .</og:description>
<og:image>http://static1.squarespace.com/static/5237604ce4b0e51f969029ae/t/5c4e498c575d1f05fb47f0b9/1548634512546/tom+fb+link+version.png?format=1500w</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.workingwa.org/instacart-eighty-cents</dc:identifier>
</item>
<item>
<title>AresDB: Uber’s GPU-Powered Open-Source, Real-Time Analytics Engine</title>
<link>https://eng.uber.com/aresdb/</link>
<guid isPermaLink="true" >https://eng.uber.com/aresdb/</guid>
<description>&lt;div class=&quot;td-post-featured-image&quot;&gt;&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/Featured-1.png&quot; data-caption=&quot;&quot;&gt;&lt;img width=&quot;696&quot; height=&quot;298&quot; class=&quot;entry-thumb td-modal-image&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-696x298.png&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-696x298.png 696w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-300x128.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-768x329.png 768w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-1024x438.png 1024w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-1068x457.png 1068w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1-981x420.png 981w, https://eng.uber.com/wp-content/uploads/2019/01/Featured-1.png 1500w&quot; sizes=&quot;(max-width: 696px) 100vw, 696px&quot; alt=&quot;&quot; title=&quot;Featured&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;At Uber, real-time analytics allow us to attain business insights and operational efficiency, enabling us to make data-driven decisions to improve experiences on the Uber platform. For example, our operations team relies on data to monitor the market health and spot potential issues on our platform; software powered by machine learning models leverages data to predict rider supply and driver demand; and data scientists use data to improve machine learning models for better forecasting.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In the past, we have utilized many third-party database solutions for real-time analytics, but none were able to simultaneously address all of our functional, scalability, performance, cost, and operational requirements.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Released in November 2018, AresDB is an open source, real-time analytics engine that leverages an unconventional power source, graphics processing units (GPUs), to enable our analytics to grow at scale. An emerging tool for real-time analytics, GPU technology has advanced significantly over the years, making it a perfect fit for real-time computation and data processing in parallel.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In the following sections, we describe the design of AresDB and how this powerful solution for real-time analytics has allowed us to more performatively and efficiently unify, simplify, and improve Uber’s real-time analytics database solutions. After reading this article, we hope you try out AresDB for your own projects and find the tool useful your own analytics needs, too!&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;Real-time analytics applications at Uber&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;Data analytics are crucial to the success of Uber’s business. Among other functions, these analytics are used to:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Build&lt;/span&gt; &lt;strong&gt;dashboards&lt;/strong&gt; &lt;span&gt;to monitor our business metrics&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Make&lt;/span&gt; &lt;strong&gt;automated decisions&lt;/strong&gt; &lt;span&gt;(such as&lt;/span&gt; &lt;a href=&quot;https://www.uber.com/drive/partner-app/how-surge-works/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;trip pricing&lt;/span&gt;&lt;/a&gt; &lt;span&gt;and&lt;/span&gt; &lt;a href=&quot;https://eng.uber.com/advanced-technologies-detecting-preventing-fraud-uber/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;fraud detection&lt;/span&gt;&lt;/a&gt;&lt;span&gt;) based on aggregated metrics that we collect&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Make&lt;/span&gt; &lt;strong&gt;ad hoc queries&lt;/strong&gt; &lt;span&gt;to diagnose and troubleshoot business operations issues&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;We can summarize these functions into categories with different requirements as follows:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td&gt;&lt;strong&gt;Dashboards&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Decision Systems&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Ad hoc Queries&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Query Pattern&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Well known&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Well known&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Arbitrary&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Query QPS&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;High&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;High&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Low&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Query Latency&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Low&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Low&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;High&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Dataset&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Subset&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Subset&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;All data&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;span&gt;Dashboards and decision systems leverage real-time analytical systems to make similar queries over relatively small, yet highly valuable, subsets of data (with maximum data freshness) at high QPS and low latency.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;The need for another analytical engine&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;The most common problem that real-time analytics solves at Uber is how to compute time series aggregates, calculations that give us insight into the user experience so we can improve our services accordingly. With these computations, we can request metrics by specific dimensions (such as day, hour, city ID, and trip status) over a time range on arbitrarily filtered (or sometimes joined) data. Over the years, Uber has deployed multiple solutions to solve this problem in different ways.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Some of the third-party solutions we’ve used for solving this type of problem include:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/apache/incubator-pinot&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Apache Pinot&lt;/strong&gt;&lt;/a&gt;&lt;span&gt;, an open source distributed analytical database written in Java, can be leveraged for large-scale data analytics. Pinot employs a lambda architecture internally to query batch and real-time data in columnar storage, uses inverted bitmap index for filtering, and relies on star-tree for aggregate result caching. However, it does not support key-based deduplication, upsert, joins, and advanced query features such as geo-spatial-filtering. In addition, being a JVM-based database, query execution on Pinot runs at a higher cost in terms of memory usage.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt;&lt;/a&gt; &lt;span&gt;is used at Uber for a variety of streaming analytics needs. It was built on Apache&lt;/span&gt; &lt;a href=&quot;http://lucene.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Lucene&lt;/span&gt;&lt;/a&gt; &lt;span&gt;for full-text keyword search that stores documents and inverted index. It has been widely adopted and extended to also support aggregates. The inverted index enables filtering, yet it is not optimized for time range-based storage and filtering. It stores records as JSON documents, imposing additional storage and query access overhead. Like Pinot, Elasticsearch is a JVM-based database, and as such, does not support joins and its query execution runs at a higher memory cost.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;While these technologies have strengths of their own, they lacked crucial functionalities for our use case. We needed a unified, simplified, and optimized solution, and thought outside-of-the-box (or rather, inside the GPU) to reach a solution.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;Leveraging GPUs for real-time analytics&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;To render realistic views of images at a high frame rate, GPUs process a massive amount of geometries and pixels in parallel at high speed. While the clock-rate increase for processing units has plateaued over the past few years, the number of transistors on a chip has only increased per&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%27s_law&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Moore’s law&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. As a result, GPU computation speeds, measured in Gigaflops per second (GFLOP/s), are rapidly increasing. Figure 1, below, depicts the theoretical GFLOP/s trend comparing NVIDIA GPUs and Intel CPUs over the years:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image11-2.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5345&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image11-2.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;314&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image11-2.png 1999w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-300x157.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-768x402.png 768w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-1024x536.png 1024w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-696x365.png 696w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-1068x559.png 1068w, https://eng.uber.com/wp-content/uploads/2019/01/image11-2-802x420.png 802w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 1: Comparison of CPU and GPU single precision floating point performance through the years. Image taken from Nvidia’s CUDA C programming guide.
&lt;p&gt;&lt;span&gt;When designing our real-time analytics querying engine, integrating GPU processing was a natural fit. At Uber, the typical real-time analytical query processes a few days of data with millions to billions of records and then filters and aggregates them in a short amount of time. This computation task fits perfectly into the parallel processing model of general purpose GPUs because they:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Process data in parallel very quickly.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Deliver greater computation throughput (GFLOPS/s), making them a good fit for heavy computation tasks (per unit data) that can be parallelized.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Offer greater compute-to-storage (ALU to GPU global memory) data access throughput (not latency) compared to central processing units (CPUs), making them ideal for processing I/O (memory)-bound parallel tasks that require a massive amount of data.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;Once we settled on using a GPU-based analytical database, we assessed a few existing analytics solutions that leveraged GPUs for our needs:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.kinetica.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Kinetica&lt;/strong&gt;&lt;/a&gt;&lt;span&gt;, a GPU-based analytics engine, was initially marketed towards U.S. military and intelligence applications in 2009. While it demonstrates the great potential of GPU technology in analytics, we found many key features missing&lt;/span&gt; &lt;span&gt;for our use case, including&lt;/span&gt; &lt;span&gt;schema alteration, partial insertion or updates, data compression, column-level memory/disk retention configuration, and join by geospatial relationships.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.omnisci.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;OmniSci&lt;/strong&gt;&lt;/a&gt;&lt;span&gt;, an open source, SQL-based query engine, seemed like a promising option, but as we evaluated the product, we realized that it did not have critical&lt;/span&gt; &lt;span&gt;features for Uber’s use case, such as deduplication. While&lt;/span&gt; &lt;span&gt;OminiSci&lt;/span&gt; &lt;span&gt;open sourced their project in 2017, after some analysis of their C++-based solution, we concluded that neither contributing back nor forking their codebase was viable.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;GPU-based real-time analytics engines, including&lt;/span&gt; &lt;a href=&quot;https://www.cse.ust.hk/gpuqp/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;GPUQP&lt;/span&gt;&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;http://cogadb.dfki.de/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;CoGaDB&lt;/span&gt;&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;http://www.vldb.org/pvldb/vol6/p817-yuan.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;GPUDB&lt;/span&gt;&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;http://www.vldb.org/pvldb/vol6/p709-heimel.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Ocelot&lt;/span&gt;&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;http://www.vldb.org/pvldb/vol6/p1374-he.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;OmniDB&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, and&lt;/span&gt; &lt;a href=&quot;https://github.com/bakks/virginian&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Virginian&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, are frequently used by academic institutions. However, given their academic purpose, these solutions focus on developing algorithms and designing proof of concepts as opposed to handling real-world production scenarios. For this reason, we discounted them for our scope and scale.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;Overall,&lt;/span&gt; &lt;span&gt;these engines&lt;/span&gt; &lt;span&gt;demonstrate the great advantage and potential of data processing using GPU technology, and they inspired us to build our own GPU-based, real-time analytics solution tailored to Uber’s needs. With these concepts in mind, we built and open sourced AresDB.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;AresDB architecture overview&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;At a high level, AresDB stores most of its data in host memory (RAM that is connected to CPUs), handling data ingestion using CPUs and data recovery via disks. At query time, AresDB transfers data from host memory to GPU memory for parallel processing on GPU. As shown in Figure 2, below, AresDB consists of a memory store, a meta datastore, and a disk store:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image20.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5346&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image20.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;340&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image20.png 1999w, https://eng.uber.com/wp-content/uploads/2019/01/image20-300x170.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image20-768x435.png 768w, https://eng.uber.com/wp-content/uploads/2019/01/image20-1024x580.png 1024w, https://eng.uber.com/wp-content/uploads/2019/01/image20-696x394.png 696w, https://eng.uber.com/wp-content/uploads/2019/01/image20-1068x605.png 1068w, https://eng.uber.com/wp-content/uploads/2019/01/image20-741x420.png 741w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 2: The AresDB single instance architecture features memory and disk stores, and meta stores.
&lt;h4&gt;Tables&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;Unlike most&lt;/span&gt; &lt;span&gt;relational database management systems (&lt;/span&gt;&lt;span&gt;RDBMSs), there is no database or schema scope in AresDB. All tables belong to the same scope in the same AresDB cluster/instance, enabling users to refer to them directly. Users store their data as fact tables and dimension tables.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;Fact table&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;A fact table stores an infinite stream of time series events. Users use a fact table to store events/facts that are happening in real time, and each event is associated with an event time, with the table often queried by the event time. An example of the type of information stored by fact tables are trips, where each trip is an event and the trip request time is often designated as the event time. In case an event has multiple timestamps associated with it, only one timestamp is designated as the time of the event displayed in the fact table.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;Dimension table&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;A dimension table stores current properties for entities (including cities, clients, and drivers). For example, users can store city information, such as city name, time zone, and country, in a dimension table. Compared to fact tables, which grow infinitely over time, dimension tables are always bounded by size (e.g., for Uber, the cities table is bounded by the actual number of cities in the world). Dimension tables do not need a special time column.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Data types&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;Table below details the current data types supported in AresDB:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody readability=&quot;7.4910277324633&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://github.com/uber/aresdb/wiki/Data-Types&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;Data Types&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Storage (in Bytes)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Details&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;&lt;span&gt;Bool&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1/8&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Boolean type data, stored as single bit&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;Int8, Uint8&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td rowspan=&quot;3&quot;&gt;&lt;span&gt;Integer number types. User can choose based on cardinality of field and memory cost.&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Int16, Uint16&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;Int32, Uint32&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;SmallEnum&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Strings are auto translated into enums. SmallEnum can holds string type with cardinality up to 256&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;&lt;span&gt;BigEnum&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Similar to BigEnum, but holds higher cardinality up to 65532&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;Float32&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Floating point number. We support Float32 and intend to add Float64 support as needed&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.43243243243243&quot;&gt;&lt;td&gt;&lt;span&gt;UUID&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;16&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Universally_unique_identifier&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Universally unique identifier&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;GeoPoint&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;4&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Geographic points&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;GeoShape&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Variable Length&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Polygon or multi-polygons&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;With AresDB, strings are converted to&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Enumerated_type&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;enumerated types&lt;/span&gt;&lt;/a&gt; &lt;span&gt;(enums) automatically before they enter the database for better storage and query efficiency. This allows case-sensitive equality checking, but does not support advanced operations such as concatenation, substrings, globs, and regex matching. We intend to add full string support in the future.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;&lt;span&gt;Key features&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;AresDB’s architecture supports the following features:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Column-based storage with compression&lt;/strong&gt; &lt;span&gt;for storage efficiency (less memory usage in terms of bytes to store data) and query efficiency (less data transfer from CPU memory to GPU memory during querying)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Real-time upsert with primary key deduplication&lt;/strong&gt; &lt;span&gt;for&lt;/span&gt; &lt;span&gt;high data accuracy and near real-time data freshness within seconds&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPU powered query processing&lt;/strong&gt; &lt;span&gt;for highly parallelized data processing powered by GPU, rendering low query latency (sub-seconds to seconds)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Columnar storage&lt;/h4&gt;
&lt;h5&gt;Vector&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;AresDB stores all data in a columnar format. The values of each column are stored as a columnar value vector. Validity/nullness of the values in each column&lt;/span&gt; &lt;span&gt;is stored in a separate null vector, with the validity of each value represented by one bit.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;&lt;span&gt;Live store&lt;/span&gt;&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;AresDB&lt;/span&gt; &lt;span&gt;stores uncompressed and unsorted columnar data (live vectors) in a live store&lt;/span&gt;&lt;strong&gt;.&lt;/strong&gt; &lt;span&gt;Data records in a live store are partitioned into (live) batches of configured capacity. New batches are created at ingestion, while old batches are purged after records are archived. A primary key index is used to locate the records for deduplication and updates. Figure 3, below, demonstrates how we organize live records and use a primary key value to locate them:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image23.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5347 size-full&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image23.png&quot; alt=&quot;&quot; width=&quot;577&quot; height=&quot;232&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image23.png 577w, https://eng.uber.com/wp-content/uploads/2019/01/image23-300x121.png 300w&quot; sizes=&quot;(max-width: 577px) 100vw, 577px&quot;/&gt;&lt;/a&gt;Figure 3: We use a primary key value to locate the batch and position within the batch for each record.
&lt;p&gt;&lt;span&gt;The values of each column within a batch are stored as a columnar vector. Validity/nullness of the values in each value vector is stored as a separate null vector, with the validity of each value represented by one bit. In Figure 4, below, we offer an an example with five values for a&lt;/span&gt; &lt;span&gt;city_id&lt;/span&gt; &lt;span&gt;column:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5348&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;440&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771.png 1635w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-300x220.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-768x563.png 768w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-1024x751.png 1024w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-80x60.png 80w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-696x510.png 696w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-1068x783.png 1068w, https://eng.uber.com/wp-content/uploads/2019/01/image3-4-e1548709370771-573x420.png 573w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 4: We store values (actual value) and null vectors (validity) for uncompressed columns in the data table.
&lt;h5&gt;Archive store&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;AresDB also stores mature, sorted, and compressed columnar data (archive vectors) in an archive store via fact tables. Records in archive store are also partitioned into batches. Unlike live batches,&lt;/span&gt; &lt;span&gt;an&lt;/span&gt; &lt;span&gt;archive batch contains records of a particular Universal Time Coordinated (UTC) day. Archive batch uses the number of days since Unix Epoch as its batch ID.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Records are kept sorted according to a user configured column sort order. As depicted in Figure 5, below, we sort by&lt;/span&gt; &lt;span&gt;city_id&lt;/span&gt; &lt;span&gt;column first, followed by a status column:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5349&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;239&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337.png 1994w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-300x119.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-768x306.png 768w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-1024x408.png 1024w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-696x277.png 696w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-1068x425.png 1068w, https://eng.uber.com/wp-content/uploads/2019/01/image16-1-e1548709469337-1055x420.png 1055w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 5: We sort all rows by&lt;span&gt;&lt;em&gt;city_id&lt;/em&gt;&lt;/span&gt;, followed by status, then compress each column using run-length encoding. Each column will have a count vector after being sorted and compressed.
&lt;p&gt;&lt;span&gt;The goal of configuring the user-configured column sort order is to:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Maximize compression effects by sorting low cardinality columns earlier. Maximized compression increases storage efficiency (less bytes needed for storing data) and query efficiency (less bytes transferred from CPU to GPU memory).&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Allow cheap range-based prefiltering for common equi-filters such as&lt;/span&gt; &lt;span&gt;city_id=12&lt;/span&gt;&lt;span&gt;. Prefiltering enables us to minimize the bytes needed to be transferred from CPU memory to GPU memory, thereby maximizing query efficiency.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;A column is compressed only if it appears in the user-configured sort order. We do not attempt to compress high cardinality columns because the amount of storage saved by compressing high cardinality columns is negligible.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;After sorting, the data for each qualified column is compressed using a variation of run-length encoding. In addition to the value vector and null vector, we introduce the count vector to represent a repetition of the same value.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Real-time ingestion with upsert support&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;Clients ingest data through the ingestion HTTP API by posting an upsert batch. The upsert batch is a custom, serialized binary format that minimizes space overhead while still keeping the data randomly accessible.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;When AresDB receives an upsert batch for ingestion, it first writes the upsert batch to redo logs for recovery. After an upsert batch is appended to the end of the redo log, AresDB then identifies and skips late records on fact tables for ingestion into the live store. A record is considered “late” if its event time is older than the archived cut-off event time. For records not considered “late,” AresDB uses the primary key index to locate the batch within live store where they should be applied to. As depicted in Figure 6, below, brand new records (not seen before based on the primary key value) will be applied to the empty space while existing records will be updated directly:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image10-2.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5350&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image10-2.png&quot; alt=&quot;&quot; width=&quot;586&quot; height=&quot;308&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image10-2.png 586w, https://eng.uber.com/wp-content/uploads/2019/01/image10-2-300x158.png 300w&quot; sizes=&quot;(max-width: 586px) 100vw, 586px&quot;/&gt;&lt;/a&gt;Figure 6: During ingestion, after the upsert batch is appended to the redo log, “late” records will be appended to a backfill queue while other records will be applied to the live store.
&lt;h5&gt;Archiving&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;At ingestion time, records are either appended/updated in the live store or appended to a backfill queue waiting to be placed in the archive store.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;We periodically run a scheduled process, referred to as archiving, on live store records to merge the new records (records that have never been archived before) into the archive store. Archiving will only process records in the live store with their event time falling into the range of the old cut-off time (the cut-off time from last archiving process) and new cut-off time (the new cut-off time based on the archive delay setting in the table schema).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The records’ times of event will be used to determine which archive batch the records should be merged into as we batch archived data into daily batches. Archiving does not require primary key value index deduplication during merging since only records between the old cut-off and new cut-off ranges will be archived.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Figure 7, below, depicts the timeline based on the given record’s event time:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image14.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5351&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image14.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;433&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image14.png 622w, https://eng.uber.com/wp-content/uploads/2019/01/image14-300x217.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image14-324x235.png 324w, https://eng.uber.com/wp-content/uploads/2019/01/image14-582x420.png 582w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 7: We use event time and cut-off times to determine which records are new (live) and old (with an event time older than the archiving cut-off).
&lt;p&gt;&lt;span&gt;In this scenario, the archiving interval is the time between two archiving runs, while the archiving delay is the duration after the event time but before an event can be archived. Both are defined in AresDB’s table schema configurations.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;Backfill&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;As shown in Figure 7, above, old records (with event time older than the archiving cut-off) for fact tables are appended to the backfill queue and eventually handled by the backfill process. This process is also triggered by the time or size of the backfill queue onces it reaches its threshold. Compared to ingestion by the live store, backfilling is asynchronous and relatively more expensive in terms of CPU and memory resources. Backfill is used in the following scenarios:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Handling occasional very late arrivals&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Manual fixing of historical data from upstream&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Populating historical data for recently added columns&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;Unlike archiving, backfilling is idempotent and requires primary key value-based deduplication.  The data being backfilled will eventually be visible to queries.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The backfill queue is maintained in memory with a pre-configured size, and, during massive backfill loads, the client will be blocked from proceeding before the queue is cleared by a backfill run.&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Query processing&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;With the current implementation, the user will need to use&lt;/span&gt; &lt;a href=&quot;https://github.com/uber/aresdb/wiki/Ares-Query-Language&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Ares Query Language&lt;/span&gt;&lt;/a&gt; &lt;span&gt;(AQL), created by Uber to run queries against AresDB. AQL is an effective time series analytical query language and does not follow the standard SQL syntax of SELECT FROM WHERE GROUP BY like other SQL-like languages. Instead, AQL is specified in structured fields and can be carried with JSON, YAML, and Go objects. For instance, instead of&lt;/span&gt; &lt;span&gt;&lt;span&gt;SELECT count(*) FROM trips GROUP BY city_id WHERE status =&lt;/span&gt; &lt;span&gt;‘completed’&lt;/span&gt;&lt;/span&gt; &lt;span&gt;AND &lt;span&gt;request_at &amp;gt;= 1512000000&lt;/span&gt;&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;the equivalent AQL in JSON is written as:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;{&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; “table”: “trips”,&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; “dimensions”: [&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt;   {“sqlExpression”: “city_id”}&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; ],&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; “measures”: [&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt;   {“sqlExpression”: “count(*)”}&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; ],&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;;”&amp;gt; &lt;span&gt; “rowFilters”: [&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt;   “status = ‘completed'”&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt; ],&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “timeFilter”: {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;   “column”: “request_at”,&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt; &lt;span&gt;   “from”: “2 days ago”&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt; }&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In JSON-format, AQL provides better programmatic query experience than SQL for dashboard and decision system developers, because it allows them to easily compose and manipulate queries using code without worrying about issues like SQL injection. It serves as the universal query format on typical architectures from web browsers, front-end servers, and back-end servers, all the way back into the database (AresDB). In addition, AQL provides handy&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Syntactic_sugar&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;syntactic sugar&lt;/span&gt;&lt;/a&gt; &lt;span&gt;for time filtering and bucketization, with native time zone support. The language also supports features like&lt;/span&gt; &lt;span&gt;implicit sub-queries&lt;/span&gt; &lt;span&gt;to avoid common query mistakes and makes query analysis and rewriting easy for back-end developers.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Despite the various benefits AQL provides, we are fully aware that most engineers are more familiar with SQL. Exposing a SQL interface for querying is one of the next steps that we will look into to enhance the AresDB user experience.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;We depict the AQL query execution flow in Figure 8, below:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image19.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5352&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image19.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;267&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image19.png 624w, https://eng.uber.com/wp-content/uploads/2019/01/image19-300x134.png 300w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 8: AresDB’s query execution flow leverages our homegrown AQL query language for fast, efficient data processing and retrieval.
&lt;h5&gt;Query compilation&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;An AQL query is compiled into&lt;/span&gt; &lt;span&gt;internal query context&lt;/span&gt;&lt;strong&gt;.&lt;/strong&gt; &lt;span&gt;Expressions in filters, dimensions, and measurements are parsed into abstract syntax trees (AST) for later processing via GPU.&lt;/span&gt;&lt;/p&gt;
&lt;h5&gt;Data feeding&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;AresDB&lt;/span&gt; &lt;span&gt;utilizes pre-filters&lt;/span&gt; &lt;span&gt;to cheaply filter archived data before sending them to a GPU for parallel processing. Since archived data is sorted according to a configured column order, some filters may be able to utilize this sorted order by applying binary search to locate the corresponding matching range. In particular, equi-filters on all of the first X-sorted columns and optionally range filter on sorted X+1 columns can be processed as pre-filters, as depicted in Figure 9, below:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image13-2.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5353&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image13-2.png&quot; alt=&quot;&quot; width=&quot;566&quot; height=&quot;413&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image13-2.png 566w, https://eng.uber.com/wp-content/uploads/2019/01/image13-2-300x219.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image13-2-324x235.png 324w&quot; sizes=&quot;(max-width: 566px) 100vw, 566px&quot;/&gt;&lt;/a&gt;Figure 9: AresDB pre-filters columnar data before sending it to the GPU for processing.
&lt;p&gt;&lt;span&gt;After prefiltering, only the green values (satisfying filter condition) need to be pushed to the GPU for parallel processing.&lt;/span&gt; &lt;span&gt;Input data is fed to the GPU and executed there one batch at a time. This includes both live batches and archive batches.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AresDB utilizes&lt;/span&gt; &lt;a href=&quot;https://devblogs.nvidia.com/gpu-pro-tip-cuda-7-streams-simplify-concurrency/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;CUDA streams&lt;/span&gt;&lt;/a&gt; &lt;span&gt;for pipelined data feeding and execution. Two streams are used alternately on each query for processing in two overlapping stages. In Figure 10, below, we offer a timeline illustration of this process:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image8-2.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5354&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image8-2.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;128&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image8-2.png 622w, https://eng.uber.com/wp-content/uploads/2019/01/image8-2-300x64.png 300w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 10: With AresDB, two CUDA streams alternate on data transfer and processing.
&lt;h5&gt;Query execution&lt;/h5&gt;
&lt;p&gt;&lt;span&gt;For simplicity, AresDB utilizes the&lt;/span&gt; &lt;a href=&quot;https://developer.nvidia.com/thrust&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Thrust library&lt;/span&gt;&lt;/a&gt; &lt;span&gt;to implement query execution procedures, which offers fine-tuned parallel algorithm building blocks for quick implementation in the current query engine.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In Thrust, input and output vector data is accessed using random access iterators. Each GPU thread seeks the input iterators to its workload position, reads the values and performs the computation, and then writes the result to the corresponding position on the output iterator.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AresDB follows the one-operator-per-kernel (OOPK) model for evaluating expressions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Figure 11, below, demonstrates this procedure on an example AST, generated from a dimension expression&lt;/span&gt; &lt;span&gt;&lt;span&gt;request_at – request_at % 8640&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;&lt;/span&gt; &lt;span&gt;in the query compilation stage:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image7-1.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5355 size-full&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image7-1.png&quot; alt=&quot;&quot; width=&quot;368&quot; height=&quot;333&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image7-1.png 368w, https://eng.uber.com/wp-content/uploads/2019/01/image7-1-300x271.png 300w&quot; sizes=&quot;(max-width: 368px) 100vw, 368px&quot;/&gt;&lt;/a&gt;Figure 11: AresDB leverages the OOPK model model for expression evaluation.
&lt;p&gt;&lt;span&gt;In the OOPK model, the AresDB query engine traverses each leaf node of the AST tree and returns an iterator for its parent node. In cases where the root node is also a leaf, the root action is taken directly on the input iterator.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;At each non-root non-leaf node (&lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Modulo_operation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;modulo operation&lt;/span&gt;&lt;/a&gt; &lt;span&gt;in this example), a temporary scratch space vector is allocated to store the intermediate result produced from&lt;/span&gt; &lt;span&gt;request_at % 86400&lt;/span&gt; &lt;span&gt;expression. Leveraging Thrust, a kernel function is launched to compute the output for this operator on GPU. The results are stored in the scratch space iterator.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;At the root node, a kernel function is launched in the same manner as a non-root, non-leaf node. Different output actions are taken based on the expression type, detailed below:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Filter action to reduce the cardinality of input vectors&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Write dimension output to the dimension vector for later aggregation&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Write measure output to the measure vector for later aggregation&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;After expression evaluation,&lt;/span&gt; &lt;a href=&quot;https://thrust.github.io/doc/group__sorting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;sorting&lt;/span&gt;&lt;/a&gt; &lt;span&gt;and&lt;/span&gt; &lt;a href=&quot;https://thrust.github.io/doc/group__reductions.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;reduction&lt;/span&gt;&lt;/a&gt; &lt;span&gt;are executed to conduct final aggregation. In both sorting and reduction operations, we use the values of the dimension vector as the key values of sorting and reduction, and the values of the measure vector as the values to aggregate on. In this way, rows with same dimension values will be grouped together and aggregated. Figure 12, below, depicts this sorting and reduction process:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image22.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5356&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image22.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;233&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image22.png 624w, https://eng.uber.com/wp-content/uploads/2019/01/image22-300x116.png 300w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 12: After expression evaluation, AresDB sorts and reduces data by key value on the dimension (key value) and measure (value) vectors.
&lt;p&gt;&lt;span&gt;AresDB also supports the following advanced query features:&lt;/span&gt;&lt;/p&gt;
&lt;h4&gt;Resource management&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;As an in-memory-based database, AresDB needs to manage the following types of memory usage:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody readability=&quot;8.5&quot;&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td&gt;&lt;strong&gt;Allocation&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Management Mode&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;Live Store Vectors (live store columnar data)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;C&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Tracked&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;&lt;span&gt;Archive Store Vectors (archive store columnar data)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;C&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Managed (Load and eviction)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;Primary Key Index (hash table for record deduplication)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;C&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Tracked&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;span&gt;Backfill Queue (store “late” arrival data waiting for backfill)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Golang&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Tracked&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;&lt;span&gt;Archive / Backfill Process Temporary Storage (Temporary memory allocated during the Archive and Backfill process)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;C&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Tracked&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td readability=&quot;5&quot;&gt;&lt;span&gt;Ingestion / Query Temporary Storage;&lt;/span&gt;
&lt;p&gt;&lt;span&gt;Process Overheads;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Allocation fragmentations&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Golang and C&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;Statically Configured Estimate&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;When AresDB goes into production, it leverages a configured total memory budget. This budget is shared by all six memory types and should also leave enough room for the operating system and other processes. This budget also covers a statically configured overhead estimation, live data storage monitored by the server, and archived data that the server can decide to load and evict depending on the remaining memory budget.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Figure 13, below, depicts the AresDB host memory model:&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image12-2.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5357&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image12-2.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;273&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image12-2.png 617w, https://eng.uber.com/wp-content/uploads/2019/01/image12-2-300x137.png 300w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 13: AresDB manages its own memory usage so that it does not exceed the configured total process budget.
&lt;p&gt;&lt;span&gt;AresDB allows users to configure pre-loading days and priority at the column level for fact tables, and only pre-loads archive data within pre-loading days. Non-preloaded data is loaded into memory from disk on demand. Once full, AresDB also evicts archived data from the host memory. AresDB’s eviction policies are based on the number of preloading days, column priorities, the day of the batch, and the column size.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AresDB also manages multiple GPU devices and models device resources as GPU threads and device memory, tracking GPU memory usage as processing queries. AresDB manages GPU devices through device manager, which models GPU device resources in two dimensions–GPU threads and device memory–and tracks the usage while processing queries. After query compilation, AresDB enables users to estimate the amount of resources needed to execute the query. Device memory requirements must be satisfied before a query is allowed to start; the query must wait to run if there is not enough memory at that moment on any device. Currently, AresDB can run either one or several queries on the same GPU device simultaneously, so long as the device satisfies all resource requirements.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In the current implementation, AresDB does not cache input data in device memory for reuse across multiple queries. AresDB targets supporting queries on datasets that are constantly updated in real time and hard to cache correctly. We intend to implement a data caching functionality GPU memory in future iterations of AresDB, a step that will help optimize query performance.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;Use Case: Uber’s Summary Dashboard&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;At Uber, we use AresDB to build dashboards for extracting real-time business insights. AresDB plays the role of storing fresh raw events with constant updates and computing crucial metrics  against them in sub seconds using GPU power with low cost so that users can utilize the dashboards interactively. For example, anonymized trip data, which has a long lifespan in the datastore, is updated by multiple services, including our dispatch, payments, and ratings systems. To utilize trips data effectively, users will slice and dice the data into different dimensions to get insights for real-time decisions.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Leveraging AresDB, Uber’s Summary Dashboard is a widely used analytics dashboard leveraged by teams across the company to retrieve relevant product metrics and respond in real time to improve user experience.&lt;/span&gt;&lt;/p&gt;
&lt;a href=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image18.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-5358&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image18.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;293&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2019/01/image18.png 624w, https://eng.uber.com/wp-content/uploads/2019/01/image18-300x147.png 300w, https://eng.uber.com/wp-content/uploads/2019/01/image18-533x261.png 533w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;Figure 14: The Uber Summary Dashboard’s hourly view uses AresDB to view real-time data analytics during specific time periods.
&lt;p&gt;&lt;span&gt;To build the mock-up dashboard, above, we modeled the following tables:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trips (fact table)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;trip_id&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;request_at&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;city_id&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;status&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;driver_id&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;fare&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1542058870&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;completed&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;8.5&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1541977200&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;rejected&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;3&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;10.75&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;…&lt;/span&gt;&lt;/td&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Cities (dimension table)&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;city_id&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;city_name&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;timezone&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;1&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;San Francisco&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;America/Los_Angeles&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;2&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;New York&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;America/New_York&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;span&gt;…&lt;/span&gt;&lt;/td&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table schemas in AresDB&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;To create the two modeled tables described above, we will first need to create the tables in AresDB in the following schemas:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody readability=&quot;22.5&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Trips&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Cities&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;45&quot;&gt;&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “name”: “trips”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “columns”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “request_at”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “Uint32”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “trip_id”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “UUID”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “city_id”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “Uint16”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “status”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “SmallEnum”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “driver_id”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “UUID”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “fare”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “Float32”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “primaryKeyColumns”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   1&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “isFactTable”: true,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “config”: {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “batchSize”: 2097152,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “archivingDelayMinutes”: 1440,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “archivingIntervalMinutes”: 180,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;  “recordRetentionInDays”: 30&lt;/span&gt;&lt;br/&gt;&lt;span&gt; },&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “archivingSortColumns”: [2,3]&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “name”: “cities”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “columns”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt; {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “city_id”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;    “type”: “Uint16”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “city_name”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “SmallEnum”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   },&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “timezone”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “type”: “SmallEnum”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “primaryKeyColumns”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   0&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “isFactTable”: false,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “config”: {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “batchSize”: 2097152&lt;/span&gt;&lt;br/&gt;&lt;span&gt; }&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;As described in schema, trips tables are created as fact tables, representing trips events that are happening in real time, while cities tables are created as dimension tables, storing information about actual cities.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;After tables are created, users may leverage the&lt;/span&gt; &lt;a href=&quot;https://github.com/uber/aresdb/blob/master/client/connector.go#L48&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;AresDB client library&lt;/span&gt;&lt;/a&gt; &lt;span&gt;to ingest data from an event bus such as Apache&lt;/span&gt; &lt;a href=&quot;https://kafka.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Kafka&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, or streaming or batch processing platforms such as Apache&lt;/span&gt; &lt;a href=&quot;https://flink.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Flink&lt;/span&gt;&lt;/a&gt; &lt;span&gt;or Apache&lt;/span&gt; &lt;a href=&quot;https://spark.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Spark&lt;/span&gt;&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample queries against AresDB&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In the mock-up dashboards, we choose two metrics as examples, total trip fare and active drivers. In the dashboard, users can filter the city for the metrics, eg. San Francisco. To draw the time series for these two metrics for the last 24 hours shown in the dashboards, we can run the following queries in AQL:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody readability=&quot;18&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;&lt;strong&gt;Total trips fare in San Francisco in the last 24 hours group by hours&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Active drivers in San Francisco in the last 24 hours group by hours&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;32&quot;&gt;&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “table”: “trips”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “joins”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “alias”: “cities”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “cities”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “conditions”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;       “cities.id = trips.city_id”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     ]&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “dimensions”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “sqlExpression”: “request_at”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “timeBucketizer”: “hour”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “measures”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “sqlExpression”: “sum(fare)”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “rowFilters”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “status = ‘completed'”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “cities.city_name = ‘San Francisco'”&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “timeFilter”: {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “column”: “request_at”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “from”: “24 hours ago”&lt;/span&gt;&lt;br/&gt;&lt;span&gt; },&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “timezone”: “America/Los_Angeles”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “table”: “trips”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “joins”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “alias”: “cities”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “name”: “cities”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “conditions”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;       “cities.id = trips.city_id”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     ]&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “dimensions”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “sqlExpression”: “request_at”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “timeBucketizer”: “hour”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “measures”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “sqlExpression”: “countDistinctHLL(driver_id)”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “rowFilters”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “status = ‘completed'”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “cities.city_name = ‘San Francisco'”&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ],&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “timeFilter”: {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “column”: “request_at”,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   “from”: “24 hours ago”&lt;/span&gt;&lt;br/&gt;&lt;span&gt; },&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “timezone”: “America/Los_Angeles”&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Sample results from the query:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The above mock-up queries will produce results in the following time series result, which can be easily drawn into time-series graphs, as shown below:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tbody readability=&quot;11&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;&lt;strong&gt;Total trips fare in San Francisco in the last 24 hours group by hours&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Active drivers in San Francisco in the last 24 hours group by hours&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;18&quot;&gt;&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “results”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547060400”: 1000.0,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547064000”: 1000.0,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547067600”: 1000.0,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547071200”: 1000.0,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547074800”: 1000.0,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     …&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ]&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;&lt;span&gt;{&lt;/span&gt;&lt;br/&gt;&lt;span&gt; “results”: [&lt;/span&gt;&lt;br/&gt;&lt;span&gt;   {&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547060400”: 100,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547064000”: 100,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547067600”: 100,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547071200”: 100,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;     “1547074800”: 100,&lt;/span&gt;&lt;br/&gt;&lt;span&gt;    …  &lt;/span&gt;&lt;br/&gt;&lt;span&gt;   }&lt;/span&gt;&lt;br/&gt;&lt;span&gt; ]&lt;/span&gt;&lt;br/&gt;&lt;span&gt;}&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;In the above example, we demonstrated how to leverage AresDB to ingest raw events happening in real-time within seconds and issue arbitrary user queries against the data right away to compute metrics in sub seconds. AresDB helps engineers to easily build data products that extract metrics crucial to businesses that requires real-time insights for human or machine decisions.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;Next steps&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;AresDB is widely used at Uber to power our real-time data analytics dashboards, enabling us to make data-driven decisions at scale about myriad aspects of our business. By open sourcing this tool, we hope others in the community can leverage AresDB for their own analytics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In the future, we intend to enhance the project with the following features:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Distributed design: &lt;span&gt;We are working on building out the distributed design of AresDB, including replication, sharding management, and schema management to improve its scalability and reduce operational costs.&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Developer support and tooling:&lt;/strong&gt; &lt;span&gt;Since open sourcing AresDB in November 2018, we have been working on building more intuitive tooling, refactoring code structures, and enriching documentation to improve the onboarding experience, enabling developers to quickly integrate AresDB to their analytics stack.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Expanding feature set:&lt;/strong&gt; &lt;span&gt;We also plan to expand our query feature set to include functionality such as window functions and nested loop joins, thereby allowing the tool to support more use cases.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Query engine optimization&lt;/strong&gt;&lt;span&gt;: We will also be looking into developing more advanced ways to optimize query performance, such as&lt;/span&gt; &lt;a href=&quot;https://llvm.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Low Level Virtual Machine (LLVM)&lt;/span&gt;&lt;/a&gt; &lt;span&gt;and GPU memory caching.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span&gt;AresDB is&lt;/span&gt; &lt;a href=&quot;https://github.com/uber/aresdb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;open sourced&lt;/span&gt;&lt;/a&gt; &lt;span&gt;under the Apache License. We encourage you to try out AresDB and join our community.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;span&gt;If building large-scale, real-time data analytics technologies interests you, consider applying for a role on our team.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Acknowledgements&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;A huge thanks to the rest of Uber Real-time Streaming Analytics team: Shengyue Ji, Xiang Fu, David Chen, and Li Ning.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



</description>
<pubDate>Tue, 29 Jan 2019 18:49:07 +0000</pubDate>
<dc:creator>manojlds</dc:creator>
<og:image>https://eng.uber.com/wp-content/uploads/2019/01/Featured-1.png</og:image>
<og:type>article</og:type>
<og:title>Introducing AresDB: Uber’s GPU-Powered Open Source, Real-time Analytics Engine</og:title>
<og:description>AresDB, Uber's open source real-time analytics engine, leverages GPUs to enable real-time computation and data processing in parallel.</og:description>
<og:url>https://eng.uber.com/aresdb/</og:url>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://eng.uber.com/aresdb/</dc:identifier>
</item>
<item>
<title>YC 120</title>
<link>https://blog.ycombinator.com/yc-120/</link>
<guid isPermaLink="true" >https://blog.ycombinator.com/yc-120/</guid>
<description>&lt;div class=&quot;post-content&quot; readability=&quot;68.601386481802&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1.png&quot;&gt;&lt;img src=&quot;https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1-300x300.png&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;300&quot; class=&quot;aligncenter size-medium wp-image-1103414&quot; srcset=&quot;https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1-300x300.png 300w, https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1-150x150.png 150w, https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1-768x768.png 768w, https://blog.ycombinator.com/wp-content/uploads/2019/01/yc120-v1.png 1000w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The world is shaped by people with intelligence, drive and vision. At YC, we’ve learned a lot about cultivating a network of such people — and we believe that having a strong network is such a force multiplier that it is one of the most important assets for anyone who wants to have a significant impact on the world.&lt;/p&gt;
&lt;p&gt;We’d like to find more curious, creative people who are doing exciting work in emerging fields and give them an opportunity to start building their network. We want to connect them to each other, to people who have already done impactful work, and to us.&lt;/p&gt;
&lt;p&gt;We’d like to bring 120 of those people together for a weekend conference in Colorado from Friday, April 26 to Sunday, April 28.&lt;/p&gt;
&lt;p&gt;Are you interested in gene editing? Using technology to improve democracy and a return to fact-based debate? Finally solving physics? Building AGI? Nuclear fusion? Building a space colony? We’d like to hear from you.&lt;/p&gt;
&lt;p&gt;We’ll invite about 100 people who could use more of a network, and 20 people who are already at the top of their fields. We’ll pay for a plane ticket from anywhere in the world, lodging, and food. To continue to build the community, there will also be a few optional follow-up dinners throughout the course of the year.&lt;/p&gt;
&lt;p&gt;Aside from some basic information, all you need to do to apply is submit a one-minute video with answers to these three questions:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;What are you interested in and what are you working on?&lt;/li&gt;
&lt;li&gt;What have you done so far that shows your potential for greatness, adjusted for whatever life circumstances you were born into?&lt;/li&gt;
&lt;li&gt;In a best-case scenario, what do you want your obituary to say?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Applications are now open. You can &lt;a href=&quot;https://apply.ycombinator.com/events/200/event_app/new&quot;&gt;apply here&lt;/a&gt;. The application deadline is Feb 18th at 11:59 PM PST.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;mc_embed_signup&quot;&gt;
&lt;h3 id=&quot;mc_copy&quot;&gt;Sign up for weekly updates from Y Combinator.&lt;/h3&gt;

&lt;/div&gt;
</description>
<pubDate>Tue, 29 Jan 2019 17:03:59 +0000</pubDate>
<dc:creator>jameshk</dc:creator>
<og:title>YC 120</og:title>
<og:url>https://blog.ycombinator.com/yc-120/</og:url>
<og:type>article</og:type>
<og:description>We’d like to find more curious, creative people who are doing exciting work in emerging fields and give them an opportunity to start building their network. We’ll bring 120 of those people together for a weekend conference in Colorado from Friday, April 26 to Sunday, April 28.</og:description>
<og:image>https://blog.ycombinator.com/wp-content/uploads/2019/01/YC-120.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.ycombinator.com/yc-120/</dc:identifier>
</item>
<item>
<title>The Man Who Invented Information Theory (2017)</title>
<link>http://bostonreview.net/science-nature/tom-rutledge-man-who-invented-information-theory</link>
<guid isPermaLink="true" >http://bostonreview.net/science-nature/tom-rutledge-man-who-invented-information-theory</guid>
<description>&lt;div class=&quot;detail-secondary-mobile&quot; id=&quot;detail-secondary-mobile&quot;&gt;
&lt;div class=&quot;detail-secondary-mobile-topper&quot;&gt;
&lt;div class=&quot;detail-date&quot;&gt;
&lt;p&gt;Aug 16, 2017&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;detail-readtime&quot;&gt;
&lt;p&gt;16 Min read time&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;
&lt;p class=&quot;caption caption_first&quot;&gt;&lt;em&gt;Image: &lt;a href=&quot;https://www.flickr.com/photos/tekniskamuseet/6832884236/&quot;&gt;Tekniska Museet&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;detailsummary&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Of the pioneers who drove the information technology revolution, Claude Shannon may have been the most brilliant. A new book resurrects his legacy.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In a video from the early 1950s, Bell Labs scientist Claude Shannon demonstrates one of his new inventions: a toy mouse named Theseus that looks like it could be a wind-up. The gaunt Shannon, looking a bit like Gary Cooper, stands next to a handsomely crafted tabletop maze and explains that Theseus (which Shannon pronounces with two syllables: “THEE-soose”) has been built to solve the maze. Through trial and error, the mouse finds a series of unimpeded openings and records the successful route. On its second attempt, Theseus follows the right path, error-free from start to finish.&lt;/p&gt;
&lt;p&gt;Shannon then unveils the secret to Theseus’s success: a dense array of electrical relays, sourced from the Bell System’s trove of phone-switching hardware. It is the 1950s equivalent of a computer chip, but it’s about a thousand times bigger and only a millionth as powerful as today’s hardware.&lt;/p&gt;
&lt;p&gt;Claude Shannon's achievements were at the level of an Einstein or a Feynman, but he has not achieved commensurate fame.&lt;/p&gt;
&lt;p&gt;While some scientists and engineers may have recognized Theseus as something important—a tidy and clever example of a thinking machine—many in Shannon’s audience probably dismissed the contraption as a fancy wind-up toy, or maybe a fraudulent automaton in the tradition of the &lt;a href=&quot;http://www.atlasobscura.com/articles/object-of-intrigue-the-turk&quot;&gt;chess-playing Turk&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But the intellect behind Theseus was prodigious. Of the computer pioneers who drove the mid-20&lt;sup&gt;th&lt;/sup&gt;-century information technology revolution—an elite men’s club of scholar-engineers who also helped &lt;a href=&quot;http://www.newyorker.com/magazine/2006/02/06/code-breaker&quot;&gt;crack Nazi codes&lt;/a&gt; and pinpoint missile trajectories—Shannon may have been the most brilliant of them all. His achievements were at the level of an Einstein or a Feynman, but Shannon has not achieved commensurate fame. It’s possible his playful tinkering caused some to write him off as unserious. But it’s also possible that his greatest work seemed unapproachable to most.&lt;/p&gt;
&lt;p&gt;Shannon’s seminal work was profoundly abstract.  As the “father of information theory,” he took the bold step of divorcing information from meaning, conceiving of messages as just collections of bits, devoid of an explicit connection to the world. In many ways his work is not only counterintuitive, but dismal and remote.&lt;/p&gt;
&lt;p&gt;A new Shannon biography, &lt;em&gt;A Mind at Play: How Claude Shannon Invented the Information Age,&lt;/em&gt; may help reverse this legacy. Authors Jimmy Soni and Rob Goodman make a strong bid to expose Shannon’s work to a popular audience, balancing a chronological narrative, the “Eureka!” moments that sprang from his disciplined approach to solving puzzles, and his propensity for playfulness. The book begins, for example, in the 1920s, when a young Shannon electrified the fence around his small-town Michigan home to transform it into a telegraph wire. By the end of the century, we find Shannon playing tour guide to a parade of MIT students at his suburban Boston home, &lt;a href=&quot;http://spectrum.ieee.org/computing/software/claude-shannon-tinkerer-prankster-and-father-of-information-theory&quot;&gt;a virtual museum of homemade gadgets and toys&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In between, Soni and Goodman sink their teeth into Shannon’s two greatest achievements. First, Shannon is likely the reason that almost all computers today are digital. In the 1930s, computer pioneers were essentially using watchmaking techniques to refine the wheels and cogs of hulking analog difference engines—“one of engineering’s long, blind alleys,” in Soni and Goodman’s words. Shannon set computer science definitively on the digital track with what is often called the most influential Master’s thesis ever.&lt;/p&gt;
&lt;p&gt;His &lt;a href=&quot;http://dspace.mit.edu/handle/1721.1/11173#files-area&quot;&gt;1937 MIT thesis&lt;/a&gt;, completed at age 21, demonstrated that the on-off switches of digital devices could be represented with the true-false notation developed eighty years prior by the English logician George Boole. Shannon single-handedly imported Boolean algebra into the task of electronic circuit design, radically streamlining the process and sealing off the blind alley of analog design once and for all.&lt;/p&gt;
&lt;p&gt;Shannon took the bold step of divorcing information from meaning, conceiving of messages as just collections of bits, devoid of an explicit connection to the world.&lt;/p&gt;
&lt;p&gt;Shannon may also be the reason that modern communication has progressed from the fritzy TV pictures of the 1950s to a civilization saturated with high-speed and ubiquitous multimedia data. Shannon’s crowning academic achievement, his 1948 publication of &lt;a href=&quot;http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf&quot;&gt;“A Mathematical Theory of Communication,”&lt;/a&gt; immediately found an audience among engineers looking to send messages faster and with greater fidelity. The paper’s deep analysis of messages—their information content, how that content can be converted to signals sent through a communication channel and then received intact at the end—provided the principles and lexicon for the transmission of all kinds of information, everywhere. Even if the names of the technologies (e.g. data compression, channel optimization, and noise reduction) mean nothing to you, you rely on them when you make a phone call, binge-watch a Netflix series, or tweet.&lt;/p&gt;
&lt;p&gt;But while information theory’s offspring have been plentiful, its pure form has no obvious calling card. It’s too intangible; the essence of information theory is in its least practical aspects. As such, it is the perfect expression of Shannon’s gift for abstraction. Soni and Goodman write that Shannon “had a way of getting behind things. He loved the objects under his hands, right up to the point when he abstracted his way past them.”&lt;/p&gt;
&lt;p&gt;• • •&lt;/p&gt;
&lt;p&gt;As Shannon began work on information theory, he faced mid-century problems: making and breaking codes, sending messages intact over long distances through wires and through the air, and building a common phone network that could connect anyone to anyone. At the time, Soni and Goodman write, “Information was a presence offstage.” Shannon’s goal was to unite the many disparate problems of information under one comprehensive solution.&lt;/p&gt;
&lt;p&gt;He delivered in 1948, after a solid decade of solitary “tooling” (in the parlance of MIT-bred engineers). His math had expanded to a consistent and complete system, applicable to every form of message transmitted through every possible communication channel. Shannon’s achievement in information theory was comparable to what Euclid’s &lt;em&gt;Elements&lt;/em&gt; had done for geometry.&lt;/p&gt;
&lt;p&gt;To make a rigorous and technical solution possible, Shannon constrained the problem. At the outset, he declared that meaning was “irrelevant to the engineering problem.” It would simply be too hard to evaluate successful transmission while taking into account all the “correlated. . . physical or conceptual entities” that make up a message’s meaning. He reduced the act of sending a message to selecting among finite possibilities and replicating the selection at the other end. This made accuracy measurable—simply compare the received message to the original.&lt;/p&gt;
&lt;p&gt;Information theory corresponds elegantly to the human elements of communication.&lt;/p&gt;
&lt;p&gt;For his working example, Shannon chose the English language. By using English, Shannon was able to appeal to readers’ intuition about what was sensible and what was nonsense. Although sense and nonsense were irrelevant under his “no meaning” stipulation, they were good intuitive proxies for accurate/inaccurate. The choice of English also opened up the language’s entire history as a literary database for both analytical and empirical information about how letters were used—their frequencies, as well as the patterns and frequencies of word combinations. Those statistics were an essential part of his model.&lt;/p&gt;
&lt;p&gt;Shannon needed an atomic unit of information, so he created one. Harkening back to Boole, Shannon reduced letters, images and sounds to bits—strings of ones and zeros. Once a message was reduced to bits, the mathematical relationships began to emerge. A message delivered in text could be measured for its contribution to the recipient’s existing knowledge—or, put differently, its ability to resolve uncertainty. In information theory, that’s “information.”&lt;/p&gt;
&lt;p&gt;Following Euclid’s model of axioms and postulates, Shannon went about the task of defining information theory's elements and their roles in the system. For example, &quot;redundancy&quot;—predictable or even repetitive strings of bits—could either be dead weight in a message or counterweight to a garbled message unintelligible without repetition. Soni and Goodman relate how early transatlantic telegraphy, due to the distortion created by primitive underwater cables, often devolved into either long sequences of the same word over again, or requests for more redundancy—“Repeat, please.” (The authors describe a scene of “communication about communication, telegraphy as an especially bleak Samuel Beckett play.”)&lt;/p&gt;
&lt;p&gt;Shannon then observed that this complex system, dynamic and variable but governed by parameters, could be characterized as a Markov process. In a nutshell, Markov processes are random, but their patterns depend on their current state. For example, the next letter in an English sentence is random but depends on the current letter—“u” is very likely after “q.” This observation availed communication of the rich analytical toolkit already in use with Markov processes and present in real-world phenomena, such as &lt;a href=&quot;https://www.researchgate.net/profile/Charles_Nelson8/publication/222463053_A_Markov_Model_of_Heteroskedasticity_Risk_and_Learning_in_the_Stock_Market/links/02e7e5182f0b5534b7000000/A-Markov-Model-of-Heteroskedasticity-Risk-and-Learning-in-the-Stoc&quot;&gt;stock price movements&lt;/a&gt;, population growth, and queues for ice cream.&lt;/p&gt;
&lt;p&gt;• • •&lt;/p&gt;
&lt;p&gt;Just as measures of length, area, and volume are fundamental to Euclid’s geometry, the actual measure of information in a message was an essential building block for information theory. The units (bits) were defined, but how would Shannon go about determining the total bits in a message? Making a leap reminiscent of his Boolean insight, Shannon imported a concept from thermodynamics. In information theory, he argued, the amount of information in a message is its “entropy.”&lt;/p&gt;
&lt;p&gt;In thermodynamics, the attributes of a system (such as temperature, volume, and energy) define its state. We may know a thermodynamic state has all of the above attributes but not know their values. Similarly, we may know a message uses a certain number of letters but not know which ones. In both cases, entropy measures the expected value of that state (or message). A very attribute-rich thermodynamic state and a very elaborate message both have high entropy.&lt;/p&gt;
&lt;p&gt;If you want to see the tools of information theory at work, consider this:&lt;/p&gt;
&lt;p&gt;The Kindle e-book of Mario Puzo’s &lt;em&gt;The Godfather&lt;/em&gt; is about a million bytes. I downloaded a 35,000-byte picture of Marlon Brando as Vito Corleone. So, since the book is about 172,000 words, or 7.5 bytes per word, that makes the picture of Vito worth almost 5,000 words.&lt;/p&gt;
&lt;p&gt;For the record, the Word file of this review is about 35,000 bytes, the same size as the Vito Corleone photo. (When you’re writing about communication, the air grows thick with &lt;em&gt;meta&lt;/em&gt; very quickly.)&lt;/p&gt;
&lt;p&gt;Turning away from meaning was a pragmatic decision that led to Shannon’s greatest triumph. It was also a kind of trick.&lt;/p&gt;
&lt;p&gt;Having built a construct as complete and powerful as information theory, Shannon gently made a point of noting the barriers to replicating his act. The “no meaning” caveat, in addition to making the math work, also challenged those following Shannon to match his standards of analytical rigor—and generally, to not overthink things.&lt;/p&gt;
&lt;p&gt;Many did not get the message. After the release of Shannon’s 1948 paper, sojourners from all disciplines projected their own problems onto the blank canvas of information theory. (Soni and Goodman sum up the public’s overly effusive response to information theory in a chapter titled “TMI.”) Perhaps this was to be expected; in some respects, every scholar traffics in “information.” And despite his caveats, Shannon left the door ajar for cross-pollination by reaching into thermodynamics for the framework of entropy. But when interlopers didn’t take his cues about the necessity of rigor, he spelled it out for them.&lt;/p&gt;
&lt;p&gt;In his 1956 essay “The Bandwagon,” Shannon wrote, &quot;The establishing of [new] applications is not a trivial matter of translating words to a new domain, but rather the slow tedious process of hypothesis and experimental verification.&quot;&lt;/p&gt;
&lt;p&gt;Shannon rejected most of the new applications of information theory, but there was one exception. In the 1950s, he advised John L. Kelly Jr.—a younger member of the Bell Labs-MIT men’s club—on a paper linking information theory to gambling. Kelly noticed the mathematical similarities between the processes of deciding how much to bet on a given risk and determining the amount of information that can be successfully transmitted over a noisy channel. The paper laid out what is now known in finance theory as the Kelly Criterion, a rule for allocating capital to risky propositions, &lt;a href=&quot;https://www.amazon.com/Fortunes-Formula-Scientific-Betting-Casinos/dp/0809045990&quot;&gt;whether at the blackjack table or in the stock market&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;• • •&lt;/p&gt;
&lt;p&gt;In the interest of keeping their narrative manageable and centered on its subject, Soni and Goodman limit discussions of information theory’s extensions. But entropy and Kelly provide seductive examples, and it’s worth letting out the leash a bit to see where the extensions might run.&lt;/p&gt;
&lt;p&gt;Information scholars are currently following the lead of physics, exploring beyond classical assumptions about the state of a communication system and incorporating concepts from quantum mechanics. Instead of using bits that resolve to one of two binary states, quantum information processing considers the possibility of information having multiple states that can be superposed. (The resulting quantum information measure is known as a “qubit.”) “Quantum Shannon theory” &lt;a href=&quot;https://books.google.com/books/about/Quantum_Information_Theory.html?id=T36v2Sp7DnIC&amp;amp;printsec=frontcover&amp;amp;source=kp_read_button#v=onepage&amp;amp;q&amp;amp;f=false&quot;&gt;posits the existence of efficiencies&lt;/a&gt; (such as data compression or noise reduction techniques) that could apply to processes in the quantum world.&lt;/p&gt;
&lt;p&gt;Shannon had a way of getting behind things. He loved the objects under his hands, right up to the point when he abstracted his way past them.&lt;/p&gt;
&lt;p&gt;Are there other phenomena that could be understood and described using the template provided by information theory?&lt;/p&gt;
&lt;p&gt;Maybe. In 1961, the Cambridge physiologist H. B. Barlow wrote a paper on how nervous systems may have evolved to encode and deliver messages with maximum efficiency through an organism’s nervous system. Barlow termed his model “the efficient coding hypothesis,” evoking Eugene Fama’s “efficient-market hypothesis.”&lt;/p&gt;
&lt;p&gt;Fama’s work gave the world an analytical toolkit and lexicon for financial market risk and return. The parallels between information and economics identified by Kelly open the question of whether the concepts should complete the round-trip from Shannon to Kelly and back to Shannon. The information theory equivalents of financial concepts like risk, return, volatility, and the &lt;a href=&quot;https://www.wsj.com/articles/SB10001424052702304692804577283773166995992&quot;&gt;Sharpe Ratio&lt;/a&gt; could offer insight and discipline to some forms of communication.&lt;/p&gt;
&lt;p&gt;Turning away from meaning was a pragmatic decision that led to Shannon’s greatest triumph. At the same time, it was a kind of trick, a litigator’s crafty courtroom maneuver. Shannon approached the bench at the beginning of the trial and had the judge declare any references to “meaning” inadmissible. This enabled him to win the case at hand—the engineering problem—but left unfulfilled the promise implied by the name “information theory.”&lt;/p&gt;
&lt;p&gt;Without meaning, information theory can solve the engineering problem—but only the engineering problem. Granting engineering problems primacy in questions of meaning, however, seems like a hasty, ignominious surrender. We should at least wait to size up our coming mechanical overlords by taking stock of how current advances in artificial intelligence and machine learning play out. Soni and Goodman write that the rejection of meaning “formalized an intuition wired into the phone company—which was, after all, in the business of transmission, not interpretation.” It’s hard to avoid grim images of pale kings locked in bureaucracies executing menial tasks.&lt;/p&gt;
&lt;p&gt;Shannon clearly did not envision that kind of future for information theory. Smack in the middle of his 1948 paper, in a statement that almost serves as a giant asterisk, Shannon turned to James Joyce:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;Two extremes of redundancy in English prose are represented by Basic English and by James Joyce’s book &lt;em&gt;Finnegans Wake&lt;/em&gt;. The Basic English vocabulary is limited to 850 words and the redundancy is very high. This is reflected in the expansion that occurs when a passage is translated into Basic English. Joyce on the other hand enlarges the vocabulary and is alleged to achieve a compression of semantic content.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Nowhere else in the paper does Shannon use the phrase “semantic content,” nor does he again suggest that some version of data compression can apply to it. In the paper’s second paragraph, fourteen pages earlier, he had already ruled out meaning. Yet it is impossible to conclude that this comment, referencing possibly the least-mechanical English-language writer of all, was anything but intentional. While winning his court case for a meaning-free information theory, Shannon left the door open for appeals.&lt;/p&gt;
&lt;p&gt;Theoretical approaches to meaning, in fact, do abound. Contemporary literary criticism, for example, has made many &lt;a href=&quot;https://newleftreview.org/II/68/franco-moretti-network-theory-plot-analysis&quot;&gt;Shannon-inspired appropriations of scientific concepts&lt;/a&gt;, as have Saussure’s semiotics, Derrida’s deconstructionism, a wide range of &lt;a href=&quot;http://news.mit.edu/2012/applying-information-theory-to-linguistics-1010&quot;&gt;philosophical linguistics&lt;/a&gt;, and the more culturally oriented realm of media studies associated with Marshall McLuhan.&lt;/p&gt;
&lt;p&gt;• • •&lt;/p&gt;
&lt;p&gt;Despite Shannon’s admonitions, the hard scientists and engineers have also forged ahead into meaning.&lt;/p&gt;
&lt;p&gt;One was Warren Weaver. Weaver was a scholar known for republishing Shannon’s 1948 paper with a less daunting, less math-y introduction, but around the same time, he also began making pivotal breakthroughs in machine translation. By definition, translation is a task that requires more than the mere replication of a message. Weaver’s approach embraced Shannon-inspired ideas about how to use word clusters and universal language elements to improve the power and accuracy of a translation. Weaver once wrote about a hack that would help prepare verbal inputs to make them digestible for even early computers:&lt;/p&gt;
&lt;blockquote readability=&quot;21&quot;&gt;
&lt;p&gt;“It is, of course, true that Basic [English] puts multiple use on an action verb such as 'get.' But, even so, the two-word combinations such as 'get up,' 'get over,' 'get back,' etc., are, in Basic, not really very numerous. Suppose we take a vocabulary of 2,000 words, and admit for good measure all the two-word combinations as if they were single words. The vocabulary is still only four million: and that is not so formidable a number to a modern computer, is it?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Theseus, the toy mouse, was itself a harbinger of information theory’s expanding borders. It did more than just send or receive information about its maze. It sought information out and then used it to find the right path, Theseus identified the relationships among “certain physical or conceptual entities” that Shannon considered “meaning.”&lt;/p&gt;
&lt;p&gt;In considering meaning, the humanists and the scientists are heading toward the same destination. &lt;a href=&quot;http://spectrum.ieee.org/geek-life/history/meet-the-authors-of-a-mind-at-play-how-claude-shannon-invented-the-information-age&quot;&gt;In a recent interview&lt;/a&gt;, &lt;em&gt;A Mind at Play&lt;/em&gt; co-author Rob Goodman noted information theory’s potential to unify the two tribes:&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;Shannon’s life and work really called into question the whole “two cultures” paradigm, that math and science and the humanities on the other hand have very little to say to each other. . . . What Shannon was doing was not all simply hard math. It was thinking about problems that really, at the same time, consumed people in linguistics and philosophy as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Could information theory open a channel between the math-science tribe and the humanities tribe? Or perhaps between people and machines?&lt;/p&gt;
&lt;p&gt;Consider &lt;a href=&quot;http://www.independent.co.uk/voices/facebook-shuts-down-robots-ai-artificial-intelligence-develop-own-language-common-a7871341.html&quot;&gt;Facebook’s recent decision to shut down Bob and Alice&lt;/a&gt;, two artificially intelligent chatbots. The bots were trained in English, but suddenly became fluent in a pidgin understandable only to each other. “I can i i everything else,” for example, was a phrase Bob used to negotiate with Alice about how to split up a task.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.shortlist.com/tech/facebook-chatbot-zo-evil-ai-apocalypse-robot&quot;&gt;Some observers&lt;/a&gt; framed the occasion as a harbinger of the “technological singularity,” in which “superintelligent” machines develop the ability to improve themselves—and, so the story goes, take over the world. The concern is certainly legitimate, but in a calmer moment, it’s also interesting to consider how these bots mirror human behavior, and what that may say.&lt;/p&gt;
&lt;p&gt;What was Bob’s comment if not jargon? It is reminiscent of the transformations that &lt;a href=&quot;http://blog.oxforddictionaries.com/2014/06/social-media-changing-language/&quot;&gt;human language has undergone&lt;/a&gt; at the hand of text messaging (IMHO). When we use jargon, acronyms, metaphors—and adding more dimensions, stories and multimedia—isn’t data compression the goal?&lt;/p&gt;
&lt;p&gt;Despite an apparent cultural chasm between bibliophiles and technophiles, the engineering concepts of Claude Shannon’s information theory correspond elegantly to the more human elements of communication. If we develop that thought further, information theory may have a role to play as a rubric for good communication—one that cuts across at least two cultures.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://ezsubscription.com/brv/store/memberships&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;http://bostonreview.net/sites/default/files/ANoteToOurReaders_4.PNG&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;span id=&quot;most-related-view-mobile&quot;&gt;&lt;/span&gt;

</description>
<pubDate>Tue, 29 Jan 2019 15:49:05 +0000</pubDate>
<dc:creator>huihuiilly</dc:creator>
<og:type>article</og:type>
<og:title>The Man Who Invented Information Theory</og:title>
<og:url>http://bostonreview.net/science-nature/tom-rutledge-man-who-invented-information-theory</og:url>
<og:description>Of the pioneers who drove the information technology revolution, Claude Shannon may have been the most brilliant. A new book resurrects his legacy.</og:description>
<og:image>http://bostonreview.net/sites/default/files/shannonfinal2.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://bostonreview.net/science-nature/tom-rutledge-man-who-invented-information-theory</dc:identifier>
</item>
<item>
<title>Firefox 65.0 released</title>
<link>https://www.mozilla.org/en-US/firefox/65.0/releasenotes/</link>
<guid isPermaLink="true" >https://www.mozilla.org/en-US/firefox/65.0/releasenotes/</guid>
<description>&lt;li id=&quot;note-787851&quot; readability=&quot;4.8503740648379&quot;&gt;
&lt;p&gt;Enhanced tracking protection: Simplified content blocking settings give users standard, strict, and custom options to control online trackers. A redesigned content blocking section in the site information panel (viewed by expanding the small “i” icon in the address bar) shows what Firefox detects and blocks on each website you visit. To learn more about content blocking, visit the &lt;a href=&quot;https://blog.mozilla.org/blog/2019/01/29/todays-firefox-gives-users-more-control-over-their-privacy/&quot;&gt;Mozilla Blog&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;&lt;li id=&quot;note-787846&quot; readability=&quot;1.8032786885246&quot;&gt;
&lt;p&gt;A better experience for multilingual users: An &lt;a href=&quot;https://support.mozilla.org/kb/use-firefox-another-language&quot;&gt;updated Language section&lt;/a&gt; in Preferences allows users to install multiple language packs and order language preferences for Firefox and websites, without having to download locale-specific versions.&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&quot;note-787847&quot; readability=&quot;0&quot;&gt;
&lt;p&gt;Support for Handoff on macOS: Continue browsing across devices. Pick up where you left off with iOS (via Firefox or Safari) on Firefox on Mac.&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&quot;note-787852&quot; readability=&quot;1.8403755868545&quot;&gt;
&lt;p&gt;A better video streaming experience for Windows users: Firefox now supports the next-generation, royalty-free video compression technology called AV1. Read about Mozilla’s contribution to this &lt;a href=&quot;https://medium.com/mozilla-tech/mozilla-celebrates-release-of-free-high-quality-video-compression-technology-av1-in-firefox-65-7c95f2b7e56&quot;&gt;new open standard&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&quot;note-787854&quot; readability=&quot;2.7548076923077&quot;&gt;
&lt;p&gt;Improved performance and web compatibility, with support for the &lt;a href=&quot;https://developers.google.com/speed/webp/&quot;&gt;WebP image format&lt;/a&gt;: WebP brings the same image quality as existing formats at smaller file sizes, which saves bandwidth and speeds up page load.&lt;/p&gt;
&lt;/li&gt;
</description>
<pubDate>Tue, 29 Jan 2019 14:15:19 +0000</pubDate>
<dc:creator>theodorejb</dc:creator>
<og:type>website</og:type>
<og:url>https://www.mozilla.org/en-US/firefox/65.0/releasenotes/</og:url>
<og:image>https://www.mozilla.org/media/img/firefox/template/page-image.4b108ed0b8d8.png</og:image>
<og:title>Firefox 65.0, See All New Features, Updates and Fixes</og:title>
<og:description></og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.mozilla.org/en-US/firefox/65.0/releasenotes/</dc:identifier>
</item>
<item>
<title>Microsoft decides IE 10 has had its fun: Termination set for Jan 2020</title>
<link>https://www.theregister.co.uk/2019/01/29/microsoft_internet_explorer_10/</link>
<guid isPermaLink="true" >https://www.theregister.co.uk/2019/01/29/microsoft_internet_explorer_10/</guid>
<description>&lt;p&gt;Microsoft has warned that it isn't only Windows 7 for the chop in 2020. Unloved Internet Explorer 10 will be joining it. Finally.&lt;/p&gt;
&lt;p&gt;Internet Explorer 10 first appeared back in 2012 and in 2016 Microsoft made &lt;a target=&quot;_blank&quot; href=&quot;https://www.theregister.co.uk/2015/02/26/microsoft_spartan_browser_rationale/&quot;&gt;a concerted effort to kill the thing&lt;/a&gt; by focusing its &lt;a target=&quot;_blank&quot; rel=&quot;nofollow&quot; href=&quot;https://www.microsoft.com/en-us/WindowsForBusiness/End-of-IE-support&quot;&gt;support efforts on Internet Explorer 11&lt;/a&gt;. Anything not Edge-related or without &quot;11&quot; after it would no longer be supported.&lt;/p&gt;
&lt;p&gt;However, not every operating system was capable of actually running Internet Explorer 11 and Microsoft infamously restricted its Edge browser to Windows 10 (and later iOS and Android). Notable exceptions to the IE10 crackdown were Windows Server 2012 and Windows 8 Embedded.&lt;/p&gt;
&lt;div class=&quot;promo_article&quot;&gt;&lt;img src=&quot;https://regmedia.co.uk/2015/01/26/spartan.jpg?x=174&amp;amp;y=115&amp;amp;crop=1&quot; width=&quot;174&quot; height=&quot;115&quot; alt=&quot;Gerard Butler in 300&quot;/&gt;&lt;h2 title=&quot;Even Redmond is fed up with 'IE-specific behavior'&quot;&gt;Microsoft man: Internet Explorer had to go because it's garbage&lt;/h2&gt;
&lt;a href=&quot;https://www.theregister.co.uk/2015/02/26/microsoft_spartan_browser_rationale/&quot;&gt;&lt;span&gt;READ MORE&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;At this point administrators will doubtless be shuddering at the memory of having to run Internet Explorer in their pristine Server environment in order to get access to some recalcitrant function or component.&lt;/p&gt;
&lt;p&gt;Alas, the shuddering must resume since after a two-year stay of execution, Microsoft has decided that IE10 must be stamped out completely. Windows Embedded 8 Standard and Windows Server 2012 will &lt;a target=&quot;_blank&quot; rel=&quot;nofollow&quot; href=&quot;https://support.microsoft.com/en-gb/lifecycle/search?alpha=windows%208%20embedded&quot;&gt;remain supported until 2023&lt;/a&gt; after all, and keeping IE10 patched for another four years is doubtless keeping the engineers awake at night.&lt;/p&gt;
&lt;p&gt;Microsoft &lt;a target=&quot;_blank&quot; rel=&quot;nofollow&quot; href=&quot;https://techcommunity.microsoft.com/t5/Windows-IT-Pro-Blog/Bringing-Internet-Explorer-11-to-Windows-Server-2012-and-Windows/ba-p/325297&quot;&gt;has therefore warned&lt;/a&gt; that as well killing off Windows 7 in 2020, enterprises that prefer to take a slower path will have to update IE on their 2012 Servers, since IE10 support will finally end for everything in January 2020.&lt;/p&gt;
&lt;p&gt;Unlike Windows 7, you won't even be able to pay for patches.&lt;/p&gt;
&lt;p&gt;Over the course of the northern hemisphere's spring, Microsoft will make IE11 available for Server 2012 admins to play with via the Microsoft Update Catalog and also the Update Service. This would be a good a time as any to enable Enterprise Mode to make the browser behave like older versions of IE for those pesky corporate intranet applications that insist on a specific incarnation of a specific renderer. You have until 2020 to leave IE10 behind.&lt;/p&gt;
&lt;p&gt;Or you could make Microsoft very happy by migrating to Windows Sever 2016 or 2019, which both have IE11 raring to go in their Long-Term Servicing Branch (LTSB) editions. ®&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 14:08:08 +0000</pubDate>
<dc:creator>myinnerbanjo</dc:creator>
<og:image>https://regmedia.co.uk/2017/07/27/shutterstock_guillotine.jpg</og:image>
<og:type>article</og:type>
<og:url>https://www.theregister.co.uk/2019/01/29/microsoft_internet_explorer_10/</og:url>
<og:title>Microsoft decides Internet Explorer 10 has had its fun: Termination set for January 2020</og:title>
<og:description>Windows Server 2012 admins should crank it up to 11</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theregister.co.uk/2019/01/29/microsoft_internet_explorer_10/</dc:identifier>
</item>
<item>
<title>Visual Programming Doesn’t Suck</title>
<link>https://blog.statebox.org/why-visual-programming-doesnt-suck-2c1ece2a414e</link>
<guid isPermaLink="true" >https://blog.statebox.org/why-visual-programming-doesnt-suck-2c1ece2a414e</guid>
<description>&lt;div class=&quot;uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup&quot;&gt;
&lt;div class=&quot;u-flex0&quot;&gt;&lt;a class=&quot;link u-baseColor--link avatar&quot; href=&quot;https://blog.statebox.org/@anton.livaja?source=post_header_lockup&quot; data-action=&quot;show-user-card&quot; data-action-source=&quot;post_header_lockup&quot; data-action-value=&quot;571cda76ff31&quot; data-action-type=&quot;hover&quot; data-user-id=&quot;571cda76ff31&quot; data-collection-slug=&quot;statebox&quot; dir=&quot;auto&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/fit/c/100/100/0*tA7Hp5SRVgY24RQt.&quot; class=&quot;avatar-image u-size50x50&quot; alt=&quot;Go to the profile of Anton Livaja&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;u-flex1 u-paddingLeft15 u-overflowHidden&quot;&gt;

&lt;p&gt;&lt;time datetime=&quot;2018-08-30T13:16:47.241Z&quot;&gt;Aug 30, 2018&lt;/time&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p name=&quot;eb3e&quot; id=&quot;eb3e&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;I’m here to tell you that visual programming, and diagrammatic reasoning in particular, is a formidable tool-set if used the right way. That is, it only seems to work well &lt;em class=&quot;markup--em markup--p-em&quot;&gt;if based on a&lt;/em&gt; &lt;em class=&quot;markup--em markup--p-em&quot;&gt;solid foundation rooted in mathematics and computer science&lt;/em&gt;. We already abstract our code in order to make it easier to handle — doing it using visual methods such as diagrams is just another way of achieving this.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*_rBfj7yE12ulJKyfYyVgjQ.png&quot; data-width=&quot;648&quot; data-height=&quot;441&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*_rBfj7yE12ulJKyfYyVgjQ.png&quot;/&gt;&lt;/div&gt;
Abstracting from low level code to something that’s easier to reason about. (Bob Coecke’s “Picturing Quantum Processes”)
&lt;h3 name=&quot;a2e3&quot; id=&quot;a2e3&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;The Drawbacks of What’s Out There&lt;/h3&gt;
&lt;p name=&quot;bf13&quot; id=&quot;bf13&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Before you pack up and leave never to return, hear me out. The stigma around visual programming and the ease with which people dismiss it is totally warranted considering most of the tooling that has been around and the way it has been used.&lt;/p&gt;
&lt;p name=&quot;a65b&quot; id=&quot;a65b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To begin with, drag and drop tools have been a major source of frustration for me personally and seem to be the thing that gives visual ways of programming a bad name. For example, in the context of building a website, using drag and drop instead of, let’s say a Node.js stack, severely impedes the programmer’s work-flow. Also, tooling they’re accustomed to can’t be used properly.&lt;/p&gt;
&lt;p name=&quot;8036&quot; id=&quot;8036&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Additionally, the amount of freedom and control one has is considerably reduced. This is the typical example of how visual programming fails. There are plenty more examples of clunky drag and drop UIs which try to make things easier but end up over complicating things and eventually break down. This happens because complexity can’t be managed due to a lack of proper structure.&lt;/p&gt;
&lt;p name=&quot;c859&quot; id=&quot;c859&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The pros and cons don’t seem to work out positively and one is left with a poorly constructed abstraction which removes some lower level complexity at a severe cost most programmers simply don’t want to pay. In some situations drag and drop tools may be a good solution for people without programming experience but professionals run from it like the devil. If you’re one of these people, I commend you for getting this far and not leaving as soon as you read “Visual Programming” in the title.&lt;/p&gt;
&lt;h3 name=&quot;4b14&quot; id=&quot;4b14&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;What is Diagrammatic Reasoning&lt;/h3&gt;
&lt;p name=&quot;8642&quot; id=&quot;8642&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Diagrammatic reasoning is about abstracting unnecessary detail in a particular context in order to focus on details you actually care about and often ease the formation of intuitions. This improves one’s ability to reason about things in a particular way. One example is the encoding of a video. We don’t watch “0”s and “1”s which encode the video — that would be silly.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*jnIUC6fyxkH9gfc8CWlaRA.png&quot; data-width=&quot;589&quot; data-height=&quot;230&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*jnIUC6fyxkH9gfc8CWlaRA.png&quot;/&gt;&lt;/div&gt;
Binary encoding of a video vs the visual representation. (Bob Coecke’s “Picturing Quantum Processes”)
&lt;p name=&quot;2ac3&quot; id=&quot;2ac3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The main thing to realize is that “visual programming” doesn’t necessarily mean a “drag and drop”. &lt;em class=&quot;markup--em markup--p-em&quot;&gt;Visual programming done right is based on diagrammatic reasoning and proper mathematical formalisms which result in a very robust tool-kit the programmer can wield to their advantage&lt;/em&gt;. When I say “diagrammatic reasoning”, I am referring to &lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot; data-href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;state machines&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Petri_net&quot; data-href=&quot;https://en.wikipedia.org/wiki/Petri_net&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Petri nets&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/String_diagram&quot; data-href=&quot;https://en.wikipedia.org/wiki/String_diagram&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;string diagrams&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Category_theory&quot; data-href=&quot;https://en.wikipedia.org/wiki/Category_theory&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;category theory&lt;/a&gt; in particular, although there are other well defined mathematical formalisms which can be used. Explaining what these are is completely out of the scope of this blog post but I have included a short selection of resources which you can refer to at the end of the post.&lt;/p&gt;
&lt;h3 name=&quot;0caf&quot; id=&quot;0caf&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;More Examples&lt;/h3&gt;
&lt;p name=&quot;b7eb&quot; id=&quot;b7eb&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Instead of explaining the math behind it, I would like to give you some powerful examples of how hiding lower level details by using a visual approach brings forward insights which are more often than not completely opaque to the observer. One of my favorite examples is in Bob Coecke’s book “Picturing Quantum Processes” where he shows the reader that describing quantum processes can be done in a “formal way” (properly defined in terms of mathematics) via the use of several diagrams rather than a page full of obscure characters which have no clear intuition about them and require a much larger amount of prerequisite knowledge to be understood. Check out the diagram below.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*0qg4L_qNM-q-n3JJ31CnZA.png&quot; data-width=&quot;571&quot; data-height=&quot;139&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*0qg4L_qNM-q-n3JJ31CnZA.png&quot;/&gt;&lt;/div&gt;
Low level language vs high level language (visual) of describing a quantum process. (Bob Coecke’s “Picturing Quantum Processes”)
&lt;p name=&quot;42d6&quot; id=&quot;42d6&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This example doesn’t map directly on to programming so let’s try something different. The caveat here is that we need to understand some diagram semantics to make sense of this but what follow is the Fibonacci sequence written in JavaScript and an equal representation as a diagram.&lt;/p&gt;
&lt;pre name=&quot;c554&quot; id=&quot;c554&quot; class=&quot;graf graf--pre graf-after--p&quot; readability=&quot;5&quot;&gt;
function fibonacci(num){  &lt;br/&gt;var a = 1, b = 0, temp;   &lt;p&gt;while (num &amp;gt;= 0){    &lt;br/&gt;temp = a;    &lt;br/&gt;a = a + b;    &lt;br/&gt;b = temp;    &lt;br/&gt;num--;  &lt;br/&gt;}   &lt;/p&gt;&lt;p&gt;return b;&lt;br/&gt;}
&lt;/p&gt;&lt;/pre&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*WhIJd6bH_LMzNm2YvoRsTg.png&quot; data-width=&quot;766&quot; data-height=&quot;228&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*WhIJd6bH_LMzNm2YvoRsTg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*WhIJd6bH_LMzNm2YvoRsTg.png&quot;/&gt;&lt;/div&gt;
Fibonacci sequence expressed as a signal flow diagram (Pawel Sobocinski’s blog —&lt;a href=&quot;https://graphicallinearalgebra.net/2016/09/07/31-fibonacci-and-sustainable-rabbit-farming/&quot; data-href=&quot;https://graphicallinearalgebra.net/2016/09/07/31-fibonacci-and-sustainable-rabbit-farming/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt; Graphical Linear Algebra&lt;/a&gt;)
&lt;p name=&quot;cba6&quot; id=&quot;cba6&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Here is an example of a practical application of a visual approach of modelling a process. Think of an ATM, and since I’m a fan of blockchain technology, a Bitcoin ATM. From a technical perspective this is a machine which has to be designed carefully in order to do the exact thing it’s supposed to by avoiding illegal states. Examples of illegal states could be double dispensing of money, sending the user the wrong amount of crypto currency, or none at all, and so on.&lt;/p&gt;
&lt;p name=&quot;f2b1&quot; id=&quot;f2b1&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I’ll leave it to you to figure out how you would go about building this type of system with your existing knowledge and present a &lt;em class=&quot;markup--em markup--p-em&quot;&gt;diagrammatic way&lt;/em&gt; of doing it instead. Let’s take the example of exchanging Bitcoin for regular currency. This will be a simplified version of what it would look like in a real setting but it will serve well for demonstration purposes.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*i1sgKaodpQ2owhPRI_nM4A.gif&quot; data-width=&quot;600&quot; data-height=&quot;133&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*i1sgKaodpQ2owhPRI_nM4A.gif&quot;/&gt;&lt;/div&gt;
Process for an ATM converting regular money to Bitcoin.
&lt;p name=&quot;48de&quot; id=&quot;48de&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The diagram above is a simple Petri net. The little black dot traveling around is a “token” which represents the current state of the Petri net. As you can see, this is not just a diagram, it’s one that can capture different states by “firing transitions” (the little rectangles are transitions). The interesting thing is that because Petri nets are well structured, one can compile the diagram above into a lower level language to run as a program which interacts with multiple micro-services and/or modules. This is exactly what we are doing at Statebox. Take a moment to compare the approach you would usually take to design a process, to the advantages you may gain from using a visual approach of modelling the architecture (in this case of a process) for a piece of software which runs in a Bitcoin ATM.&lt;/p&gt;
&lt;h4 name=&quot;2a27&quot; id=&quot;2a27&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Some advantages that naturally come from using a visual approach to modelling processes:&lt;/h4&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;a19e&quot; id=&quot;a19e&quot; class=&quot;graf graf--li graf-after--h4&quot;&gt;It’s easier for non-technical people to contribute to the modelling of the process in a meaningful way&lt;/li&gt;
&lt;li name=&quot;8856&quot; id=&quot;8856&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;The surface area for errors is reduced (on the process level)&lt;/li&gt;
&lt;li name=&quot;15dc&quot; id=&quot;15dc&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;One gets access to mathematical tools which can be applied to Petri nets, such as state space analysis to detect dead locks and prevent illegal states&lt;/li&gt;
&lt;li name=&quot;1bf6&quot; id=&quot;1bf6&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;It’s easier to be sure all edge cases and errors have been addressed because diagrammatic reasoning helps us abstract from low level information (for example, the equivalent of the diagram above written out in plain English or conditional statements is a lot harder to reason about than the nice visual representation we have here)&lt;/li&gt;
&lt;li name=&quot;75b8&quot; id=&quot;75b8&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Building UIs based on the different states of the process — this makes it easier to manage the complexity of matching state of the system with what’s being shown in the UI and has a nice synergy with reactive programming.&lt;/li&gt;
&lt;li name=&quot;28e4&quot; id=&quot;28e4&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Although not necessarily an exclusive property of modelling things visually, having a structure such as a Petri net is able to manage complexity as it has a lot of information embeded into it (such as the set of legal “next” states). Consequently, this doesn’t have to be addressed in code and often simplifies APIs and middleware.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 name=&quot;f79f&quot; id=&quot;f79f&quot; class=&quot;graf graf--h3 graf-after--li&quot;&gt;Different “Levels” to Consider&lt;/h3&gt;
&lt;p name=&quot;ccc4&quot; id=&quot;ccc4&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;A thing to keep in mind is that visual programming doesn’t have to be about implementing low level logic such as interaction with third party services, APIs. At Statebox, we believe that the effective way to use it is to model things on a higher level instead (higher level can be a bit misleading here because it can refer to high level system architecture or a lower level process such as the one above which runs in an ATM). &lt;em class=&quot;markup--em markup--p-em&quot;&gt;By leveraging the power of Petri nets and marrying them with category theory to impose additional restrictions on their behavior, we are building a well defined language for defining processes/protocols.&lt;/em&gt; Our tool-kit (which is under development but already being used in several real world use-cases to be announced soon) is designed for visual modelling of processes using these “categorified” Petri nets&lt;em class=&quot;markup--em markup--p-em&quot;&gt;.&lt;/em&gt; In addition, it provides an engine which allows the transitioning of the net/process from one state to another. This in and of itself is a powerful tool but we are creating an entire open source ecosystem and tooling around this approach to programming.&lt;/p&gt;
&lt;p name=&quot;f516&quot; id=&quot;f516&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Some features of the Statebox system so far include a string diagram editing tool for Petri nets, event sourcing based data storage scheme, categorical data schema migrations, private-public key “role” permission system, blockchain like merkle trees and hashing for data integrity, relatively easy functorial mapping (think integration) to other systems (cloud services, blockchains, languages etc.), strong typing (places and transitions in our flavor of Petri nets have types), automatic API and form generation, well structured data which has built in meta-data about different roles in the system (KPIs), termination guarantees and other insights gained from applying state space analysis, simplified formal verification of processes and out-of-the-box formally verified components. Additionally, we are developing a type construction language based on polynomials which allows easy “type translation” between different languages called &lt;a href=&quot;https://github.com/typedefs&quot; data-href=&quot;https://github.com/typedefs&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Typedefs&lt;/a&gt;.&lt;/p&gt;
&lt;p name=&quot;f122&quot; id=&quot;f122&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We are planning to write more blogs to expand on some of the topics above so if there is anything you would like for us to write about or would like clarifications, please let me know in the comments section. Below are some resources that can help you learn about Petri nets, category theory and string diagrams.&lt;/p&gt;
&lt;h3 name=&quot;3287&quot; id=&quot;3287&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Resources&lt;/h3&gt;
&lt;p name=&quot;28fb&quot; id=&quot;28fb&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;The resources below were produced by our advisers who are world renowned experts in their respective fields. If you’re looking for more similar stuff, join our &lt;a href=&quot;https://t.me/stateboxorg&quot; data-href=&quot;https://t.me/stateboxorg&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Telegram&lt;/a&gt; channel and we’ll be happy to chat with you.&lt;/p&gt;
&lt;p name=&quot;86dd&quot; id=&quot;86dd&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Pawel Sobocinski’s blog about&lt;/strong&gt; &lt;a href=&quot;https://graphicallinearalgebra.net&quot; data-href=&quot;https://graphicallinearalgebra.net&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;graphical linear algebra&lt;/strong&gt;&lt;/a&gt; — I recommend starting from Episode 1 and skipping any sections you are familiar with. Pawel does an amazing job of taking one from basically 0 understanding of visual algebra and category theory to being able to reason about both and understanding the relationship between the two in an intuitive way by using diagrams and building up the complexity of the semantics over time.&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.05316&quot; data-href=&quot;https://arxiv.org/abs/1803.05316&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;7 Sketches in Compositionality&lt;/strong&gt;&lt;/a&gt; &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;by Brendan Fong and David Spivak&lt;/strong&gt; — This is an amazing book about category theory and compositionality explained through real world problems in a way that doesn’t require prerequisite knowledge. It’s designed so as to not build too much on itself so that each section makes sense separately.&lt;br/&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Bob Coecke’s&lt;/strong&gt; &lt;a href=&quot;https://www.amazon.com/Picturing-Quantum-Processes-Diagrammatic-Reasoning/dp/110710422X&quot; data-href=&quot;https://www.amazon.com/Picturing-Quantum-Processes-Diagrammatic-Reasoning/dp/110710422X&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Picturing Quantum Processes&lt;/strong&gt;&lt;/a&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt; &lt;/strong&gt;— an amazing book about using visual approaches to understanding quantum theory.&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 12:00:25 +0000</pubDate>
<dc:creator>mathgenius</dc:creator>
<og:title>Why Visual Programming Doesn’t Suck – Statebox</og:title>
<og:url>https://blog.statebox.org/why-visual-programming-doesnt-suck-2c1ece2a414e</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*WhIJd6bH_LMzNm2YvoRsTg.png</og:image>
<og:description>I’m here to tell you that visual programming, and diagrammatic reasoning in particular, is a formidable tool-set if used the right way…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.statebox.org/why-visual-programming-doesnt-suck-2c1ece2a414e?gi=4a4f6682eb17</dc:identifier>
</item>
<item>
<title>Show HN: Open startups with their revenue, metrics, and stories</title>
<link>https://postmake.io/open</link>
<guid isPermaLink="true" >https://postmake.io/open</guid>
<description>&lt;p data-v-188f004c=&quot;&quot;&gt;Postmake is a directory of curated tools and resources for your next projects. If you liked this list, check out the &lt;strong data-v-188f004c=&quot;&quot;&gt;&lt;a href=&quot;https://postmake.io/#roadmap&quot; data-v-188f004c=&quot;&quot;&gt;full directory&lt;/a&gt;&lt;/strong&gt; for more tools and resources, or join the newsletter to be notified when we compile more cool stuff.&lt;/p&gt;
&lt;p data-v-188f004c=&quot;&quot;&gt;Drop your email below to receive our weekly newsletter. Here's &lt;strong data-v-188f004c=&quot;&quot;&gt;&lt;a href=&quot;https://mailchi.mp/238e40221e1f/building-up-the-postmake-directory&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; data-v-188f004c=&quot;&quot;&gt;an example&lt;/a&gt;&lt;/strong&gt; of what you'll receive.&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 11:44:51 +0000</pubDate>
<dc:creator>Malfunction92</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://postmake.io/open</dc:identifier>
</item>
<item>
<title>Gmail Services Global Outage</title>
<link>https://outage.report/gmail</link>
<guid isPermaLink="true" >https://outage.report/gmail</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://outage.report/gmail&quot;&gt;https://outage.report/gmail&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19025502&quot;&gt;https://news.ycombinator.com/item?id=19025502&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 263&lt;/p&gt;
&lt;p&gt;# Comments: 152&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 11:31:02 +0000</pubDate>
<dc:creator>nedsma</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://outage.report/gmail</dc:identifier>
</item>
<item>
<title>Lessons learned scaling a PostgreSQL database to 1.2bn records/month</title>
<link>https://medium.com/@gajus/lessons-learned-scaling-postgresql-database-to-1-2bn-records-month-edc5449b3067</link>
<guid isPermaLink="true" >https://medium.com/@gajus/lessons-learned-scaling-postgresql-database-to-1-2bn-records-month-edc5449b3067</guid>
<description>&lt;h2 name=&quot;026f&quot; id=&quot;026f&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;Choosing where to host the database, materialising data and using database as a job queue&lt;/h2&gt;
&lt;div class=&quot;uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup&quot;&gt;
&lt;div class=&quot;u-flex0&quot;&gt;
&lt;div class=&quot;u-relative u-inlineBlock u-flex0&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/fit/c/100/100/0*6vr2NLSkjfCrLKgn.png&quot; class=&quot;avatar-image u-size50x50&quot; alt=&quot;Go to the profile of Gajus Kuizinas&quot;/&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;p name=&quot;46ff&quot; id=&quot;46ff&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;This isn’t my first rodeo with large datasets. The authentication and product management database that I have designed for the largest UK public Wi-Fi provider had impressive volumes too. We were tracking authentication for millions of devices daily. However, that project had a funding that allowed us to pick any hardware, any supporting services and hire any DBAs to assist with replication/data warehousing/troubleshooting. Furthermore, all analytics queries/reporting were done off logical replicas and there were multiple sysadmins that looked after the supporting infrastructure. Whereas this was a venture of my own, with limited funding and 20x the volume.&lt;/p&gt;
&lt;h4 name=&quot;625e&quot; id=&quot;625e&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Others’ mistakes&lt;/h4&gt;
&lt;p name=&quot;3634&quot; id=&quot;3634&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;This is not to say that if we did have &lt;a href=&quot;https://www.youtube.com/watch?v=ON-7v4qnHP8&quot; data-href=&quot;https://www.youtube.com/watch?v=ON-7v4qnHP8&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;loadsamoney&lt;/a&gt; we would have spent it on purchasing top-of-the-line hardware, flashy monitoring systems or DBAs (Okay, maybe having a dedicated DBA would have been nice). Over many years of consulting I have developed a view that the root of all evil lies in the unnecessarily complex data processing pipeline. You don’t need a message queue for ETL and you don’t need an application-layer cache for database queries. More often than not, these are workarounds for the underlying database issues (e.g. latency, poor indexing strategy) that create more issues down the line. In ideal scenario, you want to have all data contained within a single database and all data loading operations abstracted into atomic transactions. My goal was not to repeat these mistakes.&lt;/p&gt;
&lt;h4 name=&quot;850a&quot; id=&quot;850a&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Our goals&lt;/h4&gt;
&lt;p name=&quot;bd71&quot; id=&quot;bd71&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;As you have already guessed, our PostgreSQL database became the central piece of the business (aptly called ‘mother’, although my co-founder insists that me calling various infrastructure components ‘mother’, ‘mothership’, ‘motherland’, etc is worrying). We don’t have a standalone message queue service, cache service or replicas for data warehousing. Instead of maintaining the supporting infrastructure, I have dedicated my efforts to eliminating any bottlenecks by minimizing latency, provisioning the most suitable hardware, and carefully planning the database schema. What we have is an easy to scale infrastructure with a single database and many data processing agents. I love the simplicity of it — if something breaks, we can pin point and fix the issue within minutes. However, a lot of mistakes were done along the way — this articles summarizes some of them.&lt;/p&gt;
&lt;h3 name=&quot;a8d8&quot; id=&quot;a8d8&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Dataset&lt;/h3&gt;
&lt;p name=&quot;1073&quot; id=&quot;1073&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Before we dig in, let’s quickly summaries the dataset.&lt;/p&gt;
&lt;p name=&quot;2c85&quot; id=&quot;2c85&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I am a co-founder of a company &lt;a href=&quot;https://applaudience.com/&quot; data-href=&quot;https://applaudience.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Applaudience&lt;/a&gt;. We aggregate cinema data. Our primary dataset includes movie showtimes, ticket prices and admissions. We combine this data with all sorts of supporting data, including data that we get from YouTube, Twitter and weather reports. The end result is a comprehensive time-series dataset describing the entire theatrical movie release window. The goal is to predict movie performance far into the future.&lt;/p&gt;
&lt;p name=&quot;3e6a&quot; id=&quot;3e6a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We currently track 3200+ cinemas across 22 territories in Europe and the US. This approximates to 47,000 showtimes/day. Every time a person reserves or purchases a ticket from either of these cinemas, we capture a snapshot describing attributes of every seat in the auditorium.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*FnqYC9mCLohaWYYiPX_RLQ.png&quot; data-width=&quot;2872&quot; data-height=&quot;1830&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*FnqYC9mCLohaWYYiPX_RLQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*FnqYC9mCLohaWYYiPX_RLQ.png&quot;/&gt;&lt;/div&gt;
How we monitor data aggregation and detect anomalies is whole another topic. However, having PostgreSQL as the single source of truth about all data that is being aggregated and all the processes that aggregate the data made it a lot easier.
&lt;p name=&quot;d0b3&quot; id=&quot;d0b3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This adds up to 1.2bn records/month, and thats just for the admissions data.&lt;/p&gt;
&lt;h3 name=&quot;821d&quot; id=&quot;821d&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Choosing where to host the database&lt;/h3&gt;
&lt;p name=&quot;64fb&quot; id=&quot;64fb&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;We went through several providers:&lt;/p&gt;
&lt;p name=&quot;4c36&quot; id=&quot;4c36&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;1. Google&lt;br/&gt;2. Amazon&lt;br/&gt;3. Aiven.io&lt;br/&gt;4. Self-hosting&lt;/p&gt;
&lt;h4 name=&quot;1a5b&quot; id=&quot;1a5b&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Google Cloud SQL for PosetgreSQL&lt;/h4&gt;
&lt;p name=&quot;c0d8&quot; id=&quot;c0d8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;We got &lt;a href=&quot;https://cloud.google.com/developers/startups/&quot; data-href=&quot;https://cloud.google.com/developers/startups/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;USD 100k in startup credits from Google&lt;/a&gt;. This was the primary deciding factor for choosing their services. We used &lt;a href=&quot;https://cloud.google.com/sql/docs/postgres/&quot; data-href=&quot;https://cloud.google.com/sql/docs/postgres/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;Cloud SQL for PostgreSQL&lt;/a&gt; for about 6 months. The primary reason we migrated away from Google SQL for PostgreSQL is because we discovered a bug that was causing data corruption. This was a known bug that is fixed in newer PostgreSQL versions. However, Google SQL for PostgreSQL is several versions behind. The lack of response from the support acknowledging the issue was a big enough red-flag to move on. I am glad we did move on, because it has been 8 months since we have raised the issue, and the version of PostgreSQL has not been updated:&lt;/p&gt;
&lt;pre name=&quot;66c7&quot; id=&quot;66c7&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
postgres=&amp;gt; SELECT version();                                                PostgreSQL 9.6.6 on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4, 64-bit(1 row)
&lt;/pre&gt;
&lt;h4 name=&quot;7ef5&quot; id=&quot;7ef5&quot; class=&quot;graf graf--h4 graf-after--pre&quot;&gt;Amazon RDS for PostgreSQL&lt;/h4&gt;
&lt;p name=&quot;2945&quot; id=&quot;2945&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Then we secured credits from Amazon and migrated to &lt;a href=&quot;https://aws.amazon.com/rds/postgresql/&quot; data-href=&quot;https://aws.amazon.com/rds/postgresql/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Amazon RDS for PostgreSQL &lt;/a&gt;— their version of PostgreSQL was kept up to date and my research into the RDS community raised no concerns. However, Amazon RDS for PostgreSQL does not support &lt;a href=&quot;https://www.timescale.com/&quot; data-href=&quot;https://www.timescale.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;TimescaleDB&lt;/a&gt; extension that we planed to use for partitioning of our database. As Amazon announced &lt;a href=&quot;https://aws.amazon.com/timestream/&quot; data-href=&quot;https://aws.amazon.com/timestream/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Timestream&lt;/a&gt; (their own time-series database), it became clear that this requirement will not be addressed in the foreseeable future (&lt;a href=&quot;https://github.com/timescale/timescaledb/issues/65&quot; data-href=&quot;https://github.com/timescale/timescaledb/issues/65&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;this issue has been already open for 2 years&lt;/a&gt;).&lt;/p&gt;
&lt;h4 name=&quot;fece&quot; id=&quot;fece&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Aiven.io&lt;/h4&gt;
&lt;p name=&quot;08c8&quot; id=&quot;08c8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Then we moved to &lt;a href=&quot;https://aiven.io/&quot; data-href=&quot;https://aiven.io/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Aiven.io&lt;/a&gt;. Aiven.io manages PostgreSQL database for you on your cloud service provider of choice. It had all the &lt;a href=&quot;https://help.aiven.io/postgresql/extensions/supported-postgresql-extensions&quot; data-href=&quot;https://help.aiven.io/postgresql/extensions/supported-postgresql-extensions&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;extensions&lt;/a&gt; that I needed (including TimescaleDB), it did not lock us in with a particular server provider (meaning we could host our Kubernetes cluster on either of the Aiven.io supported providers), their support was helpful from the first interaction and my due diligence came back only with praise. However, what I have overlooked is that you &lt;a href=&quot;https://help.aiven.io/postgresql/operations/postgresql-superuser-access&quot; data-href=&quot;https://help.aiven.io/postgresql/operations/postgresql-superuser-access&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;do not get &lt;em class=&quot;markup--em markup--p-em&quot;&gt;superuser&lt;/em&gt; access&lt;/a&gt;. This resulted in numerous issues (e.g., various maintenance procedures we have been using stopped working and we couldn’t use our monitoring software due to permission issues; unable to use &lt;code class=&quot;markup--code markup--p-code&quot;&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/auto-explain.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/auto-explain.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;auto_explain&lt;/a&gt;&lt;/code&gt;; logical replication requires custom extensions) and long outages that could have been prevented.&lt;/p&gt;
&lt;p name=&quot;6fab&quot; id=&quot;6fab&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In general, I didn’t understand what added value Aiven.io provides – we weren’t even warned when the database was running out of storage.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*sbFAP1vIkOxG4tN5PBBY5A.png&quot; data-width=&quot;2876&quot; data-height=&quot;620&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*sbFAP1vIkOxG4tN5PBBY5A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*sbFAP1vIkOxG4tN5PBBY5A.png&quot;/&gt;&lt;/div&gt;
Running out of the disk space due to an unattended replication slot that kept the WAL growing.
&lt;p name=&quot;9f8e&quot; id=&quot;9f8e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;When this happened, support offered to upgrade the instance to one with a larger volume. While this is a fine solution, it caused a longer than necessary outage. Someone with SSH access could have diagnosed and fixed this issue in couple of minutes.&lt;/p&gt;
&lt;p name=&quot;45d2&quot; id=&quot;45d2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;And when we started to experience continuous outages due to (what later turned out to be) a &lt;a href=&quot;https://github.com/timescale/timescaledb/issues/468#issuecomment-457654923&quot; data-href=&quot;https://github.com/timescale/timescaledb/issues/468#issuecomment-457654923&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;bug in TimescaleDB extension&lt;/a&gt; used by Aiven.io support did not offer any workarounds for the issue.&lt;/p&gt;
&lt;blockquote name=&quot;e949&quot; id=&quot;e949&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;6.2105263157895&quot;&gt;
&lt;p&gt;We’re looking into this issue and working with the timescale team, but response to most things isn’t immediate. Our help article at &lt;a href=&quot;https://help.aiven.io/support/aiven-support-details&quot; data-href=&quot;https://help.aiven.io/support/aiven-support-details&quot; class=&quot;markup--anchor markup--blockquote-anchor&quot; rel=&quot;nofollow noopener noreferrer nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://help.aiven.io/support/aiven-support-details&lt;/a&gt; describes the response times we provide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;bcd9&quot; id=&quot;bcd9&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Which is a terribly passive response when your customer’s server is in a crashing loop (Two days later: No follow up from Aiven.io.)&lt;/p&gt;
&lt;p name=&quot;0b0c&quot; id=&quot;0b0c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Despite me giving shit to Aiven.io over a couple of issues, overall their support was great. Tolerating my questions that are already covered in documentation and aiding with troubleshooting issues. The primary reason we are moving away is the lack of SSH/superuser.&lt;/p&gt;
&lt;h4 name=&quot;f84d&quot; id=&quot;f84d&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Self-hosted&lt;/h4&gt;
&lt;p name=&quot;9b98&quot; id=&quot;9b98&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;All this time I was trying to avoid the unavoidable — managing the database ourselves. Now we are renting our own hardware and maintain the database. We have a lot better hardware than any of the cloud service providers could offer, point in time recovery (thanks to &lt;a href=&quot;https://www.pgbarman.org/index.html&quot; data-href=&quot;https://www.pgbarman.org/index.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Barman&lt;/a&gt;) and no vendor lock-in, and (on paper) it is about 30% cheaper than hosting using Google Cloud or AWS. That 30% we can use to hire a freelance DBA to check in on the servers once a day.&lt;/p&gt;
&lt;h4 name=&quot;b963&quot; id=&quot;b963&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Takeaway&lt;/h4&gt;
&lt;p name=&quot;f249&quot; id=&quot;f249&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;The takeaway here is that Google and Amazon prioritise their proprietary solutions (Google BigQuery, AWS Redshift). Therefore, you must plan for what features you will require in the future. For a simple database that will not grow into billions of records and does not require custom extensions, I would pick either without a second thought (the near instant ability to scale the instance, migrate servers to different territories, point-in-time recovery, built-in monitoring tools and managed replication saves a lot of time.).&lt;/p&gt;
&lt;p name=&quot;2238&quot; id=&quot;2238&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;If your business is all about the data and you know that you will require custom hardware configuration and whatnot, then your best bet is hosting and managing the database yourself. That said, logical migration is simple enough — if you can start with either of the managed providers and leverage their startup credits, then that is a great way to kick start a project and you can migrate later as/if it becomes necessary.&lt;/p&gt;
&lt;p name=&quot;755d&quot; id=&quot;755d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;If I would start over and would have spent time to estimate how quick and how large we are going to grow, I would have used bare-metal setup and hired a freelance DBA from the first day.&lt;/p&gt;
&lt;h4 name=&quot;0137&quot; id=&quot;0137&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Bonus: Performance&lt;/h4&gt;
&lt;p name=&quot;ac80&quot; id=&quot;ac80&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;My primary criteria for choosing managed services was the reduced management overhead. I assumed that the cost and hardware is going to be about the same. Aiven.io has written an article where they compare &lt;a href=&quot;https://aiven.io/blog/postgresql-cloud-performance/&quot; data-href=&quot;https://aiven.io/blog/postgresql-cloud-performance/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;PostgreSQL performance in AWS, GCP, Azure, DO and UpCloud&lt;/a&gt; (GCP beats AWS by a factor of 2 in all tests).&lt;/p&gt;
&lt;h3 name=&quot;caf1&quot; id=&quot;caf1&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Materializing data&lt;/h3&gt;
&lt;p name=&quot;9801&quot; id=&quot;9801&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Did I mention that this is the first time I have used PostgreSQL?&lt;/p&gt;
&lt;p name=&quot;c2b6&quot; id=&quot;c2b6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Up to this venture, I have been primarily using MySQL. The reason I decided to use PostgreSQL for this startup was because PostgreSQL has support for &lt;a href=&quot;https://www.postgresql.org/docs/current/rules-materializedviews.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/rules-materializedviews.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;materialized views&lt;/a&gt; and &lt;a href=&quot;https://www.postgresql.org/docs/current/server-programming.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/server-programming.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;programming languages&lt;/a&gt;. I thought that the materialized views is a good enough feature on its own to learn PostgreSQL. In contrast, I thought I will never run scripts in the database (MySQL teaches you that database is only for storing data and all logic must be implemented in the application code).&lt;/p&gt;
&lt;p name=&quot;2f38&quot; id=&quot;2f38&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Two years later, we got rid of most materialized views and we are using hundreds of custom procedures. But before that, there were multiple botched attempts at using materialized views.&lt;/p&gt;
&lt;h4 name=&quot;cfd3&quot; id=&quot;cfd3&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;First attempt at using PostgreSQL materialized views&lt;/h4&gt;
&lt;p name=&quot;36c1&quot; id=&quot;36c1&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;My first use case for materialized views could be summarized as ‘having a base table enriched with a metadata’, e.g.&lt;/p&gt;
&lt;pre name=&quot;6ef3&quot; id=&quot;6ef3&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE MATERIALIZED VIEW venue_view AS&lt;br/&gt;WITH&lt;br/&gt;auditorium_with_future_events AS (&lt;br/&gt;SELECT&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;FROM event e1&lt;br/&gt;WHERE&lt;br/&gt;-- The 30 days interval ensures that we do not remove auditoriums&lt;br/&gt;-- that are temporarily unavailable.&lt;br/&gt;e1.start_time &amp;gt; now() - INTERVAL '30 day' AND&lt;br/&gt;e1.auditorium_id IS NOT NULL&lt;br/&gt;GROUP BY&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;),&lt;br/&gt;auditorium_with_future_events_count AS (&lt;br/&gt;SELECT&lt;br/&gt;awfe1.venue_id,&lt;br/&gt;count(*) auditorium_count&lt;br/&gt;FROM auditorium_with_future_events awfe1&lt;br/&gt;GROUP BY&lt;br/&gt;awfe1.venue_id&lt;br/&gt;),&lt;br/&gt;venue_auditorium_seat_count AS (&lt;br/&gt;SELECT DISTINCT ON (e1.venue_id, e1.auditorium_id)&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id,&lt;br/&gt;e1.seat_count&lt;br/&gt;FROM auditorium_with_future_events awfe1&lt;br/&gt;INNER JOIN event e1 ON e1.venue_id = awfe1.venue_id AND e1.auditorium_id = awfe1.auditorium_id&lt;br/&gt;WHERE&lt;br/&gt;e1.start_time &amp;gt; now() - INTERVAL '30 day' AND&lt;br/&gt;e1.auditorium_id IS NOT NULL AND&lt;br/&gt;e1.seat_count IS NOT NULL&lt;br/&gt;ORDER BY&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;),&lt;br/&gt;venue_seat_count AS (&lt;br/&gt;SELECT&lt;br/&gt;vasc1.venue_id,&lt;br/&gt;sum(vasc1.seat_count) seat_count&lt;br/&gt;FROM venue_auditorium_seat_count vasc1&lt;br/&gt;GROUP BY vasc1.venue_id&lt;br/&gt;)&lt;br/&gt;SELECT DISTINCT ON (v1.id)&lt;br/&gt;v1.id,&lt;br/&gt;v1.google_place_id,&lt;br/&gt;v1.fuid,&lt;br/&gt;v1.cinema_id,&lt;br/&gt;v1.street_1,&lt;br/&gt;v1.street_2,&lt;br/&gt;v1.postcode,&lt;br/&gt;v1.coordinates,&lt;br/&gt;gp1.country_id,&lt;br/&gt;gp1.timezone_id,&lt;br/&gt;COALESCE(v1.phone_number, c1.phone_number) AS phone_number,&lt;br/&gt;v1.display_name AS name,&lt;br/&gt;COALESCE(v1.alternative_url, v1.url) AS url,&lt;br/&gt;v1.permanently_closed_at,&lt;br/&gt;awfec1.auditorium_count,&lt;br/&gt;nearest_venue.id nearest_venue_id,&lt;br/&gt;CASE&lt;br/&gt;WHEN nearest_venue.id IS NULL&lt;br/&gt;THEN NULL&lt;br/&gt;ELSE round(ST_DistanceSphere(gp1.location, nearest_venue.location))&lt;br/&gt;END nearest_venue_distance,&lt;br/&gt;vsc1.seat_count seat_count&lt;br/&gt;FROM venue v1&lt;br/&gt;LEFT JOIN venue_seat_count vsc1 ON vsc1.venue_id = v1.id&lt;br/&gt;LEFT JOIN google_place gp1 ON gp1.id = v1.google_place_id&lt;br/&gt;LEFT JOIN LATERAL (&lt;br/&gt;SELECT&lt;br/&gt;v2.id,&lt;br/&gt;gp2.location&lt;br/&gt;FROM venue v2&lt;br/&gt;INNER JOIN google_place gp2 ON gp2.id = v2.google_place_id&lt;br/&gt;WHERE v2.id != v1.id&lt;br/&gt;ORDER BY gp1.location &amp;lt;-&amp;gt; gp2.location&lt;br/&gt;LIMIT 1&lt;br/&gt;) nearest_venue ON TRUE&lt;br/&gt;LEFT JOIN auditorium_with_future_events_count awfec1 ON awfec1.venue_id = v1.id&lt;br/&gt;INNER JOIN cinema c1 ON c1.id = v1.cinema_id&lt;br/&gt;WITH NO DATA;
&lt;/pre&gt;
&lt;pre name=&quot;3818&quot; id=&quot;3818&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE UNIQUE INDEX ON venue_view (id);
&lt;/pre&gt;
&lt;pre name=&quot;7f60&quot; id=&quot;7f60&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE INDEX ON venue_view (google_place_id);&lt;br/&gt;CREATE INDEX ON venue_view (cinema_id);&lt;br/&gt;CREATE INDEX ON venue_view (country_id);&lt;br/&gt;CREATE INDEX ON venue_view (nearest_venue_id);
&lt;/pre&gt;
&lt;p name=&quot;0eb4&quot; id=&quot;0eb4&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;Here &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue&lt;/code&gt; is the base table that we extend with additional data and call it &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue_view&lt;/code&gt;. There were only two rules to adhere:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;42c1&quot; id=&quot;42c1&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;code class=&quot;markup--code markup--li-code&quot;&gt;_view&lt;/code&gt; must include all columns of the base table.&lt;/li&gt;
&lt;li name=&quot;1bec&quot; id=&quot;1bec&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;code class=&quot;markup--code markup--li-code&quot;&gt;_view&lt;/code&gt; must include all rows of the base table.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;5976&quot; id=&quot;5976&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;There is nothing wrong with the above query. This approach worked for a long time. However, as the number of records grew to millions and billions the time it took to refresh materialized views grew from a couple of seconds to hours. (If you are not familiar with materialized views, then it is worth noting that you can only refresh the entire materialized view; there is no way to refresh a subset of a view based on a condition.)&lt;/p&gt;
&lt;h4 name=&quot;c30f&quot; id=&quot;c30f&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Second attempt: divide and conquer&lt;/h4&gt;
&lt;p name=&quot;ae14&quot; id=&quot;ae14&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;I tried to solve the issue by breaking down MVs into multiple smaller MVs, e.g.&lt;/p&gt;
&lt;p name=&quot;b48d&quot; id=&quot;b48d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;(Notice that we have moved queries from CTEs to dedicated MVs.)&lt;/p&gt;
&lt;pre name=&quot;05f0&quot; id=&quot;05f0&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE MATERIALIZED VIEW auditorium_with_future_events_view&lt;br/&gt;SELECT&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;FROM event e1&lt;br/&gt;WHERE&lt;br/&gt;-- The 30 days interval ensures that we do not remove auditoriums&lt;br/&gt;-- that are temporarily unavailable.&lt;br/&gt;e1.start_time &amp;gt; now() - INTERVAL '30 day' AND&lt;br/&gt;e1.auditorium_id IS NOT NULL&lt;br/&gt;GROUP BY&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;WITH NO DATA;
&lt;/pre&gt;
&lt;pre name=&quot;f7fd&quot; id=&quot;f7fd&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE UNIQUE INDEX ON auditorium_with_future_events_view (venue_id, auditorium_id);
&lt;/pre&gt;
&lt;pre name=&quot;cbde&quot; id=&quot;cbde&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE MATERIALIZED VIEW venue_auditorium_seat_count_view&lt;br/&gt;SELECT DISTINCT ON (e1.venue_id, e1.auditorium_id)&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id,&lt;br/&gt;e1.seat_count&lt;br/&gt;FROM auditorium_with_future_events_view awfe1&lt;br/&gt;INNER JOIN event e1 ON e1.venue_id = awfe1.venue_id AND e1.auditorium_id = awfe1.auditorium_id&lt;br/&gt;WHERE&lt;br/&gt;e1.start_time &amp;gt; now() - INTERVAL '30 day' AND&lt;br/&gt;e1.auditorium_id IS NOT NULL AND&lt;br/&gt;e1.seat_count IS NOT NULL&lt;br/&gt;ORDER BY&lt;br/&gt;e1.venue_id,&lt;br/&gt;e1.auditorium_id&lt;br/&gt;WITH NO DATA;
&lt;/pre&gt;
&lt;pre name=&quot;7efc&quot; id=&quot;7efc&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE UNIQUE INDEX ON venue_auditorium_seat_count_view (venue_id, auditorium_id);
&lt;/pre&gt;
&lt;pre name=&quot;5c1b&quot; id=&quot;5c1b&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE MATERIALIZED VIEW venue_view AS&lt;br/&gt;WITH&lt;br/&gt;auditorium_with_future_events_count AS (&lt;br/&gt;SELECT&lt;br/&gt;awfe1.venue_id,&lt;br/&gt;count(*) auditorium_count&lt;br/&gt;FROM auditorium_with_future_events_view awfe1&lt;br/&gt;GROUP BY&lt;br/&gt;awfe1.venue_id&lt;br/&gt;),&lt;br/&gt;venue_seat_count AS (&lt;br/&gt;SELECT&lt;br/&gt;vasc1.venue_id,&lt;br/&gt;sum(vasc1.seat_count) seat_count&lt;br/&gt;FROM venue_auditorium_seat_count_view vasc1&lt;br/&gt;GROUP BY vasc1.venue_id&lt;br/&gt;)&lt;br/&gt;SELECT DISTINCT ON (v1.id)&lt;br/&gt;v1.id,&lt;br/&gt;v1.google_place_id,&lt;br/&gt;v1.fuid,&lt;br/&gt;v1.cinema_id,&lt;br/&gt;v1.street_1,&lt;br/&gt;v1.street_2,&lt;br/&gt;v1.postcode,&lt;br/&gt;v1.coordinates,&lt;br/&gt;gp1.country_id,&lt;br/&gt;gp1.timezone_id,&lt;br/&gt;COALESCE(v1.phone_number, c1.phone_number) AS phone_number,&lt;br/&gt;v1.display_name AS name,&lt;br/&gt;COALESCE(v1.alternative_url, v1.url) AS url,&lt;br/&gt;v1.permanently_closed_at,&lt;br/&gt;awfec1.auditorium_count,&lt;br/&gt;nearest_venue.id nearest_venue_id,&lt;br/&gt;CASE&lt;br/&gt;WHEN nearest_venue.id IS NULL&lt;br/&gt;THEN NULL&lt;br/&gt;ELSE round(ST_DistanceSphere(gp1.location, nearest_venue.location))&lt;br/&gt;END nearest_venue_distance,&lt;br/&gt;vsc1.seat_count seat_count&lt;br/&gt;FROM venue v1&lt;br/&gt;LEFT JOIN venue_seat_count vsc1 ON vsc1.venue_id = v1.id&lt;br/&gt;LEFT JOIN google_place gp1 ON gp1.id = v1.google_place_id&lt;br/&gt;LEFT JOIN LATERAL (&lt;br/&gt;SELECT&lt;br/&gt;v2.id,&lt;br/&gt;gp2.location&lt;br/&gt;FROM venue v2&lt;br/&gt;INNER JOIN google_place gp2 ON gp2.id = v2.google_place_id&lt;br/&gt;WHERE v2.id != v1.id&lt;br/&gt;ORDER BY gp1.location &amp;lt;-&amp;gt; gp2.location&lt;br/&gt;LIMIT 1&lt;br/&gt;) nearest_venue ON TRUE&lt;br/&gt;LEFT JOIN auditorium_with_future_events_count awfec1 ON awfec1.venue_id = v1.id&lt;br/&gt;INNER JOIN cinema c1 ON c1.id = v1.cinema_id&lt;br/&gt;WITH NO DATA;
&lt;/pre&gt;
&lt;pre name=&quot;9006&quot; id=&quot;9006&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE UNIQUE INDEX ON venue_view (id);
&lt;/pre&gt;
&lt;pre name=&quot;3f46&quot; id=&quot;3f46&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE INDEX ON venue_view (google_place_id);&lt;br/&gt;CREATE INDEX ON venue_view (cinema_id);&lt;br/&gt;CREATE INDEX ON venue_view (country_id);&lt;br/&gt;CREATE INDEX ON venue_view (nearest_venue_id);
&lt;/pre&gt;
&lt;p name=&quot;cf2e&quot; id=&quot;cf2e&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;The benefit of this approach is that:&lt;/p&gt;
&lt;p name=&quot;d7cd&quot; id=&quot;d7cd&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;1. We broke-down one long-transaction into many shorter transactions.&lt;br/&gt;2. We are able to use indexes to speed up the JOINs.&lt;br/&gt;3. We are able to refresh individual materialized views (some data changes more often than the other).&lt;/p&gt;
&lt;p name=&quot;1c2f&quot; id=&quot;1c2f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The downside of this approach is that it proliferated the number of materialized views that we use and required to develop a custom solution to orchestrate refreshing of the materialized views. At the time, it seemed reasonable and I went with it. Thus was &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_view_refresh_schedule&lt;/code&gt; table born and our first in-database queue:&lt;/p&gt;
&lt;pre name=&quot;fbba&quot; id=&quot;fbba&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE TABLE materialized_view_refresh_schedule (&lt;br/&gt;id SERIAL PRIMARY KEY,&lt;br/&gt;materialized_view_name citext NOT NULL,&lt;br/&gt;refresh_interval interval NOT NULL,&lt;br/&gt;last_attempted_at timestamp with time zone,&lt;br/&gt;maximum_execution_duration interval NOT NULL DEFAULT '00:30:00'::interval&lt;br/&gt;);
&lt;/pre&gt;
&lt;pre name=&quot;4edf&quot; id=&quot;4edf&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE UNIQUE INDEX materialized_view_refresh_schedule_materialized_view_name_idx ON materialized_view_refresh_schedule(materialized_view_name citext_ops);
&lt;/pre&gt;
&lt;pre name=&quot;bc78&quot; id=&quot;bc78&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE TABLE materialized_view_refresh_schedule_execution (&lt;br/&gt;id integer DEFAULT nextval('materialized_view_refresh_id_seq'::regclass) PRIMARY KEY,&lt;br/&gt;materialized_view_refresh_schedule_id integer NOT NULL REFERENCES materialized_view_refresh_schedule(id) ON DELETE CASCADE,&lt;br/&gt;started_at timestamp with time zone NOT NULL,&lt;br/&gt;ended_at timestamp with time zone,&lt;br/&gt;execution_is_successful boolean,&lt;br/&gt;error_name text,&lt;br/&gt;error_message text,&lt;br/&gt;terminated_at timestamp with time zone,&lt;br/&gt;CONSTRAINT materialized_view_refresh_schedule_execution_check CHECK (terminated_at IS NULL OR ended_at IS NOT NULL)&lt;br/&gt;);
&lt;/pre&gt;
&lt;pre name=&quot;1a70&quot; id=&quot;1a70&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
CREATE INDEX materialized_view_refresh_schedule_execution_materialized_view_ ON materialized_view_refresh_schedule_execution(materialized_view_refresh_schedule_id int4_ops);
&lt;/pre&gt;
&lt;p name=&quot;068f&quot; id=&quot;068f&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;Names of the materialized views are stored in &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_view_refresh_schedule&lt;/code&gt; table with instructions as to how often they need be refreshed. A separate program was written to perform materialization using these instructions.&lt;/p&gt;
&lt;pre name=&quot;1588&quot; id=&quot;1588&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE OR REPLACE FUNCTION schedule_new_materialized_view_refresh_schedule_execution()&lt;br/&gt;RETURNS table(materialized_view_refresh_schedule_id int)&lt;br/&gt;AS $$&lt;br/&gt;BEGIN&lt;br/&gt;RETURN QUERY&lt;br/&gt;UPDATE materialized_view_refresh_schedule&lt;br/&gt;SET last_attempted_at = now()&lt;br/&gt;WHERE id IN (&lt;br/&gt;SELECT mvrs1.id&lt;br/&gt;FROM materialized_view_refresh_schedule mvrs1&lt;br/&gt;LEFT JOIN LATERAL (&lt;br/&gt;SELECT 1&lt;br/&gt;FROM materialized_view_refresh_schedule_execution mvrse1&lt;br/&gt;WHERE&lt;br/&gt;mvrse1.ended_at IS NULL AND&lt;br/&gt;mvrse1.materialized_view_refresh_schedule_id = mvrs1.id&lt;br/&gt;) AS unendeded_materialized_view_refresh_schedule_execution ON TRUE&lt;br/&gt;WHERE&lt;br/&gt;unendeded_materialized_view_refresh_schedule_execution IS NULL AND&lt;br/&gt;(&lt;br/&gt;mvrs1.last_attempted_at IS NULL OR&lt;br/&gt;mvrs1.last_attempted_at + mvrs1.refresh_interval &amp;lt; now()&lt;br/&gt;)&lt;br/&gt;ORDER BY mvrs1.last_attempted_at ASC NULLS FIRST&lt;br/&gt;LIMIT 1&lt;br/&gt;FOR UPDATE OF mvrs1 SKIP LOCKED&lt;br/&gt;)&lt;br/&gt;RETURNING id;&lt;br/&gt;END&lt;br/&gt;$$&lt;br/&gt;LANGUAGE plpgsql;
&lt;/pre&gt;
&lt;p name=&quot;96cf&quot; id=&quot;96cf&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;This program would call &lt;code class=&quot;markup--code markup--p-code&quot;&gt;schedule_new_materialized_view_refresh_schedule_execution&lt;/code&gt; to schedule a materialized view refresh, evaluate &lt;code class=&quot;markup--code markup--p-code&quot;&gt;REFRESH MATERIALIZED VIEW … CONCURRENTLY&lt;/code&gt;, and log the result. In general, this approach worked well. However, we soon outgrew this approach. A view that requires to scan an entire table was not feasible for large tables with billions of records.&lt;/p&gt;
&lt;h4 name=&quot;e770&quot; id=&quot;e770&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Third attempt: using MVs to abstract a subset of data&lt;/h4&gt;
&lt;p name=&quot;493e&quot; id=&quot;493e&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;I have described how we have used MVs to effectively extend a table. This approach did not scale with large tables. Thus the third iteration was born: instead of using materialized views to extend the base table, create materialized views that abstract a data domain. Due to its size &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue_view&lt;/code&gt; could remain as it was, but a hypothetical view such as &lt;code class=&quot;markup--code markup--p-code&quot;&gt;event_view&lt;/code&gt; with billions of records would become &lt;code class=&quot;markup--code markup--p-code&quot;&gt;last_week_event&lt;/code&gt;, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;future_event&lt;/code&gt;, etc. This approach works and we continue to use several such materialized views.&lt;/p&gt;
&lt;h4 name=&quot;0500&quot; id=&quot;0500&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Fourth attempt: materialized table columns&lt;/h4&gt;
&lt;p name=&quot;f850&quot; id=&quot;f850&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;While the latter approach covered all our day to day operations, we still needed to run queries on the historical data. Running these queries without materialized views would take &lt;em class=&quot;markup--em markup--p-em&quot;&gt;a lot&lt;/em&gt; of index planning for individual queries. Furthermore, running long transactions against the master instance would have prevented &lt;code class=&quot;markup--code markup--p-code&quot;&gt;&lt;a href=&quot;https://blog.2ndquadrant.com/when-autovacuum-does-not-vacuum/&quot; data-href=&quot;https://blog.2ndquadrant.com/when-autovacuum-does-not-vacuum/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;autovacuum&lt;/a&gt;&lt;/code&gt; &lt;a href=&quot;https://blog.2ndquadrant.com/when-autovacuum-does-not-vacuum/&quot; data-href=&quot;https://blog.2ndquadrant.com/when-autovacuum-does-not-vacuum/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;and caused table bloat&lt;/a&gt;. I could have created a logical replication and allowed analysts to run whatever queries on that instance without blocking &lt;code class=&quot;markup--code markup--p-code&quot;&gt;autovacuum&lt;/code&gt;. However, the bigger problem is that as a startup we cannot afford queries that take hours or days to run. We need to move faster than anyone else. Thus was born the current solution: materialized table columns.&lt;/p&gt;
&lt;p name=&quot;caae&quot; id=&quot;caae&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The principal is simple:&lt;/p&gt;
&lt;p name=&quot;881d&quot; id=&quot;881d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Tables that describe entities that we want to enrich with additional information are altered to include a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_at timestamptz&lt;/code&gt; column and a column for each data point that we want to materialize. In the example of the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue_view&lt;/code&gt;, we would get rid of the materialized view entirely and add &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_at&lt;/code&gt;, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;country_id&lt;/code&gt;, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;timezone_id&lt;/code&gt;, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;phone_number&lt;/code&gt; and other columns that were present in the original &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue_view&lt;/code&gt; materialized view to the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;venue&lt;/code&gt; table itself.&lt;/p&gt;
&lt;p name=&quot;9437&quot; id=&quot;9437&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Then there is a script that observes all tables that have &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_at&lt;/code&gt; column and every time it detects a row where &lt;code class=&quot;markup--code markup--p-code&quot;&gt;materialized_at IS NULL&lt;/code&gt; it computes new values for the materialized columns and updates the row, e.g.&lt;/p&gt;
&lt;pre name=&quot;6565&quot; id=&quot;6565&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE OR REPLACE FUNCTION materialize_event_seat_state_change()&lt;br/&gt;RETURNS void&lt;br/&gt;AS $$&lt;br/&gt;BEGIN&lt;br/&gt;WITH&lt;br/&gt;event_seat_state_count AS (&lt;br/&gt;SELECT&lt;br/&gt;essc1.id,&lt;br/&gt;count(*)::smallint seat_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'BLOCKED')::smallint seat_blocked_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'BROKEN')::smallint seat_broken_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'EMPTY')::smallint seat_empty_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'HOUSE')::smallint seat_house_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'SOLD')::smallint seat_sold_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid = 'UNKNOWN')::smallint seat_unknown_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.id IS NULL)::smallint seat_unmapped_count,&lt;br/&gt;count(*) FILTER (WHERE ss1.nid IN ('BLOCKED', 'BROKEN', 'HOUSE', 'SOLD', 'UNKNOWN')) seat_unavailable_count&lt;br/&gt;FROM event e1&lt;br/&gt;LEFT JOIN event_seat_state_change essc1 ON essc1.event_id = e1.id&lt;br/&gt;LEFT JOIN event_seat_state_change_seat_state esscss1 ON esscss1.event_seat_state_change_id = essc1.id&lt;br/&gt;LEFT JOIN cinema_foreign_seat_state fcss1 ON fcss1.id = cinema_foreign_seat_state_id&lt;br/&gt;LEFT JOIN seat_state ss1 ON ss1.id = fcss1.seat_state_id&lt;br/&gt;WHERE&lt;br/&gt;essc1.id IN (&lt;br/&gt;SELECT id&lt;br/&gt;FROM event_seat_state_change&lt;br/&gt;WHERE&lt;br/&gt;materialized_at IS NULL&lt;br/&gt;ORDER BY materialized_at DESC&lt;br/&gt;LIMIT 100&lt;br/&gt;)&lt;br/&gt;GROUP BY essc1.id&lt;br/&gt;)&lt;br/&gt;UPDATE event_seat_state_change essc1&lt;br/&gt;SET&lt;br/&gt;materialized_at = now(),&lt;br/&gt;seat_count = essc2.seat_count,&lt;br/&gt;seat_blocked_count = essc2.seat_blocked_count,&lt;br/&gt;seat_broken_count = essc2.seat_broken_count,&lt;br/&gt;seat_empty_count = essc2.seat_empty_count,&lt;br/&gt;seat_house_count = essc2.seat_house_count,&lt;br/&gt;seat_sold_count = essc2.seat_sold_count,&lt;br/&gt;seat_unknown_count = essc2.seat_unknown_count,&lt;br/&gt;seat_unmapped_count = essc2.seat_unmapped_count&lt;br/&gt;FROM event_seat_state_count essc2&lt;br/&gt;WHERE&lt;br/&gt;essc1.id = essc2.id;&lt;br/&gt;END&lt;br/&gt;$$&lt;br/&gt;LANGUAGE plpgsql&lt;br/&gt;SET work_mem='1GB'&lt;br/&gt;SET max_parallel_workers_per_gather=4;
&lt;/pre&gt;
&lt;p name=&quot;fe58&quot; id=&quot;fe58&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;Once again, this required to write a custom solution that observes tables and manages their materialization, row and column expiration logic, etc. I am currently developing an open-source version that I plan to publish in the near future.&lt;/p&gt;
&lt;p name=&quot;ddba&quot; id=&quot;ddba&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The biggest benefit of this approach is that you can be as granular as you want about updating the materialized table columns: you can update individual rows and you can update individual columns (e.g. when new materialized column is added and there is a need to populate new column values, you would only need to generate value for that column; no need to run full materialization query). Furthermore, as the updates are granular, they can all be applied in a near real-time.&lt;/p&gt;
&lt;h4 name=&quot;442a&quot; id=&quot;442a&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Takeaway&lt;/h4&gt;
&lt;p name=&quot;7a25&quot; id=&quot;7a25&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;The takeaway here is that PostgreSQL materialized views are a great feature for small datasets. However, as the dataset grows, careful planning is required for how data is going to be accessed and what materialization strategy supports such requirement. Using a combination of granular materialized views and materialized table columns we were able to enrich the database in real-time and use it for all our analytics queries without adding the complexity of a logical replicate for data-warehousing.&lt;/p&gt;
&lt;h3 name=&quot;7282&quot; id=&quot;7282&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Using database as a job queue&lt;/h3&gt;
&lt;p name=&quot;af13&quot; id=&quot;af13&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;This has less to do with the volume of the data that we process and more with how we are using the database. As I mentioned earlier, my goal was to reduce the number of services that participate in the data processing pipeline. The added benefit of containing the job queue within a database is that you are able to keep and query records of all jobs (and their attributes) associated with every data point that is in the database. Being able query jobs and logs associated with every data point, join it with parent and descendent jobs, etc. proved &lt;em class=&quot;markup--em markup--p-em&quot;&gt;extremely valuable for flagging failing jobs and pin-pointing the origin of the issue.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*nkMn2DWK8xA5GUQOq-C3Ig.jpeg&quot; data-width=&quot;537&quot; data-height=&quot;720&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*nkMn2DWK8xA5GUQOq-C3Ig.jpeg&quot;/&gt;&lt;/div&gt;
Building a simple, reliable and efficient concurrent work queues using PostgreSQL.
&lt;p name=&quot;2ea1&quot; id=&quot;2ea1&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;It is worth noting that normally, a RDBMs would be a poor choice for a concurrent job queue (for reasons outlined in &lt;a href=&quot;https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/&quot; data-href=&quot;https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;What is SKIP LOCKED for in PostgreSQL 9.5?&lt;/a&gt;). However, in case of PostgreSQL, we can use &lt;code class=&quot;markup--code markup--p-code&quot;&gt;FOR UPDATE … SKIP LOCKED&lt;/code&gt; to build a simple, reliable and efficient concurrent work queues. The downside is the performance:&lt;/p&gt;
&lt;blockquote name=&quot;c3f1&quot; id=&quot;c3f1&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;Each transaction scans the table and skips over locked rows, so with high numbers of active workers it can land up doing a bit of work to acquire a new item. It’s not just popping items off a stack. The query will probably have to walk an index with an index scan, fetching each candidate item from the heap and checking the lock status. With any reasonable queue this will all be in memory but it’s still a fair bit of churn.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;a6ea&quot; id=&quot;a6ea&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;– &lt;a href=&quot;https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/&quot; data-href=&quot;https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://blog.2ndquadrant.com/what-is-select-skip-locked-for-in-postgresql-9-5/&lt;/a&gt;&lt;/p&gt;
&lt;p name=&quot;1a9b&quot; id=&quot;1a9b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I did not pay enough attention to this warning and landed myself in quite a bit of trouble.&lt;/p&gt;
&lt;p name=&quot;5508&quot; id=&quot;5508&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The short version is that the first version of the query used to schedule jobs took a long time to execute, which meant meant that worker nodes were primarily sitting idle, we were wasting valuable resources and important tasks were not done in time.&lt;/p&gt;
&lt;p name=&quot;a10c&quot; id=&quot;a10c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The solution was quite simple: a dedicated table that is populated with a list of outstanding tasks. Picking up a job from this table is as simple as:&lt;/p&gt;
&lt;pre name=&quot;ab2b&quot; id=&quot;ab2b&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE OR REPLACE FUNCTION schedule_cinema_data_task()&lt;br/&gt;RETURNS table(cinema_data_task_id int)&lt;br/&gt;AS $$&lt;br/&gt;DECLARE&lt;br/&gt;scheduled_cinema_data_task_id int;&lt;br/&gt;BEGIN&lt;br/&gt;UPDATE&lt;br/&gt;cinema_data_task_queue&lt;br/&gt;SET&lt;br/&gt;attempted_at = now()&lt;br/&gt;WHERE&lt;br/&gt;id = (&lt;br/&gt;SELECT cdtq1.id&lt;br/&gt;FROM cinema_data_task_queue cdtq1&lt;br/&gt;WHERE cdtq1.attempted_at IS NULL&lt;br/&gt;ORDER BY cdtq1.id ASC&lt;br/&gt;LIMIT 1&lt;br/&gt;FOR UPDATE OF cdtq1 SKIP LOCKED&lt;br/&gt;)&lt;br/&gt;RETURNING cinema_data_task_queue.cinema_data_task_id&lt;br/&gt;INTO scheduled_cinema_data_task_id;
&lt;/pre&gt;
&lt;pre name=&quot;d6eb&quot; id=&quot;d6eb&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
UPDATE cinema_data_task&lt;br/&gt;SET last_attempted_at = now()&lt;br/&gt;WHERE id = scheduled_cinema_data_task_id;
&lt;/pre&gt;
&lt;pre name=&quot;043e&quot; id=&quot;043e&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
RETURN QUERY SELECT scheduled_cinema_data_task_id;&lt;br/&gt;END&lt;br/&gt;$$&lt;br/&gt;LANGUAGE plpgsql&lt;br/&gt;SET work_mem='100MB';
&lt;/pre&gt;
&lt;p name=&quot;7104&quot; id=&quot;7104&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;The main task definition is stored in &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cinema_data_task&lt;/code&gt; . &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cinema_data_task_queue&lt;/code&gt; is used only for queuing ready to execute tasks.&lt;/p&gt;
&lt;p name=&quot;35f4&quot; id=&quot;35f4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The biggest gotcha is that the priority and limitations of which tasks can run changes every time a new task is executed. Therefore, instead of scheduling large number of jobs, we are running a process that every second checks if the queue is running dry and populates it with new tasks, e.g.&lt;/p&gt;
&lt;pre name=&quot;b4b0&quot; id=&quot;b4b0&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
CREATE OR REPLACE FUNCTION update_cinema_data_task_queue()&lt;br/&gt;RETURNS void&lt;br/&gt;AS $$&lt;br/&gt;DECLARE&lt;br/&gt;outstanding_task_count int;&lt;br/&gt;BEGIN&lt;br/&gt;SELECT count(*)&lt;br/&gt;FROM cinema_data_task_queue&lt;br/&gt;WHERE attempted_at IS NULL&lt;br/&gt;INTO outstanding_task_count;
&lt;/pre&gt;
&lt;pre name=&quot;53be&quot; id=&quot;53be&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
IF outstanding_task_count &amp;lt; 100 THEN&lt;br/&gt;INSERT INTO cinema_data_task_queue (cinema_data_task_id)&lt;br/&gt;SELECT&lt;br/&gt;cdtq1.cinema_data_task_id&lt;br/&gt;FROM cinema_data_task_queue(100, 50, 100, false) cdtq1&lt;br/&gt;WHERE&lt;br/&gt;NOT EXISTS (&lt;br/&gt;SELECT 1&lt;br/&gt;FROM cinema_data_task_queue&lt;br/&gt;WHERE&lt;br/&gt;cinema_data_task_id = cdtq1.cinema_data_task_id AND&lt;br/&gt;attempted_at IS NULL&lt;br/&gt;)&lt;br/&gt;ON CONFLICT (cinema_data_task_id) WHERE attempted_at IS NULL&lt;br/&gt;DO NOTHING;&lt;br/&gt;END IF;&lt;br/&gt;END&lt;br/&gt;$$&lt;br/&gt;LANGUAGE plpgsql&lt;br/&gt;SET work_mem='50MB';
&lt;/pre&gt;
&lt;p name=&quot;5d82&quot; id=&quot;5d82&quot; class=&quot;graf graf--p graf-after--pre&quot;&gt;After the task is completed, the reference to the task is deleted from &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cinema_data_task_queue&lt;/code&gt; . This ensured that table scans are quick and do not keep the CPU busy.&lt;/p&gt;
&lt;p name=&quot;a7fe&quot; id=&quot;a7fe&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This approach allowed us to scale to 2000+ concurrent data aggregation agents.&lt;/p&gt;
&lt;p name=&quot;4079&quot; id=&quot;4079&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Note: The 100 outstanding tasks limit is somewhat arbitrary. I have experimented with values as large as 10k without any measurable performance penalty. However, as long as we can keep the queue from drying out, then the more granular the scheduling is, the better we load-balance data aggregation between different sources, the sooner we can stop pulling data from failing data sources, etc.&lt;/p&gt;
&lt;h4 name=&quot;59e8&quot; id=&quot;59e8&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Takeway&lt;/h4&gt;
&lt;p name=&quot;08e9&quot; id=&quot;08e9&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;If you are going to use database as a job queue, the table containing the jobs must be reasonable size and the query used to schedule the next job execution must not take more than couple of milliseconds.&lt;/p&gt;
&lt;h3 name=&quot;ff52&quot; id=&quot;ff52&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Miscs&lt;/h3&gt;
&lt;p name=&quot;4f9a&quot; id=&quot;4f9a&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;These 3 things were the biggest challenges when scaling the database. Some other gotchas include:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;c765&quot; id=&quot;c765&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;When you have hundreds of clients each running dozens of queries a second then &lt;em class=&quot;markup--em markup--li-em&quot;&gt;latency between the database and the database clients matters a lot&lt;/em&gt;. I have observed that the latency between our database (at the time) hosted on AWS RDS and our Kubernetes cluster hosted on GKE was 12ms. By moving the database to the same datacenter and reducing latency to &amp;lt;1ms, our job throughout increased 4x.&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*FSXQItecVhCx0PhLkB5RXw.png&quot; data-width=&quot;1800&quot; data-height=&quot;1800&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*FSXQItecVhCx0PhLkB5RXw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*FSXQItecVhCx0PhLkB5RXw.png&quot;/&gt;&lt;/div&gt;
Identifying latency between different cloud providers.
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;58a0&quot; id=&quot;58a0&quot; class=&quot;graf graf--li graf-after--figure&quot;&gt;Column order matters. We have tables with 60+ columns. Ordering columns to avoid padding saved 20%+ storage (&lt;a href=&quot;https://blog.2ndquadrant.com/on-rocks-and-sand/&quot; data-href=&quot;https://blog.2ndquadrant.com/on-rocks-and-sand/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://blog.2ndquadrant.com/on-rocks-and-sand/&lt;/a&gt;).&lt;/li&gt;
&lt;li name=&quot;346b&quot; id=&quot;346b&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;If you are going to run long-queries on master, evaluate &lt;code class=&quot;markup--code markup--li-code&quot;&gt;vacuum_freeze_table_age&lt;/code&gt; to prevent table bloat.&lt;/li&gt;
&lt;li name=&quot;3c85&quot; id=&quot;3c85&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Two configurations that I do not see being talked enough about: &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/runtime-config-query.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/runtime-config-query.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;from_collapse_limit&lt;/a&gt;&lt;/code&gt;, &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/runtime-config-query.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/runtime-config-query.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;join_collapse_limit&lt;/a&gt;&lt;/code&gt; . Both configurations default to 8. Not knowing about these configuration caused a lot of headache debugging confusing execution plans. We increased &lt;code class=&quot;markup--code markup--li-code&quot;&gt;from_collapse_limit&lt;/code&gt; to 20 and &lt;code class=&quot;markup--code markup--li-code&quot;&gt;join_collapse_limit&lt;/code&gt; to 50. It is unclear to me what is the reason the defaults are low. There appears to be no penalty for having them infinitely high.&lt;/li&gt;
&lt;li name=&quot;4ba9&quot; id=&quot;4ba9&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Plan for table bloat and how to repair it. As the database grows large, &lt;code class=&quot;markup--code markup--li-code&quot;&gt;VACUUM FULL&lt;/code&gt; becomes unfeasible. Explore &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;http://reorg.github.io/pg_repack/&quot; data-href=&quot;http://reorg.github.io/pg_repack/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;pg_repack&lt;/a&gt;&lt;/code&gt; and &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;https://www.cybertec-postgresql.com/en/products/pg_squeeze/&quot; data-href=&quot;https://www.cybertec-postgresql.com/en/products/pg_squeeze/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;pg_squeeze&lt;/a&gt;&lt;/code&gt; .&lt;/li&gt;
&lt;li name=&quot;c912&quot; id=&quot;c912&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Constantly monitor &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/pgstatstatements.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/pgstatstatements.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;pg_stat_statements&lt;/a&gt;&lt;/code&gt; . Sort by &lt;code class=&quot;markup--code markup--li-code&quot;&gt;total_time&lt;/code&gt;. Top queries are the low hanging fruits.&lt;/li&gt;
&lt;li name=&quot;8c66&quot; id=&quot;8c66&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Constantly monitor &lt;code class=&quot;markup--code markup--li-code&quot;&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/monitoring-stats.html&quot; data-href=&quot;https://www.postgresql.org/docs/current/monitoring-stats.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;pg_stat_user_tables&lt;/a&gt;&lt;/code&gt;. Identify underused indexes and monitor dead tuple accumulation.&lt;/li&gt;
&lt;li name=&quot;1858&quot; id=&quot;1858&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Constantly monitor &lt;code class=&quot;markup--code markup--li-code&quot;&gt;pg_stat_activity&lt;/code&gt; . Identify bottlenecks due to locks and refactor the offending transactions.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 name=&quot;32e2&quot; id=&quot;32e2&quot; class=&quot;graf graf--h3 graf-after--li&quot;&gt;Bonus: Slonik PostgreSQL client&lt;/h3&gt;
&lt;p name=&quot;26b3&quot; id=&quot;26b3&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;We were using PostgreSQL a lot. We began by using &lt;code class=&quot;markup--code markup--p-code&quot;&gt;&lt;a href=&quot;https://github.com/brianc/node-postgres&quot; data-href=&quot;https://github.com/brianc/node-postgres&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;node-postgres&lt;/a&gt;&lt;/code&gt;. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;node-postgres&lt;/code&gt; provided a great protocol abstraction. However, the code felt verbose and we kept adding new helpers to abstract repeating patterns and to enable debugging experience. We needed these helpers across many different programs. Therefore, I ended up developing &lt;a href=&quot;https://github.com/gajus/slonik&quot; data-href=&quot;https://github.com/gajus/slonik&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Slonik&lt;/a&gt; – A PostgreSQL client with strict types, detail logging and assertions.&lt;/p&gt;
&lt;p name=&quot;0feb&quot; id=&quot;0feb&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Slonik helps us to keep the code lean, protects against SQL injections, enables detail logging and application log correlation with &lt;code class=&quot;markup--code markup--p-code&quot;&gt;auto_explain&lt;/code&gt;.&lt;/p&gt;
&lt;h3 name=&quot;aa14&quot; id=&quot;aa14&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Acknowledgements&lt;/h3&gt;
&lt;p name=&quot;e18f&quot; id=&quot;e18f&quot; class=&quot;graf graf--p graf-after--h3 graf--trailing&quot;&gt;I want to thank &lt;a href=&quot;https://freenode.net/&quot; data-href=&quot;https://freenode.net/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;Freenode&lt;/a&gt; #postgresql community for the warm welcome, mentoring and aiding throughout my PostgreSQL journey. Whether I asked esoteric questions, dumb questions, needed help debugging an issue or just wanted to learn a bit of history about PostgreSQL origins, I always got full support from #postgresql community. I especially want to thank Berge, depesz, ilmari, Myon, nickb, peerce, RhodiumToad, Snow-Man, xocolatl and Zr40: your support halved the time I needed to learn as much as I have up to now.&lt;/p&gt;
</description>
<pubDate>Tue, 29 Jan 2019 09:03:16 +0000</pubDate>
<dc:creator>willvarfar</dc:creator>
<og:title>Lessons learned scaling PostgreSQL database to 1.2bn records/ month</og:title>
<og:url>https://medium.com/@gajus/lessons-learned-scaling-postgresql-database-to-1-2bn-records-month-edc5449b3067</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*FnqYC9mCLohaWYYiPX_RLQ.png</og:image>
<og:description>Choosing where to host the database, materialising data and using database as a job queue</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/@gajus/lessons-learned-scaling-postgresql-database-to-1-2bn-records-month-edc5449b3067</dc:identifier>
</item>
</channel>
</rss>