<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>An updated daily front page of The New York Times as artwork on your wall</title>
<link>https://alexanderklopping.medium.com/an-updated-daily-front-page-of-the-new-york-times-as-artwork-on-your-wall-3b28c3261478</link>
<guid isPermaLink="true" >https://alexanderklopping.medium.com/an-updated-daily-front-page-of-the-new-york-times-as-artwork-on-your-wall-3b28c3261478</guid>
<description>&lt;h2 id=&quot;f20d&quot; class=&quot;fx cz fa ar b fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm gn bq&quot;&gt;As a news junkie, tech journalist and founder of an online news platform called Blendle, I love a good old front page of a newspaper. When I saw &lt;a href=&quot;https://onezero.medium.com/the-morning-paper-revisited-35b407822494?source=linkShare-8ba55ab4d1c9-1604866028&quot; class=&quot;dg go&quot; rel=&quot;noopener&quot;&gt;this post&lt;/a&gt; about a Google engineer who built an e-ink device that displays the current front page of The New York Times on his wall, I immediately wanted one.&lt;/h2&gt;
&lt;div class=&quot;gp&quot;&gt;
&lt;div class=&quot;n cc gq gr gs&quot;&gt;
&lt;div class=&quot;o n&quot;&gt;
&lt;div&gt;&lt;a rel=&quot;noopener&quot; href=&quot;https://alexanderklopping.medium.com/?source=post_page-----3b28c3261478--------------------------------&quot;&gt;&lt;img alt=&quot;Alexander Klöpping&quot; class=&quot;s gt gu gv&quot; src=&quot;https://miro.medium.com/fit/c/56/56/1*lHrAJ1Byh4l8NWIiGDTAtw.jpeg&quot; width=&quot;28&quot; height=&quot;28&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p id=&quot;7500&quot; class=&quot;hk hl fa hm b fy hn ho gb hp hq hr hs ht hu hv hw hx hy hz ia ib db fw&quot;&gt;Although Max Braun kindly explains how he built his display, buying an e-ink screen from Asia, using BMP files and pouring concrete for the frame was a bit much for me, so I found an easier (albeit pricier) solution. He &lt;a href=&quot;https://onezero.medium.com/meet-accent-352cfa95813a&quot; class=&quot;dg go&quot; rel=&quot;noopener&quot;&gt;writes&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote class=&quot;ic id ie&quot; readability=&quot;10.529914529915&quot;&gt;
&lt;p id=&quot;5e5f&quot; class=&quot;hk hl if hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;As more and more connected devices arrive in our homes, it’s a good time to remember the principles of &lt;a href=&quot;https://calmtech.com/&quot; class=&quot;dg go&quot; rel=&quot;noopener nofollow&quot;&gt;Calm Technology&lt;/a&gt;, first formulated at Xerox PARC in 1995. They talk about how technology should respect our attention and remain in the background most of the time, how relevant information should be presented calmly and make use of the periphery.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;86a2&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;For this reason, e-ink screens have always fascinated me.&lt;/p&gt;
&lt;p id=&quot;5214&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;Most screens create the opposite of calm, but e-ink screens, having no backlight and a slow refresh rate, induce a sense of calm. And, because of their limited power usage, the battery on e-ink screens can easily last an entire year.&lt;/p&gt;
&lt;p id=&quot;a055&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;This is w&lt;span id=&quot;rmm&quot;&gt;&lt;span id=&quot;rmm&quot;&gt;h&lt;/span&gt;&lt;/span&gt;y e-ink screens are uniquely suited for displaying wall art. Unlike paper, the screen allows for the display of dynamic content (making it possible to display the ever-changing front page of The Times), while the absence of a backlight makes it less distracting than an LCD screen.&lt;/p&gt;
&lt;p id=&quot;a431&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;In my search for sellers of e-ink devices, I discovered a company from Slovenia called &lt;a href=&quot;https://www.visionect.com&quot; class=&quot;dg go&quot; rel=&quot;noopener nofollow&quot;&gt;Visionect&lt;/a&gt; that sells e-ink screens for corporate use. Known in some circles for their &lt;a href=&quot;https://getjoan.com&quot; class=&quot;dg go&quot; rel=&quot;noopener nofollow&quot;&gt;Joan&lt;/a&gt; office screens that show whether a meeting room is available or not, they also sell &lt;a href=&quot;https://www.visionect.com/product/place-and-play-32/&quot; class=&quot;dg go&quot; rel=&quot;noopener nofollow&quot;&gt;larger screens&lt;/a&gt; for use in, for example, airports (to show flight times) and hotels (to provide conference room overviews).&lt;/p&gt;
&lt;p id=&quot;39a3&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;It turns out, the screen they sell for corporate use is a perfect match to the proportions of an unfolded New York Times. The Place &amp;amp; Play 32 inch is a pricey (2300 euro’s) but very beautiful piece of hardware. It’s made of stainless steel with sharp edges and a glass panel on top. The USB connectors are nicely hidden and the device feels sturdy.&lt;/p&gt;
&lt;h2 id=&quot;e6e2&quot; class=&quot;il im fa ar in io ip ga iq ir is gd it ge iu gg iv gh iw gj ix gk iy gm iz ja fw&quot;&gt;Here’s how I converted my new screen into a piece of journalism artwork.&lt;/h2&gt;
&lt;p id=&quot;fd10&quot; class=&quot;hk hl fa hm b fy jb hn ho gb jc hp hq hr jd hs ht hu je hv hw hx jf hy hz ib db fw&quot;&gt;The New York Times publishes a &lt;a href=&quot;https://static01.nyt.com/images/2020/10/05/nytfrontpage/scan.pdf&quot; class=&quot;dg go&quot; rel=&quot;noopener nofollow&quot;&gt;PDF version&lt;/a&gt; of their front page daily. A friend of mine wrote a basic script that converts the PDF into a JPG and copies it to a self-hosted HTML page, which I’ve shared below. The only change required is to the variable “$outputfile”.&lt;/p&gt;

&lt;p id=&quot;e2b9&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;The device comes with Wi-Fi and with a cloud-based content management system (CMS) that allows you to load a webpage, making it easy to load the page on the screen. The CMS allows you to set how often the screen refreshes (in my case, once daily was sufficient).&lt;/p&gt;

&lt;p id=&quot;a4d0&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;The screen has a convenient VESA mount on the back, making it super easy to hang on the wall using a standard TV mount and allowing it to blend in seamlessly with existing décor. Because of the low refresh rate, the battery in the device will last for about a year on one charge, eliminating the need for an outlet and messy cables.&lt;/p&gt;
&lt;p id=&quot;467a&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;Remember, the device has no buttons and is not a touch screen; it only shows the front page of the paper. But that’s enough to get the gist of what’s going on in the world, and if I want to continue reading an article that caught my attention, I use the Times app (most of the time it’s somewhere near the top of the app as well).&lt;/p&gt;
&lt;p id=&quot;7c70&quot; class=&quot;hk hl fa hm b fy ig hn ho gb ih hp hq hr ii hs ht hu ij hv hw hx ik hy hz ib db fw&quot;&gt;Every morning, I wake up to a fresh edition of the Times on my wall. I find it wonderful to hover for a bit with a cup of coffee, scanning the headlines or reading an article. Mission accomplished and I am one satisfied news junkie.&lt;/p&gt;



</description>
<pubDate>Wed, 11 Nov 2020 22:23:39 +0000</pubDate>
<dc:creator>knes</dc:creator>
<og:type>article</og:type>
<og:title>An updated daily front page of The New York Times as artwork on your wall</og:title>
<og:description>As a news junkie, tech journalist and founder of an online news platform called Blendle, I love a good old front page of a newspaper. When…</og:description>
<og:url>https://alexanderklopping.medium.com/an-updated-daily-front-page-of-the-new-york-times-as-artwork-on-your-wall-3b28c3261478</og:url>
<og:image>https://miro.medium.com/max/1200/1*dfY9ypciD1DaYuFrg1YkZg.jpeg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://alexanderklopping.medium.com/an-updated-daily-front-page-of-the-new-york-times-as-artwork-on-your-wall-3b28c3261478</dc:identifier>
</item>
<item>
<title>Notion Timeline View</title>
<link>https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team</link>
<guid isPermaLink="true" >https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team</guid>
<description>&lt;style&gt;&lt;![CDATA[body{background:#fff}body.dark{background:#2f3437}.initial-loading-spinner{-webkit-animation:rotate 1s linear infinite;animation:rotate 1s linear infinite;-webkit-transform-origin:center center;transform-origin:center center;width:1em;height:1em;opacity:.5;display:block;pointer-events:none}@-webkit-keyframes rotate{0%{-webkit-transform:rotate(0) translateZ(0);transform:rotate(0) translateZ(0)}100%{-webkit-transform:rotate(360deg) translateZ(0);transform:rotate(360deg) translateZ(0)}}@keyframes rotate{0%{-webkit-transform:rotate(0) translateZ(0);transform:rotate(0) translateZ(0)}100%{-webkit-transform:rotate(360deg) translateZ(0);transform:rotate(360deg) translateZ(0)}}]]&gt;&lt;/style&gt;</description>
<pubDate>Wed, 11 Nov 2020 19:15:37 +0000</pubDate>
<dc:creator>AlphaWeaver</dc:creator>
<og:type>website</og:type>
<og:url>https://www.notion.so</og:url>
<og:title>Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.</og:title>
<og:description>A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team</og:description>
<og:image>https://www.notion.so/images/meta/default.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team</dc:identifier>
</item>
<item>
<title>Google Photos will end its free unlimited storage in June 2021</title>
<link>https://www.theverge.com/2020/11/11/21560810/google-photos-unlimited-cap-free-uploads-15gb-ending</link>
<guid isPermaLink="true" >https://www.theverge.com/2020/11/11/21560810/google-photos-unlimited-cap-free-uploads-15gb-ending</guid>
<description>&lt;p id=&quot;6fNcdD&quot;&gt;After &lt;a href=&quot;https://www.theverge.com/a/sundars-google/google-photos-google-io-2015&quot;&gt;five years&lt;/a&gt; of offering unlimited free photo backups at “high quality,” Google Photos will start charging for storage once more than 15 gigs on the account have been used. The change will happen on June 1st, 2021, and it comes with other Google Drive policy changes like counting Google Workspace documents and spreadsheets against the same cap. Google is also introducing a new policy of deleting data from inactive accounts that haven’t been logged in to for at least two years.&lt;/p&gt;
&lt;p id=&quot;5UqfYW&quot;&gt;All photos and documents uploaded before June 1st will &lt;em&gt;not&lt;/em&gt; count against that 15GB cap, so you have plenty of time to decide whether to continue using Google Photos or switching to another cloud storage provider for your photos. Only photos uploaded &lt;em&gt;after&lt;/em&gt; June 1st will begin counting against the cap.&lt;/p&gt;
&lt;p id=&quot;c3QEUB&quot;&gt;Google already counts “original quality” photo uploads against a storage cap in Google Photos. However, taking away unlimited backup for “high quality” photos and video (which are automatically &lt;a href=&quot;https://support.google.com/photos/answer/6220791?co=GENIE.Platform%3DAndroid&amp;amp;hl=en&quot;&gt;compressed for more efficient storage&lt;/a&gt;) also takes away one of the service’s biggest selling points. It was the photo service where you just didn’t have to worry about how much storage you had.&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;
&lt;aside id=&quot;pb30Kn&quot;&gt;&lt;q&gt;Everything uploaded before June 1st won’t count against your cap&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id=&quot;gYXNhS&quot;&gt;As a side note, Pixel owners will still be able to upload high-quality (not original) photos for free after June 1st without those images counting against their cap. It’s not as good as the Pixel’s original deal of getting unlimited original quality, but it’s a small bonus for the few people who buy Google’s devices.&lt;/p&gt;
&lt;p id=&quot;RzfWCx&quot;&gt;Google points out that it offers more free storage than others — you get 15GB instead of the paltry 5GB that Apple’s iCloud gives you — and it also claims that 80 percent of Google Photos users won’t hit that 15GB cap for at least three years.&lt;/p&gt;
&lt;p id=&quot;2yvU7C&quot;&gt;The company will send alerts and warnings when you begin to approach that cap. Google is also putting new storage management tools into Google Photos, including a tool that makes it easier to find and delete photos you might not want anyway, like blurry images or screenshots.&lt;/p&gt;
&lt;span class=&quot;e-image__inner&quot;&gt;&lt;span class=&quot;e-image__image&quot; data-original=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/22029451/Storage_management_tool.gif&quot;&gt;&lt;img class=&quot;c-dynamic-image&quot; alt=&quot;&quot; data-chorus-optimize-field=&quot;main_image&quot; src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs&quot; data-cid=&quot;site/dynamic_size_image-1605142868_1681_37774&quot; data-cdata=&quot;{&amp;quot;asset_id&amp;quot;:22029451,&amp;quot;ratio&amp;quot;:&amp;quot;*&amp;quot;}&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/22029451/Storage_management_tool.gif&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/span&gt;&lt;/span&gt;
&lt;p id=&quot;Woc2Tv&quot;&gt;Google is also going to show a more useful “personalized estimate” of how much longer a storage tier will last in terms of time instead of gigabytes. It estimates each user’s average uploads over time to guess how much longer they’ll be able to use their current tier.&lt;/p&gt;
&lt;p id=&quot;teCBtA&quot;&gt;Why the change? One possibility is that it’s part of a larger push to get more people to sign up for Google One storage. The service now also includes a free VPN for Android at some of its higher tiers, and it seems as though many Google products are aligning with Google One. Google’s explanation in a brief interview is simpler: there is already a nearly unfathomable number of photos and videos uploaded to Google Photos, and the service needs to be sustainable. That’s the gist if you read between the lines of its blog post:&lt;/p&gt;
&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p id=&quot;18nvNK&quot;&gt;Today, more than 4 trillion photos are stored in Google Photos, and every week 28 billion new photos and videos are uploaded. Since so many of you rely on Google Photos to store your memories, it’s important that it’s not just a great product, but also continues to meet your needs over the long haul. In order to welcome even more of your memories and build Google Photos for the future, we are changing our unlimited High quality storage policy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;span class=&quot;e-image__inner&quot;&gt;&lt;span class=&quot;e-image__image&quot; data-original=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/22029533/Storage_management_tool.gif&quot;&gt;&lt;img class=&quot;c-dynamic-image&quot; alt=&quot;&quot; data-chorus-optimize-field=&quot;main_image&quot; src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs&quot; data-cid=&quot;site/dynamic_size_image-1605142868_1782_37775&quot; data-cdata=&quot;{&amp;quot;asset_id&amp;quot;:22029533,&amp;quot;ratio&amp;quot;:&amp;quot;*&amp;quot;}&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/22029533/Storage_management_tool.gif&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/span&gt;&lt;/span&gt;
&lt;p id=&quot;RwiHmv&quot;&gt;&lt;a href=&quot;https://one.google.com/about&quot;&gt;Google One pricing&lt;/a&gt; is not changing. It starts at $1.99 / month for 100GB and has tiers going through 200GB ($2.99 / month), 2TB ($9.99 / month), and all the way up to 30TB ($149.99 / month).&lt;/p&gt;
&lt;p id=&quot;podCZJ&quot;&gt;Alongside photos, “Google Docs, Sheets, Slides, Drawings, Forms and Jamboard files” will also begin counting against storage caps. The reasoning is “to bring our policies more in line with industry standards,” Google says. (This puts an end to some very clever hacks like &lt;a href=&quot;https://news.ycombinator.com/item?id=19907271&quot;&gt;this one that turned files into Google Docs through a binary conversion tool&lt;/a&gt;.)&lt;/p&gt;
&lt;p id=&quot;N5AxLB&quot;&gt;As for the inactive account policy, it seems fairly reasonable: if you haven’t touched your Google account for two years and don’t respond in any way to the multiple warning emails and notifications Google sends you, the company may delete data from your account. Here’s how Google explains it:&lt;/p&gt;
&lt;blockquote readability=&quot;17&quot;&gt;
&lt;p id=&quot;HAVDiy&quot;&gt;If you’re inactive in one or more of these services for two years (24 months), Google may delete the content in the product(s) in which you’re inactive. [...] Similarly, if you’re over your storage limit for two years, Google may delete your content across Gmail, Drive and Photos.&lt;/p&gt;
&lt;p id=&quot;bE38r7&quot;&gt;We will notify you multiple times before we attempt to remove any content so you have ample opportunities to take action. The simplest way to keep your account active is to periodically visit Gmail, Drive or Photos on the web or mobile, while signed in and connected to the internet.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
<pubDate>Wed, 11 Nov 2020 18:10:26 +0000</pubDate>
<dc:creator>mvgoogler</dc:creator>
<og:description>Nothing uploaded before then will count, but it’s still a bummer.</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/9KOV-1bCti-pcYu0xS_coVYan8E=/0x146:2040x1214/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/22019652/1BEB77B1_EB2C_4E1F_A9C1_1B863908E956.jpeg</og:image>
<og:title>Google Photos will end its free unlimited storage on June 1st, 2021</og:title>
<og:type>article</og:type>
<og:url>https://www.theverge.com/2020/11/11/21560810/google-photos-unlimited-cap-free-uploads-15gb-ending</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theverge.com/2020/11/11/21560810/google-photos-unlimited-cap-free-uploads-15gb-ending</dc:identifier>
</item>
<item>
<title>Show HN: Self-hosted offline Internet from your browsing history</title>
<link>https://github.com/c9fe/22120.git</link>
<guid isPermaLink="true" >https://github.com/c9fe/22120.git</guid>
<description>&lt;p&gt;&lt;a href=&quot;https://hits.seeyoufarm.com&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/9ead7c538413012d9b25a581612e131c71a5b8378333d0a1876650677e086952/68747470733a2f2f686974732e736565796f756661726d2e636f6d2f6170692f636f756e742f696e63722f62616467652e7376673f75726c3d68747470732533412532462532466769746875622e636f6d25324663396665253246323231323026636f756e745f62673d253233373943383344267469746c655f62673d2532333535353535352669636f6e3d2669636f6e5f636f6c6f723d253233453745374537267469746c653d253238746f646179253246746f74616c25323925323076697369746f727325324225324225324225323073696e63652532304f637425323032372532303230323026656467655f666c61743d66616c7365&quot; alt=&quot;visitors+++&quot; data-canonical-src=&quot;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fc9fe%2F22120&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23555555&amp;amp;icon=&amp;amp;icon_color=%23E7E7E7&amp;amp;title=%28today%2Ftotal%29%20visitors%2B%2B%2B%20since%20Oct%2027%202020&amp;amp;edge_flat=false&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;🏛️ - An archivist browser controller that caches everything you browse, a library server with full text search to serve your archive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;News - new binaries&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;hr/&gt;&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright (c) 2018, 2020, Dosyago and/or its affiliates. All rights reserved.&lt;/p&gt;
&lt;p&gt;This is a release of 22120, a web archiver.&lt;/p&gt;
&lt;p&gt;License information can be found in the LICENSE file.&lt;/p&gt;
&lt;p&gt;This software is dual-licensed. For information about commercial licensing, see &lt;a href=&quot;https://github.com/dosyago/dual-licensing&quot;&gt;Dosyago Commercial License for OEMs, ISVs and VARs&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;This project literally makes your web browsing available COMPLETELY OFFLINE.&lt;/strong&gt; Your browser does not even know the difference. It's literally that amazing. Yes.&lt;/p&gt;
&lt;p&gt;Save your browsing, then switch off the net and go to &lt;code&gt;http://localhost:22120&lt;/code&gt; and switch mode to &lt;strong&gt;serve&lt;/strong&gt; then browse what you browsed before. It all still works.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;warning: if you have Chrome open, it will close it automatically when you open 22120, and relaunch it. You may lose any unsaved work.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Get 22120&lt;/h2&gt;
&lt;p&gt;3 ways to get it:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Get binary from the &lt;a href=&quot;https://github.com/c9fe/22120/releases&quot;&gt;releases page.&lt;/a&gt;, or&lt;/li&gt;
&lt;li&gt;Run with npx: &lt;code&gt;npx archivist1&lt;/code&gt;, or&lt;/li&gt;
&lt;li&gt;Clone this repo and run as a Node.JS app: &lt;code&gt;npm i &amp;amp;&amp;amp; npm start&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Also, coming soon is a Chrome Extension.&lt;/p&gt;
&lt;h2&gt;Using&lt;/h2&gt;
&lt;h3&gt;Pick save mode or serve mode&lt;/h3&gt;
&lt;p&gt;Go to &lt;a href=&quot;http://localhost:22120&quot; rel=&quot;nofollow&quot;&gt;http://localhost:22120&lt;/a&gt; in your browser, and follow the instructions.&lt;/p&gt;
&lt;h3&gt;Exploring your 22120 archive&lt;/h3&gt;
&lt;p&gt;Archive will be located in &lt;code&gt;$your_user_home_directory/22120-arc/public/library&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;But it's not public, don't worry!&lt;/p&gt;
&lt;h2&gt;Format&lt;/h2&gt;
&lt;p&gt;The archive format is:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;22120-arc/public/library/&amp;lt;resource-origin&amp;gt;/&amp;lt;path-hash&amp;gt;.json&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Inside the JSON file, is a JSON object with headers, response code, key and a base 64 encoded response body.&lt;/p&gt;
&lt;h2&gt;Why not WARC (or another format like MHTML) ?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The case for the 22120 format.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Other formats save translations of the resources you archive. They create modifications, such as altering the internal structure of the HTML, changing hyperlinks and URLs into &quot;flat&quot; embedded data URIs, and require other &quot;hacks* in order to save a &quot;perceptually similar&quot; copy of the archived resource.&lt;/p&gt;
&lt;p&gt;22120 throws all that out, and calls rubbish on it. 22120 saves a &lt;em&gt;verbatim&lt;/em&gt; &lt;strong&gt;high-fidelity&lt;/strong&gt; copy of the resources your archive. It does not alter their internal structure in any way. Instead it records each resource in its own metadata file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At 22120, we believe in the resources and in verbatim copies. We don't annoint ourselves as all knowing enough to modify the resource source of truth before we archive it, just so it can &quot;fit the format* we choose. We don't believe we should be modifying or altering resources we archive. We belive we should save them exactly as they were presented. We believe the format should fit (or at least accommodate, and be suited to) the resource, not the other way around. We don't believe in conflating &lt;strong&gt;metadata&lt;/strong&gt; with &lt;strong&gt;content&lt;/strong&gt;; so we separate them. We believe separating metadata and content, and keeping the content pure and altered throughout the archiving process is not only the right thing to do, it simplifies every part of the audit trail, because we know that the modifications between archived copies of a resource of due to changes to the resources themselves, not artefacts of the format or archiving process.&lt;/p&gt;
&lt;p&gt;Both WARC and MHTML require mutilatious modifications of the resources so that the resources can be &quot;forced to fit&quot; the format. At 22120, we believe this is not required (and in any case should never be performed). We see it as akin to lopping off the arms of a Roman statue in order to fit it into a presentation and security display box. How ridiculous! The web may be a more &quot;pliable&quot; medium but that does not mean we should treat it without respect for its inherent content.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is changing the internal structure of resources so bad?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In our view, the internal structure of the resource as presented, &lt;em&gt;is the cannon&lt;/em&gt;. Internal structure is not just substitutable &quot;presentation&quot; - no, in fact it encodes vital semantic information such as hyperlink relationships, source choices, and the &quot;strokes&quot; of the resource author as they create their content, even if it's mediated through a web server or web framework.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why else is 22120 the obvious and natural choice?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;22120 also archives resources exactly as they are sent to the browser. It runs connected to a browser, and so is able to access the full-scope of resources (with, currently, the exception of video, audio and websockets, for now) in their highest fidelity, without modification, that the browser receives and is able to archive them in the exact format presented to the user. Many resources undergo presentational and processing changes before they are presented to the user. This is the ubiquitous, &quot;web app&quot;, where client-side scripting enabled by JavaScript, creates resources and resource views on the fly. These sorts of &quot;hyper resources&quot; or &quot;realtime&quot; or &quot;client side&quot; resources, prevalent in SPAs, are not able to be archived, at least not utilizing the normal archive flow, within traditional &lt;code&gt;wget&lt;/code&gt;-based archiving tools.&lt;/p&gt;
&lt;p&gt;In short, the web is an &lt;em&gt;online&lt;/em&gt; medium, and it should be archived and presented in the same fashion. 22120 archives content exactly as it is received and presented by a browser, and it also replays that content exactly as if the resource were being taken from online. Yes, it requires a browser for this exercise, but that browser need not be connected to the internet. It is only natural that viewing a web resource requires the web browser. And because of 22120 the browser doesn't know the difference! Resources presented to the browser form a remote web site, and resources given to the browser by 22120, are seen by the browser as &lt;em&gt;&lt;strong&gt;exactly the same.&lt;/strong&gt;&lt;/em&gt; This ensures that the people viewing the archive are also not let down and are given the change to have the exact same experience as if they were viewing the resource online.&lt;/p&gt;
&lt;h2&gt;How it works&lt;/h2&gt;
&lt;p&gt;Uses DevTools protocol to intercept all requests, and caches responses against a key made of (METHOD and URL) onto disk. It also maintains an in memory set of keys so it knows what it has on disk.&lt;/p&gt;
&lt;h2&gt;FAQ&lt;/h2&gt;
&lt;h3&gt;Can I use this with a browser that's not Chrome-based?&lt;/h3&gt;
&lt;p&gt;No.&lt;/p&gt;
&lt;h3&gt;How does this interact with Ad blockers?&lt;/h3&gt;
&lt;p&gt;Interacts just fine. The things ad blockers stop will not be archived.&lt;/p&gt;
&lt;h3&gt;How secure is running chrome with remote debugging port open?&lt;/h3&gt;
&lt;p&gt;Seems pretty secure. It's not exposed to the public internet, and pages you load that tried to use it cannot use the protocol for anything (except to open a new tab, which they can do anyway).&lt;/p&gt;
&lt;h3&gt;Is this free?&lt;/h3&gt;
&lt;p&gt;Yes this is totally free to download and use. It's also open source so do what you want with it.&lt;/p&gt;
&lt;h3&gt;What's the roadmap?&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Full text search&lt;/li&gt;
&lt;li&gt;Library server to serve archive publicly.&lt;/li&gt;
&lt;li&gt;Distributed p2p web browser on IPFS&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;What about streaming content?&lt;/h3&gt;
&lt;p&gt;The following are probably hard (and I haven't thought much about):&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Streaming content (audio, video)&lt;/li&gt;
&lt;li&gt;&quot;Impure&quot; request response pairs (such as if you call GET /endpoint 1 time you get &quot;A&quot;, if you call it a second time you get &quot;AA&quot;, and other examples like this).&lt;/li&gt;
&lt;li&gt;WebSockets (how to capture and replay that faithfully?)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Probably some way to do this tho.&lt;/p&gt;
&lt;h3&gt;Can I black list domains to not archive them?&lt;/h3&gt;
&lt;p&gt;Yes! Put any domains into &lt;code&gt;$HOME/22120-arc/no.json&lt;/code&gt;, eg:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-json&quot; readability=&quot;8&quot;&gt;
&lt;pre&gt;
[
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;*.google.com&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;*.cnn.co?&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Will not cache any resource with a host matching those. Wildcards:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;*&lt;/code&gt; (0 or more anything) and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;?&lt;/code&gt; (0 or 1 anything)&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Is there a DEBUG mode for troubleshooting?&lt;/h3&gt;
&lt;p&gt;Yes, just make sure you set an environment variable called &lt;code&gt;DEBUG_22120&lt;/code&gt; to anything non empty.&lt;/p&gt;
&lt;p&gt;So for example in posix systems:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;export&lt;/span&gt; DEBUG_22120=True
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Can I change the archive path?&lt;/h3&gt;
&lt;p&gt;Yes, there's a control for changing the archive path in the control page: &lt;a href=&quot;http://localhost:22120&quot; rel=&quot;nofollow&quot;&gt;http://localhost:22120&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Can I change this other thing?&lt;/h3&gt;
&lt;p&gt;There's a few command line arguments. You'll see the format printed as the first printed line when you start the program.&lt;/p&gt;
&lt;p&gt;For other things you can examine the source code.&lt;/p&gt;
</description>
<pubDate>Wed, 11 Nov 2020 16:15:20 +0000</pubDate>
<dc:creator>graderjs</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/22254235?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>c9fe/22120</og:title>
<og:url>https://github.com/c9fe/22120</og:url>
<og:description>:classical_building: 22120 - Self-host the Internet with an Offline Archive. Like binaries? https://github.com/dosyago/22120/releases Similar to ArchiveBox, SingleFile and WebMemex, but gooderer. -...</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/c9fe/22120</dc:identifier>
</item>
<item>
<title>Swiss report reveals new details on CIA spying operation</title>
<link>https://www.washingtonpost.com/national-security/swiss-report-reveals-new-details-on-cia-spying-operation/2020/11/10/c93ca7fc-2386-11eb-8672-c281c7a2c96e_story.html</link>
<guid isPermaLink="true" >https://www.washingtonpost.com/national-security/swiss-report-reveals-new-details-on-cia-spying-operation/2020/11/10/c93ca7fc-2386-11eb-8672-c281c7a2c96e_story.html</guid>
<description>&lt;div readability=&quot;11&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The report marks the culmination of a Swiss investigation launched after the history of the Crypto operation was revealed earlier this year by The Washington Post in collaboration with ZDF, German public television, and Swiss broadcaster SRF.&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;14&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The Crypto operation exploited “Switzerland’s image abroad as a neutral state,” according to the report, which also said that Swiss authorities had effectively allowed the CIA and its German counterpart, the BND, to carry out “intelligence operations to the detriment of other states by hiding behind a Swiss company.”&lt;/p&gt;
&lt;/div&gt;



&lt;div readability=&quot;11&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The probe marks the first public accounting by a foreign government of an espionage operation so successful and extensive that a classified CIA history referred to it as “the intelligence coup of the century.” The CIA did not respond to a request for comment, and the BND previously declined to comment.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;16&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;Based in Zug, Switzerland, Crypto was one of the world’s leading suppliers of encryption machines used by foreign governments to keep the communications of their spies, soldiers and diplomats secret. But the company was since the 1970s secretly owned by the CIA and the BND, and had clandestinely collaborated with the National Security Agency, the U.S. code-breaking service, beginning in the 1950s.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;15&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The CIA’s ownership of the company was shielded from public view through complex financial transactions orchestrated by a law firm in Liechtenstein. Under NSA and CIA control, Crypto sold rigged systems to more than 100 governments, including Iran, military juntas in Latin America, nuclear rivals India and Pakistan, and even the Vatican.&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;By exploiting hidden vulnerabilities in the machines’ algorithms, U.S. and German spies read the diplomatic cables and other communications of adversaries as well as some allies. The operation was known internally by code names including “Thesaurus” and “Rubicon.”&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;11&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;A detailed CIA history obtained by The Post depicted the program as a triumph of 20th-century espionage, marveling that “foreign governments were paying good money to the U.S. and [what was then] West Germany for the privilege of having their most secret communications read by at least two (and possibly as many as five or six) foreign countries.”&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;8&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;That line alluded to the sharing of intelligence gleaned from Crypto devices with allies including the United Kingdom.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;13&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The CIA history indicates that Swiss authorities were generally aware of, but not directly involved in, the operation. The Swiss report confirms aspects of that classified account but goes further in describing alleged Swiss complicity. Citing Swiss intelligence documents, the report indicates that the Swiss intelligence service was aware by 1993 that Crypto “belonged to foreign intelligence services and exported ‘vulnerable’ devices.”&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The report goes on to suggest that the Swiss spy agency, the Strategic Intelligence Service, entered a formal arrangement with the CIA that included access to other countries’ communications.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;11&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The SIS was able “to guarantee reliable access to this knowledge with the agreement of American intelligence services,” according to a translation of the Swiss report. The document does not provide more substantial detail on this arrangement. Swiss intelligence officials did not comment on the report.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;12&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The disclosures provide the latest glimpse into an espionage operation that has been the focus of intrigue and speculation almost since the company’s founding in the 1950s by a Swedish inventor, Boris Hagelin, who had made a fortune selling portable encryption machines to U.S. forces in World War II.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;9&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The findings make clear that Swiss authorities had suspicions about Crypto as early as the 1970s, when employees at the Zug-based company began coming forward with concerns about its activities.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;Only top Crypto executives were ever told about the company’s true ownership, according to CIA records. But employees in the engineering and research departments repeatedly identified vulnerabilities in the products’ designs that they were mysteriously prevented from fixing.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;8&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;Swiss authorities mounted several investigations of Crypto beginning in the 1970s that raised suspicions about Western intelligence involvement but could never confirm a CIA connection.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;12&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;Those suspicions became more acute in 1992, when a Crypto salesman, Hans Buehler, was arrested during a business trip to Iran and interrogated about the company’s products. He was held captive for nine months before Crypto secretly paid $1 million for his release.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;12&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The operation faced one of its gravest threats of exposure when Buehler returned and began speaking with news outlets about his ordeal and his Iranian interrogators’ questions. The Swiss report implies that the country’s intelligence service entered into a more formal, if passive, arrangement with the CIA in this time frame.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The report indicates that Swiss officials did not violate Swiss law by allowing the CIA-Crypto operation to continue, but concludes that doing so risked significant political and reputational damage to the country.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;9&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;The operation continued until 2018 when the CIA effectively shut down the program and sold Crypto’s headquarters building in Zug, as well as other assets.&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;12&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md&quot;&gt;Crypto’s international business was purchased by a Swedish entrepreneur, Andreas Linde, who said in an email exchange with The Post earlier this year that he was not aware of the CIA’s ownership when he purchased the assets. Export controls imposed by Swiss authorities in the aftermath of public disclosures about Crypto earlier this year have threatened the company’s survival.&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;font--body font-copy gray-darkest ma-0 pb-md italic&quot;&gt;Reporting for this article was done in collaboration with Peter F. Mueller, a journalist and documentary filmmaker based in Cologne, Germany.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Wed, 11 Nov 2020 14:44:42 +0000</pubDate>
<dc:creator>tpurves</dc:creator>
<og:type>article</og:type>
<og:url>https://www.washingtonpost.com/national-security/swiss-report-reveals-new-details-on-cia-spying-operation/2020/11/10/c93ca7fc-2386-11eb-8672-c281c7a2c96e_story.html</og:url>
<og:image>https://www.washingtonpost.com/wp-apps/imrs.php?src=https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/K4SKK2RDUAI6XBTSYKA4PIWJNY.jpg&amp;w=1440</og:image>
<og:title>Swiss report reveals new details on CIA spying operation</og:title>
<og:description>Investigators concluded that CIA involvement in Crypto AG, a company that made encryption machines, posed a threat to Swiss neutrality.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.washingtonpost.com/national-security/swiss-report-reveals-new-details-on-cia-spying-operation/2020/11/10/c93ca7fc-2386-11eb-8672-c281c7a2c96e_story.html</dc:identifier>
</item>
<item>
<title>Introduction to Linear Algebra for Applied Machine Learning with Python</title>
<link>https://pabloinsente.github.io/intro-linear-algebra</link>
<guid isPermaLink="true" >https://pabloinsente.github.io/intro-linear-algebra</guid>
<description>&lt;time datetime=&quot;2020-05-26T00:00:00+00:00&quot; class=&quot;by-line&quot;&gt;26 May 2020&lt;/time&gt;&lt;div class=&quot;sharebuttons&quot;&gt;
&lt;hr/&gt;&lt;ul&gt;&lt;li&gt;
&lt;p class=&quot;sharetitle&quot;&gt;Share this:&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;facebook&quot;/&gt;
&lt;li class=&quot;twitter&quot;/&gt;
&lt;li class=&quot;linkedin&quot;/&gt;
&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;Linear algebra is to machine learning as flour to bakery: &lt;strong&gt;every machine learning model is based in linear algebra, as every cake is based in flour&lt;/strong&gt;. It is not the only ingredient, of course. Machine learning models need vector calculus, probability, and optimization, as cakes need sugar, eggs, and butter. Applied machine learning, like bakery, is essentially about combining these mathematical ingredients in clever ways to create useful (tasty?) models.&lt;/p&gt;
&lt;p&gt;This document contains &lt;strong&gt;introductory level linear algebra notes for applied machine learning&lt;/strong&gt;. It is meant as a reference rather than a comprehensive review. If you ever get confused by matrix multiplication, don’t remember what was the $L_2$ norm, or the conditions for linear independence, this can serve as a quick reference. It also a good introduction for people that don’t need a deep understanding of linear algebra, but still want to learn about the fundamentals to read about machine learning or to use pre-packaged machine learning solutions. Further, it is a good source for people that learned linear algebra a while ago and need a refresher.&lt;/p&gt;
&lt;p&gt;These notes are based in a series of (mostly) freely available textbooks, video lectures, and classes I’ve read, watched and taken in the past. If you want to obtain a deeper understanding or to find exercises for each topic, you may want to consult those sources directly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Free resources&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Mathematics for Machine Learning&lt;/strong&gt; by Deisenroth, Faisal, and Ong. 1st Ed. &lt;a href=&quot;https://mml-book.github.io/&quot;&gt;Book link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Introduction to Applied Linear Algebra&lt;/strong&gt; by Boyd and Vandenberghe. 1sr Ed. &lt;a href=&quot;http://vmls-book.stanford.edu/&quot;&gt;Book link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra Ch. in Deep Learning&lt;/strong&gt; by Goodfellow, Bengio, and Courville. 1st Ed. &lt;a href=&quot;https://www.deeplearningbook.org/contents/linear_algebra.html&quot;&gt;Chapter link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Linear Algebra Ch. in Dive into Deep Learning&lt;/strong&gt; by Zhang, Lipton, Li, And Smola. &lt;a href=&quot;https://d2l.ai/chapter_preliminaries/linear-algebra.html&quot;&gt;Chapter link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Pavel Grinfeld’s Linear Algebra Lectures&lt;/strong&gt; at Lemma. &lt;a href=&quot;https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prof. Gilbert Strang’s Linear Algebra Lectures&lt;/strong&gt; at MIT. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Salman Khan’s Linear Algebra Lectures&lt;/strong&gt; at Khan Academy. &lt;a href=&quot;https://www.khanacademy.org/math/linear-algebra&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3blue1brown’s Linear Algebra Series&lt;/strong&gt; at YouTube. &lt;a href=&quot;https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&quot;&gt;Videos link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Not-free resources&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Introduction to Linear Algebra&lt;/strong&gt; by Gilbert Strang. 5th Ed. &lt;a href=&quot;https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775&quot;&gt;Book link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No Bullshit Guide to Linear Algebra&lt;/strong&gt; by Ivan Savov. 2nd Ed. &lt;a href=&quot;https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021&quot;&gt;Book Link&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I’ve consulted all these resources at one point or another. Pavel Grinfeld’s lectures are my absolute favorites. Salman Khan’s lectures are really good for absolute beginners (they are long though). The famous 3blue1brown series in linear algebra is delightful to watch and to get a solid high-level view of linear algebra.&lt;/p&gt;
&lt;p&gt;If you have to pic one book, I’d pic &lt;strong&gt;Boyd’s and Vandenberghe’s Intro to applied linear algebra&lt;/strong&gt;, as it is the most beginner friendly book on linear algebra I’ve encounter. Every aspect of the notation is clearly explained and pretty much all the key content for applied machine learning is covered. The Linear Algebra Chapter in Goodfellow et al is a nice and concise introduction, but it may require some previous exposure to linear algebra concepts. Deisenroth et all book is probably the best and most comprehensive source for linear algebra for machine learning I’ve found, although it assumes that you are good at reading math (and at math more generally). Savov’s book it’s also great for beginners but requires time to digest. Professor Strang lectures are great too but I won’t recommend it for absolute beginners.&lt;/p&gt;
&lt;p&gt;I’ll do my best to keep notation consistent. Nevertheless, learning to adjust to changing or inconsistent notation is a useful skill, since most authors will use their own preferred notation, and everyone seems to think that its/his/her own notation is better.&lt;/p&gt;
&lt;p&gt;To make everything more dynamic and practical, I’ll introduce bits of Python code to exemplify each mathematical operation (when possible) with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, which is the facto standard package for scientific computing in Python.&lt;/p&gt;
&lt;p&gt;Finally, keep in mind this is created by a non-mathematician for (mostly) non-mathematicians. I wrote this as if I were talking to myself or a dear friend, which explains why my writing is sometimes conversational and informal.&lt;/p&gt;
&lt;p&gt;If you find any mistake in notes feel free to reach me out at pcaceres@wisc.edu and to https://pablocaceres.org/ so I can correct the issue.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;em&gt;underlined sections&lt;/em&gt; are the newest sections and/or corrected ones.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#preliminary-concepts&quot;&gt;Preliminary concepts&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#vectors&quot;&gt;Vectors&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#matrices&quot;&gt;Matrices&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#linear-and-affine-mappings&quot;&gt;Linear and affine mappings&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#matrix-decompositions&quot;&gt;Matrix decompositions&lt;/a&gt;&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://pabloinsente.github.io/intro-linear-algebra#epilogue&quot;&gt;Epilogue&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;While writing about linear mappings, I realized the importance of having a basic understanding of a few concepts before approaching the study of linear algebra. If you are like me, you may not have formal mathematical training beyond high school. If so, I encourage you to read this section and spent some time wrapping your head around these concepts before going over the linear algebra content (otherwise, you might prefer to skip this part). I believe that reviewing these concepts is of great help to understand the &lt;em&gt;notation&lt;/em&gt;, which in my experience is one of the main barriers to understand mathematics for nonmathematicians: we are &lt;em&gt;non&lt;/em&gt;native speakers, so we are continuously building up our vocabulary. I’ll keep this section very short, as is not the focus of this mini-course.&lt;/p&gt;
&lt;p&gt;For this section, my notes are based on readings of:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Geometric transformations (Vol. 1)&lt;/strong&gt; (1966) by Modenov &amp;amp; Parkhomenko&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Naive Set Theory&lt;/strong&gt; (1960) by P.R. Halmos&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Abstract Algebra: Theory and Applications&lt;/strong&gt; (2016) by Judson &amp;amp; Beeer. &lt;a href=&quot;http://abstract.pugetsound.edu/download/aata-20160809.pdf&quot;&gt;Book link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;sets&quot;&gt;Sets&lt;/h2&gt;
&lt;p&gt;Sets are one of the most fundamental concepts in mathematics. They are so fundamental that they are not defined in terms of anything else. On the contrary, other branches of mathematics are defined in terms of sets, including linear algebra. Put simply, &lt;strong&gt;sets are well-defined collections of objects&lt;/strong&gt;. Such objects are called &lt;strong&gt;elements or members&lt;/strong&gt; of the set. The crew of a ship, a caravan of camels, and the LA Lakers roster, are all examples of sets. The captain of the ship, the first camel in the caravan, and LeBron James are all examples of “members” or “elements” of their corresponding sets. We denote a set with an upper case italic letter as $\textit{A}$. In the context of linear algebra, we say that a line is a set of points, and the set of all lines in the plane is a set of sets. Similarly, we can say that &lt;em&gt;vectors&lt;/em&gt; are sets of points, and &lt;em&gt;matrices&lt;/em&gt; sets of vectors.&lt;/p&gt;
&lt;h2 id=&quot;belonging-and-inclusion&quot;&gt;Belonging and inclusion&lt;/h2&gt;
&lt;p&gt;We build sets using the notion of &lt;strong&gt;belonging&lt;/strong&gt;. We denote that $a$ &lt;em&gt;belongs&lt;/em&gt; (or is an &lt;em&gt;element&lt;/em&gt; or &lt;em&gt;member&lt;/em&gt; of) to $\textit{A}$ with the Greek letter epsilon as:&lt;/p&gt;
&lt;p&gt;Another important idea is &lt;strong&gt;inclusion&lt;/strong&gt;, which allow us to build &lt;em&gt;subsets&lt;/em&gt;. Consider sets $\textit{A}$ and $\textit{B}$. When every element of $\textit{A}$ is an element of $\textit{B}$, we say that $\textit{A}$ is a &lt;em&gt;subset&lt;/em&gt; of $\textit{B}$, or that $\textit{B}$ &lt;em&gt;includes&lt;/em&gt; $\textit{A}$. The notation is:&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;Belonging and inclusion are derived from &lt;strong&gt;axion of extension&lt;/strong&gt;: &lt;em&gt;two sets are equal if and only if they have the same elements&lt;/em&gt;. This axiom may sound trivially obvious but is necessary to make belonging and inclusion rigorous.&lt;/p&gt;
&lt;h2 id=&quot;set-specification&quot;&gt;Set specification&lt;/h2&gt;
&lt;p&gt;In general, anything we assert about the elements of a set results in &lt;strong&gt;generating a subset&lt;/strong&gt;. In other words, asserting things about sets is a way to manufacture subsets. Take as an example the set of all dogs, that I’ll denote as $\textit{D}$. I can assert now “$d$ is black”. Such an assertion is true for some members of the set of all dogs and false for others. Hence, such a sentence, evaluated for &lt;em&gt;all&lt;/em&gt; member of $\textit{D}$, generates a subset: &lt;em&gt;the set of all black dogs&lt;/em&gt;. This is denoted as:&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;The colon ($:$) or vertical bar ($\vert$) read as “such that”. Therefore, we can read the above expression as: &lt;em&gt;all elements of $d$ in $\textit{D}$ such that $d$ is black&lt;/em&gt;. And that’s how we obtain the set $\textit{B}$ from $\textit{A}$.&lt;/p&gt;
&lt;p&gt;Set generation, as defined before, depends on the &lt;strong&gt;axiom of specification&lt;/strong&gt;: &lt;em&gt;to every set $\textit{A}$ and to every condition $\textit{S}(x)$ there corresponds a set $\textit{B}$ whose elements are exactly those elements $a \in \textit{A}$ for which $\textit{S}(x)$ holds.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A condition $\textit{S}(x)$ is any &lt;em&gt;sentence&lt;/em&gt; or &lt;em&gt;assertion&lt;/em&gt; about elements of $\textit{A}$. Valid sentences are either of &lt;em&gt;belonging&lt;/em&gt; or &lt;em&gt;equality&lt;/em&gt;. When we combine belonging and equality assertions with logic operators (not, if, and or, etc), we can build any legal set.&lt;/p&gt;
&lt;h2 id=&quot;ordered-pairs&quot;&gt;Ordered pairs&lt;/h2&gt;
&lt;p&gt;Pairs of sets come in two flavors: &lt;em&gt;unordered&lt;/em&gt; and &lt;em&gt;ordered&lt;/em&gt;. We care about pairs of sets as we need them to define a notion of relations and functions (from here I’ll denote sets with lower-case for convenience, but keep in mind we’re still talking about sets).&lt;/p&gt;
&lt;p&gt;Consider a pair of sets $\textit{x}$ and $\textit{y}$. An &lt;strong&gt;unordered pair&lt;/strong&gt; is a set whose elements are ${ \textit{x},\textit{y} }$, and ${ \textit{x},\textit{y} } = { \textit{y},\textit{x} } $. Therefore, presentation order does not matter, the set is the same.&lt;/p&gt;
&lt;p&gt;In machine learning, we usually do care about presentation order. For this, we need to define an &lt;strong&gt;ordered pair&lt;/strong&gt; (I’ll introduce this at an intuitive level, to avoid to introduce too many new concepts). An &lt;strong&gt;ordered pair&lt;/strong&gt; is denoted as $( \textit{x},\textit{y} )$, with $\textit{x}$ as the &lt;em&gt;first coordinate&lt;/em&gt; and $\textit{y}$ as the &lt;em&gt;second coordinate&lt;/em&gt;. A valid ordered pair has the property that $( \textit{x},\textit{y} ) \ne ( \textit{y},\textit{x} )$.&lt;/p&gt;
&lt;h2 id=&quot;relations&quot;&gt;Relations&lt;/h2&gt;
&lt;p&gt;From ordered pairs, we can derive the idea of &lt;strong&gt;relations&lt;/strong&gt; among sets or between elements and sets. Relations can be binary, ternary, quaternary, or N-ary. Here we are just concerned with binary relationships. In set theory, &lt;strong&gt;relations&lt;/strong&gt; are defined as &lt;em&gt;sets of ordered pairs&lt;/em&gt;, and denoted as $\textit{R}$. Hence, we can express the relation between $\textit{x}$ and $\textit{y}$ as:&lt;/p&gt;
&lt;p&gt;Further, for any $\textit{z} \in \textit{R}$, there exist $\textit{x}$ and $\textit{y}$ such that $\textit{z} = (\textit{x}, \textit{y})$.&lt;/p&gt;
&lt;p&gt;From the definition of $\textit{R}$, we can obtain the notions of &lt;strong&gt;domain&lt;/strong&gt; and &lt;strong&gt;range&lt;/strong&gt;. The &lt;strong&gt;domain&lt;/strong&gt; is a set defined as:&lt;/p&gt;
&lt;p&gt;This reads as: the values of $\textit{x}$ such that for at least one element of $\textit{y}$, $\textit{x}$ has a relation with $\textit{y}$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;range&lt;/strong&gt; is a set defined as:&lt;/p&gt;
&lt;p&gt;This reads: the set formed by the values of $\text{y}$ such that at least one element of $\textit{x}$, $\textit{x}$ has a relation with $\textit{y}$.&lt;/p&gt;
&lt;h2 id=&quot;functions&quot;&gt;Functions&lt;/h2&gt;
&lt;p&gt;Consider a pair of sets $\textit{X}$ and $\textit{Y}$. We say that a &lt;strong&gt;function&lt;/strong&gt; from $\textit{X}$ to $\textit{Y}$ is relation such that:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;$dom \textit{ f} = \textit{X}$ and&lt;/li&gt;
&lt;li&gt;such that for each $\textit{x} \in \textit{X}$ there is a unique element of $\textit{y} \in \textit{Y}$ with $(\textit{x}, \textit{y}) \in {f}$&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;More informally, we say that a function “&lt;em&gt;transform&lt;/em&gt;” or “&lt;em&gt;maps&lt;/em&gt;” or “&lt;em&gt;sends&lt;/em&gt;” $\textit{x}$ onto $\textit{y}$, and for each “&lt;em&gt;argument&lt;/em&gt;” $\textit{x}$ there is a unique value $\textit{y}$ that $\textit{f }$ “&lt;em&gt;assummes&lt;/em&gt;” or “&lt;em&gt;takes&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;We typically denote a relation or function or transformation or mapping from X onto Y as:&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;The simples way to see the effect of this definition of a function is with a chart. In &lt;strong&gt;Fig. 1&lt;/strong&gt;, the left-pane shows a valid function, i.e., each value $\textit{f}(\textit{x})$ &lt;em&gt;maps&lt;/em&gt; uniquely onto one value of $\textit{y}$. The right-pane is not a function, since each value $\textit{f}(\textit{x})$ &lt;em&gt;maps&lt;/em&gt; onto multiple values of $\textit{y}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 1: Functions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-function.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;For $\textit{f}: \textit{X} \rightarrow \textit{Y}$, the &lt;em&gt;domain&lt;/em&gt; of $\textit{f}$ equals to $\textit{X}$, but the &lt;em&gt;range&lt;/em&gt; does not necessarily equals to $\textit{Y}$. Just recall that the &lt;em&gt;range&lt;/em&gt; includes only the elements for which $\textit{Y}$ has a relation with $\textit{X}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The ultimate goal of machine learning is learning functions from data&lt;/strong&gt;, i.e., transformations or mappings from the &lt;em&gt;domain&lt;/em&gt; onto the &lt;em&gt;range&lt;/em&gt; of a function. This may sound simplistic, but it’s true. The &lt;em&gt;domain&lt;/em&gt; $\textit{X}$ is usually a vector (or set) of &lt;em&gt;variables&lt;/em&gt; or &lt;em&gt;features&lt;/em&gt; mapping onto a vector of &lt;em&gt;target&lt;/em&gt; values. Finally, I want to emphasize that in machine learning the words transformation and mapping are used interchangeably, but both just mean function.&lt;/p&gt;
&lt;p&gt;This is all I’ll cover about sets and functions. My goals were just to introduce: (1) &lt;strong&gt;the concept of a set&lt;/strong&gt;, (2) &lt;strong&gt;basic set notation&lt;/strong&gt;, (3) &lt;strong&gt;how sets are generated&lt;/strong&gt;, (4) &lt;strong&gt;how sets allow the definition of functions&lt;/strong&gt;, (5) &lt;strong&gt;the concept of a function&lt;/strong&gt;. Set theory is a monumental field, but there is no need to learn everything about sets to understand linear algebra. Halmo’s &lt;strong&gt;Naive set theory&lt;/strong&gt; (not free, but you can find a copy for ~\$8-$10 US) is a fantastic book for people that just need to understand the most fundamental ideas in a relatively informal manner.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;themes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dark'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;ThemeRegistry.enable('dark')
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Linear algebra is the study of vectors. At the most general level, vectors are &lt;strong&gt;ordered finite lists of numbers&lt;/strong&gt;. Vectors are the most fundamental mathematical object in machine learning. We use them to &lt;strong&gt;represent attributes of entities&lt;/strong&gt;: age, sex, test scores, etc. We represent vectors by a bold lower-case letter like $\bf{v}$ or as a lower-case letter with an arrow on top like $\vec{v}$.&lt;/p&gt;
&lt;p&gt;Vectors are a type of mathematical object that can be &lt;strong&gt;added together&lt;/strong&gt; and/or &lt;strong&gt;multiplied by a number&lt;/strong&gt; to obtain another object of &lt;strong&gt;the same kind&lt;/strong&gt;. For instance, if we have a vector $\bf{x} = \text{age}$ and a second vector $\bf{y} = \text{weight}$, we can add them together and obtain a third vector $\bf{z} = x + y$. We can also multiply $2 \times \bf{x}$ to obtain $2\bf{x}$, again, a vector. This is what we mean by &lt;em&gt;the same kind&lt;/em&gt;: the returning object is still a &lt;em&gt;vector&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;types-of-vectors&quot;&gt;Types of vectors&lt;/h2&gt;
&lt;p&gt;Vectors come in three flavors: (1) &lt;strong&gt;geometric vectors&lt;/strong&gt;, (2) &lt;strong&gt;polynomials&lt;/strong&gt;, (3) and &lt;strong&gt;elements of $\mathbb{R^n}$ space&lt;/strong&gt;. We will defined each one next.&lt;/p&gt;
&lt;h3 id=&quot;geometric-vectors&quot;&gt;Geometric vectors&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Geometric vectors are oriented segments&lt;/strong&gt;. Therse are the kind of vectors you probably learned about in high-school physics and geometry. Many linear algebra concepts come from the geometric point of view of vectors: space, plane, distance, etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 2: Geometric vectors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-geometric-vectors.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;polynomials&quot;&gt;Polynomials&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;A polynomial is an expression like $f(x) = x^2 + y + 1$&lt;/strong&gt;. This is, a expression adding multiple “terms” (nomials). Polynomials are vectors because they meet the definition of a vector: they can be added together to get another polynomial, and they can be multiplied together to get another polynomial.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig. 3: Polynomials&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-polynomials-vectors.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;elements-of-r&quot;&gt;Elements of R&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Elements of $\mathbb{R}^n$ are sets of real numbers&lt;/strong&gt;. This type of representation is arguably the most important for applied machine learning. It is how data is commonly represented in computers to build machine learning models. For instance, a vector in $\mathbb{R}^3$ takes the shape of:&lt;/p&gt;
&lt;p&gt;Indicating that it contains three dimensions.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; vectors are represented as n-dimensional arrays. To create a vector in $\mathbb{R^3}$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can inspect the vector shape by:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (3 dimensions, 1 element on each)
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'A 3-dimensional vector:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;A 3-dimensional vector:
[[1]
 [2]
 [3]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;zero-vector-unit-vector-and-sparse-vector&quot;&gt;Zero vector, unit vector, and sparse vector&lt;/h2&gt;
&lt;p&gt;There are a couple of “special” vectors worth to remember as they will be mentioned frequently on applied linear algebra: (1) zero vector, (2) unit vector, (3) sparse vectors&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zero vectors&lt;/strong&gt;, are vectors composed of zeros, and zeros only. It is common to see this vector denoted as simply $0$, regardless of the dimensionality. Hence, you may see a 3-dimensional or 10-dimensional with all entries equal to 0, refered as “the 0” vector. For instance:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unit vectors&lt;/strong&gt;, are vectors composed of a single element equal to one, and the rest to zero. Unit vectors are important to understand applications like norms. For instance, $\bf{x_1}$, $\bf{x_2}$, and $\bf{x_3}$ are unit vectors:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sparse vectors&lt;/strong&gt;, are vectors with most of its elements equal to zero. We denote the number of nonzero elements of a vector $\bf{x}$ as $nnz(x)$. The sparser possible vector is the zero vector. Sparse vectors are common in machine learning applications and often require some type of method to deal with them effectively.&lt;/p&gt;
&lt;h2 id=&quot;vector-dimensions-and-coordinate-system&quot;&gt;Vector dimensions and coordinate system&lt;/h2&gt;
&lt;p&gt;Vectors can have any number of dimensions. The most common are the 2-dimensional cartesian plane, and the 3-dimensional space. Vectors in 2 and 3 dimensions are used often for pedgagogical purposes since we can visualize them as geometric vectors. Nevetheless, most problems in machine learning entail more dimensions, sometiome hundreds or thousands of dimensions. The notation for a vector $\bf{x}$ of arbitrary dimensions, $n$ is:&lt;/p&gt;
&lt;p&gt;Vectors dimensions map into &lt;strong&gt;coordinate systems or perpendicular axes&lt;/strong&gt;. Coordinate systems have an origin at $(0,0,0)$, hence, when we define a vector:&lt;/p&gt;
&lt;p&gt;we are saying: starting from the origin, move 3 units in the 1st perpendicular axis, 2 units in the 2nd perpendicular axis, and 1 unit in the 3rd perpendicular axis. We will see later that when we have a set of perpendicular axes we obtain the basis of a vector space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 4: Coordinate systems&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-coordinate-system.svg&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;basic-vector-operations&quot;&gt;Basic vector operations&lt;/h2&gt;
&lt;h3 id=&quot;vector-vector-addition&quot;&gt;Vector-vector addition&lt;/h3&gt;
&lt;p&gt;We used vector-vector addition to define vectors without defining vector-vector addition. Vector-vector addition is an element-wise operation, only defined for vectors of the same size (i.e., number of elements). Consider two vectors of the same size, then:&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;p&gt;Vector addition has a series of &lt;strong&gt;fundamental properties&lt;/strong&gt; worth mentioning:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Commutativity: $x + y = y + x$&lt;/li&gt;
&lt;li&gt;Associativity: $(x + y) + z = x + (y + z)$&lt;/li&gt;
&lt;li&gt;Adding the zero vector has no effect: $x + 0 = 0 + x = x$&lt;/li&gt;
&lt;li&gt;Substracting a vector from itself returns the zero vector: $x - x = 0$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we add two vectors of the same with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; operator or the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;vector-scalar-multiplication&quot;&gt;Vector-scalar multiplication&lt;/h3&gt;
&lt;p&gt;Vector-scalar multiplication is an element-wise operation. It’s defined as:&lt;/p&gt;
&lt;p&gt;Consider $\alpha = 2$ and $\bf{x} = \begin{bmatrix} 1 \ 2 \ 3 \end{bmatrix}$:&lt;/p&gt;
&lt;p&gt;Vector-scalar multiplication satisfies a series of important properties:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Associativity: $(\alpha \beta) \bf{x} = \alpha (\beta \bf{x})$&lt;/li&gt;
&lt;li&gt;Left-distributive property: $(\alpha + \beta) \bf{x} = \alpha \bf{x} + \beta \bf{x}$&lt;/li&gt;
&lt;li&gt;Right-distributive property: $\bf{x} (\alpha + \beta) = \bf{x} \alpha + \bf{x} \beta$&lt;/li&gt;
&lt;li&gt;Right-distributive property for vector addition: $\alpha (\bf{x} + \bf{y}) = \alpha \bf{x} + \alpha \bf{y}$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we compute scalar-vector multiplication with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
             &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[2],
       [4],
       [6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;linear-combinations-of-vectors&quot;&gt;Linear combinations of vectors&lt;/h3&gt;
&lt;p&gt;There are only two legal operations with vectors in linear algebra: &lt;strong&gt;addition&lt;/strong&gt; and &lt;strong&gt;multiplication by numbers&lt;/strong&gt;. When we combine those, we get a &lt;strong&gt;linear combination&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Consider $\alpha = 2$, $\beta = 3$, $\bf{x}=\begin{bmatrix}2 \ 3\end{bmatrix}$, and $\begin{bmatrix}4 \ 5\end{bmatrix}$.&lt;/p&gt;
&lt;p&gt;We obtain:&lt;/p&gt;
&lt;p&gt;Another way to express linear combinations you’ll see often is with summation notation. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta_1, …, \beta_k \in \mathbb{R}$, then:&lt;/p&gt;
&lt;p&gt;Note that $:=$ means “&lt;em&gt;is defined as&lt;/em&gt;”.&lt;/p&gt;
&lt;p&gt;Linear combinations are the most fundamental operation in linear algebra. Everything in linear algebra results from linear combinations. For instance, linear regression is a linear combination of vectors. &lt;strong&gt;Fig. 2&lt;/strong&gt; shows an example of how adding two geometrical vectors looks like for intuition.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we do linear combinations as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[16],
       [21]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;vector-vector-multiplication-dot-product&quot;&gt;Vector-vector multiplication: dot product&lt;/h3&gt;
&lt;p&gt;We covered vector addition and multiplication by scalars. Now I will define vector-vector multiplication, commonly known as a &lt;strong&gt;dot product&lt;/strong&gt; or &lt;strong&gt;inner product&lt;/strong&gt;. The dot product of $\bf{x}$ and $\bf{y}$ is defined as:&lt;/p&gt;
&lt;p&gt;Where the $T$ superscript denotes the transpose of the vector. Transposing a vector just means to “flip” the column vector to a row vector counterclockwise. For instance:&lt;/p&gt;
&lt;p&gt;Dot products are so important in machine learning, that after a while they become second nature for practitioners.&lt;/p&gt;
&lt;p&gt;To multiply two vectors with dimensions (rows=2, cols=1) in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we need to transpose the first vector at using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[-14]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;vector-space-span-and-subspace&quot;&gt;Vector space, span, and subspace&lt;/h2&gt;
&lt;h3 id=&quot;vector-space&quot;&gt;Vector space&lt;/h3&gt;
&lt;p&gt;In its more general form, a &lt;strong&gt;vector space&lt;/strong&gt;, also known as &lt;strong&gt;linear space&lt;/strong&gt;, is a collection of objects that follow the rules defined for vectors in $\mathbb{R}^n$. We mentioned those rules when we defined vectors: they can be added together and multiplied by scalars, and return vectors of the same type. More colloquially, a vector space is the set of proper vectors and all possible linear combinatios of the vector set. In addition, vector addition and multiplication must follow these eight rules:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;commutativity: $x + y = y + x$&lt;/li&gt;
&lt;li&gt;associativity: $x + (y + x) = (y + x) + z$&lt;/li&gt;
&lt;li&gt;unique zero vector such that: $x + 0 = x$ $\forall$ $x$&lt;/li&gt;
&lt;li&gt;$\forall$ $x$ there is a unique vector $x$ such that $x + -x = 0$&lt;/li&gt;
&lt;li&gt;identity element of scalar multiplication: $1x = x$&lt;/li&gt;
&lt;li&gt;distributivity of scalar multiplication w.r.t vector addition: $x(y + z) = xz + zy$&lt;/li&gt;
&lt;li&gt;$x(yz) = (xy)z$&lt;/li&gt;
&lt;li&gt;$(y + z)x = yx + zx$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In my experience remembering these properties is not really important, but it’s good to know that such rules exist.&lt;/p&gt;
&lt;h3 id=&quot;vector-span&quot;&gt;Vector span&lt;/h3&gt;
&lt;p&gt;Consider the vectors $\bf{x}$ and $\bf{y}$ and the scalars $\alpha$ and $\beta$. If we take &lt;em&gt;all&lt;/em&gt; possible linear combinations of $\alpha \bf{x} + \beta \bf{y}$ we would obtain the &lt;strong&gt;span&lt;/strong&gt; of such vectors. This is easier to grasp when you think about geometric vectors. If our vectors $\bf{x}$ and $\bf{y}$ point into &lt;strong&gt;different directions&lt;/strong&gt; in the 2-dimensional space, we get that the $span(x,y)$ is equal to &lt;strong&gt;the entire 2-dimensional plane&lt;/strong&gt;, as shown in the middle-pane in &lt;strong&gt;Fig. 5&lt;/strong&gt;. Just imagine having an unlimited number of two types of sticks: one pointing vertically, and one pointing horizontally. Now, you can reach any point in the 2-dimensional space by simply combining the necessary number of vertical and horizontal sticks (including taking fractions of sticks).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 5: Vector Span&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-vector-span.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;What would happen if the vectors point in the same direction? Now, if you combine them, you just can &lt;strong&gt;span a line&lt;/strong&gt;, as shown in the left-pane in &lt;strong&gt;Fig. 5&lt;/strong&gt;. If you have ever heard of the term “multicollinearity”, it’s closely related to this issue: when two variables are “colinear” they are pointing in the same direction, hence they provide redundant information, so can drop one without information loss.&lt;/p&gt;
&lt;p&gt;With three vectors pointing into different directions, we can span the entire 3-dimensional space or a &lt;strong&gt;hyper-plane&lt;/strong&gt;, as in the right-pane of &lt;strong&gt;Fig. 5&lt;/strong&gt;. Note that the sphere is just meant as a 3-D reference, not as a limit.&lt;/p&gt;
&lt;p&gt;Four vectors pointing into different directions will span the 4-dimensional space, and so on. From here our geometrical intuition can’t help us. This is an example of how linear algebra can describe the behavior of vectors beyond our basics intuitions.&lt;/p&gt;
&lt;h3 id=&quot;vector-subspaces&quot;&gt;Vector subspaces&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;vector subspace (or linear subspace) is a vector space that lies within a larger vector space&lt;/strong&gt;. These are also known as linear subspaces. Consider a subspace $S$. For a vector to be a valid subspace it has to meet &lt;strong&gt;three conditions&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Contains the zero vector, $\bf{0} \in S$&lt;/li&gt;
&lt;li&gt;Closure under multiplication, $\forall \alpha \in \mathbb{R} \rightarrow \alpha \times s_i \in S$&lt;/li&gt;
&lt;li&gt;Closure under addition, $\forall s_i \in S \rightarrow s_1 + s_2 \in S$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Intuitively, you can think in closure as being unable to “jump out” from space into another. A pair of vectors laying flat in the 2-dimensional space, can’t, by either addition or multiplication, “jump out” into the 3-dimensional space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 6: Vector subspaces&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-vector-subspace.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Consider the following questions: Is $\bf{x}=\begin{bmatrix} 1 \ 1 \end{bmatrix}$ a valid subspace of $\mathbb{R^2}$? Let’s evaluate $\bf{x}$ on the three conditions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Contains the zero vector&lt;/strong&gt;: it does. Remember that the span of a vector are all linear combinations of such a vector. Therefore, we can simply multiply by $0$ to get $\begin{bmatrix}0 \ 0 \end{bmatrix}$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closure under multiplication&lt;/strong&gt; implies that if take any vector belonging to $\bf{x}$ and multiply by any real scalar $\alpha$, the resulting vector stays within the span of $\bf{x}$. Algebraically is easy to see that we can multiply $\begin{bmatrix} 1 \ 1 \end{bmatrix}$ by any scalar $\alpha$, and the resulting vector remains in the 2-dimensional plane (i.e., the span of $\mathbb{R}^2$).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closure under addition&lt;/strong&gt; implies that if we add together any vectors belonging to $\bf{x}$, the resulting vector remains within the span of $\mathbb{R}^2$. Again, algebraically is clear that if we add $\bf{x}$ + $\bf{x}$, the resulting vector will remain in $\mathbb{R}^2$. There is no way to get to $\mathbb{R^3}$ or $\mathbb{R^4}$ or any space outside the two-dimensional plane by adding $\bf{x}$ multiple times.&lt;/p&gt;
&lt;h2 id=&quot;linear-dependence-and-independence&quot;&gt;Linear dependence and independence&lt;/h2&gt;
&lt;p&gt;The left-pane shows a triplet of &lt;strong&gt;linearly dependent&lt;/strong&gt; vectors, whereas the right-pane shows a triplet of &lt;strong&gt;linearly independent&lt;/strong&gt; vectors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 7: Linear dependence and independence&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-linear-independence.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;A set of vectors is &lt;strong&gt;linearly dependent&lt;/strong&gt; if at least one vector can be obtained as a linear combination of other vectors in the set. As you can see in the left pane, we can combine vectors $x$ and $y$ to obtain $z$.&lt;/p&gt;
&lt;p&gt;There is more rigurous (but slightly harder to grasp) definition of linear dependence. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta \in \mathbb{R}$. If there is a way to get $0 = \sum_{i=1}^k \beta_i x_i$ with at least one $\beta \ne 0$, we have linearly dependent vectors. In other words, if we can get the zero vector as a linear combination of the vectors in the set, with weights that &lt;em&gt;are not&lt;/em&gt; all zero, we have a linearly dependent set.&lt;/p&gt;
&lt;p&gt;A set of vectors is &lt;strong&gt;linearly independent&lt;/strong&gt; if none vector can be obtained as a linear combination of other vectors in the set. As you can see in the right pane, there is no way for us to combine vectors $x$ and $y$ to obtain $z$. Again, consider a set of vectors $x_1, …, x_k$ and scalars $\beta \in \mathbb{R}$. If the only way to get $0 = \sum_{i=1}^k \beta_i x_i$ requires all $\beta_1, …, \beta_k = 0$, the we have linearly independent vectors. In words, the only way to get the zero vectors in by multoplying each vector in the set by $0$.&lt;/p&gt;
&lt;p&gt;The importance of the concepts of linear dependence and independence will become clearer in more advanced topics. For now, the important points to remember are: linearly dependent vectors contain &lt;strong&gt;redundant information&lt;/strong&gt;, whereas linearly independent vectors do not.&lt;/p&gt;
&lt;h2 id=&quot;vector-null-space&quot;&gt;Vector null space&lt;/h2&gt;
&lt;p&gt;Now that we know what subspaces and linear dependent vectors are, we can introduce the idea of the &lt;strong&gt;null space&lt;/strong&gt;. Intuitively, the null space of a set of vectors are &lt;strong&gt;all linear combinations that “map” into the zero vector&lt;/strong&gt;. Consider a set of geometric vectors $\bf{w}$, $\bf{x}$, $\bf{y}$, and $\bf{z}$ as in &lt;strong&gt;Fig. 8&lt;/strong&gt;. By inspection, we can see that vectors $\bf{x}$ and $\bf{z}$ are parallel to each other, hence, independent. On the contrary, vectors $\bf{w}$ and $\bf{y}$ can be obtained as linear combinations of $\bf{x}$ and $\bf{z}$, therefore, dependent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 8: Vector null space&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-vector-null-space.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;As result, with this four vectors, we can form the following two combinations that will “map” into the origin of the coordinate system, this is, the zero vector $(0,0)$:&lt;/p&gt;
&lt;p&gt;We will see how this idea of the null space extends naturally in the context of matrices later.&lt;/p&gt;
&lt;h2 id=&quot;vector-norms&quot;&gt;Vector norms&lt;/h2&gt;
&lt;p&gt;Measuring vectors is another important operation in machine learning applications. Intuitively, we can think about the &lt;strong&gt;norm&lt;/strong&gt; or the &lt;strong&gt;length&lt;/strong&gt; of a vector as the distance between its “origin” and its “end”.&lt;/p&gt;
&lt;p&gt;Norms “map” vectors to non-negative values. In this sense are functions that assign length $\lVert \bf{x} \rVert \in \mathbb{R^n}$ to a vector $\bf{x}$. To be valid, a norm has to satisfy these properties (keep in mind these properties are a bit abstruse to understand):&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Absolutely homogeneous&lt;/strong&gt;: $\forall \alpha \in \mathbb{R}, \lVert \alpha \bf{x} \rVert = \vert \alpha \Vert \lVert \bf{x} \rVert$. In words: for all real-valued scalars, the norm scales proportionally with the value of the scalar.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Triangle inequality&lt;/strong&gt;: $\lVert \bf{x} + \bf{y} \rVert \le \lVert \bf{x} \rVert + \lVert \bf{y} \rVert $. In words: in geometric terms, for any triangle the sum of any two sides must be greater or equal to the lenght of the third side. This is easy to see experimentally: grab a piece of rope, form triangles of different sizes, measure all the sides, and test this property.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Positive definite&lt;/strong&gt;: $\lVert \bf{x} \rVert \ge 0$ and $ \lVert \bf{x} \rVert = 0 \Longleftrightarrow \bf{x}= 0$. In words: the length of any $\bf{x}$ has to be a positive value (i.e., a vector can’t have negative length), and a length of $0$ occurs only of $\bf{x}=0$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Grasping the meaning of these three properties may be difficult at this point, but they probably become clearer as you improve your understanding of linear algebra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 9: Vector norms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-l2-norm.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;euclidean-norm&quot;&gt;Euclidean norm&lt;/h3&gt;
&lt;p&gt;The Euclidean norm is one of the most popular norms in machine learning. It is so widely used that sometimes is refered simply as “the norm” of a vector. Is defined as:&lt;/p&gt;
&lt;p&gt;Hence, in &lt;strong&gt;two dimensions&lt;/strong&gt; the $L_2$ norm is:&lt;/p&gt;
&lt;p&gt;Which is equivalent to the formula for the hypotenuse a triangle with sides $x_1^2$ and $x_2^2$.&lt;/p&gt;
&lt;p&gt;The same pattern follows for higher dimensions of $\mathbb{R^n}$&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the $L_2$ norm as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;If you remember the first “Pythagorean triple”, you can confirm that the norm is correct.&lt;/p&gt;
&lt;h3 id=&quot;manhattan-norm&quot;&gt;Manhattan norm&lt;/h3&gt;
&lt;p&gt;The Manhattan or $L_1$ norm gets its name in analogy to measuring distances while moving in Manhattan, NYC. Since Manhattan has a grid-shape, the distance between any two points is measured by moving in vertical and horizontals lines (instead of diagonals as in the Euclidean norm). It is defined as:&lt;/p&gt;
&lt;p&gt;Where $\vert x_i \vert$ is the absolute value. The $L_1$ norm is preferred when discriminating between elements that are exactly zero and elements that are small but not zero.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; we compute the $L_1$ norm as&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Is easy to confirm that the sum of the absolute values of $3$ and $-4$ is $7$.&lt;/p&gt;
&lt;h3 id=&quot;max-norm&quot;&gt;Max norm&lt;/h3&gt;
&lt;p&gt;The max norm or infinity norm is simply the absolute value of the largest element in the vector. It is defined as:&lt;/p&gt;
&lt;p&gt;Where $\vert x_i \vert$ is the absolute value. For instance, for a vector with elements $\bf{x} = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \end{bmatrix}$, the $\lVert \bf{x} \rVert_\infty = 3$&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; we compute the $L_\infty$ norm as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;vector-inner-product-length-and-distance&quot;&gt;Vector inner product, length, and distance.&lt;/h2&gt;
&lt;p&gt;For practical purposes, inner product and length are used as equivalent to dot product and norm, although technically are not the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inner products&lt;/strong&gt; are a more general concept that dot products, with a series of additional properties (see &lt;a href=&quot;https://en.wikipedia.org/wiki/Inner_product_space#Elementary_properties&quot;&gt;here&lt;/a&gt;). In other words, every dot product is an inner product, but not every inner product is a dot product. The notation for the inner product is usually a pair of angle brackets as $\langle .,. \rangle$ as. For instance, the scalar inner product is defined as:&lt;/p&gt;
&lt;p&gt;In $\mathbb{R}^n$ the inner product is a dot product defined as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Length&lt;/strong&gt; is a concept from geometry. We say that geometric vectors have length and that vectors in $\mathbb{R}^n$ have norm. In practice, many machine learning textbooks use these concepts interchangeably. I’ve found authors saying things like “we use the $l_2$ norm to compute the &lt;em&gt;length&lt;/em&gt; of a vector”. For instance, we can compute the length of a directed segment (i.e., geometrical vector) $\bf{x}$ by taking the square root of the inner product with itself as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distance&lt;/strong&gt; is a relational concept. It refers to the length (or norm) of the difference between two vectors. Hence, we use norms and lengths to measure the distance between vectors. Consider the vectors $\bf{x}$ and $\bf{y}$, we define the distance $d(x,y)$ as:&lt;/p&gt;
&lt;p&gt;When the inner product $\langle x - y, x - y \rangle$ is the dot product, the distance equals to the Euclidean distance.&lt;/p&gt;
&lt;p&gt;In machine learning, unless made explicit, we can safely assume that an inner product refers to the dot product. We already reviewed how to compute the dot product in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; 
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[-14]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As with the inner product, usually, we can safely assume that &lt;strong&gt;distance&lt;/strong&gt; stands for the Euclidean distance or $L_2$ norm unless otherwise noted. To compute the $L_2$ distance between a pair of vectors:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'L_2 distance : &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;L_2 distance : 7.810249675906656
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;vector-angles-and-orthogonality&quot;&gt;Vector angles and orthogonality&lt;/h2&gt;
&lt;p&gt;The concepts of angle and orthogonality are also related to geometrical vectors. We saw that inner products allow for the definition of length and distance. In the same manner, inner products are used to define &lt;strong&gt;angles&lt;/strong&gt; and &lt;strong&gt;orthogonality&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In machine learning, the &lt;strong&gt;angle&lt;/strong&gt; between a pair of vectors is used as a &lt;strong&gt;measure of vector similarity&lt;/strong&gt;. To understand angles let’s first look at the &lt;strong&gt;Cauchy–Schwarz inequality&lt;/strong&gt;. Consider a pair of non-zero vectors $\bf{x}$ and $\bf{y}$ $\in \mathbb{R}^n$. The Cauchy–Schwarz inequality states that:&lt;/p&gt;
&lt;p&gt;In words: &lt;em&gt;the absolute value of the inner product of a pair of vectors is less than or equal to the products of their length&lt;/em&gt;. The only case where both sides of the expression are &lt;em&gt;equal&lt;/em&gt; is when vectors are colinear, for instance, when $\bf{x}$ is a scaled version of $\bf{y}$. In the 2-dimensional case, such vectors would lie along the same line.&lt;/p&gt;
&lt;p&gt;The definition of the angle between vectors can be thought as a generalization of the &lt;strong&gt;law of cosines&lt;/strong&gt; in trigonometry, which defines for a triangle with sides $a$, $b$, and $c$, and an angle $\theta$ are related as:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 10: Law of cosines and Angle between vectors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-vector-angle.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;We can replace this expression with vectors lengths as:&lt;/p&gt;
&lt;p&gt;With a bit of algebraic manipulation, we can clear the previous equation to:&lt;/p&gt;
&lt;p&gt;And there we have a &lt;strong&gt;definition for (cos) angle $\theta$&lt;/strong&gt;. Further, from the Cauchy–Schwarz inequality we know that $\cos \theta$ must be:&lt;/p&gt;
&lt;p&gt;This is a necessary conclusion (range between ${-1, 1}$) since the numerator in the equation always is going to be smaller or equal to the denominator.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the $\cos \theta$ between a pair of vectors as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;16&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# here we translate the cos(theta) definition
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'cos of the angle = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;cos of the angle = [[0.988]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We get that $\cos \theta \approx 0.988$. Finally, to know the exact value of $\theta$ we need to take the trigonometric inverse of the cosine function as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arccos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'angle in radiants = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;angle in radiants = [[0.157]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We obtain $\theta \approx 0.157 $. To fo from radiants to degrees we can use the following formula:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'angle in degrees = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;angle in degrees = [[8.973]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We obtain $\theta \approx 8.973^{\circ}$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Orthogonality&lt;/strong&gt; is often used interchangeably with “independence” although they are mathematically different concepts. Orthogonality can be seen as a generalization of perpendicularity to vectors in any number of dimensions.&lt;/p&gt;
&lt;p&gt;We say that a pair of vectors $\bf{x}$ and $\bf{y}$ are &lt;strong&gt;orthogonal&lt;/strong&gt; if their inner product is zero, $\langle x,y \rangle = 0$. The notation for a pair of orthogonal vectors is $\bf{x} \perp \bf{y}$. In the 2-dimensional plane, this equals to a pair of vectors forming a $90^{\circ}$ angle.&lt;/p&gt;
&lt;p&gt;Here is an example of orthogonal vectors&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 11: Orthogonal vectors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-orthogonal-vectors.svg&quot;/&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'cos of the angle = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;cos of the angle = [[0.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We see that this vectors are &lt;strong&gt;orthogonal&lt;/strong&gt; as $\cos \theta=0$. This is equal to $\approx 1.57$ radiants and $\theta = 90^{\circ}$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arccos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'angle in radiants = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos_inverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;angle in degrees =&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;angle in radiants = [[1.571]]
angle in degrees =[[90.]] 
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;systems-of-linear-equations&quot;&gt;Systems of linear equations&lt;/h2&gt;
&lt;p&gt;The purpose of linear algebra as a tool is to &lt;strong&gt;solve systems of linear equations&lt;/strong&gt;. Informally, this means to figure out the right combination of linear segments to obtain an outcome. Even more informally, think about making pancakes: In what proportion ($w_i \in \mathbb{R}$) we have to mix ingredients to make pancakes? You can express this as a linear equation:&lt;/p&gt;
&lt;p&gt;The above expression describe &lt;em&gt;a&lt;/em&gt; linear equation. A &lt;em&gt;system&lt;/em&gt; of linear equations involve multiple equations that have to be solved &lt;strong&gt;simultaneously&lt;/strong&gt;. Consider:&lt;/p&gt;
&lt;p&gt;Now we have a system with two unknowns, $x$ and $y$. We’ll see general methods to solve systems of linear equations later. For now, I’ll give you the answer: $x=2$ and $y=3$. Geometrically, we can see that both equations produce a straight line in the 2-dimensional plane. The point on where both lines encounter is the solution to the linear system.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;y1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;x2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;y2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;equation1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;y1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;equation2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;x2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;y2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;equation1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;equation2&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Matrices are as fundamental as vectors in machine learning. With vectors, we can represent single variables as sets of numbers or instances. With matrices, we can represent sets of variables. In this sense, a matrix is simply an ordered &lt;strong&gt;collection of vectors&lt;/strong&gt;. Conventionally, column vectors, but it’s always wise to pay attention to the authors’ notation when reading matrices. Since computer screens operate in two dimensions, matrices are the way in which we interact with data in practice.&lt;/p&gt;
&lt;p&gt;More formally, we represent a matrix with a italicized upper-case letter like $\textit{A}$. In two dimensions, we say the matrix $\textit{A}$ has $m$ rows and $n$ columns. Each entry of $\textit{A}$ is defined as $a_{ij}$, $i=1,…, m,$ and $j=1,…,n$. A matrix $\textit{A} \in \mathbb{R^{m\times n}}$ is defines as:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we construct matrices with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;array&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 1st row
&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2nd row
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'a 2x2 Matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;a 2x2 Matrix:
[[0 2]
 [1 4]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;basic-matrix-operations&quot;&gt;Basic Matrix operations&lt;/h2&gt;
&lt;h3 id=&quot;matrix-matrix-addition&quot;&gt;Matrix-matrix addition&lt;/h3&gt;
&lt;p&gt;We add matrices in a element-wise fashion. The sum of $\textit{A} \in \mathbb{R}^{m\times n}$ and $\textit{B} \in \mathbb{R}^{m\times n}$ is defined as:&lt;/p&gt;
&lt;p&gt;For instance: &lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we add matrices with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 3,  3],
       [-2,  6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 3,  3],
       [-2,  6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;matrix-scalar-multiplication&quot;&gt;Matrix-scalar multiplication&lt;/h3&gt;
&lt;p&gt;Matrix-scalar multiplication is an element-wise operation. Each element of the matrix $\textit{A}$ is multiplied by the scalar $\alpha$. Is defined as:&lt;/p&gt;
&lt;p&gt;Consider $\alpha=2$ and $\textit{A}=\begin{bmatrix}1 &amp;amp; 2 \ 3 &amp;amp; 4\end{bmatrix}$, then:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we compute matrix-scalar multiplication with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multiply&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[2, 4],
       [6, 8]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[2, 4],
       [6, 8]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;matrix-vector-multiplication-dot-product&quot;&gt;Matrix-vector multiplication: dot product&lt;/h3&gt;
&lt;p&gt;Matrix-vector multiplication equals to taking the dot product of each column $n$ of a $\textit{A}$ with each element $\bf{x}$ resulting in a vector $\bf{y}$. Is defined as:&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;p&gt;In numpy, we compute the matrix-vector product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dot&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[4],
       [9]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[4],
       [9]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;matrix-matrix-multiplication&quot;&gt;Matrix-matrix multiplication&lt;/h3&gt;
&lt;p&gt;Matrix-matrix multiplication is a dot produt as well. To work, the number of columns in the first matrix $\textit{A}$ has to be equal to the number of rows in the second matrix $\textit{B}$. Hence, $\textit{A} \in \mathbb{R^{m\times n}}$ times $\textit{B} \in \mathbb{R^{n\times p}}$ to be valid. One way to see matrix-matrix multiplication is by taking a series of dot products: the 1st column of $\textit{A}$ times the 1st row of $\textit{B}$, the 2nd column of $\textit{A}$ times the 2nd row of $\textit{B}$, until the $n_{th}$ column of $\textit{A}$ times the $n_{th}$ row of $\textit{B}$.&lt;/p&gt;
&lt;p&gt;We define $\textit{A} \in \mathbb{R^{n\times p}} \cdot \textit{B} \in \mathbb{R^{n\times p}} = \textit{C} \in \mathbb{R^{m\times p}}$:&lt;/p&gt;
&lt;p&gt;A compact way to define the matrix-matrix product is:&lt;/p&gt;
&lt;p&gt;For instance&lt;/p&gt;
&lt;p&gt;Matrix-matrix multiplication has a series of important properties:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Associativity: $(\textit{A}\textit{B}) \textit{C} = \textit{A}(\textit{B}\textit{C})$&lt;/li&gt;
&lt;li&gt;Associativity with scalar multiplication: $\alpha (\textit{A}\textit{B}) = (\alpha \textit{A}) \textit{B}$&lt;/li&gt;
&lt;li&gt;Distributivity with addition: $\textit{A}(\textit{B}+\textit{C}) = A+B + AC$&lt;/li&gt;
&lt;li&gt;Transpose of product: $(\textit{A}\textit{B})^T = \textit{B}^T\textit{A}^T$&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;It’s also important to remember that &lt;strong&gt;matrix-matrix multiplication orders matter&lt;/strong&gt;, this is, it is &lt;strong&gt;not commutative&lt;/strong&gt;. Hence, in general, $AB \ne BA$.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we obtan the matrix-matrix product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dot&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[4, 2],
       [9, 7]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[4, 2],
       [9, 7]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;matrix-identity&quot;&gt;Matrix identity&lt;/h3&gt;
&lt;p&gt;An identity matrix is a square matrix with ones on the diagonal from the upper left to the bottom right, and zeros everywhere else. We denote the identity matrix as $\textit{I}_n$. We define $\textit{I} \in \mathbb{R}^{n \times n}$ as:&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;p&gt;You can think in the inverse as playing the same role than $1$ in operations with real numbers. The inverse matrix does not look very interesting in itself, but it plays an important role in some proofs and for the inverse matrix (which can be used to solve system of linear equations).&lt;/p&gt;
&lt;h3 id=&quot;matrix-inverse&quot;&gt;Matrix inverse&lt;/h3&gt;
&lt;p&gt;In the context of real numbers, the &lt;em&gt;multiplicative inverse (or reciprocal)&lt;/em&gt; of a number $x$, is the number that when multiplied by $x$ yields $1$. We denote this by $x^{-1}$ or $\frac{1}{x}$. Take the number $5$. Its multiplicative inverse equals to $5 \times \frac{1}{5} = 1$.&lt;/p&gt;
&lt;p&gt;If you recall the matrix identity section, we said that the identity plays a similar role than the number one but for matrices. Again, by analogy, we can see the &lt;em&gt;inverse&lt;/em&gt; of a matrix as playing the same role than the multiplicative inverse for numbers but for matrices. Hence, the &lt;em&gt;inverse matrix&lt;/em&gt; is a matrix than when multiplies another matrix &lt;em&gt;from either the right or the left side&lt;/em&gt;, returns the identity matrix.&lt;/p&gt;
&lt;p&gt;More formally, consider the square matrix $\textit{A} \in \mathbb{R}^{n \times n}$. We define $\textit{A}^{-1}$ as matrix with the property:&lt;/p&gt;
&lt;p&gt;The main reason we care about the inverse, is because it allows to &lt;strong&gt;solve systems of linear equations&lt;/strong&gt; in certain situations. Consider a system of linear equations as:&lt;/p&gt;
&lt;p&gt;Assuming $\textit{A}$ has an inverse, we can multiply by the inverse on both sides:&lt;/p&gt;
&lt;p&gt;And get:&lt;/p&gt;
&lt;p&gt;Since the $\textit{I}$ does not affect $\bf{x}$ at all, our final expression becomes:&lt;/p&gt;
&lt;p&gt;This means that we just need to know the inverse of $\textit{A}$, multiply by the target vector $\bf{y}$, and we obtain the solution for our system. I mentioned that this works only in &lt;em&gt;certain situations&lt;/em&gt;. By this I meant: &lt;strong&gt;if and only if $\textit{A}$ happens to have an inverse&lt;/strong&gt;. Not all matrices have an inverse. When $\textit{A}^{-1}$ exist, we say $\textit{A}$ is &lt;em&gt;nonsingular&lt;/em&gt; or &lt;em&gt;invertible&lt;/em&gt;, otherwise, we say it is &lt;em&gt;noninvertible&lt;/em&gt; or &lt;em&gt;singular&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The lingering question is how to find the inverse of a matrix. We can do it by reducing $\textit{A}$ to its &lt;em&gt;reduced row echelon form&lt;/em&gt; by using Gauss-Jordan Elimination. If $\textit{A}$ has an inverse, we will obtain the identity matrix as the row echelon form of $\textit{A}$. I haven’t introduced either just yet. You can jump to the &lt;em&gt;Solving systems of linear equations with matrices&lt;/em&gt; if you are eager to learn about it now. For now, we relie on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the inverse of a matrix with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.linalg.inv&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'A inverse:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;A inverse:
[[-7. -7.  6.]
 [ 2.  1. -1.]
 [ 4.  5. -4.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can check the $\textit{A}^{-1}$ is correct by multiplying. If so, we should obtain the identity $\textit{I}_3$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'A_i times A resulsts in I_3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;A_i times A resulsts in I_3:
[[ 1.  0.  0.]
 [ 0.  1. -0.]
 [ 0. -0.  1.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;matrix-transpose&quot;&gt;Matrix transpose&lt;/h3&gt;
&lt;p&gt;Consider a matrix $\textit{A} \in \mathbb{R}^{m \times n}$. The &lt;strong&gt;transpose&lt;/strong&gt; of $\textit{A}$ is denoted as $\textit{A}^T \in \mathbb{R}^{m \times n}$. We obtain $\textit{A}^T$ as:&lt;/p&gt;
&lt;p&gt;In other words, we get the $\textit{A}^T$ by switching the columns by the rows of $\textit{A}$. For instance:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we obtain the transpose with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;33.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[1, 3, 5],
       [2, 4, 6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;hadamard-product&quot;&gt;Hadamard product&lt;/h3&gt;
&lt;p&gt;It is tempting to think in matrix-matrix multiplication as an element-wise operation, as multiplying each overlapping element of $\textit{A}$ and $\textit{B}$. &lt;em&gt;It is not&lt;/em&gt;. Such operation is called &lt;strong&gt;Hadamard product&lt;/strong&gt;. I’m introducing this to avoid confusion. The Hadamard product is defined as&lt;/p&gt;
&lt;p&gt;For instance:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;numpy&lt;/code&gt;, we compute the Hadamard product with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; operator or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multiply&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[0, 6],
       [2, 4]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[0, 6],
       [2, 4]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;special-matrices&quot;&gt;Special matrices&lt;/h2&gt;
&lt;p&gt;There are several matrices with special names that are commonly found in machine learning theory and applications. Knowing these matrices beforehand can improve your linear algebra fluency, so we will briefly review a selection of 12 common matrices. For an extended list of special matrices see &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_named_matrices&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/special.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;rectangular-matrix&quot;&gt;Rectangular matrix&lt;/h3&gt;
&lt;p&gt;Matrices are said to be &lt;em&gt;rectangular&lt;/em&gt; when the number of rows is $\ne$ to the number of columns, i.e., $\textit{A}^{m \times n}$ with $m \ne n$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;square-matrix&quot;&gt;Square matrix&lt;/h3&gt;
&lt;p&gt;Matrices are said to be &lt;strong&gt;square&lt;/strong&gt; when the number of rows $=$ the number of columns, i.e., $\textit{A}^{n \times n}$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;diagonal-matrix&quot;&gt;Diagonal matrix&lt;/h3&gt;
&lt;p&gt;Square matrices are said to be &lt;strong&gt;diagonal&lt;/strong&gt; when each of its non-diagonal elements is zero, i.e., For $\textit{D} = (d_{i,j})$, we have $\forall i,j \in n, i \ne j \implies d_{i,j} = 0$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;upper-triangular-matrix&quot;&gt;Upper triangular matrix&lt;/h3&gt;
&lt;p&gt;Square matrices are said to be &lt;strong&gt;upper triangular&lt;/strong&gt; when the elements below the main diagonal are zero, i.e., For $\textit{D} = (d_{i,j})$, we have $d_{i,j} = 0, \text{for } i&amp;gt;j$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;lower-triangular-matrix&quot;&gt;Lower triangular matrix&lt;/h3&gt;
&lt;p&gt;Square matrices are said to be &lt;strong&gt;lower triangular&lt;/strong&gt; when the elements above the main diagonal are zero, i.e., For $\textit{D} = (d_{i,j})$, we have $d_{i,j} = 0, \text{for } i&amp;lt;j$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;symmetric-matrix&quot;&gt;Symmetric matrix&lt;/h3&gt;
&lt;p&gt;Square matrices are said to be symmetric its equal to its transpose, i.e., $\textit{A} = \textit{A}^T$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;identity-matrix&quot;&gt;Identity matrix&lt;/h3&gt;
&lt;p&gt;A diagonal matrix is said to be the identity when the elements along its main diagonal are equal to one. For instance:&lt;/p&gt;
&lt;h3 id=&quot;scalar-matrix&quot;&gt;Scalar matrix&lt;/h3&gt;
&lt;p&gt;Diagonal matrices are said to be scalar when all the elements along its main diaonal are equal, i.e., $\textit{D} = \alpha\textit{I}$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;null-or-zero-matrix&quot;&gt;Null or zero matrix&lt;/h3&gt;
&lt;p&gt;Matrices are said to be null or zero matrices when all its elements equal to zero, wich is denoted as $0_{m \times n}$. For instance:&lt;/p&gt;
&lt;h3 id=&quot;echelon-matrix&quot;&gt;Echelon matrix&lt;/h3&gt;
&lt;p&gt;Matrices are said to be on &lt;strong&gt;echelon form&lt;/strong&gt; when it has undergone the process of Gaussian elimination. More specifically:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Zero rows are at the bottom of the matrix&lt;/li&gt;
&lt;li&gt;The leading entry (pivot) of each nonzero row is to the right of the leading entry of the row above it&lt;/li&gt;
&lt;li&gt;Each leading entry is the only nonzero entry in its column&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For instance:&lt;/p&gt;
&lt;p&gt;In echelon form after Gaussian Elimination becomes:&lt;/p&gt;
&lt;h3 id=&quot;antidiagonal-matrix&quot;&gt;Antidiagonal matrix&lt;/h3&gt;
&lt;p&gt;Matrices are said to be &lt;strong&gt;antidiagonal&lt;/strong&gt; when all the entries are zero but the antidiagonal (i.e., the diagonal starting from the bottom left corner to the upper right corner). For instance:&lt;/p&gt;
&lt;h3 id=&quot;design-matrix&quot;&gt;Design matrix&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Design matrix&lt;/strong&gt; is a special name for matrices containing explanatory variables or features in the context of statistics and machine learning. Some authors favor this name to refer to the set of variables or features in a model.&lt;/p&gt;
&lt;h2 id=&quot;matrices-as-systems-of-linear-equations&quot;&gt;Matrices as systems of linear equations&lt;/h2&gt;
&lt;p&gt;I introduced the idea of systems of linear equations as a way to figure out the right combination of linear segments to obtain an outcome. I did this in the context of vectors, now we can extend this to the context of matrices.&lt;/p&gt;
&lt;p&gt;Matrices are ideal to represent systems of linear equations. Consider the matrix $\textit{M}$ and vectors $w$ and $y$ in $\in \mathbb{R}^3$. We can set up a system of linear equations as $\textit{M}w = y$ as:&lt;/p&gt;
&lt;p&gt;This is equivalent to: &lt;/p&gt;
&lt;p&gt;Geometrically, the solution for this representation equals to plot a &lt;strong&gt;set of planes in 3-dimensional space&lt;/strong&gt;, one for each equation, and to find the segment where the planes intersect.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 12: Visualiation system of equations as planes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-planes-intersection.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;An alternative way, which I personally prefer to use, is to represent the system as a &lt;strong&gt;linear combination of the column vectors times a scaling term&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Geometrically, the solution for this representation equals to plot a set of &lt;strong&gt;vectors in 3-dimensional&lt;/strong&gt; space, one for each column vector, then scale them by $w_i$ and add them up, tip to tail, to find the resulting vector $y$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 13: System of equations as linear combination of vectors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-vectors-combination.svg&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;the-four-fundamental-matrix-subsapces&quot;&gt;The four fundamental matrix subsapces&lt;/h2&gt;
&lt;p&gt;Let’s recall the definition of a subspace in the context of vectors:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Contains the zero vector, $\bf{0} \in S$&lt;/li&gt;
&lt;li&gt;Closure under multiplication, $\forall \alpha \in \mathbb{R} \rightarrow \alpha \times s_i \in S$&lt;/li&gt;
&lt;li&gt;Closure under addition, $\forall s_i \in S \rightarrow s_1 + s_2 \in S$&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;These conditions carry on to matrices since matrices are simply collections of vectors. Thus, now we can ask what are all possible subspaces that can be “covered” by a collection of vectors in a matrix. Turns out, there are four fundamental subspaces that can be “covered” by a matrix of valid vectors: (1) the column space, (2) the row space, (3) the null space, and (4) the left null space or null space of the transpose.&lt;/p&gt;
&lt;p&gt;These subspaces are considered fundamental because they express many important properties of matrices in linear algebra.&lt;/p&gt;
&lt;h3 id=&quot;the-column-space&quot;&gt;The column space&lt;/h3&gt;
&lt;p&gt;The column space of a matrix $\textit{A}$ is composed by &lt;strong&gt;all linear combinations of the columns of $\textit{A}$&lt;/strong&gt;. We denote the column space as $C(\textit{A})$. In other words, $C(\textit{A})$ equals to the &lt;strong&gt;span of the columns of $\textit{A}$&lt;/strong&gt;. This view of a matrix is what we represented in &lt;strong&gt;Fig. 12&lt;/strong&gt;: vectors in $\mathbb{R}^n$ scaled by real numbers.&lt;/p&gt;
&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{v} \in \mathbb{R}^m$, the column space is defined as:&lt;/p&gt;
&lt;p&gt;In words: all linear combinations of the column vectors of $\textit{A}$ and entries of an $n$ dimensional vector $\bf{v}$.&lt;/p&gt;
&lt;h3 id=&quot;the-row-space&quot;&gt;The row space&lt;/h3&gt;
&lt;p&gt;The row space of a matrix $\textit{A}$ is composed of all linear combinations of the rows of a matrix. We denote the row space as $R(\textit{A})$. In other words, $R(\textit{A})$ equals to the &lt;strong&gt;span of the rows&lt;/strong&gt; of $\textit{A}$. Geometrically, this is the way we represented a matrix in &lt;strong&gt;Fig. 11&lt;/strong&gt;: each row equation represented as planes. Now, a different way to see the row space, is by transposing $\textit{A}^T$. Now, we can define the row space simply as $R(\textit{A}^T)$&lt;/p&gt;
&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{w} \in \mathbb{R}^m$, the row space is defined as:&lt;/p&gt;
&lt;p&gt;In words: all linear combinations of the row vectors of $\textit{A}$ and entries of an $m$ dimensional vector $\bf{w}$.&lt;/p&gt;
&lt;h3 id=&quot;the-null-space&quot;&gt;The null space&lt;/h3&gt;
&lt;p&gt;The null space of a matrix $\textit{A}$ is composed of all vectors that are map into the zero vector when multiplied by $\textit{A}$. We denote the null space as $N(\textit{A})$.&lt;/p&gt;
&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{v} \in \mathbb{R}^n$, the null space is defined as:&lt;/p&gt;
&lt;h3 id=&quot;the-null-space-of-the-transpose&quot;&gt;The null space of the transpose&lt;/h3&gt;
&lt;p&gt;The left null space of a matrix $\textit{A}$ is composed of all vectors that are map into the zero vector when multiplied by $\textit{A}$ from the left. By “from the left”, the vectors on the left of $\textit{A}$. We denote the left null space as $N(\textit{A}^T)$&lt;/p&gt;
&lt;p&gt;For a matrix $\textit{A} \in \mathbb{R}^{m\times n}$ and a vector $\bf{w} \in \mathbb{R}^m$, the null space is defined as:&lt;/p&gt;
&lt;h2 id=&quot;solving-systems-of-linear-equations-with-matrices&quot;&gt;Solving systems of linear equations with Matrices&lt;/h2&gt;
&lt;h3 id=&quot;gaussian-elimination&quot;&gt;Gaussian Elimination&lt;/h3&gt;
&lt;p&gt;When I was in high school, I learned to solve systems of two or three equations by the methods of elimination and substitution. Nevertheless, as systems of equations get larger and more complicated, such inspection-based methods become impractical. By inspection-based, I mean “just by looking at the equations and using common sense”. Thus, to approach such kind of systems we can use the method of &lt;strong&gt;Gaussian Elimination&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gaussian Elimination&lt;/strong&gt;, is a robust algorithm to solve linear systems. We say is robust, because it works in general, it all possible circumstances. It works by &lt;em&gt;eliminating&lt;/em&gt; terms from a system of equations, such that it is simplified to the point where we obtain the &lt;strong&gt;row echelon form&lt;/strong&gt; of the matrix. A matrix is in row echelon form when all rows contain zeros at the bottom left of the matrix. For instance:&lt;/p&gt;
&lt;p&gt;The $p$ values along the diagonal are the &lt;strong&gt;pivots&lt;/strong&gt; also known as basic variables of the matrix. An important remark about the pivots, is that they indicate which vectors are linearly independent in the matrix, once the matrix has been reduced to the row echelon form.&lt;/p&gt;
&lt;p&gt;There are three &lt;em&gt;elementary transformations&lt;/em&gt; in Gaussian Elimination that when combined, allow simplifying any system to its row echelon form:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Addition and subtraction of two equations (rows)&lt;/li&gt;
&lt;li&gt;Multiplication of an equation (rows) by a number&lt;/li&gt;
&lt;li&gt;Switching equations (rows)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Consider the following system $\textit{A} \bf{w} = \bf{y}$:&lt;/p&gt;
&lt;p&gt;We want to know what combination of columns of $\textit{A}$ will generate the target vector $\bf{y}$. Alternatively, we can see this as a decomposition problem, as how can we decompose $\bf{y}$ into columns of $\textit{A}$. To aid the application of Gaussian Elimination, we can generate an &lt;strong&gt;augmented matrix&lt;/strong&gt; $(\textit{A} \vert \bf{y})$, this is, appending $\bf{y}$ to $\textit{A}$ on this manner:&lt;/p&gt;
&lt;p&gt;We start by multiplying row 1 by and substracting it from row 2 as $R_2 - 2R_1$ to obtain:&lt;/p&gt;
&lt;p&gt;If we substract row 1 from row 3 as $R_3 - R_1$ we get:&lt;/p&gt;
&lt;p&gt;At this point, we have found the row echelon form of $\textit{A}$. If we divide row 3 by -3, We know that $w_3 = -1$. By &lt;strong&gt;backsubsitution&lt;/strong&gt;, we can solve for $w_2$ as:&lt;/p&gt;
&lt;p&gt;Again, taking $w_2=2$ and $w_3=-1$ we can solve for $w_1$ as:&lt;/p&gt;
&lt;p&gt;In this manner, we have found that the solution for our system is $w_1 = -2$, $w_2=2$, and $w_3 = -1$.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can solve a system of equations with Gaussian Elimination with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.solve&lt;/code&gt; method as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;11&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;17&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;solve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[-2.],
       [ 2.],
       [-1.]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Which confirms our solution is correct.&lt;/p&gt;
&lt;h3 id=&quot;gauss-jordan-elimination&quot;&gt;Gauss-Jordan Elimination&lt;/h3&gt;
&lt;p&gt;The only difference between &lt;strong&gt;Gaussian Elimination&lt;/strong&gt; and &lt;strong&gt;Gauss-Jordan Elimination&lt;/strong&gt;, is that this time we “keep going” with the elemental row operations until we obtain the &lt;strong&gt;reduced row echelon form&lt;/strong&gt;. The &lt;em&gt;reduced&lt;/em&gt; part means two additionak things: (1) the pivots must be $1$, (2) and the entries above the pivots must be $0$. This is simplest form a system of linear equations can take. For instance, for a 3x3 matrix:&lt;/p&gt;
&lt;p&gt;Let’s retake from where we left Gaussian elimination in the above section. If we divide row 3 by -3 and row 2 by -4 as $\frac{R_3}{-3}$ and $\frac{R_2}{-4}$, we get:&lt;/p&gt;
&lt;p&gt;Again, by this point we we know $w_3 = -1$. If we multiply row 2 by 3 and substract from row 1 as $R_1 - 3R_2$:&lt;/p&gt;
&lt;p&gt;Finally, we can add 3.25 times row 3 to row 1, and substract 2.75 times row 3 to row 2, as $R_1 + 3.25R_3$ and $R_2 - 2.75R_3$ to get the &lt;strong&gt;reduced row echelon form&lt;/strong&gt; as:&lt;/p&gt;
&lt;p&gt;Now, by simply following the rules of matrix-vector multiplication, we get =&lt;/p&gt;
&lt;p&gt;There you go, we obtained that $w_1 = -2$, $w_2 = 2$, and $w_3 = -1$.&lt;/p&gt;
&lt;h2 id=&quot;matrix-basis-and-rank&quot;&gt;Matrix basis and rank&lt;/h2&gt;
&lt;p&gt;A set of $n$ linearly independent column vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt;. For instance, the column vectors of $\textit{A}$ are a basis:&lt;/p&gt;
&lt;p&gt;“A basis for what?” You may be wondering. In the case of $\textit{A}$, for any vector $\bf{y} \in \mathbb{R}^2$. On the contrary, the column vectors for $\textit{B}$ &lt;em&gt;do not&lt;/em&gt; form a basis for $\mathbb{R}^2$:&lt;/p&gt;
&lt;p&gt;In the case of $\textit{B}$, the third column vector is a linear combination of first and second column vectors.&lt;/p&gt;
&lt;p&gt;The definition of a &lt;em&gt;basis&lt;/em&gt; depends on the &lt;strong&gt;independence-dimension inequality&lt;/strong&gt;, which states that a &lt;em&gt;linearly independent set of $n$ vectors can have at most $n$ elements&lt;/em&gt;. Alternatively, we say that any set of $n$ vectors with $n+1$ elements is, &lt;em&gt;necessarily&lt;/em&gt;, linearly dependent. Given that each vector in a &lt;em&gt;basis&lt;/em&gt; is linearly independent, we say that any vector $\bf{y}$ with $n$ elements, can be generated in a unique linear combination of the &lt;em&gt;basis&lt;/em&gt; vectors. Hence, any matrix more columns than rows (as in $\textit{B}$) will have dependent vectors. &lt;em&gt;Basis&lt;/em&gt; are sometimes referred to as the &lt;em&gt;minimal generating set&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;An important question is how to find the &lt;em&gt;basis&lt;/em&gt; for a matrix. Another way to put the same question is to found out which vectors are linearly independent of each other. Hence, we need to solve:&lt;/p&gt;
&lt;p&gt;Where $a_i$ are the column vectors of $\textit{A}$. We can approach this by using &lt;strong&gt;Gaussian Elimination&lt;/strong&gt; or &lt;strong&gt;Gauss-Jordan Elimination&lt;/strong&gt; and reducing $\textit{A}$ to its &lt;strong&gt;row echelon form&lt;/strong&gt; or &lt;strong&gt;reduced row echelon form&lt;/strong&gt;. In either case, recall that the &lt;em&gt;pivots&lt;/em&gt; of the echelon form indicate the set of linearly independent vectors in a matrix.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; does not have a method to obtain the row echelon form of a matrix. But, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Sympy&lt;/code&gt;, a Python library for symbolic mathematics that counts with a module for Matrices operations.&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SymPy&lt;/code&gt; has a method to obtain the reduced row echelon form and the pivots, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rref&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sympy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt; 
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;15.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;26&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A_pivots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reduced row echelon form of A:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Reduced row echelon form of A:
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Column pivots of A: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_pivots&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Column pivots of A: (0, 1)
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B_rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B_pivots&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Reduced row echelon form of B:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Reduced row echelon form of B:
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Column pivots of A: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B_pivots&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Column pivots of A: (0, 1, 3)
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;For $\textit{A}$, we found that the first and second column vectors are the &lt;em&gt;basis&lt;/em&gt;, whereas for $\textit{B}$ is the first, second, and fourth.&lt;/p&gt;
&lt;p&gt;Now that we know about a &lt;em&gt;basis&lt;/em&gt; and how to find it, understanding the concept of &lt;em&gt;rank&lt;/em&gt; is simpler. The &lt;strong&gt;rank&lt;/strong&gt; of a matrix $\textit{A}$ is the dimensionality of the vector space generated by its number of linearly independent column vectors. This happens to be identical to the dimensionality of the vector space generated by its row vectors. We denote the &lt;em&gt;rank&lt;/em&gt; of matrix as $rk(\textit{A})$ or $rank(\textit{A})$.&lt;/p&gt;
&lt;p&gt;For an square matrix $\mathbb{R}^{m\times n}$ (i.e., $m=n$), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every column and/or row is linearly independent. For a non-square matrix with $m&amp;gt;n$ (i.e., more rows than columns), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every row is linearly independent. When $m&amp;lt;n$ (i.e., more columns than rows), we say is &lt;strong&gt;full rank&lt;/strong&gt; when every column is linearly independent.&lt;/p&gt;
&lt;p&gt;From an applied machine learning perspective, the &lt;em&gt;rank&lt;/em&gt; of a matrix is relevant as a measure of the &lt;a href=&quot;https://math.stackexchange.com/questions/21100/importance-of-matrix-rank&quot;&gt;information content of the matrix&lt;/a&gt;. Take matrix $\textit{B}$ from the example above. Although the original matrix has 5 columns, we know is rank 4, hence, it has less information than it appears at first glance.&lt;/p&gt;
&lt;h2 id=&quot;matrix-norm&quot;&gt;Matrix norm&lt;/h2&gt;
&lt;p&gt;As with vectors, we can measure the size of a matrix by computing its &lt;strong&gt;norm&lt;/strong&gt;. There are multiple ways to define the norm for a matrix, as long it satisfies the same properties defined for vectors norms: (1) absolutely homogeneous, (2) triangle inequality, (3) positive definite (see vector norms section). For our purposes, I’ll cover two of the most commonly used norms in machine learning: (1) &lt;strong&gt;Frobenius norm&lt;/strong&gt;, (2) &lt;strong&gt;max norm&lt;/strong&gt;, (3) &lt;strong&gt;spectral norm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: I won’t cover the spectral norm just yet, because it depends on concepts that I have not introduced at this point.&lt;/p&gt;
&lt;h3 id=&quot;frobenius-norm&quot;&gt;Frobenius norm&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Frobenius norm&lt;/strong&gt; is an element-wise norm named after the German mathematician Ferdinand Georg Frobenius. We denote this norm as $\Vert \textit{A} \Vert_F$. You can thing about this norm as flattening out the matrix into a long vector. For instance, a $3 \times 3$ matrix would become a vector with $n=9$ entries. We define the Frobenius norm as:&lt;/p&gt;
&lt;p&gt;In words: square each entry of $\textit{A}$, add them together, and then take the square root.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the Frobenius norm as with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linal.norm&lt;/code&gt; method ant &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fro&lt;/code&gt; as the argument:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;16.881943016134134
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;max-norm-1&quot;&gt;Max norm&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;max norm&lt;/strong&gt; or &lt;strong&gt;infinity norm&lt;/strong&gt; of a matrix equals to the largest sum of the absolute value of row vectors. We denote the max norm as $\Vert \textit{A} \Vert_max$. Consider $\textit{A} \in \mathbb{R}^{m \times n}$. We define the max norm for $\textit{A}$ as:&lt;/p&gt;
&lt;p&gt;This equals to go row by row, adding the absolute value of each entry, and then selecting the largest sum.&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we compute the max norm as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In this case, is easy to see that the third row has the largest absolute value.&lt;/p&gt;
&lt;h3 id=&quot;spectral-norm&quot;&gt;Spectral norm&lt;/h3&gt;
&lt;p&gt;To understand this norm, is necessary to first learn about eigenvectors and eigenvalues, which I cover later.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;spectral norm&lt;/strong&gt; of a matrix equals to the largest singular value $\sigma_1$. We denote the spectral norm as $\Vert \textit{A} \Vert_2$. Consider $\textit{A} \in \mathbb{R}^{m \times n}$. We define the spectral for $\textit{A}$ as:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Numpy&lt;/code&gt;, we compute the max norm as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;16.84810335261421
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Libraries for this section 
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;altair&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;themes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dark'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;ThemeRegistry.enable('dark')
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;linear-mappings&quot;&gt;Linear mappings&lt;/h2&gt;
&lt;p&gt;Now we have covered the basics of vectors and matrices, we are ready to introduce the idea of a linear mapping. &lt;strong&gt;Linear mappings&lt;/strong&gt;, also known as &lt;em&gt;linear transformations&lt;/em&gt; and &lt;em&gt;linear functions&lt;/em&gt;, indicate the correspondence between vectors in a vector space $\textit{V}$ and the same vectors in a different vector space $\textit{W}$. This is an abstract idea. I like to think about this in the following manner: imagine there is a multiverse as in Marvel comics, but instead of humans, aliens, gods, stars, galaxies, and superheroes, we have &lt;em&gt;vectors&lt;/em&gt;. In this context, a linear mapping would indicate the &lt;em&gt;correspondence&lt;/em&gt; of entities (i.e., planets, humans, superheroes, etc) &lt;em&gt;between universes&lt;/em&gt;. Just imagine us, placidly existing in our own universe, and suddenly a &lt;em&gt;linear mapping&lt;/em&gt; happens: our entire universe would be transformed into a different one, according to whatever rules the linear mapping has enforced. Now, switch &lt;em&gt;universes&lt;/em&gt; for &lt;em&gt;vector spaces&lt;/em&gt; and &lt;em&gt;us&lt;/em&gt; by vectors, and you’ll get the full picture.&lt;/p&gt;
&lt;p&gt;So, linear mappings transform vector spaces into others. Yet, such transformations are constrained to a spefic kind: &lt;strong&gt;linear ones&lt;/strong&gt;. Consider a linear mapping $\textit{T}$ and a pair of vectors $\bf{x}$ and $\bf{y}$. To be valid, a linear mapping must satisfies these rules:&lt;/p&gt;
&lt;p&gt;In words:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The transformation of the sum of the vectors must be equal to taking the transformation of each vector individually and then adding them up.&lt;/li&gt;
&lt;li&gt;The transformation of a scaled version of a vector must be equal to taking the transformation of the vector first and then scaling the result.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The two properties above can be condenced into one, the &lt;strong&gt;superposition property&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;As a result of satisfying those properties, linear mappings &lt;strong&gt;preserve the structure of the original vector space&lt;/strong&gt;. Imagine a vector space $\in \mathbb{R}^2$, like a grid on lines in a cartesian plane. Visually, preserving the structure of the vector space after a mapping means to: (1) the origin of the coordinate space remains fixed, and (2) the lines remain lines and parallel to each other.&lt;/p&gt;
&lt;p&gt;In linear algebra, linear mappings are represented as matrices and performed by matrix multiplication. Take a vector $\bf{x}$ and a matrix $\textit{A}$. We say that when $\textit{A}$ multiplies $\bf{x}$, the matrix transform the vector into another one:&lt;/p&gt;
&lt;p&gt;The typicall notation for a linear mapping is the same we used for functions. For the vector spaces $\textit{V}$ and $\textit{W}$, we indicate the linear mapping as $\textit{T}: \textit{V} \rightarrow \textit{W}$&lt;/p&gt;
&lt;h2 id=&quot;examples-of-linear-mappings&quot;&gt;Examples of linear mappings&lt;/h2&gt;
&lt;p&gt;Let’s examine a couple of examples of proper linear mappings. In general, &lt;em&gt;dot products are linear mappings&lt;/em&gt;. This should come as no surprise since dot products are linear operations by definition. Dot products sometimes take special names, when they have a well-known effect on a linear space. I’ll examine two simple cases: &lt;strong&gt;negation&lt;/strong&gt; and &lt;strong&gt;reversal&lt;/strong&gt;. Keep in mind that although we will test this for one vector, this mapping work on the entire vector space (i.e., the span) of a given dimensionality.&lt;/p&gt;
&lt;h3 id=&quot;negation-matrix&quot;&gt;Negation matrix&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;negation matrix&lt;/strong&gt; returns the opposite sign of each element of a vector. It can be defined as:&lt;/p&gt;
&lt;p&gt;This is, the negative identity matrix. Consider a pair of vectors $\bf{x} \in \mathbb{R}^3$ and $\bf{x} \in \mathbb{y}^3$, and the negation matrix $\textit{-I} \in \mathbb{R}^{3 \times 3}$. Let’s test the linear mapping properties with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;12.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;20&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We first test $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left side of the equation:
[[ 4]
 [ 0]
 [-3]]
Right side of the equation:
[[ 4]
 [ 0]
 [-3]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Hence, we confirm we get the same results.&lt;/p&gt;
&lt;p&gt;Let’s check the second property $\text{T}(\alpha \bf{x}) = \alpha\textit{T} (\bf{x}) \text{, } \forall \alpha$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left side of the equation:
[[ 2]
 [ 0]
 [-2]]
Right side of the equation:
[[ 2]
 [ 0]
 [-2]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Again, we confirm we get the same results for both sides of the equation&lt;/p&gt;
&lt;h3 id=&quot;reversal-matrix&quot;&gt;Reversal matrix&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;reversal matrix&lt;/strong&gt; returns reverses the order of the elements of a vector. This is, the last become the first, the second to last becomes the second, and so on. For a matrix in $\mathbb{R}^{3 \times 3}$ is defined as:&lt;/p&gt;
&lt;p&gt;In general, it is the &lt;em&gt;identity matrix but backwards&lt;/em&gt;, with ones from the bottom left corner to the top right corern. Consider a pair of vectors $\bf{x} \in \mathbb{R}^3$ and $\bf{x} \in \mathbb{y}^3$, and the reversal matrix $\textit{T} \in \mathbb{R}^{3 \times 3}$. Let’s test the linear mapping properties with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;12.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;20&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We first test $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x_reversal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_reversal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;x before reversal:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;x after reversal &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_reversal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;y before reversal:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;y after reversal &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_reversal&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Left side of the equation (add reversed vectors):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Right side of the equation (add reversed vectors):&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;x before reversal:
[[-1]
 [ 0]
 [ 1]]
x after reversal 
[[ 1]
 [ 0]
 [-1]]
y before reversal:
[[-3]
 [ 0]
 [ 2]]
y after reversal 
[[ 2]
 [ 0]
 [-3]]
Left side of the equation (add reversed vectors):
[[ 3]
 [ 0]
 [-4]]
Right side of the equation (add reversed vectors):
[[ 3]
 [ 0]
 [-4]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This works fine. Let’s check the second property $\text{T}(\alpha \bf{x}) = \alpha\textit{T} (\bf{x}) \text{, } \forall \alpha$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Left side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Right side of the equation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;right_side_2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left side of the equation:
[[ 2]
 [ 0]
 [-2]]
Right side of the equation:
[[ 2]
 [ 0]
 [-2]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;examples-of-nonlinear-mappings&quot;&gt;Examples of nonlinear mappings&lt;/h2&gt;
&lt;p&gt;As with most subjects, examining examples of &lt;em&gt;what things are not&lt;/em&gt; can be enlightening. Let’s take a couple of non-linear mappings: &lt;strong&gt;norms&lt;/strong&gt; and &lt;strong&gt;translation&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;
&lt;p&gt;This may come as a surprise, but norms are not linear transformations. Not “some” norms, but all norms. This is because of the very definition of a norm, in particular, the &lt;strong&gt;triangle inequality&lt;/strong&gt; and &lt;strong&gt;positive definite&lt;/strong&gt; properties, colliding with the requirements of linear mappings.&lt;/p&gt;
&lt;p&gt;First, the triangle inequality defines: $\lVert \bf{x} + \bf{y} \rVert \le \lVert \bf{x} \rVert + \lVert \bf{y} \rVert$. Whereas the first requirement for linear mappings demands: $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$. The problem here is in the $\le$ condition, which means adding two vectors and then taking the norm &lt;em&gt;can&lt;/em&gt; be less than the sum of the norms of the individual vectors. Such condition is, by defnition, not allowed for linear mappings.&lt;/p&gt;
&lt;p&gt;Second, the positive definite defines: $\lVert \bf{x} \rVert \ge 0$ and $ \lVert \bf{x} \rVert = 0 \Longleftrightarrow \bf{x}= 0$. Put simply, norms &lt;em&gt;have to&lt;/em&gt; be a postive value. For instance, the norm of $\Vert - x \Vert = \Vert x \Vert$, instead of $\Vert - x \Vert$. But, the second property for linear mappings requires $\Vert -\alpha \bf{x} \Vert = -\alpha \Vert \bf{x} \Vert$. Hence, it fails when we multiply by a negative number (i.e., it can preserve the negative sign).&lt;/p&gt;
&lt;h3 id=&quot;translation&quot;&gt;Translation&lt;/h3&gt;
&lt;p&gt;Translation is a geometric transformation that moves every vector in a vector space by the same distance in a given direction. Translation is an operation that matches our everyday life intuitions: move a cup of coffee from your left to your right, and you would have performed translation in $\mathbb{R}^3$ space.&lt;/p&gt;
&lt;p&gt;Contrary to what we have seen so far, the translation matrix is represented with &lt;strong&gt;homogeneous coordinates&lt;/strong&gt; instead of cartesian coordinates. Put simply, the homogeneous coordinate system adds a extra $1$ at the end of vectros. For instance, the vector in $\mathbb{R}^2$ cartesian coordinates:&lt;/p&gt;
&lt;p&gt;Becomes the following in $\mathbb{R}^2$ homogeneous coordinates:&lt;/p&gt;
&lt;p&gt;In fact, the translation matrix for the general case can’t be represented with cartesian coordinates. Homogeneous coordinates are the standard in fields like computer graphics since they allow us to better represent a series of transformations (or mappings) like scaling, translation, rotation, etc.&lt;/p&gt;
&lt;p&gt;A translation matrix in $\mathbb{R}^3$ can be denoted as:&lt;/p&gt;
&lt;p&gt;Where $v_1$ and $v_2$ are the values added to each dimension for translation. For instance, consider $\bf{x} = \begin{bmatrix} 2 &amp;amp; 2 \end{bmatrix}^T \in \mathbb{R}^2$. If we want translate this $3$ units in the first dimension, and $1$ units in the second dimension, we first transfor the vector to homogeneous coordinates $\bf{x} = \begin{bmatrix} 2 &amp;amp; 2 &amp;amp; 1 \end{bmatrix}^T$ , and then perfom matrix-vector multiplication as usual:&lt;/p&gt;
&lt;p&gt;The first two vectors in the translation matrix simple reproduce the original vector (i.e., the identity), and the third vector is the one actually “moving” the vectors.&lt;/p&gt;
&lt;p&gt;Translation is &lt;strong&gt;not a linear mapping&lt;/strong&gt; simply because $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x}) + \textit{T}(\bf{y})$ &lt;strong&gt;does not hold&lt;/strong&gt;. In the case of translation $\textit{T}(\bf{x} + \bf{y}) = \textit{T}(\bf{x} + v_1) + \textit{T}(\bf{y} + + v_1)$, which invalidates the operation as a linear mapping. This type of mapping is known as an &lt;strong&gt;affine mapping or transformation&lt;/strong&gt;, which is the topic I’ll review next.&lt;/p&gt;
&lt;h2 id=&quot;affine-mappings&quot;&gt;Affine mappings&lt;/h2&gt;
&lt;p&gt;The simplest way to describe affine mappings (or transformations) is as a &lt;em&gt;linear mapping&lt;/em&gt; + &lt;em&gt;translation&lt;/em&gt;. Hence, an affine mapping $\textit{M}$ takes the form of:&lt;/p&gt;
&lt;p&gt;Where $\textit{A}$ is a linear mapping or transformation and $\textbf{b}$ is the translation vector.&lt;/p&gt;
&lt;p&gt;If you are familiar with linear regression, you would notice that the above expression is its matrix form. Linear regression is usually analyzed as a linear mapping plus noise, but it can also be seen as an affine mapping. Alternative, we can say that $\textit{A}\textbf{x} + \textbf{b}$ is a linear mapping &lt;em&gt;if and only if&lt;/em&gt; $\textbf{b}=0$.&lt;/p&gt;
&lt;p&gt;From a geometrical perspective, affine mappings displace spaces (lines or hyperplanes) from the origin of the coordinate space. Consequently, affine mappings do not operate over &lt;em&gt;vector spaces&lt;/em&gt; as the zero vector condition $\bf{0} \in S$ does not hold anymore. Affine mappings act onto &lt;em&gt;affine subspaces&lt;/em&gt;, that I’ll define later in this section.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 14: Affine mapping&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-affine-mapping.svg&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;affine-combination-of-vectors&quot;&gt;Affine combination of vectors&lt;/h2&gt;
&lt;p&gt;We can think in affine combinations of vectors, as linear combinations with an added constraint. Let’s recall de definitoon for a linear combination. Consider a set of vectors $x_1, …, x_k$ and scalars $\beta_1, …, \beta_k \in \mathbb{R}$, then a linear combination is:&lt;/p&gt;
&lt;p&gt;For affine combinations, we add the condition:&lt;/p&gt;
&lt;p&gt;In words, we constrain the sum of the weights $\beta$ to $1$. In practice, this defines a &lt;em&gt;weighted average of the vectors&lt;/em&gt;. This restriction has a palpable effect which is easier to grasp from a geometric perspective.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 15: Affine combinations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-affine-combination.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 15&lt;/strong&gt; shows two affine combinations. The first combination with weights $\beta_1 = \frac{1}{2}$ and $\beta_2 = \frac{1}{2}$, which yields the midpoint between vectors $\bf{x}$ and $\bf{y}$. The second combination with weights $\beta_1 = 3$ and $\beta_2 =-1$ (add up to $1$), which yield a point over the vector $\bf{z}$. In both cases, we have that the resulting vector lies on the same line. This is a general consequence of constraining the sum of the weights to $1$: &lt;em&gt;every affine combination of the same set of vectors will map onto the same space&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;affine-span&quot;&gt;Affine span&lt;/h2&gt;
&lt;p&gt;The set of all linear combinations, define the the vector span. Similarly, the set of all affine combinations determine the &lt;strong&gt;affine span&lt;/strong&gt;. As we saw in &lt;strong&gt;Fig. 15&lt;/strong&gt;, every affine of vectors $\textbf{x}$ and $\textbf{y}$ maps onto the line $\textbf{z}$. More generally, we say that the &lt;strong&gt;affine span&lt;/strong&gt; of vectors $\textbf{x}_1, \cdots, \textbf{x}_k$ is:&lt;/p&gt;
&lt;p&gt;Again, in words: the affine span is the set of all linear combinations of the vector set, such that the weights add up to $1$ and all weights are real numbers. Hence, the fundamental difference between vector spaces and affine spaces, is the former will span the entire $\mathbb{R}^n$ space (assuming independent vectors), whereas the latter will span a line.&lt;/p&gt;
&lt;p&gt;Let’s consider three cases in $\mathbb{R}^3$: (1) three linearly independent vectors; (2) two linearly independent vectors and one dependent vector; (3) three linearly dependent vectors. In case (1), the affine span is the 2-dimensional plane containing those vectors. In case (2), the affine space is a line. Finally, in case (3), the span a single point. This may not be entirely obvious, so I encourage you to draw and the three cases, take the affine combinations and see what happens.&lt;/p&gt;
&lt;h2 id=&quot;affine-space-and-subspace&quot;&gt;Affine space and subspace&lt;/h2&gt;
&lt;p&gt;In simple terms, &lt;strong&gt;affine spaces&lt;/strong&gt; are &lt;em&gt;translates&lt;/em&gt; of vector spaces, this is, vector spaces that have been offset from the origin of the coordinate system. Such a notion makes sound affine spaces as a special case of vector spaces, but they are actually more general. Indeed, affine spaces provide a more general framework to do geometric manipulation, as they work independently of the choice of the coordinate system (i.e., it is not constrained to the origin). For instance, the set of solutions of the system of linear equations $\textit{A}\textbf{x}=\textbf{y}$ (i.e., linear regression), is an affine space, not a linear vector space.&lt;/p&gt;
&lt;p&gt;Consider a vector space $\textit{V}$, a vector $\textbf{x}_0 \in \textit{V}$, and a subset $\textit{U} \subseteq \textit{V}$. We define an affine subspace $\textit{L}$ as:&lt;/p&gt;
&lt;p&gt;Further, any point, line, plane, or hyperplane in $\mathbb{R}^n$ that does not go through the origin, is an affine subspace.&lt;/p&gt;
&lt;h2 id=&quot;affine-mappings-using-the-augmented-matrix&quot;&gt;Affine mappings using the augmented matrix&lt;/h2&gt;
&lt;p&gt;Consider the matrix $\textit{A} \in \mathbb{R}^{m \times n}$, and vectors $\textbf{x}, \textbf{b}, \textbf{y} \in \mathbb{R}^n$&lt;/p&gt;
&lt;p&gt;We can represent the system of linear equations:&lt;/p&gt;
&lt;p&gt;As a single matrix vector multiplication, by using an &lt;strong&gt;augmented matrix&lt;/strong&gt; of the form:&lt;/p&gt;
&lt;p&gt;This form is known as the &lt;strong&gt;affine transformation matrix&lt;/strong&gt;. We made use of this form when we exemplified &lt;em&gt;translation&lt;/em&gt;, which happens to be an affine mapping.&lt;/p&gt;
&lt;h2 id=&quot;special-linear-mappings&quot;&gt;Special linear mappings&lt;/h2&gt;
&lt;p&gt;There are several important linear mappings (or transformations) that can be expressed as matrix-vector multiplications of the form $\textbf{y} = \textit{A}\textbf{x}$. Such mappings are common in image processing, computer vision, and other linear applications. Further, combinations of linear and nonlinear mappings are what complex models as neural networks do to learn mappings from inputs to outputs. Here we briefly review six of the most important linear mappings.&lt;/p&gt;
&lt;h3 id=&quot;scaling&quot;&gt;Scaling&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Scaling&lt;/strong&gt; is a mapping of the form $\textbf{y} = \textit{A}\textbf{x}$, with $\textit{A} = \alpha \textit{I}$. Scaling &lt;em&gt;stretches&lt;/em&gt; $\textbf{x}$ by a factor $\vert \alpha \vert$ when $\alpha &amp;lt; 1$, &lt;em&gt;shrinks&lt;/em&gt; $\textbf{x}$ when $\alpha &amp;lt; 1$, and &lt;em&gt;reverses&lt;/em&gt; the direction of the vector when $\alpha &amp;lt; 0$. For geometrical objects in Euclidean space, scaling changes the size but not the shape of objects. An scaling matrix in $\mathbb{R}^2$ takes the form:&lt;/p&gt;
&lt;p&gt;Where $s_1, s_2$ are the scaling factors.&lt;/p&gt;
&lt;p&gt;Let’s scale a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a scaling matrix $\textit{A}$, a vector $\textbf{x}$ to scale, and then plot the original and scaled vectors with Altair.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To scale $\textbf{x}$, we perform matrix-vector multiplication as usual&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tran'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tran'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;dim-1&lt;/th&gt;
&lt;th&gt;dim-2&lt;/th&gt;
&lt;th&gt;type&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;tran&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;8.0&lt;/td&gt;
&lt;td&gt;tran&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;base&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;base&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;We see that the resulting scaled vector (‘tran’) is indeed two times the original vector (‘base’). Now let’s plot. The light blue line solid line represents the original vector, whereas the dashed orange line represents the scaled vector.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;opacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strokeDash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'type'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;reflection&quot;&gt;Reflection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Reflection&lt;/strong&gt; is the mirror image of an object in Euclidean space. For the general case, reflection of a vector $\textbf{x}$ through a line that passes through the origin is obtained as:&lt;/p&gt;
&lt;p&gt;where $\theta$ are radians of inclination with respect to the horizontal axis. I’ve been purposely avoiding trigonometric functions, so let’s examine a couple of special cases for a vector $\textbf{x}$ in $\mathbb{R}^2$ (that can be extended to an arbitrary number of dimensions).&lt;/p&gt;
&lt;p&gt;Reflection along the horizontal axis, or around the line at $0^{\circ}$ from the origin:&lt;/p&gt;
&lt;p&gt;Reflection along the vertical axis, or around the line at $90^{\circ}$ from the origin:&lt;/p&gt;
&lt;p&gt;Reflection along the line where the horizontal axis equals the vertical axis, or around the line at $45^{\circ}$ from the origin:&lt;/p&gt;
&lt;p&gt;Reflection along the line where the horizontal axis equals the negative of the vertical axis, or around the line at $-45^{\circ}$ from the origin:&lt;/p&gt;
&lt;p&gt;Let’s reflect a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a reflection matrix $\textit{A}$, a vector $\textbf{x}$ to reflect, and then plot the original and reflected vectors with Altair.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;16&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;27&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# rotation along the horiontal axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the vertical axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the line at 45 degrees from the origin
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# rotation along the line at -45 degrees from the origin
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;12.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;20&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;s&quot;&gt;'reflection'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;s&quot;&gt;'0-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'0-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                  &lt;span class=&quot;s&quot;&gt;'90-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'90-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                  &lt;span class=&quot;s&quot;&gt;'45-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'45-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                  &lt;span class=&quot;s&quot;&gt;'neg-45-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'neg-45-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;dim-1&lt;/th&gt;
&lt;th&gt;dim-2&lt;/th&gt;
&lt;th&gt;reflection&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;-4.0&lt;/td&gt;
&lt;td&gt;0-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;90-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;-2.0&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;90-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;45-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;45-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;8&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;neg-45-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;9&lt;/th&gt;
&lt;td&gt;-4.0&lt;/td&gt;
&lt;td&gt;-2.0&lt;/td&gt;
&lt;td&gt;neg-45-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;16&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ran2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'''return base chart with coordinate space'''&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ran1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ran2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'vertical'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)})&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'white'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'reflection'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;shear&quot;&gt;Shear&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Shear&lt;/strong&gt; mappings are hard to describe in words but easy to understand with images. I recommend to look at the shear mapping below and then read this description: a shear mapping displaces points of an object in a given direction (e.g., all points to the right), in a proportion equal to their perpendicular distance from an axis (e.g., the line on the $x$ axis) that remains fixed. A “proportion equal to their perpendicular distance” means that points further away from the reference axis displace more than points near to the axis.&lt;/p&gt;
&lt;p&gt;For an object in $\mathbb{R}^2$, a &lt;strong&gt;horizontal shear&lt;/strong&gt; matrix (i.e., paraller to the horizontal axis) takes the form:&lt;/p&gt;
&lt;p&gt;Where $m$ is the &lt;em&gt;shear factor&lt;/em&gt;, that essentially determines how pronounced is the shear.&lt;/p&gt;
&lt;p&gt;For an object in $\mathbb{R}^2$, a &lt;strong&gt;vertical shear&lt;/strong&gt; matrix (i.e., paraller to the vertical axis) takes the form:&lt;/p&gt;
&lt;p&gt;Let’s shear a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a shear matrix $\textit{A}$, a pair of vectors $\textbf{x}$ and $\textbf{u}$ to shear, and then plot the original and shear vectors with Altair. The reason we define two vectors, is that shear mappings are easier to appreciate with planes or multiple sides figures than single lines.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;13&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;21&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# shear along the horiontal axis
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;11&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;17&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;s&quot;&gt;'shear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'horizontal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'horizontal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'original-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'original-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'horizontal-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'horizontal-2'&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'shear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;rotation&quot;&gt;Rotation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Rotation&lt;/strong&gt; mappings do exactly what their name indicates: they move objects (by convection) counterclockwise in Euclidean space. For the general case in $\mathbb{R}^2$, counterclockwise of vector $\textbf{x}$ by $\theta$ radiants rotations is obtained as:&lt;/p&gt;
&lt;p&gt;Again, let’s examine a couple of special cases.&lt;/p&gt;
&lt;p&gt;A $90^{\circ}$ rotation matrix in $\mathbb{R}^2$ :&lt;/p&gt;
&lt;p&gt;A $180^{\circ}$ rotation matrix in $\mathbb{R}^2$:&lt;/p&gt;
&lt;p&gt;A $270^{\circ}$ rotation matrix in $\mathbb{R}^2$:&lt;/p&gt;
&lt;p&gt;Let’s rotate a vector using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will define a rotation matrix $\textit{A}$, a vector $\textbf{x}$, and then plot the original and rotated vectors with Altair.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;14&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;23&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 90-degrees roration 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 180-degrees roration 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 270-degrees roration 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;11&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;17&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;s&quot;&gt;'rotation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'original'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'90-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'90-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'180-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'180-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;s&quot;&gt;'270-degrees'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'270-degrees'&lt;/span&gt;
                            &lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;dim-1&lt;/th&gt;
&lt;th&gt;dim-2&lt;/th&gt;
&lt;th&gt;rotation&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;original&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;90-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;-4.0&lt;/td&gt;
&lt;td&gt;2.0&lt;/td&gt;
&lt;td&gt;90-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;180-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;-2.0&lt;/td&gt;
&lt;td&gt;-4.0&lt;/td&gt;
&lt;td&gt;180-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;0.0&lt;/td&gt;
&lt;td&gt;270-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;4.0&lt;/td&gt;
&lt;td&gt;-2.0&lt;/td&gt;
&lt;td&gt;270-degrees&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rotation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;projections&quot;&gt;Projections&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Projections&lt;/strong&gt; are a fundamental type of linear (and affine) mappings for machine learning. If you have ever heard concepts like “embeddings”, “low-dimensional representation”, or “dimensionality reduction”, they all are examples of projections. Even linear regression and principal component analysis are exemplars of projections. Thus, projections allow working with high-dimensional spaces (i.e., problems with many features or variables) more efficiently, by projecting such spaces into lower-dimensional spaces. this works because is often the case that a few dimensions contain most of the information to understand the relation between inputs and outputs. Moreover, projections can be represented as &lt;em&gt;matrices acting on vectors&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Put simply, projections are &lt;em&gt;mappings from a space onto a subpace&lt;/em&gt;, or from a set of vectors onto a subset of vectors. Additionally, projections are “idempotent”, this is, the projection has the property to be &lt;em&gt;equal to its composition with itself&lt;/em&gt;. In other words, when you wrap a projection $\phi(x) = y$ into itself as $\phi (\phi (x))$, the result does not change, i.e., $\phi (\phi (x)) = y$. Formally, for a vector space $\textit{V}$ and a vector subset $\textit{U} \subset \textit{V}$, we define a projection $\phi$ as:&lt;/p&gt;
&lt;p&gt;with&lt;/p&gt;
&lt;p&gt;Here we are concerned with the matrix representation of projections, which receive the special name of &lt;strong&gt;projection matrices&lt;/strong&gt;, denoted as $\textit{P}_\phi$. By extension, projection matrices are also “idempotent”:&lt;/p&gt;
&lt;h3 id=&quot;projections-onto-lines&quot;&gt;Projections onto lines&lt;/h3&gt;
&lt;p&gt;In Freudian psychoanalysis, &lt;em&gt;projection&lt;/em&gt; is a defense mechanism of the “ego” (i.e., the sense of self), where a person denies the possession of an undesired characteristic while attributing it to someone else, i.e., “projecting” what we don’t like of us onto others.&lt;/p&gt;
&lt;p&gt;It turns out, that the concept of projection in mathematics is not that different from the Freudian one. Just make the following analogy: imagine you and foe of you are represented as vectors in a 2-dimensional cartesian plane, as $\textit{x}$ and $\textit{y}$ respectively. The way on which you would project yourself onto your foe is by tracing a perpendicular line ($\textit{z}$) from you onto him. Why perpendicular? Because this is the shortest distance between you and him, hence, the most efficient way to project yourself onto him. Now, the projection would be “how much” of yourself was “splattered” onto him, which is represented by the segment $\textit{p}$ from the origin until the point where the perpendicular line touched your foe.&lt;/p&gt;
&lt;p&gt;Now, recall that lines crossing the origin form subspaces, hence vector $\textbf{y}$ is a subspace, and that perpendicular lines form $90^{\circ}$ angles, hence the &lt;strong&gt;projection is orthogonal&lt;/strong&gt;. More formally, we can define the projection of $\textbf{x} \in \mathbb{R}^2$ onto subspace $\textit{U} \in \mathbb{R}^2$ formed by $\textbf{y}$ as:&lt;/p&gt;
&lt;p&gt;Where $\phi_{\textit{U}}(\textbf{x})$ must be the minimal distance between $\textbf{x}$ and $\textbf{y}$ (i.e., $\textbf{x}$ and $\textit{U}$), where distance is:&lt;/p&gt;
&lt;p&gt;Further, the resulting projection $\phi_{\textit{U}}(\textbf{x})$ must lie in the span of $\textit{U}$. Therefore, we can conclude that $\phi_{\textit{U}}(\textbf{x}) = \alpha \textbf{y}$, where alpha is a scalar in $\mathbb{R}$.&lt;/p&gt;
&lt;p&gt;The formula to find the orthogonal projection (I’m skipping the derivation on purpose) $\phi_{\textit{U}}(\textbf{x})$ is:&lt;/p&gt;
&lt;p&gt;In words: we take the dot product between $\textbf{x}$ and $\textbf{y}$, divide by the norm of $\textbf{y}$, and multiply by $\textbf{y}$. In this case, $\textbf{y}$ is also known as a basis vector, so we can say that $\textbf{x}$ is projected onto the basis $\textbf{y}$.&lt;/p&gt;
&lt;p&gt;Now, we want to express projections as matrices, i.e., as the matrix vector product $\textit{P}_\phi \textbf{x}$. For this, recall that matrix-scalar multiplication is &lt;em&gt;commutative&lt;/em&gt;, hence we can perform a little of algeabric manipulation to find:&lt;/p&gt;
&lt;p&gt;In this form, we can indeed express the projection as a matrix-vector multiplication, because $\textbf{y} \cdot \textbf{y}^T$ results in a symmetrix matrix, and $\Vert \textbf{y} \Vert ^2$ is a scalar, which means that it can be expressed as a matrix:&lt;/p&gt;
&lt;p&gt;In sum, the matrix $\textit{P}_\phi$ will project any vector onto $\textbf{y}$.&lt;/p&gt;
&lt;p&gt;Let’s use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; to find the projection $\textit{P}_\phi$ from $\textbf{x}$ onto a basis vector $\textbf{y}$.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# base vector
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Projection matrix for y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Projection matrix for y:
[[0.69230769 0.46153846]
 [0.46153846 0.30769231]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Projection from x onto y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Projection from x onto y:
[[2.07692308]
 [1.38461538]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s plot the vectors to make things clearer&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# origin coordinate space
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;15.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;26&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;s&quot;&gt;'vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'x-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;s&quot;&gt;'y-base-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'y-base-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;s&quot;&gt;'z-projection'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'z-projection'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;s&quot;&gt;'orthogonal-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'orthogonal-vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;'size-line'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]})&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;dim-1&lt;/th&gt;
&lt;th&gt;dim-2&lt;/th&gt;
&lt;th&gt;vector&lt;/th&gt;
&lt;th&gt;size-line&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;th&gt;0&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;x-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;1&lt;/th&gt;
&lt;td&gt;1.000000&lt;/td&gt;
&lt;td&gt;3.000000&lt;/td&gt;
&lt;td&gt;x-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;2&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;y-base-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;3&lt;/th&gt;
&lt;td&gt;3.000000&lt;/td&gt;
&lt;td&gt;2.000000&lt;/td&gt;
&lt;td&gt;y-base-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;4&lt;/th&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;0.000000&lt;/td&gt;
&lt;td&gt;z-projection&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;5&lt;/th&gt;
&lt;td&gt;2.076923&lt;/td&gt;
&lt;td&gt;1.384615&lt;/td&gt;
&lt;td&gt;z-projection&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;6&lt;/th&gt;
&lt;td&gt;1.000000&lt;/td&gt;
&lt;td&gt;3.000000&lt;/td&gt;
&lt;td&gt;orthogonal-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;7&lt;/th&gt;
&lt;td&gt;2.076923&lt;/td&gt;
&lt;td&gt;1.384615&lt;/td&gt;
&lt;td&gt;orthogonal-vector&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;16&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Chart&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mark_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'horizontal-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dim-2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertical-axis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;strokeDash&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vector'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'size-line'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;base_coor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chart&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;projections-onto-general-subspaces&quot;&gt;Projections onto general subspaces&lt;/h3&gt;
&lt;p&gt;From the previous section, we learned that the projection matrix for the one-dimensional project of $\textbf{x}$ onto $\textit{U}$ (i.e. $\textbf{y}$) $\phi_{\textit{U}}(\textbf{x})$ can be expressed as:&lt;/p&gt;
&lt;p&gt;Which implies that we the projection is entirely defined in terms of the basis subspace. Now, we are interested in projections for the general case, this is, for &lt;strong&gt;set of basis vectors $\textbf{y}_1, \cdots, \textbf{y}_m$&lt;/strong&gt;. By extension, we can define such projection as:&lt;/p&gt;
&lt;p&gt;Where $\textit{Y}$ is the matrix of basis vectors.&lt;/p&gt;
&lt;p&gt;Nothing fancy going on here: we just need to take the sum for the product between each basis vector and $\alpha$. As with the one-dimensional case, we want the projection to be the minimal distance from $\textbf{x}$ onto $\textit{Y}$, which we know implies orthogonal lines (or hyperplanes) connecting $\textbf{x}$ with $\textit{Y}$. The condition for orthogonality (again, I’m skipping the derivation on purpose) here equals:&lt;/p&gt;
&lt;p&gt;Now, recall what we really want is to find $\alpha$ (we know $\textit{Y}$ already). Therefore, with a bit of algeabric manipulation we can clear the expression above as:&lt;/p&gt;
&lt;p&gt;Such expression is known as the &lt;em&gt;pseudo-inverse&lt;/em&gt; or &lt;em&gt;Moore–Penrose inverse&lt;/em&gt; of $\textit{Y}$. To work, it requires $\textit{Y}$ to be full rank (i.e., independent columns, which should be the case for basis). It can be used to solve linear regression problems, although you’ll probably find the notation flipped as: $\alpha = (\textit{X}^T \textit{X})^{-1} \textit{X}^T \textbf{y}$ (my bad choice of notation!).&lt;/p&gt;
&lt;p&gt;Going back to our Freudian projection analogy, this is like a group of people projecting themselves onto someone else, with that person representing a rough approximation of the character of the group.&lt;/p&gt;
&lt;h3 id=&quot;projections-as-approximate-solutions-to-systems-of-linear-equations&quot;&gt;Projections as approximate solutions to systems of linear equations&lt;/h3&gt;
&lt;p&gt;Machine learning prediction problems usually require to find a solution to systems of linear equations of the form:&lt;/p&gt;
&lt;p&gt;In other words, to represent $\textbf{y}$ as linear combinations of the columns of $\textit{A}$. Unfortunately, in most cases $\textbf{y}$ is not in the column space of $\textit{A}$, i.e., &lt;em&gt;there is no way to find a linear combination of its columns to obtain the target&lt;/em&gt; $\textbf{y}$. In such cases, we can use orthogonal projections to find &lt;strong&gt;approximate solutions&lt;/strong&gt; to the system. We usually denote approximated solutions for systems of linear equations as $\hat{\textbf{y}}$. Now, $\hat{\textbf{y}}$ will be in the span of the columns of $\textit{A}$ and will be the result of projecting $\textbf{y}$ onto the subspace of the columns of $\textit{A}$. That solution will be the best (closest) approximation of $\textbf{y}$ given the span of the columns of $\textit{A}$. In sum: &lt;strong&gt;the approximated solution $\hat{\textbf{y}}$ is the orthogonal projection of $\textbf{y}$ onto $\textit{A}$&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the Japanese manga/anime series &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Fullmetal_Alchemist&quot;&gt;Fullmetal Alchemist&lt;/a&gt;&lt;/em&gt;, &lt;a href=&quot;https://fma.fandom.com/wiki/Alchemy&quot;&gt;Alchemy&lt;/a&gt; is understood as the metaphysical science of altering objects by manipulating its natural components, act known as &lt;em&gt;Transmutation&lt;/em&gt; (Rensei). There are three steps to Transmutation: (1) &lt;em&gt;Comprehension&lt;/em&gt;, to understand the atomic structure and properties of the object, (2) &lt;em&gt;Deconstruction&lt;/em&gt;, to break down the structure of the object into its fundamental elements (3) &lt;em&gt;Reconstruction&lt;/em&gt;, to use the natural flow of energy to reform the object into a new shape.&lt;/p&gt;
&lt;p&gt;Metaphorically speaking, we can understand linear combinations and matrix decompositions in analogy to &lt;em&gt;Transmutation&lt;/em&gt;. &lt;strong&gt;Matrix decomposition&lt;/strong&gt; is essentially about to break down a matrix into simpler “elements” or matrices (deconstruction), which allows us to better understand its fundamental structure (comprehension). Linear combinations are essentially about taking the fundamental elements of a matrix (i.e., set of vectors) to generate a new object.&lt;/p&gt;
&lt;p&gt;Matrix decomposition is also known as &lt;strong&gt;matrix factorization&lt;/strong&gt;, in reference the fact that matrices can be broken down into simpler matrices, more on less in the same way that Prime factorization breaks down large numbers into simpler primes (e.g., $112 = 2 \times 2 \times 2 \times 2 \times 7$).&lt;/p&gt;
&lt;p&gt;There are several important applications of matrix factorization in machine learning: clustering, recommender systems, dimensionality reduction, topic modeling, and others. In what follows I’ll cover a selection of several basic and common matrix decomposition techniques.&lt;/p&gt;
&lt;h2 id=&quot;lu-decomposition&quot;&gt;LU decomposition&lt;/h2&gt;
&lt;p&gt;There are multiple ways to decompose or factorize matrices. One of the simplest ways is by decomposition a matrix into a &lt;strong&gt;lower triangular matrix&lt;/strong&gt; and an &lt;strong&gt;upper triangular matrix&lt;/strong&gt;, the so-called &lt;strong&gt;LU or Lower-Upper decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;LU decomposition is of great interest to us since it’s one of the methods computers use to solve linear algebra problems. In particular, LU decomposition is a way to represent &lt;strong&gt;Gaussian Elimination in numerical linear algebra&lt;/strong&gt;. LU decomposition is flexible as it can be obtained from noninvertible or singular matrices, and from non-square matrices.&lt;/p&gt;
&lt;p&gt;The general expression for LU decomposition is:&lt;/p&gt;
&lt;p&gt;Meaning that $\textit{A}$ can be represented as the product of the lower triangular matrix $\textit{L}$ and upper triangular matrix $\textit{U}$. In the next sections, we explain the mechanics of the LU decomposition.&lt;/p&gt;
&lt;h3 id=&quot;elementary-matrices&quot;&gt;Elementary matrices&lt;/h3&gt;
&lt;p&gt;Our first step to approach LU decomposition is to introduce &lt;strong&gt;elementary matrices&lt;/strong&gt;. When considering matrices as functions or mappings, we can associate special meaning to a couple of basic or “elementary” operations performed by matrices. Our starting point is the &lt;strong&gt;identity matrix&lt;/strong&gt;, for instance:&lt;/p&gt;
&lt;p&gt;As we saw before, the identity matrix does not change the values of another matrix under multiplication:&lt;/p&gt;
&lt;p&gt;Because it is essentially saying: &lt;em&gt;give me $1$ of each column of the matrix&lt;/em&gt;, i.e., return the original matrix. Now, consider the following matrix:&lt;/p&gt;
&lt;p&gt;The only thing we did to the $\textit{I}$ to obtain $\textit{I}_2$ was to multiply the first row by $2$. This can be considered an elementary operation. Here is another example:&lt;/p&gt;
&lt;p&gt;Clearly, we can’t obtain $\textit{I}_3$ by multiplication only. From a column perspective, what we did was to add $2$ times the second column to the first column. Alternatively, from a row perspective, we can say we added $2$ times the first row to the second row.&lt;/p&gt;
&lt;p&gt;One last example:&lt;/p&gt;
&lt;p&gt;From the column perspective, we added $-3$ times the third column to the second column. From the row perspective, we added $-3$ times the second row to the third row.&lt;/p&gt;
&lt;p&gt;You can probably see the pattern by now: by performing simple or “elementary” column or row operations, this is, &lt;em&gt;multiplication&lt;/em&gt; and &lt;em&gt;addition&lt;/em&gt;, we can obtain any lower triangular matrix. This type of matrices are what we call &lt;strong&gt;elementary matrices&lt;/strong&gt;. In a way, we can say elementary matrices “encode” fundamental column and row operations. To see this, consider the following generic matrix:&lt;/p&gt;
&lt;p&gt;Let’s what happens when we multiply $\textit{A}\textit{I}_3$:&lt;/p&gt;
&lt;p&gt;The result of $\textit{A}\textit{I}_3$ reflects the same elementary operations we performed on $\textit{I}$ to obtain $\textit{I}_3$ from the &lt;strong&gt;column perspective&lt;/strong&gt;: to add $2$ times the second column to the first one.&lt;/p&gt;
&lt;p&gt;Now consider what happens when we multiply from the left:&lt;/p&gt;
&lt;p&gt;Now we obtain the same elementary operations we performed on $\textit{I}$ to obtain $\textit{I}_3$ from the &lt;strong&gt;row perspective&lt;/strong&gt;: to add $2$ times the first row to the second one.&lt;/p&gt;
&lt;h3 id=&quot;the-inverse-of-elementary-matrices&quot;&gt;The inverse of elementary matrices&lt;/h3&gt;
&lt;p&gt;A nice property of elementary matrices, is that the inverse is simply the opposite operation. For instance, the inverse of $\textit{I}_2$ is:&lt;/p&gt;
&lt;p&gt;This is because instead of multiplying the first row of $\textit{I}$ by $2$, we divide it by $2$. Similarly, the inverse of $\textit{I}_3$ is:&lt;/p&gt;
&lt;p&gt;Again, instead of adding $2$, we add $-2$ (or substract $2$). Finally, the inverse of $\textit{I}_4$ is:&lt;/p&gt;
&lt;p&gt;The reason we care about elementary matrices and its inverse is that it will be fundamental to understand LU decomposition.&lt;/p&gt;
&lt;h3 id=&quot;lu-decomposition-as-gaussian-elimination&quot;&gt;LU decomposition as Gaussian Elimination&lt;/h3&gt;
&lt;p&gt;Let’s briefly recall Gaussian Elimination: it’s an robust algorithm to solve systems of linear equations, by sequentially applying three elementary transformations:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Addition and subtraction of two equations (rows)&lt;/li&gt;
&lt;li&gt;Multiplication of an equation (rows) by a number&lt;/li&gt;
&lt;li&gt;Switching equations (rows)&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Gaussian Elimination will reduce matrices to its &lt;strong&gt;row echelon form&lt;/strong&gt;, which is an upper triangular matrix, with zero rows at the bottom, and zeros below the pivot for each column.&lt;/p&gt;
&lt;p&gt;It turns out, there is a clever way to organize the steps from Gaussian Elimination: with &lt;strong&gt;elementary matrices&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Consider the following matrix $\textit{A}$:&lt;/p&gt;
&lt;p&gt;The first step consist of substracting two times row 1 from row 1. Before, we represented this operation as $R_2 - 2R_1$, and write down the result, which is:&lt;/p&gt;
&lt;p&gt;Alternatively, as we learned in the previous section, &lt;em&gt;we can represent row operations as multiplication by elementary matrices&lt;/em&gt;, to obtain the same result. Since we want to substract $2$ times the first row from the second, we need to (1) multiply from the left, and (2) add a $-2$ to the first element of the second row:&lt;/p&gt;
&lt;p&gt;You don’t have to believe me. Let’s confirm this is correct with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;14&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;23&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[  1,   3,   5],
       [  0,  -4, -11],
       [  1,   3,   2]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As you can see, the result is exactly what we obtained before by $R_2 - 2R_1$. But, we are not done. We still need to get rid of the $1$ and $3$ in the third row. For this, we would normally do $R_3 - R_1$ to obtain:&lt;/p&gt;
&lt;p&gt;Again, we can encode this using elementary matrices as:&lt;/p&gt;
&lt;p&gt;Once again, let’s confirm this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;14&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;23&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[  1,   3,   5],
       [  0,  -4, -11],
       [  0,   0,  -3]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Indeed, the result is correct. At this point, we have reduced $\textit{A}$ to its &lt;strong&gt;row echelon form&lt;/strong&gt;. We will call $\textit{U}$ to the resulting matrix from $\textit{l} \textit{A}$, as it is an &lt;em&gt;upper triangular matrix&lt;/em&gt;. Hence, we arrived to the identity:&lt;/p&gt;
&lt;p&gt;This is not quite LU decomposition. To get there, we just need to multiply both sides of the equality by the inverse of $\textit{l}$, that we will call $\textit{L}$, which yields:&lt;/p&gt;
&lt;p&gt;There you go: we arrived to the LU decomposition expression. As a final note, recall that the inverse of $\textit{l}$ is:&lt;/p&gt;
&lt;p&gt;Let’s confirm with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; this works, by multiplying $\textit{L}$ by $\textit{U}$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;14.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;24&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# inverse of l
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
               &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# upper triangular resulting from Gaussian Elimination
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 1,  3,  5],
       [ 2,  2, -1],
       [ 1,  3,  2]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Indeed, we recover $\textit{A}$ by multiplying $\textit{L}\textit{U}$.&lt;/p&gt;
&lt;h3 id=&quot;lu-decomposition-with-pivoting&quot;&gt;LU decomposition with pivoting&lt;/h3&gt;
&lt;p&gt;If you recall the three elementary operations allowed in Gaussian Elimination, we had: (1) multiplication, (2) addition, (3) switching. At this point, we haven’t seen switching with LU decomposition. It turns out, that LU decomposition does not work when switching or permutations of rows are required to solve a system of linear equations. Further, even when pivoting is not required to solve a system, the numerical stability of Gaussian Elimination when implemented in computers is problematic, and pivoting helps to tackle that issue as well.&lt;/p&gt;
&lt;p&gt;Let’s see a simple example of pivoting. Consider the following matrix $\textit{A}$:&lt;/p&gt;
&lt;p&gt;In this case, we can’t get rid of the first $1$ in the second column by substraction. If we do that, we obtain:&lt;/p&gt;
&lt;p&gt;Which is the opposite of what we want. A simple way to fix this is by switching rows 1 and 2 as:&lt;/p&gt;
&lt;p&gt;And then substracting row 1 from row 2 to obtain:&lt;/p&gt;
&lt;p&gt;Bam! Problem fixed. Now, as with multiplication and addition, we can &lt;strong&gt;represent permutations with matrices&lt;/strong&gt; as well. In particular, by using &lt;strong&gt;permutation matrices&lt;/strong&gt;. For our previous example, we can do:&lt;/p&gt;
&lt;p&gt;Let’s confirm this is correct with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[1, 1],
       [0, 1]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It works. Now we can put all the pieces together and decompose $\textit{A}$ by using the following expression:&lt;/p&gt;
&lt;p&gt;This is known as LU decomposition with pivoting. An alternative expression of the same decomposition is:&lt;/p&gt;
&lt;p&gt;In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Python&lt;/code&gt;, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SciPy&lt;/code&gt; to perform LUP decomposition by using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linalg.lu&lt;/code&gt; method. Let’s decompose a larger matrix to make things more interesting.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;13.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;22&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.linalg&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Pivot matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Pivot matrix:
[[0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Lower triangular matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Lower triangular matrix:
[[ 1.    0.    0.    0.  ]
 [ 0.75  1.    0.    0.  ]
 [ 0.5  -0.29  1.    0.  ]
 [ 0.25 -0.43  0.33  1.  ]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Upper triangular matrix:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Upper triangular matrix:
[[ 8.    7.    9.    5.  ]
 [ 0.    1.75  2.25  4.25]
 [ 0.    0.   -0.86 -0.29]
 [ 0.    0.    0.    0.67]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can confirm the decomposition is correct by multiplying the obtained matrices&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A_recover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'PLU multiplicatin:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_recover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;PLU multiplicatin:
[[2 1 1 0]
 [4 3 3 1]
 [8 7 9 5]
 [6 7 9 8]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We recover $\textit{A}$ perfectly.&lt;/p&gt;
&lt;h2 id=&quot;qr-decomposition&quot;&gt;QR decomposition&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;QR decomposition&lt;/strong&gt; or &lt;strong&gt;QR factorization&lt;/strong&gt;, is another very relevant decomposition in the context of numerical linear algebra. As with LU decomposition, It can be used to solve systems of linear equations like least square problems and to find eigenvalues of a general matrix.&lt;/p&gt;
&lt;p&gt;QR decomposition works by decomposing $\textit{A}$ into an orthogonal matrix $\textit{Q}$, and a upper traingular matrix $\textit{R}$ as:&lt;/p&gt;
&lt;p&gt;Next, we will review a few concepts to properly explain QR decomposition.&lt;/p&gt;
&lt;h3 id=&quot;orthonormal-basis&quot;&gt;Orthonormal basis&lt;/h3&gt;
&lt;p&gt;In previous sections we learned about &lt;em&gt;basis&lt;/em&gt; and &lt;em&gt;orthogonal basis&lt;/em&gt;. Specifically, we said that a set of $n$ linearly independent column vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt;. We also said that a pair of vectors $\bf{x}$ and $\bf{y}$ are &lt;strong&gt;orthogonal&lt;/strong&gt; if their inner product is zero, $\langle x,y \rangle = 0$ or $\textbf{x}^T \textbf{y} = 0$. Consequently, &lt;em&gt;a set of orthogonal vectors form an orthogonal basis for a matrix $\textit{A}$ and for the vector space spanned by such matrix&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To go from orthogonal basis vectors to &lt;strong&gt;orthonomal basis vectors&lt;/strong&gt;, we just need to divide each vector by its lenght or norm. When we divide a basis vector by its norm we obtain a &lt;strong&gt;unit basis vector&lt;/strong&gt;. More formally, a set of vectors $\textbf{x}_1, \cdots,\textbf{x}_n$ is orthonormal if:&lt;/p&gt;
&lt;p&gt;In words: when we take the inner product of a pair of orthogonal vectors, it results in $0$; when we take the inner product of a vector with itself, it results in $1$.&lt;/p&gt;
&lt;p&gt;For instance, consider $\textbf{x}$ and $\textbf{y}$:&lt;/p&gt;
&lt;p&gt;To obtain the normalized version of $\textbf{x}$ or $\textbf{y}$, we divide by its Euclidean norm as:&lt;/p&gt;
&lt;p&gt;We add a “hat” to the normalized vector to distinguish it from the un-normalized version.&lt;/p&gt;
&lt;p&gt;Let’s try an example with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. I’ll define vectors $\textbf{x},\textbf{y} \in \mathbb{R}^3$, compute its Euclidean norm, and then perform element-wise division $\frac{\textbf{x}}{\Vert \textbf{x} \Vert}$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;11&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;17&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# euclidean norm of x and y
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# normalized x or unit vector
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Euclidean norm of x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_norm&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Euclidean norm of y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_norm&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Normalized x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Normalized y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Euclidean norm of x:
5.0

Euclidean norm of y:
5.385164807134504

Normalized x:
[[0.6]
 [0.8]
 [0. ]]

Normalized y:
[[-0.74278135]
 [ 0.55708601]
 [ 0.37139068]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can confirm that the Euclidean norm of the normalized versions of $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$ equals $1$ by:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Euclidean norm of normalized x:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Euclidean norm of normalized y:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Euclidean norm of normalized x:
1.0

Euclidean norm of normalized y:
1.0
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Taking $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$ as a set, we can confirm the conditions for the definition of orthonormal vectors are correct.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Inner product normalized vectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Inner product normalized x with itself:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Inner product normalized y with itself:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Inner product normalized vectors:
[[-0.]]

Inner product normalized x with itself:
[[1.]]

Inner product normalized y with itself:
[[1.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Sets of vectors can be represented as matrices. &lt;strong&gt;We denote as $\textit{Q}$ the special case of a matrix composed of orthonormal vectors&lt;/strong&gt;. The same properties we defined for sets of vectors hold when represented in matrix form.&lt;/p&gt;
&lt;h3 id=&quot;orthonormal-basis-transpose&quot;&gt;Orthonormal basis transpose&lt;/h3&gt;
&lt;p&gt;A nice property of $\textit{Q}$ is that &lt;em&gt;the matrix product with its transpose equals the identity&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;This is true even when $\textit{Q}$ is not square. Let’s see this with the $\textit{Q} \in \mathbb{R}^{3 \times 3}$ orthonormal matrix resulting from stacking $\hat{\textbf{x}}$ and $\hat{\textbf{y}}$.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Orthonormal matrix Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Orthonormal matrix Q:
[[ 0.6        -0.74278135]
 [ 0.8         0.55708601]
 [ 0.          0.37139068]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now we confirm $\textit{Q}^T \textit{Q}= \textit{I}$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 1., -0.],
       [-0.,  1.]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This property will be useful for several applications. For instance, the &lt;em&gt;coupling matrix&lt;/em&gt; or &lt;em&gt;correlation matrix&lt;/em&gt; of a matrix $\textit{A}$ equals $\textit{A}^T \textit{A}$. If we are able to transform the vectors of $\textit{A}$ into orthonormal vectors, such expressions reduces to $\textit{Q}^T \textit{Q}= \textit{I}$. Other applications are the Fourier series and Least Square problems (as we will see later).&lt;/p&gt;
&lt;h3 id=&quot;gram-schmidt-orthogonalization&quot;&gt;Gram-Schmidt Orthogonalization&lt;/h3&gt;
&lt;p&gt;In the previous section, I selected orthogonal vectors to illustrate the idea of an orthonormal basis. Unfortunately, in most cases, matrices are not full rank, i.e., not composed of a set of orthogonal vectors. Fortunately, there are ways to &lt;em&gt;transform a set of non-orthogonal vectors into orthogonal vectors&lt;/em&gt;. This is the so-called &lt;strong&gt;Gram-Schmidt orthogonalization procedure&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The Gram-Schmidt orthogonalization consist of &lt;em&gt;taking the vectors of a matrix, one by one, and making each subsequent vector orthonormal to the previous one&lt;/em&gt;. This is easier to grasp with an example. Consider the matrix $\textit{A}$:&lt;/p&gt;
&lt;p&gt;What we want to do, is to find the set of orthonormal vectors $\textbf{q}_1, \textbf{q}_2, \textbf{q}_3$, starting from the columns of $\textit{A}$, i.e., $\textbf{a}_1, \textbf{a}_2, \textbf{a}_3$. We can select any vector to begin with. Recall that we normalize vectors by dividing by its norm as:&lt;/p&gt;
&lt;p&gt;Let’s approach this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A simple way to check the columns of $\textit{A}$ are not orthonormal is to compute $\textit{A}^T \textit{A}$, which should be equal to the identity in the orthonormal case.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 62, -34,   6],
       [-34,  35, -10],
       [  6, -10,   6]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To build our orthogonal set, we begin by denoting $\textbf{a}_1$ as $\textbf{q}_1$.&lt;/p&gt;
&lt;p&gt;Our &lt;strong&gt;first step&lt;/strong&gt; is to generate the vector $\textbf{q}_2$ from $\textbf{a}_2$ such that is orthogonal to $\textbf{q}_1$ (i.e., $\textbf{a}_1$). To do this, we start with $\textbf{a}_2$ and subtract its projection along $\textbf{q}_1$, which yields the following expression:&lt;/p&gt;
&lt;p&gt;Think in this expression carefully. What are we doing, is to subtract $\frac{\textbf{q}_1^T \textbf{a}_2}{\textbf{q}_1^T \textbf{q}_1}$ &lt;em&gt;times&lt;/em&gt; the first column from the second column. Let’s denote $\frac{\textbf{q}_1^T \textbf{a}_2}{\textbf{q}_1^T \textbf{q}_1}$ as $\alpha$, then, we can rewrite our expression as:&lt;/p&gt;
&lt;p&gt;As we will see, $\alpha$ is a scalar, so effectively we are substracting an scaled version of column one from column two. The figure below express geometrically, what I have been saying: the &lt;em&gt;non-orthogonal&lt;/em&gt; $\textbf{a}_2$ is projected onto $\textbf{q}_1$. Then, we subtract the projection $\textbf{p}$ from $\textbf{a}_2$ to obtain $\textbf{q}_2$ which is orthogonal to $\textbf{q}_1$ as you can appreciate visually (recall $\textbf{a}_1 = \textbf{q}_1$).&lt;/p&gt;
&lt;p&gt;Keep these ideas in mind as it will be important later for QR decomposition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 16: Orthogonalization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-gram-schmidt.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Let’s compute $\textbf{q}_2$ now:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s check that $\textbf{q}_1$ and $\textbf{q}_2$ are actually orthogonal. If so, their dot product should be $0$.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Next, we need to generate $\textbf{q}_3$ from $\textbf{a}_3$. This time, we want $\textbf{q}_3$ to be orthogonal to both $\textbf{q}_1$ and $\textbf{q}_2$. Therefore, we need to subtract its projection along $\textbf{q}_1$ and $\textbf{q}_2$, which yields:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Verify orthogonality&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Dot product q1 and q3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Dot product q2 and q3:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Dot product q1 and q3:
-0.0

Dot product q2 and q3:
0.0
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can put $\textbf{q}_1, \textbf{q}_2, \textbf{q}_3$ into $\textit{Q}’$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Orthogonal matrix Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Orthogonal matrix Q:
[[ 2.          2.09677419 -1.33333333]
 [ 7.          0.83870968  0.66666667]
 [-3.          3.35483871  0.66666667]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The reason we call this matrix $\textit{Q}’$ is that although vectors are orthogonal, they are not normal.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Norms of vectors in Q-prime:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Norms of vectors in Q-prime:
[7.87400787 4.04411161 1.63299316]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We rename $\textit{Q}’$ to $\textit{Q}$ by normalizing its vectors.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q_norms&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([1., 1., 1.])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To confirm we did this right, let’s evaluate $\textit{Q}^T \textit{Q}$, that should return the identity:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ 1., -0., -0.],
       [-0.,  1.,  0.],
       [-0.,  0.,  1.]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There you go: we performed Gram-Schmidt orthogonalization of $\textit{A}$&lt;/p&gt;
&lt;h3 id=&quot;qr-decomposition-as-gram-schmidt-orthogonalization&quot;&gt;QR decomposition as Gram-Schmidt Orthogonalization&lt;/h3&gt;
&lt;p&gt;Gaussian Elimination can be represented as LU decomposition. Similarly, &lt;strong&gt;Gram-Schmidt Orthogonalization can be represented as QR decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We learned $\textit{Q}$ is an orthonormal matrix. Now let’s examine $\textit{R}$, which is an upper triangular matrix. In LU decomposition, we used &lt;strong&gt;elementary matrices&lt;/strong&gt; to perform &lt;em&gt;row operations&lt;/em&gt;. Similarly, in the case of QR decomposition, we will use &lt;strong&gt;elementary matrices&lt;/strong&gt; to perform &lt;em&gt;column operations&lt;/em&gt;. We used a lower triangular matrix to perform row operations in LU decomposition by multiplying $\textit{A}$ from the &lt;em&gt;left side&lt;/em&gt;. This time, we will use an upper triangular matrix to perform column operations in QR decomposition by multiplying $\textit{A}$ from the &lt;em&gt;right side&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Once again, our starting point is the identity matrix. The idea is to alter the identity with the operations we want to perform over $\textit{A}$. Consider the matrix from our previous example:&lt;/p&gt;
&lt;p&gt;What we did in out first step, wast to subtract $\alpha = \frac{\textbf{a}_1 \cdot \textbf{a}_2} {\textbf{a}_1 \cdot \textbf{a}_1}$ of column $\textbf{a}_1$ from column $\textbf{a}_2$. Let’s compute $\alpha$ first:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;12&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;19&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'alpha factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;alpha factor:-0.55
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Now we need to subtract $\alpha = -0.55$ times $\textbf{a}_1$ from $\textbf{a}_2$. We can represent this operation with an &lt;strong&gt;elementary matrix&lt;/strong&gt;, by doing applying the same operations the identity:&lt;/p&gt;
&lt;p&gt;Next, we have to subtract $\beta = \frac{\textbf{a}_1 \cdot \textbf{a}_3} {\textbf{a}_1 \cdot \textbf{a}_1}$ times column $\textbf{a}_1$ and $\gamma = \frac{\textbf{a}_2 \cdot \textbf{a}_3} {\textbf{a}_2 \cdot \textbf{a}_2}$ times $\textbf{a}_2$ from $\textbf{a}_3$. Let’s compute the new $\beta$ and $\gamma$:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'beta factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'gamma factor:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;beta factor:0.1
gamma factor:-0.29
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can add this operations to our elementary matrix by subtracting $0.1$ times the first column from the third, and $-0.29$ times the second from the third:&lt;/p&gt;
&lt;p&gt;The last step is to normalize each vector of $\textit{l}$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;At this point, we should be able to recover $\textit{A}$:&lt;/p&gt;
&lt;p&gt;As the matrix product of $\textit{Q}’$ and $\textit{l}$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Q-prime and l product:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_prime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Q-prime and l product:
[[ 2.  1. -2.]
 [ 7. -3.  1.]
 [-3.  5. -1.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;It works! Now, to recover $\textit{Q}$ will be difficult because of numerical stability and approximation issues in how we have computed things. Actually, if you remove the rounding from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;np.round(Q_prime @ l)&lt;/code&gt; you will obtain different numbers. Fortunately, there is no need to compute $\textit{Q}$ and $\textit{R}$ by hand. We follow the previous steps merely for pedagogical purposes. In &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;, we can compute the QR decomposition as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Q_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;qr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Let’s compare our $\textit{Q}$ with $\textit{Q}_1$&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Q:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Q:
[[ 0.25400025  0.51847585 -0.81649658]
 [ 0.88900089  0.20739034  0.40824829]
 [-0.38100038  0.82956136  0.40824829]]

Q:
[[-0.25400025  0.51847585 -0.81649658]
 [-0.88900089  0.20739034  0.40824829]
 [ 0.38100038  0.82956136  0.40824829]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The numbers are the same, but some signs are flipped. This stability and approximation issues is why you probably always want to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; functions (when available).&lt;/p&gt;
&lt;h2 id=&quot;determinant&quot;&gt;Determinant&lt;/h2&gt;
&lt;p&gt;If matrices had personality, the &lt;strong&gt;determinant&lt;/strong&gt; would be the personality trait that reveals most information about the matrix character. The determinant of a matrix is a single number that tells &lt;strong&gt;whether a matrix is invertible or singular&lt;/strong&gt;, this is, whether its columns are linearly independent or not, which is one of the most important things you can learn about a matrix. Actually, the name “determinant” refers to the property of “determining” if the matrix is singular or not. Specifically, for an square matrix $\textit{A} \in \mathbb{R}^{n \times n}$, a determinant equal to $0$, denoted as $\text{det}(\textit{A}=0)$, implies &lt;em&gt;the matrix is singular&lt;/em&gt; (i.e., noninvertible), whereas a determinant equal to $1$, denoted as $\text{det}(\textit{A})=1$, implies the &lt;em&gt;matrix is not singular&lt;/em&gt; (i.e., invertible). Although determinants can reveal if matrices are singular with a single number, it’s not used for large matrices as Gaussian Elimination is faster.&lt;/p&gt;
&lt;p&gt;Recall that matrices can be thought of as function action on vectors or other matrices. Thus, the determinant can also be considered a linear mapping of a matrix $\textit{A}$ onto a single number. But, what does that number mean? So far, we have defined determinants based on their utility of determining matrix invertibility. Before going into the calculation of determinants, let’s examine determinants from a geometrical perspective to gain insight into the meaning of determinants.&lt;/p&gt;
&lt;h3 id=&quot;determinant-as-measures-of-volume&quot;&gt;Determinant as measures of volume&lt;/h3&gt;
&lt;p&gt;From a geometric perspective, determinants indicate the $\textit{sign}$ &lt;strong&gt;area of a parallelogram&lt;/strong&gt; (e.g., a rectangular area) and the $\textit{sign}$ &lt;strong&gt;volume of the parallelepiped&lt;/strong&gt;, for a matrix whose columns consist of the basis vectors in Euclidean space.&lt;/p&gt;
&lt;p&gt;Let’s parse out the above phrase: the $\textit{sign}$ area indicates the absolute value of the area, and the $\textit{sign}$ volume equals the absolute value of the volume. You may be wondering why we need to take the absolute value since real-life objects can’t have negative area or volume. In linear algebra, we say the area of a parallelogram is &lt;strong&gt;negative&lt;/strong&gt; when the vectors forming the figure are &lt;em&gt;clockwise oriented&lt;/em&gt; (i.e., negatively oriented), and &lt;strong&gt;positive&lt;/strong&gt; when the vectors forming the figure are &lt;em&gt;counterclockwise oriented&lt;/em&gt; (i.e., positively oriented).&lt;/p&gt;
&lt;p&gt;Here is an example of a matrix $\textit{A}$ with vectors &lt;em&gt;clockwise&lt;/em&gt; or &lt;em&gt;negatively&lt;/em&gt; oriented:&lt;/p&gt;
&lt;p&gt;The elements of the first column, indicate the first vector of the matrix, while the elements of the second column, the second vector of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from left to right, i.e., &lt;em&gt;clockwise&lt;/em&gt;, meaning that the vectors are &lt;strong&gt;negatively oriented&lt;/strong&gt;, and the &lt;strong&gt;area of the matrix will be negative&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here is the same matrix $\textit{A}$ with vectors &lt;em&gt;counterclockwise&lt;/em&gt; or &lt;em&gt;positively&lt;/em&gt; oriented:&lt;/p&gt;
&lt;p&gt;Again, the elements of the &lt;em&gt;first column&lt;/em&gt;, indicate the &lt;em&gt;first vector&lt;/em&gt; of the matrix, while the elements of the &lt;em&gt;second column&lt;/em&gt;, the &lt;em&gt;second vector&lt;/em&gt; of the matrix. Therefore, when we measure the area of the parallelogram formed by the pair of vectors, we move from &lt;em&gt;right to left&lt;/em&gt;, i.e., &lt;em&gt;counterclockwise&lt;/em&gt;, meaning that the vectors are &lt;strong&gt;positively oriented&lt;/strong&gt;, and the &lt;strong&gt;area of the matrix will be positive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The figure below exemplifies what I just said.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 17: Vector orientation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-determinant-orientation.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The situation for the $\textit{sign}$ volume of the parallelepiped is no different: when the vectors are &lt;em&gt;counterclockwise&lt;/em&gt; oriented, we say the vectors are &lt;em&gt;positively oriented&lt;/em&gt; (i.e., positive volume); when the vectors are &lt;em&gt;clockwise&lt;/em&gt; oriented, we say the vectors are &lt;em&gt;negatively oriented&lt;/em&gt; (i.e., negative volume).&lt;/p&gt;
&lt;h3 id=&quot;the-2-x-2-determinant&quot;&gt;The 2 X 2 determinant&lt;/h3&gt;
&lt;p&gt;Recall that matrices are invertible or nonsingular when their columns are linearly independent. By extension, the determinant allow us to whether the columns of a matrix a linearly independent. To understand this method, let’s examine the $2 \times 2$ special case first.&lt;/p&gt;
&lt;p&gt;Consider a square matrix as:&lt;/p&gt;
&lt;p&gt;How can we decide whether the columns are linearly independent? A strategy that I often use in simple cases like this, is just to examine whether the second column equals the first column times some factor. In the case of $\textit{A}$ is easy to see that the second column equals four times the first column, so the columns are linearly &lt;em&gt;dependent&lt;/em&gt;. We can express such criteria by comparing the &lt;em&gt;elementwise division&lt;/em&gt; between each element of the second column by each element of the first column as:&lt;/p&gt;
&lt;p&gt;We obtain that both entries equal $4$, meaning that the second column can be divided exactly by the first column (i.e., linearly &lt;em&gt;dependent&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Consider this matrix now:&lt;/p&gt;
&lt;p&gt;Let’s try again our method for $\textit{B}$:&lt;/p&gt;
&lt;p&gt;Now we got into a problem because division by $0$ is undefined, so we can determine the relationship between columns of $\textit{B}$. Yet, by inspection, we can see the first column is simply $0$ times the second column, therefore linearly dependent. Here is when &lt;strong&gt;determinants&lt;/strong&gt; come to the rescue.&lt;/p&gt;
&lt;p&gt;Consider the generic matrix:&lt;/p&gt;
&lt;p&gt;According to our previous strategy, we had:&lt;/p&gt;
&lt;p&gt;This is, we tested the elementwise division of the second column by the first column. Before, we failed because of division, so we probably want a method that does not involve it. Notice that we can rearrange our expression as:&lt;/p&gt;
&lt;p&gt;Let’s try again with this method for $\textit{A}$:&lt;/p&gt;
&lt;p&gt;And for $\textit{B}$:&lt;/p&gt;
&lt;p&gt;It works. Indeed, $ad = bc$ are equal for both matrices, $\textit{A}$ and $\textit{B}$, meaning their columns are linearly dependent. Finally, notice that we can rearange all the terms on one side of the equation as:&lt;/p&gt;
&lt;p&gt;There you go: the above expression is what is known as the &lt;strong&gt;determinant of a matrix&lt;/strong&gt;. We denote the determinant as:&lt;/p&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;h3 id=&quot;the-n-x-n-determinant&quot;&gt;The N X N determinant&lt;/h3&gt;
&lt;p&gt;As matrices larger, computing the determinant gets more complicated. Consider the $3 \times 3$ case as:&lt;/p&gt;
&lt;p&gt;The problem now is that linearly independent columns can be either: (1) multiples of another column, and (2) linear combinations of pairs of columns. The determinant for a $3 \times 3$ is:&lt;/p&gt;
&lt;p&gt;Such expression is hard to memorize, and it will get even more complicated for larger matrices. For instance, the $4 \times 4$ entails 24 terms. As with most things in mathematics, there is a general formula to express the determinant compactly, which is known as the Leibniz’s formula:&lt;/p&gt;
&lt;p&gt;Where $\sigma$ computes the permutation for the rows and columns vectors of the matrix. Is of little importance for us to break down the meaning of this formula since we are interested in its applicability and conceptual value. What is important to notice, is that for an arbitrary square $n \times n$ matrix, we will have $n!$ terms to sum over. For instance, for a $10 \times 10$ matrix, $10! = 3,628,800$, which is a gigantic number considering the size of the matrix. In machine learning, we care about matrices with thousands or even millions of columns, so there is no use for such formula. Nonetheless, this does not mean that the determinant is useless, but the direct calculation with the above algebraic expression is not used.&lt;/p&gt;
&lt;h3 id=&quot;determinants-as-scaling-factors&quot;&gt;Determinants as scaling factors&lt;/h3&gt;
&lt;p&gt;When we think in matrices as linear mappings, this is, as functions applied to vectors (or vectors spaces), the determinant acquires an intuitive geometrical interpretation: &lt;strong&gt;as the factor by which areas are scaled under a mapping&lt;/strong&gt;. Plainly, if you do a mapping or transformation, and the area increases by a factor of $3$, then the determinant of the transformation matrix equals $3$. Consider the matrix $\textit{A}$ and the basis vector $\textbf{x}$:&lt;/p&gt;
&lt;p&gt;Is easy to see that the parallelogram formed by the basis vectors of $\textbf{x}$ is $1 \times 1 = 1$. When we apply $\textit{A}\textbf{x}$, we get:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([4, 3])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Meaning that the vertical axis was scaled by $4$ and the horizontal axis by $3$, hence, the new parallelogram has area $4 \times 3 = 12$. Since the new area has increased by a factor of $12$, the determinant $\vert \textit{A} \vert = 12$. Although we exemplified this with the basis vectors in $\textit{x}$, the determinant of $\textit{A}$ for mappings of the entire vector space. The figure below visually illustrates this idea.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 18: Determinants&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-determinant-scaling.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-importance-of-determinants&quot;&gt;The importance of determinants&lt;/h3&gt;
&lt;p&gt;Considering that calculating the determinant is not computationally feasible for large matrices and that we can determine linear independence via Gaussian Elimination, you may be wondering what’s the point of learning about determinants in the first place. I also asked myself more than once. It turns out that determinants play a crucial conceptual role in other topics in matrix decomposition, particularly eigenvalues and eigenvectors. Some books I reviewed devote a ton of space to determinants, whereas others (like Strang’s Intro to Linear Algebra) do not. In any case, we study determinants mostly because of its conceptual value to better understand linear algebra and matrix decomposition.&lt;/p&gt;
&lt;h2 id=&quot;eigenthings&quot;&gt;Eigenthings&lt;/h2&gt;
&lt;p&gt;Eigenvectors, eigenvalues, and their associated mathematical objects and properties (which I call “Eigen-things”) have important applications in machine learning like Principal Component Analysis (PCA), Spectral Clustering (K-means), Google’s PageRank algorithm, Markov processes, and others. Next, we will review several of these “eigen-things”.&lt;/p&gt;
&lt;h3 id=&quot;change-of-basis&quot;&gt;Change of basis&lt;/h3&gt;
&lt;p&gt;Previously, we said that a set of $n$ linearly independent vectors with $n$ elements forms a &lt;strong&gt;basis&lt;/strong&gt; for a vector space. For instance, we say that the &lt;em&gt;orthogonal&lt;/em&gt; pair of vectors $\textbf{x}$ and $\textbf{y}$ (or horizontal and vertical axes), describe the Cartesian plane or $\mathbb{R}^2$ space. Further, if we think in the $\textbf{x}$ and $\textbf{y}$ pair as unit vectors, then we can describe any vector in $\mathbb{R}^2$ as a linear combination of $\textbf{x}$ and $\textbf{y}$. For example, the vector:&lt;/p&gt;
&lt;p&gt;Can be described as scaling the unit vector $\textbf{x}=\begin{bmatrix} 1 \ 0 \end{bmatrix}$ by $-3$, and scaling the unit vector $\textbf{y}=\begin{bmatrix} 0 \ 1 \end{bmatrix}$ by $-1$.&lt;/p&gt;
&lt;p&gt;If you are like me, you have probably gotten use to the idea of describing any 2-dimensional space as $\textbf{x}$ and $\textbf{y}$ coordinates, with $\textbf{x}$ lying perfectly horizontal and $\textbf{y}$ perpendicular to it, as if this were the only natural way of thinking on coordinates in space. It turns out, there is nothing “natural” about it. You could literally draw a pair of orthogonal vectors on any orientation in space, define the first one as $\textbf{x’}=\begin{bmatrix} 1 \ 0 \end{bmatrix}$, and the second one as $\textbf{y’}=\begin{bmatrix} 0 \ 1 \end{bmatrix}$, and that would be perfectly fine. It may look different, but every single mathematical property we have studied so far about vectors would hold. For instance, in Fig 19. the &lt;strong&gt;alternative coordinates&lt;/strong&gt; $\textbf{x’}$ and $\textbf{y’}$ are equivalent to the vectors $\textbf{a}=\begin{bmatrix} -2 \ 2 \end{bmatrix}$ and $\textbf{b}=\begin{bmatrix} 2 \ 2 \end{bmatrix}$, in the standard $\textbf{x}$ and $\textbf{y}$ coordinates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 19: Change of basis&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-change-basis.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The question now is how to “move” from one set of basis vectors to the other. The answer is with &lt;strong&gt;linear mappings&lt;/strong&gt;. We know already that $\textbf{x’, y’}$ equals to $\textbf{a}=\begin{bmatrix} -2 \ 2 \end{bmatrix}$ and $\textbf{b}=\begin{bmatrix} 2 \ 2 \end{bmatrix}$ in $\textbf{x, y}$ coordinates. To find the values of $\textbf{x, y}$ in $\textbf{x’, y’}$, we need to take the &lt;strong&gt;inverse of $\textit{T}$&lt;/strong&gt;. Think about it in this way: we represented $\textbf{x’=a, y’=a}$ in $\textbf{x, y}$ by scaling its unit vectors by the transformation matrix $\textit{T}$ as:&lt;/p&gt;
&lt;p&gt;Now, to do the &lt;em&gt;opposite&lt;/em&gt;, i.e., to “translate” the values of the coordinates $\textbf{x, y}$ to values in $\textbf{x’, y’}$, we scale $\textbf{x, y}$ by the inverse of $\textit{T}$ as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;11.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;18&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Transformation or mapping matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Inverse of T
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# x, y vectors
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;x, y vectors in x', y' coordinates:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;x, y vectors in x', y' coordinates:
[[ 0.25  0.25]
 [-0.25  0.25]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;That is our answer: $\textbf{x, y}$ equals to $\begin{bmatrix}0.25 \ -0.25 \end{bmatrix}$ and $\begin{bmatrix}0.25 \ 0.25 \end{bmatrix}$ in $\textbf{x’, y’}$ coordinate space. &lt;strong&gt;Fig. 19&lt;/strong&gt; illustrate this as well.&lt;/p&gt;
&lt;p&gt;This may become clearer by mapping $\textbf{c}=\begin{bmatrix}-3 \ -1 \end{bmatrix}$ in $\textbf{x, y}$, onto $\textbf{c’}$ in $\textbf{x’, y’}$ alternative coordinates. To do the mapping, again, we need to multiply $\textbf{c}$ by $\textit{T}^{-1}$. Let’s try this out with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# vector to map
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Vector a=[1,3] in x&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; and y&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; basis:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Vector a=[1,3] in x' and y' basis:
[[-1. ]
 [ 0.5]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In &lt;strong&gt;Fig. 19&lt;/strong&gt;, we can confirm the mapping by simply visual inspection.&lt;/p&gt;
&lt;h3 id=&quot;eigenvectors-eigenvalues-and-eigenspaces&quot;&gt;Eigenvectors, Eigenvalues and Eigenspaces&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Eigen&lt;/em&gt; is a German word meaning “own” or “characteristic”. Thus, roughly speaking, the eigenvector and eigenvalue of a matrix refer to their “characteristic vector” and “characteristic value” for that vector, respectively. As a cognitive scientist, I like to think in eigenvectors as the “pivotal” personality trait of someone, i.e., the personality “axis” around which everything else revolves, and the eigenvalue as the “intensity” of that trait.&lt;/p&gt;
&lt;p&gt;Put simply, the &lt;strong&gt;eigenvector of a matrix&lt;/strong&gt; is a non-zero vector that &lt;em&gt;only gets scaled&lt;/em&gt; when multiplied by a transformation matrix $\textit{A}$. In other words, the vector does not rotate or change direction in any manner. It just gets larger or shorter. The &lt;strong&gt;eigenvalue of a matrix&lt;/strong&gt; is the factor by which the eigenvector gets scaled. This is a bit of a stretch, but in terms of the personality analogy, we can think in the eigenvector as the personality trait that does not change even when an individual change of context: Lisa Simpson “pivotal” personality trait is &lt;em&gt;conscientiousness&lt;/em&gt;, and no matter where she is, home, school, etc., their personality revolves around that. Following the analogy, the eigenvalue would represent the magnitude or intensity of such traits in Lisa. Fig 20. illustrate the geometrical representation of an eigenvector with a cube rotation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 20: Eigenvector in a 3-dimensional rotation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-eigenvector.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;More formally, we define eigenvectors and eigenvalues as:&lt;/p&gt;
&lt;p&gt;Where $\textit{A}$ is a square matrix in $\mathbb{R}^{n \times n}$, $\textbf{x}$ the eigenvector, and $\lambda$ an scalar in $\mathbb{R}$. This identity may look weird to you: &lt;strong&gt;How do we go from matrix-vector multiplication to scalar-vector multiplication?&lt;/strong&gt; We are basically saying that somehow multiplying $\textbf{x}$ by a matrix $\textit{A}$ or a scalar $\lambda$ yields the same result. To make sense of this, recall our discussion about the effects of a matrix on a vector. Mappings or transformation like reflection and shear boils down to &lt;em&gt;a combination of scaling and rotation&lt;/em&gt;. If a mapping $\textit{A}$ does not rotate $\textbf{x}$, it makes sense that such mapping can be reduced to a simpler scalar-vector multiplication $\lambda \textbf{x}$.&lt;/p&gt;
&lt;p&gt;If you recall our discussion about elementary matrices, you may see a simple way to make the $\textit{A}\textbf{x} = \lambda \textbf{x}$ more intuitive. Elementary matrices allow us to encode row and column operations on a matrix. Scalar multiplication, can be represented as by multiplying either the rows or columns of the identity matrix by the desired factor. For instance, for $\lambda = 2$:&lt;/p&gt;
&lt;p&gt;This allow us to rewrite as $\lambda \textit{I}\textbf{x}$, and to maintain the matrix-vector multiplication form as:&lt;/p&gt;
&lt;p&gt;We can go further, and rearange our expression to:&lt;/p&gt;
&lt;p&gt;And to factor our $\textbf{x}$ to get:&lt;/p&gt;
&lt;p&gt;The first part of our new expression, $(\textit{A} -\lambda \textit{I})$, will yield a matrix, meaning that now we have matrix-vector multiplication. In particular, we want a non-zero vector $\textbf{x}$ that when multiplied by $(\textit{A} -\lambda \textit{I})$ yields $0$. The only way to achieve this is when the scaling factor associated with $(\textit{A} -\lambda \textit{I})$ is $0$ as well. Here is when &lt;strong&gt;determinants&lt;/strong&gt; come into play. Recall that the determinant of a matrix represents the scaling factor of such mapping, which in this specific case, happens to be the &lt;em&gt;eigenvalue&lt;/em&gt; of the matrix. Consequently, we want:&lt;/p&gt;
&lt;p&gt;Since $\textit{A}$ and $\textit{I}$ are fixed, in practice, we want to find a value of $\lambda$ that will yield a $0$ determinant of the matrix. Any matrix with a determinant of $0$ will be &lt;em&gt;singular&lt;/em&gt;. This time, we want the matrix to be singular, as we are trying to solve a problem with three unknowns and two equations, therefore, it is the only way to solve it.&lt;/p&gt;
&lt;p&gt;By finding a value for $\lambda$ that makes the determinat $0$, we are effectively making the equality $(\textit{A} -\lambda \textit{I})\textbf{x} = 0$ true.&lt;/p&gt;
&lt;p&gt;Let’s do an example to make these ideas more concrete. Consider the following matrix:&lt;/p&gt;
&lt;p&gt;Let’s first multiply $\textit{A} -\lambda \textit{I}$ to get a single matrix:&lt;/p&gt;
&lt;p&gt;We begin by computing the determinant as:&lt;/p&gt;
&lt;p&gt;Which yield the following polynomial:&lt;/p&gt;
&lt;p&gt;That we solve as any other quadratic polynomial, which receives the special name of &lt;strong&gt;characteristc polynomial&lt;/strong&gt;. When we equate the characteristic polynomial to $0$, we call such expression the &lt;strong&gt;characteristic equation&lt;/strong&gt;. The roots of the characteristic equation, are the eigenvalues of the matrix:&lt;/p&gt;
&lt;p&gt;Wich can be factorized as:&lt;/p&gt;
&lt;p&gt;There you go: we obtain &lt;strong&gt;eigenvalues $\lambda_1 = 2$, and $\lambda_2 = 5$.&lt;/strong&gt; this simply means that $\textit{A}\textbf{x} = \lambda \textbf{x}$ can be solved for eigenvalues equal to $2$ and $5$, assuming non-zero eigenvectors.&lt;/p&gt;
&lt;p&gt;Once we find the eigenvalues, we can compute the eigenvector for each of them. Let’s start with $\lambda_1 = 2$:&lt;/p&gt;
&lt;p&gt;Since the first and second column are identical, we obtain that the solution for the system is pair of such that $\textbf{x}_1 = - \textbf{x}_2$, for instance:&lt;/p&gt;
&lt;p&gt;Such vector correspond to the &lt;strong&gt;eigenspace&lt;/strong&gt; for the eigenvalue $\lambda = 2$. An eigenspace denotes all the vectors that correspond to a given eigenvalue, which in this case is the span of $\textit{E}_{\lambda=2}$.&lt;/p&gt;
&lt;p&gt;Now let’s evaluate for $\lambda = 5$:&lt;/p&gt;
&lt;p&gt;Since the first column is just $-2$ times the second, the solution for the system will be any pair such that $2\textbf{x}_1 = \textbf{x}_2$, i.e.:&lt;/p&gt;
&lt;p&gt;With the span of $\textit{E}_{\lambda=5}$ as the eigenspace for the eigenvalue $\lambda = 5$.&lt;/p&gt;
&lt;p&gt;As usual, we can find the eigenvectors and eigenvalues of a matrix with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. Let’s check our computation:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Eigenvalues of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Eigenvectors of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Eigenvalues of A:
[5. 2.]

Eigenvectors of A:
[[ 0.894 -0.707]
 [ 0.447  0.707]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The eigenvalues are effectively $5$ and $2$. The eigenvectors (aside rounding error), match exactly what we found. For $\lambda=5$, $2\textbf{x}_1 = \textbf{x}_1$, and for $\lambda=2$ that $\textbf{x}_1 = - \textbf{x}_2$.&lt;/p&gt;
&lt;p&gt;Not all matrices will have eigenvalues and eigenvectors in $\mathbb{R}$. Recall that we said that eigenvalues essentially indicate scaling, whereas eigenvectors indicate the vectors that remain unchanged under a linear mapping. It follows that if a linear transformation does not stretch vectors and rotates all of them, then no eigenvectors and eigenvalues should be found. An example of this is a rotation matrix:&lt;/p&gt;
&lt;p&gt;Let’s compute its eigenvectors and eigenvalues in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'B Eigenvalues:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'B Eigenvectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectors&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;B Eigenvalues:
[0.+1.j 0.-1.j]

B Eigenvectors:
[[0.70710678+0.j         0.70710678-0.j        ]
 [0.        -0.70710678j 0.        +0.70710678j]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;strong&gt;&lt;em&gt;+0.j&lt;/em&gt;&lt;/strong&gt; indicates the solution yield imaginary numbers, meaning that there are not eigenvectors or eigenvalues for the matrix $\textit{B} \in \mathbb{R}$&lt;/p&gt;
&lt;h3 id=&quot;trace-and-determinant-with-eigenvalues&quot;&gt;Trace and determinant with eigenvalues&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;trace&lt;/strong&gt; of a matrix is the &lt;em&gt;sum of its diagonal elements&lt;/em&gt;. Formally, we define the trace for a square matrix $\textit{A} \in \mathbb{R}^{n \times n}$ as:&lt;/p&gt;
&lt;p&gt;There is something very special about eigenvalues: &lt;em&gt;its sum equals the trace of the matrix&lt;/em&gt;. Recall the matrix $\textit{A}$ from the previous section:&lt;/p&gt;
&lt;p&gt;Which has a trace equal to $4 + 3 = 7$. We found that their eigenvalues were $\lambda_1 = 2$ and $\lambda_2 = 5$, which also add up to $7$.&lt;/p&gt;
&lt;p&gt;Here is another curious fact about eigenvalues: &lt;em&gt;its product equals to the determinant of the matrix&lt;/em&gt;. The determinant of $\textit{A}$ equals to $(4 \times 3) - (2 \times 1) = 10$. The product of the eigenvalues is also $10$.&lt;/p&gt;
&lt;p&gt;These two properties hold only when we have a full set of eigenvalues, this is when we have as many eigenvalues as dimensions in the matrix.&lt;/p&gt;
&lt;h3 id=&quot;eigendecomposition&quot;&gt;Eigendecomposition&lt;/h3&gt;
&lt;p&gt;In previous sections, we associated LU decomposition with Gaussian Elimination and QR decomposition with Gram-Schmidt Orthogonalization. Similarly, we can associate the Eigenvalue algorithm to find the eigenvalues and eigenvectors of a matrix, wit the &lt;strong&gt;Eigendecomposition&lt;/strong&gt; or &lt;strong&gt;Eigenvalue Decomposition&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We learned that we can find the eigenvalues and eigenvectors of a square matrix (assuming they exist) with:&lt;/p&gt;
&lt;p&gt;Process that entail to first solve the characteristic equation for the polynomial, and then evaluate each eigenvalue to find the corresponding eigenvector. The question now is how to express such process as a single matrix-matrix operation. Let’s consider the following transformation matrix:&lt;/p&gt;
&lt;p&gt;Let’s begin by computing the eigenvalues and eigenvectors with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'B Eigenvalues:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'B Eigenvectors:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;B Eigenvalues:
[2. 8. 3.]

B Eigenvectors:
[[ 0.          0.6882472   0.18291323]
 [ 0.          0.6882472  -0.12194215]
 [ 1.          0.22941573  0.97553722]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We obtained a vector of eigenvalues and a Now, we know that the following identity must be true for scalar-matrix multiplication:&lt;/p&gt;
&lt;p&gt;Since we want to multiply a matrix of eigenvalues by the matrix of eigenvectors, we have to be careful about selecting the order of the multiplication. Recall that matrix-matrix multiplication &lt;em&gt;is not commutative&lt;/em&gt;, meaning that the multiplication order matters. Before this wasn’t a problem, because scalar-matrix multiplication is commutative. What we want, is in operation such that eigenvalues scale eigenvectors. For this, we will put the eigenvectors in a matrix $\textit{X}$, the result of $\lambda \textit{I}$ in a matrix $\Lambda$, and multiply $\textit{X}$ by $\Lambda$ from the right side as:&lt;/p&gt;
&lt;p&gt;Let’s do this with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvectors&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigenvalues&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Left-side of the equation AX:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Right-side of the equation XL:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left-side of the equation AX:
[[ 0.          5.50597761  0.54873968]
 [ 0.          5.50597761 -0.36582646]
 [ 2.          1.83532587  2.92661165]]

Right-side of the equation XL:
[[ 0.          5.50597761  0.54873968]
 [ 0.          5.50597761 -0.36582646]
 [ 2.          1.83532587  2.92661165]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Verify equality&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Entry-wise comparison: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allclose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Entry-wise comparison: True
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A a side note, it is not a good idea to compare &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; arrays with the equality operator, as rounding error and the finite internal bit representation may yield &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt; when values are technically equal. For instance:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;35&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;array([[ True,  True, False],
       [ True,  True, False],
       [ True,  True,  True]])
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We still have one issue to address to complete the Eigendecomposition of $\textit{A}$: to get rid of $\textit{X}$ on the left side of the equation. A first thought is simply to multiply by the $\textit{X}^{-1}$ to cancel $\textit{X}$ on both sides. This won’t work because on the left side of the equation, $\textit{X}$ is multiplying from the right of $\textit{A}$, whereas on the right side of the equation, $\textit{X}$ is multiplying from the left of $\Lambda$. Yet, we still can get take the inverse to eliminate only from the left side of the equation and obtain:&lt;/p&gt;
&lt;p&gt;Lo and behold, &lt;strong&gt;we have found the expression for the Eigendecomposition.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let’s confirm this works:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;X_inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Original matrix A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Reconstruction of A with Eigen Decomposition of A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_inv&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Original matrix A:
[[ 5  3  0]
 [ 2  6  0]
 [ 4 -2  2]]

Reconstruction of A with Eigen Decomposition of A:
[[ 5.  3.  0.]
 [ 2.  6.  0.]
 [ 4. -2.  2.]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;eigenbasis-are-a-good-basis&quot;&gt;Eigenbasis are a good basis&lt;/h3&gt;
&lt;p&gt;There are cases when a transformation or mapping $\textit{T}$ has associated a full set of eigenvectors, i.e., as many eigenvectors as dimensions in $\textit{T}$. We call this set of eigenvectors an &lt;strong&gt;eigenbasis&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;When approaching linear algebra problems, selecting a “good” basis for the matrix or vector space can significantly simplify computation, and also reveals several facts about the matrix that would be otherwise hard to see. Eigenbasis, are an example of a basis that would make our life easier in several situations.&lt;/p&gt;
&lt;p&gt;From the previous section, we learned that the Eigenvalue Decomposition is defined as:&lt;/p&gt;
&lt;p&gt;Conceptually, a first lesson is that transformations, like $\textit{A}$, have two main components: a matrix $\Lambda$ that stretch, shrink, or flip, the vectors, and $\textit{X}$, which represent the “axes” around which the transformation occurs.&lt;/p&gt;
&lt;p&gt;Eigenbasis also make computing the power of a matrix easy. Consider the case of $\textit{A}^2$:&lt;/p&gt;
&lt;p&gt;Since $\textit{X}^{-1} \textit{X}$ equals the identity, we obtain:&lt;/p&gt;
&lt;p&gt;The pattern:&lt;/p&gt;
&lt;p&gt;Generalizes to any power. For powers of $n=2$ or $n=3$ such approach may not be the best, as computing the power directly on $\textit{A}$ may be easier. But, when dealing with large matrices with powers of thousands or millions, this approach is far superior. Further, it even works for the inverse:&lt;/p&gt;
&lt;p&gt;We can see this is true by testing that $\textit{A} \textit{A}^{-1}$ equals the identity:&lt;/p&gt;
&lt;p&gt;Pay attention to what happens now: $\textit{X}^{-1} \textit{X}= \textit{I}$, which yields:&lt;/p&gt;
&lt;p&gt;Now, $\Lambda \Lambda^{-1} $, also yields the identity:&lt;/p&gt;
&lt;p&gt;Finally, $\textit{X} \textit{X}^{-1}$, also yields the identity:&lt;/p&gt;
&lt;h3 id=&quot;geometric-interpretation-of-eigendecomposition&quot;&gt;Geometric interpretation of Eigendecomposition&lt;/h3&gt;
&lt;p&gt;We said that Eigenbasis is a good basis as it allows us to perform computations more easily and to better understand the nature of linear mappings or transformations. The geometric interpretation of Eigendecomposition further reinforces that point. In concrete, the Eigendecomposition elements can be interpreted as follow:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;$\textit{X}^{-1}$ change basis (rotation) from the standard basis into the eigenbasis&lt;/li&gt;
&lt;li&gt;$\Lambda$ scale (stretch, shrinks, or flip) the corresponding eigenvectors&lt;/li&gt;
&lt;li&gt;$\textit{X}$ change of basis (rotation) from the eigenbasis basis onto the original standard basis orientation&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Fig 21. illustrate the action of $\textit{X}^{-1}$, $\Lambda$, and $\textit{X}$ in a pair of vectors in the standard basis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 21: Eigendecomposition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-eigendecomposition.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;the-problem-with-eigendecomposition&quot;&gt;The problem with Eigendecomposition&lt;/h3&gt;
&lt;p&gt;The problem is simple: &lt;strong&gt;Eigendecomposition can only be performed on square matrices, and sometimes the decomposition does not even exist&lt;/strong&gt;. This is very limiting from an applied perspective, as most practical problems involve non-square matrices.&lt;/p&gt;
&lt;p&gt;Ideally, we would like to have a more general decomposition, that allows for non-square matrices and that exist for all matrices. In the next section we introduce the &lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;, which takes care of these issues.&lt;/p&gt;
&lt;h2 id=&quot;singular-value-decomposition&quot;&gt;Singular Value Decomposition&lt;/h2&gt;
&lt;p&gt;Singular Value Decomposition (SVD) is one the most relevant decomposition in applied settings, as it goes beyond the limitations of Eigendecomposition. Specifically, SVD can be performed for &lt;strong&gt;non-squared matrices and singular matrices (i.e., matrices without a full set of eigenvectors)&lt;/strong&gt;. SVD can be used for the same applications that Eigendecomposition (e.g., low-rank approximations) plus the cases for which Eigendecomposition does not work.&lt;/p&gt;
&lt;h3 id=&quot;singular-value-decomposition-theorem&quot;&gt;Singular Value Decomposition Theorem&lt;/h3&gt;
&lt;p&gt;Since we reviewed Eigendecomposition already, understanding SVD becomes easier. The SVD theorem states that any rectangular matrix $\textit{A} \in \mathbb{R}^{m \times n}$ can be decomposed as the product of an orthogonal matrix $\textit{U} \in \mathbb{R}^{m \times m}$, a diagonal matrix $\Sigma \in \mathbb{R}^{m \times m}$, and another orthogonal matrix $\textit{X}^{-1} \in \mathbb{R}^{n \times n}$:&lt;/p&gt;
&lt;p&gt;Another common notation is: $\textit{A} := \textit{U} \Sigma \textit{V}^{T}$. Here I’m using $\textit{X}^{-1}$ just to denote that the right orthogonal matrix is the same as in the Eigenvalue decomposition. Also notice that the inverse of an square orthogonal matrix is $\textit{X}^{-1} = \textit{X}^{T}$.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Singular Values&lt;/em&gt; are the non-negative values along the diagonal of $\Sigma$, which play the same role as eigenvalues in Eigendecomposition. You may even find some authors call them eigenvalues as well. Since $\Sigma$ is a rectangular matrix of the shape as $\textit{A}$, the diagonal of the matrix which contains the singular values will necessarily define a square submatrix within $\Sigma$. There are two situations to pay attention to: (1) when $m &amp;gt; n$, i.e., more rows than columns, and (2) when $m &amp;lt; n$, i.e., more columns than rows.&lt;/p&gt;
&lt;p&gt;For the first case, $m &amp;gt; n$, we will have zero-padding at the bottom of $\Sigma$ as:&lt;/p&gt;
&lt;p&gt;For the second case, $m &amp;lt; n$, we will have zero-padding at the right of $\Sigma$ as:&lt;/p&gt;
&lt;p&gt;Take the case of $\textit{A}^{3 \times 2}$, the SVD is defined as:&lt;/p&gt;
&lt;p&gt;Now let’s evaluate the opposite case, $\textit{A}^{2 \times 3}$, the SVD is defined as:&lt;/p&gt;
&lt;h3 id=&quot;singular-value-decomposition-computation&quot;&gt;Singular Value Decomposition computation&lt;/h3&gt;
&lt;p&gt;SVD computation leads to messy calculations in most cases, so this time I’ll just use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt;. We will compute three cases: a wide matrix $\textit{A}^{2 \times 3}$, a tall matrix $\textit{A}^{3 \times 2}$, and a square matrix $\textit{A}^{3 \times 3}$ with a pair of linearly dependent vectors (i.e., a “defective” matrix, or singular, or not full rank, etc.).&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;16&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;27&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 2 x 3 matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_wide&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3 x 2 matrix
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_tall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                   &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3 x 3 matrix: col 3 equals 2 x col 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_square&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;14&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_wide&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_tall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;U3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Left orthogonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Singular values diagonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Right orthogonal matrix wide A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left orthogonal matrix wide A:
[[-0.55  0.83]
 [ 0.83  0.55]]

Singular values diagonal matrix wide A:
[3.74 1.  ]

Right orthogonal matrix wide A:
[[-0.96 -0.15  0.22]
 [-0.    0.83  0.55]
 [ 0.27 -0.53  0.8 ]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As expected, we obtain a $n \times n$ orthogonal matrix on the left, and a $m \times m$ orthogonal matrix on the right. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; only returns the singular values along the diagonal instead of the $2 \times 3$ matrix, yet it makes no difference regarding the values of the SVD.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Left orthogonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Singular values diagonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Right orthogonal matrix for tall A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left orthogonal matrix for tall A:
[[-0.59 -0.24 -0.77]
 [ 0.8  -0.32 -0.51]
 [-0.13 -0.91  0.38]]

Singular values diagonal matrix for tall A:
[3.67 2.13]

Right orthogonal matrix for tall A:
[[-0.97 -0.23]
 [ 0.23 -0.97]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As expected, we obtain a $m \times m$ orthogonal matrix on the left and a $n \times n$ orthogonal matrix on the right. Notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; returns the singular values in descending order of magnitude. This is a convention you’ll find in the literature frequently.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Left orthogonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Singular values diagonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;S3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'Right orthogonal matrix for square A:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V_T3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Left orthogonal matrix for square A:
[[-0.54 -0.2  -0.82]
 [ 0.79 -0.46 -0.41]
 [-0.29 -0.86  0.41]]

Singular values diagonal matrix for square A:
[8.44 1.95 0.  ]

Right orthogonal matrix for square A:
[[-0.44 -0.13 -0.89]
 [ 0.06 -0.99  0.12]
 [ 0.89  0.   -0.45]]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Although column three is just two times column one (i.e., linearly dependent), we obtain the SVD for $\textit{A}$. Notice that the third singular value equals $0$, which is a reflection of the fact that the third column just contains redundant information.&lt;/p&gt;
&lt;h3 id=&quot;geometric-interpretation-of-the-singular-value-decomposition&quot;&gt;Geometric interpretation of the Singular Value Decomposition&lt;/h3&gt;
&lt;p&gt;As with Eigendecomposition, SVD has a nice geometric interpretation as a sequence of linear mappings or transformations. Concretely:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;$\textit{V}^T$ change basis (rotation) from the standard basis into a set of orthogonal basis&lt;/li&gt;
&lt;li&gt;$\Sigma$ scale (stretch, shrinks, or flip) the corresponding orthogonal basis&lt;/li&gt;
&lt;li&gt;$\textit{U}$ change of basis (rotation) from the new orthogonal basis onto some other orientation, i.e., not necessarily where we started.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The key difference with Eigendecomposition is in $\textit{U}$: instead of going back to the standard basis, $\textit{U}$ performs a change of basis onto another direction.&lt;/p&gt;
&lt;p&gt;Fig 22. illustrate the effect of $\textit{A}^{3 \times 2}$, i.e., $\textit{V}^T$, $\Sigma$, and $\textit{U}$, in a pair of vectors in the standard basis. The fact that the right orthogonal matrix has $3$ column vectors generates the third dimension which is orthogonal to the ellipse surface.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fig. 22: Singular Value Decomposition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://pabloinsente.github.io/assets/post-10/b-svd.svg&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;singular-value-decomposition-vs-eigendecomposition&quot;&gt;Singular Value Decomposition vs Eigendecomposition&lt;/h3&gt;
&lt;p&gt;The SVD and Eigendecomposition are very similar, so it’s easy to get confused about how they differ. Here is a list of the most important ways on which both are different:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The SVD decomposition exist for any rectangular matrix $\in \mathbb{R}^{m \times n}$ , while the Eigendecomposition exist only for square matrices $\in \mathbb{R}^{n \times n}$.&lt;/li&gt;
&lt;li&gt;The SVD decomposition exists even if the matrix $\textit{A}$ is defective, singular, or not full rank, whereas the Eigendecomposition does not have a solution in $\mathbb{R}$ in such a case.&lt;/li&gt;
&lt;li&gt;Eigenvectors $\textit{X}$ are orthogonal only for &lt;em&gt;symmetric matrices&lt;/em&gt;, whereas the vectors in the $\textit{U}$ and $\textit{V}$ are orthonormal. Hence, $\textit{X}$ represents a rotation only for symmetric matrices, whereas $\textit{U}$ and $\textit{V}$ are always rotations.&lt;/li&gt;
&lt;li&gt;In the Eigendecomposition, $\textit{X}$ and $\textit{X}^T$ are the inverse fo each other, whereas $\textit{U}$ and $\textit{V}$ in the SVD are not.&lt;/li&gt;
&lt;li&gt;The singular values in $\Sigma$ are always real and positive, which is not necessarily the case for $\Lambda$ in the Eigendecomposition.&lt;/li&gt;
&lt;li&gt;The SVD change basis in both the domain and codomain. The Eigendecomposition change basis in the same vector space.&lt;/li&gt;
&lt;li&gt;For symmetric matrices, $\textit{A} \in \mathbb{R}^{n \times n}$, the SVD and Eigendecomposition yield the same results.&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;matrix-approximation&quot;&gt;Matrix Approximation&lt;/h2&gt;
&lt;p&gt;In machine learning applications, it is common to find matrices with thousands, hundreds of thousands, and even millions of rows and columns. Although the Eigendecomposition and Singular Value Decomposition make matrix factorization efficient to compute, such large matrices can consume an enormous amount of time and computational resources. One common way to “get around” these issues is to utilize &lt;strong&gt;low-rank approximations&lt;/strong&gt; of the original matrices. By low-rank we mean utilizing a subset of orthogonal vectors instead of the full set of orthogonal vectors, such that we can obtain a “reasonably” good approximation of the original matrix.&lt;/p&gt;
&lt;p&gt;There are many well-known and widely use low-approximation procedures in machine learning, like Principal Component Analysis, Factor Analysis, and Latent Semantic analysis, and dimensionality reduction techniques more generally. Low-rank approximations are possible because in most instances, a small subset of vectors contains most of the information in the matrix, which is a way to say the most data points can be computed as linear combinations of a subset of orthogonal vectors.&lt;/p&gt;
&lt;h3 id=&quot;best-rank-k-approximation-with-svd&quot;&gt;Best rank-k approximation with SVD&lt;/h3&gt;
&lt;p&gt;So far we have represented the SVD as the product of three matrices, $\textit{U}$, $\Sigma$, and $\textit{V}^T$. We can represent this same computation as a the sum of the matching columns of each of these components as:&lt;/p&gt;
&lt;p&gt;Notice that each iteration of $\sum_{i=1}^{r} \textbf{u}_i \textbf{u}_i^T $ will generate a matrix $\sigma_i \textit{A}_i$, which then can be multiplied by $\sigma_i$. In other words, the above expression also equals:&lt;/p&gt;
&lt;p&gt;In matrix notation, we can express the same idea as:&lt;/p&gt;
&lt;p&gt;Now, we can approximate $\textit{A}$ by taking the sum over $k$ values instead of $r$ values. For instance, for a square matrix with $r=100$ orthogonal vectors, we can compute an approximation with the $k=5$ orthogonal vectors as:&lt;/p&gt;
&lt;p&gt;In practice, this means that we take $k=5$ orthogonal vectors from $\textit{U}$ and $\textit{V}^T$, times $5$ singular values, which requires considerably less computation and memory than the $100 \times 100$ matrix. We call this the &lt;strong&gt;best low-rank approximation&lt;/strong&gt; simply because it takes the $5$ largest singular values, which account for most of the information. Nonetheless, we still a precise way to estimate how good is our estimation, for which we need to compute the norm for $\hat{\textit{A}}$ and $\textit{A}$, and how they differ.&lt;/p&gt;
&lt;h3 id=&quot;best-low-rank-approximation-as-a-minimization-problem&quot;&gt;Best low-rank approximation as a minimization problem&lt;/h3&gt;
&lt;p&gt;In the previous section, we mentioned we need to compute some norm for $\hat{\textit{A}}$ and $\textit{A}$, and then compare. This can be conceptualized as a error minimization problem, where we search for the smallest distance between $\textit{A}$ and the low-rank approximation $\hat{\textit{A}}$. For instance, we can use the Frobenius and compute the distance between $\hat{\textit{A}}$ and $\textit{A}$ as:&lt;/p&gt;
&lt;p&gt;Alternatively, we can compute the explained variance for the decomposition, where the highest the variance the better the approximation, ranging from $0$ to $1$. We can perform the SVD approximation with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NumPy&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; as:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.decomposition&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TruncatedSVD&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TruncatedSVD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;33&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;TruncatedSVD(algorithm='randomized', n_components=10, n_iter=7, random_state=1,
             tol=0.0)
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Explained variance by component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Explained variance by component:

SVD approximation with 1 component:
[0.01]

SVD approximation with 5 components:
[0.01 0.04 0.03 0.03 0.03]

SVD approximation with 10 component:
[0.01 0.04 0.03 0.03 0.03 0.03 0.03 0.03 0.03 0.03]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Singular values for each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;singular_values_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Singular values for each approximation:

SVD approximation with 1 component:
[50.42]

SVD approximation with 5 components:
[50.42  5.47  5.26  5.16  5.08]

SVD approximation with 10 component:
[50.42  5.47  5.26  5.16  5.08  5.06  4.99  4.88  4.72  4.63]
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Total explained variance by each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Singular values for each approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 1 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 5 components:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'SVD approximation with 10 component:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SVD10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;explained_variance_ratio_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Total explained variance by each approximation:

Singular values for each approximation:

SVD approximation with 1 component:
0.01

SVD approximation with 5 components:
0.15

SVD approximation with 10 component:
0.29
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As expected, the more components (i.e., the highest the rank of the approximation), the highest the explained variance.&lt;/p&gt;
&lt;p&gt;We can compute and compare the norms by first capturing each matrix of the SVD as recovering $\hat{\textit{A}}$, then compute the Frobenius norm of the difference between $\textit{A}$ and $\hat{\textit{A}}$.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.utils.extmath&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randomized_svd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randomized_svd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_components&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;U&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_T&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;Norm of the difference between A and rank 5 approximation:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'fro'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Norm of the difference between A and rank 5 approximation:
26.32
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This number is not very informative in itself, so we usually utilize the explained variance as an indication of how good is the low-rank approximation.&lt;/p&gt;

&lt;p&gt;Linear algebra is an enormous and fascinating subject. These notes are just an introduction to the subject with machine learning in mind. I am no mathematician, and I have no formal mathematical training, yet, I greatly enjoyed writing this document. I have learned quite a lot by doing it and I hope it may help others that, like me, embark on the journey of acquiring a new skill by themselves, even when such effort may seem crazy to others.&lt;/p&gt;
</description>
<pubDate>Wed, 11 Nov 2020 14:24:43 +0000</pubDate>
<dc:creator>Anon84</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://pabloinsente.github.io/intro-linear-algebra</dc:identifier>
</item>
<item>
<title>$200k in sales from a $6k advertisement</title>
<link>https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad</link>
<guid isPermaLink="true" >https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad</guid>
<description>&lt;table class=&quot;table table-flush table-clickable mb-0&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Count&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;2&quot;&gt;&lt;tr data-href=&quot;#!&quot; readability=&quot;4&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;Time period&lt;/td&gt;
&lt;td&gt;Aug 17,2020 - Aug 24,2020&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;Visitors&lt;/td&gt;
&lt;td&gt;7,200&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;Emails&lt;/td&gt;
&lt;td&gt;45&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;Video calls&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;&lt;strong&gt;Units sold&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;10&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;&lt;strong&gt;Unit price&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$20,000&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr data-href=&quot;#!&quot;&gt;&lt;td class=&quot;table-clickable-hover&quot;&gt;&lt;strong&gt;Total revenue from DF&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$200,000&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;ul class=&quot;list-checked list-checked-circle list-checked-primary mb-9&quot;&gt;&lt;li&gt;Build cheap broadband distribution technology.&lt;/li&gt;
&lt;li&gt;Prove the tech works by connecting 1M people in one city.&lt;/li&gt;
&lt;li&gt;Deploy across 1,000 cities in India&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;mb-9&quot;&gt;Our goal at &lt;a href=&quot;https://www.wifidabba.com/&quot;&gt;Wifi Dabba&lt;/a&gt; is to lower the cost of broadband access in India. We use lasers instead of underground fiber as our core network and commodity components to dramatically lower the cost of deploying a broadband network. We've been running a beta network in Bengaluru, India for the last 9 months serving thousands of live customers. We're now ready to deploy a city wide network and provide cheap internet access to a million people.&lt;/p&gt;
&lt;iframe width=&quot;100%&quot; height=&quot;450&quot; src=&quot;https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1&quot; srcdoc=&quot;&amp;lt;style&amp;gt;*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}&amp;lt;/style&amp;gt;&amp;lt;a href=https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1&amp;gt;&amp;lt;img width='100%' style='min-height:250px;' src='https://img.youtube.com/vi/LwVWJXBNQg8/hqdefault.jpg' alt='Wifi Dabba overview'&amp;gt;&amp;lt;span&amp;gt;▶&amp;lt;/span&amp;gt;&amp;lt;/a&amp;gt;&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; title=&quot;Wifi Dabba overview&quot;&gt;[embedded content]&lt;/iframe&gt;

&lt;p&gt;A core tenet of the Wifi Dabba network is distributed ownership. We believe that ownership of the internet should be in the hands of as many people as possible. If the cost of broadband tech drops, then more people can help pay for the cost of the network. And if you're one of the people paying for the distribution, we believe you should get revenue in return.&lt;/p&gt;
&lt;p&gt;We've divided the city of Bengaluru into 100 regions called PoPs. Anyone can buy a region and get a share in the revenue from those subscribers.&lt;/p&gt;
&lt;p&gt;The Wifi Dabba franchise model:&lt;/p&gt;
&lt;ul class=&quot;list-checked list-checked-circle list-checked-primary mb-9&quot;&gt;&lt;li&gt;&lt;strong&gt;$20,000&lt;/strong&gt; to purchase a 4sqkm. PoP.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimum guaranteed revenue&lt;/strong&gt; Paid quarterly with a 6 year rev share agreement.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully managed service&lt;/strong&gt; Be an absentee landlord&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;text-underline-primary-light&quot;&gt;We've sold 40 as of the time of writing this.&lt;/span&gt;&lt;br/&gt;Wifi Dabba is insanely lucky for the amount of public support we have as a company. We regularly get phone calls, emails and even people dropping by our office just to tell us they like our service. Over the last 3 years we've received dozens of emails from people requesting franchises or other types of partnerships. We're incredibly humbled and thankful for this support on a daily basis.&lt;/p&gt;

&lt;p&gt;We believe there is a large group of people that care about the future of the internet and would be willing to put their money where their mouths are. As long as the price and the level of risk involved is reasonable. Our gut told us that this group would most likely be people that have seen success in the technology business as engineers, operators and entreprenuers.&lt;/p&gt;

&lt;p&gt;We've had our heads down over the last three years building and testing our network stack. Publicity or notariety has never been high on our list. We've begun ramping up our social media efforts but it was clear that to kickstart our outreach, we had to do a little bit of advertising.&lt;/p&gt;

&lt;div&gt;&lt;img class=&quot;text-center&quot; src=&quot;https://www.wifidabba.com/images/df-venn.png&quot; height=&quot;300&quot; width=&quot;330&quot; alt=&quot;...&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://www.daringfireball.net&quot;&gt;Daringfireball.net&lt;/a&gt; is a great blog authored by &lt;a href=&quot;https://en.wikipedia.org/wiki/John_Gruber&quot;&gt;John Gruber&lt;/a&gt; who is also the creator of &lt;a href=&quot;https://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt;. DF was a natural choice for us as we've been readers of the blog for a long while and we knew from experience that DF readers would fit our target market rather well. Given the high quality of John's writing and insights into the industry, we felt that there would be a large pool of senior tech veterans that would be interested in Wifi Dabba among DF's audience.&lt;/p&gt;
&lt;p&gt;The sponsorship cost us $6,500 and ran for the week starting Aug 17, 2020 and we got:&lt;/p&gt;
&lt;ul class=&quot;list-checked list-checked-circle list-checked-primary mb-9&quot;&gt;&lt;li&gt;A display ad in the sidebar on every page of the site, all week long.&lt;/li&gt;
&lt;li&gt;A post from the sponsor in the RSS feed at the start of the week. Us, the sponsor, got to address Daring Fireball’s most dedicated readers directly.&lt;/li&gt;
&lt;li&gt;At the end of the week, John also posts an item thanking and linking to the feed sponsor.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Stats about DF readership&lt;/p&gt;
&lt;ul class=&quot;list-checked list-checked-circle list-checked-primary mb-9&quot;&gt;&lt;li&gt;Typical weekday web page views: 80,000–100,000.&lt;/li&gt;
&lt;li&gt;Estimated monthly web page views: 2.5 million.&lt;/li&gt;
&lt;li&gt;Estimated Daring Fireball RSS feed subscribers: Over 200,000.&lt;/li&gt;
&lt;li&gt;Twitter followers on the @daringfireball account: Over 92,000.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We created two variants of our message. Designed in bold colours to stand out against DF's dark theme. These creatives rotated randomly. We decided to focus on the technology because of the nature of the audience and hoped that the website did a good job of explaining the product.&lt;/p&gt;
&lt;div class=&quot;text-center&quot;&gt;&lt;img src=&quot;https://www.wifidabba.com/images/df-ads.png&quot; width=&quot;100%&quot; alt=&quot;Buy internet POP&quot;/&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Click Ad -&amp;gt; Browse site -&amp;gt; Setup a call&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We expected visitors to click on the ad in DF and land on our homepage. Once on our site, we hoped that visitors would check out our videos as well as browse through a few pages. If they liked what they saw, we had a prominent buy button on the front page which led to a page to setup a video call.&lt;/p&gt;
&lt;p&gt;It's worth noting here that we knew going in that a large percentage of DF's audience would be using Ad-blockers. Nothing wrong with that, we use ad-blockers ourselves.&lt;/p&gt;
&lt;p&gt;Furthermore, we made a deliberate choice to add a high friction call to action and contact process. In order to purchase a PoP, a visitor would be directed to a calendar managed by calendly that would help them setup a call with someone from our team at a convenient time.&lt;/p&gt;
&lt;a href=&quot;https://wifidabba.com/buy&quot;&gt;&lt;img class=&quot;text-center&quot; width=&quot;100%&quot; src=&quot;https://www.wifidabba.com/images/df-buy.png&quot; alt=&quot;Setup Call&quot;/&gt;&lt;/a&gt;&lt;p&gt;The reason for this is that we knew DF would deliver a few hundred visitors a day to our site. We're a small team and our core focus is deploying the network, not necessarily sales and our goal is to sell the PoPs to people that are really excited a lot about our idea and show a high level of interest. The $20,000 price point of our product + the high friction of the contact process + users that are OK with ads = A high signal to noise ratio from DF visitors. We'd love to hear any feedback on what you think about this.&lt;/p&gt;

&lt;p&gt;Our thesis turned out to be pretty spot on. Senior engineers from Google, Apple and a host of other technology companies purchased the PoPs. The actual sales process turned out to be fairly quick and straight forward. Most of the people that purchased the PoPs did so within a period of 48 hours of having the call.&lt;/p&gt;

</description>
<pubDate>Wed, 11 Nov 2020 13:51:11 +0000</pubDate>
<dc:creator>mildlyclassic</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad</dc:identifier>
</item>
<item>
<title>Apple&amp;#039;s Shifting Differentiation</title>
<link>https://stratechery.com/2020/apples-shifting-differentiation/</link>
<guid isPermaLink="true" >https://stratechery.com/2020/apples-shifting-differentiation/</guid>
<description>&lt;p&gt;If you ask Apple — or watch their seemingly never-ending series of events — they will happily tell you exactly what the company’s differentiation is based on; from this year alone:&lt;/p&gt;

&lt;p&gt;This integration is at the core of Apple’s incredibly successful business model: the company makes the majority of its money by selling hardware, but while other manufacturers can, at least in theory, create similar hardware, which should lead to commoditization, only Apple’s hardware runs its proprietary operating systems.&lt;/p&gt;
&lt;p&gt;Of course software is even more commoditizable than hardware: once written, software can be duplicated endlessly, which means its marginal cost of production is zero. This is why many software-based companies are focused on serving as large of a market as possible, the better to leverage their investments in creating the software in the first place. However, zero marginal cost is not the only inherent quality of software: it is also infinitely customizable, which means that Apple can create something truly unique, and by tying said software to its hardware, make its hardware equally unique as well, allowing it to charge a sustainable premium.&lt;/p&gt;
&lt;p&gt;This is, to be sure, a simplistic view of Apple: many aspects of its software are commoditized, often to Apple’s benefit, while many aspects of its hardware are differentiated. What is fascinating is that while modern Apple is indeed characterized by the integration of hardware and software, the balance of which differentiates the other has shifted over time, culminating in yesterday’s announcement of new Macs powered by Apple Silicon.&lt;/p&gt;
&lt;h4&gt;Apple 1.0: Software Over Hardware&lt;/h4&gt;
&lt;p&gt;When Steve Jobs returned to Apple in 1996, the company was famously in terrible financial shape; unsurprisingly the company’s computer lineup was in terrible shape as well: too many models that were too unremarkable. The only difference from PCs was that Macs had a different operating system that was technically obsolete, PowerPC processors that were falling behind x86, and also they were more expensive. Not exactly a winning combination!&lt;/p&gt;
&lt;p&gt;Jobs made a number of changes in short order: he killed off the Macintosh clone market, re-asserting Apple’s integrated business model; he dramatically simplified the product lineup; and, having found a promising young designer already working at Apple named Jony Ive, he put all of the company’s efforts behind the iMac. This was truly a product where the hardware carried the software; the iMac was a cultural phenomenon, not because of Classic Mac OS’s ease-of-use, and certainly not because of its lack of memory protection, but simply because the hardware was so simple and so adorable.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://foxtrot.com/&quot;&gt;&lt;img src=&quot;https://stratechery.com/wp-content/uploads/2020/11/appleintegration-2.png&quot; alt=&quot;Foxtrot and the iMac&quot; width=&quot;599&quot; height=&quot;190&quot; class=&quot;aligncenter size-large wp-image-5390&quot; srcset=&quot;https://stratechery.com/wp-content/uploads/2020/11/appleintegration-2.png 599w, https://stratechery.com/wp-content/uploads/2020/11/appleintegration-2-300x95.png 300w&quot; sizes=&quot;(max-width: 599px) 100vw, 599px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OS X brought software to the forefront, delivering not simply a technically sound operating system, but one that was based on Unix, making it &lt;a href=&quot;http://www.paulgraham.com/mac.html&quot;&gt;particularly attractive to developers&lt;/a&gt;. And, on the consumer side, Apple released iLife, a suite of applications that made a Mac useful for normal users. I myself bought my first Mac in this era because I wanted to use GarageBand; 16 years on and my musical ambitions are abandoned, but my Mac usage remains.&lt;/p&gt;
&lt;p&gt;By that point I was buying a Mac despite its hardware: while my iBook was attractive enough, its processor was a Motorola G4 that was not remotely competitive with Intel’s x86 processors; later that year Jobs made the then-shocking-but-in-retrospect-obvious decision to shift Macs to Intel processors. In this case having the same hardware as everyone else in the industry would be a big win for Apple, the better to let their burgeoning software differentiation shine.&lt;/p&gt;
&lt;h4&gt;Apple 2.0: The Apex of Integration&lt;/h4&gt;
&lt;p&gt;Meanwhile, Apple had an exploding hit on its hands with the iPod, which combined beautiful hardware and superior storage capacity with iTunes, software that offloaded the complexity of managing your music to your far more capable Mac and, starting in 2003, your PC; notably Apple avoided the trap of integrating hardware (the iPod) with hardware (the Mac), which would have handicapped the former to prop up the latter. Instead the company took advantage of the flexibility of software to port iTunes to Windows.&lt;/p&gt;
&lt;p&gt;The iPhone followed the path blazed by the iPod: while the first few versions of the iPhone were remarkably limited in their user-facing software capabilities, that was acceptable because much of that complexity was offloaded to the PC or Mac you plugged it into. To that point much of the software work had gone into making the iPhone usable on hardware that was barely good enough; &lt;a href=&quot;https://web.archive.org/web/20150517013510/http://www.macnn.com/articles/10/12/27/rim.thought.apple.was.lying.on.iphone.in.2007/&quot;&gt;RIM famously thought Jobs was lying about the iPhone’s capabilities at launch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Over time the iPhone would gradually wean itself off of iTunes and the need to sync with a PC or Mac, making itself a standalone computer in its own right; it was also on its way to being the most valuable product in history. This was &lt;a href=&quot;https://stratechery.com/2016/everything-as-a-service/&quot;&gt;the ultimate in integration&lt;/a&gt;, both in terms of how the product functioned, and also in the business model that integration unlocked.&lt;/p&gt;
&lt;h4&gt;Apple 3.0: Hardware Over Software&lt;/h4&gt;
&lt;p&gt;Sixteen years on from the PowerPC-to-Intel transition, and Apple’s software differentiation is the smallest it has been since the dawn of OS X. Windows has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux&quot;&gt;Subsystem for Linux&lt;/a&gt;, which, combined with the company’s laser focus on developers, makes Microsoft products increasingly attractive for software development. Meanwhile, most customers use web apps on their computers, PC or Mac. There has been an explosion in creativity, but that explosion has occurred on smartphones, and is centered around distribution channels, not one’s personal photo or movie library.&lt;/p&gt;
&lt;p&gt;Those distribution channels and the various apps customers use to create and consume are available on both leading platforms, iOS and Android. &lt;a href=&quot;https://twitter.com/ditheringfm/status/1303710471665512448&quot;&gt;I personally feel&lt;/a&gt; that the iPhone retains an advantage in the smoothness of its interface and quality of its apps, but Android is more flexible and well-suited to power users, and much better integrated with Google’s superior web services; there are strong arguments to be made for both ecosystems.&lt;/p&gt;
&lt;p&gt;Where the iPhone is truly differentiated is in hardware: Apple has — &lt;a href=&quot;https://www.theverge.com/21555901/iphone-12-pro-max-review&quot;&gt;for now&lt;/a&gt; — the best camera system, and has had for years the best system-on-a-chip. These two differentiators are related: smartphone cameras are not simply about lenses and sensors, but also about how the resultant image is processed; that involves both software and the processor, and what is notable about smartphone cameras is that Google’s photo-processing software is generally thought to be superior. What makes the iPhone a better camera, though, is its chip.&lt;/p&gt;
&lt;h4&gt;Apple Silicon and Sketch&lt;/h4&gt;
&lt;p&gt;It is difficult to overstate just how far ahead Apple’s A-series of smartphone chips is relative to the competition; &lt;a href=&quot;https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/3&quot;&gt;AnandTech found&lt;/a&gt; that the A14 delivered nearly double the performance of its closest competitors for the same amount of power — indeed, the A14’s only true competitor was last year’s A13. At least, that is, as far as mobile is concerned; the most noteworthy graph from &lt;a href=&quot;https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/4&quot;&gt;that AnandTech article&lt;/a&gt; is about how the A14 stacks up against those same Intel chips that power Macs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/4&quot;&gt;&lt;img src=&quot;https://stratechery.com/wp-content/uploads/2020/11/appleintegration-1.png&quot; alt=&quot;AnandTech charts A-series chips versus Intel chips&quot; width=&quot;640&quot; height=&quot;691&quot; class=&quot;aligncenter size-large wp-image-5389&quot; srcset=&quot;https://stratechery.com/wp-content/uploads/2020/11/appleintegration-1.png 678w, https://stratechery.com/wp-content/uploads/2020/11/appleintegration-1-278x300.png 278w, https://stratechery.com/wp-content/uploads/2020/11/appleintegration-1-584x630.png 584w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whilst in the past 5 years Intel has managed to increase their best single-thread performance by about 28%, Apple has managed to improve their designs by 198%, or 2.98x (let’s call it 3x) the performance of the Apple A9 of late 2015.&lt;/p&gt;
&lt;p&gt;Apple’s performance trajectory and unquestioned execution over these years is what has made Apple Silicon a reality today. Anybody looking at the absurdness of that graph will realise that there simply was no other choice but for Apple to ditch Intel and x86 in favour of their own in-house microarchitecture – staying par for the course would have meant stagnation and worse consumer products.&lt;/p&gt;
&lt;p&gt;Today’s announcements only covered Apple’s laptop-class Apple Silicon, whilst we don’t know the details at time of writing as to what Apple will be presenting, Apple’s enormous power efficiency advantage means that the new chip will be able to offer either vastly increased battery life, and/or, vastly increased performance, compared to the current Intel MacBook line-up.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What makes the timing of this move ideal from Apple’s perspective is not simply that this is the year that the A-series of chips are surpassing Intel’s, but also the Mac’s slipping software differentiation. Sketch, makers of the eponymous vector graphics app, wrote, on the occasion of their 10th anniversary, &lt;a href=&quot;https://www.sketch.com/blog/2020/10/26/part-of-your-world-why-we-re-proud-to-build-a-truly-native-mac-app/&quot;&gt;a paean to Mac apps&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ten years after the first release of Sketch, a lot has changed. The design tools space has grown. Our amazing community has, too. Even macOS itself has evolved. But one thing has remained the same: our love for developing a truly native Mac app. Native apps bring so many benefits — from personalization and performance to familiarity and flexibility. And while we’re always working hard to make Cloud an amazing space to collaborate, we still believe the Mac is the perfect place to let your ideas and imagination flourish.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The fly in Sketch’s celebratory ointment is that phrase “even macOS itself has evolved”; the truth is that most of the macOS changes over Sketch’s lifetime — which started with Snow Leopard, regarded by many (including yours truly) as the best version of OS X — have been at best cosmetic, at worst clumsy attempts to protect novice users that often got in the way of power users.&lt;/p&gt;
&lt;p&gt;Meanwhile, it is the cloud that is the real problem facing Sketch: &lt;a href=&quot;https://www.figma.com/&quot;&gt;Figma&lt;/a&gt;, which is built from the ground-up as a collaborative web app, is taking the design world by storm, because rock-solid collaboration with good enough web apps is more important for teams than tacked-on collaboration with native software built for the platform.&lt;/p&gt;
&lt;p&gt;Sketch, to be sure, bears the most responsibility for its struggles; frankly, that native app piece reads like a refusal to face its fate. Apple, though, shares a lot of the blame: imagine if &lt;a href=&quot;https://daringfireball.net/linked/2015/12/01/sketch-leaves-mac-app-store&quot;&gt;instead of effectively forcing Sketch out of the App Store&lt;/a&gt; with its zealous approach to security, &lt;a href=&quot;https://twitter.com/robenkleene/status/1321480968914558976?s=21&quot;&gt;Apple had evolved AppKit&lt;/a&gt;, macOS’s framework for building applications, to provide built-in support for collaboration and live-editing.&lt;/p&gt;
&lt;p&gt;Instead the future is web apps, with all of the performance hurdles they entail, which is why, from Apple’s perspective, the A-series is arriving just in time. Figma in Electron may destroy your battery, but that destruction will take twice as long, if not more, with an A-series chip inside!&lt;/p&gt;
&lt;h4&gt;Integration Wins Again&lt;/h4&gt;
&lt;p&gt;This isn’t the first time I have noted that Apple is inclined to fix an ecosystem problem with hardware; five years ago, after the launch of the iPad Pro, I wrote in &lt;a href=&quot;https://stratechery.com/2015/from-products-to-platforms/&quot;&gt;From Products to Platforms&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note that phrase: “How could &lt;strong&gt;we&lt;/strong&gt; take the iPad even further?” Cook’s assumption is that the iPad problem is Apple’s problem, and given that Apple is a company that makes hardware products, Cook’s solution is, well, a new product.&lt;/p&gt;
&lt;p&gt;My contention, though, is that when it comes to the iPad Apple’s product development hammer is not enough. Cook described the iPad as “A simple multi-touch piece of glass that instantly transforms into virtually anything that you want it to be”; the transformation of glass is what happens when you open an app. One moment your iPad is a music studio, the next a canvas, the next a spreadsheet, the next a game. The vast majority of these apps, though, are made by 3rd-party developers, which means, by extension, 3rd-party developers are even more important to the success of the iPad than Apple is: Apple provides the glass, developers provide the experience.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The iPad has since recovered from its 2017 nadir in sales, but seems locked in at around 8% of Apple’s revenue, a far cry from the 20% share it had in its first year, when it looked set to rival the iPhone; I remain convinced that the lack of a thriving productivity software market that treated the iPad like &lt;a href=&quot;https://stratechery.com/2020/the-ipad-at-10-the-ipad-disappointment-ipads-missing-ecosystem/&quot;&gt;the unique device Jobs thought it was&lt;/a&gt;, instead of a laptop replacement, is the biggest reason why.&lt;/p&gt;
&lt;p&gt;Perhaps Apple Silicon in Macs will turn out better: it is possible that Apple’s chip team is so far ahead of the competition, not just in 2020, but particularly as it develops even more powerful versions of Apple Silicon, that the commoditization of software inherent in web apps will work to Apple’s favor, just as the its move to Intel commoditized hardware, highlighting Apple’s then-software advantage in the 00s.&lt;/p&gt;
&lt;p&gt;Apple is pricing these new Macs as if that is the case: the M1 probably costs around $75 (an educated guess), which is less than the Intel chips it replaces, but Apple is mostly holding the line on prices (the new Mac Mini is $100 cheaper, but also has significantly less I/O). That suggests the company believes it can both take share and margin, and it’s a reasonable bet from my perspective. The company has the best chips in the world, and you have to buy the entire integrated widget to get them.&lt;/p&gt;


</description>
<pubDate>Wed, 11 Nov 2020 13:21:07 +0000</pubDate>
<dc:creator>kaboro</dc:creator>
<og:type>article</og:type>
<og:title>Apple’s Shifting Differentiation</og:title>
<og:url>https://stratechery.com/2020/apples-shifting-differentiation/</og:url>
<og:description>Apple is about the integration of hardware and software, but the balance between the two has shifted over time.</og:description>
<og:image>https://stratechery.com/wp-content/uploads/2020/11/appleintegration-1.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://stratechery.com/2020/apples-shifting-differentiation/</dc:identifier>
</item>
<item>
<title>Postgres Observability</title>
<link>https://pgstats.dev/</link>
<guid isPermaLink="true" >https://pgstats.dev/</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://pgstats.dev/&quot;&gt;https://pgstats.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=25058045&quot;&gt;https://news.ycombinator.com/item?id=25058045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 366&lt;/p&gt;
&lt;p&gt;# Comments: 41&lt;/p&gt;
</description>
<pubDate>Wed, 11 Nov 2020 13:12:51 +0000</pubDate>
<dc:creator>fforflo</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://pgstats.dev/</dc:identifier>
</item>
<item>
<title>Will R Work on Apple Silicon?</title>
<link>https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/</link>
<guid isPermaLink="true" >https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/</guid>
<description>&lt;p&gt;At WWDC 2020 earlier this year, Apple announced a transition from Intel to ARM-based processors in their laptops. This blog is about the prospects of when R will work on that platform, based on experimentation on a developer machine running A12Z, one of the “Apple silicon” processors.&lt;/p&gt;&lt;p&gt;The new platform will include &lt;a href=&quot;https://developer.apple.com/documentation/apple_silicon/about_the_rosetta_translation_environment&quot;&gt;Rosetta 2&lt;/a&gt;, a dynamic translation framework which runs binaries built for 64-bit Intel Macs using just-in-time, dynamic translation of binary code. The good news is that R seems to be working fine with the dynamic translation, so R users don’t need to worry even if they use current releases. However, the interesting question is whether R will also work natively. Native execution is expected to be faster, and the transition probably has to be done eventually, anyway.&lt;/p&gt;
&lt;p&gt;The executive summary is: while there is currently no released Fortran compiler for the platform, a development version of GNU Fortran already seems to be working fine. There are some surprising results with NaN payload propagation leading to unexpected results when computing with numeric NAs, but these can be overcome by changing the mode of the floating-point unit, which has already been done in R-devel. More details follow in this post.&lt;/p&gt;
&lt;div id=&quot;r-needs-a-fortran-90-compiler&quot; class=&quot;section level2&quot; readability=&quot;30.918133802817&quot;&gt;
&lt;h2&gt;R needs a Fortran 90 compiler&lt;/h2&gt;
&lt;p&gt;R needs at least a Fortran 90 compiler to build. Most of the Fortran code in R and base and recommended packages is still in Fortran 77, so it can be translated by &lt;code&gt;f2c&lt;/code&gt; to C and compiled by a C compiler. However, some code already uses Fortran 90 features and back-porting that would require a non-trivial effort.&lt;/p&gt;
&lt;p&gt;In addition, R ships with a slightly modified version of reference LAPACK and BLAS which need a Fortran 90 compiler to build. It is highly preferable to have reference LAPACK/BLAS available also on the new platform, even though Apple provides an optimized version of BLAS and LAPACK as part of the Accelerate framework in macOS.&lt;/p&gt;
&lt;p&gt;GCC’s GFortran supports 64-bit ARMs: an earlier blog post from &lt;a href=&quot;https://developer.r-project.org/Blog/public/20/05/29/testing-r-on-emulated-platforms&quot;&gt;May&lt;/a&gt; was about building and testing R on Linux running on 64-bit ARM (Aarch64) inside QEMU emulator. However, the Apple silicon platform uses a different application binary interface (ABI) which GFortran does not support, yet.&lt;/p&gt;
&lt;p&gt;Currently, there seems to be no other Fortran 90 compiler, neither free nor commercial. Specifically, LLVM’s Fortran compiler (now called Flang again) is not yet finished.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;building-development-version-of-gccgfortran&quot; class=&quot;section level2&quot; readability=&quot;19&quot;&gt;
&lt;h2&gt;Building development version of GCC/GFortran&lt;/h2&gt;
&lt;p&gt;While GFortran does not support Apple silicon yet (neither any release nor the GCC trunk), there is a private development branch of GCC including GFortran by Ian Sandoe, which we experimented with.&lt;/p&gt;
&lt;p&gt;Building GCC from source as usual requires first building also GMP, MPFR and MPC for the platform. GMP (version 6.2.0) required back-porting a patch from the trunk (configure script for Apple silicon, assembler macros). Re-generating the configure script and make files natively also required building libtool. The configure script for GMP had to be explicitly run with &lt;code&gt;--build=aarch64-apple-darwin20.0.0&lt;/code&gt;, even when building natively. The configure script for GCC was run with &lt;code&gt;--with-sysroot&lt;/code&gt;, specifying a directory to &lt;code&gt;MacOSX.sdk&lt;/code&gt; installed via &lt;code&gt;xcode-select -p&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;testing-r&quot; class=&quot;section level2&quot; readability=&quot;21.96875&quot;&gt;
&lt;h2&gt;Testing R&lt;/h2&gt;
&lt;p&gt;R requires a number of dependencies, which can be built natively following &lt;a href=&quot;https://cran.r-project.org/doc/manuals/r-release/R-admin.html#Installing-R-under-macOS&quot;&gt;R Installation and Administration&lt;/a&gt;, using the Apple/LLVM toolchain provided by Apple (Fortran compiler is not needed). We have also compiled a native build of Subversion, even though in principle one could build R from a tarball.&lt;/p&gt;
&lt;p&gt;R and recommended packages were built using Apple/LLVM clang to compile C (with &lt;code&gt;CFLAGS=-Wno-error=implicit-function-declaration&lt;/code&gt;) and Objective C and using the development version of GFortran to compile Fortran code.&lt;/p&gt;
&lt;p&gt;A number of tests for R and recommended packages have failed for a platform-specific reason, but it turned out that all for the same reason: surprising propagation of NaN payload, where e.g. &lt;code&gt;NA * 1&lt;/code&gt; is &lt;code&gt;NaN&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;nanan-payload-propagation&quot; class=&quot;section level2&quot; readability=&quot;70.5&quot;&gt;
&lt;h2&gt;NA/NaN payload propagation&lt;/h2&gt;
&lt;p&gt;R’s NA for floating point numbers is represented using NaN with a special payload value. NaNs that originate from computations not involving NA have a different (e.g. zero) payload, so can be distinguished from NA. NaNs are often passed to computations inside R without explicit checks and the same happens inside package code and external numerical code, which have no idea about R’s NA concept nor representation.&lt;/p&gt;
&lt;p&gt;The IEEE 754 standard for floating point arithmetics does not mandate how NaN payloads should be propagated through computations. The result of computations involving NAs and/or other NaNs depends on the CPU/floating point unit, on compiler optimizations (compiler may re-order computations), and on the algorithm (e.g. it is tempting to ignore input values not needed to compute the result under the assumption they are finite, but without actually checking they are finite).&lt;/p&gt;
&lt;p&gt;More information can be found in R’s online help (&lt;code&gt;?NA&lt;/code&gt;, &lt;code&gt;?NaN&lt;/code&gt;), including disclaimers that the difference between NA and NaN should not be relied on (citing in the wording of recent R-devel):&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;Computations involving ‘NaN’ will return ‘NaN’ or perhaps ‘NA’: which of those two is not guaranteed and may depend on the R platform (since compilers may re-order computations).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;Numerical computations using ‘NA’ will normally result in ‘NA’: a possible exception is where ‘NaN’ is also involved, in which case either might result (which may depend on the R platform). However, this is not guaranteed and future CPUs and/or compilers may behave differently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Intel FPUs worked relatively well with respect to R NAs: binary operations with NAs resulted in NAs, even when NaNs were involved (based on experimentation). However, currently R on Intel machines is typically built to use SSE instructions for computations, which do not work that well for R NAs: binary operations with NAs only result in NA when the other argument is not a NaN, or when the NA is the first, so &lt;code&gt;NaN + NA&lt;/code&gt; is &lt;code&gt;NaN&lt;/code&gt; but &lt;code&gt;NA + NaN&lt;/code&gt; is &lt;code&gt;NA&lt;/code&gt;. As 64-bit Intel with SSE has most likely been the prevailing setup for R recently, tests have already been updated to accept this behavior.&lt;/p&gt;
&lt;p&gt;However, it turns out that on A12Z, R’s NA becomes a normal NaN after any binary operation (the payload is lost), so even &lt;code&gt;NA * 1&lt;/code&gt; is &lt;code&gt;NaN&lt;/code&gt;. This is within what has been warned against in the R documentation cited above, but still a number of tests for R and recommended packages capture the (documented as unreliable) behavior expecting that operations like this will return &lt;code&gt;NA&lt;/code&gt;. We have not investigated how many of other CRAN/BIOC packages do.&lt;/p&gt;
&lt;p&gt;The ARM architecture floating point units (VFP, NEON) support RunFast mode, which includes flush-to-zero and default NaN. The latter means that payload of NaN operands is not propagated, all result NaNs have the default payload, so in R, even &lt;code&gt;NA * 1&lt;/code&gt; is &lt;code&gt;NaN&lt;/code&gt;. Luckily, RunFast mode can be disabled, and when it is, the NaN payload propagation is friendlier to R NAs than with Intel SSE (&lt;code&gt;NaN + NA&lt;/code&gt; is &lt;code&gt;NA&lt;/code&gt;). We have therefore updated R to disable RunFast mode on ARM on startup, which resolved all the issues observed.&lt;/p&gt;
&lt;p&gt;We have not run into this issue earlier as RunFast has been disabled by default on other platforms, including Raspberry Pi (tested on an old model 2 with 32-bit ARM, BCM2835) and on QEMU emulating Aarch64 (64-bit ARM, Cortex-A72).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;summary&quot; class=&quot;section level2&quot; readability=&quot;27&quot;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;It turns out there is hope that R will work on Apple silicon. A usable Fortran 90 compiler for Apple silicon will hopefully be available relatively soon, since the development version of GFortran already seems to be working (&lt;code&gt;check-all&lt;/code&gt; passed for R including reference LAPACK/BLAS) and there is a strong need for such compiler not only for R, but any scientific computing on that platform.&lt;/p&gt;
&lt;p&gt;Any package native code that wants to reliably preserve NAs (computations with at least one NA value on input provide NA on output) has to include explicit checks, be it for computations implemented in the package native code or in external libraries. That is the only portable, reliable way, and has been the only one for long time. Packages that choose to not guarantee such propagation, on the other hand, should not capture in tests the coincidental propagation on the developer’s platform. On ARM, and hence also Apple silicon, R now masks some of these issues by disabling the RunFast mode, but another new platform may appear where this won’t be possible, and more importantly, NAs may be “lost” also due to compiler optimizations or algorithmically in external libraries.&lt;/p&gt;
&lt;/div&gt;

</description>
<pubDate>Wed, 11 Nov 2020 13:03:56 +0000</pubDate>
<dc:creator>nojito</dc:creator>
<og:title>Will R Work on Apple Silicon? - The R Blog</og:title>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/</dc:identifier>
</item>
</channel>
</rss>