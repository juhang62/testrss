<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>SuperTuxKart 1.0 Release</title>
<link>http://blog.supertuxkart.net/2019/04/supertuxkart-10-release.html</link>
<guid isPermaLink="true" >http://blog.supertuxkart.net/2019/04/supertuxkart-10-release.html</guid>
<description>Yes, if you have followed our development a bit, that might be a bit of a surprise. But we have been asked why we don't call this release 1.0, and the majority of us developers discussed this and decided that indeed this release is a major milestone that deserves the big 1.0 number.&lt;div&gt;&lt;iframe allowfullscreen=&quot;&quot; class=&quot;YOUTUBE-iframe-video&quot; data-thumbnail-src=&quot;https://i.ytimg.com/vi/Lm1TFDBiIIg/0.jpg&quot; frameborder=&quot;0&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/Lm1TFDBiIIg?feature=player_embedded&quot; width=&quot;640&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;span&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;&lt;span&gt;SuperTuxKart 1.0 official trailer&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;&lt;br/&gt;Should you not have followed our development: this 1.0 release adds support for networking races to SuperTuxKart. You can play with your friends online, and it doesn't have to be split screen anymore - your gaming partners can be in a different city, country, or even continent - you can still meet for a race or two. Cross-continent may not work the best; you should try to have a reasonable ping - 100ms works fine, and we had some races with 300ms ping, though unavoidable sudden stuttering and karts jumping around is more frequently at higher latencies. Even more important than the ping (unless it's abnormally high) is that there isn't packet loss or packet slowdowns (jitter), so a stable connection is required, especially for the server hosts. The bandwidth use is minimal even for hosting 10 players, so as long as you don't use a metered connection it should be fine for anyone with 150 kilobytes (~1.2 megabit) per second upload speed to spare. If that's still too much, less bandwidth is used with fewer players.&lt;p&gt;With the new version you can play various game modes online: normal race, time trial, soccer mode, battle mode and the new Capture-The-Flag mode. While you can easily start your own server (just select 'create server' in the GUI), the community also provides a set of servers to be used by everyone. Certain servers provide online ranking (since we have to make sure that the servers are not modified to favor certain players), and you can find the current rankings &lt;a href=&quot;https://online.supertuxkart.net/rankings.php&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. You can use one of the servers provided by the community. We would request that you do not abuse the servers provided by the community - they are meant to be used by everyone. If you want to play only with a certain set of friends, let SuperTuxKart create a server for you, and do not grab a public server and kick everyone else who wants to join.&lt;/p&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-uwC4394QWVk/XLtIqm29skI/AAAAAAAAAeo/5u5C2R4IwDU8c2KpmpboohZ6OUmmpOQfgCLcBGAs/s1600/STK-1.0-ravenbridge.jpg&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;900&quot; data-original-width=&quot;1600&quot; height=&quot;360&quot; src=&quot;https://1.bp.blogspot.com/-uwC4394QWVk/XLtIqm29skI/AAAAAAAAAeo/5u5C2R4IwDU8c2KpmpboohZ6OUmmpOQfgCLcBGAs/s640/STK-1.0-ravenbridge.jpg&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;Ravenbridge Mansion&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;The networking implementation has been a very big and ambitious task, which took even longer than the port from PLIB (anyone remember that?) to Irrlicht in 2010, which took around 14 months. But it is done: more than 20,000 lines of network related code and 18 months later, we have reached our goal, and online play is a reality. Across the whole game, more than 2500 commits and 300 resolved issues separate the 1.0 release from 0.9.3. And it was a very concerted effort, so a big thanks to all contributors involved, For the people who don't know the team, here a short introduction:&lt;p&gt;First and foremost Benau, who originally told me he would only like to work on some of the GUI aspects of networking, but who then grew to meet the challenges the network implementation threw at us. Without his work networking support would nowhere close to being ready or as complete as it is.&lt;/p&gt;&lt;p&gt;Alayan, a relatively new main contributor started several months ago, with a keen interest in balancing game play. He is an experienced speed runner for SuperTuxKart, and one of the best players in our online ranking system. While he was also essential in testing online racing, he made significant contributions to balancing SuperTuxKart and overall improving the playing experience.&lt;/p&gt;&lt;p&gt;We are still reaping the benefits from Google's Summer of Code program: leyyin is still with us, and he is helping us with the server side implementation. E.g. anytime you are looking for servers, his code will be involved. Then let's not forget Hilnius, who back in 2013 started to design the protocol implementation on which our networking code is still based. A very successful result for our participation in the GSoC.&lt;/p&gt;&lt;p&gt;Then the older core team, and I'll be a bit briefer here: Deve as always makes sure that all is fine on the Linux front, do many bugfixes and small enhancements, and maintains and improves our Android port. Auria keeps everything else running - pull requests, bug reports, forum posts, ...  Samuncle, though busy with his private life, came through with the wonderful new &quot;Ravenbridge Mansion&quot; track. And Arthur keeping our social media feeds updated. Oh yes, there is also me (hiker) :)&lt;/p&gt;&lt;p&gt;For the record: is the 1.0 release 'perfect'? No, it is not. There are improvements to be done, features to be added, and gameplay to be improved. Still, it is probably the most significant step forward in SuperTuxKart's more than 12 years of history. We would like to invite all of you to join some online games by fetching the new 1.0 release from our &lt;a href=&quot;https://supertuxkart.net/Download&quot;&gt;Download&lt;/a&gt; page.&lt;/p&gt;&lt;p&gt;On behalf of the SuperTuxKart team: thank you for your continued support through the years, and enjoy the big 1.0!&lt;/p&gt;
</description>
<pubDate>Sun, 21 Apr 2019 09:02:45 +0000</pubDate>
<dc:creator>shimabukuro</dc:creator>
<og:url>http://blog.supertuxkart.net/2019/04/supertuxkart-10-release.html</og:url>
<og:title>SuperTuxKart 1.0 Release</og:title>
<og:description>Yes, if you have followed our development a bit, that might be a bit of a surprise. But we have been asked why we don't call this release 1....</og:description>
<og:image>https://lh5.googleusercontent.com/proxy/Kg5Vkb0WZhErD6O_cALPOL0UW2Gu2bR2GAzIfEqPTQad5_7a5SqyHZguVz8L304M6XqMgtKzJnru2RW0=w1200-h630-n-k-no-nu</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://blog.supertuxkart.net/2019/04/supertuxkart-10-release.html</dc:identifier>
</item>
<item>
<title>Goodbye Joe</title>
<link>https://ferd.ca/goodbye-joe.html</link>
<guid isPermaLink="true" >https://ferd.ca/goodbye-joe.html</guid>
<description>&lt;span class=&quot;date&quot;&gt;2019/04/20&lt;/span&gt;&lt;h2&gt;Goodbye Joe&lt;/h2&gt;
&lt;p&gt;Joe Armstrong is mainly known as the father of Erlang, and the Erlang family has always been relatively small and closely knit. Anyone whose first Erlang conference (usually Erlang Factory, Erlang User Conference, or CodeBEAM) had Joe in the attendance would have a similar reaction. There was a feeling of awe about how accessible the community was. Here you were, and big names like Joe and Robert—who everyone knew by their first names—were right around the same room, friendly, and willing to talk to anybody. You'd feel like you were welcome no matter who you were.&lt;/p&gt;
&lt;p&gt;Today, we've learned of Joe's passing away. I wasn't a super close friend of Joe, but I have known him and talked with him at various conferences over the last ten years or so. He's unsurprisingly been a huge influence in my career, and so I thought I should write this little post about him and his impact. My words can't give justice to the man he was, but I felt I needed to write this up.&lt;/p&gt;
&lt;p&gt;Around 10 years ago, I used to sling PHP for a dating site, fresh out of a multimedia program in college. I had done a bit of C, Python, Scheme, Javascript, and a few other languages for fun by then. I had been told to figure out how to renew our chat system, and since back then Facebook had written their first one in Erlang (before rewriting it in C++, then buying WhatsApp), my boss threw a copy of Joe Armstrong's Programming Erlang (the first edition) on my desk.&lt;/p&gt;
&lt;p&gt;This started my whole story with Erlang, which still goes on today. Joe's book was approachable, the same way he was. He could explain like no other what the principles of Erlang are, what led to its design, and the core approach to fault tolerance that it embodies. It's one of the few language-specific books that is not content with getting you to write code, but lets you understand &lt;em&gt;why&lt;/em&gt; you should write it that way. The language didn't just have features because they were cool, it had features because they were needed for fault tolerance. What Joe told you applied anywhere else.&lt;/p&gt;
&lt;p&gt;I would end up reading almost everything I could that Joe wrote. Among them:&lt;/p&gt;
&lt;p&gt;One of the amazing things Joe mentioned in his texts that was out of the ordinary compared to everything I had read before is that &lt;em&gt;developers would make mistakes and we could not prevent them all&lt;/em&gt;. Instead, we had to be able to &lt;em&gt;cope&lt;/em&gt; with them. He did not just tell you about a language, he launched you on a trail that taught you how to write entire systems. But Joe's advice and writing went further than this. He was always a huge fan of systematic simplicity, self-contained straightforwardness, and stories from the older times of computing that gave a fresh perspective on everything.&lt;/p&gt;
&lt;p&gt;Rather than thinking software is never finished, he wanted software that was so simple it &lt;em&gt;could&lt;/em&gt; actually be completed. The strength of your program should be from how extensible and composable its usage would be, not your ability to never be done writing it. On top of that, he wanted code to keep working forever.&lt;/p&gt;
&lt;p&gt;One of his blog posts, &lt;a href=&quot;https://joearms.github.io/#2013-11-21%20My%20favorite%20Erlang%20Program&quot;&gt;My favorite Erlang Program&lt;/a&gt;, perfectly embodies this. Joe was the kind of person who would take a simple idea, and push it as far as possible, uncovering some great insights on the way. Then he'd share it with everyone.&lt;/p&gt;
&lt;p&gt;Joe was a man who would get a thousand ideas a day. Maybe a few of them would be good or practical. For each of these, you'd probably need a whole team to take the underlying principles and make them usable for the rest of the world. But Joe would go on undeterred, always excited, and happy as ever exploring ideas and new concepts all the time. Some would be bad, some would be great. You never really knew what you'd end up with, but you know you'd have passion and great discussions. Should PDFs be both the documentation for a piece of code and the compiled artifact? What if no piece of code ever bore a name and instead you'd just use hashes for all of them and had globally unique everything? It would be good if everyone aimed for inter-program composability rather than whatever it is they are doing right now!&lt;/p&gt;
&lt;img src=&quot;https://ferd.ca/static/img/joe-fred.jpg&quot; alt=&quot;Joe and I debugging some AV setup&quot;/&gt;&lt;p&gt;Joe and I, back at Chicago Erlang in 2014, debugging some presentation setup. Photo by &lt;a href=&quot;https://www.dropbox.com/sh/18w4l9vbmgu98ov/AAAiTRknnIBbJAOEmn72INRfa?dl=0&quot;&gt;Brian Troutwine&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you were talking with people at a conference, all standing in a circle, and you suddenly got Joe's attention, he'd be inching closer and closer to you, entirely focused, eventually ending up face-to-face with you, the circle crushed by his interest. You'd be able to have his undivided attention for a while, between huge bursts of laughter. He carried this everywhere. You'd invite him to talk about something, and if he had discovered something he felt was more interesting than what he had initially planned, he'd talk about that instead, would go over time if he had to, and everyone was fine with it because, well, that's just Joe. And it was always fun stuff, too.&lt;/p&gt;
&lt;p&gt;On a more personal level, Joe was super enthusiastic about some of my first contributions to the language. He wrote the foreword to &lt;em&gt;Learn You Some Erlang&lt;/em&gt;, and was always up for a chat. He's had a major influence on my career, and I owe him much. He has unknowingly sent me down a path I am still gladly walking.&lt;/p&gt;
&lt;p&gt;Outside of software, Joe really enjoyed music, typesetting, and creative writing, which he would gladly tell you about. He's not known for these to nearly the same extent, but he did it all with the same passion and enjoyment.&lt;/p&gt;
&lt;p&gt;He was part of the Erlang landscape, always interested in what people had to say. His passion and enjoyment about the craft, even in his 60s, was still high up at levels I don't even know I ever had or will ever have, and I have to say I am envious of him for that. I don't know what it will be like to have this community without him around. He was humble. He was approachable. He was excited. He was creative. His legacy is not just in code, but in the communities in which he instantly became a central part. He will be missed.&lt;/p&gt;
&lt;p&gt;Hello Mike,&lt;br/&gt;Hello Robert,&lt;br/&gt;Goodbye Joe.&lt;/p&gt;
</description>
<pubDate>Sun, 21 Apr 2019 02:55:18 +0000</pubDate>
<dc:creator>mononcqc</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://ferd.ca/goodbye-joe.html</dc:identifier>
</item>
<item>
<title>Smoke seen for miles as SpaceX Crew Dragon suffers anomaly at Cape Canaveral</title>
<link>https://www.floridatoday.com/story/tech/science/space/2019/04/20/smoke-seen-miles-spacex-crew-dragon-suffers-anomaly-cape-canaveral/3531086002/</link>
<guid isPermaLink="true" >https://www.floridatoday.com/story/tech/science/space/2019/04/20/smoke-seen-miles-spacex-crew-dragon-suffers-anomaly-cape-canaveral/3531086002/</guid>
<description>&lt;p class=&quot;speakable-p-1 p-text&quot;&gt;A SpaceX Crew Dragon capsule suffered an anomaly during an engine test firing at Cape Canaveral Air Force Station on Saturday afternoon, company and 45th Space Wing officials confirmed.&lt;/p&gt;
&lt;p class=&quot;speakable-p-2 p-text&quot;&gt;&quot;On April 20, 2019, an anomaly occurred at Cape Canaveral Air Force Station during the Dragon 2 static test fire,&quot; Wing Spokesman Jim Williams told FLORIDA TODAY. &quot;The anomaly was contained and there were no injuries.&quot;&lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;FLORIDA TODAY photographer Craig Bailey, covering a surf fest in Cocoa Beach, captured an image of orange plumes rising from SpaceX facilities at the Cape around 3:30 p.m. Unconfirmed reports indicated the capsule was destroyed.&lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;“Earlier today, SpaceX conducted a series of engine tests on a Crew Dragon test vehicle on our test stand at Landing Zone 1 in Cape Canaveral, Florida,&quot; SpaceX said in a statement. &quot;The initial tests completed successfully but the final test resulted in an anomaly on the test stand.&quot;&lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;&quot;Ensuring that our systems meet rigorous safety standards and detecting anomalies like this prior to flight are the main reasons why we test. Our teams are investigating and working closely with our NASA partners,&quot; the company said.&lt;/p&gt;

&lt;p class=&quot;p-text&quot;&gt;SpaceX's Crew Dragon, also referred to as Dragon 2, is designed to take humans to the International Space Station and &lt;a data-track-label=&quot;inline|intext|n/a&quot; href=&quot;https://www.floridatoday.com/story/tech/science/space/2019/03/08/spacex-crew-dragon-departs-iss-atlantic-ocean-splashdown/3097044002/&quot; target=&quot;_blank&quot;&gt;successfully flew for the first time in March&lt;/a&gt;. The company was planning to launch a crewed version of the spacecraft no earlier than July, but was also planning an in-flight abort test, or a demonstration of its life-saving abort capabilities, sometime before then. &lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;It is unknown which Crew Dragon was involved in the Saturday anomaly, but each spacecraft has Super Draco thrusters designed to be used as a launch abort system. All SpaceX engines require occasional test firings to evaluate readiness and performance.&lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;SpaceX's timeline to return crews to the ISS from U.S. soil will now likely be modified as the investigation into the incident continues &lt;/p&gt;
&lt;p class=&quot;p-text&quot;&gt;&lt;em&gt;Contact Emre Kelly at aekelly@floridatoday.com or 321-242-3715. Follow him on &lt;a data-track-label=&quot;inline|intext|n/a&quot; href=&quot;https://twitter.com/EmreKelly&quot; target=&quot;_blank&quot;&gt;Twitter&lt;/a&gt;, &lt;a data-track-label=&quot;inline|intext|n/a&quot; href=&quot;https://www.facebook.com/emre.kelly&quot; target=&quot;_blank&quot;&gt;Facebook&lt;/a&gt; and &lt;a data-track-label=&quot;inline|intext|n/a&quot; href=&quot;https://www.instagram.com/emrekelly/&quot; target=&quot;_blank&quot;&gt;Instagram&lt;/a&gt; at @EmreKelly.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&quot;module-position-Run0kTb2_0Y&quot; class=&quot;story-asset gallery-asset&quot;&gt;


&lt;div class=&quot;companion-story-gallery js-llc&quot; data-name=&quot;compStoryGallery&quot;&gt;

&lt;div class=&quot;companion-galleries embedded_story hasendslate galleries&quot; data-gallery-id=&quot;3038737002&quot; data-title=&quot;Photos: SpaceX launches Crew Dragon on mission to ISS&quot; data-seo-title=&quot;&quot; data-ssts=&quot;tech/science/space&quot; data-cst=&quot;tech/science&quot; data-published-date=&quot;2019-03-02T10:38:08.614Z&quot; data-gal-pageurl=&quot;https://www.floridatoday.com/picture-gallery/tech/science/space/2019/03/02/photos-spacex-launches-crew-dragon-on-mission-to-iss/3038737002/&quot;&gt;
&lt;div class=&quot;viewport gallery-viewport&quot;&gt;
&lt;div class=&quot;slide gallery-viewport-slide horizontal active&quot; itemprop=&quot;primaryImageOfPage&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.6969696969697&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo horizontal gallery-photo-first&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/f056ee08-ad68-4865-82ac-66a9cba818e4-uscp-74awx22kpvrisl1kgbl_original.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/f056ee08-ad68-4865-82ac-66a9cba818e4-uscp-74awx22kpvrisl1kgbl_original.jpg?width=500&amp;amp;height=339&quot; data-byline=&quot;Craig Bailey / FLORIDA TODAY&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/f056ee08-ad68-4865-82ac-66a9cba818e4-uscp-74awx22kpvrisl1kgbl_original.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/f056ee08-ad68-4865-82ac-66a9cba818e4-uscp-74awx22kpvrisl1kgbl_original.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;A SpaceX Falcon 9 rocket lifts off from pad 39A at Kennedy Space Center on Saturday, March 2, 2019. This is the first launch of the rocket with the Crew Dragon capsule designed to carry humans into space.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide vertical&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.6969696969697&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo vertical&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3df0f176-e30a-493f-a8b9-fca2a264b25f-uscp-74awwg48o39cd31xgbl_original.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3df0f176-e30a-493f-a8b9-fca2a264b25f-uscp-74awwg48o39cd31xgbl_original.jpg?width=299&amp;amp;height=400&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3df0f176-e30a-493f-a8b9-fca2a264b25f-uscp-74awwg48o39cd31xgbl_original.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3df0f176-e30a-493f-a8b9-fca2a264b25f-uscp-74awwg48o39cd31xgbl_original.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;A SpaceX Falcon 9 rocket lifts off from pad 39A at Kennedy Space Center on Saturday, March 2, 2019. This is the first launch of the rocket with the Crew Dragon capsule designed to carry humans into space.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide horizontal&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.1268656716418&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo horizontal&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/6f5303d1-14e6-4f58-8ea7-770a24e580bb-uscp-74awwc1utno112j36gbl_original.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/6f5303d1-14e6-4f58-8ea7-770a24e580bb-uscp-74awwc1utno112j36gbl_original.jpg?width=500&amp;amp;height=336&quot; data-byline=&quot;MALCOLM DENEMARK/FLORIDA TODAY&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/6f5303d1-14e6-4f58-8ea7-770a24e580bb-uscp-74awwc1utno112j36gbl_original.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/6f5303d1-14e6-4f58-8ea7-770a24e580bb-uscp-74awwc1utno112j36gbl_original.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;As seen from Cocoa: A SpaceX Falcon 9 rocket launches from Kennedy Space Center with the Crew Dragon spacecraft on Saturday, March 2, 2019.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide vertical&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.1113989637306&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo vertical&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/e03808be-8e90-45a7-8d61-84944cc8ce4a-uscp-74aww4qd9gzkn5sugbl_original.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/e03808be-8e90-45a7-8d61-84944cc8ce4a-uscp-74aww4qd9gzkn5sugbl_original.jpg?width=374&amp;amp;height=400&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/e03808be-8e90-45a7-8d61-84944cc8ce4a-uscp-74aww4qd9gzkn5sugbl_original.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/e03808be-8e90-45a7-8d61-84944cc8ce4a-uscp-74aww4qd9gzkn5sugbl_original.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;As seen from Cocoa: SpaceX's Falcon 9 rocket and Crew Dragon capsule launch from Kennedy Space Center on Saturday, March 2, 2019.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide horizontal&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;6.051724137931&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo horizontal&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3a41b6e7-7ea2-496e-afab-28e8436963bc-SpaceX_as_seen_from_Viera_1.JPG&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3a41b6e7-7ea2-496e-afab-28e8436963bc-SpaceX_as_seen_from_Viera_1.JPG?width=500&amp;amp;height=305&quot; data-byline=&quot;TIM SHORTT/ FLORIDA TODAY&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3a41b6e7-7ea2-496e-afab-28e8436963bc-SpaceX_as_seen_from_Viera_1.JPG?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/3a41b6e7-7ea2-496e-afab-28e8436963bc-SpaceX_as_seen_from_Viera_1.JPG?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;The SpaceX rocket launch with Crew Dragon capsule as seen in a time exposure from Viera.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide horizontal&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.672131147541&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo horizontal&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/b0c564aa-fd75-49a0-91bc-b8ccfb1db5b6-ElonMuskDM1.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/b0c564aa-fd75-49a0-91bc-b8ccfb1db5b6-ElonMuskDM1.jpg?width=500&amp;amp;height=333&quot; data-byline=&quot;Emre Kelly / FLORIDA TODAY&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/b0c564aa-fd75-49a0-91bc-b8ccfb1db5b6-ElonMuskDM1.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/b0c564aa-fd75-49a0-91bc-b8ccfb1db5b6-ElonMuskDM1.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;At center, SpaceX CEO Elon Musk speaks to the press during a post-launch briefing at Kennedy Space Center. At left is NASA Administrator Jim Bridenstine; at right, Astronaut Bob Behnken.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide horizontal&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;7.7080291970803&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo horizontal&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/dd86bc26-da82-4651-b9e6-5783d933c293-crb030219_spacex_7_.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/dd86bc26-da82-4651-b9e6-5783d933c293-crb030219_spacex_7_.jpg?width=500&amp;amp;height=355&quot; data-byline=&quot;Craig Bailey/FLORIDA TODAY&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/dd86bc26-da82-4651-b9e6-5783d933c293-crb030219_spacex_7_.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/dd86bc26-da82-4651-b9e6-5783d933c293-crb030219_spacex_7_.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;A SpaceX Falcon 9 rocket lifts off from Pad 39A at Kennedy Space Center early Saturday, March 2, 2019. This is the first launch of the rocket with the Crew Dragon capsule that was designed to carry humans into space.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;slide gallery-viewport-slide vertical&quot; itemprop=&quot;associatedMedia&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/ImageObject&quot;&gt;
&lt;div class=&quot;gallery-photo-border&quot; readability=&quot;8.1909090909091&quot;&gt;&lt;img itemprop=&quot;image&quot; class=&quot;gallery-photo vertical&quot; data-mycapture-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/29729d12-da28-417f-9063-f859883bf0ff-crb030219_spacex_6_.jpg&quot; data-mycapture-sm-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/29729d12-da28-417f-9063-f859883bf0ff-crb030219_spacex_6_.jpg?width=291&amp;amp;height=400&quot; data-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/29729d12-da28-417f-9063-f859883bf0ff-crb030219_spacex_6_.jpg?width=520&amp;amp;height=390&amp;amp;fit=bounds&amp;amp;auto=webp&quot; data-large-src=&quot;https://www.gannett-cdn.com/presto/2019/03/02/PBRE/29729d12-da28-417f-9063-f859883bf0ff-crb030219_spacex_6_.jpg?width=1920&amp;amp;height=1440&amp;amp;fit=bounds&amp;amp;auto=webp&quot; alt=&quot;A SpaceX Falcon 9 rocket lifts off from Pad 39A at Kennedy Space Center early Saturday, March 2, 2019. This is the first launch of the rocket with the Crew Dragon capsule that was designed to carry humans into space.&quot; data-id=&quot;&quot;/&gt;&lt;p&gt;&lt;span class=&quot;companion-mycapture-btn mycapture-small-btn mycapture-gallery-btn mycapture-btn-with-text js-mycapture-btn&quot;&gt;Buy Photo&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;

&lt;div class=&quot;feature-btns&quot;&gt;

&lt;p&gt;&lt;span class=&quot;playbtn feature-btns-label&quot;&gt;Autoplay&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;thumbLabel feature-btns-label&quot;&gt;Show Thumbnails&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;captionLabel feature-btns-label&quot;&gt;Show Captions&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;a class=&quot;slide-nav prev gallery-prev&quot; rel=&quot;prev&quot;&gt;Last Slide&lt;/a&gt;&lt;a class=&quot;slide-nav next gallery-next&quot; rel=&quot;next&quot;&gt;Next Slide&lt;/a&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;p-text p-text-last&quot; id=&quot;article-body-p-last&quot;&gt; &lt;/p&gt;

&lt;p&gt;Read or Share this story: https://www.floridatoday.com/story/tech/science/space/2019/04/20/smoke-seen-miles-spacex-crew-dragon-suffers-anomaly-cape-canaveral/3531086002/&lt;/p&gt;
</description>
<pubDate>Sun, 21 Apr 2019 00:40:41 +0000</pubDate>
<dc:creator>jbuzbee</dc:creator>
<og:image>https://www.gannett-cdn.com/presto/2019/04/20/PBRE/595e3324-f26a-4aeb-8657-64563981fc28-crb042019_spacex_3_.jpg?crop=2426,1371,x0,y0&amp;width=3200&amp;height=1680&amp;fit=bounds</og:image>
<og:title>Smoke seen for miles as SpaceX Crew Dragon suffers anomaly at Cape Canaveral</og:title>
<og:description>A SpaceX Crew Dragon capsule suffered an anomaly during a routine test fire at Cape Canaveral Air Force Station, the 45th Space Wing confirmed.</og:description>
<og:url>https://www.floridatoday.com/story/tech/science/space/2019/04/20/smoke-seen-miles-spacex-crew-dragon-suffers-anomaly-cape-canaveral/3531086002/</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.floridatoday.com/story/tech/science/space/2019/04/20/smoke-seen-miles-spacex-crew-dragon-suffers-anomaly-cape-canaveral/3531086002/</dc:identifier>
</item>
<item>
<title>Antenna Theory (2016)</title>
<link>http://www.antenna-theory.com/</link>
<guid isPermaLink="true" >http://www.antenna-theory.com/</guid>
<description>&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;The work on this website is copyrighted. It is ok to reference the site material online with an appropriate link to the site, or proper citation if in print form. Copyright 2009-2016 Antenna-Theory.com. Antennas, Antenna Basics, &lt;a href=&quot;http://www.antenna-theory.com/basics/gain.php&quot;&gt;Antenna Gain&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;center readability=&quot;1.6129032258065&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://www.antenna-theory.com/spanish/antena.php&quot;&gt;Teoría de la antena en Español&lt;/a&gt;   &lt;a href=&quot;http://www.antenna-theory.com/cn/antenna.php&quot;&gt;天线理论（中文）&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Check out this Fourier Transform website:   &lt;a href=&quot;http://www.thefouriertransform.com&quot;&gt;Fourier Transforms&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;hr width=&quot;50%&quot;/&gt;&lt;/center&gt;
&lt;span&gt;About this Site:&lt;/span&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;Antennas and Antenna Theory has always been a fascinating subject for me, and it is this excitement that leads me to present this tutorial. In my life, I have found that once I thoroughly understand a subject, I am amazed at how simple it seems, despite the initial complexity. This I have found true for a wide range of activities, be it riding a motorcycle, learning about antennas, or understanding physical phenomena such as electromagnetics. With that in mind, I endeavor to write this Antenna Theory website in the simplest of all possible manners.&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;Specifically, consider this statement: &lt;em&gt;Complexity is not a sign of intelligence; simplify.&lt;/em&gt; I have found this to a priceless amount of wisdom. In that regard, one need not know the intricacies of Lebesgue integration or complex integrals involving Cauchy residues in order to fully understand Antenna Theory. In fact, most people who focus on the mathematical intricacies of &lt;a href=&quot;http://www.maxwells-equations.com&quot;&gt;Maxwell's Equations&lt;/a&gt; tend to be poor practicing antenna engineers (probably because they try to write code or derive integrals instead of put a product together). The subject of Antennas is best understood intuitively; this is in stark contrast to the methods in University, where complex math pervades every page. I do not think this approach effectively teaches antenna theory. In these pages you will not find rigorous mathematical analysis which only apply in the simplest of antenna cases (and are ultimately artificial for the real world); I will try to state facts and a minimum of math except where necessary. I also try to avoid making things unnecessarily complicated. For instance, &lt;a href=&quot;http://www.antenna-theory.com/definitions/vswr.php&quot;&gt;VSWR&lt;/a&gt; stands for &lt;em&gt;Voltage Standing Wave Ratio&lt;/em&gt;, and sounds very complicated. But it is simply a measure of how much power is reflected from an antenna. Avoiding unnecessary complexity is a lifelong goal of mine.&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;About me:&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;I am a practicing antenna engineer, with a PhD in antennas and I have worked for many years in defense, university and the consumer electronics field as an antenna engineer. My dissertation in pdf form is available here: &lt;em&gt;&lt;a href=&quot;http://www.antenna-theory.com/Bevelacqua-Dissertation.pdf&quot;&gt;Antenna Arrays: Performance Limits and Geometry Optimization&lt;/a&gt;&lt;/em&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;Other Projects:&lt;/span&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;span&gt;A simple physics site entitled &lt;a href=&quot;http://www.whyistheskyblue.co&quot;&gt;Why is the sky blue?&lt;/a&gt; and a daylight saving time project entitled &lt;a href=&quot;http://www.whendoesthetimechange.com&quot;&gt;when does the time change&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 20 Apr 2019 21:24:31 +0000</pubDate>
<dc:creator>dosshell</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.antenna-theory.com/</dc:identifier>
</item>
<item>
<title>Half of England Is Owned by Less Than 1% of Its Population, Researcher Says</title>
<link>https://www.nytimes.com/2019/04/19/world/europe/england-land-inequality.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2019/04/19/world/europe/england-land-inequality.html</guid>
<description>&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;LONDON — Land ownership in England, a source of enormous wealth, is often shielded by a culture of secrecy harking back to the Middle Ages. But a researcher says that after years of digging, he has an answer:&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Less than 1 percent of the population — including aristocrats, royals and wealthy investors — owns about half of the land, according to “Who Owns England,” a book that is to be published in May. And many of them inherited the property as members of families that have held it for generations — even centuries.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;In the book, a copy of which was obtained by The New York Times, the author, Guy Shrubsole, an environmental activist and writer, identifies many of the owners and compiles data gathered by peppering public bodies with freedom of information requests and combing through the 25 million title records in the government’s Land Registry.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;He reached a striking conclusion — that in England, home to about 56 million people, half the country belongs to just 25,000 landowners, some of them corporations.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The findings go to the heart of a potent political issue — economic inequality — that is roiling nations and feeding populist movements on multiple continents. Leaders of the opposition Labour Party seized on Mr. Shrubsole’s findings, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theguardian.com/money/2019/apr/17/who-owns-england-thousand-secret-landowners-author?CMP=Share_AndroidApp_WhatsApp&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;first published this week in t&lt;/a&gt;he newspaper &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theguardian.com/money/2019/apr/17/who-owns-england-thousand-secret-landowners-author?CMP=Share_AndroidApp_WhatsApp&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;The Guardian&lt;/a&gt;, as evidence for the case they have made for years against the governing Conservative Party.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;

&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“Don’t let anyone tell you our country doesn’t need radical change,” Jeremy Corbyn, the party leader, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://twitter.com/jeremycorbyn/status/1118623502565871617&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;wrote on Twitter&lt;/a&gt; as he shared The Guardian’s article on Thursday.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Comparison to other developed countries is difficult, because they do not have national land registries. Records can be viewed only one at a time through hundreds of local registry officers, they are not fully open to the public and, as in the United States, ownership can be obscured through shell corporations.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;But Britain has &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://databank.worldbank.org/data/reports.aspx?source=2&amp;amp;series=SI.POV.GINI&amp;amp;country=&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;greater wealth inequality&lt;/a&gt; than peers like Germany, France, the Low Countries and Scandinavia — though less than the United States. And Britain has not seen the kinds of wars and revolutions that over centuries wiped away sprawling estates owned by nobility in most of Europe.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-79elbk&quot; data-testid=&quot;photoviewer-wrapper&quot;&gt;

&lt;div data-testid=&quot;photoviewer-children&quot; class=&quot;css-1a48zt4 ehw59r15&quot;&gt;
&lt;div class=&quot;css-1xdhyk6 erfvjey0&quot;&gt;&lt;span class=&quot;css-1ly73wi e1tej78p0&quot;&gt;Image&lt;/span&gt;&lt;img alt=&quot;&quot; class=&quot;css-1m50asq&quot; src=&quot;https://static01.nyt.com/images/2019/04/19/world/19england-land2/merlin_150577857_fb201e5e-1a35-42af-9394-e7f2273e7476-articleLarge.jpg?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot; srcset=&quot;https://static01.nyt.com/images/2019/04/19/world/19england-land2/merlin_150577857_fb201e5e-1a35-42af-9394-e7f2273e7476-articleLarge.jpg?quality=90&amp;amp;auto=webp 600w,https://static01.nyt.com/images/2019/04/19/world/19england-land2/merlin_150577857_fb201e5e-1a35-42af-9394-e7f2273e7476-jumbo.jpg?quality=90&amp;amp;auto=webp 1024w,https://static01.nyt.com/images/2019/04/19/world/19england-land2/merlin_150577857_fb201e5e-1a35-42af-9394-e7f2273e7476-superJumbo.jpg?quality=90&amp;amp;auto=webp 2048w&quot; sizes=&quot;((min-width: 600px) and (max-width: 1004px)) 84vw, (min-width: 1005px) 60vw, 100vw&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2019/04/19/world/19england-land2/merlin_150577857_fb201e5e-1a35-42af-9394-e7f2273e7476-articleLarge.jpg?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;css-8i9d0s e13ogyst0&quot;&gt;Working at a Rhubarb farm in Pudsey, near Leeds in northern England.&lt;/span&gt;&lt;span itemprop=&quot;copyrightHolder&quot; class=&quot;css-vuqh7u e1z0qqy90&quot;&gt;&lt;span class=&quot;css-1ly73wi e1tej78p0&quot;&gt;Credit&lt;/span&gt;&lt;span&gt;Oli Scarff/Agence France-Presse — Getty Images&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Who owns the &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.poetryfoundation.org/poems/54684/jerusalem-and-did-those-feet-in-ancient-time&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;“green and pleasant land”&lt;/a&gt; of the English countryside can be a well-kept secret, in part because a large segment of it does not even figure in public records. Government efforts to make a public accounting of land ownership date to the 19th century, but according to the Land Registry, about 15 percent of the country’s area, most of it rural, is still unrecorded.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“Much of the land owned by the Crown, the aristocracy, and the Church has not been registered, because it has never been sold, which is one of the main triggers for compulsory registration,” &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://hmlandregistry.blog.gov.uk/2018/02/05/search-owner-unregistered-land/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;the registry, which covers England and Wales, says on its website&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Mr. Shrubsole &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://whoownsengland.org/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;began documenting&lt;/a&gt; England’s estates after the referendum on Britain’s withdrawal from the European Union, known as Brexit, in 2016. “If Brexit really meant ‘taking back control of our country,’ then I’d like at least to know who owns it,” he wrote in &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theguardian.com/commentisfree/2017/mar/20/take-back-control-england-land-ownership&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;an op-ed in The Guardian&lt;/a&gt; a year after the vote.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Real estate prices in England are among the highest in Europe and have soared over the last generation. Mr. Shrubsole’s book documents ownership, maps unregistered land and argues that the concentration of ownership helps keep available land scarce and expensive.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Houses, stores, office buildings, schools and farms are often held under long-term leases, paying a steady stream of rents — directly or through intermediate leaseholders — to major landowners.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Mr. Shrubsole said that by publishing his research, he wanted to start a conversation.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“It should prompt a proper debate about the need for land reform in England,” Mr. Shrubsole said. The issue of land relates to the country’s housing crisis, to economic inequality, to climate change and the intensive use of farmland, he added.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;The ancient idea that wealth meant land does not always hold true in modern times. But in Britain, land accounted for half of the country’s net worth in 2016, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.ons.gov.uk/economy/nationalaccounts/uksectoraccounts/bulletins/nationalbalancesheet/2018#uk-net-worth-rises-by-almost-half-a-trillion-pounds&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;according to data from the Office of National Statistics&lt;/a&gt; — double that of Germany and higher than in countries like France, Canada and Japan.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Britain’s net worth more than tripled between 1995 and 2017, driven primarily by the value of land, which rose much faster than other kinds of assets.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“The main economic challenge and the social justice issue is that for the last 30, 40 years, landowners have enjoyed enormous unearned windfall gains at a faster rate than wages or the economy have grown,” said Josh Ryan-Collins, head of research at the &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.ucl.ac.uk/bartlett/public-purpose/home&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;Institute for Innovation and Public Purpose&lt;/a&gt; at University College London.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-79elbk&quot; data-testid=&quot;photoviewer-wrapper&quot;&gt;

&lt;div data-testid=&quot;photoviewer-children&quot; class=&quot;css-1a48zt4 ehw59r15&quot;&gt;

&lt;span class=&quot;css-8i9d0s e13ogyst0&quot;&gt;Sandringham, Queen Elizabeth II’s estate, is also a working farm, which entitles it to significant subsidies from the European Union.&lt;/span&gt;&lt;span itemprop=&quot;copyrightHolder&quot; class=&quot;css-vuqh7u e1z0qqy90&quot;&gt;&lt;span class=&quot;css-1ly73wi e1tej78p0&quot;&gt;Credit&lt;/span&gt;&lt;span&gt;Peter Macdiarmid/Getty Images&lt;/span&gt;&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“There is nothing that the landowners have done to earn those incomes,” he said.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Even agricultural land has become the object of speculative demand, pushing prices and gains for landowners up further, he said.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;But even if land reform has not been on the agenda of the Conservative government, it has had to address the housing crisis and agricultural subsidies. Recently, Conservatives have focused their criticism on &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2017/09/03/world/europe/uk-queen-elizabeth-brexit-subsidies.html?module=inline&quot; title=&quot;&quot;&gt;the European Union’s farming and forestry subsidy system&lt;/a&gt;, which has put aristocrats, the royal family and wealthy investors among the top recipients of taxpayer-funded aid.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Queen Elizabeth II’s estate in Sandringham, north of London, received £695,000 in aid in 2017, or more than $900,000, according to &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;http://cap-payments.defra.gov.uk/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;a public database of payments&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;&lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.gov.uk/government/news/landmark-agriculture-bill-to-deliver-a-green-brexit&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;An agriculture bill&lt;/a&gt;, currently in Parliament, promises to change farm subsidies after Brexit. Instead of direct payments based on the total amount of land farmed, payments in the new system would be based on factors like contributions to the environment, animal welfare and public access to the property.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1fanzo5 StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-53u6y8&quot;&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“As we know, many of the beneficiaries are not even U.K. or E.U. citizens, but foreign citizens who happen to have invested in agricultural land,” Michael Gove, Britain’s environment secretary, said &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theyworkforyou.com/debates/?id=2018-10-10b.149.0#g155.4&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;during a debate on the bill in Parliament&lt;/a&gt; last year. “It is a simple matter of social justice and economic efficiency that we need to change that system.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Most of the European Union is also grappling with concentrated ownership of farmland, though not to the same degree. &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;http://www.europarl.europa.eu/doceo/document/A-8-2017-0119_EN.html&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;A 2017 report by European Parliament lawmakers&lt;/a&gt; said that in 2010, 3 percent of farms controlled half the agricultural land with in the bloc.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“Agricultural land is not an ordinary traded good, as soil is nonrenewable and access to it is a human right,” the report said. “As with the concentration of financial wealth, too high a concentration of agricultural land splits society, destabilizes rural areas, threatens food safety and thus jeopardizes the environmental and social objectives of Europe.”&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;Scotland, where land ownership is &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.holyrood.com/articles/comment/land-reform-and-inequality-what-does-debate-tell-us-about-scotland&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;in the hands of even fewer people and organizations&lt;/a&gt;, has enacted &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.gov.scot/policies/land-reform/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;a set of land reform laws&lt;/a&gt;. In 2004, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www2.gov.scot/Publications/2004/08/19790/41578&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;it abolished feudal rules&lt;/a&gt; that were still in effect, helping many longtime tenants to become outright owners of their land. Other legislation introduced the right to roam, giving the public access to vast privately held lands.&lt;/p&gt;
&lt;p class=&quot;css-1ygdjhk evys1bk0&quot;&gt;“The example of successful land reform programs in other countries, like Scotland, should give us hope,” Mr. Shrubsole wrote in his book. “Get land reform right, and we can go a long way towards ending the housing crisis, restoring nature and making our society more equal.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-o6xoe7&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Sat, 20 Apr 2019 20:39:19 +0000</pubDate>
<dc:creator>pseudolus</dc:creator>
<og:url>https://www.nytimes.com/2019/04/19/world/europe/england-land-inequality.html</og:url>
<og:type>article</og:type>
<og:title>Half of England Is Owned by Less Than 1% of Its Population, Researcher Says</og:title>
<og:image>https://static01.nyt.com/images/2019/04/19/world/19england-land1/19england-land1-facebookJumbo.jpg</og:image>
<og:description>A new book identifies aristocrats, investors and companies that own vast portions of the country, amid growing complaints about land scarcity and economic inequality.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2019/04/19/world/europe/england-land-inequality.html</dc:identifier>
</item>
<item>
<title>A visual proof that neural nets can approximate any continuous function</title>
<link>http://neuralnetworksanddeeplearning.com/chap4.html</link>
<guid isPermaLink="true" >http://neuralnetworksanddeeplearning.com/chap4.html</guid>
<description>&lt;p&gt;One of the most striking facts about neural networks is that they can compute any function at all. That is, suppose someone hands you some complicated, wiggly function, $f(x)$:&lt;/p&gt;
&lt;p&gt; No matter what the function, there is guaranteed to be a neural network so that for every possible input, $x$, the value $f(x)$ (or some close approximation) is output from the network, e.g.:&lt;/p&gt;
&lt;p&gt;This result holds even if the function has many inputs, $f = f(x_1, \ldots, x_m)$, and many outputs. For instance, here's a network computing a function with $m = 3$ inputs and $n = 2$ outputs:&lt;/p&gt;
&lt;p&gt;This result tells us that neural networks have a kind of &lt;em&gt;universality&lt;/em&gt;. No matter what function we want to compute, we know that there is a neural network which can do the job.&lt;/p&gt;
&lt;p&gt;What's more, this universality theorem holds even if we restrict our networks to have just a single layer intermediate between the input and the output neurons - a so-called single hidden layer. So even very simple network architectures can be extremely powerful.&lt;/p&gt;
&lt;p&gt;The universality theorem is well known by people who use neural networks. But why it's true is not so widely understood. Most of the explanations available are quite technical. For instance, one of the original papers proving the result*&lt;span class=&quot;marginnote&quot;&gt;*&lt;a href=&quot;http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf&quot;&gt;Approximation by superpositions of a sigmoidal function&lt;/a&gt;, by George Cybenko (1989). The result was very much in the air at the time, and several groups proved closely related results. Cybenko's paper contains a useful discussion of much of that work. Another important early paper is &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/0893608089900208&quot;&gt;Multilayer feedforward networks are universal approximators&lt;/a&gt;, by Kurt Hornik, Maxwell Stinchcombe, and Halbert White (1989). This paper uses the Stone-Weierstrass theorem to arrive at similar results.&lt;/span&gt; did so using the Hahn-Banach theorem, the Riesz Representation theorem, and some Fourier analysis. If you're a mathematician the argument is not difficult to follow, but it's not so easy for most people. That's a pity, since the underlying reasons for universality are simple and beautiful.&lt;/p&gt;
&lt;p&gt;In this chapter I give a simple and mostly visual explanation of the universality theorem. We'll go step by step through the underlying ideas. You'll understand why it's true that neural networks can compute any function. You'll understand some of the limitations of the result. And you'll understand how the result relates to deep neural networks.&lt;/p&gt;
&lt;p&gt;To follow the material in the chapter, you do not need to have read earlier chapters in this book. Instead, the chapter is structured to be enjoyable as a self-contained essay. Provided you have just a little basic familiarity with neural networks, you should be able to follow the explanation. I will, however, provide occasional links to earlier material, to help fill in any gaps in your knowledge.&lt;/p&gt;
&lt;p&gt;Universality theorems are a commonplace in computer science, so much so that we sometimes forget how astonishing they are. But it's worth reminding ourselves: the ability to compute an arbitrary function is truly remarkable. Almost any process you can imagine can be thought of as function computation. Consider the problem of naming a piece of music based on a short sample of the piece. That can be thought of as computing a function. Or consider the problem of translating a Chinese text into English. Again, that can be thought of as computing a function*&lt;span class=&quot;marginnote&quot;&gt;*Actually, computing one of many functions, since there are often many acceptable translations of a given piece of text.&lt;/span&gt;. Or consider the problem of taking an mp4 movie file and generating a description of the plot of the movie, and a discussion of the quality of the acting. Again, that can be thought of as a kind of function computation*&lt;span class=&quot;marginnote&quot;&gt;*Ditto the remark about translation and there being many possible functions.&lt;/span&gt;. Universality means that, in principle, neural networks can do all these things and many more.&lt;/p&gt;
&lt;p&gt;Of course, just because we know a neural network exists that can (say) translate Chinese text into English, that doesn't mean we have good techniques for constructing or even recognizing such a network. This limitation applies also to traditional universality theorems for models such as Boolean circuits. But, as we've seen earlier in the book, neural networks have powerful algorithms for learning functions. That combination of learning algorithms + universality is an attractive mix. Up to now, the book has focused on the learning algorithms. In this chapter, we focus on universality, and what it means.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#two_caveats&quot;&gt;Two caveats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Before explaining why the universality theorem is true, I want to mention two caveats to the informal statement &quot;a neural network can compute any function&quot;.&lt;/p&gt;
&lt;p&gt;First, this doesn't mean that a network can be used to &lt;em&gt;exactly&lt;/em&gt; compute any function. Rather, we can get an &lt;em&gt;approximation&lt;/em&gt; that is as good as we want. By increasing the number of hidden neurons we can improve the approximation. For instance, &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#basic_network_precursor&quot;&gt;earlier&lt;/a&gt; I illustrated a network computing some function $f(x)$ using three hidden neurons. For most functions only a low-quality approximation will be possible using three hidden neurons. By increasing the number of hidden neurons (say, to five) we can typically get a better approximation:&lt;/p&gt;
&lt;p&gt;And we can do still better by further increasing the number of hidden neurons.&lt;/p&gt;
&lt;p&gt;To make this statement more precise, suppose we're given a function $f(x)$ which we'd like to compute to within some desired accuracy $\epsilon &amp;gt; 0$. The guarantee is that by using enough hidden neurons we can always find a neural network whose output $g(x)$ satisfies $|g(x) - f(x)| &amp;lt; \epsilon$, for all inputs $x$. In other words, the approximation will be good to within the desired accuracy for every possible input.&lt;/p&gt;
&lt;p&gt;The second caveat is that the class of functions which can be approximated in the way described are the &lt;em&gt;continuous&lt;/em&gt; functions. If a function is discontinuous, i.e., makes sudden, sharp jumps, then it won't in general be possible to approximate using a neural net. This is not surprising, since our neural networks compute continuous functions of their input. However, even if the function we'd really like to compute is discontinuous, it's often the case that a continuous approximation is good enough. If that's so, then we can use a neural network. In practice, this is not usually an important limitation.&lt;/p&gt;
&lt;p&gt;Summing up, a more precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision. In this chapter we'll actually prove a slightly weaker version of this result, using two hidden layers instead of one. In the problems I'll briefly outline how the explanation can, with a few tweaks, be adapted to give a proof which uses only a single hidden layer.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#universality_with_one_input_and_one_output&quot;&gt;Universality with one input and one output&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To understand why the universality theorem is true, let's start by understanding how to construct a neural network which approximates a function with just one input and one output:&lt;/p&gt;
&lt;p&gt;It turns out that this is the core of the problem of universality. Once we've understood this special case it's actually pretty easy to extend to functions with many inputs and many outputs.&lt;/p&gt;
&lt;p&gt;To build insight into how to construct a network to compute $f$, let's start with a network containing just a single hidden layer, with two hidden neurons, and an output layer containing a single output neuron:&lt;/p&gt;
&lt;p&gt;To get a feel for how components in the network work, let's focus on the top hidden neuron. In the diagram below, click on the weight, $w$, and drag the mouse a little ways to the right to increase $w$. You can immediately see how the function computed by the top hidden neuron changes:&lt;/p&gt;
&lt;p&gt;As we learnt &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons&quot;&gt;earlier in the book&lt;/a&gt;, what's being computed by the hidden neuron is $\sigma(wx + b)$, where $\sigma(z) \equiv 1/(1+e^{-z})$ is the sigmoid function. Up to now, we've made frequent use of this algebraic form. But for the proof of universality we will obtain more insight by ignoring the algebra entirely, and instead manipulating and observing the shape shown in the graph. This won't just give us a better feel for what's going on, it will also give us a proof*&lt;span class=&quot;marginnote&quot;&gt;*Strictly speaking, the visual approach I'm taking isn't what's traditionally thought of as a proof. But I believe the visual approach gives more insight into why the result is true than a traditional proof. And, of course, that kind of insight is the real purpose behind a proof. Occasionally, there will be small gaps in the reasoning I present: places where I make a visual argument that is plausible, but not quite rigorous. If this bothers you, then consider it a challenge to fill in the missing steps. But don't lose sight of the real purpose: to understand why the universality theorem is true.&lt;/span&gt; of universality that applies to activation functions other than the sigmoid function.&lt;/p&gt;
&lt;p&gt;To get started on this proof, try clicking on the bias, $b$, in the diagram above, and dragging to the right to increase it. You'll see that as the bias increases the graph moves to the left, but its shape doesn't change.&lt;/p&gt;
&lt;p&gt;Next, click and drag to the left in order to decrease the bias. You'll see that as the bias decreases the graph moves to the right, but, again, its shape doesn't change.&lt;/p&gt;
&lt;p&gt;Next, decrease the weight to around $2$ or $3$. You'll see that as you decrease the weight, the curve broadens out. You might need to change the bias as well, in order to keep the curve in-frame.&lt;/p&gt;
&lt;p&gt;Finally, increase the weight up past $w = 100$. As you do, the curve gets steeper, until eventually it begins to look like a step function. Try to adjust the bias so the step occurs near $x = 0.3$. The following short clip shows what your result should look like. Click on the play button to play (or replay) the video:&lt;/p&gt;

&lt;div&gt;
&lt;div id=&quot;a&quot; class=&quot;videoOverlay&quot; onclick=&quot;playVideo('a');&quot;&gt;&lt;img src=&quot;http://neuralnetworksanddeeplearning.com/images/play.png&quot; width=&quot;128px&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We can simplify our analysis quite a bit by increasing the weight so much that the output really is a step function, to a very good approximation. Below I've plotted the output from the top hidden neuron when the weight is $w = 999$. Note that this plot is static, and you can't change parameters such as the weight.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://neuralnetworksanddeeplearning.com/images/high_weight_function.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;It's actually quite a bit easier to work with step functions than general sigmoid functions. The reason is that in the output layer we add up contributions from all the hidden neurons. It's easy to analyze the sum of a bunch of step functions, but rather more difficult to reason about what happens when you add up a bunch of sigmoid shaped curves. And so it makes things much easier to assume that our hidden neurons are outputting step functions. More concretely, we do this by fixing the weight $w$ to be some very large value, and then setting the position of the step by modifying the bias. Of course, treating the output as a step function is an approximation, but it's a very good approximation, and for now we'll treat it as exact. I'll come back later to discuss the impact of deviations from this approximation.&lt;/p&gt;
&lt;p&gt;At what value of $x$ does the step occur? Put another way, how does the position of the step depend upon the weight and bias?&lt;/p&gt;
&lt;p&gt;To answer this question, try modifying the weight and bias in the diagram above (you may need to scroll back a bit). Can you figure out how the position of the step depends on $w$ and $b$? With a little work you should be able to convince yourself that the position of the step is &lt;em&gt;proportional&lt;/em&gt; to $b$, and &lt;em&gt;inversely proportional&lt;/em&gt; to $w$.&lt;/p&gt;
&lt;p&gt;In fact, the step is at position $s = -b/w$, as you can see by modifying the weight and bias in the following diagram:&lt;/p&gt;
&lt;p&gt;It will greatly simplify our lives to describe hidden neurons using just a single parameter, $s$, which is the step position, $s = -b/w$. Try modifying $s$ in the following diagram, in order to get used to the new parameterization:&lt;/p&gt;
&lt;p&gt;As noted above, we've implicitly set the weight $w$ on the input to be some large value - big enough that the step function is a very good approximation. We can easily convert a neuron parameterized in this way back into the conventional model, by choosing the bias $b = -w s$.&lt;/p&gt;
&lt;p&gt;Up to now we've been focusing on the output from just the top hidden neuron. Let's take a look at the behavior of the entire network. In particular, we'll suppose the hidden neurons are computing step functions parameterized by step points $s_1$ (top neuron) and $s_2$ (bottom neuron). And they'll have respective output weights $w_1$ and $w_2$. Here's the network:&lt;/p&gt;
&lt;p&gt;What's being plotted on the right is the &lt;em&gt;weighted output&lt;/em&gt; $w_1 a_1 + w_2 a_2$ from the hidden layer. Here, $a_1$ and $a_2$ are the outputs from the top and bottom hidden neurons, respectively*&lt;span class=&quot;marginnote&quot;&gt;*Note, by the way, that the output from the whole network is $\sigma(w_1 a_1+w_2 a_2 + b)$, where $b$ is the bias on the output neuron. Obviously, this isn't the same as the weighted output from the hidden layer, which is what we're plotting here. We're going to focus on the weighted output from the hidden layer right now, and only later will we think about how that relates to the output from the whole network.&lt;/span&gt;. These outputs are denoted with $a$s because they're often known as the neurons' &lt;em&gt;activations&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Try increasing and decreasing the step point $s_1$ of the top hidden neuron. Get a feel for how this changes the weighted output from the hidden layer. It's particularly worth understanding what happens when $s_1$ goes past $s_2$. You'll see that the graph changes shape when this happens, since we have moved from a situation where the top hidden neuron is the first to be activated to a situation where the bottom hidden neuron is the first to be activated.&lt;/p&gt;
&lt;p&gt;Similarly, try manipulating the step point $s_2$ of the bottom hidden neuron, and get a feel for how this changes the combined output from the hidden neurons.&lt;/p&gt;
&lt;p&gt;Try increasing and decreasing each of the output weights. Notice how this rescales the contribution from the respective hidden neurons. What happens when one of the weights is zero?&lt;/p&gt;
&lt;p&gt;Finally, try setting $w_1$ to be $0.8$ and $w_2$ to be $-0.8$. You get a &quot;bump&quot; function, which starts at point $s_1$, ends at point $s_2$, and has height $0.8$. For instance, the weighted output might look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://neuralnetworksanddeeplearning.com/images/bump_function.jpg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Of course, we can rescale the bump to have any height at all. Let's use a single parameter, $h$, to denote the height. To reduce clutter I'll also remove the &quot;$s_1 = \ldots$&quot; and &quot;$w_1 = \ldots$&quot; notations.&lt;/p&gt;
&lt;p&gt;Try changing the value of $h$ up and down, to see how the height of the bump changes. Try changing the height so it's negative, and observe what happens. And try changing the step points to see how that changes the shape of the bump.&lt;/p&gt;
&lt;p&gt;You'll notice, by the way, that we're using our neurons in a way that can be thought of not just in graphical terms, but in more conventional programming terms, as a kind of &lt;tt&gt;if-then-else&lt;/tt&gt; statement, e.g.:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; input &amp;gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; step point:
        add &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; to the weighted output
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;:
        add &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; to the weighted output
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the most part I'm going to stick with the graphical point of view. But in what follows you may sometimes find it helpful to switch points of view, and think about things in terms of &lt;tt&gt;if-then-else&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;We can use our bump-making trick to get two bumps, by gluing two pairs of hidden neurons together into the same network:&lt;/p&gt;
&lt;p&gt;I've suppressed the weights here, simply writing the $h$ values for each pair of hidden neurons. Try increasing and decreasing both $h$ values, and observe how it changes the graph. Move the bumps around by changing the step points.&lt;/p&gt;
&lt;p&gt;More generally, we can use this idea to get as many peaks as we want, of any height. In particular, we can divide the interval $[0, 1]$ up into a large number, $N$, of subintervals, and use $N$ pairs of hidden neurons to set up peaks of any desired height. Let's see how this works for $N = 5$. That's quite a few neurons, so I'm going to pack things in a bit. Apologies for the complexity of the diagram: I could hide the complexity by abstracting away further, but I think it's worth putting up with a little complexity, for the sake of getting a more concrete feel for how these networks work.&lt;/p&gt;
&lt;p&gt;You can see that there are five pairs of hidden neurons. The step points for the respective pairs of neurons are $0, 1/5$, then $1/5, 2/5$, and so on, out to $4/5, 5/5$. These values are fixed - they make it so we get five evenly spaced bumps on the graph.&lt;/p&gt;
&lt;p&gt;Each pair of neurons has a value of $h$ associated to it. Remember, the connections output from the neurons have weights $h$ and $-h$ (not marked). Click on one of the $h$ values, and drag the mouse to the right or left to change the value. As you do so, watch the function change. By changing the output weights we're actually &lt;em&gt;designing&lt;/em&gt; the function!&lt;/p&gt;
&lt;p&gt;Contrariwise, try clicking on the graph, and dragging up or down to change the height of any of the bump functions. As you change the heights, you can see the corresponding change in $h$ values. And, although it's not shown, there is also a change in the corresponding output weights, which are $+h$ and $-h$.&lt;/p&gt;
&lt;p&gt;In other words, we can directly manipulate the function appearing in the graph on the right, and see that reflected in the $h$ values on the left. A fun thing to do is to hold the mouse button down and drag the mouse from one side of the graph to the other. As you do this you draw out a function, and get to watch the parameters in the neural network adapt.&lt;/p&gt;
&lt;p&gt;Time for a challenge.&lt;/p&gt;
&lt;p&gt;Let's think back to the function I plotted at the beginning of the chapter:&lt;/p&gt;
&lt;p&gt;I didn't say it at the time, but what I plotted is actually the function \begin{eqnarray} f(x) = 0.2+0.4 x^2+0.3x \sin(15 x) + 0.05 \cos(50 x), \tag{113}\end{eqnarray} plotted over $x$ from $0$ to $1$, and with the $y$ axis taking values from $0$ to $1$.&lt;/p&gt;
&lt;p&gt;That's obviously not a trivial function.&lt;/p&gt;
&lt;p&gt;You're going to figure out how to compute it using a neural network.&lt;/p&gt;
&lt;p&gt;In our networks above we've been analyzing the weighted combination $\sum_j w_j a_j$ output from the hidden neurons. We now know how to get a lot of control over this quantity. But, as I noted earlier, this quantity is not what's output from the network. What's output from the network is $\sigma(\sum_j w_j a_j + b)$ where $b$ is the bias on the output neuron. Is there some way we can achieve control over the actual output from the network?&lt;/p&gt;
&lt;p&gt;The solution is to design a neural network whose hidden layer has a weighted output given by $\sigma^{-1} \circ f(x)$, where $\sigma^{-1}$ is just the inverse of the $\sigma$ function. That is, we want the weighted output from the hidden layer to be:&lt;/p&gt;
&lt;p&gt;If we can do this, then the output from the network as a whole will be a good approximation to $f(x)$*&lt;span class=&quot;marginnote&quot;&gt;*Note that I have set the bias on the output neuron to $0$.&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Your challenge, then, is to design a neural network to approximate the goal function shown just above. To learn as much as possible, I want you to solve the problem twice. The first time, please click on the graph, directly adjusting the heights of the different bump functions. You should find it fairly easy to get a good match to the goal function. How well you're doing is measured by the &lt;em&gt;average deviation&lt;/em&gt; between the goal function and the function the network is actually computing. Your challenge is to drive the average deviation as &lt;em&gt;low&lt;/em&gt; as possible. You complete the challenge when you drive the average deviation to $0.40$ or below.&lt;/p&gt;
&lt;p&gt;Once you've done that, click on &quot;Reset&quot; to randomly re-initialize the bumps. The second time you solve the problem, resist the urge to click on the graph. Instead, modify the $h$ values on the left-hand side, and again attempt to drive the average deviation to $0.40$ or below.&lt;/p&gt;
&lt;p&gt;You've now figured out all the elements necessary for the network to approximately compute the function $f(x)$! It's only a coarse approximation, but we could easily do much better, merely by increasing the number of pairs of hidden neurons, allowing more bumps.&lt;/p&gt;
&lt;p&gt;In particular, it's easy to convert all the data we have found back into the standard parameterization used for neural networks. Let me just recap quickly how that works.&lt;/p&gt;
&lt;p&gt;The first layer of weights all have some large, constant value, say $w = 1000$.&lt;/p&gt;
&lt;p&gt;The biases on the hidden neurons are just $b = -w s$. So, for instance, for the second hidden neuron $s = 0.2$ becomes $b = -1000 \times 0.2 = -200$.&lt;/p&gt;
&lt;p&gt;The final layer of weights are determined by the $h$ values. So, for instance, the value you've chosen above for the first $h$, $h = $ &lt;span id=&quot;h&quot;/&gt;, means that the output weights from the top two hidden neurons are &lt;span id=&quot;w1&quot;/&gt; and &lt;span id=&quot;w2&quot;/&gt;, respectively. And so on, for the entire layer of output weights.&lt;/p&gt;
&lt;p&gt;Finally, the bias on the output neuron is $0$.&lt;/p&gt;
&lt;p&gt;That's everything: we now have a complete description of a neural network which does a pretty good job computing our original goal function. And we understand how to improve the quality of the approximation by improving the number of hidden neurons.&lt;/p&gt;
&lt;p&gt;What's more, there was nothing special about our original goal function, $f(x) = 0.2+0.4 x^2+0.3 \sin(15 x) + 0.05 \cos(50 x)$. We could have used this procedure for any continuous function from $[0, 1]$ to $[0, 1]$. In essence, we're using our single-layer neural networks to build a lookup table for the function. And we'll be able to build on this idea to provide a general proof of universality.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#many_input_variables&quot;&gt;Many input variables&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Let's extend our results to the case of many input variables. This sounds complicated, but all the ideas we need can be understood in the case of just two inputs. So let's address the two-input case.&lt;/p&gt;
&lt;p&gt;We'll start by considering what happens when we have two inputs to a neuron:&lt;/p&gt;
&lt;p&gt;Here, we have inputs $x$ and $y$, with corresponding weights $w_1$ and $w_2$, and a bias $b$ on the neuron. Let's set the weight $w_2$ to $0$, and then play around with the first weight, $w_1$, and the bias, $b$, to see how they affect the output from the neuron:&lt;/p&gt;


&lt;p&gt;As you can see, with $w_2 = 0$ the input $y$ makes no difference to the output from the neuron. It's as though $x$ is the only input.&lt;/p&gt;
&lt;p&gt;Given this, what do you think happens when we increase the weight $w_1$ to $w_1 = 100$, with $w_2$ remaining $0$? If you don't immediately see the answer, ponder the question for a bit, and see if you can figure out what happens. Then try it out and see if you're right. I've shown what happens in the following movie:&lt;/p&gt;
&lt;div&gt;
&lt;div id=&quot;b&quot; class=&quot;videoOverlay&quot; onclick=&quot;playVideo('b');&quot;&gt;&lt;img src=&quot;http://neuralnetworksanddeeplearning.com/images/play.png&quot; width=&quot;128px&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Just as in our earlier discussion, as the input weight gets larger the output approaches a step function. The difference is that now the step function is in three dimensions. Also as before, we can move the location of the step point around by modifying the bias. The actual location of the step point is $s_x \equiv -b / w_1$.&lt;/p&gt;
&lt;p&gt;Let's redo the above using the position of the step as the parameter:&lt;/p&gt;

&lt;p&gt;Here, we assume the weight on the $x$ input has some large value - I've used $w_1 = 1000$ - and the weight $w_2 = 0$. The number on the neuron is the step point, and the little $x$ above the number reminds us that the step is in the $x$ direction. Of course, it's also possible to get a step function in the $y$ direction, by making the weight on the $y$ input very large (say, $w_2 = 1000$), and the weight on the $x$ equal to $0$, i.e., $w_1 = 0$:&lt;/p&gt;

&lt;p&gt;The number on the neuron is again the step point, and in this case the little $y$ above the number reminds us that the step is in the $y$ direction. I could have explicitly marked the weights on the $x$ and $y$ inputs, but decided not to, since it would make the diagram rather cluttered. But do keep in mind that the little $y$ marker implicitly tells us that the $y$ weight is large, and the $x$ weight is $0$.&lt;/p&gt;
&lt;p&gt;We can use the step functions we've just constructed to compute a three-dimensional bump function. To do this, we use two neurons, each computing a step function in the $x$ direction. Then we combine those step functions with weight $h$ and $-h$, respectively, where $h$ is the desired height of the bump. It's all illustrated in the following diagram:&lt;/p&gt;

&lt;p&gt;Try changing the value of the height, $h$. Observe how it relates to the weights in the network. And see how it changes the height of the bump function on the right.&lt;/p&gt;
&lt;p&gt;Also, try changing the step point $0.30$ associated to the top hidden neuron. Witness how it changes the shape of the bump. What happens when you move it past the step point $0.70$ associated to the bottom hidden neuron?&lt;/p&gt;
&lt;p&gt;We've figured out how to make a bump function in the $x$ direction. Of course, we can easily make a bump function in the $y$ direction, by using two step functions in the $y$ direction. Recall that we do this by making the weight large on the $y$ input, and the weight $0$ on the $x$ input. Here's the result:&lt;/p&gt;

&lt;p&gt;This looks nearly identical to the earlier network! The only thing explicitly shown as changing is that there's now little $y$ markers on our hidden neurons. That reminds us that they're producing $y$ step functions, not $x$ step functions, and so the weight is very large on the $y$ input, and zero on the $x$ input, not vice versa. As before, I decided not to show this explicitly, in order to avoid clutter.&lt;/p&gt;
&lt;p&gt;Let's consider what happens when we add up two bump functions, one in the $x$ direction, the other in the $y$ direction, both of height $h$:&lt;/p&gt;

&lt;p&gt;To simplify the diagram I've dropped the connections with zero weight. For now, I've left in the little $x$ and $y$ markers on the hidden neurons, to remind you in what directions the bump functions are being computed. We'll drop even those markers later, since they're implied by the input variable.&lt;/p&gt;
&lt;p&gt;Try varying the parameter $h$. As you can see, this causes the output weights to change, and also the heights of both the $x$ and $y$ bump functions.&lt;/p&gt;
&lt;p&gt;What we've built looks a little like a &lt;em&gt;tower&lt;/em&gt; function:&lt;/p&gt;
&lt;center&gt;&lt;span id=&quot;tower&quot;/&gt;

&lt;/center&gt;
&lt;p&gt;If we could build such tower functions, then we could use them to approximate arbitrary functions, just by adding up many towers of different heights, and in different locations:&lt;/p&gt;
&lt;center&gt;&lt;span id=&quot;many_towers&quot;/&gt;

&lt;/center&gt;
&lt;p&gt;Of course, we haven't yet figured out how to build a tower function. What we have constructed looks like a central tower, of height $2h$, with a surrounding plateau, of height $h$.&lt;/p&gt;
&lt;p&gt;But we can make a tower function. Remember that earlier we saw neurons can be used to implement a type of &lt;tt&gt;if-then-else&lt;/tt&gt; statement:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; input &amp;gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; threshold: 
        output 1
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;:
        output 0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That was for a neuron with just a single input. What we want is to apply a similar idea to the combined output from the hidden neurons:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; combined output from hidden neurons &amp;gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; threshold:
        output 1
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;:
        output 0
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If we choose the &lt;tt&gt;threshold&lt;/tt&gt; appropriately - say, a value of $3h/2$, which is sandwiched between the height of the plateau and the height of the central tower - we could squash the plateau down to zero, and leave just the tower standing.&lt;/p&gt;
&lt;p&gt;Can you see how to do this? Try experimenting with the following network to figure it out. Note that we're now plotting the output from the entire network, not just the weighted output from the hidden layer. This means we add a bias term to the weighted output from the hidden layer, and apply the sigma function. Can you find values for $h$ and $b$ which produce a tower? This is a bit tricky, so if you think about this for a while and remain stuck, here's two hints: (1) To get the output neuron to show the right kind of &lt;tt&gt;if-then-else&lt;/tt&gt; behaviour, we need the input weights (all $h$ or $-h$) to be large; and (2) the value of $b$ determines the scale of the &lt;tt&gt;if-then-else&lt;/tt&gt; threshold.&lt;/p&gt;

&lt;p&gt;With our initial parameters, the output looks like a flattened version of the earlier diagram, with its tower and plateau. To get the desired behaviour, we increase the parameter $h$ until it becomes large. That gives the &lt;tt&gt;if-then-else&lt;/tt&gt; thresholding behaviour. Second, to get the threshold right, we'll choose $b \approx -3h/2$. Try it, and see how it works!&lt;/p&gt;
&lt;p&gt;Here's what it looks like, when we use $h = 10$:&lt;/p&gt;
&lt;div&gt;
&lt;div id=&quot;c&quot; class=&quot;videoOverlay&quot; onclick=&quot;playVideo('c');&quot;&gt;&lt;img src=&quot;http://neuralnetworksanddeeplearning.com/images/play.png&quot; width=&quot;128px&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Even for this relatively modest value of $h$, we get a pretty good tower function. And, of course, we can make it as good as we want by increasing $h$ still further, and keeping the bias as $b = -3h/2$.&lt;/p&gt;
&lt;p&gt;Let's try gluing two such networks together, in order to compute two different tower functions. To make the respective roles of the two sub-networks clear I've put them in separate boxes, below: each box computes a tower function, using the technique described above. The graph on the right shows the weighted output from the &lt;em&gt;second&lt;/em&gt; hidden layer, that is, it's a weighted combination of tower functions.&lt;/p&gt;

&lt;p&gt;In particular, you can see that by modifying the weights in the final layer you can change the height of the output towers.&lt;/p&gt;
&lt;p&gt;The same idea can be used to compute as many towers as we like. We can also make them as thin as we like, and whatever height we like. As a result, we can ensure that the weighted output from the second hidden layer approximates any desired function of two variables:&lt;/p&gt;
&lt;center&gt;&lt;span id=&quot;many_towers_2&quot;/&gt;

&lt;/center&gt;
&lt;p&gt;In particular, by making the weighted output from the second hidden layer a good approximation to $\sigma^{-1} \circ f$, we ensure the output from our network will be a good approximation to any desired function, $f$.&lt;/p&gt;
&lt;p&gt;What about functions of more than two variables?&lt;/p&gt;
&lt;p&gt;Let's try three variables $x_1, x_2, x_3$. The following network can be used to compute a tower function in four dimensions:&lt;/p&gt;
&lt;p&gt;Here, the $x_1, x_2, x_3$ denote inputs to the network. The $s_1, t_1$ and so on are step points for neurons - that is, all the weights in the first layer are large, and the biases are set to give the step points $s_1, t_1, s_2, \ldots$. The weights in the second layer alternate $+h, -h$, where $h$ is some very large number. And the output bias is $-5h/2$.&lt;/p&gt;
&lt;p&gt;This network computes a function which is $1$ provided three conditions are met: $x_1$ is between $s_1$ and $t_1$; $x_2$ is between $s_2$ and $t_2$; and $x_3$ is between $s_3$ and $t_3$. The network is $0$ everywhere else. That is, it's a kind of tower which is $1$ in a little region of input space, and $0$ everywhere else.&lt;/p&gt;
&lt;p&gt;By gluing together many such networks we can get as many towers as we want, and so approximate an arbitrary function of three variables. Exactly the same idea works in $m$ dimensions. The only change needed is to make the output bias $(-m+1/2)h$, in order to get the right kind of sandwiching behavior to level the plateau.&lt;/p&gt;
&lt;p&gt;Okay, so we now know how to use neural networks to approximate a real-valued function of many variables. What about vector-valued functions $f(x_1, \ldots, x_m) \in R^n$? Of course, such a function can be regarded as just $n$ separate real-valued functions, $f^1(x_1, \ldots, x_m), f^2(x_1, \ldots, x_m)$, and so on. So we create a network approximating $f^1$, another network for $f^2$, and so on. And then we simply glue all the networks together. So that's also easy to cope with.&lt;/p&gt;
&lt;h4&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#problem_863961&quot;&gt;Problem&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;We've seen how to use networks with two hidden layers to approximate an arbitrary function. Can you find a proof showing that it's possible with just a single hidden layer? As a hint, try working in the case of just two input variables, and showing that: (a) it's possible to get step functions not just in the $x$ or $y$ directions, but in an arbitrary direction; (b) by adding up many of the constructions from part (a) it's possible to approximate a tower function which is circular in shape, rather than rectangular; (c) using these circular towers, it's possible to approximate an arbitrary function. To do part (c) it may help to use ideas from a &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#fixing_up_the_step_functions&quot;&gt;bit later in this chapter&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#extension_beyond_sigmoid_neurons&quot;&gt;Extension beyond sigmoid neurons&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We've proved that networks made up of sigmoid neurons can compute any function. Recall that in a sigmoid neuron the inputs $x_1, x_2, \ldots$ result in the output $\sigma(\sum_j w_j x_j + b)$, where $w_j$ are the weights, $b$ is the bias, and $\sigma$ is the sigmoid function:&lt;/p&gt;
&lt;p&gt;What if we consider a different type of neuron, one using some other activation function, $s(z)$:&lt;/p&gt;
&lt;p&gt;That is, we'll assume that if our neurons has inputs $x_1, x_2, \ldots$, weights $w_1, w_2, \ldots$ and bias $b$, then the output is $s(\sum_j w_j x_j + b)$.&lt;/p&gt;
&lt;p&gt;We can use this activation function to get a step function, just as we did with the sigmoid. Try ramping up the weight in the following, say to $w = 100$:&lt;/p&gt;
&lt;p&gt;Just as with the sigmoid, this causes the activation function to contract, and ultimately it becomes a very good approximation to a step function. Try changing the bias, and you'll see that we can set the position of the step to be wherever we choose. And so we can use all the same tricks as before to compute any desired function.&lt;/p&gt;
&lt;p&gt;What properties does $s(z)$ need to satisfy in order for this to work? We do need to assume that $s(z)$ is well-defined as $z \rightarrow -\infty$ and $z \rightarrow \infty$. These two limits are the two values taken on by our step function. We also need to assume that these limits are different from one another. If they weren't, there'd be no step, simply a flat graph! But provided the activation function $s(z)$ satisfies these properties, neurons based on such an activation function are universal for computation.&lt;/p&gt;
&lt;h4&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#problems_963556&quot;&gt;Problems&lt;/a&gt;&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;Earlier in the book we met another type of neuron known as a &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap3.html#other_models_of_artificial_neuron&quot;&gt;rectified linear unit&lt;/a&gt;. Explain why such neurons don't satisfy the conditions just given for universality. Find a proof of universality showing that rectified linear units are universal for computation.&lt;/li&gt;
&lt;li&gt;Suppose we consider linear neurons, i.e., neurons with the activation function $s(z) = z$. Explain why linear neurons don't satisfy the conditions just given for universality. Show that such neurons can't be used to do universal computation.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#fixing_up_the_step_functions&quot;&gt;Fixing up the step functions&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Up to now, we've been assuming that our neurons can produce step functions exactly. That's a pretty good approximation, but it is only an approximation. In fact, there will be a narrow window of failure, illustrated in the following graph, in which the function behaves very differently from a step function:&lt;/p&gt;
&lt;p&gt;In these windows of failure the explanation I've given for universality will fail.&lt;/p&gt;
&lt;p&gt;Now, it's not a terrible failure. By making the weights input to the neurons big enough we can make these windows of failure as small as we like. Certainly, we can make the window much narrower than I've shown above - narrower, indeed, than our eye could see. So perhaps we might not worry too much about this problem.&lt;/p&gt;
&lt;p&gt;Nonetheless, it'd be nice to have some way of addressing the problem.&lt;/p&gt;
&lt;p&gt;In fact, the problem turns out to be easy to fix. Let's look at the fix for neural networks computing functions with just one input and one output. The same ideas work also to address the problem when there are more inputs and outputs.&lt;/p&gt;
&lt;p&gt;In particular, suppose we want our network to compute some function, $f$. As before, we do this by trying to design our network so that the weighted output from our hidden layer of neurons is $\sigma^{-1} \circ f(x)$:&lt;/p&gt;
&lt;p&gt;If we were to do this using the technique described earlier, we'd use the hidden neurons to produce a sequence of bump functions:&lt;/p&gt;
&lt;p&gt;Again, I've exaggerated the size of the windows of failure, in order to make them easier to see. It should be pretty clear that if we add all these bump functions up we'll end up with a reasonable approximation to $\sigma^{-1} \circ f(x)$, except within the windows of failure.&lt;/p&gt;
&lt;p&gt;Suppose that instead of using the approximation just described, we use a set of hidden neurons to compute an approximation to &lt;em&gt;half&lt;/em&gt; our original goal function, i.e., to $\sigma^{-1} \circ f(x) / 2$. Of course, this looks just like a scaled down version of the last graph:&lt;/p&gt;
&lt;p&gt;And suppose we use another set of hidden neurons to compute an approximation to $\sigma^{-1} \circ f(x)/ 2$, but with the bases of the bumps &lt;em&gt;shifted&lt;/em&gt; by half the width of a bump:&lt;/p&gt;
&lt;p&gt;Now we have two different approximations to $\sigma^{-1} \circ f(x) / 2$. If we add up the two approximations we'll get an overall approximation to $\sigma^{-1} \circ f(x)$. That overall approximation will still have failures in small windows. But the problem will be much less than before. The reason is that points in a failure window for one approximation won't be in a failure window for the other. And so the approximation will be a factor roughly $2$ better in those windows.&lt;/p&gt;
&lt;p&gt;We could do even better by adding up a large number, $M$, of overlapping approximations to the function $\sigma^{-1} \circ f(x) / M$. Provided the windows of failure are narrow enough, a point will only ever be in one window of failure. And provided we're using a large enough number $M$ of overlapping approximations, the result will be an excellent overall approximation.&lt;/p&gt;
&lt;h3&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html#conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The explanation for universality we've discussed is certainly not a practical prescription for how to compute using neural networks! In this, it's much like proofs of universality for &lt;tt&gt;NAND&lt;/tt&gt; gates and the like. For this reason, I've focused mostly on trying to make the construction clear and easy to follow, and not on optimizing the details of the construction. However, you may find it a fun and instructive exercise to see if you can improve the construction.&lt;/p&gt;
&lt;p&gt;Although the result isn't directly useful in constructing networks, it's important because it takes off the table the question of whether any particular function is computable using a neural network. The answer to that question is always &quot;yes&quot;. So the right question to ask is not whether any particular function is computable, but rather what's a &lt;em&gt;good&lt;/em&gt; way to compute the function.&lt;/p&gt;
&lt;p&gt;The universality construction we've developed uses just two hidden layers to compute an arbitrary function. Furthermore, as we've discussed, it's possible to get the same result with just a single hidden layer. Given this, you might wonder why we would ever be interested in deep networks, i.e., networks with many hidden layers. Can't we simply replace those networks with shallow, single hidden layer networks?&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;strong&gt;Chapter acknowledgments:&lt;/strong&gt; Thanks to &lt;a href=&quot;http://jendodd.com&quot;&gt;Jen Dodd&lt;/a&gt; and &lt;a href=&quot;http://colah.github.io/about.html&quot;&gt;Chris Olah&lt;/a&gt; for many discussions about universality in neural networks. My thanks, in particular, to Chris for suggesting the use of a lookup table to prove universality. The interactive visual form of the chapter is inspired by the work of people such as &lt;a href=&quot;http://bost.ocks.org/mike/algorithms/&quot;&gt;Mike Bostock&lt;/a&gt;, &lt;a href=&quot;http://www-cs-students.stanford.edu/~amitp/&quot;&gt;Amit Patel&lt;/a&gt;, &lt;a href=&quot;http://worrydream.com&quot;&gt;Bret Victor&lt;/a&gt;, and &lt;a href=&quot;http://acko.net/&quot;&gt;Steven Wittens&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While in principle that's possible, there are good practical reasons to use deep networks. As argued in &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html#toward_deep_learning&quot;&gt;Chapter 1&lt;/a&gt;, deep networks have a hierarchical structure which makes them particularly well adapted to learn the hierarchies of knowledge that seem to be useful in solving real-world problems. Put more concretely, when attacking problems such as image recognition, it helps to use a system that understands not just individual pixels, but also increasingly more complex concepts: from edges to simple geometric shapes, all the way up through complex, multi-object scenes. In later chapters, we'll see evidence suggesting that deep networks do a better job than shallow networks at learning such hierarchies of knowledge. To sum up: universality tells us that neural networks can compute any function; and empirical evidence suggests that deep networks are the networks best adapted to learn the functions useful in solving many real-world problems.&lt;/p&gt;
&lt;p&gt; &lt;span&gt;.&lt;/span&gt; &lt;span&gt;.&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 20 Apr 2019 20:10:07 +0000</pubDate>
<dc:creator>antman</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://neuralnetworksanddeeplearning.com/chap4.html</dc:identifier>
</item>
<item>
<title>We Don&amp;#039;t Have a Talent Shortage. We Have a Sucker Shortage</title>
<link>https://resumeskills.us/talent/shortage</link>
<guid isPermaLink="true" >https://resumeskills.us/talent/shortage</guid>
<description>&lt;header&gt;
&lt;/header&gt;&lt;p&gt;I just started the second real &quot;motivated&quot; job search in my 22 years. The first doesn't really count, since it was during the depths of the 2009 recession (when everyone was out of work) and I've been continuously employed outside of that.&lt;/p&gt;
&lt;p&gt;Like any other business blog reader, I've seen plenty of articles about the oncoming &quot;talent shortage&quot;. My sympathies were with HR. It truly sounded like we had a real problem.&lt;/p&gt;
&lt;p&gt;Well, &lt;strong&gt;no mas!&lt;/strong&gt; After 30 days, I'm convinced this problem is entirely created by HR.&lt;/p&gt;
&lt;p&gt;Go sell crazy somewhere else. We're all stocked up here.&lt;/p&gt;
&lt;h3&gt;Why Are You Reposting Jobs With Over 1000 applications?&lt;/h3&gt;
&lt;p&gt;Let's start with my pet peeve. Employers continuously reposting positions with hundreds or thousands of responses. LinkedIn has a lovely competitive intelligence service, available to paying customers, which tells us a little bit about who else has applied. And as any good sales person knows, nothing beats a little knowledge when you're trying to close a deal. At which point, we can take a look at this little gem:&lt;/p&gt;
Reposting After 1000 Resumes? Seriously?
&lt;p&gt;Reposted for at least the 4th time in as many months. We're not talking about a CEO gig here either - once you peel back the fluff, this is a general manager role for a startup offering in digital services, with a few layers of bosses above them. In Atlanta, not New York. It's a nice role (with P&amp;amp;L) but still we're talking about an early stage business sitting in the middle of the startup hub for the Southeast. There are easily several thousand people in the town who are capable of building that business. Get over yourselves and just freakin hire someone. You're running a website, not curing cancer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Somewhere along the line, the &quot;recruiting industry&quot; forgot their job was to get butts in seats, doing actual work.&lt;/strong&gt;&lt;/p&gt;
&lt;h3&gt;Narrow / Irrelevant Experience Requirements&lt;/h3&gt;
&lt;center readability=&quot;1&quot;&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong&gt;&quot;Experience with Home Depot’s Vendor Portal required&quot;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/center&gt;
&lt;p&gt;Posted by a national recruiting firm, no less. So let me get this straight. You're going to toss a resume in the trash because they haven't been keypunching data into a particular retailer's portal? For a &lt;strong&gt;Sales Director&lt;/strong&gt; Job?&lt;/p&gt;
&lt;p&gt;Are you people &lt;strong&gt;complete morons&lt;/strong&gt;?&lt;/p&gt;
&lt;p&gt;I spent the past year partnered up with the exact kind of person that you need to hire. He led the sales team. When it came to getting new items slotted at a retailer, he could make it rain. I can make an educated guess as to his paycheck: &quot;A lot&quot;. Enough that it would be an absolutely stupid waste of time to lock him away in an office entering data into some portal. We had (very talented) admins that handled that and the arrangement worked out nicely for everyone involved. We got orders, they entered them, prosperity ensued.&lt;/p&gt;
&lt;p&gt;Gee, I guess you're not going to be able to hire him. Pity, he built a $40 MM business.&lt;/p&gt;
&lt;p&gt;So if you're seriously disqualifying candidates for this, I'm beyond words.&lt;/p&gt;
&lt;h3&gt;Super Narrow Selection Criteria&lt;/h3&gt;
&lt;p&gt;So apparently you won't be considered for a senior position unless you've worked in almost &lt;strong&gt;exactly&lt;/strong&gt; that role in a company that was almost &lt;strong&gt;exactly&lt;/strong&gt; the same at almost &lt;strong&gt;exactly&lt;/strong&gt; the same career level.&lt;/p&gt;
&lt;p&gt;That's not whining, just looking at who bothers to reply back to the 100 resumes I sent.&lt;/p&gt;
&lt;p&gt;Which is ridiculous, because certain skills are either universal or can be applied within a very broad space. I happen to know quite a bit about analyzing manufacturer and distributor profitability. You may safely assume that while most of my experience happens to deal with packaging and janitorial supplies, I can probably figure out your welding supplies business without much effort. Sell product, put on truck, ship it. Same process, same math.&lt;/p&gt;
&lt;p&gt;Same thing with manufacturing leadership. We've got this massive building with huge machines and tons of people running around. Here are the detailed manuals on the equipment if we need to tweak a specific machine. Most of the rest of the job is pretty universal. Just keep reminding people to keep their fingers away from moving parts, ok?&lt;/p&gt;
&lt;p&gt;Aside from making it difficult to change industries or functions, there's another huge blind-spot in this approach to assessing candidates. It's very easy to fake knowledge of a line of business at your company that you didn't personally work on, especially if you're part of a corporate function that crossed multiple areas. Take a few people in that business unit to lunch, take good notes, and you can rattle off factoids like any good management consultant. Most recruiters are utterly incapable of distinguishing this from actual work experience.&lt;/p&gt;
&lt;p&gt;Seriously. Ask me about my deep expertise in selling printing paper. I dare you.&lt;/p&gt;
&lt;h3&gt;Outright Age &amp;amp; Lifestyle Discrimination&lt;/h3&gt;

&lt;p&gt;Every good movies needs a comedy scene. I nominate the &quot;office&quot; interview for this role.&lt;/p&gt;
&lt;p&gt;I'm 45 and have gray hair. That does tend to happen when you spend a few years raising teenagers and attempting to fix extremely messed up companies. It apparently indicates I am completely ignorant about technology and social media.&lt;/p&gt;
&lt;p&gt;Never mind that billion dollar public companies have used code I've written in the past three years to make legally binding statements to their shareholders. As in &quot;go to jail if you get this wrong&quot; stuff. Silly executives. They shouldn't have used an old guy to write that code.&lt;/p&gt;
&lt;p&gt;But the real prizewinner comes when we speak about moving on from a remote working arrangement. &quot;Are you willing to commute into the office after working remotely?&quot;&lt;/p&gt;
&lt;p&gt;What kind of question is that? The job is an office job and I applied. Why is this an issue?&lt;/p&gt;
&lt;h3&gt;What Are Your Salary Requirements?&lt;/h3&gt;
&lt;p&gt;So let me get this straight. You want me to bootstrap a multi-million dollar business, from scratch, operating with minimal marketing and operating funding in a highly competitive channel. Or get your consumer product slotted at a major retailer. Or launch your new web application. For $100,000 a year and no equity.&lt;/p&gt;
&lt;p&gt;No, seriously - what are you smoking? Because I want some of it.&lt;/p&gt;
&lt;p&gt;Let's be clear - all of those tasks are achievable. And will generate millions of dollars in equity for your company. And I'm pretty sure you're going to fire me within a year or two if this doesn't work out, so this isn't a low risk deal.&lt;/p&gt;
&lt;p&gt;What you're missing is that while you may &lt;strong&gt;need&lt;/strong&gt; someone like me to get this done, people like me don't necessarily &lt;strong&gt;need&lt;/strong&gt; you to create that kind of value. I can outsource or import product. I can hire programmers. In the larger scheme of things $50,000 for a marketing budget isn't a lot of money. Financing is out there. I may not want to assume the risk of startup business right now but if you're going to offer the deal on those terms...&lt;/p&gt;
&lt;p&gt;Basically you want me create millions of dollars in value, get paid $100 grand for my troubles, and get stuck holding the bag if things don't work out. Instead of going &quot;all-in&quot; and taking the entire multi-million dollar pile of equity at a similar level of personal risk. You're not recruiting geniuses, you're recruiting suckers.&lt;/p&gt;
&lt;p&gt;Stop wasting people's time and make experienced candidates a real offer.&lt;/p&gt;
&lt;h3&gt;No Kidding People Don't Get Hired&lt;/h3&gt;
&lt;p&gt;I mean seriously, it's probably faster to just try to go sell your company something than actually get a job there. Probably more profitable too, while we're talking about it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;To heck with Human Resources. Anyone got the number for purchasing?&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 20 Apr 2019 16:56:04 +0000</pubDate>
<dc:creator>sixtypoundhound</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://resumeskills.us/talent/shortage</dc:identifier>
</item>
<item>
<title>Joe Armstrong has died</title>
<link>https://twitter.com/FrancescoC/status/1119596234166218754</link>
<guid isPermaLink="true" >https://twitter.com/FrancescoC/status/1119596234166218754</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://twitter.com/FrancescoC/status/1119596234166218754&quot;&gt;https://twitter.com/FrancescoC/status/1119596234166218754&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19706514&quot;&gt;https://news.ycombinator.com/item?id=19706514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 1837&lt;/p&gt;
&lt;p&gt;# Comments: 179&lt;/p&gt;
</description>
<pubDate>Sat, 20 Apr 2019 13:50:28 +0000</pubDate>
<dc:creator>okket</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://mobile.twitter.com/FrancescoC/status/1119596234166218754</dc:identifier>
</item>
<item>
<title>Vendors must start adding physical on/off switches to devices that can spy on us</title>
<link>https://larrysanger.org/2019/04/vendors-must-start-adding-physical-on-off-switches-to-devices-that-can-spy-on-us/</link>
<guid isPermaLink="true" >https://larrysanger.org/2019/04/vendors-must-start-adding-physical-on-off-switches-to-devices-that-can-spy-on-us/</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://larrysanger.org/2019/04/vendors-must-start-adding-physical-on-off-switches-to-devices-that-can-spy-on-us/&quot;&gt;https://larrysanger.org/2019/04/vendors-must-start-adding-physical-on-off-switches-to-devices-that-can-spy-on-us/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19705741&quot;&gt;https://news.ycombinator.com/item?id=19705741&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 416&lt;/p&gt;
&lt;p&gt;# Comments: 177&lt;/p&gt;
</description>
<pubDate>Sat, 20 Apr 2019 10:27:06 +0000</pubDate>
<dc:creator>jrepinc</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://mobile.twitter.com/FrancescoC/status/1119596234166218754</dc:identifier>
</item>
<item>
<title>Show HN: I Built Darwin Mail as a Google Inbox Replacement</title>
<link>https://www.darwinmail.app</link>
<guid isPermaLink="true" >https://www.darwinmail.app</guid>
<description>&lt;p&gt;So what happens exactly is:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The user logs into DarwinMail.app,&lt;/li&gt;
&lt;li&gt;DarwinMail makes a login request to Google's servers,&lt;/li&gt;
&lt;li&gt;Google logs the user in,&lt;/li&gt;
&lt;li&gt;DarwinMail asks for the users emails at which point this data is rendered in the user's browser.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;None of this email data is stored on DarwinMail's servers. There is no need! Think of all the space it would take up and the time that would be spent retrieving the data.&lt;/p&gt;
&lt;p&gt;Google's API &amp;amp; servers do the heavy lifting. DarwinMail allows you to view your emails just like Inbox, and hopefully provides (or will soon provide) the same kind of functionality :)&lt;/p&gt;
&lt;h4&gt;How does DarwinMail technically work?&lt;/h4&gt;
&lt;p&gt;The follwing is taken straight from Google's documentation, describing the exact process.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://developers.google.com/identity/protocols/OAuth2UserAgent&quot;&gt;This document&lt;/a&gt; explains how to implement OAuth 2.0 authorization to access Google APIs from a JavaScript web application. OAuth 2.0 allows users to share specific data with an application while keeping their usernames, passwords, and other information private. For example, an application can use OAuth 2.0 to obtain permission from users to store files in their Google Drives.&lt;/p&gt;
&lt;p&gt;This OAuth 2.0 flow is called the implicit grant flow. It is designed for applications that access APIs only while the user is present at the application. These applications are not able to store confidential information.&lt;/p&gt;
&lt;p&gt;In this flow, your app opens a Google URL that uses query parameters to identify your app and the type of API access that the app requires. You can open the URL in the current browser window or a popup. The user can authenticate with Google and grant the requested permissions. Google then redirects the user back to your app. The redirect includes an access token, which your app verifies and then uses to make API requests.&lt;/p&gt;
&lt;p&gt;Please feel free to check out &lt;a target=&quot;_blank&quot; href=&quot;https://developers.google.com/identity/protocols/OAuth2UserAgent&quot;&gt;the official documentation from Google&lt;/a&gt; for further reading.&lt;/p&gt;
</description>
<pubDate>Sat, 20 Apr 2019 08:35:05 +0000</pubDate>
<dc:creator>DarwinMailApp</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.darwinmail.app/</dc:identifier>
</item>
</channel>
</rss>