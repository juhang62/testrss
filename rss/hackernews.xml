<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>We do not use foreign keys (2016)</title>
<link>https://github.com/github/gh-ost/issues/331#issuecomment-266027731</link>
<guid isPermaLink="true" >https://github.com/github/gh-ost/issues/331#issuecomment-266027731</guid>
<description>&lt;p&gt;At GitHub we do not use foreign keys, ever, anywhere.&lt;/p&gt;
&lt;p&gt;Personally, it took me quite a few years to make up my mind about whether foreign keys are good or evil, and for the past 3 years I'm in the unchanging strong opinion that foreign keys should not be used. Main reasons are:&lt;/p&gt;
&lt;ul readability=&quot;3&quot;&gt;&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;FKs are in your way to shard your database. Your app is accustomed to rely on FK to maintain integrity, instead of doing it on its own. It may even rely on FK to cascade deletes (shudder).&lt;br/&gt;When eventually you want to shard or extract data out, you need to change &amp;amp; test the app to an unknown extent.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;FKs are a performance impact. The fact they require indexes is likely fine, since those indexes are needed anyhow. But the lookup made for each &lt;code&gt;insert&lt;/code&gt;/&lt;code&gt;delete&lt;/code&gt; is an overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;FKs don't work well with online schema migrations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;This last bullet is not a chicken and an egg, as you might think. FKs impose a lot of constraints on what's possible and what's not possible.&lt;/p&gt;
&lt;p&gt;Here's an old post of mine, reviewing the first appearance of Facebook's OSC, and which includes some thoughts on foreign keys: &lt;a rel=&quot;nofollow&quot; href=&quot;http://code.openark.org/blog/mysql/mk-schema-change-check-out-ideas-from-oak-online-alter-table&quot;&gt;http://code.openark.org/blog/mysql/mk-schema-change-check-out-ideas-from-oak-online-alter-table&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Let's say you have two tables, P &amp;amp; C, standing for Parent &amp;amp; Child, respectively. There's a foreign key in C such that each row in C points to some &quot;parent&quot; value in P.&lt;/p&gt;
&lt;p&gt;Doing schema migration of C is possible. However since foreign keys have unique names, the new (migrated) C table will have a FK with a different name than the original one.&lt;/p&gt;
&lt;p&gt;Doing schema migration of P is just not going to work. Recall that &lt;code&gt;gh-ost&lt;/code&gt; renames the table at the end. Alas, when renaming a table away, the FK will move with the renamed table. To create a parent-side FK on the &lt;em&gt;ghost&lt;/em&gt; table, one would need to migrate C ; and because &lt;code&gt;gh-ost&lt;/code&gt; uses async approach, P and P-ghost are never in complete sync at any point in time (except at lock time) which makes it impossible for C to have both a FK to P and to P-ghost. some integrity will be broken.&lt;/p&gt;
&lt;p&gt;There's more discussion on the documentation of &lt;a href=&quot;https://www.percona.com/doc/percona-toolkit/2.2/pt-online-schema-change.html&quot; rel=&quot;nofollow&quot;&gt;pt-online-schema-change&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 20:41:20 +0000</pubDate>
<dc:creator>Scarbutt</dc:creator>
<og:image>https://avatars3.githubusercontent.com/u/9919?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>Thoughts on Foreign Keys? · Issue #331 · github/gh-ost</og:title>
<og:url>https://github.com/github/gh-ost/issues/331</og:url>
<og:description>Hello, Thanks for your hard work on gh-ost! As I familiarize myself with the way it all works, I noted that foreign keys are explicitly not supported, but that they may be to some extent in the fut...</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/github/gh-ost/issues/331</dc:identifier>
</item>
<item>
<title>Apple has made it difficult to use web-based technology on its platforms</title>
<link>https://onezero.medium.com/apple-is-trying-to-kill-web-technology-a274237c174d</link>
<guid isPermaLink="true" >https://onezero.medium.com/apple-is-trying-to-kill-web-technology-a274237c174d</guid>
<description>&lt;p&gt;
&lt;h2 class=&quot;as cu et eu ax&quot;&gt;The company has made it extremely difficult to use web-based technology on its platforms, and it hopes developers won’t bother&lt;/h2&gt;
&lt;/p&gt;
&lt;div class=&quot;ev&quot;&gt;
&lt;div class=&quot;o n&quot;&gt;
&lt;div&gt;&lt;a href=&quot;https://onezero.medium.com/@ow?source=post_page-----a274237c174d----------------------&quot; rel=&quot;noopener&quot;&gt;&lt;img alt=&quot;Owen Williams&quot; class=&quot;r ew ex ey&quot; src=&quot;https://miro.medium.com/fit/c/96/96/2*W0zZWZkd4F5GQ2LjW75Hmw.jpeg&quot; width=&quot;48&quot; height=&quot;48&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

Credit: SOPA Images/Getty Images
&lt;p id=&quot;5efa&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl hm&quot;&gt;&lt;span class=&quot;r hn hw hq gf hx&quot;&gt;&lt;img alt=&quot;T&quot; class=&quot;hy hz ia ib ic id do t u gj ak&quot; src=&quot;https://miro.medium.com/max/158/1*x3MLNrKVby4_GZ1VCCYktA.png&quot; width=&quot;79&quot; height=&quot;79&quot;/&gt;&lt;span class=&quot;r hn ho hp hq hr hs ht hu hv gf gh&quot;&gt;T&lt;/span&gt;&lt;/span&gt;he programming languages used to build the web often find their way into apps, too. That’s largely due to software that allows developers to “reuse” the code they write for the web in products they build to run on operating systems like Linux, Android, Windows, and macOS.&lt;/p&gt;
&lt;p id=&quot;85e7&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;But Apple has a reason not to like this recycling of web technology. It wants its Mac App Store to be filled with apps that you can’t find anywhere else, not apps that are available on every platform. &lt;a href=&quot;https://9to5mac.com/2019/11/04/electron-app-rejections/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;With a recent policy change&lt;/a&gt;, the company has made it a little more difficult for developers to submit apps containing web code.&lt;/p&gt;
&lt;p id=&quot;92d7&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;The Mac App Store has quietly started rejecting apps made with a popular tool called Electron that allows developers to base all of their apps on the web-based code. Some of the most popular apps in the App Store, like Slack, Spotify, Discord, and WhatsApp, fall into this category.&lt;/p&gt;
&lt;p id=&quot;2ce7&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;&lt;a href=&quot;https://github.com/electron/electron/issues/20027&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;In a discussion&lt;/a&gt; on the programming community Github, several developers say rejections for apps that they built using Electron — which would were approved in the past — came with an explanation that these apps “attempt to hide the use of private APIs,” which are APIs built for Apple’s internal usage, rather than for third-party developers. Using private APIs to build public-facing apps is commonly frowned upon because they may change or break over time, and Apple &lt;a href=&quot;https://developer.apple.com/app-store/review/guidelines/#software-requirements&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;bans&lt;/a&gt; apps that use them.&lt;/p&gt;
&lt;p id=&quot;0969&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Electron has used these private APIs for years without issue. These private APIs allow developers to, for instance, &lt;a href=&quot;https://mozillagfx.wordpress.com/2019/10/22/dramatically-reduced-power-usage-in-firefox-70-on-macos-with-core-animation/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;drastically improve power usage&lt;/a&gt; whereas Apple’s sanctioned tools make the user experience worse. In the majority of these cases, Apple doesn’t provide real alternatives for developers who want to access these private API features.&lt;/p&gt;
&lt;p id=&quot;2e70&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Now it’s unlikely that the thousands of developers who have built their apps using Electron can release updates to them unless the Electron framework releases a major change to its implementation.&lt;/p&gt;
&lt;p id=&quot;5aab&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Developers could distribute their apps from their own websites, asking users to download them directly. But that means abandoning features like Apple’s auto-update mechanism from the Mac App Store and iCloud sync. And this direct-to-consumer method could soon be locked down, too, with Apple’s controversial &lt;a href=&quot;https://developer.apple.com/documentation/security/notarizing_your_app_before_distribution&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;notarization requirements&lt;/a&gt; potentially requiring their review.&lt;/p&gt;
&lt;p id=&quot;7f1d&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Apple has a history of stunting the web’s progress on its platforms. On iOS, &lt;a href=&quot;https://www.howtogeek.com/184283/why-third-party-browsers-will-always-be-inferior-to-safari-on-iphone-and-ipad/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;Apple doesn’t allow fully independent third-party browsers&lt;/a&gt;, requiring all apps to leverage its Safari browser when rendering web-based content. While browsers like Chrome and Opera are available in the App Store, they must use Apple’s Safari browser behind the scenes to render web pages, rather than their own. That means Apple has a monopoly on how iPhone and iPad users access the web. To push developers toward building native apps on iOS rather than using web technologies, Apple ignores popular parts of the &lt;a href=&quot;https://www.w3.org/TR/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;open web specification&lt;/a&gt; that other browsers implement, to its own benefit.&lt;/p&gt;
&lt;blockquote class=&quot;ii&quot; readability=&quot;5.5&quot;&gt;
&lt;div id=&quot;0fa2&quot; class=&quot;ij ik il at ec b et im in io ip iq hl&quot; readability=&quot;10&quot;&gt;
&lt;p class=&quot;ec b ir is ax&quot;&gt;Apple’s subtle, anti-competitive practices don’t look terrible in isolation, but together they form a clear strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p id=&quot;b1c4&quot; class=&quot;gy gz eb at ha b hb it hd iu hf iv hh iw hj ix hl&quot;&gt;A technology called WebRTC, for example, allows video calling in a web browser without additional software. It powers tools like Google Meet. But Apple was incredibly slow to &lt;a href=&quot;https://webkit.org/blog/8672/on-the-road-to-webrtc-1-0-including-vp8/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;implement the specification&lt;/a&gt;, leaving out key pieces of functionality, and the technology didn’t work &lt;a href=&quot;https://bugs.webkit.org/show_bug.cgi?id=183201&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;when embedded inside apps&lt;/a&gt;.&lt;/p&gt;
&lt;p id=&quot;dbe2&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Apple also handicapped an emerging standard called Progressive Web Apps (PWAs) — which, like Electron, allows developers to build native-like apps for both desktop and mobile — by &lt;a href=&quot;https://caniuse.com/#feat=web-app-manifest&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;partially&lt;/a&gt; &lt;a href=&quot;https://medium.com/@firt/whats-new-on-ios-12-2-for-progressive-web-apps-75c348f8e945&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;implementing&lt;/a&gt; it in a way that makes it too inconsistent to rely on. PWA doesn’t have the same problem if users open apps in Chrome or Firefox, but iPhone and iPad users can’t install third-party browsers, which makes PWA-based technology a non-starter.&lt;/p&gt;
&lt;p id=&quot;c7ff&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Developers use technologies like Electron and PWA because they allow for faster updates across platforms without an array of different codebases. Some argue that this results in lower quality apps, but I’d argue the alternative is no app at all or apps that are rarely updated because maintaining unique Windows, Mac, and web-based products is complex and expensive. Apple recently launched a &lt;a class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; href=&quot;https://onezero.medium.com/to-revive-the-mac-apple-wants-to-kill-electron-154873336e78&quot;&gt;competing framework called Catalyst&lt;/a&gt;, which allows developers with iPad apps to bring them to macOS quickly — a great tool for developers exclusively targeting Apple users, but not those building cross-platform apps.&lt;/p&gt;
&lt;p id=&quot;a1ee&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Apple’s subtle, anti-competitive practices don’t look terrible in isolation, but together they form a clear strategy: Make it so painful to build with web-based technology on Apple platforms that developers won’t bother. Now that the App Store is not accepting apps built using Electron, developers will likely find creative ways to work around it, but Apple is setting up for a continual cat-and-mouse game as it &lt;a href=&quot;https://www.macrumors.com/2019/09/03/apple-macos-catalina-notarization-mac-apps/&quot; class=&quot;cy by ie if ig ih&quot; target=&quot;_blank&quot; rel=&quot;noopener nofollow&quot;&gt;plans to exert more control&lt;/a&gt; over which apps can run on the platform in the future.&lt;/p&gt;
&lt;p id=&quot;48a8&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;These types of changes may be made in the name of privacy or security, but the reality is that the argument looks weak when both users and developers simply don’t have a choice because Apple controls the platform, browser engine, and the distribution method. Regardless of your opinion of Electron app quality, choice is important.&lt;/p&gt;
&lt;p id=&quot;cd4f&quot; class=&quot;gy gz eb at ha b hb hc hd he hf hg hh hi hj hk hl&quot;&gt;Apple’s control over its app ecosystem is a new type of monopoly that’s hard to understand for lawmakers, and difficult for us to fight back against — because there simply isn’t a way out of these restrictions when the company controls both the distribution method and the platform itself.&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 20:33:33 +0000</pubDate>
<dc:creator>Lordarminius</dc:creator>
<og:type>article</og:type>
<og:title>Apple Is Trying to Kill Web Technology</og:title>
<og:description>The company has made it extremely difficult to use web-based technology on its platforms, and it hopes developers won’t bother</og:description>
<og:url>https://onezero.medium.com/apple-is-trying-to-kill-web-technology-a274237c174d</og:url>
<og:image>https://miro.medium.com/focal/1200/632/48/33/1*zqENj8mfvANOuM2PEcFPxw.jpeg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://onezero.medium.com/apple-is-trying-to-kill-web-technology-a274237c174d?gi=5f2db3355e4d</dc:identifier>
</item>
<item>
<title>Open Letter to the Linux Foundation</title>
<link>https://blog.cleancoder.com/uncle-bob/2019/11/08/OpenLetterLinuxFoundation.html</link>
<guid isPermaLink="true" >https://blog.cleancoder.com/uncle-bob/2019/11/08/OpenLetterLinuxFoundation.html</guid>
<description>&lt;p&gt;To: The Linux Foundation&lt;br/&gt;Jim Zemlin: Executive Director&lt;br/&gt;Angela Brown: VP of Events&lt;br/&gt;Andy Updegrove: Legal Council&lt;br/&gt;&lt;/p&gt;&lt;p&gt;From: Robert Martin (@unclebobmartin) (unclebob@cleancoder.com)&lt;/p&gt;
&lt;p&gt;Re: Code of Conduct case of Charles Max Wood.&lt;/p&gt;
&lt;p&gt;Dear Linux Foundation:&lt;/p&gt;
&lt;p&gt;I am writing to you as a concerned member of the software development community which I have enjoyed serving for the last 50 years. I am writing in public because the events I wish to describe took place in public. I fear that something has gone terribly wrong within your organization; and that it will have deep repercussions within this industry that I cherish.&lt;/p&gt;
&lt;p&gt;The timeline of events, as far as I can determine them, is as follows:&lt;/p&gt;
&lt;p&gt;The Linux Foundation received a public tweet sent to the @KubeCon twitter address. That tweet recommended that Kube Con discontinue their association with Charles Max Wood. The reasons given in this complaint were his request for an open and civil phone call, and a picture of Mr. Wood wearing a MAGA hat.&lt;/p&gt;
&lt;p&gt;The Linux Foundation &lt;em&gt;publicly&lt;/em&gt; replied from the @linuxfoundation twitter account as follows:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;em&gt;Hi all, We have reviewed social and videos and determined that the Event Code of Conduct was violated and his registration to the event has been revoked. Our events should and will be a safe space.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First let me say that I find it highly problematic that the complaint and the decision were public. Indeed I am surprised that LF would accept a publicly submitted code of conduct complaint. I am much more than surprised that LF would ever consider &lt;em&gt;publicly&lt;/em&gt; responding to such a complaint. Indeed, it seems to me that the public complaint, and perhaps even the public response by LF, could be seen as public harassment – which is explicitly prohibited by the LF Code of Conduct.&lt;/p&gt;
&lt;p&gt;It seems to me that Code of Conduct complaints made in public must be immediately rejected and viewed as Code of Conduct violations in and of themselves. Code of Conduct complaints should be submitted in private and remain private and confidential in order to prevent their use as a means of harassment. It also seems to me that while the process of accepting, reviewing, and adjudicating such complaints should be public, the proceedings and decision of each individual case should remain private and confidential in order to protect the parties from harm. Making them a public showcase is, simply, horrible.&lt;/p&gt;
&lt;p&gt;Was the Code of Conduct actually violated by Mr. Wood? I have watched the videos in question and read the tweets and I can find no instance where Mr Wood violated the LF Code of Conduct. I understand that LF can make any decision they like about what constitutes a Code of Conduct violation. However, when both the complaint and the response are so blatantly public, it seems to me that LF owes it to the observing community to explain their decision and describe the due process that was used to make it – including the decision to make the public response that undoubtedly caused harm to Mr. Wood. To date no such explanation has been forthcoming, despite repeated requests.&lt;/p&gt;
&lt;p&gt;The software community needs to understand how decisions like this are going to be made. Otherwise those of us who have watched this case may be forced to conclude that LF has no internal process, that no due diligence will be applied to Code of Conduct complaints and determinations, that the accused will have no rights either of appeal or privacy, that LF feels free to make its decisions based on the blowing of political winds, and will loudly announce their decisions regardless of the harm it might cause.&lt;/p&gt;
&lt;p&gt;Therefore I have the following questions:&lt;/p&gt;
&lt;ul readability=&quot;11.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Why was the initial complaint accepted and acknowledged in public? It was clearly political in nature, and very clearly intended to cause harm to Mr. Wood.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Is it LF policy to accept complaints that, in and of themselves, violate the LF Code of Conduct?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Why was the Code of Conduct determination announced publicly, despite the harm it would obviously cause to Mr. Wood?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Can LF specifically justify the determination that Mr. Wood violated the Code of Conduct?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Does LF have a documented process by which Code of Conduct complaints are to be submitted, reviewed, and adjudicated?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;Is it LF policy to consider political affiliation, or support of certain public officials, as Code of Conduct violations?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Is it LF policy to publicly denounce registrants who have been determined to have violated the LF Code of Conduct?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Does LF have a Code of Conduct for how it conducts itself?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In summary, it appears to this humble observer that The Code of Conduct process at The Linux Foundation went very badly off the rails with regard to Charles Max Wood. That LF owes Mr. Wood, and the Software Community at large, a &lt;em&gt;profound&lt;/em&gt; apology. That LF should keep all future Code of Conduct complaints and decisions personal and confidential. That LF should publish and follow a well defined process for accepting, reviewing, and adjudicating future Code of Conduct complaints. And that some form of reparation be provided to Mr. Wood for the public harm that was done to him by the careless and unprofessional behavior of The Linux Foundation.&lt;/p&gt;
&lt;p&gt;Yours&lt;/p&gt;
&lt;p&gt;Robert C. Martin.&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 17:17:27 +0000</pubDate>
<dc:creator>anujbahuguna</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.cleancoder.com/uncle-bob/2019/11/08/OpenLetterLinuxFoundation.html</dc:identifier>
</item>
<item>
<title>Amazon Is Accused of Forcing Up Prices in Antitrust Complaint</title>
<link>https://www.bloomberg.com/news/articles/2019-11-08/amazon-merchant-lays-out-antitrust-case-in-letter-to-congress</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2019-11-08/amazon-merchant-lays-out-antitrust-case-in-letter-to-congress</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2019-11-08/amazon-merchant-lays-out-antitrust-case-in-letter-to-congress&quot;&gt;https://www.bloomberg.com/news/articles/2019-11-08/amazon-merchant-lays-out-antitrust-case-in-letter-to-congress&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=21484250&quot;&gt;https://news.ycombinator.com/item?id=21484250&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 237&lt;/p&gt;
&lt;p&gt;# Comments: 102&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 17:09:09 +0000</pubDate>
<dc:creator>ikeboy</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=75a33a50-028c-11ea-9988-89bb73c21ead&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOS0xMS0wOC9hbWF6b24tbWVyY2hhbnQtbGF5cy1vdXQtYW50aXRydXN0LWNhc2UtaW4tbGV0dGVyLXRvLWNvbmdyZXNz</dc:identifier>
</item>
<item>
<title>System design hack: Postgres is a great pub/sub and job server</title>
<link>https://layerci.com/blog/postgres-is-the-answer/</link>
<guid isPermaLink="true" >https://layerci.com/blog/postgres-is-the-answer/</guid>
<description>&lt;p&gt;If you're making any project of sufficient complexity, you'll need a &lt;a href=&quot;https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern&quot;&gt;publish/subscribe&lt;/a&gt; server to process events. This article will introduce you to Postgres, explain the alternatives, and walk you through an example use case of pub/sub and its solution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Postgres is an amazing relational database&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you aren't too familiar with Postgres, it's a feature-packed relational database that many companies use as a traditional central data store. By storing your &quot;users&quot; table in Postgres, you can immediately scale to 100 columns and a row for every living person.&lt;/p&gt;
&lt;p&gt;It's possible to scale Postgres to storing a billion 1KB rows entirely in memory - This means you could quickly run queries against the full name of everyone on the planet on commodity hardware and with little fine-tuning.&lt;/p&gt;
&lt;p&gt;I'm not going to belabor the point that something called &quot;PostgresSQL&quot; is a good SQL database. I'll show you a more interesting use case for it where we combine a few features to turn Postgres into a powerful pubsub / job server.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Postgres makes a great persistent pubsub server&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you do enough system design, you'll inevitably need to solve a problem with &lt;a href=&quot;https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern&quot;&gt;publish/subscribe architecture&lt;/a&gt;. We hit it quickly at LayerCI - we needed to keep the viewers of a test run's page and the github API notified about a run as it progressed.&lt;/p&gt;
&lt;p&gt;For your pub/sub server, you have a lot of options:&lt;/p&gt;
&lt;p&gt;There are very few use cases where you'd need a dedicated pub/sub server like Kafka. Postgres can &lt;a href=&quot;https://severalnines.com/blog/benchmarking-postgresql-performance&quot;&gt;easily handle 10,000 insertions per second&lt;/a&gt;, and it can be tuned to even higher numbers. It's rarely a mistake to start with Postgres and then switch out the most performance critical parts of your system when the time comes.&lt;/p&gt;
&lt;h3 id=&quot;pub-sub-atomic-operations-no-job-server-necessary-&quot;&gt;Pub/sub + atomic operations ⇒ no job server necessary.&lt;/h3&gt;
&lt;p&gt;In the list above, I skipped things similar to pub/sub servers called &quot;job queues&quot; - they only let one &quot;subscriber&quot; watch for new &quot;events&quot; at a time, and keep a queue of unprocessed events:&lt;/p&gt;
&lt;p&gt;It turns out that Postgres generally supersedes job servers as well. You can have your workers &quot;watch&quot; the &quot;new events&quot; channel and try to claim a job whenever a new one is pushed. As a bonus, Postgres lets other services watch the status of the events with no added complexity.&lt;/p&gt;
&lt;h3 id=&quot;our-use-case-ci-runs-processed-by-sequential-workers&quot;&gt;Our use case: CI runs processed by sequential workers&lt;/h3&gt;
&lt;p&gt;At LayerCI, we run &quot;test runs&quot;, which start by cloning a repository, and then running some user specified tests. There are microservices that do various initialization steps for the test run, and additional microservices (such as the websocket gateway) that need to listen to the status of the runs.&lt;/p&gt;
&lt;img src=&quot;https://layerci.com/blog/content/images/2019/11/run-flow.svg&quot; class=&quot;kg-image&quot;/&gt;How a test run is processed at LayerCI
&lt;p&gt;An instance of an API server creates a run by inserting a row into the &quot;Runs&quot; row of a Postgres table:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-SQL&quot;&gt;CREATE TYPE ci_job_status AS ENUM ('new', 'initializing', 'initialized', 'running', 'success', 'error');

CREATE TABLE ci_jobs(
        id SERIAL, 
        repository varchar(256), 
        status ci_job_status, 
        status_change_time timestamp
);

/*on API call*/
INSERT INTO ci_job_status(repository, status, status_change_time) VALUES ('https://github.com/colinchartier/layerci-color-test', 'new', NOW());&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;How do the workers worker &quot;claim&quot; a job? By setting the job status atomically:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-SQL&quot;&gt;UPDATE ci_jobs SET status='initializing'
WHERE id = (
  SELECT id
  FROM ci_jobs
  WHERE status='new'
  ORDER BY id
  FOR UPDATE SKIP LOCKED
  LIMIT 1
)
RETURNING *;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Finally, we can use a trigger and a channel to notify the workers that there might be new work available:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-SQL&quot;&gt;CREATE OR REPLACE FUNCTION ci_jobs_status_notify()
        RETURNS trigger AS
$$
BEGIN
        PERFORM pg_notify('ci_jobs_status_channel', NEW.id::text);
        RETURN NEW;
END;
$$ LANGUAGE plpgsql;


CREATE TRIGGER ci_jobs_status
        AFTER INSERT OR UPDATE OF status
        ON ci_jobs
        FOR EACH ROW
EXECUTE PROCEDURE ci_jobs_status_notify();&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All the workers have to do is &quot;listen&quot; on this status channel and try to claim a job whenever a job's status changes:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-go&quot;&gt;tryPickupJob := make(chan interface{})
//equivalent to 'LISTEN ci_jobs_status_channel;'
listener.Listen(&quot;ci_jobs_status_channel&quot;)
go func() {
  for event := range listener.Notify {
    select {
    case tryPickupJob &amp;lt;- true:
    }
  }
  close(tryPickupJob)
}

for job := range tryPickupJob {
  //try to &quot;claim&quot; a job
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When we combine these elements, we get something like the following:&lt;/p&gt;
&lt;img src=&quot;https://layerci.com/blog/content/images/2019/11/queue-system-3.svg&quot; class=&quot;kg-image&quot;/&gt;&lt;p&gt;This architecture scales to many sequential workers processing the job in a row, all you need is a &quot;processing&quot; state and a &quot;processed&quot; state for each worker. For LayerCI that looks like: new, initializing, initialized, running, complete.&lt;/p&gt;
&lt;p&gt;It also allows other services to watch the &lt;code&gt;ci_jobs_status_channel&lt;/code&gt; - Our websocket gateway for the /run page and github notification services simply watch the channel and notify any relevant parties of the published events.&lt;/p&gt;
&lt;h2 id=&quot;other-benefits-of-using-postgres-for-pub-sub&quot;&gt;Other benefits of using Postgres for Pub/Sub&lt;/h2&gt;
&lt;p&gt;There are also a bunch of other benefits to using postgres instead of something like Redis Pub/Sub:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Many SQL users will already have Postgres installed for use as a database, so there are no extra setup costs for using it for pub/sub.&lt;/li&gt;
&lt;li&gt;As a database, Postgres has very good persistence guarantees - It's easy to query &quot;dead&quot; jobs with, e.g., &lt;code&gt;SELECT * FROM ci_jobs WHERE status='initializing' AND NOW() - status_change_time &amp;gt; '1 hour'::interval&lt;/code&gt; to handle workers crashing or hanging.&lt;/li&gt;
&lt;li&gt;Since jobs are defined in SQL, it's easy to generate graphql and protobuf representations of them (i.e., to provide APIs that checks the run status.)&lt;/li&gt;
&lt;li&gt;It's easy to have multiple watchers of status changes, you can have other services use the same &quot;LISTEN ci_jobs_status_channel&quot;&lt;/li&gt;
&lt;li&gt;Postgres has very good language support, with bindings for most popular languages. This is a stark difference from most other pub/sub servers.&lt;/li&gt;
&lt;li&gt;You can also run complicated SQL queries on things that are still in your &quot;work queues&quot; to give highly tailored API endpoints to your users. LayerCI has pages like &lt;a href=&quot;https://layerci.com/github/distributed-containers-inc/sanic&quot;&gt;https://layerci.com/github/distributed-containers-inc/sanic&lt;/a&gt; that show the status of various jobs (even running ones.)&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;If you need a publish/subscribe or job server at any point in your project, it's not a bad idea to start by using Postgres. It'll give you lots of data integrity and performance guarantees, and it doesn't require you or your team learning any new technology.&lt;/p&gt;
&lt;p&gt;Discussion: &lt;a href=&quot;https://www.reddit.com/r/programming/comments/dtib5r/system_design_tricks_postgres_is_a_great_pubsub/&quot;&gt;reddit&lt;/a&gt;, &lt;a href=&quot;https://news.ycombinator.com/item?id=21484215&quot;&gt;hacker news&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:6598614638965989376&quot;&gt;linkedin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://layerci.com/&quot;&gt;LayerCI&lt;/a&gt; runs tests up to 95% faster by taking snapshots of the VM running the test as it progresses. You can try LayerCI for free and get running in under five minutes by clicking &lt;a href=&quot;https://github.com/apps/layerci/installations/new&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 17:05:55 +0000</pubDate>
<dc:creator>colinchartier</dc:creator>
<og:type>article</og:type>
<og:title>System design hack: Postgres is a great pub/sub &amp; job server</og:title>
<og:description>If you're making any project of sufficient complexity, you'll need a publish/subscribe [https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern] server to process events. This article will introduce you to Postgres, explain the alternatives, and walk you through an example use case of pub/sub and its solution. Postgres is an amazing relational database If you aren't too familiar with Postgres, it's a feature-packed relational database that many companies use as a traditional central d</og:description>
<og:url>https://layerci.com/blog/postgres-is-the-answer/</og:url>
<og:image>https://layerci.com/blog/content/images/2019/11/postgres.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://layerci.com/blog/postgres-is-the-answer/</dc:identifier>
</item>
<item>
<title>Go Turns 10</title>
<link>https://blog.golang.org/10years</link>
<guid isPermaLink="true" >https://blog.golang.org/10years</guid>
<description>&lt;p class=&quot;author&quot;&gt;Russ Cox, for the Go team&lt;br/&gt;8 November 2019&lt;/p&gt;
&lt;p&gt;Happy birthday, Go!&lt;/p&gt;
&lt;p&gt;This weekend we celebrate the 10th anniversary of &lt;a href=&quot;https://opensource.googleblog.com/2009/11/hey-ho-lets-go.html&quot; target=&quot;_blank&quot;&gt;the Go release&lt;/a&gt;, marking the 10th birthday of Go as an open-source programming language and ecosystem for building modern networked software.&lt;/p&gt;
&lt;p&gt;To mark the occasion, &lt;a href=&quot;https://twitter.com/reneefrench&quot; target=&quot;_blank&quot;&gt;Renee French&lt;/a&gt;, the creator of the &lt;a href=&quot;https://blog.golang.org/gopher&quot; target=&quot;_blank&quot;&gt;Go gopher&lt;/a&gt;, painted this delightful scene:&lt;/p&gt;

&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://blog.golang.org/10years/gopher10th-small.jpg&quot; width=&quot;850&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Celebrating 10 years of Go makes me think back to early November 2009, when we were getting ready to share Go with the world. We didn’t know what kind of reaction to expect, whether anyone would care about this little language. I hoped that even if no one ended up using Go, we would at least have drawn attention to some good ideas, especially Go’s approach to concurrency and interfaces, that could influence follow-on languages.&lt;/p&gt;
&lt;p&gt;Once it became clear that people were excited about Go, I looked at the history of popular languages like C, C++, Perl, Python, and Ruby, examining how long each took to gain widespread adoption. For example, Perl seemed to me to have appeared fully-formed in the mid-to-late 1990s, with CGI scripts and the web, but it was first released in 1987. This pattern repeated for almost every language I looked at: it seems to take roughly a decade of quiet, steady improvement and dissemination before a new language really takes off.&lt;/p&gt;
&lt;p&gt;I wondered: where would Go be after a decade?&lt;/p&gt;
&lt;p&gt;Today, we can answer that question: Go is everywhere, used by at least &lt;a href=&quot;https://research.swtch.com/gophercount&quot; target=&quot;_blank&quot;&gt;a million developers worldwide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Go’s original target was networked system infrastructure, what we now call cloud software. Every major cloud provider today uses core cloud infrastructure written in Go, such as Docker, Etcd, Istio, Kubernetes, Prometheus, and Terraform; the majority of the &lt;a href=&quot;https://www.cncf.io/projects/&quot; target=&quot;_blank&quot;&gt;Cloud Native Computing Foundation’s projects&lt;/a&gt; are written in Go. Countless companies are using Go to move their own work to the cloud as well, from startups building from scratch to enterprises modernizing their software stack. Go has also found adoption well beyond its original cloud target, with uses ranging from controlling tiny embedded systems with &lt;a href=&quot;https://gobot.io&quot; target=&quot;_blank&quot;&gt;GoBot&lt;/a&gt; and &lt;a href=&quot;https://tinygo.org/&quot; target=&quot;_blank&quot;&gt;TinyGo&lt;/a&gt; to detecting cancer with &lt;a href=&quot;https://medium.com/grail-eng/bigslice-a-cluster-computing-system-for-go-7e03acd2419b&quot; target=&quot;_blank&quot;&gt;massive big data analysis and machine learning at GRAIL&lt;/a&gt;, and everything in between.&lt;/p&gt;
&lt;p&gt;All this is to say that Go has succeeded beyond our wildest dreams. And Go’s success isn’t just about the language. It’s about the language, the ecosystem, and especially the community working together.&lt;/p&gt;
&lt;p&gt;In 2009, the language was a good idea with a working sketch of an implementation. The &lt;code&gt;go&lt;/code&gt; command did not exist: we ran commands like &lt;code&gt;6g&lt;/code&gt; to compile and &lt;code&gt;6l&lt;/code&gt; to link binaries, automated with makefiles. We typed semicolons at the ends of statements. The entire program stopped during garbage collection, which then struggled to make good use of two cores. Go ran only on Linux and Mac, on 32- and 64-bit x86 and 32-bit ARM.&lt;/p&gt;
&lt;p&gt;Over the last decade, with the help of Go developers all over the world, we have evolved this idea and sketch into a productive language with fantastic tooling, a production-quality implementation, a &lt;a href=&quot;https://blog.golang.org/ismmkeynote&quot; target=&quot;_blank&quot;&gt;state-of-the-art garbage collector&lt;/a&gt;, and &lt;a href=&quot;https://golang.org/doc/install/source#introduction&quot; target=&quot;_blank&quot;&gt;ports to a 12 operating systems and 10 architectures&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Any programming language needs the support of a thriving ecosystem. The open source release was the seed for that ecosystem, but since then, many people have contributed their time and talent to fill the Go ecosystem with great tutorials, books, courses, blog posts, podcasts, tools, integrations, and of course reusable Go packages importable with &lt;code&gt;go&lt;/code&gt; &lt;code&gt;get&lt;/code&gt;. Go could never have succeeded without the support of this ecosystem.&lt;/p&gt;
&lt;p&gt;Of course, the ecosystem needs the support of a thriving community. In 2019 there are dozens of Go conferences all over the world, along with &lt;a href=&quot;https://www.meetup.com/pro/go&quot; target=&quot;_blank&quot;&gt;over 150 Go meetup groups with over 90,000 members&lt;/a&gt;. &lt;a href=&quot;https://golangbridge.org&quot; target=&quot;_blank&quot;&gt;GoBridge&lt;/a&gt; and &lt;a href=&quot;https://medium.com/@carolynvs/www-loves-gobridge-ccb26309f667&quot; target=&quot;_blank&quot;&gt;Women Who Go&lt;/a&gt; help bring new voices into the Go community, through mentoring, training, and conference scholarships. This year alone, they have taught hundreds of people from traditionally underrepresented groups at workshops where community members teach and mentor those new to Go.&lt;/p&gt;
&lt;p&gt;There are &lt;a href=&quot;https://research.swtch.com/gophercount&quot; target=&quot;_blank&quot;&gt;over a million Go developers&lt;/a&gt; worldwide, and companies all over the globe are looking to hire more. In fact, people often tell us that learning Go helped them get their first jobs in the tech industry. In the end, what we’re most proud of about Go is not a well-designed feature or a clever bit of code but the positive impact Go has had in so many people’s lives. We aimed to create a language that would help us be better developers, and we are thrilled that Go has helped so many others.&lt;/p&gt;
&lt;p&gt;As &lt;a href=&quot;https://twitter.com/search?q=%23GoTurns10&quot; target=&quot;_blank&quot;&gt;#GoTurns10&lt;/a&gt;, I hope everyone will take a moment to celebrate the Go community and all we have achieved. On behalf of the entire Go team at Google, thank you to everyone who has joined us over the past decade. Let’s make the next one even more incredible!&lt;/p&gt;
&lt;div&gt;
&lt;center&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://blog.golang.org/10years/gopher10th-pin-small.jpg&quot; width=&quot;150&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/center&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 08 Nov 2019 16:42:32 +0000</pubDate>
<dc:creator>kevinconaway</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.golang.org/10years</dc:identifier>
</item>
<item>
<title>Gitlab&amp;#039;s Director of Risk and Global Compliance Resigns</title>
<link>https://www.reddit.com/r/gitlab/comments/dtfccm/gitlabs_director_of_risk_and_global_compliance/</link>
<guid isPermaLink="true" >https://www.reddit.com/r/gitlab/comments/dtfccm/gitlabs_director_of_risk_and_global_compliance/</guid>
<description>&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Who upvotes this garbage?&lt;/p&gt;
&lt;blockquote class=&quot;_28lDeogZhLGXvE95QRPeDL&quot; readability=&quot;5&quot;&gt;
&lt;blockquote class=&quot;_28lDeogZhLGXvE95QRPeDL&quot; readability=&quot;7&quot;&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;As I believe GitLab is engaging in discriminatory and retaliatory behavior, I have tendered my resignation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p class=&quot;_1qeIAgB0cPwnLhDF9XSiJM&quot;&gt;Is this a comment that will &quot;further inflame...credible personal and physical threats?&quot; I have a hard time believing that.&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 15:21:30 +0000</pubDate>
<dc:creator>FLIINO</dc:creator>
<og:title>r/gitlab - GitLab’s Director of Risk and Global Compliance resigns</og:title>
<og:type>website</og:type>
<og:url>https://www.reddit.com/r/gitlab/comments/dtfccm/gitlabs_director_of_risk_and_global_compliance/</og:url>
<og:description>55 votes and 36 comments so far on Reddit</og:description>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reddit.com/r/gitlab/comments/dtfccm/gitlabs_director_of_risk_and_global_compliance/</dc:identifier>
</item>
<item>
<title>NASA Flew Gas Detectors Above California, Found ‘Super Emitters’</title>
<link>https://www.bloomberg.com/news/articles/2019-11-06/nasa-flew-gas-detectors-above-california-found-super-emitters</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2019-11-06/nasa-flew-gas-detectors-above-california-found-super-emitters</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2019-11-06/nasa-flew-gas-detectors-above-california-found-super-emitters&quot;&gt;https://www.bloomberg.com/news/articles/2019-11-06/nasa-flew-gas-detectors-above-california-found-super-emitters&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=21482937&quot;&gt;https://news.ycombinator.com/item?id=21482937&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 348&lt;/p&gt;
&lt;p&gt;# Comments: 170&lt;/p&gt;
</description>
<pubDate>Fri, 08 Nov 2019 15:18:38 +0000</pubDate>
<dc:creator>zeristor</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=766bba70-028c-11ea-92b0-d59252d710f3&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOS0xMS0wNi9uYXNhLWZsZXctZ2FzLWRldGVjdG9ycy1hYm92ZS1jYWxpZm9ybmlhLWZvdW5kLXN1cGVyLWVtaXR0ZXJz</dc:identifier>
</item>
<item>
<title>Modern Data Practice and the SQL Tradition</title>
<link>https://tselai.com/modern-data-practice-and-the-sql-tradition.html</link>
<guid isPermaLink="true" >https://tselai.com/modern-data-practice-and-the-sql-tradition.html</guid>
<description>&lt;p class=&quot;date&quot;&gt;07 Nov 2019&lt;/p&gt;
&lt;p&gt;A specter is haunting Data Science—the specter of SQL. All the powers of new Data Management have entered into a holy alliance to exorcise this specter; Data Scientists and engineers, managers and developers, hipsters and disruptors.&lt;/p&gt;
&lt;p&gt;Especially among the young breeds of data practitioners, there is a growing consensus that SQL is not “cool”, not good enough or even worse “SQL is not professional enough and real data scientists should code”. My own experience, however, has taught me otherwise. Whether it is at the first stages of the pipeline like data collection and cleaning or the latter ones like feature engineering and report generation, I have come to appreciate the power and versatility of SQL and the effectiveness of RDBMS.&lt;/p&gt;
&lt;p&gt;In fact, it is not only the tool nature of SQL and RDBMS that appeals to me but rather the whole “SQL tradition” that goes with and encompasses them. It is the level of pragmatism that established and propelled data management since Codd introduced the relational model in the 70s.&lt;/p&gt;
&lt;p&gt;In this essay, I collected some of my thoughts on the topic of SQL and how it fits into modern data practice. It was motivated after a wave of disbelief I faced when at a presentation last week I claimed that modern data projects could benefit immensely by adopting SQL early on and sticking with it: a “start with SQLite, scale with Postgres” philosophy.&lt;/p&gt;
&lt;h2&gt;Make normalization normal again&lt;/h2&gt;
&lt;p&gt;Every data scientist occasionally writes a piece of code that “flattens” a dictionary. This nested structure either comes in as a response from an API endpoint or as a query result from a NoSQL database like MongoDB or ElasticSearch. More junior engineers even couple their discovery with a “look ma! no brackets!” enthusiasm.&lt;/p&gt;
&lt;p&gt;The schemaless nature of modern NoSQL systems virtually eliminated the need to have a well-thought data model design - with “virtually” being the operative word. You could just dump the data into a flexible “collection” of “documents” instead of a stricter “table” of “rows” and use a powerful query language to extract what you want later. Why? Because you cannot possibly anticipate how would the requirements change later on, and changing the schema in an RDBMS can become tricky. It sounds like a fair point; definitely is so in theory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The more I work with existing NoSQL deployments however, the more I believe that their schemaless nature has become an excuse for sloppiness and unwillingness to dwell on a project’s data model beforehand.&lt;/strong&gt; I see too many applications that handle “plain old” transactional data relying on MongoDB as their primary database from time-zero. Gradually then, they end up having to “unnest fields of a particular collection”, “create intermediate collections that act as anchors” or even worse commissioning projects that offer “a tabular layer on top of their NoSQL database” either by a cronjob that transforms into the data into a tabular format to a relational database, or even introducing a “Python library that everyone should use in order to get their data in a tabular format because that is why Scikit-Learn expects”.&lt;/p&gt;
&lt;p&gt;This path means that there is yet another database system that should be managed, yet another project under version control and yet some more accidental complexity introduced by the new moving pieces in the data pipeline. And of course, degraded data quality and performance are to be expected. On the one hand, because ensuring ACID-like compliance - especially durability - across the whole pipeline is too hard to achieve and on the other hand, too much data is copied around and through the disk and network IO or worse operated on with “in-memory join-like operations with Pandas”, because Mongo’s lookups and aggregations seem so awkward and difficult to get right.&lt;/p&gt;
&lt;p&gt;Here is the elephant-in-the-room-question that too many people think during the daily SCRUMs but they fear to raise: &lt;strong&gt;“Why didn’t we use an RDBMS in the first place? “&lt;/strong&gt;. There could be too many reasons for that decision - both technical and organizational, I will just focus on the “we were not sure about the requirements that early on”.&lt;/p&gt;
&lt;p&gt;Most major RDBMS today do offer some kind of schemaless support, typically with a JSON data type with rich query semantics. These implementations, having already gone through some major releases can confidently serve even the most demanding of cases. One can now model the “known” part of his data model in a typical relational manner and dump his “raw and unstructured” data into JSON columns. No need to “denormalize all the things” just because some element of the domain is “unstructured”. Why should one have to sacrifice joins on the “customers” table and have to reside to lookups by customer_id which is a nested field under the “purchases” collection?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The good thing with this approach is that one can have a single database for both their structured and unstructured data without sacrificing ACID-compliance.&lt;/strong&gt; Data integrity can be ensured both at the referential level (i.e. foreign keys), data quality can be tightly managed with constraints, indices, and triggers. Performance can be boosted at will, given that one is not reluctant to just think and look at the documentation. My favorite tool in that regard is none other than &lt;a href=&quot;https://www.sqlite.org/expridx.html&quot;&gt;“indices on expressions”&lt;/a&gt;. Plus, there are &lt;a href=&quot;https://www.citusdata.com/blog/2017/10/17/tour-of-postgres-index-types/&quot;&gt;numerous index types&lt;/a&gt; one can choose specifically tailored to their needs.&lt;/p&gt;
&lt;p&gt;For extreme cases where data is mostly read-heavy one can go even further. In a recent project, for example, I had to collect data from an API (~100M of rich JSON documents) and make them easier to analyze by typical business intelligence analysts and Excel users. My implementation was simple yet powerful. Data was crawled and dumped periodically in a “raw data table” as JSON documents and all further analysis was done through SQL views (materialized or not). It took some time to set up the basic views manually but after some guidance, even the less technical-adept of BI analysts could define their own views or query existing ones. It was tricky to achieve great performance at the first iterations; but later, the indices required were made apparent and once defined, things run smoothly. Postgres has even support for indices on materialized views - see &lt;a href=&quot;https://www.postgresql.org/docs/current/rules-materializedviews.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Bring ETL closer to the data&lt;/h2&gt;
&lt;p&gt;ETL is the cash burn machine of modern data-driven endeavors and the necessary evil in every data scientist’s daily life. And yet it is probably the less well thought of piece in the data pipeline. Countless machine learning engineers start their model selection work with the hopes of using random forests and support vectors, only to realize later that not enough clean data was available and they have to use simple regression.&lt;/p&gt;
&lt;p&gt;My main objection with data cleaning and transformation today is its “decentralized nature”. Each data scientist does their own cleaning procedure in their own client code with “df.dropna()” being thrown around like a ragdoll and before you know it each analyst have their own distorted - almost subjective - view of the data, It goes without saying that maintaining and tracking these so-called “cleaning processes” is impossible, as they are split in libraries under version control, notebooks usually not under version control and of course rules - usually residing in the database.&lt;/p&gt;
&lt;p&gt;In my view, the data pipeline would be smoother and cleaner if more data cleaning processes were pushed to the database level. First of all: types. Modern programming paradigms and languages have given type definitions a bad name (I think Java is to blame for that, but that’s another story). Setting the database column, for example, to “timestamp with timezone” is the best way to enforce a “fail fast, fairly early” policy so that bad data is rejected upon insertion and does not propagate further into the pipeline.&lt;/p&gt;
&lt;p&gt;Modern data types are usually accompanied with rich relevant algebra allowing for fine-grained operations without sacrificing power and expressivity - check out, for example, the &lt;a href=&quot;https://www.postgresql.org/docs/current/functions-datetime.html&quot;&gt;“date algebra”&lt;/a&gt; or &lt;a href=&quot;https://postgis.net/docs/PostGIS_Special_Functions_Index.html&quot;&gt;“geo algebra”&lt;/a&gt; one can do with Postgres. All these are possible with tools like Pandas of course, but why to pay the performance tax and most importantly: why pollute your client code with lambda functions and one-liners to extract simple features like “day of the week” or “hour of the day”. Leave these to the database and invest more brainpower and Python keystrokes on the more sophisticated parts of the analysis.&lt;/p&gt;
&lt;p&gt;Let’s focus on another typical feature of relational databases: &lt;strong&gt;triggers and stored procedures&lt;/strong&gt;. They both can be a significant tool in one’s data cleaning &amp;amp; transformation toolbox. In the project I mentioned above, each document had a set of fields rich with business information (IDs mainly) that could offer a clear relational structure to the data model. Whenever one document was inserted as a JSON payload, a piece of code had to run, extract the relevant fields and insert them as rows to other tables, by checking integrity constraints, adding foreign keys and so on. During the first iterations, I used jq and standard Python JSON processing to perform these CRUD operations. They worked just fine until a) performance took a hit and b) rules become increasingly more complex and verbosity was too much to bear. It was then that I decided to write a database trigger function, being called on every insert of the JSON documents. The function would extract each interesting field and insert appropriate rows in other tables accordingly. All within the same transaction! No need to move data up and down the data pipeline, parse dump JSON payloads and battle with encoding mismatches. The trigger function itself was also written in the “schema.sql” file which meant that everything was clear and documented along with the other relevant schema information, thus easier to track and manage.&lt;/p&gt;
&lt;p&gt;Database server programming used to be quite difficult to get used to, due to the fact that one has to write procedural code in an inherently declarative environment. Today however things have improved dramatically: syntax is sweeter and one can even use procedural languages to write their trigger and stored procedures functions. With &lt;a href=&quot;https://www.postgresql.org/docs/current/server-programming.html&quot;&gt;Postgres&lt;/a&gt;, one can even write Python and Perl code in the database!&lt;/p&gt;
&lt;h2&gt;SQL is Powerful&lt;/h2&gt;
&lt;p&gt;Suppose you are a data scientist: Chances are your Jupyter notebook has a pretty standard format. The first cells are used to build a DataFrame populated with data from a CSV file or a database query. Pandas pros can even do this in one single line as they know that the DataFrame constructor provides a lot of arguments. Typically this DataFrame has a few columns at first. And then “feature engineering” begins and creative-constructive chaos ensues: more columns are added subsequently to the dataset and it grows horizontally. Novices will be tempted to write for-loops to add more columns; if you are a more seasoned professional though you will squeeze some more juice from your machine and use vectorized operations instead. Practically this means that you use one-liners sequentially to define your columns as there’s probably already an implemented function you’re just calling and applying on each cell. You may still be thinking procedurally in your head (that for-loop temptation is still there) but in essence, you are doing declarative programming. You are not defining how your independent variable is calculated but rather what is its meaning: this preference for “what” instead of “how” is the essence of declarative programming.&lt;/p&gt;
&lt;p&gt;Modern data science is all about tailoring that dataset (the X matrix) to your model’s needs before you feed the data to it. If this process is not smooth and fast enough, you risk spending more mental capacity on building it yourself piece by piece rather than thinking of creative ways to add more features, describe them in a declarative way and iterate quickly. That is what SQL is good for and this is why it has been so successful for decades.&lt;/p&gt;
&lt;p&gt;My experience has shown that the more features I create at the query level, the more flexibility I have in experimenting with different feature vectors and the quicker the model selection and evaluation becomes. When you are writing a query, your database becomes a canvas you can draw beautiful models on. No need to jump up and down between disk and memory - or database and Pandas if you prefer. You can freely combine data from different tables, do simple or complex operations between various columns and let the query optimizer do the heavy lifting of figuring out the best way to create the dataset for you!&lt;/p&gt;
&lt;p&gt;SQL and relational databases have come a long way and nowadays offer almost any function a data scientist could ask. I mentioned above things like “date algebra” and “geo algebra”. Take text processing and free text search as other examples. It used to be the case that to have decent text processing and search you had to use NLTK or ElasticSearch. Yet another dependency, yet another database. Postgres (even SQLite and other major relational databases) offer some text manipulation functions and free-text search functionality that are just good enough for most applications. Does this eliminate the need for NLTK or ElasticSearch? Absolutely not. But why commit on such complexities upfront when you could test the waters first? Better yet, save your NLTK-fu for NLP-based features more sophisticated than “description text length”. &lt;strong&gt;Deploy your ElasticSearch cluster only if you need something more complex than “posts that contain at least two of these keywords my marketing guys sent me”&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Even collaboration becomes easier as different people can extend and build upon the same database views that contain the most fundamental of features - columns.&lt;/p&gt;
&lt;p&gt;If you are lucky enough that your organization has a system deployed like &lt;a href=&quot;https://prestodb.github.io/&quot;&gt;PrestoDB&lt;/a&gt;, you can take your feature engineering game at whole other level, as you have ready-made SQL layer on top of all your potential data sources.&lt;/p&gt;
&lt;p&gt;There are a couple of caveats in leveraging SQL however. First of all, due to its declarative nature, SQL will almost always give you results, but they just may not be what you’re asking for. SQL requires meticulousness and care as debugging is quite difficult to perform. You cannot print “I’m in” to check if there's a wrong loop condition and so on. Virtually, the only debugging you can do is to check and reverse-engineer the execution plan.&lt;/p&gt;
&lt;p&gt;On another more “cultural” aspect, one thing I have noticed is that best practices and concepts like “clean code” and “maintainability” are not so prevalent in the SQL world. I could attribute this to many aspects but I will just underline the fact that too many “business people” use SQL. They see it as an “ad hoc tool of obtaining data” which it is correct and pragmatic as an approach, but we should try and steer them to approaching SQL as a codebase which will be used by others and should be leveraged as a tool for communicating both with the database and with fellow programmers.&lt;/p&gt;
&lt;h2&gt;Relational databases are cost-effective&lt;/h2&gt;
&lt;p&gt;Relational databases usually make more sense financially too. &lt;strong&gt;Distributed systems like MongoDB and ElasticSearch are money-hungry beasts and can kill your technology and human resources budget&lt;/strong&gt;; unless you are absolutely certain and have run the numbers and decided that they do really make sense for your case.&lt;/p&gt;
&lt;p&gt;In one project I had to replace a 3-node managed MongoDB cluster with only 40GB of disk space with a Postgres instance with 8GB of RAM and 1TB of disk space at one-fifth of the monthly cost. The performance was significantly better with the default configuration even and stability improved too. As a rule of thumb, vertical scalability is in most cases more economical than the horizontal one.&lt;/p&gt;
&lt;p&gt;The keyword in the anecdote above is the “managed” part. With NoSQL databases, it is famously hard to get their deployment right. In many cases, you have to hire more manpower to make sure they are running 24-7 until a vendor comes in and promises to take this weight off your shoulders. Their pricing looks cheap at first but the cost can double or triple if you require marginally more disk space, or your indices need more memory to speed-up search results. In such cases, you decide to hire a data engineer like myself to make sure that your beast is tamed, works fine, is stable and really helps your business.&lt;/p&gt;
&lt;p&gt;Am I claiming that NoSQL databases are a mistake or that they cannot help? Of course not. I am just claiming that their deployment and management can be needlessly complex and over-demanding for most companies - especially with transactional and read-heavy data.&lt;/p&gt;
&lt;p&gt;Performance and stability with relational databases can be better out of the box. When it is not, you can browse the documentation and tweak the necessary configuration parameters. “So you can with NoSQL,” you say. True, but have you tried optimizing your MongoDB or ElasticSearch cluster at the deployment level? And I am talking about smarter things, beyond the “let’s add more Java heap size”. It’s not easy. In fact, I argue that is too complex in most cases. There are so many moving pieces and tradeoffs too hard to get right.&lt;/p&gt;
&lt;p&gt;On the other hand, if you check Postgres’ configuration file, most of the parameters are straightforward and tradeoffs not so hard to spot. Restarting or reloading the database without significant downtime is usually smooth as it well-integrated and tested with process managers like systemd or supervisor. Such iterations are no easy feat for distributed systems as too many things can go wrong and leave your cluster in a “red state”.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;strong&gt;PS: If you think your data pipeline could use some improvement, feel free to send me an &lt;a href=&quot;mailto:florents@tselai.com&quot;&gt;e-mail&lt;/a&gt; to discuss&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p class=&quot;tags&quot;&gt;Tags &lt;a href=&quot;https://tselai.com/tag/sql.html&quot;&gt;#sql&lt;/a&gt; &lt;a href=&quot;https://tselai.com/tag/data-science.html&quot;&gt;#data science&lt;/a&gt; &lt;a href=&quot;https://tselai.com/tag/data-engineering.html&quot;&gt;#data engineering&lt;/a&gt; &lt;a href=&quot;https://tselai.com/tag/data-modelling.html&quot;&gt;#data modelling&lt;/a&gt;&lt;/p&gt;






&lt;noscript readability=&quot;1.15625&quot;&gt;
&lt;p&gt;Please enable JavaScript to view the &lt;a href=&quot;https://disqus.com/?ref_noscript&quot; rel=&quot;nofollow&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/p&gt;
&lt;/noscript&gt;</description>
<pubDate>Fri, 08 Nov 2019 13:52:55 +0000</pubDate>
<dc:creator>MarkusWinand</dc:creator>
<og:image>https://tselai.com/img/flo.jpg</og:image>
<og:type>article</og:type>
<og:url>https://tselai.com/modern-data-practice-and-the-sql-tradition.html</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tselai.com/modern-data-practice-and-the-sql-tradition.html</dc:identifier>
</item>
<item>
<title>Nuclear energy is a vital part of solving the climate crisis</title>
<link>https://archive.is/0bVPq</link>
<guid isPermaLink="true" >https://archive.is/0bVPq</guid>
<description>&lt;p&gt;I never thought I would become a passionate champion for nuclear energy. But after 20 years of advocating for renewable energy, I’ve overcome the misconceptions I had in the past and I am convinced by the evidence we can’t fight climate change without nuclear.&lt;/p&gt;
&lt;p&gt;When I was the chief executive of the Canadian Solar Industries Association, I thought the “holy grail” was to make renewable energy cost-competitive so it could fulfill our energy needs. Today, wind and solar are among the cheapest forms of energy in many places around the world. The generous subsidies that fuelled early growth are no longer at play, yet the growth of wind and solar continues.&lt;/p&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div readability=&quot;6&quot;&gt;
&lt;div readability=&quot;7&quot;&gt;
&lt;p&gt;Story continues below advertisement&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div readability=&quot;14.445652173913&quot;&gt;Despite the strong growth, the percentage of emissions-free electricity in the world has not increased in 20 years. It’s stuck at 36 per cent, according to a &lt;a href=&quot;https://archive.is/o/0bVPq/https://www.iea.org/publications/nuclear/&quot;&gt;recent IEA report&lt;/a&gt;. This is because global demand keeps increasing, renewables often need to be backed up by new fossil fuel sources and existing nuclear plants are being shut down prematurely. We must face a sobering reality: Renewable energy alone is simply not enough to address the climate crisis.&lt;/div&gt;
&lt;p&gt;This is a difficult thing for me to admit. In 2014, I delivered a TEDx talk in which I was an unabashed champion for solar energy. I installed solar panels on the roof of my home and smart battery storage in my basement. I bought an electric vehicle. And I continue to be a supporter of wind and solar because we need every clean energy solution available. But I now realize I dedicated 20 years – very precious years from a climate-change perspective – promoting a partial solution.&lt;/p&gt;

&lt;p&gt;An overly optimistic view of renewables has affected major decisions about other energy sources, particularly nuclear. Our global focus on renewables has caused existing nuclear plants to be retired early and has stalled investment in new projects. It’s given people a false sense of security that we don’t need nuclear any more when nothing could be further from the truth.&lt;/p&gt;
&lt;p&gt;What’s worse, because wind and solar are variable (they produce electricity only when the wind blows or the sun shines), they must be paired with other energy sources to support demand, and these are almost always fossil fuels. In the absence of enough nuclear energy, renewables are effectively prolonging the life of coal and gas plants that can produce power around the clock.&lt;/p&gt;
&lt;p&gt;Unfortunately, many Canadians wrongly believe our future energy demands can be met with renewables alone. A recent Abacus Data poll found that more than 40 per cent of Canadians believe a 100-per-cent renewable energy future is possible. This is simply not true. The deadline to save the planet is approaching and we are no closer to a real solution.&lt;/p&gt;
&lt;p&gt;A critical issue is that nuclear is vastly misunderstood by policy makers and the general public. These well-intentioned people – and I used to be one of them – continue to believe fallacies, misconceptions and even fear-mongering about nuclear, including claims that it’s expensive, dangerous, and produces large quantities of radioactive waste.&lt;/p&gt;
&lt;p&gt;The truth is that when you consider the entire power generation life cycle, nuclear energy is one of the least expensive energy sources. That’s because uranium is cheap and abundant, and nuclear reactors – though costly to build – last for several decades. Furthermore, it’s safe: Used nuclear fuel is small in quantity, properly stored, strictly regulated, and poses no threat to human health or the environment.&lt;/p&gt;

&lt;div&gt;
&lt;div&gt;
&lt;div&gt;
&lt;div readability=&quot;6&quot;&gt;
&lt;div readability=&quot;7&quot;&gt;
&lt;p&gt;Story continues below advertisement&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There’s a staggering lack of knowledge and understanding of nuclear. I was active in the energy business, and I’ve lived my whole life in a province – Ontario – where nuclear makes up a significant portion of the electricity supply, and I still didn’t know the facts about nuclear energy until very recently.&lt;/p&gt;
&lt;p&gt;People fail to realize that nuclear is the only proven technology that has decarbonized the economies of entire countries, including France and Sweden. We can pair renewables with nuclear energy and start to meet our energy targets. But it will take a change in mentality and new investment in nuclear energy.&lt;/p&gt;
&lt;p&gt;So this is why I’m now on a mission to help people discover and rediscover nuclear as the clean technology solution to decarbonize our electricity systems and solve the climate crisis. We need to extend the life of existing plants rather than close them prematurely. We need to invest in new modern technologies including small modular reactors, which can be deployed in off-grid settings such as remote communities and mining sites. And we need to use nuclear alongside renewables to power the grid. We must act before it’s too late. And we can’t afford to be distracted from real, practical solutions by a completely impossible dream of 100 per cent renewable energy. We don’t want to look back on this time and realize we made the wrong decisions. The time for nuclear is now.&lt;/p&gt;
&lt;div&gt;&lt;em&gt;Your time is valuable. Have the Top Business Headlines newsletter conveniently delivered to your inbox in the morning or evening. &lt;/em&gt;&lt;a href=&quot;https://archive.is/o/0bVPq/https://www.theglobeandmail.com/newsletters/?utm_source=Arcnewsletter&amp;amp;utm_medium=onsite&amp;amp;utm_campaign=fixed_positions&amp;amp;utm_term=signuppage&amp;amp;utm_content=topbusiness_promo%23newsletter-group-2&quot;&gt;&lt;em&gt;Sign up today&lt;/em&gt;&lt;/a&gt;&lt;em&gt;. &lt;/em&gt;&lt;/div&gt;
</description>
<pubDate>Fri, 08 Nov 2019 13:49:26 +0000</pubDate>
<dc:creator>ericdanielski</dc:creator>
<og:type>article</og:type>
<og:url>http://archive.is/0bVPq</og:url>
<og:title>Nuclear energy is a vital part of solving the climate crisis - The Gl…</og:title>
<og:image>https://archive.is/0bVPq/3b90539dc1104470e3e2480c7b823f4f4223042a/scr.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://archive.is/0bVPq</dc:identifier>
</item>
</channel>
</rss>