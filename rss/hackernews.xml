<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>So you want your app/website to work in China</title>
<link>https://chanind.github.io/china/2019/01/19/launching-an-app-or-website-in-china.html</link>
<guid isPermaLink="true" >https://chanind.github.io/china/2019/01/19/launching-an-app-or-website-in-china.html</guid>
<description>&lt;p&gt;Wait, what do you mean make my app/site &lt;em&gt;work&lt;/em&gt; in China? I don’t have to do anything to make my app work in the US or Singapore or Kenya or anywhere else, and I didn’t make the Chinese government angry, so it should just work in China, right? Sadly, it’s not so simple. If your app/website servers aren’t hosted from within China, then, for all intents and purposes, it’s blocked. I mean, it will probably technically load, but will be excruciatingly, unusably slow. And sometimes it will just not load at all for hours at a time. This is true for all services hosted outside of the firewall, even in Hong Kong.&lt;/p&gt;
&lt;h3 id=&quot;the-firewall&quot;&gt;The Firewall&lt;/h3&gt;
&lt;p&gt;Any time a request needs to go from within China to the outside world, or from the outside world into China, the request crosses the Chinese Great Firewall. When this happens, there’s a lot of latency that gets added, and there’s a high chance the request will randomly fail. Requests through the firewall may appear to work most of the time, but then suddenly get fully blocked for several hours. The firewall doesn’t seem like it’s implemented uniformly across China either, so it’s possible that if you test in Shanghai your request may go through but a user in Changsha will have their requests blocked.&lt;/p&gt;
&lt;p&gt;Basically, if requests need to pass through the firewall to reach your servers outside of China you’re in for a bad time.&lt;/p&gt;
&lt;h3 id=&quot;icp-license&quot;&gt;ICP License&lt;/h3&gt;
&lt;p&gt;If you want to have any infrastructure working in China, you need to apply for an &lt;a href=&quot;https://en.wikipedia.org/wiki/ICP_license&quot;&gt;ICP license&lt;/a&gt; from the Chinese government. All the techniques below require that you have this license. It’s quite a pain to apply for, and takes several months, but there’s no way around it. You can find more info about registering for an ICP license &lt;a href=&quot;https://webdesign.tutsplus.com/articles/chinese-icp-licensing-what-why-and-how-to-get-hosted-in-china--cms-23193&quot;&gt;here&lt;/a&gt;. Alicloud also has a lot of info on registering for an ICP &lt;a href=&quot;https://www.alibabacloud.com/icp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;option-1-cloudflare-china-acceleration&quot;&gt;Option 1: Cloudflare China Acceleration&lt;/h3&gt;
&lt;p&gt;The easiest option to get your services working in China is to go use &lt;a href=&quot;https://www.cloudflare.com/network/china/&quot;&gt;Cloudflare’s China acceleration&lt;/a&gt;. Cloudflare partnered with Baidu to extend their acceleration network with points inside of China itself. Going through this method allows requests into and out of China to bypass the firewall, so your service will be fast. Cloudflare China acceleration requires an enterprise account though, so it’s going to be pricey.&lt;/p&gt;
&lt;p&gt;Using Cloudflare does effectively allow you to host your infrastructure outside of China, but depending on your business it might not be entirely legal. That’s because China has strict &lt;a href=&quot;https://www.chinalawblog.com/2018/05/china-data-protection-regulations-cdpr.html&quot;&gt;data protection laws&lt;/a&gt;, and in many cases you must store Chinese users’ data inside of China. If you’re not a huge company or don’t have much sensitive data on Chinese users this may not be an issue, but it’s something to be aware of.&lt;/p&gt;
&lt;p&gt;If most of your customers are outside of China and you just want to make sure your app/website loads quickly in China, then this is likely the best option for you.&lt;/p&gt;
&lt;h3 id=&quot;option-2-make-a-separate-chinese-version-of-your-appwebsite&quot;&gt;Option 2: Make a Separate Chinese Version of your App/Website&lt;/h3&gt;
&lt;p&gt;The most direct way to make your app/website work in China is, of course, to host your servers themselves in China. You can do that using a Chinese cloud provider like &lt;a href=&quot;https://alibabacloud.com&quot;&gt;Alicloud&lt;/a&gt; or &lt;a href=&quot;https://cloud.tencent.com&quot;&gt;Tencent cloud&lt;/a&gt;, or using the &lt;a href=&quot;https://www.amazonaws.cn/en/&quot;&gt;AWS China region&lt;/a&gt;. If you use AWS, you should be aware that the China region requires setting up a different account, and isn’t even run by Amazon!&lt;/p&gt;
&lt;p&gt;The most technically correct way to be in compliance with the Chinese government’s &lt;a href=&quot;https://www.chinalawblog.com/2018/05/china-data-protection-regulations-cdpr.html&quot;&gt;data protection laws&lt;/a&gt; is to have a separate Chinese version of your app/website and run a separate version of your infrastructure in China. This allows all data for your Chinese users to stay in China and not be transferred abroad. No requests ever have to cross the firewall, so everything remains fast. Of course, it’s practically quite annoying to run 2 separate but identical versions of your infrastructure and apps.&lt;/p&gt;
&lt;h3 id=&quot;option-3-proxy-requests-on-a-chinese-cloud-provider&quot;&gt;Option 3: Proxy Requests on a Chinese Cloud Provider&lt;/h3&gt;
&lt;p&gt;Chinese cloud providers like &lt;a href=&quot;https://alibabacloud.com&quot;&gt;Alicloud&lt;/a&gt; and &lt;a href=&quot;https://cloud.tencent.com&quot;&gt;Tencent cloud&lt;/a&gt; have fast connections between their datacenters through the firewall that you can make use of. You can create a VPC inside of China and a VPC outside of China and then connect them using a form of VPC peering. This gives you a high-speed connection through the firewall which you can use to proxy requests. If you host your main infrastructure in a Chinese region then you’ll be in compliance with China’s &lt;a href=&quot;https://www.chinalawblog.com/2018/05/china-data-protection-regulations-cdpr.html&quot;&gt;data protection laws&lt;/a&gt;, while still being able to serve requests outside of China via the proxied connection.&lt;/p&gt;
&lt;h3 id=&quot;option-4-use-a-chinese-cloud-provider-acceleration-service&quot;&gt;Option 4: Use a Chinese Cloud Provider Acceleration Service&lt;/h3&gt;
&lt;p&gt;If you’re hosting your infrastructure on &lt;a href=&quot;https://alibabacloud.com&quot;&gt;Alicloud&lt;/a&gt; or &lt;a href=&quot;https://cloud.tencent.com&quot;&gt;Tencent cloud&lt;/a&gt; in China, you can accelerate requests to your infrastructure globally using their acceleration services. These work similarly to the Cloudflare option above, but in reverse. Alicloud calls their service &lt;a href=&quot;https://www.alibabacloud.com/help/product/55629.htm&quot;&gt;Global Acceleration&lt;/a&gt;, and Tencent cloud calls their &lt;a href=&quot;https://intl.cloud.tencent.com/product/gaap&quot;&gt;GAAP&lt;/a&gt;. This allows users globally to make requests to your servers in China and still have them be fast.&lt;/p&gt;
&lt;h3 id=&quot;3rd-party-services&quot;&gt;3rd Party Services&lt;/h3&gt;
&lt;p&gt;No matter which option you go with, you still need to test that your service is working in China. Even if you’re running on Chinese infrastructure or using Cloudflare China acceleration you may still be relying on APIs that aren’t supported in China, like Facebook Login or Google Recaptcha. If your server in China needs to make API calls to services that aren’t optimized in China you may find that a lot of those requests fail as well.&lt;/p&gt;
&lt;h3 id=&quot;good-luck&quot;&gt;Good luck!&lt;/h3&gt;
</description>
<pubDate>Thu, 21 Mar 2019 21:48:27 +0000</pubDate>
<dc:creator>chanind</dc:creator>
<og:title>So you want your app/website to work in China…</og:title>
<og:description>Wait, what do you mean make my app/site work in China? I don’t have to do anything to make my app work in the US or Singapore or Kenya or anywhere else, and I didn’t make the Chinese government angry, so it should just work in China, right? Sadly, it’s not so simple. If your app/website servers aren’t hosted from within China, then, for all intents and purposes, it’s blocked. I mean, it will probably technically load, but will be excruciatingly, unusably slow. And sometimes it will just not load at all for hours at a time. This is true for all services hosted outside of the firewall, even in Hong Kong.</og:description>
<og:url>https://chanind.github.io/china/2019/01/19/launching-an-app-or-website-in-china.html</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://chanind.github.io/china/2019/01/19/launching-an-app-or-website-in-china.html</dc:identifier>
</item>
<item>
<title>VC Starter Kit</title>
<link>https://vcstarterkit.com/</link>
<guid isPermaLink="true" >https://vcstarterkit.com/</guid>
<description>What else are you going to spend your management fees on? 2 and 20 can only buy so many wine bottles.&lt;p&gt;Besides like many venture backed businesses, profits aren't really the goal for us. The profits of this sale will go to &lt;a href=&quot;https://allraise.org&quot;&gt;All Raise&lt;/a&gt;, an organization dedicated to diversity in funders &amp;amp; founders.&lt;/p&gt;</description>
<pubDate>Thu, 21 Mar 2019 21:45:53 +0000</pubDate>
<dc:creator>vinnyglennon</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://vcstarterkit.com/</dc:identifier>
</item>
<item>
<title>Repl.it GFX: Native graphics development in the browser</title>
<link>https://repl.it/site/blog/gfx?ref=updates</link>
<guid isPermaLink="true" >https://repl.it/site/blog/gfx?ref=updates</guid>
<description>&lt;p&gt;At Repl.it we live and breathe making software creation easier. With our &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/site/blog/platform&quot;&gt;programming environment&lt;/a&gt;, you could start coding in your favorite language in seconds. With &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/site/blog/deploy&quot;&gt;live deployments&lt;/a&gt;, we made web hosting a breeze. With &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/site/blog/multi&quot;&gt;Multiplayer&lt;/a&gt;, we've removed the drudgery from coding with friends. And today, we're excited to bring native GUI applications and game development to the browser.&lt;/p&gt;
&lt;p&gt;Before we go on, you have to see this in action: Just run the repl below, wait a few seconds for it to load, focus on the output window and start playing Tetris in Pygame:&lt;/p&gt;

&lt;p&gt;Let's take it up a notch and boot up this nifty desktop app we all love (might take up to a minute to load but then can be done recursively):&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why&lt;/h2&gt;
&lt;p&gt;We believe in the plurality and diversity of developer communities. That's why we spend countless days, weeks, and months working on features that can work cross language. We want programmers from all backgrounds, regardless of their language, to be able to code games and apps with ease. Plus, supporting native graphics opens us up to a wealth of frameworks, games, and educational material! We're particularly excited about supporting Pygame and Java Swing.&lt;/p&gt;
&lt;h2 id=&quot;game-jam&quot;&gt;Game Jam&lt;/h2&gt;
&lt;p&gt;We wanted to give you a reason to try out our new GUI capabilities, so we kicked off our very first &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/jam&quot;&gt;game jam&lt;/a&gt; on March 18th. It closes on April 18th, so there’s still plenty of time to show us what you can do. As for the winner? The grand prize is 1BTC ($4,031 at the time of writing).&lt;/p&gt;
&lt;p&gt;&lt;img width=&quot;50%&quot; src=&quot;https://repl.it/public/images/jam/bling-robot.png&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;technology&quot;&gt;Technology&lt;/h2&gt;
&lt;p&gt;We like to release early and often. So while this works, and we're proud of what we've done, there's still a long way to go. Right now, we're piping the X Window system through VNC through WebSockets to your browser, which is not the most efficient way to do this — we have a lot of ideas on how we could improve it.&lt;/p&gt;
&lt;p&gt;To stay true to our &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/platform&quot;&gt;adaptive IDE&lt;/a&gt; principle — the IDE should &quot;do the right thing&quot; when you need it — we use &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/site/blog/stderr&quot;&gt;LD_PRELOAD&lt;/a&gt; to figure out when an application is trying to open a window and then we start X in the background and reveal the screen in the environment. This has the effect of delighting our users:&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet tw-align-center&quot; data-lang=&quot;en&quot; readability=&quot;2.4825174825175&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Awesome! Plotting on &lt;a href=&quot;https://t.co/DcPy3gZFUO&quot;&gt;https://t.co/DcPy3gZFUO&lt;/a&gt; using GR and &lt;a href=&quot;https://twitter.com/hashtag/JuliaLang?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#JuliaLang&lt;/a&gt;. &lt;a href=&quot;https://t.co/K4GPBjOZgR&quot;&gt;pic.twitter.com/K4GPBjOZgR&lt;/a&gt;&lt;/p&gt;
— Josef Heinen (@josef_heinen) &lt;a href=&quot;https://twitter.com/josef_heinen/status/1104288994110644224?ref_src=twsrc%5Etfw&quot;&gt;March 9, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;Finally, depending on how far you are from our data center (US-central) you might feel a delay, which we're also working on making better by replicating our data center (watch out &lt;a target=&quot;_blank&quot; href=&quot;https://store.google.com/magazine/stadia&quot;&gt;Google Stadia&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Please leave us &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/feedback&quot;&gt;feedback&lt;/a&gt; so we can improve. Or come &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/jobs&quot;&gt;work with us&lt;/a&gt; so we can make it better together.&lt;/p&gt;
&lt;h2 id=&quot;get-started&quot;&gt;Get started&lt;/h2&gt;
&lt;p&gt;Early reception has been tremendous, and replers have been building amazing things. Take a look at some of the submissions in our &lt;a target=&quot;_blank&quot; href=&quot;https://repl.it/talk/challenge&quot;&gt;game jam&lt;/a&gt; or give it a spin yourself:&lt;/p&gt;
&lt;p&gt;We're supporting these frameworks out of the box, but very soon, we'll roll this out to all of our languages. If you're impatient, then you can use our base image to Install (pkg-install) and run your framework.&lt;/p&gt;
&lt;p&gt;Happy game development!&lt;/p&gt;
</description>
<pubDate>Thu, 21 Mar 2019 21:07:13 +0000</pubDate>
<dc:creator>starbugs</dc:creator>
<og:title>Repl.it GFX: Native graphics development in the browser</og:title>
<og:description>At Repl.it we live and breathe making software creation easier. With our programming environment, you could start coding in your favorite language in seconds.…</og:description>
<og:type>article</og:type>
<og:image>https://repl.it/public/images/blog/gfx.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://repl.it/site/blog/gfx?ref=updates</dc:identifier>
</item>
<item>
<title>A Julia Interpreter and Debugger</title>
<link>https://julialang.org/blog/2019/03/debuggers</link>
<guid isPermaLink="true" >https://julialang.org/blog/2019/03/debuggers</guid>
<description>&lt;p class=&quot;metadata&quot;&gt;&lt;span class=&quot;timestamp&quot;&gt;19 Mar 2019&lt;/span&gt;  |  &lt;span class=&quot;author&quot;&gt;&lt;a href=&quot;https://github.com/timholy&quot;&gt;Tim Holy&lt;/a&gt;, &lt;a href=&quot;https://github.com/KristofferC&quot;&gt;Kristoffer Carlsson&lt;/a&gt;, &lt;a href=&quot;https://github.com/pfitzseb&quot;&gt;Sebastian Pfitzner&lt;/a&gt;, &lt;a href=&quot;https://github.com/Keno&quot;&gt;Keno Fischer&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The authors are pleased to announce the release of a fully-featured debugger for Julia. You can now easily debug and introspect Julia code in a variety of ways:&lt;/p&gt;
&lt;ul readability=&quot;1.9792207792208&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Step into functions and manually walk through your code while inspecting its state&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Set breakpoints and trap errors, allowing you to discover what went wrong at the point of trouble&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Interactively update and replace existing code to rapidly fix bugs in place without restarting&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Use the full-featured IDE in &lt;a href=&quot;http://junolab.org/&quot; title=&quot;Juno&quot;&gt;Juno&lt;/a&gt; to bundle all these features together in an easy to use graphical interface&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The debugger is itself a collection of tools that enable those features. The core is powered by &lt;a href=&quot;https://github.com/JuliaDebug/JuliaInterpreter.jl&quot; title=&quot;JuliaInterpreter&quot;&gt;an interpreter&lt;/a&gt; that can faithfully run Julia code while allowing various front-ends to control its execution. Each front-end is its own package: &lt;a href=&quot;http://junolab.org/&quot; title=&quot;Juno&quot;&gt;Juno&lt;/a&gt; incorporates the debugger into its IDE, &lt;a href=&quot;https://github.com/timholy/Rebugger.jl&quot; title=&quot;Rebugger&quot;&gt;Rebugger&lt;/a&gt; provides a REPL text UI, and the traditional step/next/continue command-line interface is provided by &lt;a href=&quot;https://github.com/JuliaDebug/Debugger.jl&quot; title=&quot;Debugger&quot;&gt;Debugger&lt;/a&gt;. All these new debugging capabilities seamlessly integrate with &lt;a href=&quot;https://github.com/timholy/Revise.jl&quot; title=&quot;Revise&quot;&gt;Revise&lt;/a&gt;, so that you can continuously analyze and modify code in a single session.&lt;/p&gt;

&lt;p&gt;To orient potential users to the debugger front-ends, here we include a couple of screen shots that highlight the new capabilities.&lt;/p&gt;
&lt;h2 id=&quot;juno&quot;&gt;Juno&lt;/h2&gt;
&lt;p&gt;Juno provides a rich user interface around the interpreter and allows you to set breakpoints and step through directly in the source code. The debugger REPL can execute arbitrary code in a local context and the Workspace allows you to inspect local variables. The screenshot below shows a small debugging session for &lt;code class=&quot;highlighter-rouge&quot;&gt;gcd&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://julialang.org/images/blog/2019-03-19-debuggers/juno.png&quot; alt=&quot;Juno&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;Juno.@run&lt;/code&gt; macro interprets your code and drops you in a debugging session if it hits a breakpoint, while &lt;code class=&quot;highlighter-rouge&quot;&gt;Juno.@enter&lt;/code&gt; allows you to step through starting from the first line.&lt;/p&gt;
&lt;h2 id=&quot;debugger-and-rebugger&quot;&gt;Debugger and Rebugger&lt;/h2&gt;
&lt;p&gt;If you have a different favorite editor than Atom—or sometimes work in remote sessions through a console interface—you can alternatively perform debugging via the REPL. There are two REPL interfaces: Debugger offers a “step, next, continue” interface similar to debuggers like &lt;code class=&quot;highlighter-rouge&quot;&gt;gdb&lt;/code&gt;, whereas Rebugger aims to provide a console interface that is reminiscent of an IDE. Debugger has some capabilities that none of the other interfaces offer (e.g., very fine-grained control over stepping, the ability to execute the generator of generated functions, etc.), so it should be your go-to choice for particularly difficult cases. Below is a screenshot of a short session using Debugger.jl, showing some of its features:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://julialang.org/images/blog/2019-03-19-debuggers/debugger_jl_screenshot.png&quot; alt=&quot;Debugger.jl&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In the screenshot, the function &lt;code class=&quot;highlighter-rouge&quot;&gt;closestpair&lt;/code&gt; is debugged by prepending the call with the &lt;code class=&quot;highlighter-rouge&quot;&gt;@enter&lt;/code&gt; macro. Execution is then suspended on the first line of the function (line 4) and it is possible to see a breakpoint on line 8. Upon running the command &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; (short for “continue”), execution is resumed until a breakpoint is encountered. At this point, the command &lt;code class=&quot;highlighter-rouge&quot;&gt;fr&lt;/code&gt; (short for “frame”) shows all the local variables and their values at the point where the execution of the code suspended due to the breakpoint. Finally, a “Julia REPL mode” is entered using the &lt;code class=&quot;highlighter-rouge&quot;&gt;`&lt;/code&gt; key. This gives a normal Julia REPL mode with the addition that the local variables are available.&lt;/p&gt;
&lt;p&gt;Rebugger enters calls via a key binding. To try it, type &lt;code class=&quot;highlighter-rouge&quot;&gt;gcd(10, 20)&lt;/code&gt; and &lt;em&gt;without hitting enter&lt;/em&gt; type Meta-i (Esc-i, Alt-i, or option-i). After a short pause the display should update; type &lt;code class=&quot;highlighter-rouge&quot;&gt;?&lt;/code&gt; to see the possible actions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://julialang.org/images/blog/2019-03-19-debuggers/rebugger_interpret.png&quot; alt=&quot;Rebugger&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Rebugger also features an “edit” interface. For more information, see &lt;a href=&quot;https://timholy.github.io/Rebugger.jl/dev/&quot;&gt;Rebugger’s documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With several packages making their initial debut, and some old ones getting new capabilities, we felt it would be appropriate to provide an overview of the underpinnings of the new ecosystem.&lt;/p&gt;
&lt;h2 id=&quot;juliainterpreter&quot;&gt;JuliaInterpreter&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/JuliaDebug/JuliaInterpreter.jl&quot; title=&quot;JuliaInterpreter&quot;&gt;JuliaInterpreter&lt;/a&gt; is the lynch pin of the entire stack; it contains the logic needed to evaluate and inspect running Julia code. An &lt;a href=&quot;https://en.wikipedia.org/wiki/Interpreter_(computing)&quot;&gt;interpreter&lt;/a&gt; lends itself naturally to step-wise code evaluation and the implementation of breakpoints.&lt;/p&gt;
&lt;p&gt;JuliaInterpreter descended from an original package &lt;a href=&quot;https://github.com/JuliaDebug/ASTInterpreter2.jl&quot;&gt;ASTInterpreter2&lt;/a&gt; written by Keno Fischer. In its original form (prior to Jan 2019), ASTInterpreter2 was a fairly small but sophisticated package, capable of handling many advanced aspects of Julia’s internal representation of code. It was in need of updating to the many changes in Julia 1.0, although most of that work had already been done by &lt;a href=&quot;https://github.com/Roboneet&quot;&gt;Neethu Joy&lt;/a&gt; in late 2018. When we began our own efforts, we finished the updates and decided to extend it in many ways:&lt;/p&gt;
&lt;ul readability=&quot;36.196786016352&quot;&gt;&lt;li readability=&quot;6&quot;&gt;
&lt;p&gt;JuliaInterpreter became recursive-by-default, interpreting calls all the way down to the &lt;code class=&quot;highlighter-rouge&quot;&gt;ccall&lt;/code&gt;s, intrinsic functions, and builtin functions that define Julia’s lowest levels. By running virtually all code through the interpreter, it became more straightforward to implement breakpoints and trap errors.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;14.389623601221&quot;&gt;
&lt;p&gt;JuliaInterpreter received numerous performance enhancements, and now can run step-wise through code at roughly 50× its original speed. These optimizations reduce—but come nowhere close to eliminating—the most serious disadvantage of running all code in the interpreter: slow performance. It is hoped that the performance gap between compiled and interpreted code, which can be many orders of magnitude, will narrow in the coming months. However, the interpreter will always be slower than compiled code.&lt;/p&gt;
&lt;p&gt;It’s also worth noting that there are cases where the interpreter feels faster, at least on initial execution. Julia’s JIT compiler produces excellent results, but all that code-analysis takes time; there is interest in exploring whether running more code in the interpreter could reduce latency, a.k.a. the “time to first plot” problem. JuliaInterpreter is a potential tool for exploring that trade off, and it appears that &lt;a href=&quot;https://github.com/JuliaDebug/JuliaInterpreter.jl/issues/44&quot;&gt;not much additional work would be needed&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;35.156428119063&quot;&gt;
&lt;p&gt;JuliaInterpreter gained the ability to interpret “top-level code”, for example the code used to define packages and create test suites. This was a major change, partly because top-level code uses an expanded vocabulary, but mostly because top-level code can define new modules, structures, and methods, which in turn introduces the need to manage “world age,” the counter that determines the visibility of methods to callers. (If this fails, you see errors like “method is too new to be called…”).&lt;/p&gt;
&lt;p&gt;Supporting top-level code allowed JuliaInterpreter to achieve two goals: the ability to serve as the foundation of new code-parsing abilities for Revise, and the ability to run test suites originally designed for compiled Julia code. Once we had top-level execution partially working, we &lt;a href=&quot;https://github.com/JuliaDebug/JuliaInterpreter.jl/issues/13&quot;&gt;decided&lt;/a&gt; to evaluate JuliaInterpreter against the most extensive single test suite available, that of Julia itself. This revealed dozens of bugs in areas like the calling of C libraries (&lt;code class=&quot;highlighter-rouge&quot;&gt;ccall&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;@cfunction&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;cglobal&lt;/code&gt;), &lt;code class=&quot;highlighter-rouge&quot;&gt;llvmcall&lt;/code&gt;, keyword-argument functions, generated functions, anonymous functions, &lt;code class=&quot;highlighter-rouge&quot;&gt;struct&lt;/code&gt; definitions, global variables, the handling of try/catch, locks and threads, and the treatment of &lt;code class=&quot;highlighter-rouge&quot;&gt;@eval&lt;/code&gt;ed code. Some of these problems were isolated for us from the test suite failures by two additional contributors, &lt;a href=&quot;https://github.com/GunnarFarneback&quot;&gt;Gunnar Farnebäck&lt;/a&gt; and &lt;a href=&quot;https://github.com/macd&quot;&gt;Don MacMillen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As of this writing, most of the cleanly-isolated problems have been fixed. While we are still far from perfect, the pursuit of such a demanding goal has contributed extensively to the robustness of these young packages.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;11&quot;&gt;
&lt;p&gt;JuliaInterpreter gained support for breakpoints. While not strictly a feature of interpreters, they are necessary to build a capable debugger and can be viewed as an additional form of control-flow within the interpreter itself. These breakpoints can be set manually with functions &lt;code class=&quot;highlighter-rouge&quot;&gt;breakpoint&lt;/code&gt; and a macro &lt;code class=&quot;highlighter-rouge&quot;&gt;@breakpoint&lt;/code&gt;, manipulated in Juno, Rebugger, or Debugger, or added directly to code with the &lt;code class=&quot;highlighter-rouge&quot;&gt;@bp&lt;/code&gt; macro. Existing breakpoints can be &lt;code class=&quot;highlighter-rouge&quot;&gt;disable&lt;/code&gt;d, &lt;code class=&quot;highlighter-rouge&quot;&gt;enable&lt;/code&gt;d, or &lt;code class=&quot;highlighter-rouge&quot;&gt;remove&lt;/code&gt;d. We support setting of breakpoints at specific source lines or on entry to a specific method, conditional and unconditional breakpoints, and can automatically trap errors as if they were manually-set breakpoints.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;To explore the interpreter in its own right, you can start like this:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;using JuliaInterpreter
A = rand(1:10, 5)
@interpret sum(A)
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;If all is working well, you should see the same answer you get from running &lt;code class=&quot;highlighter-rouge&quot;&gt;sum(A)&lt;/code&gt; without &lt;code class=&quot;highlighter-rouge&quot;&gt;@interpret&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&quot;loweredcodeutils&quot;&gt;LoweredCodeUtils&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/JuliaDebug/LoweredCodeUtils.jl&quot; title=&quot;LoweredCodeUtils&quot;&gt;LoweredCodeUtils&lt;/a&gt; is the most specialized and opaque of the new packages. Its purpose is to build links between multiple cooperating methods. For example, the seemingly-simple definition&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;mymethod(x, y=0; z=&quot;Hello&quot;, msg=&quot;world&quot;) = 1
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;actually creates 5 methods: one “body method” (here, simply returning &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;), two “positional-argument” methods (ones that do not accept any keyword arguments), and two “keyword function” methods (ones that get called when you supply at least one keyword argument, then fill in defaults and standardize order). Because all five of these arise from the same user-supplied expression, they need to be implicitly linked in order to provide a satisfying user experience. In particular, changes to source files cause the line numbers of compiled methods to become outdated; if we didn’t correct that, Juno might open a file to the outdated line number when stepping through code. LoweredCodeUtils does the source-level analysis to discover these associations and handle differences that arise when parsing the same file multiple times.&lt;/p&gt;
&lt;p&gt;If you ever wanted to be able to parse Julia code and extract the signatures of the methods it defines (without redefining the methods), LoweredCodeUtils is the package for you.&lt;/p&gt;
&lt;h2 id=&quot;codetracking&quot;&gt;CodeTracking&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/timholy/CodeTracking.jl&quot; title=&quot;CodeTracking&quot;&gt;CodeTracking&lt;/a&gt; was designed to act as a simple, lightweight “query API” for retrieving data from &lt;a href=&quot;https://github.com/timholy/Revise.jl&quot; title=&quot;Revise&quot;&gt;Revise&lt;/a&gt;. Essentially, LoweredCodeUtils performs analysis, Revise manages changes that occur over time, and CodeTracking informs the rest of the world. For CodeTracking to do anything interesting, you need to be running Revise; to allow CodeTracking to be a lightweight dependency, it relies on Revise to populate its own internal variables.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&quot;https://github.com/timholy/CodeTracking.jl&quot; title=&quot;CodeTracking&quot;&gt;CodeTracking’s README&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2 id=&quot;revise-and-rebugger&quot;&gt;Revise and Rebugger&lt;/h2&gt;
&lt;p&gt;Thanks to a rewrite based on JuliaInterpreter, Revise and Rebugger are better (in some cases, much better) at their core tasks. In particular, if you used an earlier version of Rebugger, you may have noticed that it was defeated by many language constructs (e.g., functions containing keyword arguments, &lt;code class=&quot;highlighter-rouge&quot;&gt;@eval&lt;/code&gt;-generated methods, etc). Most of the underlying causes were resolved by LoweredCodeUtils, which is in turn used by Revise, which then feeds the necessary data to CodeTracking for consumption by Rebugger. As one measure of the difference, of the more than 10,000 methods in Base, Revise 1.1.0 fails to capture 1,425 method signatures (a failure rate of 13%). In contrast, Revise 2.0.0 misses only 10 (&amp;lt;0.1%).&lt;/p&gt;
&lt;p&gt;Consequently, in addition to the new “interpret” interface, the new Rebugger is much better at its original “edit” interface, too.&lt;/p&gt;
&lt;p&gt;Revise (and consequently Rebugger) has also gained some other new abilities, like handling methods defined at the REPL. In the longer term, the deep analysis of code permitted by JuliaInterpreter and LoweredCodeUtils may support features that were formerly out of reach.&lt;/p&gt;

&lt;p&gt;This has been only a high-level overview. Some of the individual packages have extensive documentation, and interested readers are encouraged to work through it. For anyone looking to develop a deeper understanding of the internals of Julia’s code, the new packages provide a powerful set of tools for introspection and analysis. And of course, we hope that the new debugging capabilities further accelerate Julia’s rapid development, and make it that much more fun of a language to use.&lt;/p&gt;
&lt;hr/&gt;</description>
<pubDate>Thu, 21 Mar 2019 18:27:30 +0000</pubDate>
<dc:creator>one-more-minute</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://julialang.org/blog/2019/03/debuggers</dc:identifier>
</item>
<item>
<title>Show HN: A Color Picker I Made</title>
<link>https://colorsupplyyy.com/app</link>
<guid isPermaLink="true" >https://colorsupplyyy.com/app</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://colorsupplyyy.com/app&quot;&gt;https://colorsupplyyy.com/app&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19455441&quot;&gt;https://news.ycombinator.com/item?id=19455441&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 418&lt;/p&gt;
&lt;p&gt;# Comments: 84&lt;/p&gt;
</description>
<pubDate>Thu, 21 Mar 2019 18:08:24 +0000</pubDate>
<dc:creator>mtgentry</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://colorsupplyyy.com/app</dc:identifier>
</item>
<item>
<title>Show HN: A raycaster that renders to random dot stereogram (like Magic Eye)</title>
<link>https://github.com/ammonb/stereogram-raycaster</link>
<guid isPermaLink="true" >https://github.com/ammonb/stereogram-raycaster</guid>
<description>&lt;div class=&quot;Box-body&quot;&gt;
&lt;article class=&quot;markdown-body entry-content p-5&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;This is a real-time 3D engine (ray caster) that renders to single-image random-dot stereogram (the images made popular in the Magic Eye books). I wrote this because I was curious if the brain would be able to follow a stereogram in motion. Click on the screen and press 3 after the program starts to render in stereogram. Let me know what you think!&lt;/p&gt;

&lt;h2&gt;Controls&lt;/h2&gt;
&lt;p&gt;The program initially renders in color. You can change the render mode (to color, depth map or stereogram) by pressing 1, 2 or 3. In the game, navigate with the arrow keys, and jump with the space bar. If you hold down the 'e' key, you can use the arrow keys to modify the block in front of you.&lt;/p&gt;
&lt;h2&gt;Raycasting&lt;/h2&gt;
&lt;p&gt;Raycasting is an algorithm for rending 3D (or pseudo-3D) geometry. It was made famous by Wolfenstein 3D and Doom. The core algorithm is delightfully simple (you can write a working renderer in about 20 lines). The best way to understand ray casting is to view it as a simplification of ray tracing.&lt;/p&gt;
&lt;p&gt;In ray tracing, rays are 'traced' from the location of a camera in a scene out into the world. One ray is calculated for each pixel in the output image. All the render needs to do, then, is calculate what color object each ray intersects first, and draw a pixel of that color. The following images (from Wikipedia) shows the idea. &lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/ammonb/stereogram-raycaster/blob/master/images/raytracing.png&quot;&gt;&lt;img src=&quot;https://github.com/ammonb/stereogram-raycaster/raw/master/images/raytracing.png&quot; alt=&quot;ray tracing&quot; title=&quot;Ray tracing&quot;/&gt;&lt;/a&gt; This algorithm produces beautiful renderings, but is computationally expensive.&lt;/p&gt;
&lt;p&gt;Raycasting is an optimization on this idea. Rather than calculate a ray per pixel, a raycaster calculates a ray per column of pixels, and reconstructs the column of pixels by considering the length of the ray. As long as the geometry satisfies certain constraints, this works like a charm, and is dramatically faster. To understand how this works, imagine a simple block maze like Wolfenstein 3D.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/ammonb/stereogram-raycaster/blob/master/images/simple.png&quot;&gt;&lt;img src=&quot;https://github.com/ammonb/stereogram-raycaster/raw/master/images/simple.png&quot; alt=&quot;Simple raycaster&quot; title=&quot;Simple raycaster&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Notice that all walls are vertical and of constant height, with the camera at the midpoint on the walls. This means that the rendered image is vertically symmetrical. As we draw this image, than, all we need to know is the height and color of the line to draw in each column. By perspective math, this height is simply the original wall height divided by the distance to the wall (or length of the ray cast for each column). The following image shows this.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/ammonb/stereogram-raycaster/blob/master/images/raycasting.png&quot;&gt;&lt;img src=&quot;https://github.com/ammonb/stereogram-raycaster/raw/master/images/raycasting.png&quot; alt=&quot;ray casting&quot; title=&quot;Ray casting&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The rendering loop used to draw the above image looks like&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;for (var x = 0; x &amp;lt; SCREEN_WIDTH; x++) {
    // camera_heading + FOV * x / SCREEN_WIDTH;
    var angle = angleForColumn(x);

    // calculate distance to wall from camera position at given angle
    var [color, distance] = castRay(angle);

    // draw wall slice
    var h = WALL_HEIGHT / distance;
    drawLine(x, SCREEN_HEIGHT/2 - h, x, SCREEN_HEIGHT/2 + h, color);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This basic idea can be extended to support arbitrary wall heights, horizontal surfaces, and vertical motion by the camera (as it is in the game Doom, and my raycaster above). To understand how this works, look at a snippet of the rendering loop above&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// draw wall slice
var h = WALL_HEIGHT / distance;
drawLine(x, SCREEN_HEIGHT/2 - h, x, SCREEN_HEIGHT/2 + h, color);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This can be re-written&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;var h1 = CAM_HEIGHT - WALL_HEIGHT / 2;
var d1 = distance;

var h2 = CAM_HEIGHT + WALL_HEIGHT / 2;
var d2 = distance;

var y1 = h1 / d1;
var y2 = h2 / d2;

drawLine(x, y1, x, y2, color);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here we have two points intersecting the ray we've cast, essentially defined in cylindrical coordinates (the angle of the ray, the distance to each point, and the height of each point). We then convert both points to screen coordinates by dividing the height by the distance, and draw a line between them. This works for the top and bottom of a wall (as we've already seen). But it works equally well for horizontal surfaces. Occlusion obviously becomes an issue, but this is easily handled by drawing back to front (or drawing front to back, disallowing transparency, and clamping all y values at the min seen so far).&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/ammonb/stereogram-raycaster/blob/master/images/complex.png&quot;&gt;&lt;img src=&quot;https://github.com/ammonb/stereogram-raycaster/raw/master/images/complex.png&quot; alt=&quot;Complex raycaster&quot; title=&quot;Complex raycaster&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I've totally ignored the issue of how you actually calculate ray/world intersections. You can read more about that [here] (&lt;a href=&quot;http://lodev.org/cgtutor/raycasting.html&quot; rel=&quot;nofollow&quot;&gt;http://lodev.org/cgtutor/raycasting.html&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Random dot stereograms&lt;/h2&gt;
&lt;p&gt;Stereograms are images (or pairs of images) that provide the illusion of a 3D scene with depth perception. The simplest way to do this is to show a separate image to each eye. Then, differences in the location of features in each image can provoke depth perception. This works, but requires an optical apparatus (like Google cardboard, or polarized light and 3D glasses) to view the two images.&lt;/p&gt;
&lt;p&gt;Random dot stereograms are stereograms where the images are seemingly random patterns of dots. Each image by itself shows nothing. However, a pair of random-dot images viewed as a stereogram can still provoke depth perception. Differences in the positions of the almost-random dots create depth perception without any color information.&lt;/p&gt;
&lt;p&gt;Random dot stereograms do not require a pair of images. This is the idea of a single-image random dot stereogram. Such an image uses a repetitive pattern of dots, similar to a chain link fence. The viewer can then spread or cross their eyes when viewing the image, and trick their brain into thinking that both eyes are focusing on the same spot when they are in fact offset by the width of the repeating pattern. Modifications to successive columns of the pattern can then create different angles between apparent features, and provoke depth perception.&lt;/p&gt;
&lt;p&gt;To understand the algorithm, consider the following diagram.&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/ammonb/stereogram-raycaster/blob/master/images/stereogram.png&quot;&gt;&lt;img src=&quot;https://github.com/ammonb/stereogram-raycaster/raw/master/images/stereogram.png&quot; alt=&quot;Stereogram diagram&quot; title=&quot;Stereogram diagram&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This diagram shows a pair of eyes focusing on a 3D scene. Between the eyes and the scene, we again have an image plane (just as we did for the raycaster). However, this time, the goal is represent not color information, but stereoscopic depth information. How can we do this? Consider points A and B in the digram. These are points on the surface of the 3D geometry that we are rendering. When both eyes focus on point A, the line from each eye passes through a different point in the image plane. Specifically, the line from the left eye passes through the point a1, and the line from the right eye passes through a2. If our image represents this scene, then, p1 and p2 must have the same color. Now consider point B on the scene. Again, the lines from the eyes pass through two points (b1 and b2). So points b1 and b2 must be the same color. We can repeat this exercise for every point on the surface of the geometry we are rendering, and the result will be a list of constraints on the output image (pairs of pixels that must be equal). All it takes to render a stereogram is to produce an image that satisfies these constraints (the degenerate case of setting all pixels to the same color is not a very interesting, however).&lt;/p&gt;
&lt;p&gt;Solving the constraints is not hard. However, it will be a per-pixel operation (like ray tracing) not a per-column operation like raycasting. First, we need a way to calculate the distance between pairs of linked pixels, as a function of the z value at a given point (the values x and y from the digram). By similar triangles, we get&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;constraint_separation  = (EYE_SEPERATION * z) / (1 + z)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;where EYE_SEPERATION is the distance between our eyes (in pixels), and z is the depth of the scene at the point in question, measured in distance from the image plane, in multiples of the distance from our eyes to the image plane.&lt;/p&gt;
&lt;p&gt;Because all constraints are between pairs of pixels in the same row, we can consider the algorithm one row at a time. For each row of pixels in the image, then, we create an array to store constraints, where we map each index pixel to an earlier pixel&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;var constraints = Array(SCREEN_WIDTH);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And fill in the constraints as follows&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;for (var x = 0; x &amp;lt; SCREEN_WIDTH; x++) {
    // get the z for xth pixel in the row
    var z = getZ(x, row);

    // distance between the image points linked by this z value
    var separation = (EYE_SEPERATION * z) / (1 + z);

    // the two linked points
    var p1 = x - Math.floor(separation/2);
    var p2 = p1 + Math.floor(separation);

    // if they're in range, record that p2 must equal p1
    if (p1 &amp;gt;= 0 &amp;amp;&amp;amp; p2 &amp;lt; SCREEN_WIDTH) {
        constraints[p2] = p1;
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then, all we have to do is scan left to right, and check the constraints. If a pixel is unconstrained, set it to a random value. Otherwise, copy the value from earlier in the image&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;for (var x = 0; x &amp;lt; SCREEN_WIDTH; x++) {
    if (constraints[x] === undefined) {
        putPixel(x, y, randomColor());
    } else {

        // get the color of the pixel pointed to by our constraint
        var c = getPixel(constraints[x], y);

        // and output that color
        putPixel(x, y, c);
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;That's it! You can read in more detail about stereograms &lt;a href=&quot;http://www.cs.waikato.ac.nz/~ihw/papers/94-HWT-SI-IHW-SIRDS-paper.pdf&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Thu, 21 Mar 2019 17:28:35 +0000</pubDate>
<dc:creator>ammon</dc:creator>
<og:image>https://avatars3.githubusercontent.com/u/124652?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>ammonb/stereogram-raycaster</og:title>
<og:url>https://github.com/ammonb/stereogram-raycaster</og:url>
<og:description>A raycaster that renders to random dot stereogram - ammonb/stereogram-raycaster</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/ammonb/stereogram-raycaster</dc:identifier>
</item>
<item>
<title>Amazon to Launch Mobile Ads, in a Threat to Google and Facebook</title>
<link>https://www.bloomberg.com/news/articles/2019-03-21/amazon-said-to-launch-mobile-ads-in-threat-to-google-facebook</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2019-03-21/amazon-said-to-launch-mobile-ads-in-threat-to-google-facebook</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2019-03-21/amazon-said-to-launch-mobile-ads-in-threat-to-google-facebook&quot;&gt;https://www.bloomberg.com/news/articles/2019-03-21/amazon-said-to-launch-mobile-ads-in-threat-to-google-facebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=19454254&quot;&gt;https://news.ycombinator.com/item?id=19454254&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 201&lt;/p&gt;
&lt;p&gt;# Comments: 192&lt;/p&gt;
</description>
<pubDate>Thu, 21 Mar 2019 16:33:42 +0000</pubDate>
<dc:creator>jmsflknr</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=b05640e0-4c78-11e9-8052-f18eac07b102&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOS0wMy0yMS9hbWF6b24tc2FpZC10by1sYXVuY2gtbW9iaWxlLWFkcy1pbi10aHJlYXQtdG8tZ29vZ2xlLWZhY2Vib29r</dc:identifier>
</item>
<item>
<title>Tesla sues former employees for allegedly stealing data, Autopilot source code</title>
<link>https://www.reuters.com/article/us-tesla-lawsuit/tesla-sues-former-employees-for-allegedly-stealing-data-autopilot-source-code-idUSKCN1R21P9</link>
<guid isPermaLink="true" >https://www.reuters.com/article/us-tesla-lawsuit/tesla-sues-former-employees-for-allegedly-stealing-data-autopilot-source-code-idUSKCN1R21P9</guid>
<description>&lt;p&gt;(Reuters) - Tesla Inc filed a lawsuit on Thursday against a former engineer at the company, claiming he copied the source code for its Autopilot technology before joining a Chinese self-driving car startup in January.&lt;/p&gt;
&lt;div class=&quot;PrimaryAsset_container&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;Image_container&quot; tabindex=&quot;-1&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;LazyImage_container LazyImage_dark&quot;&gt;&lt;img src=&quot;https://s3.reutersmedia.net/resources/r/?m=02&amp;amp;d=20190321&amp;amp;t=2&amp;amp;i=1368789406&amp;amp;r=LYNXNPEF2K19O&amp;amp;w=20&quot; aria-label=&quot;FILE PHOTO: A Tesla logo is seen at a groundbreaking ceremony of Tesla Shanghai Gigafactory in Shanghai, China January 7, 2019. REUTERS/Aly Song&quot;/&gt;
&lt;/div&gt;

&lt;p&gt;&lt;span&gt;FILE PHOTO: A Tesla logo is seen at a groundbreaking ceremony of Tesla Shanghai Gigafactory in Shanghai, China January 7, 2019. REUTERS/Aly Song&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The engineer, Guangzhi Cao, copied more than 300,000 files related to Autopilot source code as he prepared to join China’s Xiaopeng Motors Technology Company Ltd, the Silicon Valley carmaker said in the lawsuit filed in a California court.&lt;/p&gt;
&lt;p&gt;Separately, Tesla lawyers on Wednesday filed a lawsuit against four former employees and U.S. self-driving car startup Zoox Inc, alleging the employees stole proprietary information and trade secrets for developing warehousing, logistics and inventory control operations.&lt;/p&gt;
&lt;p&gt;In a statement, Xiaopeng spokeswoman Marie Cheung said the company was not aware of Cao’s alleged misconduct and that the company has started an internal investigation on the matter.&lt;/p&gt;
&lt;p&gt;The company “fully respects any third-party’s intellectual property rights and confidential information. The company has been complying and will comply [with] all applicable laws and regulations,” she said in a statement.&lt;/p&gt;
&lt;p&gt;Cao and Zoox could not immediately be reached for comment.&lt;/p&gt;
&lt;p&gt;Tesla is building a vehicle assembly facility in Shanghai, putting it in direct competition with Xiaopeng and other Chinese companies in the world’s largest electric vehicle market.&lt;/p&gt;
&lt;p&gt;Its Autopilot is a driver assistance system that handles some driving tasks and allows drivers to take their hands off the wheel, although the company stresses it still requires driver supervision and does not make the vehicle autonomous.&lt;/p&gt;
&lt;p&gt;Cao's LinkedIn profile shows &lt;a href=&quot;https://www.linkedin.com/in/guangzhi-cao-a667574&quot;&gt;here&lt;/a&gt; he has been working with Xiaopeng since January as &quot;head of perception.&quot;&lt;/p&gt;
&lt;p&gt;Xiaopeng, which debuted an electric car in Las Vegas last year, counts Alibaba Group Holding Ltd and Foxconn Technology Co Ltd among its investors.&lt;/p&gt;
&lt;p&gt;The company, also known as Xpeng Motors, employs at least five former Tesla employees, the U.S. carmaker alleged in the lawsuit.&lt;/p&gt;
&lt;p&gt;Apple Inc last year accused &lt;a href=&quot;https://ca.reuters.com/article/technologyNews/idCAKBN1K02RR-OCATC&quot;&gt;here&lt;/a&gt; one former employee of stealing trade secrets related to self-driving cars and joining Xiaopeng's U.S. subsidiary.&lt;/p&gt;
&lt;p&gt;Several companies are racing to develop the technology required to make cars drive on their own and lawsuits against former employees have become common as firms strive to keep proprietary information in-house.&lt;/p&gt;
&lt;p&gt;Alphabet Inc's Waymo self-driving vehicle unit took &lt;a href=&quot;https://in.reuters.com/article/alphabet-uber-trial/waymo-accepts-245-million-and-ubers-regret-to-settle-self-driving-car-dispute-idINKBN1FT2C6&quot;&gt;here&lt;/a&gt; Uber Technologies to court after a former employee stole thousands of confidential documents and became chief of Uber's self-driving car project. Uber later paid $245 million to settle the case.&lt;/p&gt;
&lt;div class=&quot;Attribution_container&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;Attribution_attribution&quot; readability=&quot;9&quot;&gt;
&lt;p class=&quot;Attribution_content&quot;&gt;Reporting by Supantha Mukherjee in Bengaluru and Stephen Nellis in San Francisco; editing by Patrick Graham, Bernard Orr and Dan Grebler&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;StandardArticleBody_trustBadgeContainer&quot;&gt;&lt;span class=&quot;StandardArticleBody_trustBadgeTitle&quot;&gt;Our Standards:&lt;/span&gt;&lt;span class=&quot;trustBadgeUrl&quot;&gt;&lt;a href=&quot;http://thomsonreuters.com/en/about-us/trust-principles.html&quot;&gt;The Thomson Reuters Trust Principles.&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;
</description>
<pubDate>Thu, 21 Mar 2019 16:02:26 +0000</pubDate>
<dc:creator>Element_</dc:creator>
<og:title>Tesla sues former employees for allegedly stealing data, Autopilot...</og:title>
<og:url>https://www.reuters.com/article/us-tesla-lawsuit-idUSKCN1R21P9</og:url>
<og:type>article</og:type>
<og:description>Tesla Inc filed a lawsuit on Thursday against a former engineer at the company, ...</og:description>
<og:image>https://s3.reutersmedia.net/resources/r/?m=02&amp;d=20190321&amp;t=2&amp;i=1368789406&amp;w=1200&amp;r=LYNXNPEF2K19O</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reuters.com/article/us-tesla-lawsuit/tesla-sues-former-employees-for-allegedly-stealing-data-autopilot-source-code-idUSKCN1R21P9</dc:identifier>
</item>
<item>
<title>Remastering Star Trek: Deep Space Nine with Machine Learning</title>
<link>https://captrobau.blogspot.com/2019/03/remastering-star-trek-deep-space-nine.html</link>
<guid isPermaLink="true" >https://captrobau.blogspot.com/2019/03/remastering-star-trek-deep-space-nine.html</guid>
<description>As a little side-project, I have been working on putting the artificial neural networks of &lt;a href=&quot;https://topazlabs.com/ai-gigapixel/&quot; target=&quot;_blank&quot;&gt;AI Gigapixel&lt;/a&gt; to the test and having them upscale another favorite thing of mine... Star Trek: Deep Space Nine (DS9).&lt;div&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-iOeQ-HxfULM/XIhCDO6J5fI/AAAAAAAAGbw/EptoBlhqLIc34OW9jKJk4DjH9CFvE6lLACK4BGAYYCw/s1600/logo.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;356&quot; src=&quot;https://4.bp.blogspot.com/-iOeQ-HxfULM/XIhCDO6J5fI/AAAAAAAAGbw/EptoBlhqLIc34OW9jKJk4DjH9CFvE6lLACK4BGAYYCw/s640/logo.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;&lt;h2&gt;The State of Deep Space Nine&lt;/h2&gt;
Just like Final Fantasy 7, of which I am upscaling the backgrounds, textures, and videos in &lt;a href=&quot;https://captrobau.blogspot.com/p/about.html&quot; target=&quot;_blank&quot;&gt;Remako mod&lt;/a&gt;, DS9 was also relegated to a non-HD future. While the popular Original Series and The Next Generation were mostly shot on film, the mid 90s DS9 had its visual effects shots (space battles and such) shot on video.&lt;p&gt;While you can rescan analog film at a higher resolution, video is digital and can't be rescanned. This makes it much costlier to remaster this TV show, which is one of the reasons why it hasn't happened.&lt;br/&gt;&lt;/p&gt;&lt;h2&gt;Remastering Star Trek: Deep Space Nine With Machine Learning&lt;/h2&gt;
This is where neural networks could come in, I thought. With tools like AI Gigapixel, I knew it might be possible the low definition frames of DS9 can be scaled up to a higher definition such as 1080p or 4K. It would never be the same as proper remastering, but it would a step in the good direction.&lt;p&gt;So I tried my hand at frame or two, to see what it could do. The results were great. AI Gigapixel uses neural networks trained on real photos. So while it did okay with upscaling the video game renders of Final Fantasy, it did amazing upscaling real-life footage and the bigger budget CGI effects of DS9.&lt;/p&gt;&lt;p&gt;Here are some examples below:&lt;br/&gt;&lt;/p&gt;
&lt;div&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-bifHhKh38to/XIgWomGKoMI/AAAAAAAAGa4/4aS2yRUzb4QeQfeEiFgP3UT_fXXadZ0IACK4BGAYYCw/s1600/f9115.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;428&quot; src=&quot;https://3.bp.blogspot.com/-bifHhKh38to/XIgWomGKoMI/AAAAAAAAGa4/4aS2yRUzb4QeQfeEiFgP3UT_fXXadZ0IACK4BGAYYCw/s640/f9115.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Original 480p definition (click to enlarge)&lt;/em&gt;&lt;/p&gt;

&lt;div&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-PtX8doJgPXU/XIgXP7OYktI/AAAAAAAAGbE/Szn99tjxj4I-YurqUYRh27LhjInqY_nBwCK4BGAYYCw/s1600/f9115.jpeg&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;426&quot; src=&quot;https://2.bp.blogspot.com/-PtX8doJgPXU/XIgXP7OYktI/AAAAAAAAGbE/Szn99tjxj4I-YurqUYRh27LhjInqY_nBwCK4BGAYYCw/s640/f9115.jpeg&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;DS9 Enhanced 1080p definition (click to enlarge)&lt;/em&gt;&lt;/p&gt;

&lt;div&gt;&lt;em&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-0SfYGyv6kaM/XIgfDcWAjhI/AAAAAAAAGbk/13Zy4dbjK60-2a1r26zrmpeP8UvHPuzmwCK4BGAYYCw/s1600/thumb0636.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;432&quot; src=&quot;https://4.bp.blogspot.com/-0SfYGyv6kaM/XIgfDcWAjhI/AAAAAAAAGbk/13Zy4dbjK60-2a1r26zrmpeP8UvHPuzmwCK4BGAYYCw/s640/thumb0636.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/em&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Original 480p definition (click to enlarge)&lt;/em&gt;&lt;/p&gt;

&lt;div&gt;&lt;em&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-hYxS5AigRyU/XIge5wBE8aI/AAAAAAAAGbc/hkDXHf54d6wKN-rO9nmmoGJiuC8O7WkhwCK4BGAYYCw/s1600/thumb0636.jpeg&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; height=&quot;432&quot; src=&quot;https://2.bp.blogspot.com/-hYxS5AigRyU/XIge5wBE8aI/AAAAAAAAGbc/hkDXHf54d6wKN-rO9nmmoGJiuC8O7WkhwCK4BGAYYCw/s640/thumb0636.jpeg&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/em&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;DS9 Enhanced 1080p definition (click to enlarge)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These still frames showed promise. In the first set of images, the maintenance crewmen in their spacesuits were nothing more than a few pixely blobs. The upscaling process turned the blobs into much more defined figures&lt;/p&gt;

&lt;p&gt;The close-up of the hand also improved. The creases and folds of the fingers and hand look much more detailed, and the baseball really shows off its sheen and the intricate stitching.&lt;/p&gt;
&lt;h2&gt;Moving Images&lt;/h2&gt;
&lt;p&gt;The real test, however, was going to be if the upscaling process held up with a sequence of frames aka as a video. Would there be artifacts or other unsightly issues? AI Gigapixel was after all made for upscaling single images so it wouldn't take into account the relation between the individual frames of a moving image.&lt;/p&gt;

&lt;p&gt;So I set about upscaling a portion of an episode. I settled on the season six episode: &quot;Sacrifice of Angels&quot;. A great Dominion War episode that had both epic space battles and more personal face-to-face moments. And so I set out to work.&lt;/p&gt;

&lt;p&gt;I will go into greater detail about my process in a future blog post, but it took me about two days to get everything extracted, upscaled and put it back together in a way that was pleasing. This resulted only in the first five minutes of the episode being done (the episode recap, the opening scene, and the intro). Still pretty good time for a mid-to-high end PC with software that isn't just available to professionals.&lt;/p&gt;

&lt;p&gt;The result left me pretty awestruck. It looked better than I had hoped. No weird issues or anything. It looked pretty much like an HD version of DS9. Since (moving) pictures are worth more than a thousand words, here are two comparison videos that show off the improvement I was able to get with this machine learning based upscaling technique.&lt;/p&gt;

&lt;div readability=&quot;7&quot;&gt;
&lt;p&gt;The first shows off the before-and-after situation with still frames.&lt;/p&gt;
&lt;/div&gt;


&lt;p&gt;The second puts the two videos side by side. Take note how much clearer and sharper the Enhanced version looks.&lt;/p&gt;


&lt;div readability=&quot;12&quot;&gt;I highly recommend watching all these videos through your YouTube app on your TV if possible. It gives you more of a sense of how it would feel watching an enhanced DS9 on your TV.&lt;p&gt;Comparisons are all well and good, but what does it look like if you were to watch it normally? Below is a video of the first five minutes of the episode in full 1080p:&lt;/p&gt;&lt;/div&gt;

&lt;h2&gt;What About 4K?&lt;/h2&gt;
&lt;div readability=&quot;16&quot;&gt;Honestly, I don't know. While I can upscale the image to a 4K resolution, I don't have a TV or monitor with a 4K native resolution to see if it looks better. I have nonetheless made this video and I am interested to hear from people with people with 4K equipment if it looks better over the 1080p version of the intro.&lt;p&gt;This nearly melted my computer, as it is a lot more intense to upscale than 1080p so I'll stick to this single video for 4K examples of DS9 Enhanced.&lt;/p&gt;&lt;/div&gt;

&lt;div&gt;
&lt;p&gt;&lt;iframe allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/49oj2JUtn0A&quot; width=&quot;560&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;h2&gt;What's Next?&lt;/h2&gt;
&lt;div readability=&quot;24&quot;&gt;Since I do not own DS9, I can not just do what I want with it. While I would love to release full episodes, this is just not legally possible. These videos serve more as a proof of concept for CBS to look into machine learning and neural networks to help remaster DS9 and move it a bit closer to the HD era.&lt;p&gt;Imagine what a real team could do, with more powerful equipment, custom trained neural networks (perhaps training the network on TNG vs. TNG Remastered images) and access to the original SD files instead of a DVDRip like me.&lt;/p&gt;&lt;p&gt;What I will do is go into further detail about my process, which will be the subject of a future blog.&lt;/p&gt;&lt;p&gt;Let me know what you think.&lt;/p&gt;&lt;/div&gt;

</description>
<pubDate>Thu, 21 Mar 2019 15:49:24 +0000</pubDate>
<dc:creator>donbox</dc:creator>
<og:url>https://captrobau.blogspot.com/2019/03/remastering-star-trek-deep-space-nine.html</og:url>
<og:title>Remastering Star Trek: Deep Space Nine With Machine Learning</og:title>
<og:description>The site for the Remako Mod, a Final Fantasy VII mod that uses AI to improve the graphics of this classic JRPG.</og:description>
<og:image>https://4.bp.blogspot.com/-iOeQ-HxfULM/XIhCDO6J5fI/AAAAAAAAGbw/EptoBlhqLIc34OW9jKJk4DjH9CFvE6lLACK4BGAYYCw/w1200-h630-p-k-no-nu/logo.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://captrobau.blogspot.com/2019/03/remastering-star-trek-deep-space-nine.html</dc:identifier>
</item>
<item>
<title>Facebook Stored Hundreds of Millions of User Passwords in Plain Text for Years</title>
<link>https://krebsonsecurity.com/2019/03/facebook-stored-hundreds-of-millions-of-user-passwords-in-plain-text-for-years/</link>
<guid isPermaLink="true" >https://krebsonsecurity.com/2019/03/facebook-stored-hundreds-of-millions-of-user-passwords-in-plain-text-for-years/</guid>
<description>&lt;p&gt;Hundreds of millions of &lt;strong&gt;Facebook&lt;/strong&gt; users had their account passwords stored in plain text and searchable by thousands of Facebook employees — in some cases going back to 2012, KrebsOnSecurity has learned. Facebook says an ongoing investigation has so far found no indication that employees have abused access to this data.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-30789&quot; src=&quot;https://krebsonsecurity.com/wp-content/uploads/2015/04/facebook_dislike_china_-_Google_Search.png&quot; alt=&quot;&quot; width=&quot;392&quot; height=&quot;362&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Facebook is probing a series of security failures in which employees built applications that logged unencrypted password data for Facebook users and stored it in plain text on internal company servers. That’s according to a senior Facebook employee who is familiar with the investigation and who spoke on condition of anonymity because they were not authorized to speak to the press.&lt;/p&gt;
&lt;p&gt;The Facebook source said the investigation so far indicates between 200 million and 600 million Facebook users may have had their account passwords stored in plain text and searchable by more than 20,000 Facebook employees. The source said Facebook is still trying to determine how many passwords were exposed and for how long, but so far the inquiry has uncovered archives with plain text user passwords dating back to 2012.&lt;/p&gt;
&lt;p&gt;My Facebook insider said access logs showed some 2,000 engineers or developers made approximately nine million internal queries for data elements that contained plain text user passwords.&lt;/p&gt;
&lt;p&gt;“The longer we go into this analysis the more comfortable the legal people [at Facebook] are going with the lower bounds” of affected users, the source said. “Right now they’re working on an effort to reduce that number even more by only counting things we have currently in our data warehouse.”&lt;/p&gt;
&lt;p&gt;In an interview with KrebsOnSecurity, Facebook software engineer &lt;strong&gt;Scott Renfro&lt;/strong&gt; said the company wasn’t ready to talk about specific numbers — such as the number of Facebook employees who could have accessed the data.&lt;/p&gt;
&lt;p&gt;Renfro said the company planned to alert affected Facebook users, but that no password resets would be required.&lt;/p&gt;
&lt;p&gt;“We’ve not found any cases so far in our investigations where someone was looking intentionally for passwords, nor have we found signs of misuse of this data,” Renfro said. “In this situation what we’ve found is these passwords were inadvertently logged but that there was no actual risk that’s come from this. We want to make sure we’re reserving those steps and only force a password change in cases where there’s definitely been signs of abuse.”&lt;/p&gt;
&lt;p&gt;A written statement from Facebook provided to KrebsOnSecurity says the company expects to notify “hundreds of millions of Facebook Lite users, tens of millions of other Facebook users, and tens of thousands of Instagram users.” Facebook Lite is a version of Facebook designed for &lt;a href=&quot;https://www.howtogeek.com/348076/what%E2%80%99s-the-difference-between-facebook-and-facebook-lite/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;low speed connections and low-spec phones&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Both &lt;a href=&quot;https://www.zdnet.com/article/github-says-bug-exposed-account-passwords/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Github&lt;/a&gt; and &lt;a href=&quot;https://krebsonsecurity.com/2018/05/twitter-to-all-users-change-your-password-now/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Twitter&lt;/a&gt; were forced to admit similar stumbles in recent months, but in both of those cases the plain text user passwords were available to a relatively small number of people within those organizations, and for far shorter periods of time.&lt;/p&gt;
&lt;p&gt;Renfro said the issue first came to light in January 2019 when security engineers reviewing some new code noticed passwords were being inadvertently logged in plain text.&lt;/p&gt;
&lt;p&gt;“This prompted the team to set up a small task force to make sure we did a broad-based review of anywhere this might be happening,” Renfro said. “We have a bunch of controls in place to try to mitigate these problems, and we’re in the process of investigating long-term infrastructure changes to prevent this going forward. We’re now reviewing any logs we have to see if there has been abuse or other access to that data.”&lt;/p&gt;
&lt;p&gt;Facebook’s password woes come amid a tough month for the social network. Last week, &lt;em&gt;The New York Times&lt;/em&gt; &lt;a href=&quot;https://www.nytimes.com/2019/03/13/technology/facebook-data-deals-investigation.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reported&lt;/a&gt; that federal prosecutors are conducting a criminal investigation into data deals Facebook struck with some of the world’s largest tech companies.&lt;/p&gt;
&lt;p&gt;Earlier in March, Facebook &lt;a href=&quot;https://techcrunch.com/2019/03/03/facebook-phone-number-look-up/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;came under fire&lt;/a&gt; from security and privacy experts for using phone numbers provided for security reasons — like two-factor authentication — for &lt;a href=&quot;https://twitter.com/i/web/status/1101402001907372032&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;other things&lt;/a&gt; (like marketing, advertising and making users searchable by their phone numbers across the social network’s different platforms).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update, 11:43 a.m.:&lt;/strong&gt; Facebook has posted a statement about this incident &lt;a href=&quot;https://newsroom.fb.com/news/2019/03/keeping-passwords-secure/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;mid-banner&quot;&gt;&lt;a href=&quot;https://www.akamai.com/us/en/security.jsp?utm_source=krebsonsecurity&amp;amp;utm_medium=display&amp;amp;utm_id=F-MC-44701&amp;amp;utm_campaign=unifiedsecurity_digital_2019&amp;amp;utm_content=unifiedsecurity_global&amp;amp;utm_term=unifiedsecurity_ros&quot;&gt;&lt;img src=&quot;https://krebsonsecurity.com/b-akamai/15.jpg&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;small&quot;&gt;Tags: &lt;a href=&quot;https://krebsonsecurity.com/tag/facebook/&quot; rel=&quot;tag&quot;&gt;Facebook&lt;/a&gt;, &lt;a href=&quot;https://krebsonsecurity.com/tag/plaintext-passwords/&quot; rel=&quot;tag&quot;&gt;plaintext passwords&lt;/a&gt;, &lt;a href=&quot;https://krebsonsecurity.com/tag/scott-renfro/&quot; rel=&quot;tag&quot;&gt;Scott Renfro&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;postmetadata alt&quot;&gt;&lt;small&gt;This entry was posted on Thursday, March 21st, 2019 at 11:17 am and is filed under &lt;a href=&quot;https://krebsonsecurity.com/category/sunshine/&quot; rel=&quot;category tag&quot;&gt;A Little Sunshine&lt;/a&gt;, &lt;a href=&quot;https://krebsonsecurity.com/category/comingstorm/&quot; rel=&quot;category tag&quot;&gt;The Coming Storm&lt;/a&gt;. You can follow any comments to this entry through the &lt;a href=&quot;https://krebsonsecurity.com/2019/03/facebook-stored-hundreds-of-millions-of-user-passwords-in-plain-text-for-years/feed/&quot;&gt;RSS 2.0&lt;/a&gt; feed. You can skip to the end and leave a comment. Pinging is currently not allowed.&lt;/small&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 21 Mar 2019 15:21:38 +0000</pubDate>
<dc:creator>snaky</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://krebsonsecurity.com/2019/03/facebook-stored-hundreds-of-millions-of-user-passwords-in-plain-text-for-years/</dc:identifier>
</item>
</channel>
</rss>