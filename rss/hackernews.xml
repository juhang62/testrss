<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Asahi Linux: Linux on Apple Silicon project</title>
<link>https://asahilinux.org/</link>
<guid isPermaLink="true" >https://asahilinux.org/</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://asahilinux.org/&quot;&gt;https://asahilinux.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=25649719&quot;&gt;https://news.ycombinator.com/item?id=25649719&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 359&lt;/p&gt;
&lt;p&gt;# Comments: 235&lt;/p&gt;
</description>
<pubDate>Tue, 05 Jan 2021 19:18:17 +0000</pubDate>
<dc:creator>jackhalford</dc:creator>
<og:image>https://asahilinux.org/img/AsahiLinux_logomark_256px.png</og:image>
<og:title>Asahi Linux</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://asahilinux.org/</dc:identifier>
</item>
<item>
<title>DALL·E: Creating Images from Text</title>
<link>https://openai.com/blog/dall-e/</link>
<guid isPermaLink="true" >https://openai.com/blog/dall-e/</guid>
<description>&lt;p&gt;DALL·E is a 12-billion parameter version of &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT-3&lt;/a&gt; trained to generate images from text descriptions, using a dataset of text–image pairs. We’ve found that it has a diverse set of capabilities, including creating anthropomorphized versions of animals and objects, combining unrelated concepts in plausible ways, rendering text, and applying transformations to existing images.&lt;/p&gt;&lt;div class=&quot;mt-1.75 mb-1&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 mb-1.25&quot;&gt;
&lt;div class=&quot;de-hero-wrap transition-opacity h-100&quot;&gt;
&lt;div class=&quot;h-100 d-flex flex-column justify-content-between&quot;&gt;
&lt;div class=&quot;de-hero mb-0.25 h-100 d-flex flex-column justify-content-between&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;mb-0.5&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;p&gt;an illustration of a baby daikon radish in a tutu walking a dog&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples&quot;&gt;
&lt;p&gt;AI-generated images&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;View more or edit prompt&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 mb-1.25&quot;&gt;
&lt;div class=&quot;de-hero-wrap transition-opacity h-100&quot;&gt;
&lt;div class=&quot;h-100 d-flex flex-column justify-content-between&quot;&gt;
&lt;div class=&quot;de-hero mb-0.25 h-100 d-flex flex-column justify-content-between&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;mb-0.5&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;p&gt;an armchair in the shape of an avocado […]&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples&quot;&gt;
&lt;p&gt;AI-generated images&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;View more or edit prompt&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 mb-1.25&quot;&gt;
&lt;div class=&quot;de-hero-wrap transition-opacity h-100&quot;&gt;
&lt;div class=&quot;h-100 d-flex flex-column justify-content-between&quot;&gt;
&lt;div class=&quot;de-hero mb-0.25 h-100 d-flex flex-column justify-content-between&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;mb-0.5&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;p&gt;a store front that has the word ‘openai’ written on it […]&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples&quot;&gt;
&lt;p&gt;AI-generated images&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;View more or edit prompt&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 mb-1.25&quot;&gt;
&lt;div class=&quot;de-hero-wrap transition-opacity h-100&quot;&gt;
&lt;div class=&quot;h-100 d-flex flex-column justify-content-between&quot;&gt;
&lt;div class=&quot;de-hero mb-0.25 h-100 d-flex flex-column justify-content-between&quot; readability=&quot;31&quot;&gt;
&lt;div class=&quot;mb-0.5&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Text and image prompt&lt;/p&gt;
&lt;p&gt;the exact same cat on the top as a sketch on the bottom&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples&quot;&gt;
&lt;p&gt;AI-generated images&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;View more or edit prompt&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GPT-3 showed that language can be used to instruct a large neural network to perform a variety of text generation tasks. &lt;a href=&quot;https://openai.com/blog/image-gpt&quot;&gt;Image GPT&lt;/a&gt; showed that the same type of neural network can also be used to generate images with high fidelity. We extend these findings to show that manipulating visual concepts through language is now within reach.&lt;/p&gt;
&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;p&gt;Like GPT-3, DALL·E is a transformer language model. It receives both the text and the image as a single stream of data containing up to 1280 tokens, and is trained using maximum likelihood to generate all of the tokens, one after another. This training procedure allows DALL·E to not only generate an image from scratch, but also to regenerate any rectangular region of an existing image that extends to the bottom-right corner, in a way that is consistent with the text prompt.&lt;/p&gt;
&lt;p&gt;We recognize that work involving generative models has the potential for significant, broad societal impacts. In the future, we plan to analyze how models like DALL·E relate to societal issues like economic impact on certain work processes and professions, the potential for bias in the model outputs, and the longer term ethical challenges implied by this technology.&lt;/p&gt;
&lt;h3 id=&quot;capabilities&quot;&gt;Capabilities&lt;/h3&gt;
&lt;p&gt;We find that DALL·E is able to create plausible images for a great variety of sentences that explore the compositional structure of language. We illustrate this using a series of interactive visuals in the next section. The samples shown for each caption in the visuals are obtained by taking the top 32 of 512 after reranking with &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;CLIP&lt;/a&gt;, but we do not use any manual cherry-picking, aside from the thumbnails and standalone images that appear outside.&lt;/p&gt;
&lt;h4 id=&quot;controllingattributes&quot;&gt;Controlling attributes&lt;/h4&gt;
&lt;p&gt;We test DALL·E’s ability to modify several of an object’s attributes, as well as the number of times that it appears.&lt;/p&gt;
&lt;p&gt;Click to edit text prompt or view more AI-generated images&lt;/p&gt;
&lt;div id=&quot;shape&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a pentagonal green clock. a green clock in the shape of a pentagon.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/shape/209f951d45fd17b3ad1134466d9946c7_26.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;22&quot;&gt;We find that DALL·E can render familiar objects in polygonal shapes that are sometimes unlikely to occur in the real world. For some objects, such as “picture frame” and “plate,” DALL·E can reliably draw the object in any of the polygonal shapes except heptagon. For other objects, such as “manhole cover” and “stop sign,” DALL·E’s success rate for more unusual shapes, such as “pentagon,” is considerably lower.&lt;p&gt;For several of the visuals in this post, we find that repeating the caption, sometimes with alternative phrasings, improves the consistency of the results.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;material&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a cube made of porcupine. a cube with the texture of a porcupine.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/material/YSBjdWJlIG1hZGUgb2YgcG9yY3VwaW5lLiBhIGN1YmUgd2l0aCB0aGUgdGV4dHVyZSBvZiBhIHBvcmN1cGluZS4=_1.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;We find that DALL·E can map the textures of various plants, animals, and other objects onto three dimensional solids. As in the preceding visual, we find that repeating the caption with alternative phrasing improves the consistency of the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;multiple-copies&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a collection of glasses is sitting on a table&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/multiple_copies/YSBjb2xsZWN0aW9uIG9mIGdsYXNzZXMgaXMgc2l0dGluZyBvbiBhIHRhYmxl_22.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;15&quot;&gt;
&lt;p&gt;We find that DALL·E is able to draw multiple copies of an object when prompted to do so, but is unable to reliably count past three. When prompted to draw nouns for which there are multiple meanings, such as “glasses,” “chips,” and “cups” it sometimes draws both interpretations, depending on the plural form that is used.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;drawingmultipleobjects&quot;&gt;Drawing multiple objects&lt;/h4&gt;
&lt;p&gt;Simultaneously controlling multiple objects, their attributes, and their spatial relationships presents a new challenge. For example, consider the phrase “a hedgehog wearing a red hat, yellow gloves, blue shirt, and green pants.” To correctly interpret this sentence, DALL·E must not only correctly compose each piece of apparel with the animal, but also form the associations (hat, red), (gloves, yellow), (shirt, blue), and (pants, green) without mixing them up. We test DALL·E’s ability to do this for relative positioning, stacking objects, and controlling multiple attributes.&lt;/p&gt;
&lt;div id=&quot;relative-positions&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a small red block sitting on a large green block&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/relative_positions/YSBzbWFsbCByZWQgYmxvY2sgc2l0dGluZyBvbiBhIGxhcmdlIGdyZWVuIGJsb2Nr_25.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16&quot;&gt;
&lt;p&gt;We find that DALL·E correctly responds to some types of relative positions, but not others. The choices “sitting on” and “standing in front of” sometimes appear to work, “sitting below,” “standing behind,” “standing left of,” and “standing right of” do not. DALL·E also has a lower success rate when asked to draw a large object sitting on top of a smaller one, when compared to the other way around.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;stacking-snap&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;a stack of 3 cubes. a red cube is on the top, sitting on a green cube. the green cube is in the middle, sitting on a blue cube. the blue cube is on the bottom.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/stacking_snap/YSBzdGFjayBvZiAzIGN1YmVzLiBhIHJlZCBjdWJlIGlzIG9uIHRoZSB0b3AsIHNpdHRpbmcgb24gYSBncmVlbiBjdWJlLiB0aGUgZ3JlZW4gY3ViZSBpcyBpbiB0aGUgbWlkZGxlLCBzaXR0aW5nIG9uIGEgYmx1ZSBjdWJlLiB0aGUgYmx1ZSBjdWJlIGlzIG9uIHRoZSBib3R0b20u_28.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;We find that DALL·E typically generates an image with one or two of the objects having the correct colors. However, only a few samples for each setting tend to have exactly three objects colored precisely as specified.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;multiple-attributes&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;an emoji of a baby penguin wearing a blue hat, red gloves, green shirt, and yellow pants&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/multiple_attributes/YW4gZW1vamkgb2YgYSBiYWJ5IHBlbmd1aW4gd2VhcmluZyBhIGJsdWUgaGF0LCByZWQgZ2xvdmVzLCBncmVlbiBzaGlydCwgYW5kIHllbGxvdyBwYW50cw==_3.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;We find that DALL·E typically generates an image with two or three articles of clothing having the correct colors. However, only a few of the samples for each setting tend to have all four articles of clothing with the specified colors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;While DALL·E does offer some level of controllability over the attributes and positions of a small number of objects, the success rate can depend on how the caption is phrased. As more objects are introduced, DALL·E is prone to confusing the associations between the objects and their colors, and the success rate decreases sharply. We also note that DALL·E is brittle with respect to rephrasing of the caption in these scenarios: alternative, semantically equivalent captions often yield no correct interpretations.&lt;/p&gt;
&lt;h4 id=&quot;visualizingperspectiveandthreedimensionality&quot;&gt;Visualizing perspective and three-dimensionality&lt;/h4&gt;
&lt;p&gt;We find that DALL·E also allows for control over the viewpoint of a scene and the 3D style in which a scene is rendered.&lt;/p&gt;
&lt;div id=&quot;camera-angles&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;an extreme close-up view of a capybara sitting in a field&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/camera_angles/YW4gZXh0cmVtZSBjbG9zZS11cCB2aWV3IG9mIGEgY2FweWJhcmEgc2l0dGluZyBpbiBhIGZpZWxk_3.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;We find that DALL·E can draw each of the animals in a variety of different views. Some of these views, such as “aerial view” and “rear view,” require knowledge of the animal’s appearance from unusual angles. Others, such as “extreme close-up view,” require knowledge of the fine-grained details of the animal’s skin or fur.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;three_d_views&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a capybara made of voxels sitting in a field&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/three_d_views/YSBjYXB5YmFyYSBtYWRlIG9mIHZveGVscyBzaXR0aW5nIGluIGEgZmllbGQ=_6.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;We find that DALL·E is often able to modify the surface of each of the animals according to the chosen 3D style, such as “claymation” and “made of voxels,” and render the scene with plausible shading depending on the location of the sun. The “x-ray” style does not always work reliably, but it shows that DALL·E can sometimes orient the bones within the animal in plausible (though not anatomically correct) configurations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To push this further, we test DALL·E’s ability to repeatedly draw the head of a well-known figure at each angle from a sequence of equally spaced angles, and find that we can recover a smooth animation of the rotating head.&lt;/p&gt;
&lt;div id=&quot;rotating-head&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a photograph of a bust of homer&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/rotating_head/1980579866_1.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;We prompt DALL·E with both a caption describing a well-known figure and the top region of an image showing a hat drawn at a particular angle. Then, we ask DALL·E to complete the remaining part of the image given this contextual information. We do this repeatedly, each time rotating the hat a few more degrees, and find that we are able to recover smooth animations of several well-known figures, with each frame respecting the precise specification of angle and ambient lighting.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;DALL·E appears to be able to apply some types of optical distortions to scenes, as we see with the options “fisheye lens view” and “a spherical panorama.” This motivated us to explore its ability to generate reflections.&lt;/p&gt;
&lt;div id=&quot;rotating-mirror&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;a plain white cube looking at its own reflection in a mirror. a plain white cube gazing at itself in a mirror.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/rotating_mirror/4070014681_3.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;Similar to what was done before, we prompt DALL·E to complete the bottom-right corners of a sequence of frames, each of which contains a mirror and reflective floor. While the reflection in the mirror usually resembles the object outside of it, it often does not render the reflection in a physically correct way. By contrast, the reflection of an object drawn on a reflective floor is typically more plausible.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;visualizinginternalandexternalstructure&quot;&gt;Visualizing internal and external structure&lt;/h4&gt;
&lt;p&gt;The samples from the “extreme close-up view” and “x-ray” style led us to further explore DALL·E’s ability to render internal structure with cross-sectional views, and external structure with macro photographs.&lt;/p&gt;
&lt;div id=&quot;cross-sections&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a cross-section view of a walnut&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/cross_sections/YSBjcm9zcy1zZWN0aW9uIHZpZXcgb2YgYSB3YWxudXQ=_0.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;We find that DALL·E is able to draw the interiors of several different kinds of objects.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;macro-photos&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a macro photograph of brain coral&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/macro_photos/YSBtYWNybyBwaG90b2dyYXBoIG9mIGJyYWluIGNvcmFs_2.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;We find that DALL·E is able to draw the fine-grained external details of several different kinds of objects. These details are only apparent when the object is viewed up close.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;inferringcontextualdetails&quot;&gt;Inferring contextual details&lt;/h4&gt;
&lt;p&gt;The task of translating text to images is underspecified: a single caption generally corresponds to an infinitude of plausible images, so the image is not uniquely determined. For instance, consider the caption “a painting of a capybara sitting on a field at sunrise.” Depending on the orientation of the capybara, it may be necessary to draw a shadow, though this detail is never mentioned explicitly. We explore DALL·E’s ability to resolve underspecification in three cases: changing style, setting, and time; drawing the same object in a variety of different situations; and generating an image of an object with specific text written on it.&lt;/p&gt;
&lt;div id=&quot;location-and-setting&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a painting of a capybara sitting in a field at sunrise&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/location_and_setting/YSBwYWludGluZyBvZiBhIGNhcHliYXJhIHNpdHRpbmcgaW4gYSBmaWVsZCBhdCBzdW5yaXNl_1.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;We find that DALL·E is able to render the same scene in a variety of different styles, and can adapt the lighting, shadows, and environment based on the time of day or season.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id=&quot;text-rendering&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;a store front that has the word ‘openai’ written on it. a store front that has the word ‘openai’ written on it. a store front that has the word ‘openai’ written on it. ‘openai’ store front.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/text_rendering/e45259de88f6361db852504739eb9255_4.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;21&quot;&gt;We find that DALL·E is sometimes able to render text and adapt the writing style to the context in which it appears. For example, “a bag of chips” and “a license plate” each requires different types of fonts, and “a neon sign” and “written in the sky” require the appearance of the letters to be changed.&lt;p&gt;Generally, the longer the string that DALL·E is prompted to write, the lower the success rate. We find that the success rate improves when parts of the caption are repeated. Additionally, the success rate sometimes improves as the sampling temperature for the image is decreased, although the samples become simpler and less realistic.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;With varying degrees of reliability, DALL·E provides access to a subset of the capabilities of a 3D rendering engine via natural language. It can independently control the attributes of a small number of objects, and to a limited extent, how many there are, and how they are arranged with respect to one another. It can also control the location and angle from which a scene is rendered, and can generate known objects in compliance with precise specifications of angle and lighting conditions.&lt;/p&gt;
&lt;p&gt;Unlike a 3D rendering engine, whose inputs must be specified unambiguously and in complete detail, DALL·E is often able to “fill in the blanks” when the caption implies that the image must contain a certain detail that is not explicitly stated.&lt;/p&gt;
&lt;h4 id=&quot;applicationsofprecedingcapabilities&quot;&gt;Applications of preceding capabilities&lt;/h4&gt;
&lt;p&gt;Next, we explore the use of the preceding capabilities for fashion and interior design.&lt;/p&gt;
&lt;div id=&quot;male-fashion-design&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a male mannequin dressed in an orange and black flannel shirt&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/male_fashion_design/4209119978_14.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 col-md-9 col-lg-8 col-xl-6&quot;&gt;
&lt;div class=&quot;position-xl-relative no-select line-height-1.3&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://cdn.openai.com/dall-e/v2/image_prompts/male_fashion_design.png&quot; class=&quot;col-4 col-sm-3 col-md-2.4&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;22&quot;&gt;We explore DALL·E’s ability to render male mannequins in a variety of different outfits. When prompted with two colors, e.g., “an orange and white bomber jacket” and “an orange and black turtleneck sweater,” DALL·E often exhibits a range of possibilities for how both colors can be used for the same article of clothing.&lt;p&gt;DALL·E also seems to occasionally confuse less common colors with other neighboring shades. For example, when prompted to draw clothes in “navy,” DALL·E sometimes uses lighter shades of blue, or shades very close to black. Similarly, DALL·E sometimes confuses “olive” with shades of brown or brighter shades of green.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;female-fashion-design&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a female mannequin dressed in a black leather jacket and gold pleated skirt&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/female_fashion_design/4812f86ace1b658bdaab902b7822a4fc_28.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 col-md-9 col-lg-8 col-xl-6&quot;&gt;
&lt;div class=&quot;position-xl-relative no-select line-height-1.3&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://cdn.openai.com/dall-e/v2/image_prompts/female_fashion_design.png&quot; class=&quot;col-4 col-sm-3 col-md-2.4&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;We explore DALL·E’s ability to render female mannequins in a variety of different outfits. We find that DALL·E is able to portray unique textures such as the sheen of a “black leather jacket” and “gold” skirts and leggings. As before, we see that DALL·E occasionally confuses less common colors, such as “navy” and “olive,” with other neighboring shades.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;living-room-design&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;a living room with two white armchairs and a painting of the colosseum. the painting is mounted above a modern fireplace.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/living_room_design/9e80c37dfc4d27a820be73bc2e85d655_1.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 col-md-9 col-lg-8 col-xl-6&quot;&gt;
&lt;div class=&quot;position-xl-relative no-select line-height-1.3&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://cdn.openai.com/dall-e/v2/image_prompts/living_room_design.png&quot; class=&quot;col-4 col-sm-3 col-md-2.4&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;We explore DALL·E’s ability to generate images of rooms with several details specified. We find that it can generate paintings of a wide range of different subjects, including real-world locations such as “the colosseum” and fictional characters like “yoda.” For each subject, DALL·E exhibits a variety of interpretations. While the painting is almost always present in the scene, DALL·E sometimes fails to draw the fireplace or the correct number of armchairs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;bedroom-design&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a loft bedroom with a white bed next to a nightstand. there is a fish tank beside the bed.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/bedroom_design/612f04083eb75a32a167e6d26e89e650_2.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;col-12 col-md-9 col-lg-8 col-xl-6&quot;&gt;
&lt;div class=&quot;position-xl-relative no-select line-height-1.3&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://cdn.openai.com/dall-e/v2/image_prompts/bedroom_design.png&quot; class=&quot;col-4 col-sm-3 col-md-2.4&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;We explore DALL·E’s ability to generate bedrooms with several details specified. Despite the fact that we do not tell DALL·E what should go on top of the nightstand or shelf beside the bed, we find that it sometimes decides to place the other specified object on top. As before, we see that it often fails to draw one or more of the specified objects.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;combiningunrelatedconcepts&quot;&gt;Combining unrelated concepts&lt;/h4&gt;
&lt;p&gt;The compositional nature of language allows us to put together concepts to describe both real and imaginary things. We find that DALL·E also has the ability to combine disparate ideas to synthesize objects, some of which are unlikely to exist in the real world. We explore this ability in two instances: transferring qualities from various concepts to animals, and designing products by taking inspiration from unrelated concepts.&lt;/p&gt;
&lt;div id=&quot;animal-concept-transfer&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a snail made of harp. a snail with the texture of a harp.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/animal_concept_transfer/YSBzbmFpbCBtYWRlIG9mIGhhcnAuIGEgc25haWwgd2l0aCB0aGUgdGV4dHVyZSBvZiBhIGhhcnAu_13.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;28&quot;&gt;We find that DALL·E can generate animals synthesized from a variety of concepts, including musical instruments, foods, and household items. While not always successful, we find that DALL·E sometimes takes the forms of the two objects into consideration when determining how to combine them. For example, when prompted to draw “a snail made of harp,” it sometimes relates the pillar of the harp to the spiral of the snail’s shell.&lt;p&gt;In a previous section, we saw that as more objects are introduced into the scene, DALL·E is liable to confuse the associations between the objects and their specified attributes. Here, we see a different sort of failure mode: sometimes, rather than binding some attribute of the specified concept (say, “a faucet”) to the animal (say, “a snail”), DALL·E just draws the two as separate items.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;product-design&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;an armchair in the shape of an avocado. an armchair imitating an avocado.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/product_design/YW4gYXJtY2hhaXIgaW4gdGhlIHNoYXBlIG9mIGFuIGF2b2NhZG8uIGFuIGFybWNoYWlyIGltaXRhdGluZyBhbiBhdm9jYWRvLg==_4.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;14&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;23&quot;&gt;In the preceding visual, we explored DALL·E’s ability to generate fantastical objects by combining two unrelated ideas. Here, we explore its ability to take inspiration from an unrelated idea while respecting the form of the thing being designed, ideally producing an object that appears to be practically functional. We found that prompting DALL·E with the phrases “in the shape of,” “in the form of,” and “in the style of” gives it the ability to do this.&lt;p&gt;When generating some of these objects, such as “an armchair in the shape of an avocado”, DALL·E appears to relate the shape of a half avocado to the back of the chair, and the pit of the avocado to the cushion. We find that DALL·E is susceptible to the same kinds of mistakes mentioned in the previous visual.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;animalillustrations&quot;&gt;Animal illustrations&lt;/h4&gt;
&lt;p&gt;In the previous section, we explored DALL·E’s ability to combine unrelated concepts when generating images of real-world objects. Here, we explore this ability in the context of art, for three kinds of illustrations: anthropomorphized versions of animals and objects, animal chimeras, and emojis.&lt;/p&gt;
&lt;div id=&quot;anthropomorphism&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;an illustration of a baby daikon radish in a tutu walking a dog&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/anthropomorphism/091432009673a3a126fdec860933cdce_26.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;12.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;20&quot;&gt;We find that DALL·E is sometimes able to transfer some human activities and articles of clothing to animals and inanimate objects, such as food items. We include “pikachu” and “wielding a blue lightsaber” to explore DALL·E’s ability to incorporate popular media.&lt;p&gt;We find it interesting how DALL·E adapts human body parts onto animals. For example, when asked to draw a daikon radish blowing its nose, sipping a latte, or riding a unicycle, DALL·E often draws the kerchief, hands, and feet in plausible locations.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;animal-chimeras&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;a professional high quality illustration of a giraffe turtle chimera. a giraffe imitating a turtle. a giraffe made of turtle.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/animal_chimeras/YSBwcm9mZXNzaW9uYWwgaGlnaCBxdWFsaXR5IGlsbHVzdHJhdGlvbiBvZiBhIGdpcmFmZmUgdHVydGxlIGNoaW1lcmEuIGEgZ2lyYWZmZSBpbWl0YXRpbmcgYSB0dXJ0bGUuIGEgZ2lyYWZmZSBtYWRlIG9mIHR1cnRsZS4=_0.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;10&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;15&quot;&gt;We find that DALL·E is sometimes able to combine distinct animals in plausible ways. We include “pikachu” to explore DALL·E’s ability to incorporate knowledge of popular media, and “robot” to explore its ability to generate animal cyborgs. Generally, the features of the second animal mentioned in the caption tend to be dominant.&lt;p&gt;We also find that inserting the phrase “professional high quality” before “illustration” and “emoji” sometimes improves the quality and consistency of the results.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;emojis&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a professional high quality emoji of a lovestruck cup of boba&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/emojis/YSBwcm9mZXNzaW9uYWwgaGlnaCBxdWFsaXR5IGVtb2ppIG9mIGEgbG92ZXN0cnVjayBjdXAgb2YgYm9iYQ==_8.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;We find that DALL·E is sometimes able to transfer some emojis to animals and inanimate objects, such as food items. As in the preceding visual, we find that inserting the phrase “professional high quality” before “emoji” sometimes improves the quality and consistency of the results.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;zeroshotvisualreasoning&quot;&gt;Zero-shot visual reasoning&lt;/h4&gt;
&lt;p&gt;GPT-3 can be instructed to perform many kinds of tasks solely from a description and a cue to generate the answer supplied in its prompt, without any additional training. For example, when prompted with the phrase “here is the sentence ‘a person walking his dog in the park’ translated into French:”, GPT-3 answers “un homme qui promène son chien dans le parc.” This capability is called &lt;em&gt;zero-shot reasoning.&lt;/em&gt; We find that DALL·E extends this capability to the visual domain, and is able to perform several kinds of image-to-image translation tasks when prompted in the right way.&lt;/p&gt;
&lt;div id=&quot;im2im-animal&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;the exact same cat on the top as a sketch on the bottom&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/im2im_animal/555bab3964e21adc8bb8807c556ede1f_12.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;28&quot;&gt;We find that DALL·E is able to apply several kinds of image transformations to photos of animals, with varying degrees of reliability. The most straightforward ones, such as “photo colored pink” and “photo reflected upside-down,” also tend to be the most reliable, although the photo is often not copied or reflected exactly. The transformation “animal in extreme close-up view” requires DALL·E to recognize the breed of the animal in the photo, and render it up close with the appropriate details. This works less reliably, and for several of the photos, DALL·E only generates plausible completions in one or two instances.&lt;p&gt;Other transformations, such as “animal with sunglasses” and “animal wearing a bow tie,” require placing the accessory on the correct part of the animal’s body. Those that only change the color of the animal, such as “animal colored pink,” are less reliable, but show that DALL·E is sometimes capable of segmenting the animal from the background. Finally, the transformations “a sketch of the animal” and “a cell phone case with the animal” explore the use of this capability for illustrations and product design.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;im2im-object&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;the exact same teapot on the top with ’gpt’ written on it on the bottom&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/im2im_object/0d624be9447b6f20cec1b93674708710_1.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16&quot;&gt;
&lt;p&gt;We find that DALL·E is able to apply several different kinds of image transformations to photos of teapots, with varying degrees of reliability. Aside from being able to modify the color of the teapot (e.g., “colored blue”) or its pattern (e.g., “with stripes”), DALL·E can also render text (e.g., “with ‘gpt’ written on it”) and map the letters onto the curved surface of the teapot in a plausible way. With much less reliability, it can also draw the teapot in a smaller size (for the “tiny” option) and in a broken state (for the “broken” option).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We did not anticipate that this capability would emerge, and made no modifications to the neural network or training procedure to encourage it. Motivated by these results, we measure DALL·E’s aptitude for analogical reasoning problems by testing it on Raven’s progressive matrices, a visual IQ test that saw widespread use in the 20th century.&lt;/p&gt;
&lt;div id=&quot;rpm&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a sequence of geometric shapes.&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/rpm/set_b_masked_3.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Example Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16.5&quot;&gt;
&lt;div class=&quot;small-copy color-fg-60 mt-1/3 mt-md-0&quot; readability=&quot;28&quot;&gt;Rather than treating the IQ test a multiple-choice problem as originally intended, we ask DALL·E to complete the bottom-right corner of each image using argmax sampling, and consider its completion to be correct if it is a close visual match to the original.&lt;p&gt;DALL·E is often able to solve matrices that involve continuing simple patterns or basic geometric reasoning, such as those in sets B and C. It is sometimes able to solve matrices that involve recognizing permutations and applying boolean operations, such as those in set D. The instances in set E tend to be the most difficult, and DALL·E gets almost none of them correct.&lt;/p&gt;&lt;p&gt;For each of the sets, we measure DALL·E’s performance on both the original images, and the images with the colors inverted. The inversion of colors should pose no additional difficulty for a human, yet does generally impair DALL·E’s performance, suggesting its capabilities may be brittle in unexpected ways.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;geographicknowledge&quot;&gt;Geographic knowledge&lt;/h4&gt;
&lt;p&gt;We find that DALL·E has learned about geographic facts, landmarks, and neighborhoods. Its knowledge of these concepts is surprisingly precise in some ways and flawed in others.&lt;/p&gt;
&lt;div id=&quot;countries&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a photo of the food of china&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/countries/752777209_15.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;16&quot;&gt;
&lt;p&gt;We test DALL·E’s understanding of simple geographical facts, such as country flags, cuisines, and local wildlife. While DALL·E successfully answers many of these queries, such as those involving national flags, it often reflects superficial stereotypes for choices like “food” and “wildlife,” as opposed to representing the full diversity encountered in the real world.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;neighborhood&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;a photo of alamo square, san francisco, from a street at night&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/neighborhood/3693640717_8.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;We find that DALL·E is sometimes capable of rendering semblances of certain locations in San Francisco. For locations familiar to the authors, such as San Francisco, they evoke a sense of déjà vu—eerie simulacra of streets, sidewalks and cafes that remind us of very specific locations that do not exist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;landmarks&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a photo of san francisco’s golden gate bridge&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/landmarks/2116608761_11.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompts&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;13&quot;&gt;
&lt;p&gt;We can also prompt DALL·E to draw famous landmarks. In fact, we can even dictate when the photo was taken by specifying the first few rows of the sky. When the sky is dark, for example, DALL·E recognizes it is night, and turns on the lights in the buildings.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&quot;temporalknowledge&quot;&gt;Temporal knowledge&lt;/h4&gt;
&lt;p&gt;In addition to exploring DALL·E’s knowledge of concepts that vary over space, we also explore its knowledge of concepts that vary over time.&lt;/p&gt;
&lt;div id=&quot;timelines&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;a photo of a phone from the 20s&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/timelines/1701264660_0.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Image prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;We find that DALL·E has learned about basic stereotypical trends in design and technology over the decades. Technological artifacts appear to go through periods of explosion of change, dramatically shifting for a decade or two, then changing more incrementally, becoming refined and streamlined.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;summaryofapproachandpriorwork&quot;&gt;Summary of approach and prior work&lt;/h3&gt;
&lt;p&gt;DALL·E is a simple decoder-only transformer that receives both the text and the image as a single stream of 1280 tokens—256 for the text and 1024 for the image—and models all of them autoregressively. The attention mask at each of its 64 self-attention layers allows each image token to attend to all text tokens. DALL·E uses the standard causal mask for the text tokens, and sparse attention for the image tokens with either a row, column, or convolutional attention pattern, depending on the layer. We plan to provide more details about the architecture and training procedure in an upcoming paper.&lt;/p&gt;
&lt;p&gt;Text-to-image synthesis has been an active area of research since the pioneering work of Reed et. al, whose approach uses a GAN conditioned on text embeddings. The embeddings are produced by an encoder pretrained using a contrastive loss, not unlike CLIP. StackGAN and StackGAN++ use multi-scale GANs to scale up the image resolution and improve visual fidelity. AttnGAN incorporates attention between the text and image features, and proposes a contrastive text-image feature matching loss as an auxiliary objective. This is interesting to compare to our reranking with CLIP, which is done offline. Other work incorporates additional sources of supervision during training to improve image quality. Finally, work by Nguyen et. al and Cho et. al explores sampling-based strategies for image generation that leverage pretrained multimodal discriminative models.&lt;/p&gt;
&lt;p&gt;Similar to the rejection sampling used in &lt;a href=&quot;https://arxiv.org/abs/1906.00446&quot;&gt;VQVAE-2&lt;/a&gt;, we use &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;CLIP&lt;/a&gt; to rerank the top 32 of 512 samples for each caption in all of the interactive visuals. This procedure can also be seen as a kind of language-guided search, and can have a dramatic impact on sample quality.&lt;/p&gt;
&lt;div id=&quot;clip&quot; class=&quot;de-module&quot; data-expanded=&quot;0&quot;&gt;
&lt;div class=&quot;de-card-wrap js-expand transition-opacity position-relative rounded color-fg bg-bg medium-xsmall-copy line-height-1.3 py-5/12 px-5/12&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;row narrow-gutters&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;an illustration of a baby daikon radish in a tutu walking a dog [caption 1, best 8 of 2048]&lt;/p&gt;
&lt;div class=&quot;col-auto&quot;&gt;
&lt;div class=&quot;bg-fg-2 bg-cover aspect-1/1 position-relative&quot;&gt;&lt;img class=&quot;mb-0 position-absolute trbl-0 w-100 js-lazy&quot; data-src=&quot; https://cdn.openai.com/dall-e/v2/samples/clip/0_top_8_of_2048_rank_5.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;navigatedownwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;de-samples-wrap position-relative full color-fg bg-bg py-4 my-0&quot;&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-0.5 pb-1.5 text-center w-100&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row mb-1.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;Text prompt&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;row&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;col-12 col-xl-3&quot;&gt;
&lt;p&gt;AI-generated&lt;br class=&quot;d-none d-xl-block&quot;/&gt;
images&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;col-12 col-md-3 col-lg-4 col-xl-3&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;Reranking the samples from DALL·E using CLIP can dramatically improve consistency and quality of the samples.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;js-collapse de-collapse transition-opacity position-absolute pt-1.5 pb-0.5 text-center w-100&quot; data-bottom=&quot;1&quot;&gt;
&lt;p&gt;navigateupwide&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 05 Jan 2021 19:08:21 +0000</pubDate>
<dc:creator>todsacerdoti</dc:creator>
<og:type>article</og:type>
<og:title>DALL·E: Creating Images from Text</og:title>
<og:description>We’ve trained a neural network called DALL·E that creates images from text captions for a wide range of concepts expressible in natural language.</og:description>
<og:url>https://openai.com/blog/dall-e/</og:url>
<og:image>https://openai.com/content/images/2021/01/2x-no-mark-1.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://openai.com/blog/dall-e/</dc:identifier>
</item>
<item>
<title>Wasmer 1.0</title>
<link>https://medium.com/wasmer/wasmer-1-0-3f86ca18c043</link>
<guid isPermaLink="true" >https://medium.com/wasmer/wasmer-1-0-3f86ca18c043</guid>
<description>&lt;h2 id=&quot;c41c&quot; class=&quot;es dt du et b eu ev ew ex ey ez fa fb fc fd fe ff fg fh fi fj fk&quot;&gt;&lt;em class=&quot;fl&quot;&gt;Wasmer 1.0 is now generally available&lt;/em&gt;&lt;/h2&gt;
&lt;div class=&quot;fm&quot;&gt;
&lt;div class=&quot;n fn fo fp fq&quot;&gt;
&lt;div class=&quot;o n&quot;&gt;
&lt;div&gt;&lt;a rel=&quot;noopener&quot; href=&quot;https://medium.com/@syrusakbary?source=post_page-----3f86ca18c043--------------------------------&quot;&gt;&lt;img alt=&quot;Syrus Akbary&quot; class=&quot;s fr fs ft&quot; src=&quot;https://miro.medium.com/fit/c/96/96/0*oHXbblUhIK0cuJEt.jpeg&quot; width=&quot;48&quot; height=&quot;48&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;ul class=&quot;&quot;&gt;&lt;li id=&quot;d6ca&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io er&quot;&gt;&lt;em class=&quot;ip&quot;&gt;Runtime and compiler performance is out of this world.&lt;/em&gt;&lt;/li&gt;
&lt;li id=&quot;ef78&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;em class=&quot;ip&quot;&gt;New features include better error handling, a more powerful API, cross-compilation, headless Wasmer and so much more!&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p id=&quot;0538&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;It’s been two years since we published the first line of Wasmer code, the first server-side WebAssembly (Wasm) runtime able to run &lt;a class=&quot;gf iv&quot; rel=&quot;noopener&quot; href=&quot;https://medium.com/@syrusakbary/running-nginx-with-webassembly-6353c02c08ac&quot;&gt;Nginx server-side&lt;/a&gt;.&lt;br/&gt;We believe that WebAssembly will be a &lt;strong class=&quot;hs iw&quot;&gt;crucial component for the future of software execution&lt;/strong&gt; and containerization (not only inside the browser but also outside).&lt;/p&gt;
&lt;p id=&quot;969d&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;By leveraging Wasm for software containerization, we create universal binaries that work anywhere without modification, including operating systems like Linux, macOS, Windows, and also web browsers. Wasm automatically &lt;strong class=&quot;hs iw&quot;&gt;sandboxes applications by default&lt;/strong&gt; for secure execution, shielding the host environment from malicious code, bugs and vulnerabilities in the software it runs. Wasm also provides a lean execution environment enabling Wasmer containers to run in places where &lt;strong class=&quot;hs iw&quot;&gt;Docker containers are too heavy to work&lt;/strong&gt;.&lt;/p&gt;
&lt;p id=&quot;f547&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Today we’re super excited to release Wasmer 1.0, featuring:&lt;/p&gt;
&lt;ul class=&quot;&quot;&gt;&lt;li id=&quot;19e4&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Production-ready performance&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;7f29&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Pluggable Infrastructure&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;746e&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Native Object Engine&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;5001&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Headless Wasmer — ideal for IoT&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;cc1a&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Cross-compilation&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;6cd6&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Super easy-to-use and extensible API&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;b2c5&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Wasm-C-API support&lt;/strong&gt;&lt;/li&gt;
&lt;li id=&quot;7039&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Error handling and debugging&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p id=&quot;afdf&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Now let’s dig deeper into each of these new features!&lt;/p&gt;

&lt;p id=&quot;7d85&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Wasmer 1.0 performance benchmark requires a standalone post. A complete analysis of Wasmer compared with other Wasm runtimes for you to compare and choose what’s best for your project is in the works!&lt;/p&gt;
&lt;p id=&quot;9150&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;But, to give you a sneak peek, &lt;strong class=&quot;hs iw&quot;&gt;the numbers look good, really good&lt;/strong&gt;!&lt;/p&gt;
&lt;h2 id=&quot;33c8&quot; class=&quot;jy iy du et iz jz ka ew jc kb kc ez jf fa kd fc jj fd ke ff jn fg kf fi jr kg er&quot;&gt;Up to 9x faster compilation times with Wasmer 1.0&lt;/h2&gt;
&lt;p id=&quot;c044&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;All our compilers now compile functions in parallel. Thanks to this new strategy, we achieved up to 9x speedup in compilation times across all our compilers.&lt;/p&gt;
&lt;p id=&quot;c7a7&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;These numbers are much easier to understand with a &lt;a href=&quot;https://gist.github.com/syrusakbary/6c28698193fe2efb2294d95fd084f667&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;real-world example comparison&lt;/a&gt;. Here we can see the compilation times for &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;&lt;a href=&quot;https://wapm.io/package/clang#explore&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;clang.wasm&lt;/a&gt;&lt;/code&gt; in Wasmer 0.17.1 vs. Wasmer 1.0:&lt;/p&gt;
&lt;ul class=&quot;&quot;&gt;&lt;li id=&quot;dccc&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io er&quot;&gt;Singlepass: from 18s to 2s (&lt;strong class=&quot;hs iw&quot;&gt;9x speedup&lt;/strong&gt;)&lt;/li&gt;
&lt;li id=&quot;8497&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;Cranelift: from 27s to 9s (&lt;strong class=&quot;hs iw&quot;&gt;3x speedup&lt;/strong&gt;)&lt;/li&gt;
&lt;li id=&quot;83fa&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;LLVM: from 1140s to 117s (&lt;strong class=&quot;hs iw&quot;&gt;9x speedup&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote class=&quot;kl km kn&quot; readability=&quot;7&quot;&gt;
&lt;p id=&quot;ca2f&quot; class=&quot;hq hr ip hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;When using the Wasmer CLI, compilation happens only once and then the result is cached for optimal future runs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p id=&quot;1918&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Extensibility is one of the most critical features of any infrastructure product. One of our most vital capabilities is support for multiple compilers. The ability to select and plug-in the compiler that best meets customer use cases is incredibly powerful. Wasmer 1.0 ships with out-of-the-box support for:&lt;/p&gt;
&lt;ul class=&quot;&quot;&gt;&lt;li id=&quot;b65a&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Singlepass&lt;/strong&gt;: for ultra-fast compilation times not susceptible to JIT-bombs (ideal for &lt;em class=&quot;ip&quot;&gt;blockchains&lt;/em&gt;).&lt;/li&gt;
&lt;li id=&quot;0147&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Cranelift&lt;/strong&gt;: for fast compilation times where minimal optimizations are needed (ideal for &lt;em class=&quot;ip&quot;&gt;development&lt;/em&gt;).&lt;/li&gt;
&lt;li id=&quot;9223&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;LLVM&lt;/strong&gt;: for optimal generated machine code when top performance is a must (ideal for &lt;em class=&quot;ip&quot;&gt;production&lt;/em&gt;).&lt;/li&gt;
&lt;/ul&gt;&lt;p id=&quot;d2a5&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;In addition to compilers, we introduced support for pluggable engines. A WebAssembly engine is an abstraction that decides how to manage the generated assembly-code by the compiler, including how to load and serialize it. Wasmer 1.0 supports the following compiler engines:&lt;/p&gt;
&lt;ul class=&quot;&quot;&gt;&lt;li id=&quot;a424&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;JIT engine&lt;/strong&gt;: it pushes generated code directly into memory.&lt;/li&gt;
&lt;li id=&quot;25e5&quot; class=&quot;hq hr du hs b eu iq hu hv ex ir hx hy hz is ib ic id it if ig ih iu ij ik il im in io er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Native engine&lt;/strong&gt;: it generates native code that can be loaded as a shared object. As a bonus, native engine shared objects and modules are incredibly performant and startup in just &lt;strong class=&quot;hs iw&quot;&gt;a few microseconds&lt;/strong&gt;!&lt;/li&gt;
&lt;/ul&gt;
&lt;p id=&quot;555d&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Wasmer 1.0 introduces “&lt;code class=&quot;dl kh ki kj kk b&quot;&gt;wasmer compile&lt;/code&gt;”, a new command for precompiling Wasm files. Our customers and developers use “&lt;code class=&quot;dl kh ki kj kk b&quot;&gt;wasmer compile --native&lt;/code&gt;” to precompile WebAssembly modules into native object files like &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;.so&lt;/code&gt;, &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;.dylib&lt;/code&gt;, and &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;.dll&lt;/code&gt;. Precompiled objects and modules are compatible with the Wasmer CLI or used with Wasmer Embeddings (Rust, Python, Go, …).&lt;/p&gt;
&lt;p id=&quot;b12a&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;A core advantage of precompiled native objects is that they only need a minimal runtime to &lt;strong class=&quot;hs iw&quot;&gt;run&lt;/strong&gt; compiled modules, while still providing a fully sandboxed environment. Eliminated compilation time allows direct execution of the artifact at blazing-fast startup times.&lt;/p&gt;
&lt;p id=&quot;ba9b&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;First run:&lt;/p&gt;
&lt;pre class=&quot;ko kp kq kr ks kt ku cb&quot;&gt;
&lt;span id=&quot;1997&quot; class=&quot;er jy iy du kk b kv kw kx s ky&quot;&gt;wasmer compile --native python.wasm -o python.so&lt;/span&gt;
&lt;/pre&gt;
&lt;p id=&quot;b5de&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;For standalone execution run:&lt;/p&gt;
&lt;pre class=&quot;ko kp kq kr ks kt ku cb&quot;&gt;
&lt;span id=&quot;dcc2&quot; class=&quot;er jy iy du kk b kv kw kx s ky&quot;&gt;wasmer run python.so&lt;/span&gt;
&lt;/pre&gt;
&lt;p id=&quot;2b82&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Or run embedded (Rust example…&lt;a href=&quot;https://docs.wasmer.io/integrations/examples&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;many others supported&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&quot;ko kp kq kr ks kt ku cb&quot;&gt;
&lt;span id=&quot;9d0c&quot; class=&quot;er jy iy du kk b kv kw kx s ky&quot;&gt;let module = Module::deserialize_from_file(“python.so”);&lt;br/&gt;let instance = Instance::new(module, &amp;amp;wasi_imports);&lt;/span&gt;
&lt;/pre&gt;

&lt;p id=&quot;bc27&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;It’s no secret that IoT devices running at the edge are driving the future of computing.&lt;/strong&gt; However, many devices lack optimal computing hardware or other resources like power, network, and storage. In the past, users shipped their Wasm application with Wasmer and all compilers attached. This practice was suboptimal, especially for IoT and edge computing use cases.&lt;/p&gt;

&lt;p id=&quot;c7c0&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;With Wasmer 1.0, we see Wasm and Wasmer leading the charge in delivering the most lightweight execution environment possible, essential for efficiently running Wasm on IoT devices at the edge. With our newly-added support for AOT (Ahead Of Time) compilation, you can run a “headless” version of Wasmer that weighs just a few hundred kilobytes and runs any precompiled Wasm binary on any device.&lt;/p&gt;

&lt;p id=&quot;5646&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Wasmer 1.0 now features cross-compilation. In the previous versions of Wasmer, natively compiled Wasm applications targeted and ran on the same architecture. With Wasmer 1.0, you can precompile Wasm for an &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;aarch64&lt;/code&gt; architecture from an &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;x86_64&lt;/code&gt; machine and vice-versa.&lt;/p&gt;
&lt;p id=&quot;0f7d&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;On any &lt;code class=&quot;dl kh ki kj kk b&quot;&gt;x86_64&lt;/code&gt; machine run:&lt;/p&gt;
&lt;pre class=&quot;ko kp kq kr ks kt ku cb&quot;&gt;
&lt;span id=&quot;39fd&quot; class=&quot;er jy iy du kk b kv kw kx s ky&quot;&gt;wasmer compile python.wasm -o python-arm.so --native --target=aarch64-linux-gnu&lt;/span&gt;
&lt;/pre&gt;
&lt;p id=&quot;16bd&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Then run the compiled result natively on your ARM machine:&lt;/p&gt;
&lt;pre class=&quot;ko kp kq kr ks kt ku cb&quot;&gt;
&lt;span id=&quot;3c69&quot; class=&quot;er jy iy du kk b kv kw kx s ky&quot;&gt;wasmer run python-arm.so&lt;/span&gt;
&lt;/pre&gt;
&lt;p id=&quot;e962&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Note:&lt;/strong&gt; On the last step, “&lt;code class=&quot;dl kh ki kj kk b&quot;&gt;wasmer run&lt;/code&gt;”, you can also use a headless version of Wasmer that is much more lightweight! (or you can run it embedded as demonstrated before)&lt;/p&gt;


&lt;p id=&quot;4149&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;A primary goal for Wasmer 1.0 is to make APIs very easy to use for developers that have limited knowledge of advanced WebAssembly concepts or the WebAssembly ecosystem. Wasmer 1.0 further improves our API, making it future-resilient by shaping it based on similar structures that the standard Wasm-C-API has.&lt;/p&gt;

&lt;p id=&quot;d3d6&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Our &lt;a href=&quot;https://docs.wasmer.io/integrations/rust&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;documentation&lt;/a&gt; and a complete set of &lt;a href=&quot;https://github.com/wasmerio/wasmer/tree/master/examples&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;examples&lt;/a&gt; enable developers to start using Wasmer in the blink of an eye!&lt;/p&gt;

&lt;p id=&quot;3891&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;As the WebAssembly ecosystem matures, the industry will rightfully push and unite APIs that interact with WebAssembly. Early in our journey, the &lt;a href=&quot;https://github.com/webassembly/wasm-c-api&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;Wasm-C-API&lt;/a&gt; was still very young, so we decided to build an API that closely matched our internal structures.&lt;br/&gt;However, as the industry moves towards a universal API, we’ve decided to adopt it as a part of Wasmer 1.0 as well as contribute into it so everyone can benefit.&lt;/p&gt;
&lt;p id=&quot;d6a0&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Note&lt;/strong&gt;: we are continuing to support the Wasmer-C-API. However, we recommend current users to switch to the standard Wasm-C-API.&lt;/p&gt;

&lt;p id=&quot;aeba&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Wasmer 1.0 completely reimagines and delivers a new and improved error handling feature that lets developers integrate with Wasmer with confidence. Significantly improve the time it takes to build production-ready Wasm applications. For instance, each time an error occurs, know precisely where the error is coming from: is it the Virtual Machine? Is it because there are not enough resources available? Or is it merely an expected error triggered from some of your host functions? Wasmer 1.0 lays the foundation for more advanced error detection and reporting in the future.&lt;/p&gt;

&lt;p id=&quot;f09f&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;In the immortal words of Steve Jobs, “One more thing.”&lt;/p&gt;
&lt;p id=&quot;b7c6&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;Just a few weeks ago, Apple launched its new line of computers with a completely new chipset based on ARM: Apple Silicon.&lt;/p&gt;
&lt;p id=&quot;9065&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;ARM in server-side workloads has an incredibly bright future, and it solidifies the need for universal-binaries across chipsets. We are super happy to announce that Wasmer is the first non-interpreted server-side WebAssembly VM to support Wasm in Apple Silicon!&lt;/p&gt;

&lt;p id=&quot;023c&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Wasmer 1.0 is already being used and tested by companies and individual developers across the globe. We want to thank all of them for helping materialize Wasmer’s vision for pushing Wasm forward.&lt;/p&gt;
&lt;p id=&quot;e179&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;As you have read in this article, Wasmer 1.0 showcases the numerous development cycles and iterations where we improved upon what we learned over the last two years. Wasmer’s current incarnation would not have been possible without companies like Google, Mozilla, Cloudflare, and Fastly (to name a few) pushing WebAssembly forward.&lt;/p&gt;
&lt;p id=&quot;1534&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;If you would like to try Wasmer, you can install the CLI from &lt;a href=&quot;https://wasmer.io/&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;wasmer.io&lt;/a&gt; to run Wasmer standalone or embed Wasmer in your favorite programming languages.&lt;/p&gt;
&lt;div class=&quot;lc ld le lf lg lh&quot;&gt;
&lt;div class=&quot;li n ax&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;lj n lk p ll lm&quot; readability=&quot;7&quot;&gt;
&lt;h2 class=&quot;dv b ln lo bt lp ga gb lq gd ge dt er&quot;&gt;Wasmer - The Universal WebAssembly Runtime&lt;/h2&gt;
&lt;p&gt;
&lt;h3 class=&quot;et b kv fw bt ls ga gb lq gd ge fk&quot;&gt;Wasmer - The Universal WebAssembly Runtime&lt;/h3&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;h4 class=&quot;et b gl fw bt ls ga gb lq gd ge fk&quot;&gt;Install Wasmer&lt;/h4&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;p id=&quot;e111&quot; class=&quot;hq hr du hs b eu jt hu hv ex ju hx hy hz jv ib ic id jw if ig ih jx ij ik il cu er&quot;&gt;Headquartered in San Francisco, CA, Wasmer Inc. is the company behind the popular open-source WebAssembly runtime Wasmer. In addition to the Wasmer runtime, the company has made significant investments in &lt;a href=&quot;https://wapm.io/&quot; class=&quot;gf iv&quot; rel=&quot;noopener nofollow&quot;&gt;WAPM&lt;/a&gt;, the WebAssembly Package Manager, and many other open-source projects in the WebAssembly ecosystem.&lt;/p&gt;
&lt;p id=&quot;284d&quot; class=&quot;hq hr du hs b eu ht hu hv ex hw hx hy hz ia ib ic id ie if ig ih ii ij ik il cu er&quot;&gt;&lt;strong class=&quot;hs iw&quot;&gt;Our mission is to make software universally available&lt;/strong&gt;. We are committed to the open-source community and strive to contribute to developers and companies worldwide to help make Wasmer and WebAssembly a universal standard.&lt;/p&gt;
</description>
<pubDate>Tue, 05 Jan 2021 19:02:01 +0000</pubDate>
<dc:creator>wiqar</dc:creator>
<og:type>article</og:type>
<og:title>Wasmer 1.0</og:title>
<og:description>By leveraging Wasm for software containerization, we create universal binaries that work anywhere without modification, including…</og:description>
<og:url>https://medium.com/wasmer/wasmer-1-0-3f86ca18c043</og:url>
<og:image>https://miro.medium.com/max/1200/1*C4Wc6JS2U9dlcsbW-OjnXA.jpeg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/wasmer/wasmer-1-0-3f86ca18c043</dc:identifier>
</item>
<item>
<title>GitHub is fully available in Iran</title>
<link>https://github.blog/2021-01-05-advancing-developer-freedom-github-is-fully-available-in-iran/</link>
<guid isPermaLink="true" >https://github.blog/2021-01-05-advancing-developer-freedom-github-is-fully-available-in-iran/</guid>
<description>&lt;p&gt;All developers should be free to use GitHub, no matter where they live. At the same time, GitHub respects and abides by US law, which means government sanctions have limited our ability to provide developers in some countries the full range of GitHub services.&lt;/p&gt;
&lt;p&gt;Today we are announcing a breakthrough: we have secured a license from the US government to offer GitHub to developers in Iran. This includes all services for individuals and organizations, private and public, free and paid.&lt;/p&gt;
&lt;p&gt;So what happened here?&lt;/p&gt;
&lt;p&gt;The US has long &lt;a href=&quot;https://home.treasury.gov/policy-issues/office-of-foreign-assets-control-sanctions-programs-and-information&quot;&gt;imposed broad sanctions&lt;/a&gt; on multiple countries, including Iran. These sanctions prohibit any US company from doing business with anyone in a sanctioned country. (These sanctions can also apply to non-US companies whose activities directly or indirectly involve the US, including merely having payments that flow through US banks or payment mechanisms like Visa.)&lt;/p&gt;
&lt;p&gt;And so in 2019, GitHub implemented access restrictions for developers in Iran and several other countries to comply with US sanctions laws. At the same time, in keeping with our goal of making GitHub available to everyone, we also immediately took two other actions:&lt;/p&gt;
&lt;p&gt;First, even as we complied with sanctions, we went to great lengths to &lt;a href=&quot;https://github.blog/2019-09-12-global-software-collaboration-in-the-face-of-sanctions/&quot;&gt;keep as much of GitHub available to as many developers as possible&lt;/a&gt; under US sanctions laws, making public repos available even in sanctioned countries.&lt;/p&gt;
&lt;p&gt;And separately, we took our case to the Office of Foreign Assets Control (OFAC), part of the US Treasury Department, and began a lengthy and intensive process of advocating for broad and open access to GitHub in sanctioned countries.&lt;/p&gt;
&lt;p&gt;Over the course of two years, we were able to demonstrate how developer use of GitHub advances human progress, international communication, and the enduring US foreign policy of promoting free speech and the free flow of information. We are grateful to OFAC for the engagement which has led to this great result for developers.&lt;/p&gt;
&lt;p&gt;We are in the process of rolling back all restrictions on developers in Iran, and reinstating full access to affected accounts. For developers who have questions or need help with their accounts, please visit our &lt;a href=&quot;https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-and-trade-controls&quot;&gt;help page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We still have more work to do. We want every developer to be able to collaborate on GitHub, and we are working with the US government to secure similar licenses for developers in Crimea and Syria as well. Stay tuned.&lt;/p&gt;

</description>
<pubDate>Tue, 05 Jan 2021 18:01:37 +0000</pubDate>
<dc:creator>todsacerdoti</dc:creator>
<og:type>article</og:type>
<og:title>Advancing developer freedom: GitHub is fully available in Iran - The GitHub Blog</og:title>
<og:description>All developers should be free to use GitHub, no matter where they live. At the same time, GitHub respects and abides by US law, which means government sanctions have limited our ability to provide developers</og:description>
<og:url>https://github.blog/2021-01-05-advancing-developer-freedom-github-is-fully-available-in-iran/</og:url>
<og:image>https://github.blog/wp-content/uploads/2019/03/company-twitter.png?fit=1201%2C630</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.blog/2021-01-05-advancing-developer-freedom-github-is-fully-available-in-iran/</dc:identifier>
</item>
<item>
<title>Moral Competence</title>
<link>https://evanjconrad.com/posts/moral-competence</link>
<guid isPermaLink="true" >https://evanjconrad.com/posts/moral-competence</guid>
<description>&lt;p&gt;&lt;em&gt;Since posting, a number of peple have pointed out that &quot;competence&quot; probably isn't the right word here. &quot;Morally focused&quot;, or &quot;morally effective&quot; might be better. This post isn't meant to be harsh, nor is it meant to propose an absolute binary. I hope you'll read it with the kinder tone that was intended.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Last year, we pivoted our YC startup from a socially-good mental health product &lt;a href=&quot;https://github.com/Flaque/quirk&quot;&gt;(Quirk)&lt;/a&gt;, towards a socially-neutral software infrastructure product &lt;a href=&quot;https://www.roomservice.dev/&quot;&gt;(Room Service)&lt;/a&gt;. I couldn't have imagined myself wanting to pivot when we started; our mental health company was deeply tied to my own experience and our genuine desire to help other folks. With the clarity of a year (and a global pandemic) in between the pivot, it's easier to see some of what went wrong.&lt;/p&gt;
&lt;p&gt;What we were missing, and what many social-good founders are missing, is moral competence. &lt;strong&gt;If you want to do good, you actually have to help people.&lt;/strong&gt; Merely attempting to help people is not enough. That doesn't mean that &lt;em&gt;trying&lt;/em&gt; to help people is bad. It's not, but moral good comes from moral competence. And that was something we lacked.&lt;/p&gt;
&lt;p&gt;The morally &lt;em&gt;incompetent&lt;/em&gt; want purpose; they want to be on the front-lines of the helping. But for the morally incompetent, helping people is more important than the folks being helped. They don't offer service, they seek it. The service outranks the outcome. The signature move of the morally incompetent is to be told about existing solutions that they were previously unaware of and then soldier on without any critical examination of any added value they're providing. Others working on the problem are ignored entirely or seen as a threat to their own solution. The morally incompetent are passionate about working on the problem and potentially even solving the problem, so long as they were involved in the solution. For the morally incompetent, it's important that they're helping the right people with the right problem; it would be a failure to help on an entirely different problem. All problems are stack-ranked in the morally incompetent's mind and they need to be working on the one that's most critical to them in particular, regardless of their ability to offer help. For the morally incompetent, it's of &lt;em&gt;critical importance&lt;/em&gt; that they work &lt;em&gt;directly&lt;/em&gt; on the problem, rather than help indirectly. For the morally incompetent, large societal issues are best solved by direct intervention, rather than large societal solutions. Working on the problem indirectly, or from an angle that is not obvious to an outside observer is a failure. For the morally incompetent, large societal problems are unsolved only because they personally haven't purposed a solution.&lt;/p&gt;
&lt;p&gt;It's easy to fall into the trap of moral incompetence, we did with our company. Moral competence and incompetence often looks the same to an outside observer. The world is really good at praising people who are &lt;em&gt;doing&lt;/em&gt; good things well before those things are accomplished. You get hounded for interviews, you win awards, your family and friends congratulate you. And when things &lt;em&gt;aren't&lt;/em&gt; going well, when you're &lt;em&gt;not&lt;/em&gt; helping people, it feels like a betrayal to tell anyone. After all, that would mean no longer helping people with your chosen problem.&lt;/p&gt;
&lt;p&gt;The morally &lt;em&gt;competent&lt;/em&gt; differ in intent. It's more important to cure cancer than it is to be working on curing cancer, to stop climate change than to be working on stopping climate change, to improve mental health than to be working on improving mental health. The morally competent aren't hanging around the hoop, content to be playing the game. The point isn't to be working in the field, it's to solve the problem.&lt;/p&gt;
</description>
<pubDate>Tue, 05 Jan 2021 17:37:22 +0000</pubDate>
<dc:creator>flaque</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://evanjconrad.com/posts/moral-competence</dc:identifier>
</item>
<item>
<title>U.S. Bill H.R.69 introduced – To make daylight savings time permanent</title>
<link>https://www.congress.gov/bill/117th-congress/house-bill/69?s=1&amp;r=7</link>
<guid isPermaLink="true" >https://www.congress.gov/bill/117th-congress/house-bill/69?s=1&amp;r=7</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://www.congress.gov/bill/117th-congress/house-bill/69?s=1&amp;r=7&quot;&gt;https://www.congress.gov/bill/117th-congress/house-bill/69?s=1&amp;r=7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=25646867&quot;&gt;https://news.ycombinator.com/item?id=25646867&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 324&lt;/p&gt;
&lt;p&gt;# Comments: 326&lt;/p&gt;
</description>
<pubDate>Tue, 05 Jan 2021 16:01:15 +0000</pubDate>
<dc:creator>mdm12</dc:creator>
<og:description>Summary of H.R.69 - 117th Congress(2021-2022): To make daylight savings time permanent, and for other purposes.</og:description>
<og:image>https://www.congress.gov/img/opengraph1200by630.jpg</og:image>
<og:title>H.R.69 - 117th Congress(2021-2022): To make daylight savings time permanent, and for other purposes.</og:title>
<og:type>website</og:type>
<og:url>https://www.congress.gov/bill/117th-congress/house-bill/69</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.congress.gov/bill/117th-congress/house-bill/69?s=1&amp;r=7</dc:identifier>
</item>
<item>
<title>Ticketmaster admits it hacked Songkick before it went out of business</title>
<link>https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/</link>
<guid isPermaLink="true" >https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/</guid>
<description>&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2015/08/hacked-640x438.jpg&quot; alt=&quot;Image of ones and zeros with the word &quot; hacked=&quot;&quot; superimposed.=&quot;&quot;/&gt;&lt;aside id=&quot;social-left&quot; class=&quot;social-left&quot; aria-label=&quot;Read the comments or share this article&quot;&gt;
&lt;h4 class=&quot;comment-count-before&quot;&gt;&lt;a title=&quot;117 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/?comments=1&quot;&gt;reader comments&lt;/a&gt;&lt;/h4&gt;
&lt;a title=&quot;117 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/?comments=1&quot;&gt;&lt;span class=&quot;comment-count-number&quot;&gt;140&lt;/span&gt; &lt;span class=&quot;visually-hidden&quot;&gt;with 117 posters participating&lt;/span&gt;&lt;/a&gt;
&lt;div class=&quot;share-links&quot;&gt;
&lt;h4&gt;Share this story&lt;/h4&gt;
&lt;/div&gt;
&lt;/aside&gt;&lt;p&gt;Ticketmaster has agreed to pay a $10 million criminal fine after admitting its employees repeatedly used stolen passwords and other means to hack a rival ticket sales company.&lt;/p&gt;
&lt;p&gt;The fine, which is part of a deferred prosecution agreement Ticketmaster entered with federal prosecutors, resolves criminal charges &lt;a href=&quot;https://www.justice.gov/usao-edny/press-release/file/1349741/download&quot;&gt;filed last week&lt;/a&gt; in federal court in the eastern district of New York. Charges include violations of the Computer Fraud and Abuse Act, computer intrusion for commercial advantage or private financial gain, computer intrusion in furtherance of fraud, conspiracy to commit wire fraud, and wire fraud.&lt;/p&gt;
&lt;p&gt;In the settlement, Ticketmaster admitted that an employee who used to work for a rival company emailed the login credentials for multiple accounts the rival used to manage presale ticket sales. At a San Francisco meeting attended by at least 14 employees of Ticketmaster or its parent company Live Nation, the employee used one set of credentials to log in to an account to demonstrate how it worked.&lt;/p&gt;
&lt;h2&gt;A hack, then a promotion&lt;/h2&gt;
&lt;p&gt;The employee, who wasn’t identified in court documents, later provided Ticketmaster executives with internal and confidential financial documents he had retained from his previous employer. The employee was later promoted to director of client relations and given a raise. Court documents didn’t identify the rival company, but &lt;a href=&quot;https://variety.com/2020/digital/news/ticketmaster-10-million-fine-hack-songkick-competitor-1234877108/&quot;&gt;Variety&lt;/a&gt; reported it was Songkick, which in 2017 &lt;a href=&quot;https://variety.com/2017/biz/news/songkick-live-nation-ticketmaster-hacking-1201989759/&quot;&gt;filed a lawsuit&lt;/a&gt; accusing Ticketmaster of hacking its database. A few months later, Songkick &lt;a href=&quot;https://variety.com/2017/digital/news/songkick-ticketing-operations-to-shut-down-exclusive-1202588162/&quot;&gt;went out of business&lt;/a&gt;.&lt;/p&gt;
&lt;aside class=&quot;ad_wrapper&quot; aria-label=&quot;In Content advertisement&quot;&gt;&lt;span class=&quot;ad_notice&quot;&gt;Advertisement&lt;/span&gt;

&lt;/aside&gt;&lt;p&gt;The charges against Ticketmaster come 26 months after Zeeshan Zaidi, the former head of Ticketmaster’s artist services division, pled guilty in a related case to conspiring to hack the rival company and engage in wired fraud. According to prosecutors, the former rival employee emailed the login credentials to Zaidi and another Ticketmaster employee.&lt;/p&gt;
&lt;p&gt;“When employees walk out of one company and into another, it's illegal for them to take proprietary information with them,” FBI Assistant Director William Sweeney Jr. said in a &lt;a href=&quot;https://www.justice.gov/usao-edny/pr/ticketmaster-pays-10-million-criminal-fine-intrusions-competitor-s-computer-systems-0&quot;&gt;statement&lt;/a&gt;. “Ticketmaster used stolen information to gain an advantage over its competition, and then promoted the employees who broke the law.&quot;&lt;/p&gt;
&lt;p&gt;Besides providing login credentials, the former employee also showed Ticketmaster managers how to exploit a flaw in the URL generation scheme the rival used for unpublished ticketing webpages. To prevent the pages from being accessed by outsiders before they were made public, each one had a unique numerical value. The former employee told his new employer that the values were generated sequentially, and outsiders could use this information to view artist pages while they were still in early draft stages.&lt;/p&gt;
&lt;p&gt;In early 2015, Ticketmaster assigned one of its employees to learn about this system and use it to maintain a spreadsheet listing every ticketing webpage that could be located. Ticketmaster would then identify the rival company’s clients and “attempt to dissuade them from selling tickets through the victim company,” federal prosecutors said. Zaidi, the prosecutors further said, explained that “we’re not supposed to tip anyone off that we have this view into [the victim company’s] activities.”&lt;/p&gt;
&lt;p&gt;Besides paying the $10 million fine, Ticketmaster has also agreed to maintain a compliance and ethics program designed to prevent and detect future hacking and unlawful acquisitions of competitors’ confidential information. Live Nation representatives didn’t respond to a message seeking comment for this post.&lt;/p&gt;

</description>
<pubDate>Tue, 05 Jan 2021 15:56:14 +0000</pubDate>
<dc:creator>cpascal</dc:creator>
<og:url>https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/</og:url>
<og:title>Ticketmaster admits it hacked rival company before it went out of business</og:title>
<og:image>https://cdn.arstechnica.net/wp-content/uploads/2015/08/hacked-640x215.jpg</og:image>
<og:description>Ticketmaster used stolen passwords and URL guessing to access confidential data.</og:description>
<og:type>article</og:type>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arstechnica.com/information-technology/2021/01/ticketmaster-pays-10-million-criminal-fine-for-hacking-a-rival-company/</dc:identifier>
</item>
<item>
<title>New Defaults</title>
<link>https://stratechery.com/2021/new-defaults/</link>
<guid isPermaLink="true" >https://stratechery.com/2021/new-defaults/</guid>
<description>&lt;p&gt;One of the most well-known papers in behavioral economics is &lt;a href=&quot;https://www.jstor.org/stable/2696456?seq=1&quot;&gt;The Power of Suggestion: Inertia in 401(k) Participation and Savings Behavior&lt;/a&gt; by Brigitte C. Madrian and Dennis F. Shea. From the introduction:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this paper we analyze the 401(k) savings behavior of employees in a large U. S. corporation before and after an interesting change in the company 401(k) plan. Before the plan change, employees who enrolled in the 401(k) plan were required to affirmatively elect participation. After the plan change, employees were automatically enrolled in the 401(k) plan immediately upon hire unless they made a negative election to opt out of the plan. Although none of the economic features of the plan changed, this switch to automatic and immediate enrollment dramatically changed the savings behavior of employees.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would certainly call a shift from 37 percent participation to 86 percent participation a dramatic shift! However, as Madrian and Shea note, there was a downside:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For the NEW cohort, 80 percent of 401(k) contributions are allocated to the money market fund, while only 16 percent of contributions go into stock funds. In contrast, the other cohorts allocate roughly 70 percent of their 401(k) contributions to stock funds, with less than 10 percent earmarked for the money market fund&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The issue is that the money market fund was the default choice, which meant that while the new program helped people save more, it also led folks who would have chosen better-performing funds to earn far less than they would have. Defaults are powerful!&lt;/p&gt;
&lt;p&gt;Just ask Facebook, which is conducting a (probably futile) &lt;a href=&quot;https://about.fb.com/news/2020/12/speaking-up-for-small-businesses/&quot;&gt;public relations campaign&lt;/a&gt; against Apple over iOS 14’s impending “App Tracking Transparency” requirement. Apple told &lt;a href=&quot;https://www.bloomberg.com/news/articles/2020-12-16/facebook-attacks-apple-s-ios-changes-in-full-page-newspaper-ads&quot;&gt;Bloomberg&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Apple defended its iOS updates, saying it was “standing up” for people who use its devices. “Users should know when their data is being collected and shared across other apps and websites — and they should have the choice to allow that or not,” an Apple spokeswoman said in a statement. “App Tracking Transparency in iOS 14 does not require Facebook to change its approach to tracking users and creating targeted advertising, it simply requires they give users a choice.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact, users have had a choice for several years; Apple has given customers the ability to switch off their device’s “Identifier for Advertisers” (IDFA) since 2012. What makes iOS 14 different is the change in defaults: instead of users needing to turn IDFA off, every app has to explicitly ask for it to be turned on, and given &lt;a href=&quot;https://stratechery.com/2020/privacy-labels-and-lookalike-audiences/&quot;&gt;the arguably misleading way that this tracking is presented by the media generally and Apple specifically&lt;/a&gt;, both Facebook and Apple expect customers to say no; indeed, Facebook &lt;a href=&quot;https://stratechery.com/2020/facebooks-ios-14-announcement-understanding-the-idfa-the-real-showdown/&quot;&gt;won’t even bother asking&lt;/a&gt;. Changing the defaults can change the course of a multi-billion dollar company.&lt;/p&gt;
&lt;h4&gt;China, Control, and Quarantine&lt;/h4&gt;
&lt;p&gt;One year ago, on January 5, 2020, Wuhan, Hubei province’s largest city, was &lt;a href=&quot;http://en.hubei.gov.cn/news/newslist/201912/t20191220_1776155.shtml&quot;&gt;set to host&lt;/a&gt; the 3rd Session of the 13th Hubei Provincial People’s Congress and the 3rd Session of the 12th Hubei Provincial Committee of the Chinese People’s Political Consultative Conference, the two most important political gatherings of the year. Perhaps that is why the city reported no new cases of a mysterious respiratory illness that had started appearing the previous November for the following 11 days. Three weeks later, the entire city was locked down in a drastic attempt to contain the virus we now know as SARS-CoV-2.&lt;/p&gt;
&lt;p&gt;Last week, meanwhile, came a lockdown of another sort; from the &lt;a href=&quot;https://www.nytimes.com/2020/12/28/world/asia/china-Zhang-Zhan-covid-convicted.html&quot;&gt;New York Times&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A Chinese court on Monday sentenced a citizen journalist who &lt;a href=&quot;https://www.nytimes.com/2020/12/25/world/asia/china-coronavirus-citizen-journalist.html&quot;&gt;documented the early days of the coronavirus&lt;/a&gt; outbreak to four years in prison, sending a stark warning to those challenging the government’s official narrative of the pandemic. Zhang Zhan, the 37-year-old citizen journalist, was the first known person to face trial for chronicling China’s outbreak. Ms. Zhang, a former lawyer, had traveled to Wuhan from her home in Shanghai in February, at the height of China’s outbreak, to see the toll from the virus in the city where it first emerged. For several months she shared videos that showed crowded hospitals and residents worrying about their incomes…&lt;/p&gt;
&lt;p&gt;Ms. Zhang’s trial, at the Shanghai Pudong New District People’s Court on Monday, lasted less than three hours. The official charge on which she was convicted was “picking quarrels and provoking trouble,” a vague charge commonly used against critics of the government. Prosecutors had initially recommended a sentence between four and five years.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For all of the consternation in China about the the initial cover-up, Zhang’s case is a reminder that controlling information for political purposes is China’s default approach. It is worth noting, though, that the willingness to exert control can be useful, particularly during a pandemic. While Wuhan’s lockdown drew the most attention, and some degree of emulation, that wasn’t what actually stopped the virus’ spread. The &lt;a href=&quot;https://www.wsj.com/articles/the-west-is-misinterpreting-wuhans-coronavirus-progressand-drawing-the-wrong-lessons-11585074966&quot;&gt;Wall Street Journal explained in March&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;em&gt;cordon sanitaire&lt;/em&gt; that began around Wuhan and two nearby cities on Jan. 23 helped slow the virus’s transmission to other parts of China, but didn’t really stop it in Wuhan itself, these experts say. Instead, the virus kept spreading among family members in homes, in large part because hospitals were too overwhelmed to handle all the patients, according to doctors and patients there.&lt;/p&gt;
&lt;p&gt;What really turned the tide in Wuhan was a shift after Feb. 2 to a more aggressive and systematic quarantine regime whereby suspected or mild cases — and even healthy close contacts of confirmed cases — were sent to makeshift hospitals and temporary quarantine centers. The tactics required turning hundreds of hotels, schools and other places into quarantine centers, as well as building two new hospitals and creating 14 temporary ones in public buildings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These centralized quarantines were not optional, and they were effective: China had the coronavirus largely under control by late spring, and the economy has unsurprisingly bounced back; China is expected to be the only Group of 20 country to record positive growth for the year.&lt;/p&gt;
&lt;h4&gt;The West’s Haphazard Approach&lt;/h4&gt;
&lt;p&gt;The United States (along with Europe, it should be noted), has not done so well. Actually, that’s being generous: by pursuing selective lockdowns and completely eschewing centralized quarantine, the West has managed to hurt its economies and kill its small businesses, without actually stopping the spread of the coronavirus. At the same time, as Tyler Cowen argued &lt;a href=&quot;https://www.bloomberg.com/opinion/articles/2020-05-13/forced-quarantines-are-not-the-american-way&quot;&gt;in Bloomberg last May&lt;/a&gt;, centralized quarantines were never really a serious option:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There has been surprisingly little debate in America about one strategy often cited as crucial for preventing and controlling the spread of Covid-19: coercive isolation and quarantine, even for mild cases. China, Singapore and South Korea separate people from their families if they test positive, typically sending them to dorms, makeshift hospitals or hotels. Vietnam and Hong Kong have gone further, sometimes isolating the close contacts of patients.&lt;/p&gt;
&lt;p&gt;I am here to tell you that those practices are wrong, at least for the U.S. They are a form of detainment without due process, contrary to the spirit of the Constitution and, more important, to American notions of individual rights. Yes, those who test positive should have greater options for self-isolation than they currently do. But if a family wishes to stick together and care for each other, it is not the province of the government to tell them otherwise.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Cowen’s first paragraph makes clear that the views in the second are widely held: no politician that I know of, in the U.S. or Europe, seriously argued for centralized quarantine, even though it was likely the only way to contain SARS-CoV-2. The very idea of governments locking up innocent civilians is counter to our default assumption that individual freedom is inviolate.&lt;/p&gt;
&lt;p&gt;That, though, is why it is strange that so many have acquiesced to ever-tightening restrictions on information. It seems that over the last year to have a pro-free speech position has become the exception; the default is to push for censorship, if not by the government — thanks to that pesky First Amendment — then instead by private corporations. And meanwhile, said private corporations, eager to protect their money-making monopolies (in the &lt;a href=&quot;https://stratechery.com/2020/anti-monopoly-vs-antitrust/&quot;&gt;political sense if not the legal one&lt;/a&gt;), are happy to comply; YouTube led the way, declaring in April that it would ban any coronavirus content that contradicted the same World Health Organization that tweeted on January 14th that there was no human-to-human transmission, but most tech companies have since fallen in line.&lt;/p&gt;
&lt;p&gt;To be perfectly clear, I am in no way denying the presence of huge amounts of misinformation, which, by the way, continue to circulate widely despite tech companies’ best efforts. What concerns me is that this sort of dime store authoritarianism is resulting in the worst possible outcome:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://stratechery.com/wp-content/uploads/2021/01/defaults-1-1024x651.png&quot; alt=&quot;Being half authoritarian and half free has been the worst outcome in terms of the pandemic&quot; width=&quot;640&quot; height=&quot;407&quot; class=&quot;aligncenter size-large wp-image-5518&quot; srcset=&quot;https://stratechery.com/wp-content/uploads/2021/01/defaults-1-1024x651.png 1024w, https://stratechery.com/wp-content/uploads/2021/01/defaults-1-300x191.png 300w, https://stratechery.com/wp-content/uploads/2021/01/defaults-1-768x488.png 768w, https://stratechery.com/wp-content/uploads/2021/01/defaults-1-991x630.png 991w, https://stratechery.com/wp-content/uploads/2021/01/defaults-1.png 1280w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;China’s control of information is not ideal — the Wuhan coverup is about as compelling an example as you will ever see of the downsides of information control — but at the same time, it would be dishonest to not recognize that authoritarianism can be effective in actually controlling a pandemic. The West, though, will neither do what it takes to contain the coronavirus, even as we flirt with information suppression at scale. What makes this nefarious is that the cost of the latter is often unseen — it is the ideas never broached, and the risks never taken. But how do you measure opportunity cost?&lt;/p&gt;
&lt;h4&gt;Vaccines and Defaults&lt;/h4&gt;
&lt;p&gt;Here the coronavirus again provides a compelling example, this time in the form of Moderna’s RNA vaccines. David Wallace-Wells wrote in &lt;a href=&quot;https://nymag.com/intelligencer/2020/12/moderna-covid-19-vaccine-design.html&quot;&gt;New York Magazine&lt;/a&gt; in December:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You may be surprised to learn that of the trio of long-awaited coronavirus vaccines, the most promising, Moderna’s mRNA-1273, which reported a 94.5 percent efficacy rate on November 16, had been designed by January 13. This was just two days after the genetic sequence had been made public in an act of scientific and humanitarian generosity that resulted in China’s Yong-Zhen Zhang’s being temporarily forced out of his lab. In Massachusetts, the Moderna vaccine design took all of one weekend. It was completed before China had even acknowledged that the disease could be transmitted from human to human, more than a week before the first confirmed coronavirus case in the United States. By the time the first American death was announced a month later, the vaccine had already been manufactured and shipped to the National Institutes of Health for the beginning of its Phase I clinical trial. This is — as the country and the world are rightly celebrating — the fastest timeline of development in the history of vaccines. It also means that for the entire span of the pandemic in this country, which has already killed more than 250,000 Americans, we had the tools we needed to prevent it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As Wallace-Wells notes, this does not mean that the Moderna vaccine should have — or could have — been rolled out in January. It does, though, provide a powerful thought experiment about opportunity cost.&lt;/p&gt;
&lt;p&gt;Opportunity cost is distinct from the costs normally calculated in a cost-benefit analysis: those costs are real costs, in that they are actually incurred. For example, if I want to buy a new sweater, it will cost me money. Opportunity cost, on the other hand, is the choice not made. To return to the sweater example, whatever funds I use to buy a sweater cannot be used to buy slacks — the slacks are the opportunity cost.&lt;/p&gt;
&lt;p&gt;In the case of the vaccine, the opportunity costs of not deploying it the moment it was developed are enormous: hundreds of thousands of lives saved in the U.S. alone, millions around the world, and untold economic destruction avoided. Again, to be clear, I’m not saying this choice was available to us, and you can easily concoct another thought experiment where the vaccine goes horribly wrong. What makes this thought experiment worthwhile, though, is that it is such a powerful example of opportunity costs, and it is opportunity costs — the thing not learned — that are the biggest casualty of defaulting towards information control instead of free speech.&lt;/p&gt;
&lt;p&gt;This isn’t the only mistaken default. Another topic that received minimal discussion was the concept of human challenge trials, where individuals could volunteer — and be richly compensated — to be exposed to the virus to more quickly test the vaccination’s efficacy. When I broached the idea &lt;a href=&quot;https://twitter.com/benthompson/status/1256791915913031681&quot;&gt;on Twitter&lt;/a&gt;, plenty of folks were quick to cite the very real ethical concerns with the concept, but few seemed willing to acknowledge the opportunity costs incurred by waiting a single day longer than necessary, which ought to present ethical concerns of their own. There was also no discussion of making the vaccine broadly available in conjunction with Phase III trials, despite the fact that RNA-based vaccines are inherently safer than traditional vaccines based on weakened or inactivated viruses. Similarly, when &lt;a href=&quot;https://twitter.com/benthompson/status/1334362824630394881&quot;&gt;I expressed bafflement&lt;/a&gt; that people weren’t outraged by the FDA’s delay in approving the vaccines, the response of many was to insist the agency was rightly prioritizing safety. What, though, about the safety of those suffering from a pandemic that was accelerating? At every step the default was a bias towards the status quo of no vaccine, no matter how great the opportunity cost may have been.&lt;/p&gt;
&lt;p&gt;What is most dispiriting, though, is this chart from &lt;a href=&quot;https://www.bloomberg.com/graphics/covid-vaccine-tracker-global-distribution/&quot;&gt;Bloomberg&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.bloomberg.com/graphics/covid-vaccine-tracker-global-distribution/&quot;&gt;&lt;img src=&quot;https://stratechery.com/wp-content/uploads/2021/01/defaults-2-1024x825.png&quot; alt=&quot;A map of how many vaccines have been distributed in the U.S.&quot; width=&quot;640&quot; height=&quot;516&quot; class=&quot;aligncenter size-large wp-image-5519&quot; srcset=&quot;https://stratechery.com/wp-content/uploads/2021/01/defaults-2-1024x825.png 1024w, https://stratechery.com/wp-content/uploads/2021/01/defaults-2-300x242.png 300w, https://stratechery.com/wp-content/uploads/2021/01/defaults-2-768x619.png 768w, https://stratechery.com/wp-content/uploads/2021/01/defaults-2-782x630.png 782w, https://stratechery.com/wp-content/uploads/2021/01/defaults-2.png 1280w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As of this morning only 30% of distributed vaccines have been administered; that’s not quite as bad as it seems given the U.S. policy to hold back the second shot in reserve (itself &lt;a href=&quot;https://www.nytimes.com/2021/01/03/health/coronavirus-vaccine-doses.html&quot;&gt;a conservative decision&lt;/a&gt; that seems driven by the &lt;a href=&quot;https://marginalrevolution.com/marginalrevolution/2021/01/first-doses-first-show-your-work.html&quot;&gt;status quo default&lt;/a&gt;), but that still means millions of shots are unused and risk expiration. A major hold-up has been strict prioritization protocols, which prioritize equitable distribution over speed. It’s another misplaced default.&lt;/p&gt;
&lt;h4&gt;Technology and Opportunity&lt;/h4&gt;
&lt;p&gt;At this point many of you are surely muttering that this was the fastest vaccine development program in history, and that the U.S., for all of its struggles, has already vaccinated 1.42% of its population, the 3rd most in the world. Both are true, and worth celebrating.&lt;/p&gt;
&lt;p&gt;At the same time, the timeline in that New York Magazine article is worth keeping in mind: the single most important reason these vaccines were developed so quickly was because of technological progress. &lt;a href=&quot;https://berthub.eu/articles/posts/reverse-engineering-source-code-of-the-biontech-pfizer-vaccine/&quot;&gt;This brilliant article&lt;/a&gt; explains how mRNA vaccinations work in computer programming terms, but the entire concept is built on years of work. The &lt;a href=&quot;https://www.health.harvard.edu/blog/why-are-mrna-vaccines-so-exciting-2020121021599&quot;&gt;Harvard Health Blog&lt;/a&gt; noted:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Like every breakthrough, the science behind the mRNA vaccine builds on many previous breakthroughs, including:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Understanding the structure of DNA and mRNA, and how they work to produce a protein.&lt;/li&gt;
&lt;li&gt;Inventing technology to determine the genetic sequence of a virus.&lt;/li&gt;
&lt;li&gt;Inventing technology to build an mRNA that would make a particular protein.&lt;/li&gt;
&lt;li&gt;Overcoming all of the obstacles that could keep mRNA injected into the muscle of a person’s arm from finding its way to immune system cells deep within the body, and coaxing those cells to make the critical protein.&lt;/li&gt;
&lt;li&gt;Information technology to transmit knowledge around the world at light-speed.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Every one of these past discoveries depended on the willingness of scientists to persist in pursuing their longshot dreams — often despite enormous skepticism and even ridicule — and the willingness of society to invest in their research.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Longshot dreams, enormous skepticism, and even ridicule certainly sound familiar to anyone associated with Silicon Valley, and there is an analogy to be made between how technology accelerated vaccine development, even in the face of conservative defaults, and how the technology industry broadly has driven U.S. economic growth for decades now, even in the face of stagnation elsewhere.&lt;/p&gt;
&lt;p&gt;What makes software so compelling to anyone ambitious is that (1) the potential applications are limitless and (2) the limitations on creation are your own imagination, not external regulations. This certainly has its downsides, as anyone trying to get a software release out the door understands; you can add new features and fix bugs forever, because after all, it’s just software. At the same time, you can build anything you want, without asking for permission, and what could be more exciting than that?&lt;/p&gt;
&lt;p&gt;I’m reminded of this old Steve Jobs interview:&lt;/p&gt;

&lt;p&gt;Jobs was talking about life in general, but the potential he articulates is much more easily grasped in software; what is notable is that it was the software-driven companies that performed the best throughout the pandemic. Perhaps the assumption that any problem is solvable is a muscle that can be developed in software and applied to the real world? Amazon is a striking example in this regard: the so-called “tech” company hired over 400,000 new people in 2020, as it brought its massive logistics network to bear in the face of overwhelming demand; no wonder many have been joking on Twitter that the company should be in charge of the vaccination rollout.&lt;/p&gt;
&lt;p&gt;Or, better yet, we ought to figure out how to export the Amazon mindset beyond the world of technology, but to do that we need new defaults.&lt;/p&gt;
&lt;h4&gt;New Defaults&lt;/h4&gt;
&lt;p&gt;Start with these three:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;First, it should be the default that free speech is a good thing, that more information is better than less information, and that the solution to misinformation is &lt;a href=&quot;https://stratechery.com/2020/zero-trust-information/&quot;&gt;improving our ability to tell the difference&lt;/a&gt;, not futilely trying to be China-lite without any of the upside.&lt;/li&gt;
&lt;li&gt;Second, it should be the default that the status quo is a bad thing; instead of justifying why something should be done, the burden of proof should rest on those who believe things should remain the same. This sounds radical, but given the fact that the world is undergoing &lt;a href=&quot;https://stratechery.com/2019/the-internet-and-the-third-estate/&quot;&gt;profound changes driven by the Internet&lt;/a&gt;, it is the attempt to preserve the unsustainable that is radical.&lt;/li&gt;
&lt;li&gt;Third, it should be the default to move fast, and value experimentation over perfection. The other opportunity cost of decisions not made is lessons not learned; given the speed with which information is disseminated, this cost is higher than ever.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The urgency of this reset should come from where all of this started: China. Dan Wang wrote in his &lt;a href=&quot;https://danwang.co/2020-letter/&quot;&gt;2020 letter&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;This year made me believe that China is the country with the most can-do spirit in the world. Every segment of society mobilized to contain the pandemic. One manufacturer expressed astonishment to me at how slowly western counterparts moved. US companies had to ask whether making masks aligned with the company’s core competence. Chinese companies simply decided that making money is their core competence, and therefore they should be making masks. The State Council reported that between March and May, China exported 70 billion masks and nearly 100,000 ventilators. Some of these masks had problems early on, but the manufacturers learned and fixed them or were culled by regulatory action, and China’s exports were able to grow when no one else could restart production. Soon enough, exports of masks were big enough to be seen in the export data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This, to be clear, was not the result of authoritarianism, but despite it; Taiwan exhibited &lt;a href=&quot;https://stratechery.com/2020/compaq-and-coronavirus/&quot;&gt;the exact same sort can-do attitude&lt;/a&gt; alongside a free press, elections, and &lt;a href=&quot;https://www.washingtonpost.com/world/asia_pacific/taiwan-pig-intestines-parliament-fight/2020/11/27/6eca3be0-3097-11eb-9dd6-2d0179981719_story.html&quot;&gt;pig intestines in the legislature&lt;/a&gt;. China, meanwhile, is increasing control of the private sector; the latest example is Alibaba and Jack Ma, who was &lt;a href=&quot;https://www.cnbc.com/2021/01/04/jack-mas-disappearing-act-fuels-speculation-about-the-billionaires-whereabouts.html&quot;&gt;last seen in October&lt;/a&gt; criticizing the country’s regulators; China proceeded to kill Ant Group’s IPO, in a signal to any other billionaires with big ideas about who was boss, and Ma’s whereabouts are unknown. The U.S. can absolutely compete with this approach, not by imitating it, but by doing the exact opposite.&lt;/p&gt;
&lt;h4&gt;Intentions Versus Outcomes&lt;/h4&gt;
&lt;p&gt;A few years after Madrian and Shea’s landmark study, Richard Thaler, the Nobel-prize winning economist at the University of Chicago, devised a new approach for 401(k) enrollments that sought to overcome the downside of default choices (while preserving their upside). What I wanted to highlight, though, was this bit from the introduction of &lt;a href=&quot;https://faculty.chicagobooth.edu/-/media/faculty/richard-thaler/assets/files/smartjpe.pdf&quot;&gt;Save More Tomorrow: Using Behavioral Economics to Increase Employee Saving&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Economic theory generally assumes that people solve important problems as economists would. The life cycle theory of saving is a good example. Households are assumed to want to smooth consumption over the life cycle and are expected to solve the relevant optimization problem in each period before deciding how much to consume and how much to save. Actual household behavior might differ from this optimal plan for at least two reasons. First, the problem is a hard one, even for an economist, so households might fail to compute the correct savings rate. Second, even if the correct savings rate were known, households might lack the self-control to reduce current consumption in favor of future consumption…&lt;/p&gt;
&lt;p&gt;For whatever reason, some employees at firms that offer only defined-contribution plans contribute little or nothing to the plan. In this paper, we take seriously the possibility that some of these low-saving workers are making a mistake. By calling their low-saving behavior a mistake, we mean that they might characterize the action the same way, just as someone who is 100 pounds overweight might agree that he or she weighs too much. We then use principles from psychology and behavioral economics to devise a program to help people save more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I suspect a similar story can be told about our slide to defaulting that free speech is bad, that the status quo should be the priority, and that perfect is preferable to good. These are mistakes, even as they are understandable. After all, misinformation is a bad thing, change is uncertain, and no one wants to be the one that screwed up. Everyone has good intentions; the mistake is in valuing intentions over outcomes.&lt;/p&gt;
&lt;p&gt;To that end, the point of this article was not really to discuss the coronavirus or vaccinations: with regards to the latter, there is more to praise than to criticize, and I freely admit I am not an expert about either. And yet, that isn’t a reason to settle, or to not examine our defaults: why can’t we accomplish other big projects in a year? What else can we build with so much broad benefit so quickly? And critically, what can we change about our psychology and behavior to make that happen? New defaults are the best place to start.&lt;/p&gt;


</description>
<pubDate>Tue, 05 Jan 2021 15:00:04 +0000</pubDate>
<dc:creator>kaboro</dc:creator>
<og:type>article</og:type>
<og:title>New Defaults</og:title>
<og:url>https://stratechery.com/2021/new-defaults/</og:url>
<og:description>The pandemic and vaccine rollout have highlighted where the West has lost its way; we need new defaults about information, change, and speed.</og:description>
<og:image>https://stratechery.com/wp-content/uploads/2021/01/defaults-1.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://stratechery.com/2021/new-defaults/</dc:identifier>
</item>
<item>
<title>Termux no longer updated on Google Play</title>
<link>https://wiki.termux.com/wiki/Termux_Google_Play</link>
<guid isPermaLink="true" >https://wiki.termux.com/wiki/Termux_Google_Play</guid>
<description>&lt;div class=&quot;mw-parser-output&quot; readability=&quot;25.320574162679&quot;&gt;
&lt;p&gt;Since November 2, 2020 we no longer able to publish updates of Termux application and add-ons because we are not ready for changes upcoming with SDK level 29 (Android 10).&lt;/p&gt;
&lt;p&gt;Everyone should move to F-Droid version, if possible. Check out &lt;a href=&quot;https://wiki.termux.com/wiki/Backing_up_Termux&quot; title=&quot;Backing up Termux&quot;&gt;Backing up Termux&lt;/a&gt; if you are interested on how to preserve data when re-installing the application.&lt;/p&gt;
&lt;h2&gt;&lt;span class=&quot;mw-headline&quot; id=&quot;About_the_issue&quot;&gt;About the issue&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Github discussion: &lt;a rel=&quot;nofollow&quot; class=&quot;external free&quot; href=&quot;https://github.com/termux/termux-app/issues/1072&quot;&gt;https://github.com/termux/termux-app/issues/1072&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If Termux application was built with target SDK level &quot;29&quot; or higher, it will be eligible for SELinux restriction of execve() system call on data files. That makes impossible to run package executables such as &quot;apt&quot;, &quot;bash&quot; and others located in the application's writable directory such as $PREFIX.&lt;/p&gt;
&lt;p&gt;We have chosen to stick with target SDK level &amp;lt;= 28 in order to keep Termux running on Android devices with Android OS version 10 and higher until we release next major version of Termux (v1.0). It will change app design to comply with new SELinux configuration and Google Play policy, potentially at cost of user experience.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 05 Jan 2021 12:50:03 +0000</pubDate>
<dc:creator>martinlaz</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://wiki.termux.com/wiki/Termux_Google_Play</dc:identifier>
</item>
<item>
<title>Show HN: Unclack – a macOS app that auto-mutes your keyboard</title>
<link>https://unclack.app</link>
<guid isPermaLink="true" >https://unclack.app</guid>
<description>&lt;div readability=&quot;34&quot;&gt;
          &lt;i class=&quot;d-inlin-block h2 mb-10 tf-ion-ios-star-outline&quot;/&gt;
          &lt;h4 class=&quot;font-weight-bold mb-2&quot;&gt;Auto-magic&lt;/h4&gt;
          &lt;p&gt;Unclack automatically mutes your microphone while you’re typing and un-mutes you when you stop, no interaction required.&lt;/p&gt;
        &lt;/div&gt;&lt;div readability=&quot;36&quot;&gt;
          &lt;i class=&quot;d-inlin-block h2 mb-10 tf-ion-ios-videocam-outline&quot;/&gt;
          &lt;h4 class=&quot;font-weight-bold mb-2&quot;&gt;Versatile&lt;/h4&gt;
          &lt;p&gt;Zoom, Skype, Webex, Discord, no matter. Unclack is software-agnostic and works in the background.&lt;/p&gt;
        &lt;/div&gt;</description>
<pubDate>Tue, 05 Jan 2021 12:27:22 +0000</pubDate>
<dc:creator>robotsquidward</dc:creator>
<og:title>Unclack for macOS</og:title>
<og:description>Unclack is the small but mighty Mac utility that mutes your keyboard while you type!</og:description>
<og:type>website</og:type>
<og:url>/</og:url>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://unclack.app/</dc:identifier>
</item>
</channel>
</rss>