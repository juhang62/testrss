<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Over 275 days since Equifax’s data breach settlement and no one has been paid</title>
<link>https://www.interest.com/personal-finance/275-days-since-equifax-data-breach-settlement/</link>
<guid isPermaLink="true" >https://www.interest.com/personal-finance/275-days-since-equifax-data-breach-settlement/</guid>
<description>&lt;p&gt;Have you felt someone owes you money? Last year, credit reporting agency Equifax was rocked by a massive &lt;a href=&quot;https://www.cnbc.com/2019/07/22/what-you-need-to-know-equifax-data-breach-700-million-settlement.html&quot;&gt;data breach&lt;/a&gt; affecting most (56%) Americans. The company agreed to one of the largest settlements of its kind, $700M to be disbursed, covering identity protection monitoring services and direct cash payments to help those whose data had been stolen.&lt;/p&gt;
&lt;p&gt;Many of you probably read some of these headlines in the summer of 2019, and many more still &lt;a href=&quot;https://www.equifaxbreachsettlement.com/&quot;&gt;filled out the forms&lt;/a&gt; after promises of up to &lt;a href=&quot;https://www.cnn.com/2019/09/09/business/equifax-settlement-claim-verification/index.html&quot;&gt;$125 per afflicted party&lt;/a&gt;. There were several deadlines and additional hoops people had to jump through, but &lt;strong&gt;275 days later: no one has been paid yet&lt;/strong&gt;, and it’s not clear if they ever will be. It’s been almost three times that time (&lt;strong&gt;800+ Days&lt;/strong&gt;) since the breach itself actually &lt;em&gt;occured&lt;/em&gt; in Q3 of 2017. &lt;/p&gt;
&lt;blockquote class=&quot;wp-block-quote&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;strong&gt;“275 days later: no one has been paid yet, and it’s not clear if they ever will be.”&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;According to Consumer Reports and the FTC, it’s likely that any cash payments will end up being far less than the maximum of $125. &lt;/p&gt;
&lt;p&gt;“A large number of claims for cash instead of credit monitoring means only one thing: each person who takes the money option will wind up only getting a small amount of money. Nowhere near the $125 they could have gotten if there hadn’t been such an enormous number of claims filed.”&lt;/p&gt;
&lt;p&gt;As governments, businesses and individuals scramble to save their economic futures in a post-Coronavirus world, $125 per credit-using American adults could go a long way towards paying for the weekly groceries or utility bills. It makes us wonder: Should Equifax pay interest on these missed payments? How might it affect their credit score?&lt;/p&gt;
&lt;p&gt;Equifax has placed arbitrary deadlines on their claim forms that have since elapsed, but they still allow you to “&lt;a href=&quot;https://www.equifaxbreachsettlement.com/file-a-claim&quot;&gt;file an extended claim&lt;/a&gt;” on their website through 2024. That year should give you an idea of how long you’ll likely be waiting for recompense.&lt;/p&gt;
&lt;p&gt;The total settlement was for $700M and was &lt;a href=&quot;https://www.reuters.com/article/us-equifax-cyber-settlement-factbox/factbox-biggest-u-s-data-breach-settlements-before-equifax-idUSKCN1UH22P&quot;&gt;by far the largest ever for a data breach&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;As more network &lt;a href=&quot;https://www.forbes.com/sites/zakdoffman/2019/09/14/dangerous-cyberattacks-on-iot-devices-up-300-in-2019-now-rampant-report-claims/#10a8b88c5892&quot;&gt;security issues are identified&lt;/a&gt; in personal, as well as workplace devices, prioritizing protection might seem obvious but end up being a difficult task. These issues will likely cause more breaches in the future, as well as more settlements with consumers.  &lt;/p&gt;
&lt;p&gt;But will the American people ever be paid for their exposure and hardships that stem from these breaches? That remains to be seen, and it hasn’t happened yet.&lt;/p&gt;
&lt;p&gt;The number of reported data breaches rose &lt;a href=&quot;https://www.marketwatch.com/story/data-breaches-soared-by-17-in-2019-but-theres-some-good-news-too-2020-01-29&quot;&gt;17% to 1,473 in 2019&lt;/a&gt; from 1,257 a year earlier, and we’re guessing 2020 will see yet another increase. The scope of these breaches also seems to be broadening recently, with &lt;a href=&quot;https://us.norton.com/internetsecurity-emerging-threats-2019-data-breaches.html&quot;&gt;over 4 billion records mismanaged last year&lt;/a&gt;. It remains to be seen if exploited users will ever be compensated for their data loss, but we’ll continue to report on the situation.&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 21:27:25 +0000</pubDate>
<dc:creator>ProAm</dc:creator>
<og:type>article</og:type>
<og:title>Over 275 Days Since Equifax's Data Breach Settlement and No One Has Been Paid | Interest.com</og:title>
<og:description>Equifax agreed to a $700M settlement almost a year ago, but no one has seen the money that was agreed to getting disbursed. Read our story to get the latest on the situation.</og:description>
<og:url>https://www.interest.com/personal-finance/275-days-since-equifax-data-breach-settlement/</og:url>
<og:image>https://www.interest.com/wp-content/uploads/2020/04/equifax.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.interest.com/personal-finance/275-days-since-equifax-data-breach-settlement/</dc:identifier>
</item>
<item>
<title>Xiaomi Recording ‘Private’ Web and Phone Use</title>
<link>https://www.forbes.com/sites/thomasbrewster/2020/04/30/exclusive-warning-over-chinese-mobile-giant-xiaomi-recording-millions-of-peoples-private-web-and-phone-use/#6a50cdaf1b2a</link>
<guid isPermaLink="true" >https://www.forbes.com/sites/thomasbrewster/2020/04/30/exclusive-warning-over-chinese-mobile-giant-xiaomi-recording-millions-of-peoples-private-web-and-phone-use/#6a50cdaf1b2a</guid>
<description>&lt;div&gt;&lt;img src=&quot;https://specials-images.forbesimg.com/imageserve/1196587445/960x0.jpg?fit=scale&quot; alt=&quot;Xiaomi Note 10 Pro smartphone privacy warning&quot; data-height=&quot;3228&quot; data-width=&quot;5000&quot;/&gt;&lt;/div&gt;
&lt;p class=&quot;color-body light-text&quot;&gt;Commuters pass by Xiaomi Note 10 Pro smartphone advertisement at its flagship store in Hong Kong. &lt;span class=&quot;plus&quot; data-ga-track=&quot;caption expand&quot;&gt;... [+]&lt;/span&gt; &lt;span class=&quot;expanded-caption&quot;&gt;Researchers think the huge Chinese business is infringing on people's privacy, but the company denies any wrongdoing.&lt;/span&gt;&lt;/p&gt;
&lt;small&gt;Budrul Chukrut/SOPA Images/LightRocket via Getty Images&lt;/small&gt;
&lt;p&gt;&lt;span&gt;“It’s a backdoor with phone functionality,” quips Gabi Cirlig about his new&lt;/span&gt; &lt;a href=&quot;https://www.forbes.com/companies/xiaomi/&quot; target=&quot;_self&quot; class=&quot;color-link&quot; title=&quot;https://www.forbes.com/companies/xiaomi/&quot; data-ga-track=&quot;InternalLink:https://www.forbes.com/companies/xiaomi/&quot; aria-label=&quot;Xiaomi&quot;&gt;Xiaomi&lt;/a&gt; &lt;span&gt;phone. He’s only half-joking.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Cirlig is speaking with&lt;/span&gt; &lt;em&gt;Forbes&lt;/em&gt; &lt;span&gt;after discovering that his&lt;/span&gt; &lt;span&gt;Redmi Note 8&lt;/span&gt; &lt;span&gt;smartphone was watching much of what he was doing on the phone. That data was then being sent to remote servers hosted by another Chinese tech giant,&lt;/span&gt; &lt;a href=&quot;https://www.forbes.com/companies/alibaba/&quot; target=&quot;_self&quot; class=&quot;color-link&quot; title=&quot;https://www.forbes.com/companies/alibaba/&quot; data-ga-track=&quot;InternalLink:https://www.forbes.com/companies/alibaba/&quot; aria-label=&quot;Alibaba&quot;&gt;Alibaba&lt;/a&gt;&lt;span&gt;, which were ostensibly rented by Xiaomi. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The seasoned cybersecurity researcher found a worrying amount of his behavior was being tracked, whilst various kinds of device data were also being harvested, leaving Cirlig spooked that his identity and his private life was being exposed to the Chinese company.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;When he looked around the Web on the device’s default Xiaomi browser, it recorded all the websites he visited, including search engine queries whether with Google or the privacy-focused DuckDuckGo, and every item viewed on a news feed feature of the Xiaomi software. That tracking appeared to be happening even if he used the supposedly private “incognito” mode.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The device was also recording what folders he opened and to which screens he swiped, including the status bar and the settings page. All of the data was being packaged up and sent to remote servers in Singapore and Russia, though the Web domains they hosted were registered in Beijing.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Meanwhile, at&lt;/span&gt; &lt;em&gt;Forbes&lt;/em&gt;&lt;span&gt;’ request, cybersecurity researcher Andrew Tierney investigated further. He also found browsers shipped by Xiaomi on Google Play—Mi Browser Pro and the Mint Browser—were collecting the same data. Together, they have more than 15 million downloads, according to Google Play statistics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Many more millions are likely to be affected by what Cirlig described as a serious privacy issue, though Xiaomi denied there was a problem. Valued at $50 billion, Xiaomi is&lt;/span&gt; &lt;a href=&quot;https://www.idc.com/promo/smartphone-market-share/vendor&quot; target=&quot;_blank&quot; class=&quot;color-link&quot; title=&quot;https://www.idc.com/promo/smartphone-market-share/vendor&quot; rel=&quot;nofollow noopener noreferrer&quot; data-ga-track=&quot;ExternalLink:https://www.idc.com/promo/smartphone-market-share/vendor&quot; aria-label=&quot;one of the top four smartphone makers in the world&quot;&gt;one of the top four smartphone makers in the world&lt;/a&gt; &lt;span&gt;by market share, behind Apple, Samsung and Huawei. Xiaomi’s big sell is cheap devices that have many of the same qualities as higher-end smartphones. But for customers, that low cost could come with a hefty price: their privacy.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Cirlig thinks that the problems affect many more models than the one he tested. He downloaded firmware for other Xiaomi phones—&lt;/span&gt;&lt;span&gt;including the Xiaomi MI 10, Xiaomi Redmi K20 and Xiaomi Mi MIX 3 devices. He then confirmed they had the same browser code, leading him to suspect they had&lt;/span&gt; &lt;span&gt;the same privacy issues.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;And there appear to be issues with how Xiaomi is transferring the data to its servers. Though the Chinese company claimed the data was being encrypted when transferred in an attempt to protect user privacy, Cirlig found he was able to quickly see just what was being taken from his device by decoding a chunk of information that was hidden with a form of easily crackable encoding, known as base64. It took Cirlig just a few seconds to change the garbled data into readable chunks of information.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“My main concern for privacy is that the data sent to their servers can be very easily correlated with a specific user,” warned Cirlig.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Xiaomi’s response&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;In response to the findings, Xiaomi said, “The research claims are untrue,” and “Privacy and security is of top concern,” adding that it “strictly follows and is fully compliant with local laws and regulations on user data privacy matters.” But a spokesperson confirmed it&lt;/span&gt; &lt;em&gt;was&lt;/em&gt; &lt;span&gt;collecting browsing data, claiming the information was anonymized so wasn’t tied to any identity. They said that users had consented to such tracking. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;But, as pointed out by Cirlig and Tierney, it wasn’t just the website or Web search that was sent to the server. Xiaomi was also collecting data about the phone, including unique numbers for identifying the specific device and Android version. Cirlig said such “&lt;/span&gt;&lt;span&gt;metadata” could “easily be correlated with an actual human behind the screen.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Xiaomi’s spokesperson also denied that browsing data was being recorded under incognito mode. Both Cirlig and Tierney, however, found in their independent tests that their web habits were sent off to remote servers regardless of what mode the browser was set to, providing both photos and videos as proof.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;When&lt;/span&gt; &lt;em&gt;Forbes&lt;/em&gt; &lt;span&gt;provided Xiaomi with a video made by Cirlig showing how his Google search for “porn” and a visit to the site PornHub were sent to remote servers, even when in incognito mode, the company spokesperson continued to deny that the information was being recorded. “This video shows the collection of anonymous browsing data, which is one of the most common solutions adopted by internet companies to improve the overall browser product experience through analyzing non-personally identifiable information,” they added.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Both Cirlig and Tierney said Xiaomi’s behavior was more invasive than other browsers like Google Chrome or Apple Safari. “It’s a lot worse than any of the mainstream browsers I have seen,” Tierney said. “Many of them take analytics, but it's about usage and crashing. Taking browser behavior, including URLs, without explicit consent and in private browsing mode, is about as bad as it gets.”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Cirlig also suspected that his app use was being monitored by Xiaomi, as every time he opened an app, a chunk of information would be sent to a remote server. Another researcher who’d tested Xiaomi devices, though was under an NDA to discuss the matter openly, said he’d seen the manufacturer’s phone collect such data. Xiaomi didn’t respond to questions on that issue.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;‘Behavioral Analytics’&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Xiaomi appears to have another reason for collecting the data: to better understand its users’ behavior. It’s using the services of a behavioral analytics company called Sensors Analytics. The Chinese startup, also known as Sensors Data, has raised $60 million since its founding in 2015, most recently taking $44 million in a round led by New York private equity firm Warburg Pincus, which also featured funding from Sequoia Capital China. As described in Pitchbook, a tracker of company funding, Sensors Analytics is a “provider of an in-depth user behavior analysis platform and professional consulting services.” Its tools help its clients in “exploring the hidden stories behind the indicators in exploring the key behaviors of different businesses.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Both Cirlig and Tierney found their Xiaomi apps were sending data to domains that appeared to reference Sensors Analytics, including the repeated use of SA. When clicking on one of the domains, the page contained one sentence: “Sensors Analytics is ready to receive your data!”  There was an API called SensorDataAPI—an API (application programming interface) being the software that allows third parties access to app data. Xiaomi is also listed as a customer on&lt;/span&gt; &lt;a href=&quot;https://translate.google.com/translate?hl=en&amp;amp;sl=zh-CN&amp;amp;u=https://www.sensorsdata.cn/about/aboutus.html&amp;amp;prev=search&quot; target=&quot;_blank&quot; class=&quot;color-link&quot; title=&quot;https://translate.google.com/translate?hl=en&amp;amp;sl=zh-CN&amp;amp;u=https://www.sensorsdata.cn/about/aboutus.html&amp;amp;prev=search&quot; rel=&quot;nofollow noopener noreferrer&quot; data-ga-track=&quot;ExternalLink:https://translate.google.com/translate?hl=en&amp;amp;sl=zh-CN&amp;amp;u=https://www.sensorsdata.cn/about/aboutus.html&amp;amp;prev=search&quot; aria-label=&quot;Sensors Data’s website&quot;&gt;Sensors Data’s website&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The founder and CEO of Sensors Data, Sang Wenfeng, has a long history in tracking users. At Chinese internet giant Baidu he built a big data platform for Baidu user logs, according to his company bio.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Xiaomi’s spokesperson confirmed the relationship with the startup: “&lt;/span&gt;&lt;span&gt;While Sensors Analytics provides a data analysis solution for Xiaomi, the collected anonymous data are stored on Xiaomi's own servers and will not be shared with Sensors Analytics, or any other third-party companies.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;It’s the second time in two months that a huge Chinese tech company has been seen watching over users’ phone habits. A security app with a “private” browser made by Cheetah Mobile, a public company listed on the New York Stock Exchange, was&lt;/span&gt; &lt;a href=&quot;https://www.forbes.com/sites/thomasbrewster/2020/03/03/warning-an-android-security-app-with-1-billion-downloads-is-recording-users-web-browsing/&quot; target=&quot;_self&quot; class=&quot;color-link&quot; title=&quot;https://www.forbes.com/sites/thomasbrewster/2020/03/03/warning-an-android-security-app-with-1-billion-downloads-is-recording-users-web-browsing/&quot; data-ga-track=&quot;InternalLink:https://www.forbes.com/sites/thomasbrewster/2020/03/03/warning-an-android-security-app-with-1-billion-downloads-is-recording-users-web-browsing/&quot; aria-label=&quot;seen collecting information on Web use&quot;&gt;seen collecting information on Web use&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;Wi-Fi access point names and more granular data like how a user scrolled on visited Web pages. Cheetah argued it needed to collect the information to protect users and improve their experience.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Late in his research, Cirlig also discovered that Xiaomi’s music player app on his phone was collecting information on his listening habits: what songs were played and when.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;One message was clear to the researcher: when you’re listening, Xiaomi is listening, too.&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Thu, 30 Apr 2020 19:48:07 +0000</pubDate>
<dc:creator>rock_artist</dc:creator>
<og:title>Exclusive: Warning Over Chinese Mobile Giant Xiaomi Recording Millions Of People’s ‘Private’ Web And Phone Use</og:title>
<og:type>article</og:type>
<og:url>https://www.forbes.com/sites/thomasbrewster/2020/04/30/exclusive-warning-over-chinese-mobile-giant-xiaomi-recording-millions-of-peoples-private-web-and-phone-use/</og:url>
<og:image>https://thumbor.forbes.com/thumbor/fit-in/1200x0/filters%3Aformat%28jpg%29/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F1196587445%2F0x0.jpg</og:image>
<og:description>Xiaomi is collecting users’ browser habits and phone usage, raising red flags for privacy researchers.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.forbes.com/sites/thomasbrewster/2020/04/30/exclusive-warning-over-chinese-mobile-giant-xiaomi-recording-millions-of-peoples-private-web-and-phone-use/</dc:identifier>
</item>
<item>
<title>Rust/WinRT Public Preview</title>
<link>https://blogs.windows.com/windowsdeveloper/2020/04/30/rust-winrt-public-preview/</link>
<guid isPermaLink="true" >https://blogs.windows.com/windowsdeveloper/2020/04/30/rust-winrt-public-preview/</guid>
<description>&lt;p&gt;We are excited to announce that the Rust/WinRT project finally has a permanent and public home on GitHub:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/microsoft/winrt-rs&quot;&gt;https://github.com/microsoft/winrt-rs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rust/WinRT follows in the tradition established by &lt;a href=&quot;https://github.com/microsoft/cppwinrt/&quot;&gt;C++/WinRT&lt;/a&gt; of building language projections for the Windows Runtime using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs. Rust/WinRT lets you call any WinRT API past, present, and future using code generated on the fly directly from the metadata describing the API and right into your Rust package where you can call them as if they were just another Rust module.&lt;/p&gt;
&lt;p&gt;The Windows Runtime is based on Component Object Model (COM) APIs under the hood and is designed to be accessed through language projections like C++/WinRT and Rust/WinRT. Those language projections take the metadata describing various APIs and provide natural bindings for the target programming language. As you can imagine, this allows developers to more easily build apps and components for Windows using their desired language. You can then use those Windows APIs to build desktop apps, store apps, or something more unique like a component, NT service, or device driver.&lt;/p&gt;
&lt;p&gt;Microsoft has long depended on C++ as the backbone for so much of what we do, but it has some challenges particularly when it comes to security. Modern C++ certainly makes it easier to write safe and secure C++ if you follow certain careful conventions, but that is often hard to enforce on larger projects. Rust is an intriguing language. It closely resembles C++ in many ways, hitting all the right notes when it comes to compilation, runtime model, type system and deterministic finalization. While it has its own unique learning curve, it also has the potential to solve some of the most vexing issues that plague C++ projects, and is designed from the ground up with memory safety and safe concurrency as core principles. For more information on Rust and safe systems programming, check out the &lt;a href=&quot;https://msrc-blog.microsoft.com/2019/07/22/why-rust-for-safe-systems-programming/&quot;&gt;Microsoft Security Response Center&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here below is a simple example of Rust calling a Windows API. The API itself does not matter, but it should give you a sense for how naturally Windows APIs may be called from Rust code. In the following example, we are using the XmlDocument class from the Windows.Data.Xml.Dom namespace to parse and inspect a simple XML document:&lt;/p&gt;
&lt;pre class=&quot;brush: xml; title: ; notranslate&quot; title=&quot;&quot;&gt;

use windows::data::xml::dom::*;

let doc = XmlDocument::new()?;
doc.load_xml(&quot;&amp;lt;html&amp;gt;hello world&amp;lt;/html&amp;gt;&quot;)?;

let root = doc.document_element()?;
assert!(root.node_name()? == &quot;html&quot;);
assert!(root.inner_text()? == &quot;hello world&quot;);

&lt;/pre&gt;
&lt;p&gt;If you are familiar with Rust, you will notice this looks far more like Rust than it looks like C++ or C#. Notice the snake_case on module and method names and the ? operator for error propagation.&lt;/p&gt;
&lt;p&gt;Here is another example using the Windows.ApplicationModel.DataTransfer namespace to copy some value onto the clipboard:&lt;/p&gt;
&lt;pre class=&quot;brush: xml; title: ; notranslate&quot; title=&quot;&quot;&gt;

use windows::application_model::data_transfer::*;

let content = DataPackage::new()?;
content.set_text(&quot;Rust/WinRT&quot;)?;

Clipboard::set_content(content)?;
Clipboard::flush()?;

&lt;/pre&gt;
&lt;p&gt;For a more complete example, I encourage you to have a look at Robert Mikhayelyan’s Minesweeper demo. Robert originally wrote a version of the classic game using C++/WinRT and was able to quickly port it over to using Rust/WinRT. &lt;a href=&quot;https://github.com/robmikh/minesweeper-rs&quot;&gt;https://github.com/robmikh/minesweeper-rs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-55724&quot; src=&quot;https://46c4ts1tskv22sdav81j9c69-wpengine.netdna-ssl.com/wp-content/uploads/prod/sites/3/2020/04/3e9cb9a0a010c9468750abea5ef80f22.gif&quot; alt=&quot;Minesweeper-opt&quot; width=&quot;967&quot; height=&quot;977&quot;/&gt;&lt;br/&gt;This is a very early public preview, but we have decided to work in the open from here on out. So please give it a try and let us know what you think. We would love the feedback as we continue to develop Rust/WinRT and plan to eventually publish on crates.io. We also hope to provide more seamless interop with existing Win32 and COM APIs including support for &lt;a href=&quot;http://github.com/microsoft/com-rs/&quot;&gt;the com-rs crate&lt;/a&gt;, which supports COM APIs today. &lt;a href=&quot;https://github.com/microsoft/winrt-rs&quot;&gt;https://github.com/microsoft/winrt-rs&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 17:27:24 +0000</pubDate>
<dc:creator>steveklabnik</dc:creator>
<og:type>article</og:type>
<og:title>Rust/WinRT Public Preview - Windows Developer Blog</og:title>
<og:description>We are excited to announce that the Rust/WinRT project finally has a permanent and public home on GitHub: https://github.com/microsoft/winrt-rs Rust/WinRT follows in the tradition established by C++/WinRT of building language projections for the Windows Runtime using standard languages and compilers, providing a natural and idiomatic way for Rust developers to call Windows APIs. Rust/WinRT lets …</og:description>
<og:url>https://blogs.windows.com/windowsdeveloper/2020/04/30/rust-winrt-public-preview/</og:url>
<og:image>https://46c4ts1tskv22sdav81j9c69-wpengine.netdna-ssl.com/wp-content/uploads/prod/sites/3/2020/04/3e9cb9a0a010c9468750abea5ef80f22.gif</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blogs.windows.com/windowsdeveloper/2020/04/30/rust-winrt-public-preview/</dc:identifier>
</item>
<item>
<title>Jukebox</title>
<link>https://openai.com/blog/jukebox/</link>
<guid isPermaLink="true" >https://openai.com/blog/jukebox/</guid>
<description>&lt;div class=&quot;full mt-0 mb-2 mb-md-3&quot;&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32.350597609562&quot;&gt;
&lt;div class=&quot;mt-2.5 mb-2&quot; readability=&quot;13.864541832669&quot;&gt;
&lt;h2 id=&quot;curated&quot;&gt;Curated samples&lt;/h2&gt;
&lt;p&gt;Provided with genre, artist, and lyrics as input, Jukebox outputs a new music sample produced from scratch. Below, we show some of our favorite samples.&lt;/p&gt;


&lt;p&gt;To hear all uncurated samples, check out our sample explorer.&lt;/p&gt;
&lt;a href=&quot;https://jukebox.openai.com/&quot; class=&quot;btn btn-padded icon-external right mb-0&quot;&gt;Explore All Samples&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;aside class=&quot;aside&quot;&gt;
&lt;/aside&gt;&lt;h2 id=&quot;motivationandpriorwork&quot;&gt;Motivation and prior work&lt;/h2&gt;
&lt;p&gt;Automatic music generation dates back to more than half a century. A prominent approach is to generate music symbolically in the form of a piano roll, which specifies the timing, pitch, velocity, and instrument of each note to be played. This has led to impressive results like producing Bach chorals, polyphonic music with multiple instruments, as well as minute long musical pieces.&lt;/p&gt;
&lt;p&gt;But symbolic generators have limitations—they cannot capture human voices or many of the more subtle timbres, dynamics, and expressivity that are essential to music. A different approach is to model music directly as raw audio. Generating music at the audio level is challenging since the sequences are very long. A typical 4-minute song at CD quality (44 kHz, 16-bit) has over 10 million timesteps. For comparision, GPT-2 had 1,000 timesteps and &lt;a href=&quot;https://openai.com/projects/five/&quot;&gt;OpenAI Five&lt;/a&gt; took tens of thousands of timesteps per game. Thus, to learn the high level semantics of music, a model would have to deal with extremely long-range dependencies.&lt;/p&gt;
&lt;p&gt;One way of addressing the long input problem is to use an autoencoder that compresses raw audio to a lower-dimensional space by discarding some of the perceptually irrelevant bits of information. We can then train a model to generate audio in this compressed space, and upsample back to the raw audio space.&lt;/p&gt;
&lt;p&gt;We chose to work on music because we want to continue to push the boundaries of generative models. Our previous work on &lt;a href=&quot;https://openai.com/blog/musenet&quot;&gt;MuseNet&lt;/a&gt; explored synthesizing music based on large amounts of MIDI data. Now in raw audio, our models must learn to tackle high diversity as well as very long range structure, and the raw audio domain is particularly unforgiving of errors in short, medium, or long term timing.&lt;/p&gt;
&lt;div class=&quot;mb-1/3&quot;&gt;
&lt;div class=&quot;position-relative&quot;&gt;
&lt;div id=&quot;overview-1&quot; class=&quot;position-absolute trbl-0&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/waveforms/1-original.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong class=&quot;color-fg&quot;&gt;Raw audio&lt;/strong&gt; 44.1k samples per second, where each sample is a float that represents the amplitude of sound at that moment in time&lt;/p&gt;
&lt;div class=&quot;row narrow-gutters align-items-center mb-0.25&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;col-auto col-sm-6&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/overview-arrow.svg&quot; class=&quot;ml-auto mb-0&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;col col-sm-6&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Encode using CNNs (convolutional neural networks)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;mb-1/3&quot;&gt;
&lt;div class=&quot;bg-light-warm-gray position-relative&quot;&gt;
&lt;div id=&quot;overview-2&quot; class=&quot;position-absolute trbl-0&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/overview-2.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong class=&quot;color-fg&quot;&gt;Compressed audio&lt;/strong&gt; 344 samples per second, where each sample is 1 of 2048 possible vocab tokens&lt;/p&gt;
&lt;div class=&quot;row narrow-gutters align-items-center mb-0.25&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;col-auto col-sm-6&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/overview-arrow.svg&quot; class=&quot;ml-auto mb-0&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;col col-sm-6&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Generate novel patterns from trained transformer conditioned on lyrics&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;mb-1/3&quot;&gt;
&lt;div class=&quot;position-relative&quot;&gt;
&lt;div id=&quot;overview-3&quot; class=&quot;position-absolute trbl-0&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/overview-3.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong class=&quot;color-fg&quot;&gt;Novel compressed audio&lt;/strong&gt; 344 samples per second&lt;/p&gt;
&lt;div class=&quot;row narrow-gutters align-items-center mb-0.25&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;col-auto col-sm-6&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/overview-arrow.svg&quot; class=&quot;ml-auto mb-0&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;col col-sm-6&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Upsample using transformers and decode using CNNs&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;mb-1/3&quot;&gt;
&lt;div class=&quot;position-relative&quot;&gt;
&lt;div id=&quot;overview-4&quot; class=&quot;position-absolute trbl-0&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/waveforms/1-novel.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong class=&quot;color-fg&quot;&gt;Novel raw audio&lt;/strong&gt; 44.1k samples per second&lt;/p&gt;
&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&quot;compressingmusictodiscretecodes&quot;&gt;Compressing music to discrete codes&lt;/h3&gt;
&lt;p&gt;Jukebox's autoencoder model compresses audio to a discrete space, using a quantization-based approach called VQ-VAE. Hierarchical VQ-VAEs can generate short instrumental pieces from a few sets of instruments, however they suffer from hierarchy collapse due to use of successive encoders coupled with autoregressive decoders. A simplified variant called VQ-VAE-2 avoids these issues by using feedforward encoders and decoders only, and they show impressive results at generating high-fidelity images.&lt;/p&gt;
&lt;p&gt;We draw inspiration from VQ-VAE-2 and apply their approach to music. We modify their architecture as follows:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;To alleviate codebook collapse common to VQ-VAE models, we use random restarts where we randomly reset a codebook vector to one of the encoded hidden states whenever its usage falls below a threshold.&lt;/li&gt;
&lt;li&gt;To maximize the use of the upper levels, we use separate decoders and independently reconstruct the input from the codes of each level.&lt;/li&gt;
&lt;li&gt;To allow the model to reconstruct higher frequencies easily, we add a spectral loss that penalizes the norm of the difference of input and reconstructed spectrograms.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We use three levels in our VQ-VAE, shown below, which compress the 44kHz raw audio by 8x, 32x, and 128x, respectively, with a codebook size of 2048 for each level. This downsampling loses much of the audio detail, and sounds noticeably noisy as we go further down the levels. However, it retains essential information about the pitch, timbre, and volume of the audio.&lt;/p&gt;
&lt;div id=&quot;vqvae&quot; class=&quot;full my-2 py-2 bg-fg-2&quot;&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;34.5&quot;&gt;

&lt;div class=&quot;medium-small-copy position-relative mb-1 mb-md-0.25&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;Each VQ-VAE level independently encodes the input. The bottom level encoding produces the highest quality reconstruction, while the top level encoding retains only the essential musical information.&lt;/p&gt;
&lt;p&gt;To generate novel songs, a cascade of transformers generates codes from top to bottom level, after which the bottom-level decoder can convert them to raw audio.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;wide my-0 overflow-hidden&quot;&gt;
&lt;div class=&quot;position-relative w-100&quot;&gt;
&lt;div id=&quot;vqvae-image-1&quot; class=&quot;mx-auto&quot;&gt;
&lt;div class=&quot;position-relative&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/vqvae-1-outlined.svg&quot; class=&quot;mb-0&quot;/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;vqvae-image-2&quot; class=&quot;mx-auto position-absolute trbl-0&quot;&gt;
&lt;div class=&quot;position-relative&quot;&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/vqvae-2-outlined.svg&quot; class=&quot;mb-0&quot;/&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&quot;generatingcodesusingtransformers&quot;&gt;Generating codes using transformers&lt;/h3&gt;
&lt;p&gt;Next, we train the prior models whose goal is to learn the distribution of music codes encoded by VQ-VAE and to generate music in this compressed discrete space. Like the VQ-VAE, we have three levels of priors: a top-level prior that generates the most compressed codes, and two upsampling priors that generate less compressed codes conditioned on above.&lt;/p&gt;
&lt;p&gt;The top-level prior models the long-range structure of music, and samples decoded from this level have lower audio quality but capture high-level semantics like singing and melodies. The middle and bottom upsampling priors add local musical structures like timbre, significantly improving the audio quality.&lt;/p&gt;
&lt;p&gt;We train these as autoregressive models using a simplified variant of Sparse Transformers. Each of these models has 72 layers of factorized self-attention on a context of 8192 codes, which corresponds to approximately 24 seconds, 6 seconds, and 1.5 seconds of raw audio at the top, middle and bottom levels, respectively.&lt;/p&gt;
&lt;p&gt;Once all of the priors are trained, we can generate codes from the top level, upsample them using the upsamplers, and decode them back to the raw audio space using the VQ-VAE decoder to sample novel songs.&lt;/p&gt;
&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;To train this model, we crawled the web to curate a new dataset of 1.2 million songs (600,000 of which are in English), paired with the corresponding lyrics and metadata from &lt;a href=&quot;https://lyrics.fandom.com/wiki/LyricWiki&quot;&gt;LyricWiki&lt;/a&gt;. The metadata includes artist, album genre, and year of the songs, along with common moods or playlist keywords associated with each song. We train on 32-bit, 44.1 kHz raw audio, and perform data augmentation by randomly downmixing the right and left channels to produce mono audio.&lt;/p&gt;
&lt;h3 id=&quot;artistandgenreconditioning&quot;&gt;Artist and genre conditioning&lt;/h3&gt;
&lt;p&gt;The top-level transformer is trained on the task of predicting compressed audio tokens. We can provide additional information, such as the artist and genre for each song. This has two advantages: first, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style; second, at generation time, we are able to steer the model to generate in a style of our choosing.&lt;/p&gt;
&lt;p&gt;This t-SNE below shows how the model learns, in an unsupervised way, to cluster similar artists and genres close together, and also makes some surprising associations like Jennifer Lopez being so close to Dolly Parton!&lt;/p&gt;


&lt;h3 id=&quot;lyricsconditioning&quot;&gt;Lyrics conditioning&lt;/h3&gt;
&lt;p&gt;In addition to conditioning on artist and genre, we can provide more context at training time by conditioning the model on the lyrics for a song. A significant challenge is the lack of a well-aligned dataset: we only have lyrics at a song level without alignment to the music, and thus for a given chunk of audio we don’t know precisely which portion of the lyrics (if any) appear. We also may have song versions that don’t match the lyric versions, as might occur if a given song is performed by several different artists in slightly different ways. Additionally, singers frequently repeat phrases, or otherwise vary the lyrics, in ways that are not always captured in the written lyrics.&lt;/p&gt;
&lt;p&gt;To match audio portions to their corresponding lyrics, we begin with a simple heuristic that aligns the characters of the lyrics to linearly span the duration of each song, and pass a fixed-size window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres with fast lyrics, such as hip hop. To address this, we use Spleeter to extract vocals from each song and run NUS AutoLyricsAlign on the extracted vocals to obtain precise word-level alignments of the lyrics. We chose a large enough window so that the actual lyrics have a high probability of being inside the window.&lt;/p&gt;
&lt;p&gt;To attend to the lyrics, we add an encoder to produce a representation for the lyrics, and add attention layers that use queries from the music decoder to attend to keys and values from the lyrics encoder. After training, the model learns a more precise alignment.&lt;/p&gt;
&lt;div class=&quot;mt-1.5 mb-2&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.openai.com/jukebox/assets/lyrics-attention.svg&quot; alt=&quot;lyrics-attention&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong class=&quot;color-fg&quot;&gt;Lyric–music alignment learned by encoder–decoder attention layer&lt;/strong&gt;&lt;br/&gt;Attention progresses from one lyric token to the next as the music progresses, with a few moments of uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&quot;limitations&quot;&gt;Limitations&lt;/h2&gt;
&lt;p&gt;While Jukebox represents a step forward in musical quality, coherence, length of audio sample, and ability to condition on artist, genre, and lyrics, there is a significant gap between these generations and human-created music.&lt;/p&gt;
&lt;p&gt;For example, while the generated songs show local musical coherence, follow traditional chord patterns, and can even feature impressive solos, we do not hear familiar larger musical structures such as choruses that repeat. Our downsampling and upsampling process introduces discernable noise. Improving the VQ-VAE so its codes capture more musical information would help reduce this. Our models are also slow to sample from, because of the autoregressive nature of sampling. It takes approximately 9 hours to fully render one minute of audio through our models, and thus they cannot yet be used in interactive applications. Using techniques that distill the model into a parallel sampler can significantly speed up the sampling speed. Finally, we currently train on English lyrics and mostly Western music, but in the future we hope to include songs from other languages and parts of the world.&lt;/p&gt;
&lt;h2 id=&quot;futuredirections&quot;&gt;Future directions&lt;/h2&gt;
&lt;p&gt;Our audio team is continuing to work on generating audio samples conditioned on different kinds of priming information. In particular, we've seen early success conditioning on MIDI files and stem files. Here's an example of a &lt;a href=&quot;https://soundcloud.com/openai_audio/generated-raw&quot;&gt;raw audio sample&lt;/a&gt; conditioned on &lt;a href=&quot;https://soundcloud.com/openai_audio/midi-input-given-to-model-as-midi-tokens-rendered-here-by-timidity&quot;&gt;MIDI tokens&lt;/a&gt;. We hope this will improve the musicality of samples (in the way conditioning on lyrics improved the singing), and this would also be a way of giving musicians more control over the generations. We expect human and model collaborations to be an increasingly exciting creative space. If you’re excited to work on these problems with us, &lt;a href=&quot;https://openai.com/jobs/&quot;&gt;we’re hiring&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As generative modeling across various domains continues to advance, we are also conducting research into issues like &lt;a href=&quot;https://arxiv.org/abs/1908.09203&quot;&gt;bias&lt;/a&gt; and &lt;a href=&quot;https://cdn.openai.com/policy-submissions/OpenAI+Comments+on+Intellectual+Property+Protection+for+Artificial+Intelligence+Innovation.pdf&quot;&gt;intellectual property rights&lt;/a&gt;, and are engaging with people who work in the domains where we develop tools. To better understand future implications for the music community, we shared Jukebox with an initial set of 10 musicians from various genres to discuss their feedback on this work. While Jukebox is an interesting research result, these musicians did not find it immediately applicable to their creative process given some of its current &lt;a href=&quot;https://openai.com/blog/jukebox/#limitations&quot;&gt;limitations&lt;/a&gt;. We are connecting with the wider creative community as we think generative work across text, images, and audio will continue to improve. If you're interested in being a creative collaborator to help us build &lt;a href=&quot;https://openai.com/blog/musenet/&quot;&gt;useful tools&lt;/a&gt; or new works of art in these domains, please &lt;a href=&quot;https://forms.gle/8npHSMnE5hfSxkkU9&quot;&gt;let us know&lt;/a&gt;!&lt;/p&gt;
&lt;section class=&quot;btns&quot;&gt;&lt;a href=&quot;https://forms.gle/8npHSMnE5hfSxkkU9&quot; class=&quot;btn btn-padded icon-external right&quot;&gt;Creative Collaborator Sign-Up&lt;/a&gt;&lt;/section&gt;&lt;p&gt;&lt;em&gt;To connect with the corresponding authors, please email &lt;a href=&quot;mailto:jukebox@openai.com&quot;&gt;jukebox@openai.com&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;full bg-shadow&quot;&gt;
&lt;div class=&quot;container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;ul class=&quot;timeline&quot; id=&quot;timeline&quot;&gt;&lt;li&gt;
&lt;h4&gt;Timeline&lt;/h4&gt;
&lt;/li&gt;
&lt;li data-date=&quot;August 2019&quot;&gt;Our first raw audio model, which learns to recreate instruments like Piano and Violin. We try a dataset of rock and pop songs, and surprisingly it works. 
&lt;hr class=&quot;my-5/12 mt-1&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt;
&lt;hr class=&quot;my-5/12&quot;/&gt;&lt;/li&gt;
&lt;li&gt;&lt;br/&gt;&lt;/li&gt;
&lt;li data-date=&quot;October 2019&quot;&gt;We collect a larger and more diverse dataset of songs, with labels for genres and artists. Model picks up artist and genre styles more consistently with diversity, and at convergence can also produce full-length songs with long-range coherence. 
&lt;hr class=&quot;my-5/12 mt-1&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt;
&lt;hr class=&quot;my-5/12&quot;/&gt;&lt;/li&gt;
&lt;li&gt;&lt;br/&gt;&lt;/li&gt;
&lt;li data-date=&quot;January 2020&quot;&gt;We scale our VQ-VAE from 22 to 44kHz to achieve higher quality audio. We also scale top-level prior from 1B to 5B to capture the increased information. We see better musical quality, clear singing, and long-range coherence. We also make novel completions of real songs. 
&lt;hr class=&quot;my-5/12 mt-1&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt;
&lt;hr class=&quot;my-5/12&quot;/&gt;&lt;/li&gt;
&lt;li&gt;&lt;br/&gt;&lt;/li&gt;
&lt;li data-date=&quot;February 2020&quot;&gt;We start training models conditioned on lyrics to incorporate further conditioning information. We only have unaligned lyrics, so model has to learn alignment and pronunciation, as well as singing. 
&lt;hr class=&quot;my-5/12 mt-1&quot;/&gt; 
&lt;hr class=&quot;my-5/12&quot;/&gt;
&lt;hr class=&quot;my-5/12&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Thu, 30 Apr 2020 15:55:55 +0000</pubDate>
<dc:creator>gdb</dc:creator>
<og:type>article</og:type>
<og:title>Jukebox</og:title>
<og:description>We’re introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We’re releasing the model weights and code, along with a tool to explore the generated samples.</og:description>
<og:url>https://openai.com/blog/jukebox/</og:url>
<og:image>https://openai.com/content/images/2020/04/2x-no-mark-1.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://openai.com/blog/jukebox/</dc:identifier>
</item>
<item>
<title>Selfie 2 Waifu</title>
<link>https://twitter.com/tkasasagi/status/1250427941567094786</link>
<guid isPermaLink="true" >https://twitter.com/tkasasagi/status/1250427941567094786</guid>
<description>[unable to retrieve full-text content]
&lt;p&gt;Article URL: &lt;a href=&quot;https://twitter.com/tkasasagi/status/1250427941567094786&quot;&gt;https://twitter.com/tkasasagi/status/1250427941567094786&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=23032083&quot;&gt;https://news.ycombinator.com/item?id=23032083&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Points: 225&lt;/p&gt;
&lt;p&gt;# Comments: 81&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 15:42:55 +0000</pubDate>
<dc:creator>guu</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://mobile.twitter.com/tkasasagi/status/1250427941567094786</dc:identifier>
</item>
<item>
<title>The average consumer loses over $150 a year renting a router from their ISP</title>
<link>https://www.reviews.com/utilities/internet/the-average-consumer-loses-over-150-a-year-renting-a-router-from-their-isp/</link>
<guid isPermaLink="true" >https://www.reviews.com/utilities/internet/the-average-consumer-loses-over-150-a-year-renting-a-router-from-their-isp/</guid>
<description>&lt;p&gt;Fast facts:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Our analysis of data from major ISPs found that the average customer who rents equipment for their internet service could save upwards of &lt;strong&gt;$150 per year&lt;/strong&gt; by purchasing their own router.&lt;/li&gt;
&lt;li&gt;In contrast, a top-rated router can be as cheap as $70 as a one-time expense.&lt;/li&gt;
&lt;li&gt;Purchasing personal equipment will save consumers &lt;strong&gt;$750 over the lifetime of a router&lt;/strong&gt; and thousands more over a lifetime of internet usage.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;It’s something that most people don’t think about, but when they sign up for residential internet service, there are often several hidden costs that can drastically increase their monthly bill. One of the most expensive additions is paying to rent equipment directly from the ISP like a router and modem. While most people might not notice a $10-$15 addition on the cable bill each month, over the lifetime of a service plan, those costs can add up to hundreds or even thousands of dollars in unnecessary fees.&lt;/p&gt;
&lt;p&gt;To get a specific number, we pulled data from all major ISPs to average out how much customers are paying to rent equipment directly from their cable company. For internet service, for example, the average customer is paying between $12-$15 per month to rent equipment like a router and modem.&lt;/p&gt;
&lt;p&gt;Over the lifetime of a cable contract, this could end up being thousands of dollars. In other words, you could end up making costly payments for a piece of equipment that costs less than $100 that will last years.&lt;/p&gt;
&lt;p&gt;By researching major e-commerce platforms, consumers can easily find a high-quality router for $50-$70, depending on their needs. This one-time expense will pay for itself in under six months, and will rarely need a replacement or upgrade. Even more expensive, top-of-the-line routers will pay for themselves within a year or two.&lt;/p&gt;
&lt;p&gt;There might be an argument that many consumers feel intimidated by managing their own equipment, as things like set up and maintenance can feel technically demanding for the average person. While most ISPs won’t provide service on the equipment you’re not renting from them, it’s rare for a router to need much more maintenance than the occasional reset. Once the initial setup is complete, it would be unusual to need much more technical support.&lt;/p&gt;
&lt;p&gt;We spoke with several cable customers about their experience setting up their own router, and they found it to be far less complicated than they had imagined.&lt;/p&gt;
&lt;p&gt;“It took maybe 15 minutes and I’m not a technical person. I followed the instructions in the box and had my own router working with no issues at all,” one respondent said when asked about their experience.&lt;/p&gt;
&lt;p&gt;It can be important to upgrade equipment every so often, but it’s not an annual requirement or even every two to three-year commitment. While the demand for faster downloads for things like movies and TV streaming services or online games has never been higher, most routers will be up to the task for years to come. And while technology and security changes may cause the need for an update after five years or so, major brands will continue to support their products.&lt;/p&gt;
&lt;p&gt;If you are looking for a simple way to trim expenses from your monthly budget, consider purchasing your own router instead of renting it directly from your ISP.&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 15:36:02 +0000</pubDate>
<dc:creator>sharkweek</dc:creator>
<og:type>article</og:type>
<og:title>The Average Consumer Loses Over $150 a Year Renting a Router From Their ISP - Reviews.com</og:title>
<og:description>Don't throw money away by renting a router from your internet provider. Find out how much you can save just by buying your own.</og:description>
<og:url>https://www.reviews.com/utilities/internet/the-average-consumer-loses-over-150-a-year-renting-a-router-from-their-isp/</og:url>
<og:image>https://www.reviews.com/wp-content/uploads/2020/04/renting-router-from-isp.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reviews.com/utilities/internet/the-average-consumer-loses-over-150-a-year-renting-a-router-from-their-isp/</dc:identifier>
</item>
<item>
<title>Colleges at the breaking point, forcing ‘hard choices’ about education</title>
<link>https://www.wsj.com/articles/coronavirus-pushes-colleges-to-the-breaking-point-forcing-hard-choices-about-education-11588256157</link>
<guid isPermaLink="true" >https://www.wsj.com/articles/coronavirus-pushes-colleges-to-the-breaking-point-forcing-hard-choices-about-education-11588256157</guid>
<description>&lt;p&gt;MacMurray College survived the Civil War, the Great Depression and two world wars, but not the coronavirus pandemic. The private liberal-arts school in central Illinois announced recently it will shut its doors for good in May, after 174 years.&lt;/p&gt; &lt;p&gt;Like many small schools, it faced declining enrollment and financial shortfalls. To lure prospective students, it was using steep discounts to its $30,000 listed tuition. Then the global health crisis brought unexpected costs for shifting classes online and partially reimbursing room...
  &lt;/p&gt;</description>
<pubDate>Thu, 30 Apr 2020 14:48:47 +0000</pubDate>
<dc:creator>PretzelFisch</dc:creator>
<og:title>Coronavirus Pushes Colleges to the Breaking Point, Forcing ‘Hard Choices’ About Education</og:title>
<og:description>Forecast declines in enrollment and revenue have triggered spending cuts and salary freezes. “The world order has changed,” said a Northeastern University official.</og:description>
<og:url>https://www.wsj.com/articles/coronavirus-pushes-colleges-to-the-breaking-point-forcing-hard-choices-about-education-11588256157</og:url>
<og:image>https://images.wsj.net/im-181467/social</og:image>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wsj.com/articles/coronavirus-pushes-colleges-to-the-breaking-point-forcing-hard-choices-about-education-11588256157</dc:identifier>
</item>
<item>
<title>Enigma: Erlang VM Implementation in Rust</title>
<link>https://github.com/archseer/enigma</link>
<guid isPermaLink="true" >https://github.com/archseer/enigma</guid>
<description>&lt;p&gt;&lt;a target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; href=&quot;https://github.com/archseer/enigma/blob/master/enigma.png&quot;&gt;&lt;img src=&quot;https://github.com/archseer/enigma/raw/master/enigma.png&quot; alt=&quot;Enigma&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/archseer/enigma&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/185c1dba3ce915a4e01f4ff2ba6e9b0e3f8e3a19/68747470733a2f2f6170692e7472617669732d63692e6f72672f61726368736565722f656e69676d612e7376673f6272616e63683d6d6173746572&quot; alt=&quot;Build status&quot; data-canonical-src=&quot;https://api.travis-ci.org/archseer/enigma.svg?branch=master&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://ci.appveyor.com/project/archseer/enigma&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/3f78d2cb795197bc5f19478d39cdf4e28d2a0ada/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f61726368736565722f656e69676d613f7376673d74727565&quot; alt=&quot;Windows build status&quot; data-canonical-src=&quot;https://ci.appveyor.com/api/projects/status/github/archseer/enigma?svg=true&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An implementation of the Erlang VM in Rust. We aim to be complete, correct and fast, in that order of importance.&lt;/p&gt;
&lt;p&gt;OTP 22+ compatible (sans the distributed bits for now) — all your code should eventually run on Enigma unchanged. Deprecated opcodes won't be supported.&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;Because it's fun and I've been learning a lot. BEAM and HiPE are awesome, but they're massive (~300k SLOC). A small implementation makes it easier for new people to learn Erlang internals. We also get a platform to quickly iterate on ideas for inclusion into BEAM.&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Only prerequisite to building Enigma is Rust. Use &lt;a href=&quot;https://rustup.rs/&quot; rel=&quot;nofollow&quot;&gt;rustup&lt;/a&gt; to install the latest nightly rust. At this time we don't support stable / beta anymore, because we're relying on async/await, which is scheduled to run in stable some time in Q3 2019.&lt;/p&gt;
&lt;p&gt;To boot up OTP you will also need to compile the standard library. At the moment, that relies on the BEAM build system:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot; readability=&quot;7&quot;&gt;
&lt;pre&gt;
git submodule update --init --depth 1
&lt;span class=&quot;pl-c1&quot;&gt;cd&lt;/span&gt; otp
/otp_build setup -a
make libs
make local_setup
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We hope to simplify this step in the future (once enigma can run the compiler).&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;cargo run&lt;/code&gt; to install dependencies, build and run the VM. By default, it will boot up the erlang shell (&lt;a href=&quot;https://asciinema.org/a/zIjbf5AJx9YxEycyl9ij5ISVM&quot; rel=&quot;nofollow&quot;&gt;iex also works, but has some rendering bugs&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Expect crashes, but a lot of the functionality is already available.&lt;/p&gt;
&lt;p&gt;Pre-built binaries for various platforms will be available, once we reach a certain level of stability.&lt;/p&gt;
&lt;h2&gt;Feature status&lt;/h2&gt;
&lt;p&gt;We implement most of the opcodes, and about half of all BIFs. You can view a detailed progress breakdown on &lt;a href=&quot;https://github.com/archseer/enigma/blob/master/notes/opcodes.org&quot;&gt;opcodes&lt;/a&gt; or &lt;a href=&quot;https://github.com/archseer/enigma/blob/master/notes/bifs.org&quot;&gt;BIFs&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Roadmap&lt;/h4&gt;
&lt;h4&gt;Features&lt;/h4&gt;
&lt;h3&gt;Goals, ideas &amp;amp; experiments&lt;/h3&gt;
&lt;p&gt;Process scheduling is implemented on top of rust futures:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;A process is simply a long running future, scheduled on top of tokio-threadpool work-stealing queue&lt;/li&gt;
&lt;li&gt;A timer is a delay/timeout future relying on tokio-timer time-wheel&lt;/li&gt;
&lt;li&gt;Ports are futures we can await on&lt;/li&gt;
&lt;li&gt;File I/O is AsyncRead/AsyncWrite awaitable&lt;/li&gt;
&lt;li&gt;NIF/BIFs are futures that yield at certain points to play nice with reductions (allows a much simpler yielding implementation)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Future possibilities:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Write more documentation about more sparsely documented BEAM aspects (binary matching, time wheel, process monitors, ...)&lt;/li&gt;
&lt;li&gt;Explore using immix as a GC for Erlang&lt;/li&gt;
&lt;li&gt;Eir runtime&lt;/li&gt;
&lt;li&gt;JIT via Eir&lt;/li&gt;
&lt;li&gt;BIF as a generator function (yield to suspend/on reduce)&lt;/li&gt;
&lt;li&gt;Provide built-in adapter modules for &lt;a href=&quot;https://github.com/hyperium/hyper&quot;&gt;hyper&lt;/a&gt; as a Plug Adapter / HTTP client.&lt;/li&gt;
&lt;li&gt;Cross-compile to WebAssembly (&lt;a href=&quot;https://github.com/rustasync/runtime/&quot;&gt;runtime&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Initial non-goals&lt;/h4&gt;
&lt;p&gt;Until the VM doesn't reach a certain level of completeness, it doesn't make sense to consider these.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Distributed Erlang nodes&lt;/li&gt;
&lt;li&gt;Tracing / debugging support&lt;/li&gt;
&lt;li&gt;BEAM compatible NIFs / FFI&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Note: NIF/FFI ABI compatibility with OTP is going to be quite some work. But, a rust-style NIF interface will be available. It would also probably be possible to make an adapter compatible with &lt;a href=&quot;https://github.com/rusterlium/rustler&quot;&gt;rustler&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Contributing&lt;/h2&gt;
&lt;p&gt;Contributors are very welcome!&lt;/p&gt;
&lt;p&gt;The easiest way to get started is to look at the &lt;code&gt;notes&lt;/code&gt; folder and pick a BIF or an opcode to implement. Take a look at &lt;code&gt;src/bif.rs&lt;/code&gt; and the &lt;code&gt;bif&lt;/code&gt; folder on how other BIFs are implemented. There's also a few issues open with the &lt;code&gt;good first issue&lt;/code&gt; tag, which would also be a good introduction to the internals.&lt;/p&gt;
&lt;p&gt;Alternatively, search the codebase for &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;FIXME&lt;/code&gt; or &lt;code&gt;unimplemented!&lt;/code&gt;, those mark various places where a partial implementation exists, but a bit more work needs to be done.&lt;/p&gt;
&lt;p&gt;Test coverage is currently lacking, and there's varying levels of documentation; I will be addressing these soon.&lt;/p&gt;
&lt;p&gt;We also have a #enigma channel on the &lt;a href=&quot;https://elixir-slackin.herokuapp.com/&quot; rel=&quot;nofollow&quot;&gt;Elixir Slack&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 14:48:13 +0000</pubDate>
<dc:creator>adamnemecek</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/1372918?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>archseer/enigma</og:title>
<og:url>https://github.com/archseer/enigma</og:url>
<og:description>A simple Erlang VM implementation in Rust. Contribute to archseer/enigma development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/archseer/enigma</dc:identifier>
</item>
<item>
<title>Redis 6.0 GA</title>
<link>http://antirez.com/news/132</link>
<guid isPermaLink="true" >http://antirez.com/news/132</guid>
<description>&lt;section id=&quot;newslist&quot;&gt;&lt;article data-news-id=&quot;132&quot;&gt;
&lt;/article&gt;&lt;/section&gt;&lt;article class=&quot;comment&quot; data-comment-id=&quot;132-&quot; id=&quot;132-&quot; readability=&quot;39.450783214079&quot;&gt;&lt;span class=&quot;info&quot;&gt;&lt;span class=&quot;username&quot;&gt;&lt;a href=&quot;http://antirez.com/user/antirez&quot;&gt;antirez&lt;/a&gt;&lt;/span&gt; 11 hours ago. 50478 views.&lt;/span&gt;
&lt;pre&gt;
Finally Redis 6.0.0 stable is out. This time it was a relatively short cycle between the release of the first release candidate and the final release of a stable version. It took about four months, that is not a small amount of time, but is not a lot compared to our past records :)

So the big news are the ones announced before, but with some notable changes. The old stuff are: SSL, ACLs, RESP3, Client side caching, Threaded I/O, Diskless replication on replicas, Cluster support in Redis-benchmark and improved redis-cli cluster support, Disque in beta as a module of Redis, and the Redis Cluster Proxy (now at &lt;a rel=&quot;nofollow&quot; href=&quot;https://github.com/RedisLabs/redis-cluster-proxy&quot;&gt;https://github.com/RedisLabs/redis-cluster-proxy&lt;/a&gt;).

So what changed between RC1 and today, other than stability?

1. Client side caching was redesigned in certain aspects, especially the caching slot approach was discarded in favor of just using key names. After analyzing the alternatives, with the help of other Redis core team members, in the end this approach looks better. Other than that, finally the feature was completed with the things I had in the backlog for the feature, especially the “broadcasting mode”, that I believe will be one of the most popular usage modes of the feature.

When broadcasting is used, the server no longer try to remember what keys each client requested. Instead clients subscribe to key prefixes: they’ll get notifications every time a key matching the prefix is modified. This means more messages (but only for the selected prefixes), but no memory effort in the server side. Moreover the opt-in / opt-out mode is now supported, so it is possible for clients not using the broadcasting mode, to exactly tell the server about what the client will cache, to reduce the number of invalidation messages. Basically the feature is now much better both when a low-memory mode is needed, and when a very selective (low-bandwidth) mode is needed.

2. This was an old request by many users. Now Redis supports a mode where RDB files used for replication are immediately deleted if no longer useful. In certain environments it is a good idea to never have the data around on disk, but just in memory.

3. ACLs are better in a few regards. First, there is a new ACL LOG command that allows to see all the clients that are violating the ACLs, accessing commands they should not, accessing keys they should not, or with failed authentication attempts. The log is actually in memory, so every external agent can call “ACL LOG” to see what’s going on. This is very useful in order to debug ACL problems.

But my preferred feature is the reimplementation of ACL GENPASS. Now it uses SHA256 based HMAC, and accepts an optional argument to tell the server how many bits of unguessable pseudo random string you want to generate. Redis seeds an internal key at startup from /dev/urandom, and later uses the HMAC in counter mode in order to generate the other random numbers: this way you can abuse the API, and call it every time you want, since it will be very fast. Want to generate an unguessable session ID for your application? Just call ACL GENPASS. And so forth.

4. PSYNC2, the replication protocol, is now improved. Redis will be able to partially resynchronize more often, since now is able to trim the final PINGs in the protocol, to make more likely that replicas and masters can find a common offset.

5. Redis commands with timeouts are now much better: not only BLPOP and other commands that used to accept seconds, now accept decimal numbers, but the actual resolution was improved in order to never be worse than the current “HZ” value, regardless of the number of clients connected.

6. RDB files are now faster to load. You can expect a 20/30% improvement, depending on the file actual composition (larger or smaller values). INFO is also faster now when there are many clients connected, this was a long time problem that now is finally gone.

7. We have a new command, STRALGO, that implements complex string algorithms. For now the only one implemented is LCS (longest common subsequence), an important algorithm used, among the other things, in order to compare the RNA of the coronaviruses (and in general the DNA and RNA of other organisms). What is happening is too big, somewhat a trace inside Redis needed to remain.

Redis 6 is the biggest release of Redis *ever*, so even if it is stable, handle it with care, test it for your workload before putting it in production. We never saw big issues so far, but make sure to be careful. As we collect bug reports, we will prepare to release Redis 6.0.1 ASAP.

A big thank you to the many people that wrote code with me in this release, and to all the companies that sponsored both my work (Thanks Redis Labs), and the the work of the other contributors (Thanks other companies). Also a big thank you to the many that signaled bugs with care, sometimes following the boring process of reiterating after making some changes, or that suggested improvements of any kind.

As usually you can find Redis 6 in different places: at &lt;a rel=&quot;nofollow&quot; href=&quot;https://redis.io&quot;&gt;https://redis.io&lt;/a&gt; as tarball, and in the Github repository tagged as “6.0.0”.

Enjoy Redis 6,
antirez
&lt;/pre&gt;&lt;/article&gt;
&lt;noscript readability=&quot;1.15625&quot;&gt;
&lt;p&gt;Please enable JavaScript to view the &lt;a href=&quot;http://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/p&gt;
&lt;/noscript&gt; &lt;a href=&quot;http://disqus.com&quot; class=&quot;dsq-brlink&quot;&gt;blog comments powered by &lt;span class=&quot;logo-disqus&quot;&gt;Disqus&lt;/span&gt;&lt;/a&gt;</description>
<pubDate>Thu, 30 Apr 2020 13:42:36 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://antirez.com/news/132</dc:identifier>
</item>
<item>
<title>Rules of thumb for a 1x developer</title>
<link>https://muldoon.cloud/programming/2020/04/17/programming-rules-thumb.html</link>
<guid isPermaLink="true" >https://muldoon.cloud/programming/2020/04/17/programming-rules-thumb.html</guid>
<description>&lt;p&gt;I’m not a &lt;a href=&quot;https://www.techopedia.com/definition/31673/10x-developer&quot;&gt;10x developer&lt;/a&gt;. I haven’t been building websites since the days of dialup. I picked up programming a few years out of college because I was at a dead end in my career in government and politics.&lt;/p&gt;
&lt;p&gt;So I’ve been writing code 9-5 for about five years, all of which have been at Amazon. And during that time, I also raised three kids – who were aged 0, 0, and 2 when I started. So I was not That Guy overachieving on nights and weekends. I was taking care of my family.&lt;/p&gt;
&lt;p&gt;I contributed to my first open-source project about a year ago. That was my first ever side project. And it was a conference website.&lt;/p&gt;
&lt;p&gt;I have never read a book cover-to-cover on software engineering.&lt;/p&gt;
&lt;p&gt;In summary, I am a 1x developer. I do enough to get by respectably.&lt;/p&gt;
&lt;h2 id=&quot;i-have-little-to-say-thats-of-general-value-and-havent-found-the-time-to-level-up&quot;&gt;I have little to say that’s of general value and haven’t found the time to level up&lt;/h2&gt;
&lt;p&gt;Being a 1x developer, I make heavy use of conventional wisdom. I don’t have much that’s interesting or new to say about software engineering.&lt;/p&gt;
&lt;p&gt;In general, I haven’t found the time to take long stretches away from work, or to take on bigger side projects, or to read really extensively, or to dramatically overachieve. I have little pockets of time here and there.&lt;/p&gt;
&lt;p&gt;But I’m feeling ambitious. I would like to become a 1.1x developer. I’m trying to figure out how to get there.&lt;/p&gt;
&lt;h2 id=&quot;my-theory-of-bootstrapping&quot;&gt;My theory of bootstrapping&lt;/h2&gt;
&lt;p&gt;This post comes at a moment when I’ve got a bit more flexible time, due to COVID-19 and the quarantine and the fact that I’m taking some vacation days from work but can’t actually go anywhere, while the kids are on “Spring Break” from our Pandemic Homeschool.&lt;/p&gt;
&lt;p&gt;So I’m going to try to bootstrap my software engineering longbeard wisdom in the following manner:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Write out my inane thoughts on some software engineering topics.&lt;/li&gt;
&lt;li&gt;Share out my thoughts to people smarter than me and invite vicious critique.&lt;/li&gt;
&lt;li&gt;Update based on #2.&lt;/li&gt;
&lt;li&gt;Attempt to come up with a methodology for finding and prioritizing useful information to read about software engineering, or reflections based on new projects, and integrating into #1.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With no further ado, below are my Rules of Thumb for a 1x Developer.&lt;/p&gt;

&lt;h3 id=&quot;rule-1-rules-are-good&quot;&gt;Rule 1: Rules are good&lt;/h3&gt;
&lt;p&gt;My observation is that humans are by nature irrational, inconsistent, authoritarian pattern-seekers. That is, in general, most people will implicitly tend to set their goals in a way that pleases the Leader, that copies what other people nearby are doing, and that follows some existing pattern. Being rational and being consistent in a complicated world are both monumentally hard. Therefore, it’s a really convenient shortcut to “do what your leader tells you” or “do what your peers are doing.”&lt;/p&gt;
&lt;p&gt;That strategy serves people pretty well in a lot of cases and does help build “team unity.” But when your goals are really external – something more than “team unity,” but about accomplishing an objective outside of the team, then it is pretty valuable to try to be rational and consistent.&lt;/p&gt;
&lt;p&gt;I think that rules of thumb (or if you’d like to dress them up, call them principles), help provide some guardrails, some structure, some scaffolding to allow people to step out of the pattern-following default mindset. They help reinforce a degree of logical consistency. They help focus conversations when there are a lot of competing priorities and values. And they help provide a more explicit, more thought-through shortcut when you’re trying to figure out how to make a decision when there are endless options.&lt;/p&gt;
&lt;p&gt;Amazon, where I work, is pretty well-known for having a set of principles that are fairly ubiquitous and come up in hiring, in making tough decisions, in evaluating tradeoffs. And I think the company is better for having them. Not so much because they contain any special pearl of genius, but just because having principles in general is better than falling back to the follow-the-leader behavior.&lt;/p&gt;
&lt;p&gt;Thinking is hard. Making decisions is hard. Having some thought-through rules to guide decision helps us be a bit better.&lt;/p&gt;
&lt;p&gt;That all said, the rules of thumb below aren’t really like Amazon’s principles. They’re mostly just rules of thumb that represent what’s usually going on in my mind when I think about one of these topics.&lt;/p&gt;
&lt;h2 id=&quot;productivity-and-learning&quot;&gt;Productivity and learning&lt;/h2&gt;
&lt;h3 id=&quot;rule-2-most-of-what-i-learn-is-useless-outside-of-its-immediate-context&quot;&gt;Rule 2: Most of what I learn is useless outside of its immediate context&lt;/h3&gt;
&lt;p&gt;I sometimes hear people say the only thing that an IQ test measures is “how good you are at taking an IQ test.” I agree with that, and with the general point that most knowledge is highly contextual. I’ve had about seven different “real jobs” since college, each one with a different job title. Within Amazon, I’ve worked on two totally different kinds of products and industries.&lt;/p&gt;
&lt;p&gt;And what I have found is that most – say, 90%, of what I learn at one job is completely useless for the next one. Even within the same career area. Not all of it. And that 10% might be a really important 10%, like how to be better at email or how to navigate office politics. But still most of what I learn is complete “throwaway work” when I start the next job. It’s very specific knowledge about how a very specific organization operates.&lt;/p&gt;
&lt;p&gt;Amazon is probably an extreme example, because developers here spend an enormous amount of time mastering internal tools and business concepts that literally don’t exist anywhere else. I’m probably somewhat colored by that. I know that other developers have a different experience – they master a specific technology and those skills actually do travel with them from job to job.&lt;/p&gt;
&lt;p&gt;In education (where I did spend a few years working), there’s just a fact that most knowledge is not transferable. Learning something in one domain generally does not help with any other domain.&lt;/p&gt;
&lt;p&gt;So, selfishly, I tend to think this way: if 90% of what I’m doing today won’t help me in the next job, then I should either reduce that 90% (for example, by focusing on general-purpose ideas, systems, abstractions, technologies), just not care as much about that 90% (as I used to), or find a job where I’m willing to sacrifice that 90% of my time because I think the cause is really important or the money is so good.&lt;/p&gt;
&lt;p&gt;This rule is probably more specific to me, since I’ve changed careers a bunch. For other people, they’ve been able to transfer most of their skills from one job to the next.&lt;/p&gt;
&lt;h3 id=&quot;rule-3-focus-learning-time-on-things-that-compound&quot;&gt;Rule 3: Focus “learning time” on things that compound&lt;/h3&gt;
&lt;p&gt;Compounding is a pretty important concept that shows up in compound interest, in Moore’s Law, all over the place. It’s about virtuous cycles. And so in the limited flexible time that I have, I think the rule of thumb is to focus on things that could trigger a virtuous cycle.&lt;/p&gt;
&lt;p&gt;One perfect place to start is becoming a faster typer. I’ve spent a while in the past two months on keybr.com that teaches you pretty methodically how to type faster. Getting faster allows me to write more, to communicate more, to get more done in the same amount of time, because it makes almost everything I do on the computer faster.&lt;/p&gt;
&lt;p&gt;Building lasting relationships is also a compounding activity because it gives you access to more people who can help you get things done more quickly.&lt;/p&gt;
&lt;p&gt;Consuming media (books, blogs, whatever) is not inherently a compounding thing. Only if you have some kind of method to reflect, to digest, to incorporate your knowledge into your thoughts. If there is anything valuable in this post, you, reader, will probably not benefit from it unless you do something active to “process” it immediately.&lt;/p&gt;
&lt;h2 id=&quot;programming-languages&quot;&gt;Programming languages&lt;/h2&gt;
&lt;p&gt;These will probably expose my ignorance pretty nicely.&lt;/p&gt;
&lt;h3 id=&quot;rule-4-when-to-use-java-or-c&quot;&gt;Rule 4: When to use Java or C#&lt;/h3&gt;
&lt;p&gt;Java is ideal for large enterprise applications, and it’s hard to imagine Amazon, for example, running on anything else. That’s because it’s got the deepest libraries and community support, and the static typing makes it much easier to deal with zillions of internal data models inside a large company.&lt;/p&gt;
&lt;p&gt;I view C# as a the Microsoft spin on Java and so would use it if I needed the some of the benefits of Java but was in the Microsoft ecosystem.&lt;/p&gt;
&lt;h3 id=&quot;rule-5-when-to-use-python-or-ruby&quot;&gt;Rule 5: When to use Python or Ruby&lt;/h3&gt;
&lt;p&gt;Python and Ruby seem to me to be pretty similar in that they’re scripting languages and dynamically typed and seemed like the greatest thing in the 00’s. You use them when speed is more important than legibility or debugging. And then Python for ML/AI applications.&lt;/p&gt;
&lt;p&gt;When using Python, you should do type hinting to make life a little saner.&lt;/p&gt;
&lt;h3 id=&quot;rule-6-when-to-use-a-hardcore-language&quot;&gt;Rule 6: When to use a hardcore language&lt;/h3&gt;
&lt;p&gt;I think of Go, Rust, Haskell, Erlang, Clojure, Kotlin, Scala as more hardcore languages. Out of these I’ve only ever shipped production code in Kotlin.&lt;/p&gt;
&lt;p&gt;But thinking hypothetically, I would maybe use Go/Rust if I was building a fresh web service where latency and performance were more important than community/library support. Haskell or Erlang maybe I would use if I were doing something that required a very elegant or mathematical functional approach without a lot of business logic. I don’t know when I would use Clojure – I thnk it’d be if I really came to know and love Lisp. I know that Clojure is a functional paradigm that compiles to JVM bytecode, but I’d use Kotlin for that. Since Kotlin works with Java and also has nice IntelliJ support. Same deal with Scala – I would take Kotlin first.&lt;/p&gt;
&lt;h3 id=&quot;rule-7-how-to-do-javascript&quot;&gt;Rule 7: How to do Javascript&lt;/h3&gt;
&lt;p&gt;The worst code I ever wrote was in Javascript. I was working on a mobile shopping experince for Amazon and nobody in my group know much about JS frameworks. So we went with vanilla JQuery. At first it was pretty simple, but then we got a lot of new frontend requirements and in no time, managing the front-end state was a total nightmare – think, components are disappearing and reappearing and we don’t know why, so we’re logging out every variable to the console.&lt;/p&gt;
&lt;p&gt;So I’m one of the many who’s had a bad experience. But I think it was mostly of our own doing. Now, a couple of years later, my guidelines for JS are:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Use Typescript instead. It’ll make life saner.&lt;/li&gt;
&lt;li&gt;Try to push as much logic as possible to the server. If the front end weren’t super complex, I would consider something like &lt;a href=&quot;https://www.phoenixframework.org/&quot;&gt;Phoenix&lt;/a&gt; instead which actually pushes everything to the server.&lt;/li&gt;
&lt;li&gt;Use a framework like Vue or React if you need front interactivity.&lt;/li&gt;
&lt;li&gt;Don’t skip unit tests.&lt;/li&gt;
&lt;/ol&gt;&lt;h3 id=&quot;rule-8-when-to-use-c-or-c&quot;&gt;Rule 8: When to use C or C++&lt;/h3&gt;
&lt;p&gt;I feel like you don’t choose C – it chooses you. There are classes of applications – operating systems, language design, low-level programming and hardware, where you have to use C.&lt;/p&gt;
&lt;p&gt;C++ is an interesting one. It seems to be useful for applications like robotics and video games and high frequency trading where the performance gains from no garbage collection make it preferable to Java.&lt;/p&gt;
&lt;h3 id=&quot;rule-9-use-php-or-hack-if-you-want-to-test-server-changes-without-rebuilding&quot;&gt;Rule 9: Use PHP or Hack if you want to test server changes without rebuilding&lt;/h3&gt;
&lt;p&gt;PHP is banned at Amazon due to security reasons, but its successor, Hack, runs a lot of Facebook and Slack’s backends, at least of 2020.&lt;/p&gt;
&lt;p&gt;I’ve never considered when I would use PHP or Hack until writing this. Maybe because it’s so taboo at Amazon. I know that Slack and Wikipedia use it. The Slack people claim that it’s got a really developer-friendly programming environment, where for example, you don’t need to restart a local server to view your changes, and that it’s got web-native support for concurrency.&lt;/p&gt;
&lt;h2 id=&quot;technologies&quot;&gt;Technologies&lt;/h2&gt;
&lt;h3 id=&quot;rule-10-when-to-make-a-serverless-function&quot;&gt;Rule 10: When to make a serverless function&lt;/h3&gt;
&lt;p&gt;I would use a serverless function when I have a relatively small and simple chunk of code that needs to run every once in a while.&lt;/p&gt;
&lt;h3 id=&quot;rule-11-which-database-technology-to-choose&quot;&gt;Rule 11: Which database technology to choose&lt;/h3&gt;
&lt;p&gt;Choose SQL when you need to do ad hoc queries and/or you need support for ACID and transactions. Otherwise choose no-SQL. Though noSQL is &lt;a href=&quot;https://aws.amazon.com/blogs/aws/new-amazon-dynamodb-transactions/&quot;&gt;getting better with transactions&lt;/a&gt; and PostgreSQL (I’m familiar with the AWS Aurora flavor) is &lt;a href=&quot;https://aws.amazon.com/rds/aurora/postgresql-features/#High_Availability_and_Durability&quot;&gt;getting better with availability, scalability, and durability&lt;/a&gt;, which have traditionally been the strengths of noSQL.&lt;/p&gt;
&lt;h2 id=&quot;testing&quot;&gt;Testing&lt;/h2&gt;
&lt;h3 id=&quot;rule-12-when-to-write-a-unit-test&quot;&gt;Rule 12: When to write a unit test&lt;/h3&gt;
&lt;p&gt;I try to write a unit test any time the expected value of a defect is non-trivial. Expected value would be: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Likelihood of a defect * Cost of a defect&lt;/code&gt;. This is a sort of a copout since I can never really compute these values precisely. But at least with the code I write and the requirements I’m usually given, there’s almost always a very real chance of a costly defect.&lt;/p&gt;
&lt;p&gt;But every chunk of code should have a trivial unit test around it – this verifies that the code is written in a testable way, which indeed is extremely important.&lt;/p&gt;
&lt;p&gt;But I’m not the guy who thinks that every line and branch must be covered. That’s a policy to apply if you can’t trust yourself (or others) to do a good approximation of the expected value equation above.&lt;/p&gt;
&lt;h3 id=&quot;rule-13-when-to-write-an-integration-test&quot;&gt;Rule 13: When to write an integration test&lt;/h3&gt;
&lt;p&gt;I’m defining an integration test as a test where you’re calling code that you don’t own, rather than mock it out.&lt;/p&gt;
&lt;p&gt;I try to write an integration test whenever I can’t trust the code I don’t own – especially, if it could change without me knowing.&lt;/p&gt;
&lt;h3 id=&quot;rule-14-when-to-write-an-end-to-end-test&quot;&gt;Rule 14: When to write an end-to-end test&lt;/h3&gt;
&lt;p&gt;I’m defining an end-to-end test as a test that simulates a complete “user session” with my product. The user could be a human or another computer that’s trying to accomplish something through multiple iteractions with my code. These are usually supersets of integration tests as defined in Rule 12.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;When I don’t fully understand how the product works, and can’t fully unit test a change, so I want a way to feel a little better that I didn’t break everything. Mostly like a “smoke test.”&lt;/li&gt;
&lt;li&gt;When I need regression test cases to verify functionality in the case of a future refactor.&lt;/li&gt;
&lt;li&gt;When the results are hard to know ahead of time, for example, doing a complex computation, and I want to test the effects of some code on them.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Case 1 should be avoided, but 2 and 3 you can’t really get around.&lt;/p&gt;
&lt;h2 id=&quot;devops&quot;&gt;DevOps&lt;/h2&gt;
&lt;h3 id=&quot;rule-15-when-to-bring-on-a-dedicated-support-engineer&quot;&gt;Rule 15: When to bring on a dedicated support engineer&lt;/h3&gt;
&lt;p&gt;At Amazon, by default, you support your code, so if something in your system goes seriously wrong, one of your peers is going to get paged and is expected to work around the clock until it’s resolved.&lt;/p&gt;
&lt;p&gt;It can be brutal. But there are pockets at Amazon (for example, my current team) and definitely at other companies, where they bring on a Site Reliability Engineer (SRE) whose job it is to be online outside of normal business hours to troubleshoot serious production problems.&lt;/p&gt;
&lt;p&gt;After many years of dealing with this (and pushing for this on my team), I’ve come to believe that as a programmer, you should &lt;strong&gt;always&lt;/strong&gt; advocate for having a dedicated SRE if there’s any real risk of after-hours pages that are out of your control. A prime example is if you’ve inherited somebody else’s code and could be paged for existing defects.&lt;/p&gt;
&lt;p&gt;It tends not to be hard to find people in different time zones who can be trained to support your system. It’s all about finding the money to pay them. Engineers on a team together need to work together to make this message clear to management.&lt;/p&gt;
&lt;h2 id=&quot;security&quot;&gt;Security&lt;/h2&gt;
&lt;h3 id=&quot;rule-16-if-you-delegate-all-your-it-security-to-the-infosec-they-will-come-up-with-draconian-rules&quot;&gt;Rule 16: If you delegate all your IT security to the InfoSec, they will come up with draconian rules&lt;/h3&gt;
&lt;p&gt;InfoSec is a tough job. And the game theory just works out in a way that they’re inevitably going to err on the side of caution. So I think the rule is that if you can do at least some of your own security, you’re going to avoid the wrath of draconian restrictions.&lt;/p&gt;
&lt;p&gt;One great example of this is the debacle of the Amazon internal wiki migration. Back in 2015, the Amazon InfoSec team banned PHP at the company. Our internal wiki used MediaWiki, which in turn was written in PHP. Rather than question this decision – Facebook, WordPress, and Slack were all using PHP, for example, and Facebook building the cleaned-up Hacklang to replace it – the internal wiki team just took this InfoSec mandate as an order. To replace out MediaWiki with a Java-based alternative (XWiki) ended up taking a total of 24 dev-years over 4+ calendar years for the team, not counting the interruptions to pretty much every other team at Amazon as their pages were getting constantly migrated and un-migrated. Had they pushed back on the PHP ban or done some more of their own research on Hack, they might have avoided this catastrophe.&lt;/p&gt;
&lt;h2 id=&quot;designing-and-whiteboarding&quot;&gt;Designing and Whiteboarding&lt;/h2&gt;
&lt;h3 id=&quot;rule-17-make-the-design-session-about-input-not-approval&quot;&gt;Rule 17: Make the design session about input, not approval&lt;/h3&gt;
&lt;p&gt;There are plenty of meetings where the purpose is to “get signoff” or to “get buyin.” You should avoid those and only agree to do one if you’re forced to. You don’t want to need “signoff.” You want to be able to make your own decisions and determine what degree of agreement you need from other people.&lt;/p&gt;
&lt;p&gt;Instead, design meetings or conversations should be about getting input and answering questions.&lt;/p&gt;
&lt;h2 id=&quot;project-management&quot;&gt;Project management&lt;/h2&gt;
&lt;h3 id=&quot;rule-18-estimates-serve-more-for-creating-pressure-than-for-project-planning&quot;&gt;Rule 18: Estimates serve more for creating pressure than for project planning&lt;/h3&gt;
&lt;p&gt;I have been part of a lot of estimation sessions, both as a developer and as a product manager trying to ship products.&lt;/p&gt;
&lt;p&gt;What people will say is that estimates are for planning – that their purpose is to figure out how long some piece of work is going to take, so that everybody can plan accordingly.&lt;/p&gt;
&lt;p&gt;In all my five years shipping stuff, I can only recall one project where things really worked that way. It was a very simple Android app – no external dependencies, no technical complexity, just a local mySQL database and some views on it. And in that specific case, estimates were really accurate because there were very few unknowns. And we could accurately predict when things would be ready, within a day or two, and that was helpful because we had an extremely hard deadline.&lt;/p&gt;
&lt;p&gt;That was the only project I’ve been a part of where we could accurately quantify our velocity and project it foward towards milestones.&lt;/p&gt;
&lt;p&gt;In all the other projects, the main purpose of estimates was to apply pressure, to force the “but you said it would only take 5 days” kind of conversation, which in turn pressures people to work faster or harder or longer so they can avoid those conversations in the future.&lt;/p&gt;
&lt;p&gt;I’m not against pressure per se. But I think it’s important to recognize that that’s the primary purpose of doing estimates.&lt;/p&gt;
&lt;h3 id=&quot;rule-19-be-explicit-about-the-difference-between-hard-deadlines-soft-deadlines-internal-deadlines-and-expected-completion-dates&quot;&gt;Rule 19: Be explicit about the difference between hard deadlines, soft deadlines, internal deadlines, and expected completion dates&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Hard deadline:&lt;/strong&gt; Something seriously bad to the business will happen if the deadline isn’t met.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Soft deadline:&lt;/strong&gt; Somebody will look bad if the deadline isn’t met.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Internal deadline:&lt;/strong&gt; This is a target internal to the team that will not affect anybody outside of the team.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expected completion date:&lt;/strong&gt; This is when the team currently predicts that work will be completed.&lt;/p&gt;
&lt;p&gt;I’ve seen a lot of pain caused by getting these conflated. Any time you’re given a deadline, ask which one of these it is. Of course, it’s often the case that internal deadlines are required to be “on track” for hard deadlines. But what you want to call out is when internal deadlines are presented as hard deadlines for the sake of pressuring people to work longer.&lt;/p&gt;
&lt;p&gt;One other trick to keep in mind: it can often happen than an engineer offers an expected completion date but then somebody else casts it into a hard deadline. That’s a no-no.&lt;/p&gt;
&lt;h3 id=&quot;rule-20-when-somebody-says-agile-push-for-kanban-not-scrum&quot;&gt;Rule 20: When somebody says Agile, push for Kanban, not Scrum&lt;/h3&gt;
&lt;p&gt;When somebody says that “we do Agile,” to me, that means that there is a team meeting every two weeks. That’s all that Agile means to me.&lt;/p&gt;
&lt;p&gt;Then some people will get into the difference between two “flavors” of Agile, which are Scrum and Kanban. In my mind, Scrum means that “you have to get certain things done with those two weeks.” Kanban means “do what you can do in two weeks.” If it’s not explicit to you whether the team follows Scrum or Kanban, you should have that conversation explicitly – and you should push for Kanban. Given the difficulty of estimation (see Rule 18), Scrum can easily mean that you’ll get pressured to work extra hours to complete something within that arbitary two-week horizon.&lt;/p&gt;
</description>
<pubDate>Thu, 30 Apr 2020 10:51:59 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<og:title>Rules of thumb for a 1x developer</og:title>
<og:description>Contents Contents</og:description>
<og:url>https://muldoon.cloud/programming/2020/04/17/programming-rules-thumb.html</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://muldoon.cloud/programming/2020/04/17/programming-rules-thumb.html</dc:identifier>
</item>
</channel>
</rss>