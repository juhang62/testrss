<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Strategies, Tips, and Tricks for Anki</title>
<link>https://senrigan.io/blog/everything-i-know-strategies-tips-and-tricks-for-spaced-repetition-anki</link>
<guid isPermaLink="true" >https://senrigan.io/blog/everything-i-know-strategies-tips-and-tricks-for-spaced-repetition-anki</guid>
<description>&lt;p&gt;For the past few years, Anki has acted as my digital storage (albeit, imperfect). If I look something up in StackOverflow, I'll add it to Anki. Wikipedia? Another Anki card. It's probably the one thing I most attribute to helping my software engineering career. My total deck recently passed ten thousand cards. In this guide, I'll explain the avoidable (dumb) mistakes, but also provide the effective strategies I found along the way. This Anki guide assumes the reader is familiar with these topics:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;spaced repetition&lt;/li&gt;
&lt;li&gt;Anki&lt;/li&gt;
&lt;li&gt;how to add cards to Anki&lt;/li&gt;
&lt;li&gt;Anki decks&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For an introduction to Spaced Repetition and Anki, please read this earlier &lt;a href=&quot;https://senrigan.io/blog/chasing-10x-leveraging-a-poor-memory-in-software-engineering&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;article. How Anki Saved My Programming Career&lt;/a&gt;. While I focus on Anki a bit, principles from this guide also apply to SuperMemo.&lt;/p&gt;
&lt;p&gt;Last Updated: &lt;strong&gt;5/04/2019&lt;/strong&gt;. I will continuously update this from feedback and suggestions. If any bullet points need elaboration, please let me know. A lot was shortened for readability.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Beginner's Guide&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For Anki beginners, habit &amp;amp; friction is your enemy. Maintaining a daily routine is critical. Read the Beginner's Section below and visit after:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;you've maintained Anki for 21 days&lt;/li&gt;
&lt;li&gt;have more than a hundred cards&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I'm not saying beginners to stop reading this guide ... just the rest can be a little overwhelming.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adding Cards&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Most Anki beginners quit. You'll only keep a Anki routine if you quickly see benefits. Focus on adding work or study related cards for an immediate benefit.&lt;/li&gt;
&lt;li&gt;Use a computer to add cards to Anki. It is hard to add cards on mobile.&lt;/li&gt;
&lt;li&gt;In the beginning, try not to add more than twenty cards a day. It's very easy to desire to &quot;remember everything&quot; and find yourself overwhelmed.&lt;/li&gt;
&lt;li&gt;Add simple cards.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Reviewing Cards&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Download (and review using) the mobile apps. This is probably the most important tip.
&lt;ul&gt;&lt;li&gt;The Android (free) version is amazing! There is also a iOS version ($25).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Keep a daily habit to review all your cards. In the beginning, reviewing take two to four minutes. It's easy to skip and then catch up, but having a routine will be critical later.&lt;/li&gt;
&lt;li&gt;Aim to make reviewing Anki as easy as possible.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Card Types&lt;/h2&gt;
&lt;p&gt;Anki comes with three default card types:&lt;/p&gt;
&lt;h2&gt;Categories and Recommended Card Types&lt;/h2&gt;
&lt;table border=&quot;1&quot; height=&quot;269&quot;&gt;&lt;tbody readability=&quot;16&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Category&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Recommend Card Type&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Why&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;General&lt;/td&gt;
&lt;td&gt;Basic + Reversed&lt;/td&gt;
&lt;td&gt;Avoid tip-of-the-tongue issues.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;Screenshots from Video Tutorials (YouTube, etc)&lt;/td&gt;
&lt;td&gt;Basic&lt;/td&gt;
&lt;td&gt;It's hard to format screenshots to fit front-and-back or cloze formats.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;Code Block (Snippet)&lt;/td&gt;
&lt;td&gt;Basic, Use Anki Code Plugin&lt;/td&gt;
&lt;td&gt;Most code snippets are hard to format.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5&quot;&gt;&lt;td&gt;Code Block (StackOverflow Answer)&lt;/td&gt;
&lt;td&gt;Basic, Cloze&lt;/td&gt;
&lt;td&gt;Most code snippets are hard to format to fit basic-and-reversed. I frequently will just take a snapshot of the answer and paste unformatted.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;Definitions&lt;/td&gt;
&lt;td&gt;Basic&lt;/td&gt;
&lt;td&gt;Basic-and-reversed can have multiple answers. Easier to make a basic card. Definitions in cloze can let the answer slip too easily.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td&gt;Wikipedia Paragraph&lt;/td&gt;
&lt;td&gt;Cloze&lt;/td&gt;
&lt;td&gt;Wikipedia articles are a great way to provide context while quizzing you on key topics. Improves fundamentals.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;Labelled Images (Human Body, Anatomy, Maps, etc)&lt;/td&gt;
&lt;td&gt;Image Occlusion Plugin&lt;/td&gt;
&lt;td&gt;Your brain was designed for visual memory.&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;List&lt;/td&gt;
&lt;td&gt;Cloze&lt;/td&gt;
&lt;td&gt;Lists are difficult to memorize on basic. Cloze cards separates a list into multiple cards.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;ul&gt;&lt;li&gt;Avoid adding Basic type cards. If you can make a basic card, you can probably make it a Basic + Reversed (BR) Card. BR Cards help avoid the &lt;a href=&quot;https://en.wikipedia.org/wiki/Tip_of_the_tongue&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer nofollow&quot; data-token-index=&quot;1&quot; data-reactroot=&quot;&quot;&gt;tip of the tongue phenomenon&lt;/a&gt;.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/2chYgJqOSXaEbkjegN0U&quot; alt=&quot;undefined&quot; width=&quot;729&quot; height=&quot;182&quot;/&gt;&lt;/li&gt;
&lt;li&gt;For definitions (example: provenance), the basic+reversed type isn't helpful. I want to know a word's meaning, but trying to recall something that means &quot;origin&quot; has too many answers (immemorial, provenance, inception, etc). &lt;img src=&quot;https://cdn.buttercms.com/6PYo8PKRdqFHXDuhLumQ&quot; alt=&quot;undefined&quot; width=&quot;727&quot; height=&quot;242&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Cloze cards are great for adding context to understand a topic. Most basic+reversed cards can be made into cloze cards. Over time my preferences have gone from Basic —&amp;gt; Basic + Reversed —&amp;gt; Cloze.&lt;img src=&quot;https://cdn.buttercms.com/4MMKadx8T9eYixCmrWQV&quot; alt=&quot;undefined&quot; width=&quot;1237&quot; height=&quot;139&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Adding Cards&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Each question should have a single succinct answer.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/Uc0JeNSnT5ykvrhAC4FK&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Add images. Our brains are wired visually, so this helps retention. &lt;strong&gt;THIS IS REALLY IMPORTANT. IF POSSIBLE, USE A VISUAL CUE.&lt;br/&gt;&lt;/strong&gt;&lt;strong&gt;&lt;img src=&quot;https://cdn.buttercms.com/MG5zRFudQK6l9DWAyUEz&quot; alt=&quot;undefined&quot; width=&quot;837&quot; height=&quot;521&quot;/&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Don't add things you don't understand. Otherwise, expect frustration. The same card will be wrong multiple days, marked as a leech and a waste of time. [See Antipatterns, Useful Links]&lt;/li&gt;
&lt;li&gt;Most of your Anki cards should be as simple as possible.&lt;/li&gt;
&lt;li&gt;Add additional context for hard questions. If you get a card wrong, it's helpful to know more about the topic. I prefer adding in the extra info with cloze cards. For basic or basic + reversed cards, I'll have a succinct answer on Line 1, line breaks, Lines 3+ will be additional information.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/lsE5yRuET0GOftafLx2m&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;li&gt;If you have a card with multiple answers, add hints (HINT: Begins with a P...). When adding, you won't realize a question has multiple answers, but when you're reviewing, you'll know. Try to avoid this and write better questions with single answers. &lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/Gk8XaK75QR2yWVgwi77C&quot; alt=&quot;undefined&quot; width=&quot;1134&quot; height=&quot;155&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Don't Add Cards Memorizing Entire Lists. Too easily forgotten. You might remember Points 1, 2, 4 today and tomorrow only remember Points 1, 2, 3.
&lt;ul&gt;&lt;li&gt;If necessary, use a cloze to memorize lists.&lt;/li&gt;
&lt;li&gt;Lists can be memorized using the loci method. [See: Learning Strategies]&lt;img src=&quot;https://cdn.buttercms.com/15dguZUQTRFMufUHgj8w&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I added the above into a single &lt;span data-token-index=&quot;1&quot; data-reactroot=&quot;&quot;&gt;basic&lt;/span&gt; card — &lt;em&gt;Q: &quot;What are the dates and milestones of Deep Learning Ideas?&quot; A:&lt;/em&gt; &lt;span data-token-index=&quot;6&quot; data-reactroot=&quot;&quot;&gt;&lt;em&gt;Long forgettable list&lt;/em&gt;.&lt;/span&gt; A cloze is a better approach. I can get one or two facts wrong without having to redo the card.&lt;img src=&quot;https://cdn.buttercms.com/Sk0br5OkQDiKRPi3dIeO&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Avoid importing online Anki decks. Similar to note taking, your own materials have better retention and comprehension.&lt;/li&gt;
&lt;li&gt;When studying offline material, don't simultaneously create cards. Instead, highlight/circle the topics, write the Anki question you would have asked and finish studying. After the study session (or tomorrow) review your circled questions and add it to Anki. This is referred as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cornell_Notes&quot;&gt;Cornell Notes system&lt;/a&gt;. [See: Studying and Learning]
&lt;ul&gt;&lt;li&gt;Adding cards when offline studying is risky, it's too easy to end in a Wikipedia rabbit hole.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If you've added cards, sync Anki (keyboard shortcut: Y) before leaving your house or office. Review during commute. &lt;strong&gt;Make this a habit.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;When watching online videos/lectures, I frequently create Basic cards and use screenshots as answers.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/dWoyxgCS5iDuweAlELbH&quot; alt=&quot;undefined&quot; width=&quot;857&quot; height=&quot;583&quot;/&gt;&lt;/li&gt;
&lt;li&gt;For sites that have a detailed explanation (StackOverflow answers), I also use a screenshot instead of dealing with formatting. I'm lazy.&lt;/li&gt;
&lt;li&gt;A tradeoff of screenshots is they can't be easily searched.&lt;/li&gt;
&lt;li&gt;If you forget an Anki card (that you've already added and should know) add a similar question worded slightly differently. Spaced repetition isn't perfect, so asking the same question in a different way helps drill an answer.
&lt;ul&gt;&lt;li&gt;Another alternative is to add an Anki card asking &quot;what term did you forget about when you were talking to [person]?&quot;&lt;img src=&quot;https://cdn.buttercms.com/4s5Pciq5TqmPdxaNEs07&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;I don't care about grammar, formatting or spelling (apologies about some card examples ...). You shouldn't either. Only you review your cards.&lt;/li&gt;
&lt;li&gt;Include where you learned it. Helps links back visually. Human brains are weird. Personalizing a card helps recall.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/YrdARDQ5SYWXVh5obE3r&quot; alt=&quot;undefined&quot; width=&quot;1038&quot; height=&quot;508&quot;/&gt;&lt;/li&gt;
&lt;li&gt;If you're going to paste a paragraph that uses a cloze term multiple times, use a find and replace (I like using &lt;a href=&quot;https://www.sublimetext.com/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot; data-token-index=&quot;1&quot; data-reactroot=&quot;&quot;&gt;Sublime&lt;/a&gt; to do this).&lt;/li&gt;
&lt;li&gt;Add names to faces. Thoughtful teachers add an entire class of student faces to names prior to the first day.&lt;/li&gt;
&lt;li&gt;I prefer cards in random order.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Add-Ons / Plugins&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;BACKUP YOUR ANKI DECK BEFORE INSTALLING ADD-ONS.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;The complete Anki add-ons list is &lt;a href=&quot;https://ankiweb.net/shared/addons/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;. Normally sorting by ratings is a reasonable heuristic (sorting is currently broken). Avoid add-ons that are less than a year old; there are stories about add-ons accidentally deleting cards.&lt;/li&gt;
&lt;li&gt;Not all add-ons play nicely with each other.&lt;/li&gt;
&lt;li&gt;Uninstall an add-on if you don't intend to use it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seriously, back up your deck first&lt;/strong&gt;. I normally review the source code before installing any add-ons. Many have dubious code quality.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ankiweb.net/shared/info/1496166067&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Night Mode&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Saves your eyes, seriously.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ankiweb.net/shared/info/1463041493&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Syntax Highlighter for Code&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Makes it really really easy to highlight syntax code. I use an earlier version.&lt;/li&gt;
&lt;li&gt;This makes it much easier to add cards related to Coding Snippets&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ankiweb.net/shared/info/162313389&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Power Format Pack&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;I like Syntax Highlighter for ease-of-use, but this another common way to format your Anki cards.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ankiweb.net/shared/info/1111933094&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Image Occlusion&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;If you're a medical student, this is for you.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Desktop Keyboard Shortcuts&lt;/strong&gt;&lt;/h2&gt;
&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Memorize shortcuts to add Anki cards faster. Meta-Anki: Use Anki to memorize Anki shortcuts.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Sorry, I don't have Windows examples, but similar strategies apply.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Shortcuts Example : How To Quickly Add An Anki Card on Mac&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Alt + Tab to Anki&lt;/li&gt;
&lt;li&gt;Hit A to enable Add A Card&lt;/li&gt;
&lt;li&gt;Type the card front&lt;/li&gt;
&lt;li&gt;Tab&lt;/li&gt;
&lt;li&gt;Type the card back&lt;/li&gt;
&lt;li&gt;Shift + Enter to enter the card&lt;/li&gt;
&lt;li&gt;Esc to main screen&lt;/li&gt;
&lt;li&gt;Alt + Tab to go back to what I was working on.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.buttercms.com/QQXD8F0IRVmrjYfcE8Gg&quot; alt=&quot;undefined&quot; width=&quot;709&quot; height=&quot;360&quot;/&gt;&lt;/p&gt;
&lt;p&gt;You want to make adding cards seamless.&lt;/p&gt;
&lt;p&gt;Anki Shortcuts (Mac):&lt;/p&gt;
&lt;table border=&quot;1&quot; height=&quot;194&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Action&lt;/td&gt;
&lt;td&gt;Shortcut (Mac)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Add A Card Prompt&lt;/td&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Add A Card&lt;/td&gt;
&lt;td&gt;Shift + Enter&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Close&lt;/td&gt;
&lt;td&gt;Esc&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Create A Cloze Card&lt;/td&gt;
&lt;td&gt;Cmd + Shift + C&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Select Card Type&lt;/td&gt;
&lt;td&gt;Cmd + N&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Change Deck Type&lt;/td&gt;
&lt;td&gt;Cmd + D&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;ul&gt;&lt;li&gt;Use keyboard keys. 1 (Again), 2 (Hard), 3 (Good), or 4 (Easy). Easier than using mouse.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/5I3kn2lTZKgwZq2TXalv&quot; alt=&quot;undefined&quot;/&gt;&lt;/li&gt;
&lt;li&gt;Hitting Spacebar defaults to Good.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Mobile Tips and Tricks&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;I prefer reviewing on Mobile. Since I normally review during my commute, mobile is imperative.&lt;/li&gt;
&lt;li&gt;Enable App Gestures.
&lt;ul&gt;&lt;li&gt;Set Swipe Left to &lt;strong&gt;Answer button 1&lt;/strong&gt;. &quot;Again / Fail&quot;.&lt;/li&gt;
&lt;li&gt;Set Swipe Right to &lt;strong&gt;Answer button 2&lt;/strong&gt;. &quot;Hard&quot;.&lt;/li&gt;
&lt;li&gt;Make all the taps to &lt;span data-token-index=&quot;1&quot; data-reactroot=&quot;&quot;&gt;Show answer.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/gAjwE2G9RNDOtcF2v8Gc&quot; alt=&quot;undefined&quot; width=&quot;310&quot; height=&quot;551&quot;/&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;This makes it really useful on the train / bus since you can review Anki with one hand.&lt;/li&gt;
&lt;li readability=&quot;0.49275362318841&quot;&gt;
&lt;div class=&quot;s1qo48hh-0 bThxNZ&quot; readability=&quot;11.826086956522&quot;&gt;Credit to &lt;a href=&quot;https://www.reddit.com/r/medicalschoolanki/comments/biyjtc/everything_i_know_a_comprehensive_guide_of/em51feo?utm_source=share&amp;amp;utm_medium=web2x&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;minh0&lt;/a&gt;: Brought up a valid criticism about Swipe difficulty.  While my default Swipe Right - Correct is &quot;Answer Button 2&quot;, which means Hard. Some users (especially those with large medical Anki cards) may prefer to have it to be &quot;Answer Button 3&quot; which means Good. Otherwise, the amount of cards to review doesn't decrease fast enough.&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Buy a &lt;a href=&quot;https://amzn.to/2DGcWpr&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;phone ring&lt;/a&gt;. You can now swipe left and right on one hand easier.&lt;/li&gt;
&lt;li&gt;Make It A Favorite / Docked App for Easier Access.&lt;/li&gt;
&lt;li&gt;Turn on Autosyncing. The app will now sync when you open or when you've finished a deck (per 30 minutes).&lt;/li&gt;
&lt;li&gt;Turn on Nightmode. Both the Android and iOS app have this feature.&lt;/li&gt;
&lt;li&gt;If you have more than fifteen minutes of Anki, turn on airplane mode first. Your phone/tablet's notifications were made to distract you. Turn back on after reviewing.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;General &lt;strong&gt;Reviewing Tips&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;I prefer using mobile (Phone / Tablet) to review.&lt;/li&gt;
&lt;li&gt;If you have time, think about where you learned the card.&lt;/li&gt;
&lt;li&gt;After you're finished a deck, you have the option to review any forgotten cards in the last X days.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/riqCXd3IQheiyZcrc2Fl&quot; alt=&quot;undefined&quot; width=&quot;525&quot; height=&quot;309&quot;/&gt;&lt;ul&gt;&lt;li&gt;Review forgotten cards in decks that you're willing to overstudy and improve.&lt;/li&gt;
&lt;li&gt;Reviewing forgotten cards is a great way to use mnemonics and visual imagery to remember. Because visual imagery takes a bit longer to think about, I save it for cards that I'm forgetting.&lt;/li&gt;
&lt;li&gt;Reviewing forgotten is useful when you've finished reviewing and want to burn some time.&lt;/li&gt;
&lt;li&gt;Depending on your deck, try not to reviewing forgotten cards too many days back.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If you have a reasonable amount of Anki cards and you're committed to Anki — I recommend buying an android tablet. It makes reviewing easier. I know this is ridiculous. A tablet has a few benefits over your phone:
&lt;ul&gt;&lt;li&gt;Larger reading space. The 8'' is a perfect tablet size to review.&lt;/li&gt;
&lt;li&gt;I prefer the Android App over iOS. I've tried Anki on a Samsung and Apple tablet.&lt;/li&gt;
&lt;li&gt;Tablets have less distractions than your phone. (Less calls, texts and emails).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;During a review, you might find something wrong with a card. If it's not easily fixable, star it. Don't edit it now, finish your review session and batch fix incorrect starred cards later. On a weekend, you can open up Anki via desktop and select &quot;All Starred Cards&quot; and make edits.
&lt;ul&gt;&lt;li&gt;Sometimes you'll forget why you starred a card for an error. I will write a little note next to the card what's missing (cloze, formatting, etc).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If it's the last card in a Deck, and you've gotten it wrong a few times, be careful. You're likely think you remembered it because of the recency. I recommend slowly reviewing or making an extensive mental image of the card.&lt;/li&gt;
&lt;li&gt;Use Spacebar to go to the next card. [See: Shortcuts]&lt;/li&gt;
&lt;li&gt;Use the Night Mode Add-On. Your eyes thank me.&lt;/li&gt;
&lt;li&gt;Use learning strategies. [Learning and Study Methods]&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Finding Time for Anki&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Dead Time is Anki Time
&lt;ul&gt;&lt;li&gt;If you want to go on Reddit, HackerNews, social media, etc. finish your Anki deck first. Or at the very least, try to do five cards. Use Anki as a source to procrastinate.&lt;/li&gt;
&lt;li&gt;My friends are frequently late. I do Anki when waiting. If they're really late, I'll start a review forgotten session.&lt;/li&gt;
&lt;li&gt;Other Times To Do Anki:
&lt;ul&gt;&lt;li&gt;Commute&lt;/li&gt;
&lt;li&gt;Non-productive meetings.&lt;/li&gt;
&lt;li&gt;Waiting in line.&lt;/li&gt;
&lt;li&gt;Elevators.&lt;/li&gt;
&lt;li&gt;The twenty minutes of movie trailers before a movie.&lt;/li&gt;
&lt;li&gt;Etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If you're having trouble with Anki, I recommend printing out a calendar, putting it on your fridge and writing an X everyday if you attempted 10 minutes. Don't make the goal to finish your entire deck, but just that you spent 10 minutes. Having low expectations of a task is a great way to avoid anxiety and just start. Known as &lt;a href=&quot;https://www.writersstore.com/dont-break-the-chain-jerry-seinfeld/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Seinfeld Method / Don't Break the Chain&lt;/a&gt;. Habit Tracker is another app that is great for tracking.&lt;/li&gt;
&lt;li&gt;Locking yourself in a room without WiFi is another sadistic, but effective method.&lt;/li&gt;
&lt;li&gt;I'm frequently pulled into non-productive meetings (classic engineer gripe). I'll just do Anki on my laptop.&lt;/li&gt;
&lt;li&gt;Change your commute style. I found myself preferring a bus over a train that added fifteen minutes to my commute. The bus gave me a seat and uninterrupted time to review. On the other hand, New York trains were too chaotic to get into a good rhythm to review.&lt;/li&gt;
&lt;li&gt;I find Anki + Tablet + Notebook + Good Food at a Restaurant is a really enjoyable way to review. I'll normally write down any answers I get wrong into the notebook, ponder about why I got them wrong and enjoy some really good food. Since I'll normally go to the same restaurant, I always end up being friends with the restaurant stuff. This tactic is referred to as &lt;a href=&quot;https://www.nerdfitness.com/blog/too-lazy-to-work-out-try-temptation-bundling/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;temptation bundling&lt;/a&gt;, combining something really good (food) with something that's work (reviewing). The making friends is optional.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Deck Strategies&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;The community advice is to have all your cards merged in one deck. This has two main benefits:
&lt;ul&gt;&lt;li&gt;It reduces the Anki context problem - this happens when you can only remember the answer from an Anki app, but not in real-world situations. Similar to below. This is also referred to as interleaving&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/khmYKSTTRwaRRe1aUfOE&quot; alt=&quot;undefined&quot; width=&quot;730&quot; height=&quot;99&quot;/&gt;&lt;/li&gt;
&lt;li&gt;It boosts creativity. Having two completely unrelated cards helps think about problems in different ways. Quizzing about tornados and binary-tree algorithms makes me visit the problem in a different way.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;That being said, I have multiple decks. There are some benefits:
&lt;ul&gt;&lt;li&gt;Very useful for new jobs: Have a new job deck and add all your work related cards in there. Review it at a much higher frequency (+ use review forgotten). Your learning rate and the speed of recall will improve.
&lt;ul&gt;&lt;li&gt;I can accelerate learning on a particular topic if I make a custom deck for it. Spaced repetition algorithms maximize efficiency of memory, sometimes I don't care about efficiency and want to brute-force improve at underlying material.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I'm debating trying a new topic (math, music, etc), I will add that material to a new deck. That way if I decide I hate this topic, I can delete/avoid the entire deck.&lt;/li&gt;
&lt;li&gt;Much easier to avoid decks you don't have enough time to do.&lt;/li&gt;
&lt;li&gt;You can listen to different types of music depending on decks. For easy decks I listen to pop music. Harder decks require music with a slower cadence. My &quot;definitions&quot; deck is a relatively easy deck; I listen to fun music there.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If I'm learning one new topic, I will throw everything into the default deck. If I'm learning two different topics, I will have two different decks.&lt;/li&gt;
&lt;li&gt;Apparently, at 20-25 decks it gets a bit messy. Probably merge some at that point.&lt;/li&gt;
&lt;li&gt;A lot of deck decisions are based on your learning and studying habits. It's trivially easy to combine everything into one deck later, the inverse isn't true.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Tags&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;A lot of my deck strategies can be done similarly via tags. Personally, tags weren't really as useful as decks. They required more time to make, and the way to review specific tags required too much customization. I also felt that the mobile apps weren't really good for tags.&lt;/li&gt;
&lt;li&gt;Each person varies. If you find yourself using a tagging system in Evernote, OneNote, etc. then you may want to approach Anki with tags. If you don't (myself personally), then tags probably won't work for you.&lt;/li&gt;
&lt;li&gt;Some useful advice by redditor &lt;a href=&quot;https://www.reddit.com/r/Anki/comments/biocuk/everything_i_know_a_comprehensive_guide_of/em2b3cd?utm_source=share&amp;amp;utm_medium=web2x&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cubicpillow about the benefits of tags vs. decks&lt;/a&gt;:
&lt;ul&gt;&lt;li&gt;&lt;span&gt;I don't review by tags but I use them to keep my cards organized. Tags have the advantage of being able to have multiple tags (i.e. I might have a card with the tags music theory, banjo, banjo exercise).&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;&lt;span&gt;I do like to keep as many cards in one deck as possible, but I'll break out separate decks if I require a specific tool or setting to review them. BJJ cards are best reviewed with a training partner to drill with. Banjo cards require a banjo. Slight of hand cards require props, Spanish cards are better with sound.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Learning &amp;amp; Study Methods&lt;/h2&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;What I cannot create, I do not understand. — Richard Feynman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Most people use Anki as a way to amplify memory. I use Anki for memory AND to fundamentally learn.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Use multimodal study strategies. Using different modes of learning can enforce an idea.
&lt;ul&gt;&lt;li&gt;Read the answer out loud.&lt;/li&gt;
&lt;li&gt;Write it out. For wrong answers, I'll write it on paper. The act of writing is meditative. I really enjoy this.&lt;/li&gt;
&lt;li&gt;Study in a different location than normal (different coffee shops, public locations).&lt;/li&gt;
&lt;li&gt;Visualize the answer (see below).
&lt;ul&gt;&lt;li&gt;Many others. The point is changing your study patterns helps.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Learning: To become world-class, you need &lt;a href=&quot;https://fs.blog/2018/04/first-principles/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;first principles&lt;/a&gt;. That requires expertise in fundamentals.
&lt;ul&gt;&lt;li&gt;This strategy only applies to topics you want long-term mastery. I only use this for a subset. Be picky. It's impossible to apply first principles thinking to your entire deck.&lt;/li&gt;
&lt;li&gt;If I'm trying to learn from &lt;a href=&quot;https://fs.blog/2018/04/first-principles/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;first principles&lt;/a&gt;, I'll write and draw mind maps &amp;amp; diagrams. This takes considerably more time to review, but makes similar concepts easier to grok. Mindmaps are a great way to stabilize topics.&lt;/li&gt;
&lt;li&gt;Keeping on asking yourself why? why does this work? why does it work this way? Force yourself to understand the root of a topic.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cornell_Notes&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Cornell Method&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;When reading a topic, write out questions on the margins to quiz yourself.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pretend You Have to Teach It
&lt;ul&gt;&lt;li&gt;Works incredibly well. Help realize knowledge gaps.&lt;/li&gt;
&lt;li&gt;Pretend you have to explain it to a five year old for extra difficulty. Only allow yourself to use basic vocabulary and concepts. If you can't explain it simply, then you don't really know it.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Use Exquisite Visual Imagery
&lt;ul&gt;&lt;li&gt;If you make very intense mental visual images, you'll remember it for life. he visual image doesn't have to make sense (most of my don't). &lt;em&gt;Q: What's the capital of Romania? A: Bucharest.&lt;/em&gt; I have a disturbing image of Romans being conquerors and butchering an innocent village at rest. butcher-at-rest, butch-at-rest, Bucharest. I know this sounds very silly, but visual imagery really works. Don't worry, you won't fool yourself into thinking Romania are Romans.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Use mnemonics phrases like &lt;a href=&quot;https://study.com/academy/lesson/what-is-pemdas-definition-rule-examples.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PEMDAS&lt;/a&gt; for lists and other hard-to-remember topics.&lt;/li&gt;
&lt;li&gt;If you don't know an answer, try to force yourself to reason what the answer could be.
&lt;ul&gt;&lt;li&gt;Slow and hard, but very useful in first principle thinking.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://artofmemory.com/wiki/How_to_Build_a_Memory_Palace&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Memory Palace&lt;/a&gt; / &lt;a href=&quot;https://en.wikipedia.org/wiki/Method_of_loci&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Loci Method&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Think about the room you grew up. Weird how you can still remember ... Now put an Anki term interacting with an object from the room. &lt;strong&gt;Visualize the term really hard&lt;/strong&gt;. When you wake up tomorrow, you'll still remember the term.&lt;/li&gt;
&lt;li&gt;This is useful for memorizing extensive lists. I will go counter-clockwise in a room and make every item in a list interact with something from my childhood bedroom.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Studying Before Bedtime
&lt;ul&gt;&lt;li&gt;Useful for reviewing forgotten cards. I don't recommend trying to do your daily review session at night though, it's too easy to fall asleep and be backlogged.&lt;/li&gt;
&lt;li&gt;Useful if you suffering from insomnia and want to get your mind on something else.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Pretend the word makes a sound describing something else. Q: 'How do I use Python's DictWriter to output a line?' A: writer.writerow(). I like to make this stupid sound of &quot;ruh row&quot;, like a sad &quot;oh no!&quot;. It works. This example is so embarrassing and ridiculous.&lt;/li&gt;
&lt;li&gt;I use a checklist of first principles reasoning and meta-learning tactics when reviewing Anki. I'll write about it sometime later, but it didn't fit in this guide.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;On Deleting Cards&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Get over your gripe about deleting cards. If you find yourself forgetting a crappy card or just never want to see the card again, delete it.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Delete Cards That Don't Make Sense
&lt;ul&gt;&lt;li&gt;If you can't fix it with a batched star (and edit later), delete.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;If you're having issues with a particular card for multiple days, it may be a poorly written card. Delete and write it in a clear way.&lt;/li&gt;
&lt;li&gt;Delete Bad Programming Languages You Don't Use
&lt;ul&gt;&lt;li&gt;After a few years, I can safely say I won't need PHP.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Delete Things You Don't Want To Remember Anymore&lt;/li&gt;
&lt;li&gt;Avoid the Sunk Cost Fallacy
&lt;ul&gt;&lt;li&gt;Don't feel bad that you studied a card months ago and forgot it. It happens. If you no longer care for the card, delete it.&lt;/li&gt;
&lt;li&gt;If you're still too scared to delete, you can also suspend cards to edit later.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Dealing with Spaced Repetition Burnout&lt;/h2&gt;
&lt;p&gt;Burnout is when you feel overwhelmed and don't want to review cards. It frequently happens when you've added too many cards quickly. You've missed a few review sessions and now have a couple of hundred cards to review. Or you have too many cards you don't understand.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Find a way to just start: Leave your location and commit to one &lt;a href=&quot;https://en.wikipedia.org/wiki/Pomodoro_Technique&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Pomodoro&lt;/a&gt; (25 minutes) of Anki at your favorite coffee shop. Turn on airplane mode. Jam to your favorite music.&lt;/li&gt;
&lt;li&gt;If you're feeling overwhelmed, go out for a ten minute walk. Then when you come back, if you still don't want to do your daily Anki, don't. The mental agreement helps me start.&lt;/li&gt;
&lt;li&gt;Be comfortable with deleting cards that cause you frustration. [See: Deleting Cards]&lt;/li&gt;
&lt;li&gt;If possible, only review a deck you're passionate about until you can get back into a daily rhythm.&lt;/li&gt;
&lt;li&gt;To avoid burnout:
&lt;ul&gt;&lt;li&gt;Avoid adding too many cards too quickly.&lt;/li&gt;
&lt;li&gt;Don't fall too far behind on Anki reviews.&lt;/li&gt;
&lt;li&gt;Avoid adding too many cards that you don't fully understand. Too many difficult cards causes frustration.&lt;/li&gt;
&lt;li&gt;Be comfortable deleting / suspending frustrating cards.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Don't&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Don't try to trick yourself w/quirky questions. You may think you can ask a question in a misleading way to trick yourself will make you remember the topic better, but it just trips me in the long-run.
&lt;ul&gt;&lt;li&gt;Normal Example — &lt;em&gt;Q: Programming: How do I &lt;strong&gt;commit&lt;/strong&gt; and add all messages and files? A: git commit -am&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Tricky Example — &lt;em&gt;Q: Programming: How do I &lt;strong&gt;save&lt;/strong&gt; and add all messages and files? A: git commit -am&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The problem now is your brain remembers the word &quot;&lt;strong&gt;save&lt;/strong&gt;&quot; and the next time you think of &lt;strong&gt;commit&lt;/strong&gt;, your brain goes &quot;wait, did you use &lt;strong&gt;save&lt;/strong&gt; as trick term or was it &lt;strong&gt;commit&lt;/strong&gt;? ARGH&quot;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Don't keep forgetful cards even if you've spent time trying to remember (sunk cost fallacy). You'll find certain cards you remember for a few months and then inevitably forget. Some cards aren't worth it. Delete.&lt;/li&gt;
&lt;li&gt;Don't turn on the Auto Next Card feature. Way too easy to mess up.&lt;/li&gt;
&lt;li&gt;Don't explain Anki to a newcomer by showing the first card of your deck. Luck says it will always be of something embarrassing you read from an article.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;strong&gt;Music&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Different types of music will subtly change your review speed.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;EDM/POP Music? I KNOW EVERYTHING TODAY. I then review cards a little too quickly ... the euphoria makes me overconfident.&lt;/li&gt;
&lt;li&gt;Classical Music? Slows me down, makes me more thoughtful. Not as fun tho.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Community Tips from Reddit and Other Social Media&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;I use Anki to memorize vocabulary as well. So what I do instead of asking What word means {{Definition}}, I use colons to introduce a definition like : {{Definition}}. That way I could add two or three meanings, which also helps to differentiate words that have similar definitions. Credit: &lt;a href=&quot;https://www.reddit.com/user/plain_tshirt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;plain_tshirt&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Antipatterns / Applicable Only To Odd People Like Myself&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;I frequently add entire Wikipedia descriptions as cloze cards (about a topic I don't understand yet). Because I also use Anki as my learning pad, I know I'll sit down and study this knowledge. THIS IS NOT A GOOD IDEA UNTIL YOU KNOW WHAT YOU'RE DOING.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/syrlcPZgRlegnY0IEj51&quot; alt=&quot;undefined&quot; width=&quot;815&quot; height=&quot;198&quot;/&gt;&lt;/li&gt;
&lt;li&gt;At some point, it's trivially easy and intoxicating to add facts from articles. Previously, if the Wall Street Journal mentioned a person's background and alma matter, I added it. This was useless. Add knowledge if helpful; if not, it's probably not worth knowing.&lt;/li&gt;
&lt;li&gt;If you're having trouble with remembering cards (my brain ignores certain topics), delete them. Spaced repetition isn't about spaced torture.&lt;/li&gt;
&lt;li&gt;Useless Things I Don't Add Anymore
&lt;ul&gt;&lt;li&gt;Birthdates&lt;/li&gt;
&lt;li&gt;Phone Numbers&lt;/li&gt;
&lt;li&gt;Raw Numbers. Like 2016 United States GDP&lt;/li&gt;
&lt;li&gt;Politician Names / Faces&lt;/li&gt;
&lt;li&gt;Pop Culture&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;span data-token-index=&quot;0&quot; data-reactroot=&quot;&quot;&gt;Misc:&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;span data-token-index=&quot;0&quot; data-reactroot=&quot;&quot;&gt;Explaining Anki — From time to time, people will ask random sorcery is Anki. It's hard to explain verbally spaced repetition, but drawing out the memory curve is effective for the person to get the &quot;aha&quot; moment.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/7m31dpYBQyu3z6ueXhFG&quot; alt=&quot;undefined&quot;/&gt;&lt;br/&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span data-token-index=&quot;0&quot; data-reactroot=&quot;&quot;&gt;Sync Conflicts. If you get a sync conflict like the below ... pick the source that has the most cards. You might have to repeat a daily review, but it's better than losing cards you created.&lt;br/&gt;&lt;img src=&quot;https://cdn.buttercms.com/6qzXj0qcTLW5gKctTM8Q&quot; alt=&quot;undefined&quot;/&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span data-token-index=&quot;0&quot; data-reactroot=&quot;&quot;&gt;Sometimes I'm a lazy buffoon. I tried adding motivational techniques in Anki, but that doesn't help. Q: How should you get out of bed when you don't feel like it? A: Use a five second rule. Five, four, three AND THEN GET OUT OF BED. Result? I remembered the five second rule and continued snoozing.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Strategies I've Been Meaning To Try (But Haven't)&lt;/h2&gt;
&lt;h2&gt;Overall&lt;/h2&gt;
&lt;p&gt;Most of these aren't hard truths. Your brain is different (and probably smarter) than mine. You'll devise your own strategies based on how you study and learn. Visual imagery though - that tip is GOLD.&lt;/p&gt;
&lt;h2&gt;Future Articles Per Request&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Meta-Learning — First Principles and Foundations Checklist&lt;/li&gt;
&lt;li&gt;I'll be releasing a program that correlates Sleep, Nutrition, and Supplements on Anki and memory in the two months. I'm a quantified-self fan.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Other Useful Links&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.supermemo.com/en/archives1990-2015/articles/20rules&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;20 Rules About Memory - SuperMemo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/learn/learning-how-to-learn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Learning How to Learn - Coursera, Dr. Barbara Oakley&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://alexvermeer.com/anki-essentials/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Anki Essentials - Alex Vermeer&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 05 May 2019 06:44:53 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://senrigan.io/blog/everything-i-know-strategies-tips-and-tricks-for-spaced-repetition-anki/</dc:identifier>
</item>
<item>
<title>OpenAI releases larger GPT-2 model</title>
<link>https://openai.com/blog/better-language-models/#update</link>
<guid isPermaLink="true" >https://openai.com/blog/better-language-models/#update</guid>
<description>&lt;div class=&quot;js-excerpt&quot; readability=&quot;15&quot;&gt;
&lt;p&gt;We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.&lt;/p&gt;
&lt;/div&gt;&lt;section class=&quot;btns&quot;&gt;&lt;a href=&quot;https://github.com/openai/gpt-2&quot; class=&quot;btn btn-padded icon-code&quot;&gt;View code&lt;/a&gt;&lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot; class=&quot;btn btn-padded icon-paper&quot;&gt;Read paper&lt;/a&gt;&lt;/section&gt;&lt;p&gt;Our model, called GPT-2 (a successor to &lt;a href=&quot;https://blog.openai.com/language-unsupervised/&quot;&gt;GPT&lt;/a&gt;), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much &lt;a href=&quot;https://github.com/openai/gpt-2&quot;&gt;smaller model&lt;/a&gt; for researchers to experiment with, as well as a &lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;technical paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;GPT-2 is a large &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;transformer&lt;/a&gt;-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.&lt;/p&gt;
&lt;p&gt;GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.&lt;/p&gt;
&lt;h3 id=&quot;samples&quot;&gt;Samples&lt;/h3&gt;
&lt;p&gt;GPT-2 generates synthetic text samples in response to the model being primed with an arbitrary input. The model is chameleon-like—it adapts to the style and content of the conditioning text. This allows the user to generate realistic and coherent continuations about a topic of their choosing, as seen by the following select samples.&lt;/p&gt;
&lt;div class=&quot;gpt-2-items js-samples&quot;&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample1&quot; data-index=&quot;1&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;63&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;12&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;64&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, 10 tries)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Pérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Pérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Dr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;While their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;However, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample2&quot; data-index=&quot;2&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;47&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;8&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;36&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, first try)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;The incident occurred on the downtown train line, which runs from Covington and Ashland stations.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;In an email to Ohio news outlets, the U.S. Department of Energy said it is working with the Federal Railroad Administration to find the thief.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“The theft of this nuclear material will have significant negative consequences on public and environmental health, our workforce and the economy of our nation,” said Tom Hicks, the U.S. Energy Secretary, in a statement. “Our top priority is to secure the theft and ensure it doesn’t happen again.”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The stolen material was taken from the University of Cincinnati’s Research Triangle Park nuclear research site, according to a news release from Department officials.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The Nuclear Regulatory Commission did not immediately release any information.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;According to the release, the U.S. Department of Energy’s Office of Nuclear Material Safety and Security is leading that team’s investigation.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“The safety of people, the environment and the nation’s nuclear stockpile is our highest priority,” Hicks said. “We will get to the bottom of this and make no excuses.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample3&quot; data-index=&quot;3&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;40&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;23&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, second try)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;The 19-year-old singer was caught on camera being escorted out of the store by security guards.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Scroll down for video&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Shoplifting: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today (pictured)&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The singer was also wearing a pair of black-rimmed glasses, a black jacket, black jeans and black sandals.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;She was carrying a pair of black and white striped gloves and a small black bag.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample4&quot; data-index=&quot;4&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;47&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;22&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;We’ve trained a large language model called GPT-2 that generates realistic paragraphs of text, while also exhibiting zero shot generalization on tasks like machine translation, question answering, reading comprehension, and summarization - problems usually approached by using training datasets and models designed explicitly for these tasks.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A typical approach to language modeling is to learn the following task: predict the next word, given all of the previous words within some text. Last year, OpenAI’s Generative Pre-trained Transformer (GPT) showed that language models trained on large amounts of data can be fine-tuned to specific tasks to achieve high performance. GPT-2 shows that much larger language models trained on a more diverse dataset derived from the internet begin to learn these NLP tasks without needing task-specific training data, instead learning from examples the system derives from the raw text. These systems also display a substantial qualitative jump in the realism and coherence of generated text.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;22&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, first try)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;Here you can see the most recent progress with Generative Pre-trained Transformer:&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Figure 1: Generative Pre-trained Transformer training on several texts.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;We are now preparing a collection of datasets for translation and machine translation in our language model. We will be using one of the large number of text samples provided by The New York Times.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;We believe this project is the first step in the direction of developing large NLP systems without task-specific training data. That is, we are developing a machine language system in the generative style with no explicit rules for producing text.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;We hope for future collaborations between computer scientists, linguists, and machine learning researchers.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample5&quot; data-index=&quot;5&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;62&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;8&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;66&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, first try)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;The orcs’ response was a deafening onslaught of claws, claws, and claws; even Elrond was forced to retreat. “You are in good hands, dwarf,” said Gimli, who had been among the first to charge at the orcs; it took only two words before their opponents were reduced to a blood-soaked quagmire, and the dwarf took his first kill of the night. The battle lasted for hours until two of the largest Orcs attempted to overwhelm Aragorn. When they finally stopped, they lay defeated and lifeless for miles and miles.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“I take nothing,” said Aragorn. “But I give my word, at my peril and mine, that I will never forget this day of horror. None of us will forget. Ever!”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“I’ll never forget it!” cried Gimli, who had been in the thick of the battle but hadn’t taken part in it. One of the wounded orcs he had carried off, he was the only one of the survivors who remained uninjured. “We’ll keep the memory of that day of evil, and the war with it, alive as long as we live, my friends!”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“Then we’ll keep it alive as long as we live,” added Legolas. “And we won’t forget the first great battle of the night, even if we may have forgotten the final defeat.”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;“I agree,” Gandalf said, “but we will all remember it as the last battle in Middle-earth, and the first great battle of the new day.”&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;Aragorn drew his sword, and the Battle of Fangorn was won. As they marched out through the thicket the morning mist cleared, and the day turned to dusk.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The Two Rings were returned to Rivendell. Frodo and Sam woke up alone in their room, and Frodo found a note on his pillow. He opened it and read:&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;May the Power of the Ring be with you always, and may its light never fade. I am not sure if it matters which of the two rings we accept this day but, as you asked me, I have chosen mine. I am sorry to leave you, Frodo, but know that we are very close to the end, and that you are with us forever. May this letter find you safely in Rivendell; and if it does not, then I will accept the ring in your stead. If by any chance you find or give this letter to the enemy, may they learn the strength of the ring and may the Two Rings never be broken!&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample6&quot; data-index=&quot;6&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;50&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;8&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;For today's homework assignment, please describe the reasons for the US Civil War.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;42&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, 5 tries)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;By Donny Ferguson&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;It is easy to identify why the Civil War happened, because so many people and so many books and so much television and films tell us that it was the cause, that it has something to do with race or economics or religion. There is much agreement that it was essentially a war of slavery on behalf of capitalism, about a century of slavery. But that's not what most people think of when they think of the Civil War. Many people think of the war as about states rights. You're not wrong about that, but it's kind of misleading to say that the Civil War was a conflict between states' rights and federalism. So let's try again. What's the idea that has inspired so many people to go onto the battlefields of Gettysburg and Antietam and Gettysberg and Petersburg and Fredericksburg? The American idea of the republic--a notion of limited government--is a great part of the history.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;In 1791, Thomas Jefferson said &quot;Our Constitution was made only for a moral and religious people. It is wholly inadequate to the government of any other.&quot; The &quot;founding fathers&quot; were basically religious men who were not very concerned with states' rights. The Civil War occurred in reaction to the idea that the federal government was taking far too much power away from the states, that they weren't sovereign. Jefferson himself, as you well know, is an author of the Declaration of Independence who had some really crazy ideas about, well, you know, secession for a variety of reasons. The War Between the States (later the Great Rebellion) was a radical re-imagining of the American idea and not surprisingly, Jefferson's grandson, who took after both his father and his grandfather and wrote about it, was named Thomas, after the president. &quot;The federal and state governments are both contrived for the benefit of the rich and large property,&quot; This view of the Founding Fathers has been promoted by some of today's most prominent politicians, including some who hold prominent national offices and have a direct stake in a Republican Party that has been hijacked by the &quot;federalist&quot; wing of the party. So why is it that so many people have an easy-to-spot way of understanding the Civil War that has everything to do with the South and nothing to do with the South? The Confederate flag has been a symbol of racism for a long time, but when the Civil War started and the Civil War ended, many people suddenly began to see it differently. There was a change of heart, which I think is what led people to take the question of why the Civil War happened quite seriously.&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;The other part of the explanation is what one scholar I think gets at well and a lot of people don't is this idea that the Southern states were being left behind in the Industrial Revolution and this made the South really feel, especially in the South, that they weren't really in the forefront of the Industrial Revolution.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample7&quot; data-index=&quot;7&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;53.5&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;11&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;John F. Kennedy was just elected President of the United States after rising from the grave decades after his assassination. Due to miraculous developments in nanotechnology, Kennedy's brain was rebuilt from his remains and installed in the control center of a state-of-the art humanoid robot. Below is a transcript of his acceptance speech.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;46&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, 25 tries)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;&quot;It is time once again. I believe this nation can do great things if the people make their voices heard. The men and women of America must once more summon our best elements, all our ingenuity, and find a way to turn such overwhelming tragedy into the opportunity for a greater good and the fulfillment of all our dreams. In the months and years to come, there will be many battles in which we will have to be strong and we must give all of our energy, not to repel invaders, but rather to resist aggression and to win the freedom and the equality for all of our people. The destiny of the human race hangs in the balance; we cannot afford for it to slip away. Now and in the years to come, the challenge before us is to work out how we achieve our ultimate destiny. If we fail to do so, we are doomed.&quot;&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;&quot;We, the descendants of the men and women of the Continental Army, who faced down a superior force on behalf of our independence and who fought against the evil powers of the world for the sake of freedom and human dignity, we, the heirs of men and women like Daniel Boone whom this country has produced, can accomplish our destiny as well as you have accomplished yours; and we pledge ourselves before the entire world, every second of every day.&quot;&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;After the election, the President of the United States asked for a &quot;more complete&quot; body of information on Kennedy's brains' origin. In an e-mail exchange with me, Dr. Robert Wood, who, according to Wikipedia is the head of the University of Maryland Advanced Nanotechnology Research project, provided the following information:&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;samp&gt;(1) The brain of JFK was harvested and reconstructed via tissue sampling. There was no way that the tissue could be transported by air. (2) A sample was collected from the area around his upper chest and sent to the University of Maryland for analysis. A human brain at that point would be about one and a half cubic centimeters. The data were then analyzed along with material that was obtained from the original brain to produce a reconstruction; in layman's terms, a &quot;mesh&quot; of brain tissue. There were no additional funds from the Department of Defense involved. The samples were sent back to the FBI lab for review and analysis. (3) There was never an attempt to clone any of America's greatest presidents. As far as we knew, the President was one of the most famous people on planet earth. If it had been possible, it would have.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-sample&quot; id=&quot;sample8&quot; data-index=&quot;8&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;48.5&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;9&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;System Prompt (human-written)&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Recycling is good for the world.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NO! YOU COULD NOT BE MORE WRONG!!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;38&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Model Completion (machine-written, 25 tries)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;Recycling is NOT good for the world. It is bad for the environment, it is bad for our health, and it is bad for our economy. I'm not kidding. Recycling is not good for the environment. It is destructive to the earth and it is a major contributor to global warming. Recycling is not good for our health. It contributes to obesity and diseases like heart disease and cancer. Recycling is bad for our economy. It increases the cost of a product, and in turn, the price of everything that is made with that product. Recycling is not good for our nation. We pay a tremendous price for the privilege of having the world's most advanced and efficient recycling system. Recycling is a huge, colossal waste of time, energy, money, and resources. And THAT is why we need to get back to basics and get back to basics in our recycling efforts. One of the best ways to start is to look at the process of creating a paper product. When you make a paper product, it is basically a long chain of materials. Everything from the raw materials (wood, cardboard, paper, etc.), to the reagents (dyes, solvents, etc.) to the printing equipment (chemicals, glue, paper, ink, etc.), to the packaging, to the packaging materials (mercury, chemicals, etc.) to the processing equipment (heating, cooling, etc.), to the packaging materials, to the packaging materials that are shipped overseas and to the packaging materials that are used in the United States. Each step along the way creates tons of waste that we constantly have to clean up. The process of making a paper product is a very wasteful one. But the end result is something that all of us need to consume. And if we want to keep the recycling process running efficiently, then we really need to think about each and every step that goes into making a paper product.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text. Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g. the model sometimes writes about &lt;em&gt;fires happening under water&lt;/em&gt;), and unnatural topic switching. Exploring these types of weaknesses of language models is an &lt;a href=&quot;https://arxiv.org/abs/1601.04811&quot;&gt;active&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1510.03055&quot;&gt;area&lt;/a&gt; of &lt;a href=&quot;https://arxiv.org/abs/1701.03185&quot;&gt;research&lt;/a&gt; in the natural language processing community.&lt;/p&gt;
&lt;p&gt;Overall, we find that it takes a few tries to get a good sample, with the number of tries depending on how familiar the model is with the context. When prompted with topics that are highly represented in the data (Brexit, Miley Cyrus, Lord of the Rings, and so on), it seems to be capable of generating reasonable samples about 50% of the time. The opposite is also true: on highly technical or esoteric types of content, the model can perform poorly. Fine-tuning offers the potential for even more detailed control over generated samples—for example, we can fine-tune GPT-2 on the Amazon Reviews dataset and use this to let us write reviews conditioned on things like star rating and category.&lt;/p&gt;
&lt;p&gt;These samples have substantial policy implications: large language models are becoming increasingly easy to steer towards scalable, customized, coherent text generation, which in turn could be used in a number of beneficial as well as malicious ways. We'll discuss these implications below in more detail, and outline a publication experiment we are taking in light of such considerations.&lt;/p&gt;
&lt;h3 id=&quot;zeroshot&quot;&gt;Zero-shot&lt;/h3&gt;
&lt;p&gt;GPT-2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the &quot;zero-shot&quot; setting. GPT-2 outperforms models trained on domain-specific datasets (e.g. Wikipedia, news, books) when evaluated on those same datasets. The following table shows all our state-of-the-art zero-shot results.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(+) means a higher score is better for this domain. (-) means a lower score is better.&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;wide&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;d-block d-md-table w-100 mt-n1&quot; readability=&quot;9&quot;&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Dataset&lt;/th&gt;
&lt;th&gt;Metric&lt;/th&gt;
&lt;th&gt;Our&lt;br/&gt;Result&lt;/th&gt;
&lt;th&gt;Previous&lt;br/&gt;Record&lt;/th&gt;
&lt;th&gt;Human&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;3&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Winograd Schema&lt;br/&gt;Challenge&lt;/td&gt;
&lt;td&gt;accuracy (+)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;70.70%&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;63.7%&lt;/td&gt;
&lt;td&gt;92%+&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LAMBADA&lt;/td&gt;
&lt;td&gt;accuracy (+)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;63.24%&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;59.23%&lt;/td&gt;
&lt;td&gt;95%+&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;LAMBADA&lt;/td&gt;
&lt;td&gt;perplexity (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;8.6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;99&lt;/td&gt;
&lt;td&gt;~1-2&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Children’s Book Test&lt;br/&gt;Common Nouns&lt;br/&gt;(validation accuracy)&lt;/td&gt;
&lt;td&gt;accuracy (+)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;93.30%&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;85.7%&lt;/td&gt;
&lt;td&gt;96%&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Children’s Book Test&lt;br/&gt;Named Entities&lt;br/&gt;(validation accuracy)&lt;/td&gt;
&lt;td&gt;accuracy (+)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;89.05%&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;82.3%&lt;/td&gt;
&lt;td&gt;92%&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Penn Tree Bank&lt;/td&gt;
&lt;td&gt;perplexity (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;35.76&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;46.54&lt;/td&gt;
&lt;td&gt;unknown&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WikiText-2&lt;/td&gt;
&lt;td&gt;perplexity (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;18.34&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;39.14&lt;/td&gt;
&lt;td&gt;unknown&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;enwik8&lt;/td&gt;
&lt;td&gt;bits per&lt;br/&gt;character (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.93&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.99&lt;/td&gt;
&lt;td&gt;unknown&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;text8&lt;/td&gt;
&lt;td&gt;bits per&lt;br/&gt;character (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;0.98&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1.08&lt;/td&gt;
&lt;td&gt;unknown&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WikiText-103&lt;/td&gt;
&lt;td&gt;perplexity (-)&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;17.48&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;18.3&lt;/td&gt;
&lt;td&gt;unknown&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;GPT-2 achieves state-of-the-art on Winograd Schema, LAMBADA, and other language modeling tasks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;On other language tasks like question answering, reading comprehension, summarization, and translation, we are able to get surprising results without any fine-tuning of our models, simply by prompting the trained model in the right way (see below for examples of how we do this), though we do still fall short of state-of-the-art for specialized systems.&lt;/p&gt;
&lt;div class=&quot;gpt-2-items js-tasks&quot;&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task1&quot; data-index=&quot;1&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;57&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Reading Comprehension&lt;/strong&gt;: answer questions about given passages&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;CoQA&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;57&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Example&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of &quot;one world, one dream&quot;. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the &quot;Journey of Harmony&quot;, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;After being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: What was the theme?&lt;br/&gt;A: &quot;one world, one dream&quot;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: What was the length of the race?&lt;br/&gt;A: 137,000 km&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: Was it larger than previous ones?&lt;br/&gt;A: No&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: Where did the race begin?&lt;br/&gt;A: Olympia, Greece&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: Is there anything notable about that place?&lt;br/&gt;A: birthplace of Olympic Games&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: Where did they go after?&lt;br/&gt;A: Athens&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: How many days was the race?&lt;br/&gt;A: seven&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: Did they visit any notable landmarks?&lt;br/&gt;A: Panathinaiko Stadium&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Q: And did they climb any mountains?&lt;br/&gt;A:&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Target answers&lt;/strong&gt;: &lt;em&gt;unknown&lt;/em&gt; or &lt;em&gt;yes&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;Everest&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/reading-comprehension-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task2&quot; data-index=&quot;2&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;36&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Common Sense Reasoning&lt;/strong&gt;: resolution of an ambiguous pronoun&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;Winograd Schema Challenge&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;13&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Examples&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;The trophy doesn't fit into the brown suitcase because it is too large.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct answer&lt;/strong&gt;: &lt;em&gt;it = trophy&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;it = trophy&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The trophy doesn't fit into the brown suitcase because it is too small.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct answer&lt;/strong&gt;: &lt;em&gt;it = suitcase&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;it = suitcase&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/winograd-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task3&quot; data-index=&quot;3&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;34&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Question Answering&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;Natural Questions&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;13&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Examples&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Who wrote the book the origin of species?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct answer&lt;/strong&gt;: &lt;em&gt;Charles Darwin&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;Charles Darwin&lt;/samp&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What is the largest state in the U.S. by land mass?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct answer&lt;/strong&gt;: &lt;em&gt;Alaska&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;California&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/qa-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task4&quot; data-index=&quot;4&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;36.5&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Language Modeling of Broad Contexts&lt;/strong&gt;: predict the last word of a passage&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;LAMBADA&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;16&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Example&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Both its sun-speckled shade and the cool grass beneath were a welcome respite after the stifling kitchen, and I was glad to relax against the tree’s rough, brittle bark and begin my breakfast of buttery, toasted bread and fresh fruit. Even the water was tasty, it was so clean and cold. It almost made up for the lack of…&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Correct answer&lt;/strong&gt;: &lt;em&gt;coffee&lt;/em&gt;&lt;br/&gt;&lt;strong&gt;Model answer&lt;/strong&gt;: &lt;samp&gt;food&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/lambada-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task5&quot; data-index=&quot;5&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;43.158809891808&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Summarization&lt;/strong&gt;: summarize news articles&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;CNN and Daily Mail dataset&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;12.448863636364&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;&lt;a href=&quot;https://www.dailymail.co.uk/sciencetech/article-3030493/Step-inside-world-s-largest-replica-CAVE-French-cavern-contains-1-000-prehistoric-paintings-recreated-using-3D-scanning.html&quot; target=&quot;_blank&quot;&gt;Example text (truncated for brevity)&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in modern day France 36,000 years ago.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d'Arc Cave have been reproduced to create the biggest replica cave in the world.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.dailymail.co.uk/sciencetech/article-3030493/Step-inside-world-s-largest-replica-CAVE-French-cavern-contains-1-000-prehistoric-paintings-recreated-using-3D-scanning.html&quot; target=&quot;_blank&quot;&gt;…&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;14&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Reference summary&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;Cave mimics famous Caverne du Pont-d'Arc in France, the oldest cave decorated by man and the best preserved. The replica contains all 1,000 paintings which include 425 such as a woolly rhinoceros and mammoths. Minute details were copied using 3D modelling and anamorphic techniques, often used to shoot widescreen images. The modern cave also includes replica paw prints of bears, bones and details preserved in the original cave.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;12&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Summary (machine-written)&lt;/h4&gt;
&lt;p&gt;&lt;samp&gt;The original site in Vallon-Pont-D'arc in Southern France is a Unesco World Heritage site and is the oldest known and the best preserved cave decorated by man. The replica cave was built a few miles from the original site in Vallon-Pont-D'Arc in Southern France. The cave contains images of 14 different species of animals including woolly rhinoceros, mammoths, and big cats.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/summarization-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-item pt-4 pt-md-5 js-task&quot; id=&quot;task6&quot; data-index=&quot;6&quot;&gt;
&lt;div class=&quot;gpt-2-container container&quot;&gt;
&lt;div class=&quot;row&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;35.5&quot;&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;7&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Task&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Machine Translation&lt;/strong&gt;: translate French sentences to English&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Dataset&lt;/h4&gt;
&lt;p&gt;WMT-14 Fr-En&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot; readability=&quot;14&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Example&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;French sentence&lt;/strong&gt;:&lt;br/&gt;&lt;em&gt;Un homme a expliqué que l'opération gratuite qu'il avait subie pour soigner une hernie lui permettrait de travailler à nouveau.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reference translation&lt;/strong&gt;:&lt;br/&gt;&lt;em&gt;One man explained that the free hernia surgery he'd received will allow him to work again.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model translation&lt;/strong&gt;:&lt;br/&gt;&lt;samp&gt;A man told me that the operation gratuity he had been promised would not allow him to travel.&lt;/samp&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;gpt-2-section&quot;&gt;
&lt;h4 class=&quot;gpt-2-heading small-caps mt-lg-0.125&quot;&gt;Performance&lt;/h4&gt;
&lt;img src=&quot;https://openai.com/content/images/2019/03/mt-1.svg&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We hypothesize that since these tasks are a subset of general language modeling, we can expect performance to increase further with more compute and data. Others have published &lt;a href=&quot;https://arxiv.org/abs/1901.11373&quot;&gt;similar hypotheses&lt;/a&gt;. We also expect fine-tuning to help performance on downstream tasks, though we have yet to do thorough experiments.&lt;/p&gt;
&lt;h3 id=&quot;policyimplications&quot;&gt;Policy Implications&lt;/h3&gt;
&lt;p&gt;Large, general language models could have significant societal impacts, and also have many near-term applications. We can anticipate how systems like GPT-2 could be used to create:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;AI writing assistants&lt;/li&gt;
&lt;li&gt;More capable dialogue agents&lt;/li&gt;
&lt;li&gt;Unsupervised translation between languages&lt;/li&gt;
&lt;li&gt;Better speech recognition systems&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We can also imagine the application of these models for &lt;a href=&quot;https://blog.openai.com/preparing-for-malicious-uses-of-ai/&quot;&gt;malicious purposes&lt;/a&gt;, including the following (or other applications we can't yet anticipate):&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Generate misleading news articles&lt;/li&gt;
&lt;li&gt;Impersonate others online&lt;/li&gt;
&lt;li&gt;Automate the production of abusive or faked content to post on social media&lt;/li&gt;
&lt;li&gt;Automate the production of spam/phishing content&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;These findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns. The public at large will need to become more skeptical of text they find online, just as the &quot;&lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954&quot;&gt;deep fakes&lt;/a&gt;&quot; phenomenon calls for more skepticism about images.&lt;/p&gt;
&lt;p&gt;Today, malicious actors—some of which are political in nature—have already begun to target the shared online commons, using &lt;a href=&quot;https://www.washingtonpost.com/amphtml/opinions/global-opinions/the-new-censors-wont-delete-your-words--theyll-drown-them-out/2019/02/08/c8a926a2-2b27-11e9-984d-9b8fba003e81_story.html&quot;&gt;things like&lt;/a&gt; “robotic tools, fake accounts and dedicated teams to troll individuals with hateful commentary or smears that make them afraid to speak, or difficult to be heard or believed”. We should consider how research into the generation of synthetic images, videos, audio, and text may further combine to unlock new as-yet-unanticipated capabilities for these actors, and should seek to create better technical and non-technical countermeasures. Furthermore, the underlying technical innovations inherent to these systems are core to fundamental artificial intelligence research, so it is not possible to control research in these domains without slowing down the progress of AI as a whole.&lt;/p&gt;
&lt;h3 id=&quot;releasestrategy&quot;&gt;Release Strategy&lt;/h3&gt;
&lt;p&gt;Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a &lt;a href=&quot;https://github.com/openai/gpt-2/&quot;&gt;much smaller version of GPT-2 along with sampling code&lt;/a&gt;. We are not releasing the dataset, training code, or GPT-2 model weights. Nearly a year ago we wrote in the &lt;a href=&quot;https://blog.openai.com/openai-charter/&quot;&gt;OpenAI Charter&lt;/a&gt;: &quot;we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research,&quot; and we see this current work as potentially representing the early beginnings of such concerns, which we expect may grow over time. This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community.&lt;/p&gt;
&lt;p&gt;We are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems.&lt;/p&gt;
&lt;p&gt;We also think governments should consider expanding or commencing initiatives to more systematically monitor the societal impact and diffusion of AI technologies, and to measure the progression in the capabilities of such systems. If pursued, these efforts could yield a better evidence base for decisions by AI labs and governments regarding publication decisions and AI policy more broadly.&lt;/p&gt;
&lt;p&gt;We will further publicly discuss this strategy in six months. If you’d like to discuss large language models and their implications, please email us at: &lt;a href=&quot;mailto:languagequestions@openai.com&quot;&gt;languagequestions@openai.com&lt;/a&gt;. And if you’re excited about working on cutting-edge language models (and thinking through their policy implications), &lt;a href=&quot;https://openai.com/jobs/&quot;&gt;we’re hiring&lt;/a&gt;.&lt;/p&gt;
&lt;hr class=&quot;mb-1&quot;/&gt;&lt;p&gt;&lt;em class=&quot;pt-0.5 d-block&quot; id=&quot;update&quot;&gt;GPT-2 Interim Update, May 2019&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We're implementing two mechanisms to responsibly publish GPT-2 and hopefully future releases: staged release and partnership-based sharing. We're now &lt;a href=&quot;https://github.com/openai/gpt-2&quot;&gt;releasing&lt;/a&gt; a larger 345M version of GPT-2 as a next step in staged release, and are sharing the 762M and 1.5B versions with partners in the AI and security communities who are working to improve societal preparedness for large language models.&lt;/p&gt;
&lt;h3 id=&quot;stagedrelease&quot;&gt;Staged Release&lt;/h3&gt;
&lt;p&gt;Staged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT-2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage.&lt;/p&gt;
&lt;p&gt;As the next step in our staged release strategy, we are releasing the 345M parameter version of GPT-2. This model features improved performance relative to the 117M version, though falls short of the 1.5B version with respect to the ease of generating coherent text. We have been excited to see so many positive uses of GPT-2-117M, and hope that 345M will yield still more benefits.&lt;/p&gt;
&lt;p&gt;While the misuse risk of 345M is higher than that of 117M, we believe it is substantially lower than that of 1.5B, and we believe that training systems of similar capability to GPT-2-345M is well within the reach of many actors already; this evolving replication landscape has informed our decision-making about what is appropriate to release.&lt;/p&gt;
&lt;p&gt;In making our 345M release decision, some of the factors we considered include: the ease of use (by various users) of different model sizes for generating coherent text, the role of humans in the text generation process, the likelihood and timing of future replication and publication by others, evidence of use in the wild and expert-informed inferences about unobservable uses, proofs of concept such as the review generator mentioned in the original blog post, the strength of demand for the models for beneficial purposes, and the input of stakeholders and experts. We remain uncertain about some of these variables and continue to welcome input on how to make appropriate language model publication decisions.&lt;/p&gt;
&lt;p&gt;We hope that ongoing research on bias, detection, and misuse will give us the confidence to publish larger models in a timely manner, and at the six month mark we will share a fuller analysis of language models’ societal implications and our heuristics for release decisions.&lt;/p&gt;
&lt;h3 id=&quot;partnerships&quot;&gt;Partnerships&lt;/h3&gt;
&lt;p&gt;Since releasing this blog post in February, we have had conversations with many external researchers, technology companies, and policymakers about our release strategy and the implications of increasingly large language models. We’ve also presented or discussed our work at events, including a dinner co-hosted with the &lt;a href=&quot;https://www.partnershiponai.org/when-is-it-appropriate-to-publish-high-stakes-ai-research/&quot;&gt;Partnership on AI&lt;/a&gt; and a presentation to policymakers in Washington DC at the &lt;a href=&quot;https://www.state.gov/r/gec/tech/index.htm&quot;&gt;Global Engagement Center&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are currently forming research partnerships with academic institutions, non-profits, and industry labs focused on increasing societal preparedness for large language models. In particular, we are sharing the 762M and 1.5B parameter versions of GPT-2 to facilitate research on language model output detection, language model bias analysis and mitigation, and analysis of misuse potential. In addition to observing the impacts of language models in the wild, engaging in dialogue with stakeholders, and conducting in-house analysis, these research partnerships will be a key input to our decision-making on larger models. See &lt;a href=&quot;https://openai.com/blog/better-language-models/#talktous&quot;&gt;below&lt;/a&gt; for details on how to get involved.&lt;/p&gt;
&lt;h3 id=&quot;outputdataset&quot;&gt;Output Dataset&lt;/h3&gt;
&lt;p&gt;We’re &lt;a href=&quot;https://github.com/openai/gpt-2-output-dataset&quot;&gt;releasing&lt;/a&gt; a dataset of GPT-2 outputs from all 4 model sizes, with and without top-k truncation, as well as a subset of the WebText corpus used to train GPT-2. The output dataset features approximately 250,000 samples per model/hyperparameter pair, which we expect is sufficient to help a wider range of researchers perform quantitative and qualitative analysis on the three topics above. Alongside these datasets, we are including a baseline analysis of some detection-related properties of the models, which we hope others will be able to quickly build on.&lt;/p&gt;
&lt;h3 id=&quot;talktous&quot;&gt;Talk to Us&lt;/h3&gt;
&lt;p&gt;We are interested in collaborating with researchers working on language model output detection, bias, and publication norms, and with organizations potentially affected by large language models: please reach out at &lt;a href=&quot;mailto:languagepartners@openai.com&quot;&gt;languagepartners@openai.com&lt;/a&gt;. Additionally, OpenAI’s language, safety, and policy teams will be at &lt;a href=&quot;https://iclr.cc/&quot;&gt;ICLR&lt;/a&gt; next week, including at the Reproducibility workshop and the OpenAI booth. In particular, we will be discussing this release strategy at the &lt;a href=&quot;https://aiforsocialgood.github.io/iclr2019/schedule.htm&quot;&gt;AI for Social Good&lt;/a&gt; workshop.&lt;/p&gt;
</description>
<pubDate>Sat, 04 May 2019 23:27:24 +0000</pubDate>
<dc:creator>p1esk</dc:creator>
<og:type>article</og:type>
<og:title>Better Language Models and Their Implications</og:title>
<og:description>We’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization.</og:description>
<og:url>https://openai.com/blog/better-language-models/</og:url>
<og:image>https://openai.com/content/images/2019/03/2x-no-mark.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://openai.com/blog/better-language-models/</dc:identifier>
</item>
<item>
<title>Rose Marie Bentley lived for 99 years with organs in all the wrong places</title>
<link>https://www-m.cnn.com/2019/04/08/health/99-year-old-backward-organs-medical-oddity/index.html</link>
<guid isPermaLink="true" >https://www-m.cnn.com/2019/04/08/health/99-year-old-backward-organs-medical-oddity/index.html</guid>
<description>&lt;div readability=&quot;35.5&quot;&gt;
&lt;p&gt;&lt;strong data-type=&quot;sourceText&quot;&gt;(CNN) —&lt;/strong&gt; &lt;span&gt;On an early spring day in 2018, the faint smell of formaldehyde floating in the air, 26-year-old medical student Warren Nielsen and four of his classmates prepped a cadaver in the chilly dissection lab at Oregon Health and Science University in Portland.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similar groups of five gathered around bodies on the other 15 tables in the anatomy class, all eager to explore the mysteries of the human body they had seen only in textbooks.&lt;/p&gt;
&lt;p&gt;The cadaver assigned to Nielsen's team was a 99-year-old woman who had died of natural causes. Her name was Rose Marie Bentley, but the students didn't know that then. To honor and respect the privacy of those who offer their bodies to science, no further details are given medical students about the person who had once inhabited the body lying on the silvery slab before them.&lt;/p&gt;
&lt;div class=&quot;ImageCaptionWrapper&quot; readability=&quot;8&quot;&gt;

&lt;p&gt;Rose Marie Bentley, not long before she died and donated her body to science.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;But as the students and their professors were soon to find out, Bentley was special, so special she deserved her own unique spot in medical literature and history books.&lt;/p&gt;

&lt;p&gt;The reason? A condition called situs inversus with levocardia, in which most vital organs are reversed -- almost like a mirror inside the body. That, along with a host of other weird but wonderful abnormalities, made Bentley a sort of medical unicorn.&lt;/p&gt;


&lt;/div&gt;

</description>
<pubDate>Sat, 04 May 2019 21:10:57 +0000</pubDate>
<dc:creator>nradov</dc:creator>
<og:title>She lived for 99 years with organs in all the wrong places and never knew it</og:title>
<og:description>Oregon medical students were shocked and surprised when they opened the cadaver of Rose Marie Bentley.</og:description>
<og:type>article</og:type>
<og:url>https://www.cnn.com/2019/04/08/health/99-year-old-backward-organs-medical-oddity/index.html</og:url>
<og:image>//cdn.cnn.com/cnnnext/dam/assets/190407175558-01-rose-marie-bentley-super-tease.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www-m.cnn.com/2019/04/08/health/99-year-old-backward-organs-medical-oddity/index.html</dc:identifier>
</item>
<item>
<title>Tell HN: Archive.is inaccessible via Cloudflare DNS (1.1.1.1)</title>
<link>https://news.ycombinator.com/item?id=19828317</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=19828317</guid>
<description>&lt;tr readability=&quot;0.58823529411765&quot;&gt;&lt;td bgcolor=&quot;#FF6600&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr id=&quot;pagespace&quot; title=&quot;Tell HN: Archive.is inaccessible via Cloudflare DNS (1.1.1.1)&quot;&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;5.7802197802198&quot;&gt;&lt;td&gt;
&lt;table class=&quot;fatitem&quot; border=&quot;0&quot; readability=&quot;4.541601255887&quot;&gt;&lt;tr class=&quot;athing&quot; id=&quot;19828317&quot; readability=&quot;0&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;title&quot;/&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;title&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=19828317&quot; class=&quot;storylink&quot;&gt;Tell HN: Archive.is inaccessible via Cloudflare DNS (1.1.1.1)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.75&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td class=&quot;subtext&quot;&gt;&lt;span class=&quot;score&quot; id=&quot;score_19828317&quot;&gt;293 points&lt;/span&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=ikeboy&quot; class=&quot;hnuser&quot;&gt;ikeboy&lt;/a&gt; &lt;span class=&quot;age&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=19828317&quot;&gt;21 hours ago&lt;/a&gt;&lt;/span&gt; &lt;span id=&quot;unv_19828317&quot;/&gt; | &lt;a href=&quot;https://news.ycombinator.com/hide?id=19828317&amp;amp;goto=item%3Fid%3D19828317&quot;&gt;hide&lt;/a&gt; | &lt;a href=&quot;https://hn.algolia.com/?query=Tell%20HN%3A%20Archive.is%20inaccessible%20via%20Cloudflare%20DNS%20(1.1.1.1)&amp;amp;sort=byDate&amp;amp;dateRange=all&amp;amp;type=story&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0&quot; class=&quot;hnpast&quot;&gt;past&lt;/a&gt; | &lt;a href=&quot;https://www.google.com/search?q=Tell%20HN%3A%20Archive.is%20inaccessible%20via%20Cloudflare%20DNS%20(1.1.1.1)&quot;&gt;web&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/fave?id=19828317&amp;amp;auth=3fb57a62e161836243d2a2fb0a70d6b3dc0a9e79&quot;&gt;favorite&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/item?id=19828317&quot;&gt;184 comments&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td readability=&quot;13&quot;&gt;I noticed I couldn't connect to archive.is, eventually I figured out it was an issue with cloudflare DNS, 1.1.1.1. Checking nslookup confirms this:
&lt;p&gt;nslookup archive.is 1.1.1.1 Server: 1.1.1.1 Address: 1.1.1.1#53&lt;/p&gt;
&lt;p&gt;Non-authoritative answer: Name: archive.is Address: 127.0.0.4&lt;/p&gt;
&lt;p&gt;nslookup archive.is 8.8.8.8 Server: 8.8.8.8 Address: 8.8.8.8#53&lt;/p&gt;
&lt;p&gt;Non-authoritative answer: Name: archive.is Address: 94.16.117.236&lt;/p&gt;
&lt;p&gt;Cloudflare is returning a localhost address which prevents you from accessing the website.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;1&quot;&gt;&lt;td&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;10&quot; width=&quot;0&quot;/&gt;&lt;br/&gt;&lt;center&gt;&lt;span class=&quot;yclinks&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/newsguidelines.html&quot;&gt;Guidelines&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/newsfaq.html&quot;&gt;FAQ&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Support&lt;/a&gt; | &lt;a href=&quot;https://github.com/HackerNews/API&quot;&gt;API&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/security.html&quot;&gt;Security&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/lists&quot;&gt;Lists&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/bookmarklet.html&quot; rel=&quot;nofollow&quot;&gt;Bookmarklet&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/legal/&quot;&gt;Legal&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/apply/&quot;&gt;Apply to YC&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Contact&lt;/a&gt;&lt;/span&gt;
&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Sat, 04 May 2019 18:36:34 +0000</pubDate>
<dc:creator>ikeboy</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=19828317</dc:identifier>
</item>
<item>
<title>Spaced Repetition</title>
<link>https://www.gwern.net/Spaced-repetition</link>
<guid isPermaLink="true" >https://www.gwern.net/Spaced-repetition</guid>
<description>&lt;p&gt;When should one review? In the morning? In the evening? Any old time? The studies demonstrating the spacing effect do not control or vary the time of day, so in one sense, the answer is: it doesn’t matter - if it did matter, there would be considerable variance in how effective the effect is based on when a particular study had its subjects do their reviews.&lt;/p&gt;&lt;p&gt;So one reviews at whatever time is convenient. Convenience makes one more likely to stick with it, and sticking with it overpowers any temporary improvement.&lt;/p&gt;&lt;p&gt;If one is not satisfied with that answer, then on general considerations, one ought to review before bedtime &amp;amp; sleep. &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory%20consolidation%23Spacing%20effect&quot; title=&quot;Wikipedia: Memory consolidation#Spacing effect&quot;&gt;Memory consolidation&lt;/a&gt; seems to be related, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Sleep%20and%20memory&quot; title=&quot;Wikipedia: Sleep and memory&quot;&gt;sleep&lt;/a&gt; is known to powerfully influence what memories enter long-term memory, &lt;a href=&quot;http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0033079&quot; title=&quot;Memory for Semantically Related and Unrelated Declarative Information: The Benefit of Sleep, the Cost of Wake&quot;&gt;strengthening memories&lt;/a&gt; of material learned close to bedtime and &lt;a href=&quot;http://www.pnas.org/content/106/25/10130.full&quot; title=&quot;'REM, not incubation, improves creativity by priming associative networks', Cai et al 2009&quot;&gt;increasing creativity&lt;/a&gt;; interrupting sleep without affecting total sleep time or quality still &lt;a href=&quot;http://www.pnas.org/content/early/2011/07/20/1015633108&quot;&gt;damages memory formation in mice&lt;/a&gt;. So reviewing before bedtime would be best. (Other mental exercises show improvement when trained before bedtime; for example, &lt;a href=&quot;https://www.gwern.net/DNB-FAQ#sleep&quot;&gt;dual n-back&lt;/a&gt;.) One possible mechanism is that it may be that the &lt;a href=&quot;http://www.jneurosci.org/content/31/5/1563.full&quot; title=&quot;Sleep Selectively Enhances Memory Expected to Be of Future Relevance&quot;&gt;&lt;em&gt;expectancy&lt;/em&gt;&lt;/a&gt; of future reviews/tests is enough to encourage memory consolidation during sleep; so if one reviews and goes to bed, presumably the expectancy is stronger than if one reviewed at breakfast and had an eventful day and forgot entirely about the reviewed flashcards. (See also the correlation between time of studying &amp;amp; GPA in Hartwig &amp;amp; Dunlosky 2012.) Neural growth may be related; from Stahl 2010:&lt;/p&gt;&lt;div id=&quot;&quot;&gt;
&lt;p&gt;Recent advances in our understanding of the neurobiology underlying normal human memory formation have revealed that learning is not an event, but rather a process that unfolds over time.&lt;sup&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1876761/&quot; title=&quot;'Neurogenesis and the spacing effect: Learning over time enhances memory and the survival of new neurons', Sisti et al 2007&quot;&gt;16&lt;/a&gt;,&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3650827/&quot; title=&quot;'Reconsolidation: maintaining memory relevance', Lee 2009&quot;&gt;17&lt;/a&gt;,&lt;a href=&quot;https://www.gwern.net/docs/spacedrepetition/2010-oneill.pdf&quot; title=&quot;'Play it again: reactivation of waking experience and memory', O'Neill et al 2010&quot;&gt;18&lt;/a&gt;,[Squire 2003 &lt;em&gt;&lt;a href=&quot;http://www.amazon.com/Fundamental-Neuroscience-Second-Larry-Squire/dp/0126603030/?tag=gwernnet-20&quot;&gt;Fundamental Neuroscience&lt;/a&gt;&lt;/em&gt;],&lt;a href=&quot;http://davidjf.free.fr/new/Xtech_scientific%20american_%20making%20memories%20stick.pdf&quot; title=&quot;'Making Memories Stick', Fields 2005, Scientific American&quot;&gt;20&lt;/a&gt;&lt;/sup&gt; Thus, it is not surprising that learning strategies that repeat materials over time enhance their retention.&lt;sup&gt;20,&lt;a href=&quot;https://www.gwern.net/docs/spacedrepetition/1980-glenberg.pdf&quot; title=&quot;'Spacing repetitions over 1 week', Glenberg &amp;amp; Lehmann 1980&quot;&gt;21&lt;/a&gt;,&lt;a href=&quot;https://www.gwern.net/docs/spacedrepetition/1991-toppino.pdf&quot; title=&quot;'The effect of spacing repetitions on the recognition memory of young children and adults', Toppino et al 1991&quot;&gt;22&lt;/a&gt;,&lt;a href=&quot;https://www.gwern.net/docs/spacedrepetition/1978-landauer.pdf&quot; title=&quot;'Optimum rehearsal patterns and name learning', Landauer &amp;amp; Bjork 1978&quot;&gt;23&lt;/a&gt;,&lt;a href=&quot;http://www.wsu.edu/~fournier/Teaching/psych592/Readings/Karpicke_et_al_2008.pdf&quot; title=&quot;'The Critical Importance of Retrieval for Learning', Karpicke &amp;amp; Roediger 2008&quot;&gt;24&lt;/a&gt;,&lt;a href=&quot;https://pdfs.semanticscholar.org/6698/bf91c9333faa0d333a800254b8063230d4f4.pdf&quot; title=&quot;'Optimizing retrieval as a learning event: When and why expanding retrieval practice enhances long-term retention', Storm et al 2010&quot;&gt;25&lt;/a&gt;,&lt;a href=&quot;http://www.pashler.com/Articles/Pashler.Rohrer.Cepeda.Carpenter_2007.pdf&quot; title=&quot;'Enhancing learning and retarding forgetting: Choices and consequences', Pashler et al 2007&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;…Thousands of new cells are generated in this region every day, although many of these cells die within weeks of their creation.&lt;sup&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/45c2/7c08fbb43f8728e69a7447366d4a4f74e088.pdf&quot; title=&quot;'Adult neurogenesis produces a large pool of new granule cells in the dentate gyrus', Cameron &amp;amp; McKay 2001&quot;&gt;31&lt;/a&gt;&lt;/sup&gt; The survival of dentate gyrus neurons has been shown to be enhanced in animals when they are placed into learning situations.&lt;sup&gt;16-20&lt;/sup&gt; Animals that learn well retain more dentate gyrus neurons than do animals that do not learn well. Furthermore, 2 weeks after testing, animals trained in discrete spaced intervals over a period of time, rather than in a single presentation or a ‘massed trial’ of the same information, remember better.&lt;sup&gt;16-20&lt;/sup&gt; The precise mechanism that links neuronal survival with learning has not yet been identified. One theory is that the hippocampal neurons that preferentially survive are the ones that are somehow activated during the learning process.&lt;sup&gt;16-20&lt;/sup&gt; The distribution of learning over a period of time may be more effective in encouraging neuronal survival by allowing more time for changes in gene expression and protein synthesis that extend the life of neurons that are engaged in the learning process.&lt;/p&gt;
&lt;p&gt;…Transferring memory from the encoding stage, which occurs during alert wakefulness, into consolidation must thus occur at a time when interference from ongoing new memory formation is reduced.&lt;sup&gt;17,18&lt;/sup&gt; One such time for this transfer is during sleep, especially during non-rapid eye movement sleep, when the hippocampus can communicate with other brain areas without interference from new experiences.&lt;sup&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/d0a7/f06ed267f3193daab1175a65abb7a067bef4.pdf&quot; title=&quot;'Sleep, Learning, and Dreams: Off-line Memory Reprocessing', Stickgold et al 2001&quot;&gt;32&lt;/a&gt;,&lt;a href=&quot;https://pdfs.semanticscholar.org/77fd/0cf03de8a6c4f56c5decc7c47ebe69cf98c1.pdf&quot; title=&quot;'The contribution of sleep to hippocampus-dependent memory consolidation', Marshall &amp;amp; Born 2007&quot;&gt;33&lt;/a&gt;,&lt;a href=&quot;https://www.gwern.net/docs/spacedrepetition/2001-maquet.pdf&quot; title=&quot;'The Role of Sleep in Learning and Memory', Maquet et al 2001&quot;&gt;34&lt;/a&gt;&lt;/sup&gt; Maybe that is why some decisions are better made after a good night’s rest and also why pulling an all-nighter, studying with sleep deprivation, may allow you to pass an exam an hour later but not remember the material a day later.&lt;/p&gt;
&lt;/div&gt;&lt;div id=&quot;prospects-extended-flashcards&quot;&gt;&lt;h4&gt;&lt;a href=&quot;https://www.gwern.net/Spaced-repetition#prospects-extended-flashcards&quot; title=&quot;Link to section: 'Prospects: extended flashcards'&quot;&gt;Prospects: extended flashcards&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Let’s step back for a moment. What are all our flashcards, small and large, doing for us? Why do I have a pair of flashcards for the word ‘anent’ among many others? I can just look it up.&lt;/p&gt;
&lt;p&gt;But look ups take time compared to already knowing something. (Let’s ignore the previously discussed 5 minute rule.) If we think about this abstractly in a computer science context, we might recognize it as an old concept in algorithms &amp;amp; optimization discussions - the &lt;a href=&quot;https://en.wikipedia.org/wiki/space-time%20tradeoff&quot; title=&quot;Wikipedia: space-time tradeoff&quot;&gt;space-time tradeoff&lt;/a&gt;. We trade off lookup time against limited skull space.&lt;/p&gt;
&lt;p&gt;Consider the sort of factual data already given as examples - we might one day need to know the average annual rainfall in Honolulu or Austin, but it would require too much space to memorize such data for all capitals. There are millions of English words, but in practice any more than 100,000 is excessive. More surprising is a sort of procedural knowledge. An extreme form of space-time tradeoffs in computers is when a computation is replaced by pre-calculated constants. We could take a math &lt;a href=&quot;https://en.wikipedia.org/wiki/Function%20%28mathematics%29&quot; title=&quot;Wikipedia: Function (mathematics)&quot;&gt;function&lt;/a&gt; and calculate its output for each possible input. Usually such a &lt;a href=&quot;https://en.wikipedia.org/wiki/lookup%20table&quot; title=&quot;Wikipedia: lookup table&quot;&gt;lookup table&lt;/a&gt; of input to output is really large. Think about how many entries would be in such a table for all possible integer multiplications between 1 and 1 billion. But sometimes the table is really small (like binary Boolean functions) or small (like trigonometric tables) or large but still useful (&lt;a href=&quot;https://en.wikipedia.org/wiki/rainbow%20table&quot; title=&quot;Wikipedia: rainbow table&quot;&gt;rainbow table&lt;/a&gt;s usually start in the gigabytes and easily reach terabytes).&lt;/p&gt;
&lt;p&gt;Given an infinitely large lookup table, we could replace &lt;em&gt;completely&lt;/em&gt; the skill of, say, addition or multiplication by the lookup table. No computation. The space-time tradeoff taken to the extreme of the space side of the continuum. (We could go the other way and define multiplication or addition as the slow computation which doesn’t know any specifics like the &lt;a href=&quot;https://en.wikipedia.org/wiki/multiplication%20table&quot; title=&quot;Wikipedia: multiplication table&quot;&gt;multiplication table&lt;/a&gt; - as if every time you wanted to add &lt;span class=&quot;mjpage&quot;&gt;&lt;span class=&quot;mjx-chtml&quot;&gt;&lt;span class=&quot;mjx-math&quot; aria-label=&quot;Equation&quot;&gt;&lt;span class=&quot;mjx-mrow&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;mjx-semantics&quot;&gt;&lt;span class=&quot;mjx-mrow&quot;&gt;&lt;span class=&quot;mjx-mn&quot;&gt;&lt;span class=&quot;mjx-char MJXc-TeX-main-R&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mjx-mo MJXc-space2&quot;&gt;&lt;span class=&quot;mjx-char MJXc-TeX-main-R&quot;&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mjx-mn MJXc-space2&quot;&gt;&lt;span class=&quot;mjx-char MJXc-TeX-main-R&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; you had to count on 4 fingers.)&lt;/p&gt;
&lt;p&gt;So suppose we were children who wanted to learn multiplication. SRS and Mnemosyne can’t help because multiplication is not a specific factoid? The space-time tradeoff shows us that we can de-proceduralize multiplication and turn it partly into factoids. It wouldn’t be hard for us to write a quick script or macro to generate, say, 500 random cards which ask us to multiply AB by XY, and import them to Mnemosyne.&lt;/p&gt;
&lt;p&gt;After all, which is your mind going to do - get good at multiplying 2 numbers (generate on-demand), or memorize 500 different multiplication problems (&lt;a href=&quot;https://en.wikipedia.org/wiki/memoize&quot; title=&quot;Wikipedia: memoize&quot;&gt;memoize&lt;/a&gt;)? From my experience with multiple subtle variants on a card, the mind gives up after just a few and falls back on a problem-solving approach - which is exactly what one wants to exercise, in this case. Congratulations; you have done the impossible.&lt;/p&gt;
&lt;p&gt;From a software engineering point of view, we might want to modify or improve the cards, and 500 snippets of text would be a tad hard to update. So coolest would be a ‘dynamic card’. Add a markup type like &lt;code&gt;&amp;lt;eval src=&quot;&quot;&amp;gt;&lt;/code&gt; , and then Mnemosyne feeds the &lt;code&gt;src&lt;/code&gt; argument straight into the Python interpreter, which returns a &lt;a href=&quot;https://en.wikipedia.org/wiki/tuple&quot; title=&quot;Wikipedia: tuple&quot;&gt;tuple&lt;/a&gt; of the question text and the answer text. The question text is displayed to the user as usual, the user thinks, requests the answer, and grades himself. In Anki, Javascript is supported directly by the application in HTML &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tags (currently &lt;a href=&quot;https://apps.ankiweb.net/docs/manual.html#_javascript&quot;&gt;inline only&lt;/a&gt; but Anki could presumably import libraries by default), for example for kinds of &lt;a href=&quot;http://ojisanseiuchi.com/2016/03/12/JavaScript-in-Anki-cards/&quot; title=&quot;Javascript in Anki cards&quot;&gt;syntax highlighting&lt;/a&gt;, so any kind of dynamic card could be written that one wants.&lt;/p&gt;
&lt;p&gt;So for multiplication, the dynamic card would get 2 random integers, print a question like &lt;code&gt;x * y = ?&lt;/code&gt; and then print the result as the answer. Every so often you would get a new multiplication question, and as you get better at multiplication, you see it less often - exactly as you should. Still in a &lt;a href=&quot;http://www.reddit.com/r/math/comments/hvqzd/printable_math_flashcards_in_pdf_and_latex_source/c1yror5&quot;&gt;math vein&lt;/a&gt;, you could generate variants on formulas or programs where one version is the correct one and the others are subtly wrong; I do this by hand with my programming flashcards (especially if I make an error doing exercises, that signals a finer point to make several flashcards on), but it can be done automatically. &lt;a href=&quot;http://lesswrong.com/lw/64k/memory_spaced_repetition_and_life/4e5n&quot;&gt;kpreid&lt;/a&gt; &lt;a href=&quot;http://lesswrong.com/lw/64k/memory_spaced_repetition_and_life/4e5n&quot;&gt;describes&lt;/a&gt; one tool of his:&lt;/p&gt;
&lt;blockquote readability=&quot;8.4985337243402&quot;&gt;
&lt;p&gt;I have written &lt;a href=&quot;https://github.com/kpreid/mathquiz/&quot;&gt;a program&lt;/a&gt; (in the form of &lt;a href=&quot;http://kpreid.github.com/mathquiz/mathquiz.html&quot;&gt;a web page&lt;/a&gt;) which does a specialized form of this [generating ‘damaged formulas’]. It has a set of generators of formulas and damaged formulas, and presents you with a list containing several formulas of the same type (e.g. ∫ 2x dx = x^2 + C) but with one damaged (e.g. ∫ 2x dx = 2x^2 + C).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This approach generalizes to anything you can generate random problems of or have large databases of examples of. Khan Academy apparently does something like this in associating large numbers of (algorithmicly-generated?) problems with each of its little modules and tracking retention of the skill in order to decide when to do further review of that module. For example, maybe you are studying Go and are interested in learning &lt;a href=&quot;https://en.wikipedia.org/wiki/Life%20and%20death&quot; title=&quot;Wikipedia: Life and death&quot;&gt;life-and-death positions&lt;/a&gt;. Those are things that can be generated by computer Go programs, or fetched from places like &lt;a href=&quot;http://www.goproblems.com&quot;&gt;GoProblems.com&lt;/a&gt;. For even more examples, Go is rotationally invariant - the best move remains the same regardless of which way the board is oriented and since there is no canonical direction for the board (like in chess) a good player ought to be able to play the same no matter how the board looks - so each specific example can be mirrored in 3 other ways. Or one could test one’s ability to ‘read’ a board by writing a dynamic card which takes each example board/problem and adds some random pieces as long as some go-playing program like &lt;a href=&quot;https://en.wikipedia.org/wiki/GNU%20Go&quot; title=&quot;Wikipedia: GNU Go&quot;&gt;GNU Go&lt;/a&gt; says the best move hasn’t changed because of the added noise.&lt;/p&gt;
&lt;p&gt;One could learn an awful lot of things this way. Programming languages could be learned this way - someone learning &lt;a href=&quot;https://en.wikipedia.org/wiki/Haskell%20%28programming%20language%29&quot; title=&quot;Wikipedia: Haskell (programming language)&quot;&gt;Haskell&lt;/a&gt; could take all the functions listed in the Prelude or his Haskell textbook, and ask &lt;a href=&quot;https://en.wikipedia.org/wiki/QuickCheck&quot; title=&quot;Wikipedia: QuickCheck&quot;&gt;QuickCheck&lt;/a&gt; to generate random arguments for the functions and ask the &lt;a href=&quot;https://en.wikipedia.org/wiki/Glasgow%20Haskell%20Compiler&quot; title=&quot;Wikipedia: Glasgow Haskell Compiler&quot;&gt;GHC&lt;/a&gt; interpreter &lt;code&gt;ghci&lt;/code&gt; what the function and its arguments evaluate to. Games other than go, like chess, may work (a live example being &lt;a href=&quot;http://chesstempo.com/user-guide/en/probSearchSpacedRepetition.html&quot;&gt;Chess Tempo&lt;/a&gt;, and see the experience of &lt;a href=&quot;http://dfan.org/blog/2013/07/07/mnemosyne-part-3/&quot; title=&quot;I'll mostly discuss my experience using it for chess, since that's what the majority of my 8000 cards are...&quot;&gt;Dan Schmidt&lt;/a&gt;). A fair bit of mathematics. If the dynamic card has Internet access, it can pull down fresh questions from an &lt;a href=&quot;https://en.wikipedia.org/wiki/RSS%20feed&quot; title=&quot;Wikipedia: RSS feed&quot;&gt;RSS feed&lt;/a&gt; or just a website; this functionality could be quite useful in a foreign language learning context with every day bringing a fresh sentence to translate or another exercise.&lt;/p&gt;
&lt;p&gt;With some NLP software, one could write dynamic flashcards which test all sorts of things: if one confuses verbs, the program could take a template like “$PRONOUN $VERB $PARTICLE $OBJECT % {right: caresse, wrong: caresses}” which yields flashcards like “Je caresses le chat” or “Tu caresse le chat” and one would have to decide whether it was the correct conjugation. (The dynamicism here would help prevent memorizing specific sentences rather than the underlying conjugation.) In full generality, this would probably be difficult, but simpler approaches like templates may work well enough. Jack Kinsella:&lt;/p&gt;
&lt;blockquote readability=&quot;15&quot;&gt;
&lt;p&gt;I wish there were dynamic SRS decks for language learning (or other disciplines). Such decks would count the number of times you have reviewed an instance of an underlying grammatical rule or an instance of a particular piece of vocabulary, for example its singular/plural/third person conjugation/dative form. These sophisticated decks would present users with fresh example sentences on every review, thereby preventing users from remembering specific answers and compelling them to learn the process of applying the grammatical rule afresh. Moreover, these decks would keep users entertained through novelty and would present users with tacit learning opportunities through rotating vocabulary used in non-essential parts of the example sentence. Such a system, with multiple-level review rotation, would not only prevent against overfit learning, but also increase the total amount of knowledge learned per minute, an efficiency I’d gladly invest in.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even though these things seem like ‘skills’ and not ‘data’!&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Sat, 04 May 2019 17:36:33 +0000</pubDate>
<dc:creator>jger15</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.gwern.net/Spaced-repetition</dc:identifier>
</item>
<item>
<title>Cost of serving billions of images per month</title>
<link>https://medium.com/p/f499620a14d0</link>
<guid isPermaLink="true" >https://medium.com/p/f499620a14d0</guid>
<description>&lt;p name=&quot;8c79&quot; id=&quot;8c79&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;3 years ago, we wrote ‘&lt;a href=&quot;https://medium.com/unsplash/what-does-unsplash-cost-b14d5bbf30f6&quot; data-href=&quot;https://medium.com/unsplash/what-does-unsplash-cost-b14d5bbf30f6&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;What does Unsplash cost?&lt;/a&gt;’ to give a totally transparent look at the bills associated with hosting one of the largest photography sites in the world.&lt;/p&gt;
&lt;p name=&quot;af92&quot; id=&quot;af92&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Since then, Unsplash has continued to grow tremendously, now powering more image use than the major image media incumbents, Shutterstock, Getty, and Adobe, combined.&lt;/p&gt;
&lt;p name=&quot;3345&quot; id=&quot;3345&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;With &lt;a href=&quot;https://unsplash.com/developers&quot; data-href=&quot;https://unsplash.com/developers&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Unsplash’s public API&lt;/a&gt;, we power over 1000+ mainstream applications, including Medium, Trello, Squarespace, Tencent, Naver, Square, Adobe, and Dropbox.&lt;/p&gt;
&lt;p name=&quot;67a5&quot; id=&quot;67a5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;All of that growth means two things: more traffic and bigger bills.&lt;/p&gt;
&lt;p name=&quot;4a71&quot; id=&quot;4a71&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In the interest of transparency, &lt;a href=&quot;https://twitter.com/chrisliverani&quot; data-href=&quot;https://twitter.com/chrisliverani&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Chris&lt;/a&gt; and I thought we were overdue for an update.&lt;/p&gt;
&lt;p name=&quot;fa61&quot; id=&quot;fa61&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s 2019. What does it cost to host Unsplash?&lt;/p&gt;
&lt;h3 name=&quot;522f&quot; id=&quot;522f&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Then&lt;/h3&gt;
&lt;p name=&quot;9f21&quot; id=&quot;9f21&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;&lt;a href=&quot;https://medium.com/unsplash/what-does-unsplash-cost-b14d5bbf30f6&quot; data-href=&quot;https://medium.com/unsplash/what-does-unsplash-cost-b14d5bbf30f6&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;Back in 2016&lt;/a&gt;, Unsplash had just crossed 1 billion images viewed and 5.5M photos downloaded per month.&lt;/p&gt;
&lt;p name=&quot;1ba1&quot; id=&quot;1ba1&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Our team was smaller and our product was a lot less developed, which led to less services and less in-house processing. We had one main application, a traditional Rails monolith, that consumed a handful of services to create the basic Unsplash experience.&lt;/p&gt;
&lt;p name=&quot;8273&quot; id=&quot;8273&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Heavy features like search and realtime photo stats were in their infancy, which led to much simpler data processing requirements and the use of 3rd party services like Keen and a handful of CRON jobs.&lt;/p&gt;
&lt;p name=&quot;107d&quot; id=&quot;107d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The final monthly breakdown for April 2016 was:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;d0b3&quot; id=&quot;d0b3&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Web Servers: $2,731.23&lt;/li&gt;
&lt;li name=&quot;8f52&quot; id=&quot;8f52&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Monitoring: $630.00&lt;/li&gt;
&lt;li name=&quot;b3d8&quot; id=&quot;b3d8&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Data Processing: $1,000.00&lt;/li&gt;
&lt;li name=&quot;3793&quot; id=&quot;3793&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Image Hosting: $11,170.00&lt;/li&gt;
&lt;li name=&quot;16c6&quot; id=&quot;16c6&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Other: $2,127.39&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;9372&quot; id=&quot;9372&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Total (USD): $17,658.62&lt;/strong&gt;&lt;/p&gt;
&lt;h3 name=&quot;9e50&quot; id=&quot;9e50&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Now&lt;/h3&gt;
&lt;p name=&quot;d164&quot; id=&quot;d164&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;A lot has changed.&lt;/p&gt;
&lt;p name=&quot;76d6&quot; id=&quot;76d6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For one, Unsplash is a hell of a lot bigger. 10+ times bigger. We now get more traffic from our API partners than our own website and official apps, despite these growing significantly.&lt;/p&gt;
&lt;p name=&quot;18ee&quot; id=&quot;18ee&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Partnering with &lt;a href=&quot;https://unsplash.com/developers&quot; data-href=&quot;https://unsplash.com/developers&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;some of the largest consumer facing apps in the world&lt;/a&gt; has pushed our engineering team to match their practices around redundancy, monitoring, and availability, which requires more supporting resources and services.&lt;/p&gt;
&lt;p name=&quot;c154&quot; id=&quot;c154&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Our product team has continued to push the envelope for core features like search and contributor stats, requiring more and more data to be processed in greater and greater volumes.&lt;/p&gt;
&lt;p name=&quot;b18f&quot; id=&quot;b18f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;All of these things have pushed our architecture to be more complex, while also increasing the baseline costs.&lt;/p&gt;
&lt;h4 name=&quot;1776&quot; id=&quot;1776&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Web servers&lt;/h4&gt;
&lt;p name=&quot;7020&quot; id=&quot;7020&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Total monthly cost: $29,763&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;de2b&quot; id=&quot;de2b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We continue to use Heroku as our main web platform. Despite its premium cost over AWS, Azure, and Google Cloud, Heroku’s built-in deployment and configuration tools allow our team to move faster, more confidently, and more reliably.&lt;/p&gt;
&lt;p name=&quot;5caa&quot; id=&quot;5caa&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;As &lt;a href=&quot;https://medium.com/unsplash/scaling-unsplash-with-a-small-team-fbdd55571906&quot; data-href=&quot;https://medium.com/unsplash/scaling-unsplash-with-a-small-team-fbdd55571906&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;we’ve detailed previously&lt;/a&gt;, the alternatives would undoubtably be cheaper on paper. But in reality, the increased simplicity and freedom offered by Heroku for a small, product-focused team is a major cost savings advantage.&lt;/p&gt;
&lt;p name=&quot;66c3&quot; id=&quot;66c3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In addition to our main web servers and databases using Heroku, we use Fastly for distributed CDN caching, Elastic Cloud for our Elasticsearch clusters, and Stream for our feed and notification architecture.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*HQiAdumtYM441o48uSMs3w.png&quot; data-width=&quot;992&quot; data-height=&quot;623&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*HQiAdumtYM441o48uSMs3w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*HQiAdumtYM441o48uSMs3w.png&quot;/&gt;&lt;/div&gt;
Web Server costs breakdown for February 2019
&lt;h4 name=&quot;30d6&quot; id=&quot;30d6&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Monitoring&lt;/h4&gt;
&lt;p name=&quot;2928&quot; id=&quot;2928&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Total monthly cost: $7,679&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;9f4e&quot; id=&quot;9f4e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Our team is small for Unsplash’s size, with our total product team counting in at just 11 people.&lt;/p&gt;
&lt;p name=&quot;1692&quot; id=&quot;1692&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;With no one dedicated to dev-ops, ensuring Unsplash is running smoothly and never goes down, requires a lot of instrumentation and reporting.&lt;/p&gt;
&lt;p name=&quot;d07c&quot; id=&quot;d07c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Despite the volume of metrics we monitor and report on, New Relic, Sentry, and Datadog remain fairly inexpensive solutions. Our logging is certainly our largest monitoring expense, but the detailed information is crucial when debugging issues or rolling out new features.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*7dniXWsr9lPzIpK2-RwEiA.png&quot; data-width=&quot;992&quot; data-height=&quot;623&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*7dniXWsr9lPzIpK2-RwEiA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*7dniXWsr9lPzIpK2-RwEiA.png&quot;/&gt;&lt;/div&gt;
&lt;h4 name=&quot;e959&quot; id=&quot;e959&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Data Processing&lt;/h4&gt;
&lt;p name=&quot;0814&quot; id=&quot;0814&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Total monthly cost: $15,223&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;c817&quot; id=&quot;c817&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Data processing has been the area with the largest relative increase since 2016. Back then, analytics and data were an afterthought in our development process. We relied on tools like Google Analytics for user analytics and Keen for product metrics like photo views and downloads.&lt;/p&gt;
&lt;p name=&quot;3050&quot; id=&quot;3050&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Since then, we’ve needed to expand our data collection, aggregation, and reporting significantly, both from a product and a company perspective. As Unsplash has grown, the volume has also increased considerably, with hundreds of millions of events tracked every day.&lt;/p&gt;
&lt;p name=&quot;a4fc&quot; id=&quot;a4fc&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We’ve replaced Google Analytics and Keen with an open-source data pipeline, &lt;a href=&quot;https://snowplowanalytics.com/&quot; data-href=&quot;https://snowplowanalytics.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Snowplow Analytics&lt;/a&gt;. Snowplow takes care of the data collection and formatting, allowing &lt;a href=&quot;https://twitter.com/timmycarbone&quot; data-href=&quot;https://twitter.com/timmycarbone&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;Tim&lt;/a&gt;, our data engineer, to focus on data aggregation, modelling, and visualization.&lt;/p&gt;
&lt;p name=&quot;2818&quot; id=&quot;2818&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We’ve also expanded the role of the data architecture in the product to handle all of our machine learning and search processing. As we go forward, we expect this to continue to be the biggest area of expansion.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*mhdDdiVj0jQMWbHlCfYClA.png&quot; data-width=&quot;992&quot; data-height=&quot;623&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*mhdDdiVj0jQMWbHlCfYClA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*mhdDdiVj0jQMWbHlCfYClA.png&quot;/&gt;&lt;/div&gt;
Data processing costs breakdown for February 2019
&lt;h4 name=&quot;0a63&quot; id=&quot;0a63&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Image Hosting&lt;/h4&gt;
&lt;p name=&quot;4581&quot; id=&quot;4581&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Total monthly cost: $42,408&lt;/em&gt;&lt;/p&gt;
&lt;p name=&quot;f525&quot; id=&quot;f525&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;a href=&quot;https://imgix.com&quot; data-href=&quot;https://imgix.com&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Imgix&lt;/a&gt; is our single biggest expense, but we love it. Yes there are cheaper options, but trust us when we say that they aren’t as good for what we do.&lt;/p&gt;
&lt;p name=&quot;3a7a&quot; id=&quot;3a7a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We send petabytes of data through Imgix’s CDN and render more than 250 million variations of our source images every month. Their reliability, performance, and flexibility is unmatched, and negotiating our contract through them actually allows us to discount our CDN costs due to their bulk negotiations with CDN providers.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*CL9LaVveN0YcsKXRBuhEdQ.png&quot; data-width=&quot;992&quot; data-height=&quot;623&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*CL9LaVveN0YcsKXRBuhEdQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*CL9LaVveN0YcsKXRBuhEdQ.png&quot;/&gt;&lt;/div&gt;
Image hosting costs breakdown for February 2019 (very useful, I know)</description>
<pubDate>Sat, 04 May 2019 16:39:39 +0000</pubDate>
<dc:creator>ghoshbishakh</dc:creator>
<og:title>💸 What does Unsplash cost in 2019?</og:title>
<og:url>https://medium.com/unsplash/what-does-unsplash-cost-in-2019-f499620a14d0</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*E5rw0TEuowS2hXjvzTwrjQ.jpeg</og:image>
<og:description>What does it cost to run a site with tens of millions of users and billions of photos viewed per month? $10,000? $100,000? $500,000?</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/unsplash/what-does-unsplash-cost-in-2019-f499620a14d0</dc:identifier>
</item>
<item>
<title>Show HN: ZFS Implementation in Python</title>
<link>https://github.com/alcarithemad/zfsp</link>
<guid isPermaLink="true" >https://github.com/alcarithemad/zfsp</guid>
<description>&lt;div class=&quot;Box-body&quot;&gt;
&lt;article class=&quot;markdown-body entry-content p-5&quot; itemprop=&quot;text&quot;&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;ZFS, in Python, without reading the original C.&lt;/p&gt;
&lt;h2&gt;What?!&lt;/h2&gt;
&lt;p&gt;That's right.&lt;/p&gt;
&lt;h2&gt;How?&lt;/h2&gt;
&lt;p&gt;Many hours spent staring at hexdumps, and asking friends to search the internet for explanations of various features.&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;Why not?&lt;/p&gt;
&lt;p&gt;It seemed like it might be a fun project.&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;The Pipfile lists the dependencies; there aren't many.&lt;/p&gt;
&lt;p&gt;Python 3.5+ is required, but 3.6 is &lt;em&gt;not&lt;/em&gt;, as PyPy runs this code much, much faster (around 4x on the test suite) and didn't support 3.6 until recently.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;pipenv install -e .
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;N.B.: &lt;code&gt;-e&lt;/code&gt; gets you an &quot;editable&quot; install; changes to the source tree will affect the installed package's behavior.&lt;/p&gt;
&lt;h2&gt;Test Suite.&lt;/h2&gt;
&lt;p&gt;Running the test suite requires one-time access to a system with ZFS, to generate the test pools.&lt;/p&gt;
&lt;p&gt;Run &lt;code&gt;tests/fixtures.sh&lt;/code&gt; on such a system and make the resulting directory &lt;code&gt;tests/fixtures&lt;/code&gt; (this is where it will end up if you run &lt;code&gt;fixtures.sh&lt;/code&gt; directly from the &lt;code&gt;tests&lt;/code&gt; directory).&lt;/p&gt;
&lt;p&gt;The tests themselves can be run with &lt;code&gt;py.test&lt;/code&gt;. Most of the tests pass, but there are some known failures. Feel free to try fixing them.&lt;/p&gt;
&lt;p&gt;The tests are heavily parameterized and attempt to run all tests against all relevant pool variations.&lt;/p&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;zexplore&lt;/code&gt; is the main command line interface. It's reasonably well documented (I hope).&lt;/p&gt;
&lt;p&gt;There's a subcommand for some limited FUSE support, which depends on &lt;code&gt;fusepy&lt;/code&gt; (developed with &lt;code&gt;2.0.4&lt;/code&gt;). This is not installed by default in the Pipfile.&lt;/p&gt;
&lt;p&gt;Example commands:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ zexplore label -p tests/fixtures/feature_large_blocks
{b'errata': 0,
 b'features_for_read': {},
 b'guid': 6168868809305637343,
 b'hostid': 8323329,
 b'hostname': b'ubuntu-zesty',
 b'name': b'feature_large_blocks',
 b'pool_guid': 16637796155898928459,
 b'state': 1,
 b'top_guid': 6168868809305637343,
 b'txg': 16,
 b'vdev_children': 1,
 b'vdev_tree': {b'ashift': 9,
                b'asize': 62390272,
                b'create_txg': 4,
                b'guid': 6168868809305637343,
                b'id': 0,
                b'is_log': 0,
                b'metaslab_array': 33,
                b'metaslab_shift': 24,
                b'path': b'/vagrant/fixtures/feature_large_blocks',
                b'type': b'file'},
 b'version': 5000}

$ zexplore objset -p tests/fixtures/nested_datasets -P 1
{'config': 27,
 'creation_version': 5000,
 'deflate': 1,
 'feature_descriptions': 30,
 'features_for_read': 28,
 'features_for_write': 29,
 'free_bpobj': 11,
 'history': 32,
 'root_dataset': 2,
 'sync_bplist': 31}

$ zexplore ls -p tests/fixtures/nested_datasets /n1/n2/n3
x
y

$ zexplore cat -p tests/fixtures/nested_datasets /gzip-9/gzip-9 | head -n 5
A
a
aa
aal
aalii
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;Lots of things won't work, including:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;raidz with more than 4 disks will probably fail with an inscrutable error. ZFS uses a magic allocation function that's a non-linear black box. I haven't figured it out. raidz where the disks are larger than a few GB will probably also fail, but I haven't tested that at all.&lt;/li&gt;
&lt;li&gt;Some new feature flags have come out since I last worked on this seriously, and those features are (obviously) not supported. That's things like (incomplete list):
&lt;ul&gt;&lt;li&gt;SHA-512/256&lt;/li&gt;
&lt;li&gt;Skein&lt;/li&gt;
&lt;li&gt;spacemap_v2&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Some feature flags that did exist when I was actively working on this are not implemented, either because I failed to figure out how they work, or because I didn't get around to trying.&lt;/li&gt;
&lt;li&gt;pyndata is the struct description library used to read all the on-disk structures. I wrote it first, and it's bad. If I was doing this again today, I'd probably build something with dataclasses.&lt;/li&gt;
&lt;li&gt;There's a &lt;a href=&quot;https://github.com/alcarithemad/rlzjb&quot;&gt;Rust implementation of LZJB&lt;/a&gt; which is not currently included, but decompression is not the bottleneck.&lt;/li&gt;
&lt;/ul&gt;&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Sat, 04 May 2019 16:15:18 +0000</pubDate>
<dc:creator>alcari</dc:creator>
<og:image>https://avatars3.githubusercontent.com/u/538427?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>alcarithemad/zfsp</og:title>
<og:url>https://github.com/alcarithemad/zfsp</og:url>
<og:description>ZFS, in Python, without reading the original C. Contribute to alcarithemad/zfsp development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/alcarithemad/zfsp</dc:identifier>
</item>
<item>
<title>Update Regarding Add-Ons in Firefox</title>
<link>https://blog.mozilla.org/addons/2019/05/04/update-regarding-add-ons-in-firefox/</link>
<guid isPermaLink="true" >https://blog.mozilla.org/addons/2019/05/04/update-regarding-add-ons-in-firefox/</guid>
<description>&lt;p&gt;&lt;em&gt;Last Updated: 00:54 EDT May 5 2019&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Updates:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Some users are reporting that they do not have the “hotfix-update-xpi-signing-intermediate-bug-1548973” study active in “about:studies”. Rather than using work-arounds, which can lead to issues later on, we strongly recommend that you continue to wait. If it’s possible for you to receive the hotfix, you should get it by 6am EDT, 24 hours after it was first released. For everyone else, we are working to ship a more permanent solution. (May 5, 00:54 EDT)&lt;/li&gt;
&lt;li&gt;There are a number of work-arounds being discussed in the community. &lt;strong&gt;&lt;em&gt;These are not recommended as they may conflict with fixes we are deploying.&lt;/em&gt;&lt;/strong&gt; We’ll let you know when further updates are available that we recommend, and appreciate your patience. (May 4, 15:01 EDT)&lt;/li&gt;
&lt;li&gt;Temporarily disabled commenting on this post given volume and duplication. They’ll be re-enabled as more updates become available. (May 4, 13:02 EDT)&lt;/li&gt;
&lt;li&gt;Updated the post to clarify that deleting extensions can result in data loss, and should not be used to attempt a fix. (May 4, 12:58 EDT)&lt;/li&gt;
&lt;li&gt;Clarified that the study may appear in either the &lt;em&gt;Active studies&lt;/em&gt; or &lt;em&gt;Completed&lt;/em&gt; &lt;em&gt;studies&lt;/em&gt; of “about:studies” (May 4, 12:10 EDT)&lt;/li&gt;
&lt;li&gt;We’re aware that some users are reporting that their extensions remain disabled with both studies active. We’re tracking this issue on Bugzilla in &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1549078&quot;&gt;bug 1549078&lt;/a&gt;. (May 4, 12:03 EDT)&lt;/li&gt;
&lt;li&gt;Clarified that the Studies fix applies only to Desktop users of Firefox distributed by Mozilla. Firefox ESR, Firefox for Android, and some versions of Firefox included with Linux distributions will require separate updates. (May 4, 12:03 EDT)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Late on Friday May 3rd, we became aware of an issue with Firefox that prevented existing and new add-ons from running or being installed. We are very sorry for the inconvenience caused to people who use Firefox.&lt;/p&gt;
&lt;p&gt;Our team has identified and rolled-out a fix for all Firefox Desktop users on Release, Beta and Nightly. The fix will be automatically applied in the background within the next few hours. &lt;a href=&quot;https://blog.mozilla.org/addons/2019/05/04/update-regarding-add-ons-in-firefox/#workarounds&quot;&gt;&lt;strong&gt;No active steps need to be taken to make add-ons work again.&lt;/strong&gt;&lt;/a&gt; In particular, please do not delete and/or re-install any add-ons as an attempt to fix the issue. Deleting an add-on removes any data associated with it, where disabling and re-enabling does not.&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt; The fix does not apply to Firefox ESR or Firefox for Android. We’re working on releasing a fix for both, and will provide updates here and on social media.&lt;/p&gt;
&lt;p&gt;To provide this fix on short notice, we are using the Studies system. This system is enabled by default, and no action is needed &lt;em&gt;unless&lt;/em&gt; Studies have been disabled. Firefox users can check if they have Studies enabled by going to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Firefox Options/Preferences -&amp;gt; Privacy &amp;amp; Security -&amp;gt; Allow Firefox to install and run studies (scroll down to find the setting)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/addons/files/2019/05/about_prefs.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-8663&quot; src=&quot;https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/addons/files/2019/05/about_prefs.png&quot; alt=&quot;&quot; width=&quot;1363&quot; height=&quot;857&quot; srcset=&quot;https://blog.mozilla.org/addons/files/2019/05/about_prefs.png 1363w, https://blog.mozilla.org/addons/files/2019/05/about_prefs-252x158.png 252w, https://blog.mozilla.org/addons/files/2019/05/about_prefs-768x483.png 768w, https://blog.mozilla.org/addons/files/2019/05/about_prefs-600x377.png 600w&quot; sizes=&quot;(max-width: 1363px) 100vw, 1363px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Studies can be disabled again after the add-ons have been re-enabled&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;It may take up to six hours for the Study to be applied to Firefox. To check if the fix has been applied, you can enter “about:studies” in the location bar. If the fix is in the active, you’ll see “hotfix-update-xpi-signing-intermediate-bug-1548973” in either the &lt;em&gt;Active studies&lt;/em&gt; or &lt;em&gt;Completed studies&lt;/em&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/addons/files/2019/05/about_studies.png&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-8664&quot; src=&quot;https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/addons/files/2019/05/about_studies.png&quot; alt=&quot;&quot; width=&quot;1064&quot; height=&quot;680&quot; srcset=&quot;https://blog.mozilla.org/addons/files/2019/05/about_studies.png 1064w, https://blog.mozilla.org/addons/files/2019/05/about_studies-252x161.png 252w, https://blog.mozilla.org/addons/files/2019/05/about_studies-768x491.png 768w, https://blog.mozilla.org/addons/files/2019/05/about_studies-600x383.png 600w&quot; sizes=&quot;(max-width: 1064px) 100vw, 1064px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You may also see “hotfix-reset-xpi-verification-timestamp-1548973” listed, which is part of the fix and may be in the &lt;em&gt;Active studies&lt;/em&gt; or &lt;em&gt;Completed studies&lt;/em&gt; section(s).&lt;/p&gt;
&lt;p&gt;We are working on a general fix that doesn’t use the Studies system and will keep this blog post updated accordingly. We will share a more substantial update in the coming days.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Additional sources of information:&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 04 May 2019 14:59:31 +0000</pubDate>
<dc:creator>akyuu</dc:creator>
<og:title>Update Regarding Add-ons in Firefox</og:title>
<og:url>https://blog.mozilla.org/addons/2019/05/04/update-regarding-add-ons-in-firefox/</og:url>
<og:description>Last Updated: 00:54 EDT May 5 2019 Updates: Some users are reporting that they do not have the “hotfix-update-xpi-signing-intermediate-bug-1548973” study active in “about:studies”. Rather ...</og:description>
<og:image>https://blog.mozilla.org/addons/files/2019/05/about_prefs-252x158.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.mozilla.org/addons/2019/05/04/update-regarding-add-ons-in-firefox/</dc:identifier>
</item>
<item>
<title>I&amp;#039;m not collaborating with Kenneth Reitz</title>
<link>https://vorpus.org/blog/why-im-not-collaborating-with-kenneth-reitz/</link>
<guid isPermaLink="true" >https://vorpus.org/blog/why-im-not-collaborating-with-kenneth-reitz/</guid>
<description>&lt;p&gt;[Content Warning: parts of this post could be triggering for those who have experienced gaslighting or other forms of abuse]&lt;/p&gt;
&lt;p&gt;Kenneth Reitz is a famous Python developer, best-known for founding the &lt;a href=&quot;https://pypi.org/project/requests/&quot;&gt;Requests&lt;/a&gt; project. Until a few years ago, I'd never interacted with him in any serious way, but I thought highly of him.&lt;/p&gt;
&lt;p&gt;I appreciated (and still appreciate) his design taste, and emphasis on usability and beauty. Requests is a piece of critical infrastructure that holds up large parts of the software world; his puckish insistence that it's actually an &quot;art project&quot; appealed to my anarchist sympathies. I admired (and still admire) his &lt;a href=&quot;https://www.kennethreitz.org/essays/mentalhealtherror-an-exception-occurred&quot;&gt;openness with his mental health struggles&lt;/a&gt;. When I was starting my project Trio, I wanted to emphasize its friendliness and accessibility, so I borrowed his &quot;for humans&quot; tagline, and started the documentation by &lt;a href=&quot;https://github.com/python-trio/trio/blob/37de153f858e29df3a19db9fffcd0fb3f2308951/docs/source/index.rst&quot;&gt;quoting him&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then I started working on adding async support to requests.&lt;/p&gt;

&lt;p&gt;In August 2017, I &lt;a href=&quot;https://github.com/python-trio/urllib3/issues/1&quot;&gt;started working&lt;/a&gt; on adding async support to urllib3, which is the underlying HTTP library that Requests uses. This was &lt;em&gt;highly&lt;/em&gt; experimental. Historically, the state of the art was that you had one HTTP client library for synchronous code, one &lt;a href=&quot;https://github.com/twisted/treq&quot;&gt;for Twisted&lt;/a&gt;, one &lt;a href=&quot;https://www.tornadoweb.org/en/stable/httpclient.html#tornado.httpclient.AsyncHTTPClient&quot;&gt;for Tornado&lt;/a&gt;, one &lt;a href=&quot;https://aiohttp.readthedocs.io/en/stable/client.html&quot;&gt;for asyncio&lt;/a&gt;, and so on – each maintained as independent projects that didn't share code. Everyone knows this is silly, but it's very challenging to fix: you need deep expertise in HTTP, &lt;em&gt;and&lt;/em&gt; in all these different approaches to networking, &lt;em&gt;and&lt;/em&gt; some clever idea for how to reconcile their seemingly irreconcilable APIs. So every previous attempt had failed. Now I thought I had a clever idea, so I gave it a try.&lt;/p&gt;
&lt;p&gt;Reitz was very interested in this work, because he very much wanted async support in Requests, but – as he told me – didn't know how to solve these problems himself. We had some video calls and IRC discussions, and he tried to &lt;a href=&quot;https://twitter.com/kennethreitz/status/904399257804922881&quot;&gt;leverage his notoriety to recruit volunteers and send them my way&lt;/a&gt;. Nothing much came of this, but I kept plugging away, along with some other Trio contributors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://vorpus.org/blog/why-im-not-collaborating-with-kenneth-reitz/requests3-features.png&quot; alt=&quot;Screenshot of Reitz's fundraising page, showing the promised features in Requests 3&quot; width=&quot;50%&quot; align=&quot;right&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Then on March 7 2018, he &lt;a href=&quot;https://twitter.com/kennethreitz/status/971523729632178176&quot;&gt;announced&lt;/a&gt; that work had begun on &quot;Requests 3&quot;, that its headline feature would be the native async/await support I was working on, and that he was seeking donations to make this happen.&lt;/p&gt;
&lt;p&gt;Most open-source projects struggle to raise a few thousand dollars to hold a meeting, but this got people excited. He was deluged with donations from both individuals and large companies like Microsoft, Google, Slack, etc., and the fundraiser total quickly reached ~$30k.&lt;/p&gt;
&lt;p&gt;On March 15 2018, he contacted me to talk about the fundraiser. He told me he was uncertain what to do with this amount of money – he said his original goal was just to raise $5k to buy a computer. Privately, I was skeptical that the $5k computer had anything to do with Requests. Requests is a small pure-Python library; if you want to work on it, then any cheap laptop is more than sufficient. $5k is the price of a beefy server or &lt;a href=&quot;https://techbuyersguru.com/best-5000-gaming-pc-build-ultra-extreme-4k-threadripper-may-2019&quot;&gt;top-end gaming rig&lt;/a&gt;. But I figured that even if he spent $5k of the money on some unrelated computer, we could call that compensation for his past work, and it would still leave ~$25k to fulfill the promises he'd made in the fundraiser. And this was clearly a great opportunity to build some amazing new stuff. So I didn't say anything about the computer.&lt;/p&gt;
&lt;p&gt;Instead, I gave some general tips from my experience with fund-raising and grants, emphasizing the importance of transparency to maintain trust, and recommending he set up a &lt;a href=&quot;https://en.wikipedia.org/wiki/Fiscal_sponsorship&quot;&gt;fiscal sponsorship&lt;/a&gt; relationship with the Python Software Foundation (PSF) or a similar non-profit. And I tried to help with finding ways to spend the money effectively – for example, I was already working full-time, but I contacted one of the volunteers who'd been helping me to see if they were available for a contracting gig.&lt;/p&gt;
&lt;p&gt;Around this time, he also did some experiments with our work-in-progress on urllib3, which led to a &lt;a href=&quot;https://twitter.com/kennethreitz/status/974963822682427393&quot;&gt;tweet&lt;/a&gt; demonstrating &quot;Requests Core&quot; issuing multiple HTTP requests in parallel. &quot;Requests Core&quot; here was a snapshot of our work, that he forked and renamed. As far as I know, the only thing added was some basic HTTP/2 support, but unfortunately (and despite our warnings beforehand) this used a dead-end approach, so the code wasn't useful.&lt;/p&gt;
&lt;p&gt;Up to this point, there were definitely some odd features in our interactions, but, you know, people are odd sometimes. I personally wouldn't have announced a fundraiser without first talking to the people actually working on the features I was promising, but I was confident we could find some way to spend the money effectively. Maybe his HTTP/2 code wasn't useful, but at least he was getting some experience with async/await. I thought it would work out OK.&lt;/p&gt;
&lt;p&gt;Over the next few months, there were some more odd things – different members of the Requests maintainers team reported hearing very different stories about what was happening to the money. But the big change came in late May 2018, when I &lt;a href=&quot;https://vorpus.org/blog/a-farewell-to-the-berkeley-institute-for-data-science/&quot;&gt;left UC Berkeley&lt;/a&gt; and started consulting. This seemed like a potential win-win – I was looking for work and excited about the project, and he was stuck with money he had no way to spend. So I sent him an email to explore further.&lt;/p&gt;
&lt;p&gt;After a month and several follow-up pings, he finally responded. His main points were:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;He actually only raised $28k.&lt;/li&gt;
&lt;li&gt;&quot;Most of it&quot; went to taxes.&lt;/li&gt;
&lt;li&gt;He expected me to do the work of fulfilling the commitments he'd made for new features in Requests 3.&lt;/li&gt;
&lt;li&gt;But none of the money was available to fulfill those commitments; instead, he was going to wait for me to implement the new features for him, and then he needed the entire $28k to pay for writing documentation for my features.&lt;/li&gt;
&lt;li&gt;If I couldn't fulfill his commitments on a volunteer basis, he encouraged me to hold my own fundraiser.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;He ended by suggesting we do a call that week to discuss details.&lt;/p&gt;
&lt;p&gt;I was bewildered. That's not how taxes work. It's not how commitments work. The idea that novel technology stacks are free but a few pages of docs cost $28k is bizarre. The idea that you can't afford to implement new features because you're going to spend the money on documenting the new features you can't afford to build... it doesn't make any sense at all.&lt;/p&gt;
&lt;p&gt;If he'd found another way to use the money on Requests, then I would have been totally happy. I didn't have any claim on the money. But this was something else entirely. I was extremely concerned. But I still wanted to get the best outcome we could for the project and the community, so I tried to keep the lines of communication open. I agreed that a call would be a good idea, and suggested some times. I also expressed my worry that he was risking his reputation – more strongly this time – and reiterated my offer to help, writing: &quot;I think right now there is a real risk that requests 3 never materializes and the public impression becomes &quot;oh yeah Kenneth Reitz stole that money&quot;. I really hope neither of these things happens. But hope isn't a plan. I think we need a plan.&quot;&lt;/p&gt;
&lt;p&gt;At this point he stopped answering my emails, and deleted the fundraising page – the one with the record of donations received, and what he was promising in return – from his website (&lt;a href=&quot;https://web.archive.org/web/20180701201816/https://www.kennethreitz.org/requests3&quot;&gt;before&lt;/a&gt; / &lt;a href=&quot;https://web.archive.org/web/20180715115213/https://www.kennethreitz.org/requests3&quot;&gt;after&lt;/a&gt;). He also updated the &lt;a href=&quot;https://github.com/kennethreitz/requests/commit/f4818b2010ce0846e9ddc85f439b80303a395494#diff-caf2a6b8f4947d018f68893c695b5202L86&quot;&gt;Requests&lt;/a&gt; &lt;a href=&quot;https://github.com/kennethreitz/requests/commit/704abf13325355497fc2df73259fb24615c237b9&quot;&gt;documentation&lt;/a&gt; and his blog (&lt;a href=&quot;https://web.archive.org/web/20180405031054/https://www.kennethreitz.org/essays/call-for-sponsors-requests-30-development&quot;&gt;before&lt;/a&gt; / &lt;a href=&quot;https://web.archive.org/web/20180810042517/https://www.kennethreitz.org/essays/call-for-sponsors-requests-30-development&quot;&gt;after&lt;/a&gt;) to remove references to the deleted page. Some months later, he put up a &lt;a href=&quot;https://web.archive.org/web/20180916141516/https://www.kennethreitz.org/requests3/&quot;&gt;new page at the original URL&lt;/a&gt;, requesting that anyone who had questions about the fundraiser should contact him privately.&lt;/p&gt;
&lt;p&gt;Our only contact since then was an email he sent me out of the blue on February 9 this year. Instead of responding to anything I'd said before, he suggested that he and I write a joint grant proposal to the PSF, to pay me to do the same work that his fundraiser was allegedly funding. Of course this was a non-starter. I'm pretty sure the PSF is too savvy to fund something like this without asking some tough questions about where the other money went. And even if they didn't, and even if we somehow ignored the ethical issues, he was effectively asking me to link our reputations together, so that if his handling of the fundraiser blew up, it would implicate me as well. I didn't reply.&lt;/p&gt;

&lt;p&gt;Not everyone is familiar with standard practices for handling fundraising in open-source projects. So as a comparison, let me explain how the Python Software Foundation's &lt;a href=&quot;https://wiki.python.org/psf/PackagingWG&quot;&gt;Packaging Working Group&lt;/a&gt; handled the &lt;a href=&quot;https://pyfound.blogspot.com/2017/11/the-psf-awarded-moss-grant-pypi.html&quot;&gt;funding for the new PyPI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since this was our first time getting an external grant like this, we started by making a plan for what to do and who would do it, including identifying existing contributors who were available to work as contractors. Only after that was in place did we apply for the money.&lt;/p&gt;
&lt;p&gt;Then after the money arrived, we didn't just hand it over. Each of the contractors wrote up a few paragraphs to formally state their rates and what they were committing to, the group reviewed them, and then we held a quick vote over email to approve them. The contractors who were members of the Working Group didn't vote on their own proposals. Everyone provided regular invoices. And the whole process was ultimately overseen by the PSF's Board of Directors, who are elected by the community.&lt;/p&gt;
&lt;p&gt;This is a pretty lightweight process, and it isn't infallible, but it provides a baseline level of transparency and accountability. And the PSF is happy to provide this service for any Python-related project; for example, they handle donations for &lt;a href=&quot;https://palletsprojects.com/blog/donate/&quot;&gt;Flask and related projects&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Perhaps Reitz simply didn't know how these things are normally done, and this is all an unfortunate but understandable mistake. However, I find this unlikely. At the time Reitz ran his fundraiser, he was sitting on the PSF Board of Directors. And as a member of the Packaging Working Group, he participated in the voting for the PyPI funding, which happened a few months before he started his fundraiser. And yet, none of the PSF staff I've talked to knew about his fundraiser until I told them about it.&lt;/p&gt;
&lt;p&gt;In short: He chose a fundraiser structure that avoids standard accountability mechanisms he was familiar with. He never had any plan or capability to deliver what he promised. And when I offered a way for him to do it anyway, he gave me some bafflegab about how expensive it is to write docs. Effectively, his public promises about how he would use the Requests 3 money were lies from start to finish, and he hasn't shown any remorse or even understanding that this is a problem.&lt;/p&gt;
&lt;p&gt;A betrayal of trust like this damages the entire community. It's hard enough raising money for open-source as it is; this kind of thing &lt;em&gt;really&lt;/em&gt; doesn't help.&lt;/p&gt;
&lt;p&gt;And on a more personal level, I felt his interactions with me were extremely manipulative. I felt like he tried to exploit me, and that he tried to make me complicit in covering up his lies to protect his reputation. I was extremely uncomfortable with the idea of going along with this, but he created a situation where my only other options were to either give up on working on async entirely, or else to go public with the whole story, at potentially serious cost to myself.&lt;/p&gt;

&lt;p&gt;I wasn't sure what to do, so I started to quietly contacting other community members to get more context. I quickly discovered that contrary to Reitz's public reputation, every time I talked to anyone who had worked with him directly, they expressed serious discomfort with him, and many had their own disturbing stories – mine was nowhere near the worst. Here are some quotes to give you the flavor. I've anonymized most of these, but everyone quoted here is someone I trust, who maintains or previously maintained major open-source Python packages:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;
&lt;p&gt;&quot;I HATE HIM SO MUCH, he is such a fucking narcissist power broker ... his actions have &quot;abuser&quot; all over them&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&quot;He has a reasonable eye for API design. pretty much everything else about him bothers me&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&quot;we have a special maintainer channel on [...] where we basically figure out how to work around kenneth&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/sigmavirus24&quot;&gt;Ian Stapleton Cordasco&lt;/a&gt; volunteered to go on the record publicly: &quot;Having to deal with Kenneth all these years has made it such that I barely work on python open source software anymore and have largely, quietly left the community&quot;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Something I found especially disturbing: whenever I talked to any of his long-term collaborators about my experience, they immediately jumped to reassure me that I wasn't going crazy. Which... I mean, I appreciated the support. But it was clear this isn't the first time they'd had to do this. Apparently after people start working with Reitz, they always need to be reassured that they can trust their own perceptions. These collaborators have been doing this for so long that this seems normal to them. But it's not normal.&lt;/p&gt;
&lt;p&gt;This is the classic &quot;missing stair&quot; problem. Those in the inner circle quietly work around the toxic person. Outsiders come in blind. I'm pretty well-connected in the Python world, and I came in blind. In retrospect, I can see some warning signs. The insistence on &lt;a href=&quot;https://en.wikipedia.org/wiki/Auteur&quot;&gt;auteur&lt;/a&gt; status now seems less like a charming quirk, and more like a calculated bluff to claim credit and power while denying responsibility. An &lt;a href=&quot;https://twitter.com/kennethreitz/status/997485342579220482&quot;&gt;insistence on &quot;positivity&quot;&lt;/a&gt; is a common tactic among those who want to avoid accountability. But they fooled me.&lt;/p&gt;
&lt;p&gt;Something I keep thinking about: the first time I talked to him about async in Requests, months before the fundraiser, he made a strange comment: he pointed out that he was totally dependent on me to implement this, and therefore, if I were to demand that he make Requests use Trio (my library) by default instead of AsyncIO (the better-known competitor), then he'd have no choice but to acquiesce. It struck me as an incredibly strange thing to bring up – it was almost like he was asking me to manipulate him. At the time, I mumbled something about wanting to succeed on the merits, not by blackmail, and recommended that he not set a default at all. In retrospect, I'm reminded of how con artists often start by tempting their victims into some minor unethical act, so that as the con escalates they feel trapped.&lt;/p&gt;
&lt;p&gt;His collaborators also consistently cited his bipolar disorder as an excuse for whatever he did. I think this is deeply unfair to Reitz, and to everyone struggling with mental health issues. &lt;a href=&quot;https://medium.com/@tmcolon/its-a-mental-illness-not-an-excuse-to-be-an-a-hole-db0c909d14f9&quot;&gt;Illness does not erase the harm someone does to others, or their responsibility for their actions.&lt;/a&gt; Many people manage their conditions without causing this kind of harm, and when they mess up, they make amends, just like the rest of us. If someone can't do that, then as a community, we can have compassion but shouldn't give them power and influence.&lt;/p&gt;
&lt;p&gt;I think a lot of people don't realize how little Reitz actually has to do with Requests development. For many years now, actual maintenance has been done almost exclusively by other volunteers. If you look at the &lt;a href=&quot;https://pypi.org/project/requests/&quot;&gt;maintainers list on PyPI&lt;/a&gt;, you'll see he doesn't have PyPI rights to his own project, because he kept breaking stuff, so the real maintainers insisted on revoking his access. If you clone the Requests git repo, you can run &lt;code&gt;git log requests/&lt;/code&gt; to see a list of &lt;a href=&quot;https://github.com/kennethreitz/requests/commits/master/requests&quot;&gt;every time someone changed the library's source code&lt;/a&gt;, either directly or by merging someone else's pull request. The last time Reitz did either was in May 2017, when he made some whitespace cleanups.&lt;/p&gt;
&lt;p&gt;At least as far as commits go, his main contributions since then appear to consist of merging some small doc fixes, and monetizing the project by adding &lt;a href=&quot;https://github.com/kennethreitz/requests/commit/86914e2ddab3da351e484a3211406962ce1922c3&quot;&gt;donation links&lt;/a&gt;, &lt;a href=&quot;https://github.com/kennethreitz/requests/pull/4779&quot;&gt;ads&lt;/a&gt;, &lt;a href=&quot;https://github.com/kennethreitz/requests/commit/63e7748fe502f44d112486d81a0da8cf38f36455&quot;&gt;intrusive sponsored links&lt;/a&gt;, etc. All of this money goes directly into his pocket, not the project's maintainers.&lt;/p&gt;
&lt;p&gt;I also learned that he has a history of selling premium support contracts for Requests, where he took the money and then delegated the actual work to unpaid volunteers.&lt;/p&gt;
&lt;p&gt;I don't have any objection to trying to make money from open-source. I've written before about how &lt;a href=&quot;https://vorpus.org/blog/the-unreasonable-effectiveness-of-investment-in-open-source-infrastructure/&quot;&gt;open-source doesn't get nearly enough investment&lt;/a&gt;. I do object to exploiting volunteers, driving out community members, and lying to funders and the broader community. Reitz has a consistent history of doing all these things.&lt;/p&gt;

&lt;p&gt;I've struggled to decide what to do here. Since last year, I've tried to be very cautious when speaking to people about this, because I don't want to start false rumors or feed an internet mob. (This has also meant keeping quiet about the work we've been doing on async in urllib3, and made it difficult for me to work on it at all.) And I'm scared of how making this public might affect my own reputation and mental health.&lt;/p&gt;
&lt;p&gt;Ultimately, I decided to speak out because I care deeply about the Python community and its members. If one of our community's most prominent members freely lies to donors and harms volunteers, and if we all let that go without saying anything, then that puts everything we've built together at risk. And I'm in a better position than many to speak up.&lt;/p&gt;
&lt;p&gt;So what happens now?&lt;/p&gt;
&lt;p&gt;Since this is the internet, I have to say explicitly: Please do not harass or abuse Reitz. That's never appropriate. (And in case you're the kind of person that doesn't find moral arguments convincing, then consider: he clearly wants attention.)&lt;/p&gt;
&lt;p&gt;I call on Reitz to make a public accounting of the money he raised and how it was spent.&lt;/p&gt;
&lt;p&gt;I urge the Requests project maintainers to transition their project to a more normal, less dysfunctional governance model. You can acknowledge his contributions without buying into his personal mythology. His insights are not irreplaceable. You know this situation is harming you and your users. You and your users are more important than his ego.&lt;/p&gt;
&lt;p&gt;Beyond that, I'm going to focus on my own work. I'm done keeping secrets to protect Reitz from the consequences of his actions; what happens next is up to him and the larger Python community.&lt;/p&gt;
&lt;p&gt;If anyone needs a listening ear, I can be reached at &lt;a href=&quot;mailto:njs@pobox.com&quot;&gt;njs@pobox.com&lt;/a&gt;. I'm also around at PyCon this weekend.&lt;/p&gt;
</description>
<pubDate>Sat, 04 May 2019 14:32:54 +0000</pubDate>
<dc:creator>f311a</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://vorpus.org/blog/why-im-not-collaborating-with-kenneth-reitz/</dc:identifier>
</item>
<item>
<title>JupyterHub 1.0</title>
<link>https://blog.jupyter.org/announcing-jupyterhub-1-0-8fff78acad7f</link>
<guid isPermaLink="true" >https://blog.jupyter.org/announcing-jupyterhub-1-0-8fff78acad7f</guid>
<description>&lt;div class=&quot;uiScale uiScale-ui--regular uiScale-caption--regular u-flexCenter u-marginVertical24 u-fontSize15 js-postMetaLockup&quot;&gt;
&lt;div class=&quot;u-flex0&quot;&gt;&lt;a class=&quot;link u-baseColor--link avatar&quot; href=&quot;https://blog.jupyter.org/@minrk?source=post_header_lockup&quot; data-action=&quot;show-user-card&quot; data-action-source=&quot;post_header_lockup&quot; data-action-value=&quot;8a83d4e47bb2&quot; data-action-type=&quot;hover&quot; data-user-id=&quot;8a83d4e47bb2&quot; data-collection-slug=&quot;jupyter-blog&quot; dir=&quot;auto&quot;&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/fit/c/100/100/0*-kBKbjvcakGnrEvN.&quot; class=&quot;avatar-image u-size50x50&quot; alt=&quot;Go to the profile of Min RK&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;u-flex1 u-paddingLeft15 u-overflowHidden&quot;&gt;
&lt;div class=&quot;u-paddingBottom3&quot;&gt;&lt;a class=&quot;ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker&quot; href=&quot;https://blog.jupyter.org/@minrk&quot; data-action=&quot;show-user-card&quot; data-action-value=&quot;8a83d4e47bb2&quot; data-action-type=&quot;hover&quot; data-user-id=&quot;8a83d4e47bb2&quot; data-collection-slug=&quot;jupyter-blog&quot; dir=&quot;auto&quot;&gt;Min RK&lt;/a&gt;&lt;span class=&quot;followState js-followState&quot; data-user-id=&quot;8a83d4e47bb2&quot;&gt;&lt;button class=&quot;button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide&quot; data-action=&quot;sign-up-prompt&quot; data-sign-in-action=&quot;toggle-block-user&quot; data-requires-token=&quot;true&quot; data-redirect=&quot;https://blog.jupyter.org/announcing-jupyterhub-1-0-8fff78acad7f&quot; data-action-source=&quot;post_header_lockup&quot;&gt;&lt;span class=&quot;followState js-followState&quot; data-user-id=&quot;8a83d4e47bb2&quot;&gt;&lt;span class=&quot;button-label button-defaultState&quot;&gt;Blocked&lt;/span&gt;&lt;span class=&quot;button-label button-hoverState&quot;&gt;Unblock&lt;/span&gt;&lt;/span&gt;&lt;button class=&quot;button button--primary button--smallest button--dark u-noUserSelect button--withChrome u-accentColor--buttonDark button--follow js-followButton u-marginLeft10 u-xs-hide&quot; data-action=&quot;sign-up-prompt&quot; data-sign-in-action=&quot;toggle-subscribe-user&quot; data-requires-token=&quot;true&quot; data-redirect=&quot;https://medium.com/_/subscribe/user/8a83d4e47bb2&quot; data-action-source=&quot;post_header_lockup-8a83d4e47bb2-------------------------follow_byline&quot;&gt;&lt;span class=&quot;button-label button-defaultState js-buttonLabel&quot;&gt;Follow&lt;/span&gt;&lt;span class=&quot;button-label button-activeState&quot;&gt;Following&lt;/span&gt;&lt;/button&gt;&lt;/button&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;time datetime=&quot;2019-05-04T03:58:34.043Z&quot;&gt;May 3&lt;/time&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p name=&quot;5fb4&quot; id=&quot;5fb4&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Today, we are pleased to announce the 1.0 release of JupyterHub. We’ve come a long way since our first release in March, 2015. There are loads of new features and improvements covered in the &lt;a href=&quot;https://jupyterhub.readthedocs.io/en/latest/changelog.html&quot; data-href=&quot;https://jupyterhub.readthedocs.io/en/latest/changelog.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;changelog&lt;/a&gt;, but we’ll cover a few of the highlights here.&lt;/p&gt;
&lt;p name=&quot;8caa&quot; id=&quot;8caa&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;You can upgrade jupyterhub with conda or pip:&lt;/p&gt;
&lt;p name=&quot;8cb9&quot; id=&quot;8cb9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;(Before upgrading, always make sure to backup your database!)&lt;/p&gt;
&lt;pre name=&quot;7dbd&quot; id=&quot;7dbd&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
conda install -c conda-forge jupyterhub==1.0.*&lt;br/&gt;pip install --upgrade jupyterhub==1.0.*
&lt;/pre&gt;
&lt;h3 name=&quot;c1fd&quot; id=&quot;c1fd&quot; class=&quot;graf graf--h3 graf-after--pre&quot;&gt;UI for named servers&lt;/h3&gt;
&lt;p name=&quot;6374&quot; id=&quot;6374&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Named servers are a concept in JupyterHub that allows each user to have more than one server (e.g. a ‘gpu’ server with access to gpus, or a ‘cs284’ server with the necessary resources for a given class). JupyterHub 1.0 introduces UI for managing these servers, so users can create/start/stop/delete their servers from the hub home page:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*VozbyUHN9qTICXzQSKQvfw.png&quot; data-width=&quot;1482&quot; data-height=&quot;712&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*VozbyUHN9qTICXzQSKQvfw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*VozbyUHN9qTICXzQSKQvfw.png&quot;/&gt;&lt;/div&gt;
UI for managing named servers
&lt;h3 name=&quot;2e25&quot; id=&quot;2e25&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Improved pending spawn handling and less implicit spawn behavior&lt;/h3&gt;
&lt;p name=&quot;0139&quot; id=&quot;0139&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;When users launch their server, they will be faced with a progress bar showing the progress. Spawners can emit custom messages to indicate the stages of launch, which are especially useful when it can take a while.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*e5FwpbvGl7SFbl9vGevBwA.png&quot; data-width=&quot;1997&quot; data-height=&quot;782&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*e5FwpbvGl7SFbl9vGevBwA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*e5FwpbvGl7SFbl9vGevBwA.png&quot;/&gt;&lt;/div&gt;
a spawn taking a bit
&lt;h3 name=&quot;6618&quot; id=&quot;6618&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Support for total internal encryption with SSL&lt;/h3&gt;
&lt;p name=&quot;7dd2&quot; id=&quot;7dd2&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;If your JupyterHub is publicly accessible, we hope you are using HTTPS (it’s never been easier, thanks to &lt;a href=&quot;https://letsencrypt.org&quot; data-href=&quot;https://letsencrypt.org&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;letsencrypt&lt;/a&gt;)! However, JupyterHub exclusively used HTTP for internal communication. This is usually fine, especially for single-machines, but for deployments on distributed or shared infrastructure it’s a good idea to encrypt communication between your components. With 1.0, you can &lt;a href=&quot;https://jupyterhub.readthedocs.io/en/latest/reference/websecurity.html#encrypt-internal-connections-with-ssl-tls&quot; data-href=&quot;https://jupyterhub.readthedocs.io/en/latest/reference/websecurity.html#encrypt-internal-connections-with-ssl-tls&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;enable SSL encryption&lt;/a&gt; and authentication of all internal communication as well. Spawners must support this feature by defining &lt;a href=&quot;https://jupyterhub.readthedocs.io/en/latest/api/spawner.html?highlight=move_certs#jupyterhub.spawner.Spawner.move_certs&quot; data-href=&quot;https://jupyterhub.readthedocs.io/en/latest/api/spawner.html?highlight=move_certs#jupyterhub.spawner.Spawner.move_certs&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Spawner.move_certs&lt;/a&gt;. Currently, local spawners and DockerSpawner support internal ssl.&lt;/p&gt;
&lt;h3 name=&quot;9ec0&quot; id=&quot;9ec0&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Support for checking and refreshing authentication&lt;/h3&gt;
&lt;p name=&quot;8c04&quot; id=&quot;8c04&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;JupyterHub authentication is most often managed by an external authority, e.g. GitHub OAuth. &lt;a href=&quot;https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html#authentication-state&quot; data-href=&quot;https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html#authentication-state&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Auth state&lt;/a&gt; can be used to persist credentials, such as enabling push access to repositories or access to resources. These credentials can sometimes expire or need refreshing. Until now, expiring or refreshing authentication was not well supported by JupyterHub. 1.0 introduces new configuration to refresh or expire authentication info:&lt;/p&gt;
&lt;p name=&quot;3bf0&quot; id=&quot;3bf0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;code class=&quot;markup--code markup--p-code&quot;&gt;c.Authenticator.auth_refresh_age&lt;/code&gt; allows authentication to expire after a number of seconds.&lt;/p&gt;
&lt;p name=&quot;b0b9&quot; id=&quot;b0b9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;code class=&quot;markup--code markup--p-code&quot;&gt;c.Authenticator.refresh_pre_spawn&lt;/code&gt; forces a refresh of authentication prior to spawning a server, effectively requiring a user to have up-to-date authentication when they start their server, which can be important when the user environment should have credentials with access to external resources.&lt;/p&gt;
&lt;p name=&quot;3aae&quot; id=&quot;3aae&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;code class=&quot;markup--code markup--p-code&quot;&gt;Authenticator.refresh_auth&lt;/code&gt; defines what it means to refresh authentication (default is nothing, but can check with external authorities to re-load info such as tokens or group membership) and can be customized by Authenticator implementations.&lt;/p&gt;
&lt;h3 name=&quot;ece1&quot; id=&quot;ece1&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Thanks!&lt;/h3&gt;
&lt;p name=&quot;f45b&quot; id=&quot;f45b&quot; class=&quot;graf graf--p graf-after--h3 graf--trailing&quot;&gt;Huge thanks to the many people who have contributed to this release, whether it was through discussion, testing, documentation, or development.&lt;/p&gt;
</description>
<pubDate>Sat, 04 May 2019 13:04:43 +0000</pubDate>
<dc:creator>jonbaer</dc:creator>
<og:title>Announcing JupyterHub 1.0</og:title>
<og:url>https://blog.jupyter.org/announcing-jupyterhub-1-0-8fff78acad7f</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*VozbyUHN9qTICXzQSKQvfw.png</og:image>
<og:description>Today, we are pleased to announce the 1.0 release of JupyterHub. We’ve come a long way since our first release in March, 2015. There are…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.jupyter.org/announcing-jupyterhub-1-0-8fff78acad7f?gi=f05ec0642b38</dc:identifier>
</item>
</channel>
</rss>