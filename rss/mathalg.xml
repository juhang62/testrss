<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>感动 ∣她养我长大，我陪她到老！90后女孩带痴呆症养母读研</title>
<link>http://www.jintiankansha.me/t/i6VQh6PG9i</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/i6VQh6PG9i</guid>
<description>&lt;p&gt;&lt;span&gt;2018年5月30日，西北工业大学长安校区，1991年出生的孙玉晴和母亲正在租住的房子里聊天。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;在此之前，她刚刚完成了一场令她担惊受怕却又习以为常的寻人行动&lt;/span&gt;&lt;span&gt;——母亲又走丢了，这是她不得不面对的日常“小状况”。她最担心的就是母亲喜欢出去逛，却总回不了家。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;4&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;　　&lt;/span&gt;&lt;span&gt;享受旅行、肆意追梦、甜蜜恋爱，这些同龄人的正常生活距离孙玉晴都太过遥远。自高三那年养父过世，摆在孙玉晴面前的只有身兼数职“吃饱饭”，攒钱给养母看病，不负养父嘱托完成学业。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;355&quot; data-backw=&quot;558&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1rzPB0icgXyaV9Ngey4k3ENsMXWq6Wk01ueeYiahicib89sVC8dJWnK41sg/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6354430379746835&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1rzPB0icgXyaV9Ngey4k3ENsMXWq6Wk01ueeYiahicib89sVC8dJWnK41sg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;790&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;孙玉晴勇敢、坚定，8年来她自强奋进，不仅完成了从高职到本科再到研究生的课业，还带着养母畅游北京。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;生活窘迫稍有缓解时，年近八旬的养母患上了老年痴呆症，为照顾养母，她只能把自己的活动范围限定在校内。尽管还有很多挑战，孙玉晴却坚定如初：“养母陪我长大，我陪她到老。没有她，就没有今天的我。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;400&quot; data-backw=&quot;600&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIFfnicXTxWzazLlIibVXE5fqM4vM3YATdYbXictLJ18CYUibaSSZj4moWZQ/640?&quot; data-ratio=&quot;0.6666666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIFfnicXTxWzazLlIibVXE5fqM4vM3YATdYbXictLJ18CYUibaSSZj4moWZQ/640?&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;“中国大学生自强之星”孙玉晴与养母。本人供图&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;一条用兼职搭建的&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;升本、考研路&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;section mpa-is-content=&quot;t&quot;&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;养父母捡到孙玉晴时已是65岁和51岁高龄，虽是养女，他们却视如己出，邻居们说“能养活就不错了”，他们却靠捡卖废品全力支持女儿上学。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;5岁起，孙玉晴放学的第一件事就是拿好袋子跟养母一起去捡拾废品，和养父一起卖废品，给养父母做饭、洗脚、洗衣服、捶背都是她最爱做的事。&lt;/span&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  虽然日子过得紧巴，但那却是孙玉晴心底最温暖的回忆。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;饱受高血压和风湿煎熬的年迈养父，最终还是没能等到女儿考上大学，在孙玉晴高三那年离世。&lt;/span&gt;&lt;span&gt;“那时候特别自责，一想到我妈一个人孤零零在医院照顾我爸，就痛恨自己什么也做不了。”&lt;/span&gt;&lt;span&gt;忆起往事，孙玉晴泪水哗然。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;至亲离世，还没来得及处理好悲伤的情绪，孙玉晴便不得不面对日益逼近的高考。遗憾的是，那一年她只考上了高职。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;432&quot; data-backw=&quot;540&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIN59fqLwkbFGe5nAqqEVB5ficV3sCD8bBxyOqGzZvumTaIRH8wt1iaGEA/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66640625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIN59fqLwkbFGe5nAqqEVB5ficV3sCD8bBxyOqGzZvumTaIRH8wt1iaGEA/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;“我一定要上大学！给我爸妈争气。”&lt;/span&gt;&lt;span&gt;踏入学校的第一天，孙玉晴就下了这样的决心。然而彼时，孙玉晴面临的更迫切的问题是“生活费”。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;去食堂打工，是孙玉晴初入学校时唯一能找到的兼职。卖饭、洗碗、擦餐桌、倒剩饭，早中晚三个时段的忙碌，换来80块的月薪，&lt;/span&gt;&lt;span&gt;“很多同学不明白我为什么要去干这个，但这可以省去一日三餐的生活费”，&lt;/span&gt;&lt;span&gt;孙玉晴说这份兼职她干了整整一年。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;做销售、翻译、发传单、一天三个家教……在同学眼中，孙玉晴是一个不会玩的“怪人”，因为除去兼职时间，她几乎都在图书馆度过。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;“我舍不得睡觉，从来没午休过，专升本只有一次机会，我必须确保万无一失。”&lt;/span&gt;&lt;span&gt;提起大学的日子，孙玉晴再次泪目，尽管她已说不清为什么当时会有那样的勇气，但对那份在她心中“必须承担的责任”记忆深刻。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;功夫不负有心人，孙玉晴不仅成功考取了本科，还在2016年考取了西北工业大学的研究生。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;407&quot; data-backw=&quot;556&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x16O7Bf5iaq3CwRm3qeGBkEr4xqVyrJTPl4OeqESRml8Qjia0YnC7VT5Fw/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6120906801007556&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x16O7Bf5iaq3CwRm3qeGBkEr4xqVyrJTPl4OeqESRml8Qjia0YnC7VT5Fw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;794&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;攒钱圆了养母北京旅行梦&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;除了克服学习、生活上的压力，养母每况愈下的身体状况是孙玉晴最忧心的事。&lt;/span&gt;&lt;span&gt;大学那几年，养母每年都要住院三四次，孙玉晴每半月都要从学校回家一趟，火车票放了满满一抽屉。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;子欲养而亲不待，养父离世是孙玉晴心中永远的痛。为了在还来得及的时候报恩尽孝，孙玉晴决定奢侈一次，帮养母完成“到北京看看”的心愿。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;370&quot; data-backw=&quot;556&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI5sTabyOgAwW67icxD8icFYBPVyjyB5T2hreUxeleib4RDn4LYXXs5knYw/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI5sTabyOgAwW67icxD8icFYBPVyjyB5T2hreUxeleib4RDn4LYXXs5knYw/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如今患上了老年痴呆症的养母，头脑不清醒，总跟孙玉晴闹脾气，还不时走丢。但她却也时不时无意提起“我去过北京”“坐过飞机”的事，这让孙玉晴在疲惫中多了一丝安慰。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;“感谢所有帮助过我的人”&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;一路走来，外界的每一份关怀于她而言都弥足珍贵。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;收到西北工业大学研究生录取通知书的当天，孙玉晴一边是欣喜，一边是忐忑。&lt;/span&gt;&lt;span&gt;时年76岁的养母身体越来越差，她怎能把养母独自留在湖北老家？思考再三，孙玉晴拨通了辅导员的电话。素未谋面，辅导员却在了解了情况后，立即伸以援手，不仅帮她找房子，还帮她申请一系列补助，解决经济上的困难。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;542&quot; data-backw=&quot;400&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI8MXadOWhDSv9qtRnKSibdSF0uctQ34ppw2P12xL0SNtPBoeKUxlToiaQ/640?&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/azXQmS1HA7kibSETb6V7f2dPARZ1HvFc3k5GNc4GmMq3abgicp0QxoPkMBFJica9n5xL8Mm15xlCtfLp2QMjOIRPg/0?wx_fmt=jpeg&quot; data-ratio=&quot;1.355&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/azXQmS1HA7kibSETb6V7f2dPARZ1HvFc3k5GNc4GmMq3abgicp0QxoPkMBFJica9n5xL8Mm15xlCtfLp2QMjOIRPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;孙玉晴本科毕业时与养母合影。本人供图&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;学校的关怀让孙玉晴至今感动不已，她说：“这是当时我面临的最大的困难，在陕西我人生地不熟，都是学校帮着出力。所以我发誓一定要好好学习。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;读硕士期间，孙玉晴一边细心照顾养母，一边砥砺前行。&lt;/span&gt;&lt;span&gt;在学习科研上，她赴新加坡、印度尼西亚参加外语类顶级国际会议并作口头汇报；以第一作者的身份在核心期刊等发表多篇论文并获得了多项奖学金；被评选为第十二届　“中国大学生年度人物”；在由共青团中央、全国学联主办的“青春自强·励志华章”主题活动中，获得2016年“中国大学生自强之星”称号。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;394&quot; data-backw=&quot;540&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1bDTeMWgxIjicqQdAL31gW9HmxiakJbLn3myQDcLIT7PpqT39gh0pP26w/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6068702290076335&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1bDTeMWgxIjicqQdAL31gW9HmxiakJbLn3myQDcLIT7PpqT39gh0pP26w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;786&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;“一路上我需要感谢的人太多了，我觉得自己所有的努力就是为了回馈那些帮助过我的人。”时序更迭，孙玉晴仍记得高中时期那些关于“一件衣服”“一顿饭”的关切，“可能老师觉得微不足道，但对当时的我来说是莫大的安慰”。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;梦想当教师回馈社会&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;2018年，孙玉晴27岁了，生活的磨砺让她考虑事情比同龄人更加长远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;孙玉晴对从业方向有着清晰的规划。养父曾是小学老师，成长过程中蒙受师恩，让孙玉晴对教师这个职业怀有特殊的情结。她说：&lt;/span&gt;&lt;span&gt;“我想当老师，想去关心那些有困难的学生，想让那些遇到困难的学生能从我身上学到一些东西。”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;319&quot; data-backw=&quot;542&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1eeoI4muhnedGcviclQTVWlD248ZO8NDicXDefzNfDDCI2ep61QsYMK0A/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5891276864728192&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1eeoI4muhnedGcviclQTVWlD248ZO8NDicXDefzNfDDCI2ep61QsYMK0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;791&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在实现职业理想之前，孙玉晴还有一个读博的愿望，一些了解她艰辛生活的同学对此并不能理解——“为什么不去工作？”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;孙玉晴坦言，她对学术有着一份特殊的热爱，此外在她看来，只有留在学校，才有更多时间守着母亲。&lt;/span&gt;&lt;span&gt;“我知道未来还有更多的困难，但那么难的日子都过来了，车到山前必有路，所以我会一直坚持、努力，去实现自己的梦想。”&lt;/span&gt;&lt;span&gt;说这些话时，孙玉晴笑了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;361&quot; data-backw=&quot;542&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIEGEfNH46I1zEEWax56uLHLO2lxSF36ma6EpFseXbvXLdeLKaN21l0Q/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIEGEfNH46I1zEEWax56uLHLO2lxSF36ma6EpFseXbvXLdeLKaN21l0Q/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;养母陪我长大，我要陪她到老&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;有孝心的人，最美！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 中国青年网&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 04 Jul 2018 01:46:30 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/i6VQh6PG9i</dc:identifier>
</item>
<item>
<title>想转行人工智能？机会来了！！！</title>
<link>http://www.jintiankansha.me/t/xVIuJSuWzF</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/xVIuJSuWzF</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;一个坏消息：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2018年1月 教育部印发的《普通高中课程方案和语文等学科课程标准》新加入了数据结构、人工智能、开源硬件设计等 AI 相关的课程。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这意味着职场新人和准备找工作的同学们，为了在今后十年内不被淘汰，你们要补课了，从初中开始。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;一个好消息：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;人工智能尖端人才远远不能满足需求。行业风口的人工智能，在中国人才缺口将超过500万人，而中国人工智能人才数量目前只有5万（数据来自工信部教育考试中心）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;并且目前岗位溢价相当严重，2017年人工智能在互联网岗位薪酬中位列第三，月薪20.1k，如果按照普遍的16月薪酬计算，那么人工智能在2017年一年的薪酬就是2.01*16=32.16万。那么再来看一组2018的薪酬数据： &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6058890147225368&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX8941l91Oibjp6yWFsDogPKGMXoqiacL7UgDtUjVyeaU9PUu8xIvsFFQSLNDgv9diagdjrTHbicc2yETXag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;883&quot; width=&quot;auto&quot; /&gt; &lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;所以如果你对自己的专业/工作不满意，现在正是进入人工智能领域学习就业/转业的最佳时机。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在面对众多的数学知识和编程知识里，自学会让大家耗费大量的时间金钱。因此，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;中国科学院计算技术研究所人工智能博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;开发推出了人工智能&lt;/span&gt;&lt;span&gt;&lt;strong&gt;机器学习365天特训营&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;课程。（扫描最底部二维码联系助教）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了保证大家的&lt;/span&gt;&lt;span&gt;&lt;strong&gt;学习效果&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span&gt;&lt;strong&gt;就业情况&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;幂次学院提供4项课程服务，&lt;/strong&gt;&lt;strong&gt;从发展历程、概念、基本名词、术语、评估方法讲起，到算法模型与实战演练：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1、名校大牛讲师授课：&lt;/strong&gt;&lt;strong&gt;中国科学院计算技术研究所人工智能博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;开发课程，名校大牛授课；&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;2、3&lt;/strong&gt;&lt;strong&gt;65天的系统学习&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;跟老师直播学习，&lt;strong&gt;&lt;span&gt;直播回放4年内随时随地回看&lt;/span&gt;&lt;/strong&gt;，在线答疑，并完成课后作业；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;3、优质的售后答疑：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;全天24小时课程问答与社群交流服务，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;让你的每一个问题都能够得到解答，课程资料随时下载；&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;4、颁发培训结业证书：通过幂&lt;/span&gt;&lt;/span&gt;&lt;span&gt;次学院的阶段测试和毕业测试，并颁发幂次学院人工智能培训结业证书。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;合计&lt;/strong&gt;365+天，每周两次直播&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;365天100+小时（理论+实战）课程&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（讲师直播答疑，课程7*24小时问答服务，学院社群7*24小时交流，课程资料随时下载）&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;购买课程另赠送2门辅助课程：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 机器学习之Python编程基础与数据分析 课程，课程内容由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;清华大学python大牛&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;与&lt;/span&gt;&lt;span&gt;&lt;strong&gt;美国普渡大学算法工程师&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：python基础，python数据分析，python机器学习基础与python在机器学习中的实践案例（详细课程大纲见幂次学院主页）。&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 人工智能数学基础8天集训营 课程，由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;中国科学院计算技术研究所博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：矩阵论基础，概率与信息论，数值计算三部分&lt;span&gt;（详细课程大纲见幂次学院主页）。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5102803738317757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX894Kar3aI8C9PoBa0jB21ev4ZywEDJLDznI2UicZ4fjWUAlabF9yUUJDQUHdNRPCMtAIth8d2aFpHAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1070&quot; width=&quot;668px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;助力您解决人工智能学习中所需要用到的数学知识、Python编程知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;立即开始体系化学习，所有知识一步到位！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;直播 + 回放：合计365+天，每周六19：00-21：00开课，直播回放随时随地回看。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;讲师团队介绍&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（更多讲师详情请关注幂次学院主页：https://mici.jiqishidai.com)&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;张老师，中国科学院计算技术研究所机器学习方向博士&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;专注于人机交互、机器学习等领域研究。曾在国内外知名会议期刊发表多篇论文，并荣获人工智能领域会议“最佳论文提名奖”，目前拥有国家发明专利2项、软件著作权1项。拥有机器学习、数据挖掘领域实战经验，曾参与项目：&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、面向病症的多模态在线预警方法研究—国家自然科学基金;&lt;br /&gt;2、基于人机交互技术的安全驾驶映射系统—国家国际科技合作专项;&lt;br /&gt;3、散发性病症风险基因图谱与预警评估方法研究—北京市科学技术委员会北京脑科学研究项目;&lt;br /&gt;4、广东省大数据科学中心项目“基于多模态大数据的复杂疾病临床诊断标准及应用”—广东省科技计划项目NSFC等国家级项目。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;李金老师，清华大学机器学习方向本硕双清华毕业生，阿里巴巴机器学习方向算法工程师；&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;研究方向为：推荐系统，计算机视觉，自然语言处理，深度学习等，在TNNLS，PR等杂志上发表过多篇论文，著有《自学Python—编程基础科学计算及数据分析》一书，P&lt;/span&gt;&lt;span&gt;ython笔记3K+Star，知乎python及机器学习板块12K+ zan，幂次学院签约讲师。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;赵朗老师，美国普渡大学硕士毕业生，机器学习工程师/算法工程师，曾参与研究：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  1.参与克莱斯勒公司“合金发展”项目，应用机器学习分析产品合格率与合金成分等因素之间的关系；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.参与浙江大学关于研究材料渗透率的项目，应用机器学习分析材料渗透率与材料结构之间的关系；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.应用机器学习研究各大风场的风机故障问题，在机器学习，数据挖掘等方面有丰富的实战经验，善于用简单的例子来描述复杂的机器学习概念，善于对学生进行启发，帮助学生掌握机器学习的核心概念与算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;附：机器学习365天特训营 - 课程大纲：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第一部分 基础篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第1章&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.1 引言&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.2 基本术语&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.3 假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.4 归纳偏好&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.5 发展历程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.6 应用现状&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第2章 模型评估与选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.1 经验误差与过拟合&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2 评估方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.1 留出法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.2 交叉验证法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.3 自助法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.4 调参与最终模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.1 错误率与精度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.2 查准率、查全率与F1&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.3 ROC与AUC&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.4 代价敏感错误率与代价曲线&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4 比较检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.1 假设检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.2 交叉验证t检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.3 McNemar检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.4 Friedman检验与后续检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.5 偏差与方差&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第3章 线性模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.1 基本形式&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;3.2 线性回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.3 对数几率回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.4 线性判别分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.5 多分类学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.6 类别不平衡问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第4章 决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.1 基本流程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2 划分选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.1 信息增益&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.2 增益率&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.3 基尼指数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3 剪枝处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.1 预剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.2 后剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4 连续与缺失值&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.1 连续值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.2 缺失值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.5 多变量决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第5章 神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.1 神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.2 感知机与多层网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.3 误差逆传播算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.4 全局最小与局部极小&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5 其他常见神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.1 RBF网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.2 ART网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.3 SOM网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.4 级联相关网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.5 Elman网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.6 Boltzmann机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.6 深度学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第6章 支持向量机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.1 间隔与支持向量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.2 对偶问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.3 核函数&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;6.4 软间隔与正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.5 支持向量回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.6 核方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第7章 贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.1 贝叶斯决策论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.2 极大似然估计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.3 朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.4 半朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5 贝叶斯网&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.1 结构&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.2 学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.3 推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.6 EM算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第8章 集成学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.1 个体与集成&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.2 Boosting&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3 Bagging与随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.1 Bagging&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.2 随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4 结合策略&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.1 平均法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.2 投票法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.3 学习法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5 多样性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.1 误差--分歧分解&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.2 多样性度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.3 多样性增强&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第9章 聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.1 聚类任务&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.2 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.3 距离计算&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4 原型聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.1 k均值算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.2 学习向量量化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  9.4.3 高斯混合聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.5 密度聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.6 层次聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第10章 降维与度量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.1 k近邻学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.2 低维嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.3 主成分分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.4 核化线性降维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5 流形学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.1 等度量映射&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.2 局部线性嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.6 度量学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第二部分 进阶篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第11章 特征选择与稀疏学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.1 子集搜索与评价&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.2 过滤式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.3 包裹式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.4 嵌入式选择与L_1正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.5 稀疏表示与字典学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.6 压缩感知&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第12章 计算学习理论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.1 基础知识&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.2 PAC学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3 有限假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.1 可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.2 不可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.4 VC维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.5 Rademacher复杂度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.6 稳定性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第13章 半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.1 未标记样本&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.2 生成式方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.3 半监督SVM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.4 图半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;13.5 基于分歧的方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.6 半监督聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第14章 概率图模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.1 隐马尔可夫模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.2 马尔可夫随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.3 条件随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4 学习与推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.1 变量消去&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.2 信念传播&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5 近似推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.1 MCMC采样&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.2 变分推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.6 话题模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第15章 规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.1 基本概念&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.2 序贯覆盖&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.3 剪枝优化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.4 一阶规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5 归纳逻辑程序设计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.1 最小一般泛化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.2 逆归结&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第16章 强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.1 任务与奖赏&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2 K-摇臂赌博机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.1 探索与利用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.2 ε-贪心&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.3 Softmax&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3 有模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.1 策略评估&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.2 策略改进&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.3 策略迭代与值迭代&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4 免模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.1 蒙特卡罗强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.2 时序差分学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.5 值函数近似&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;16.6 模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.1 直接模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.2 逆强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第17章 增量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1 被动攻击学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.1 梯度下降量的抑制&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.2 被动攻击分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.3 被动攻击回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2 适应正则化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.1 参数分布的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.2 适应正则化分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.3 适应正则化回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.3 增量随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第18章 迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1 迁移学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.1 什么是迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.2 迁移学习VS传统机器学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.3 应用领域&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2 迁移学习的分类方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 按迁移情境&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.2 按特征空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 按迁移方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.3 代表性研究成果&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 域适配问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.2 多源迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 深度迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第19章 主动学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.1 主动学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.2 主动学习思想&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.3 主动学习VS半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.4 主动学习VS Self-Learning&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第20章 多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.1 使用最小二乘回归的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.2 使用最小二乘概率分类器的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;20.3 多次维输出函数的学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第三部分 实战篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第21章 机器学习应用场景介绍&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.1 机器学习经典应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.2 头脑风暴：挖掘身边的应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第22章 数据预处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.1 数据降噪&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.2 数据分割&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第23章 特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.1 时域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.2 频域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.3 自动特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第24章 机器学习方法应用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.1 应用机器学习方法之前的处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.2 使用机器学习分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.3 机器学习调参&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.4 分类结果展示&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;报名费用及优惠详情：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;原价16800元：&lt;/span&gt;&lt;span&gt;&lt;strong&gt;折后特惠价：2999元。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX896cbVeGItvDKsPGhluU4vhO5vfooqXr8Ik9XssHL935SFKmmic8xyE9nqRNY25icibEnnWfsbQjicZ56A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;210px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;长按二维码咨询助教微信&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/gDmjibFWrFyPmsrETMZjrlkZibLNK8T98q6lWfibkWDwVu5VuMFgEB8nGcWkntxuOedFmmoLPm07wafujgxWYgmtA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;500&quot; width=&quot;252px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  长按二维码进入报名页面&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 04 Jul 2018 01:46:29 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/xVIuJSuWzF</dc:identifier>
</item>
<item>
<title>如何选择合适的损失函数，请看......</title>
<link>http://www.jintiankansha.me/t/cD8l43kowg</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/cD8l43kowg</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6669106881405563&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FqujqaY0DKkzblVl4UkDWLEy3wT25iaialRCRANzQ15cqicxpuXBibnOPibzSA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;机器学习中的所有算法都依赖于最小化或最大化某一个函数，我们称之为“&lt;strong&gt;目标函数&lt;/strong&gt;”。最小化的这组函数被称为“&lt;strong&gt;损失函数&lt;/strong&gt;”。&lt;strong&gt;损失函数是衡量预测模型预测期望结果表现的指标&lt;/strong&gt;。寻找函数最小值的最常用方法是“&lt;strong&gt;梯度下降&lt;/strong&gt;”。把损失函数想象成起伏的山脉，梯度下降就像从山顶滑下，目的是到达山脉的最低点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;没有一个损失函数可以适用于所有类型的数据&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。损失函数的选择取决于许多因素，包括是否有离群点，机器学习算法的选择，运行梯度下降的时间效率，是否易于找到函数的导数，以及预测结果的置信度。这个博客的目的是&lt;strong&gt;帮助你了解不同的损失函数&lt;/strong&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;损失函数可以大致分为两类：&lt;strong&gt;分类损失&lt;/strong&gt;（Classification Loss）和&lt;strong&gt;回归损失&lt;/strong&gt;（Regression Loss）。下面这篇博文，就将重点介绍5种回归损失。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;1.376543209876543&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquB8tibeKa8RuhOZvUxj92HARhJVCcgrXoicd8nYKCD8xUzIZB0KibfxHibA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;486&quot; data-width=&quot;364px&quot; height=&quot;auto&quot; width=&quot;364px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;回归函数预测实数值，分类函数预测标签&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;span&gt;▌&lt;/span&gt;&lt;span&gt;&lt;strong&gt;回归损失&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1、均方误差，二次损失，L2损失（Mean Square Error, Quadratic Loss, L2 Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;均方误差（MSE）是最常用的回归损失函数。MSE是目标变量与预测值之间距离平方之和。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.396078431372549&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquVNzmvkzX8KNw7ZvbM3OnGGXbPiasp3ZXXvT58B6wXcnibDNAKMrKchqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;255&quot; data-width=&quot;191px&quot; height=&quot;auto&quot; width=&quot;191px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面是一个MSE函数的图，其中真实目标值为100，预测值在-10,000至10,000之间。预测值（X轴）= 100时，MSE损失（Y轴）达到其最小值。损失范围为0至∞。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquiaF1WHnpwIvsO0VyibxoHcEibMiasqCRIlR7gKAXqic53jEI3dCxrMrFnHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;576&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;MSE损失（Y轴）与预测值（X轴）关系图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、平均绝对误差，L1损失（Mean Absolute Error, L1 Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标变量和预测变量之间差异绝对值之和。因此，它在一组预测中衡量误差的平均大小，而不考虑误差的方向。（如果我们也考虑方向，那将被称为平均偏差（Mean Bias Error, MBE），它是残差或误差之和）。损失范围也是0到∞。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.36964980544747084&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquxoU6zTBkGUiaNBpUjMRffsgE06XEullzicNgeqAytM4cTPiblqewGRGfw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;257&quot; data-width=&quot;192px&quot; height=&quot;auto&quot; width=&quot;192px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquI1BBUkaFWFy47B3OAYGk9c2OHaM5UrsM6AjM3eSmXQc9D9GUXOEG1g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;576&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;MAE损失（Y轴）与预测值（X轴）关系图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3、MSE vs MAE （L2损失 vs L1损失）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;简而言之， 使用平方误差更容易求解，但使用绝对误差对离群点更加鲁棒。但是，知其然更要知其所以然！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每当我们训练机器学习模型时，我们的目标就是找到最小化损失函数的点。当然，当预测值正好等于真实值时，这两个损失函数都达到最小值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面让我们快速过一遍两个损失函数的Python代码。我们可以编写自己的函数或使用sklearn的内置度量函数：&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; readability=&quot;7&quot;&gt;&lt;pre&gt;
&lt;br/&gt;＃&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;：真正的目标变量数组&lt;br/&gt;＃pred：预测数组&lt;br/&gt;**&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;, pred)&lt;/span&gt;&lt;/span&gt;:&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(((&lt;span class=&quot;&quot;&gt;true&lt;/span&gt; – pred)**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;))&lt;br/&gt;**&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt;&lt;/span&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;
&lt;span class=&quot;&quot;&gt;mae&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;, pred)&lt;/span&gt;:&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(np.abs(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt; – pred))&lt;br/&gt;**&lt;br/&gt;＃也可以在sklearn中使用&lt;br/&gt;**&lt;br/&gt;from sklearn.metrics import mean_squared_error&lt;br/&gt;from sklearn.metrics import mean_absolute_error&lt;br/&gt;&lt;/pre&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;让我们来看看两个例子的MAE值和RMSE值（RMSE，Root Mean Square Error，均方根误差，它只是MSE的平方根，使其与MAE的数值范围相同）。在第一个例子中，预测值接近真实值，观测值之间误差的方差较小。第二个例子中，有一个异常观测值，误差很高。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.31107954545454547&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjxVJ8ickgPg8afDRIjCmDNT7iaJcGNIuGjib3EUaJOS1jibyxAz09RqugzNwibFDZf9icVYEGibnRMLsfG1A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;704&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;左：误差彼此接近  右：有一个误差和其他误差相差很远&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们从中观察到什么？我们该如何选择使用哪种损失函数？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由于MSE对误差（e）进行平方操作（y - y_predicted = e），如果e&amp;gt; 1，误差的值会增加很多。如果我们的数据中有一个离群点，e的值将会很高，将会远远大于|e|。这将使得和以MAE为损失的模型相比，以MSE为损失的模型会赋予更高的权重给离群点。在上面的第二个例子中，以RMSE为损失的模型将被调整以最小化这个离群数据点，但是却是以牺牲其他正常数据点的预测效果为代价，这最终会降低模型的整体性能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;MAE损失适用于训练数据被离群点损坏的时候（即，在训练数据而非测试数据中，我们错误地获得了不切实际的过大正值或负值）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;直观来说，我们可以像这样考虑：对所有的观测数据，如果我们只给一个预测结果来最小化MSE，那么该预测值应该是所有目标值的均值。但是如果我们试图最小化MAE，那么这个预测就是所有目标值的中位数。我们知道中位数对于离群点比平均值更鲁棒，这使得MAE比MSE更加鲁棒。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;使用MAE损失（特别是对于神经网络）的一个大问题是它的梯度始终是相同的，这意味着即使对于小的损失值，其梯度也是大的。这对模型的学习可不好。为了解决这个问题，我们可以使用随着接近最小值而减小的动态学习率。MSE在这种情况下的表现很好，即使采用固定的学习率也会收敛。MSE损失的梯度在损失值较高时会比较大，随着损失接近0时而下降，从而使其在训练结束时更加精确（参见下图）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.31&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquYuZpIqhpB3XxGtmQgSgJOdy0VB0dX54BiaB0bLicbxxsIBibDLQOpCKzA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;决定使用哪种损失函数？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果离群点是会影响业务、而且是应该被检测到的异常值，那么我们应该使用MSE。另一方面，如果我们认为离群点仅仅代表数据损坏，那么我们应该选择MAE作为损失。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  我建议阅读下面这篇文章，其中有一项很好的研究，比较了在存在和不存在离群点的情况下使用L1损失和L2损失的回归模型的性能。请记住，L1和L2损失分别是MAE和MSE的另一个名称而已。&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;地址：&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;L1损失对异常值更加稳健，但其导数并不连续，因此求解效率很低。L2损失对异常值敏感，但给出了更稳定的闭式解（closed form solution）（通过将其导数设置为0）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;两种损失函数的问题：可能会出现这样的情况，即任何一种损失函数都不能给出理想的预测。例如，如果我们数据中90％的观测数据的真实目标值是150，其余10％的真实目标值在0-30之间。那么，一个以MAE为损失的模型可能对所有观测数据都预测为150，而忽略10％的离群情况，因为它会尝试去接近中值。同样地，以MSE为损失的模型会给出许多范围在0到30的预测，因为它被离群点弄糊涂了。这两种结果在许多业务中都是不可取的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这种情况下怎么做？一个简单的解决办法是转换目标变量。另一种方法是尝试不同的损失函数。这是我们的第三个损失函数——Huber Loss——被提出的动机。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3、Huber Loss，平滑的平均绝对误差&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Huber Loss对数据离群点的敏感度低于平方误差损失。它在0处也可导。基本上它是绝对误差，当误差很小时，误差是二次形式的。误差何时需要变成二次形式取决于一个超参数，(delta)，该超参数可以进行微调。当  𝛿 ~ 0时， Huber Loss接近MAE，当  𝛿 ~ ∞（很大的数）时，Huber Loss接近MSE。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.1580952380952381&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquAVbCpzRJ14mgyMRF38PKd8XVPlNBCrFIiaECMhuJu2KJIO6xV5YMUPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;525&quot; data-width=&quot;393px&quot; height=&quot;auto&quot; width=&quot;393px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.7142857142857143&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquKyy9J1HF1VJ4Yuwe36k3y0jntFmsF37VNKqsc9JVw9Lw8x8hscNAZA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; data-width=&quot;378px&quot; height=&quot;auto&quot; width=&quot;378px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Huber Loss（Y轴）与预测值（X轴）关系图。真值= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;delta的选择非常重要，因为它决定了你认为什么数据是离群点。大于delta的残差用L1最小化（对较大的离群点较不敏感），而小于delta的残差则可以“很合适地”用L2最小化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;为什么使用Huber Loss？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;使用MAE训练神经网络的一个大问题是经常会遇到很大的梯度，使用梯度下降时可能导致训练结束时错过最小值。对于MSE，梯度会随着损失接近最小值而降低，从而使其更加精确。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这种情况下，Huber Loss可能会非常有用，因为它会使最小值附近弯曲，从而降低梯度。另外它比MSE对异常值更鲁棒。因此，它结合了MSE和MAE的优良特性。但是，Huber Loss的问题是我们可能需要迭代地训练超参数delta。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;4、Log-Cosh Loss&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Log-cosh是用于回归任务的另一种损失函数，它比L2更加平滑。Log-cosh是预测误差的双曲余弦的对数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.20642201834862386&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquvdgibUuMpBgbJLS7UeicibPwXJhw8PuVFvzqCWMDRyRsEzXsPyaxdnzPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;436&quot; data-width=&quot;327px&quot; height=&quot;auto&quot; width=&quot;327px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7142857142857143&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquZGJ8QVibRPmobQMzibuEc3DVszmBiaFemAibaf75C2yfjA1IF5EbfIbvng/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; width=&quot;auto&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Log-cosh Loss（Y轴）与预测值（X轴）关系图。真值= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;优点： log(cosh(x))对于小的x来说，其大约等于 (x ** 2) / 2，而对于大的x来说，其大约等于 abs(x) - log(2)。这意味着'logcosh'的作用大部分与均方误差一样，但不会受到偶尔出现的极端不正确预测的强烈影响。它具有Huber Loss的所有优点，和Huber Loss不同之处在于，其处处二次可导。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么我们需要二阶导数？许多机器学习模型的实现（如XGBoost）使用牛顿方法来寻找最优解，这就是为什么需要二阶导数（Hessian）的原因。对于像XGBoost这样的机器学习框架，二阶可导函数更有利。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.3815261044176707&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquMWJ8VlRvBSuldIuTfLHeeyDQl4xSyrX0qJGV9YGEicxGNeLehGR9Sqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;747&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;XGBoost中使用的目标函数。注意其对一阶和二阶导数的依赖性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但Log-chsh Loss并不完美。它仍然存在梯度和Hessian问题，对于误差很大的预测，其梯度和hessian是恒定的。因此会导致XGBoost中没有分裂。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Huber和Log-cosh损失函数的Python代码：&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; readability=&quot;10.5&quot;&gt;&lt;pre readability=&quot;7&quot;&gt;
&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;sm_mae&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(true, pred, delta)&lt;/span&gt;:&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;&quot; readability=&quot;2&quot;&gt;&quot;&quot;&quot;&lt;br/&gt;true: array of true values    &lt;br/&gt;pred: array of predicted values&lt;p&gt;returns: smoothed mean absolute error loss&lt;br/&gt;&quot;&quot;&quot;&lt;/p&gt;&lt;/span&gt;&lt;br/&gt;loss = np.where(np.abs(true-pred) 0.5*((true-pred)**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;), delta*np.abs(true - pred) - &lt;span class=&quot;&quot;&gt;0.5&lt;/span&gt;*(delta**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;))&lt;br/&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;
&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(loss)&lt;p&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;logcosh&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(true, pred)&lt;/span&gt;:&lt;/span&gt;&lt;br/&gt;loss = np.log(np.cosh(pred - true))&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(loss)&lt;br/&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/section&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;5、Quantile Loss（分位数损失）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在大多数真实预测问题中，我们通常想了解我们预测的不确定性。了解预测值的范围而不仅仅是单一的预测点可以显着改善许多业务问题的决策过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当我们有兴趣预测一个区间而不仅仅是预测一个点时，Quantile Loss函数就很有用。最小二乘回归的预测区间是基于这样一个假设：残差（y - y_hat）在独立变量的值之间具有不变的方差。我们不能相信线性回归模型，因为它违反了这一假设。当然，我们也不能仅仅认为这种情况一般使用非线性函数或基于树的模型就可以更好地建模，而简单地抛弃拟合线性回归模型作为基线的想法。这时，Quantile Loss就派上用场了。因为基于Quantile Loss的回归模型可以提供合理的预测区间，即使是对于具有非常数方差或非正态分布的残差亦是如此。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;让我们看一个有效的例子，以更好地理解为什么基于Quantile Loss的回归模型对异方差数据表现良好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Quantile 回归 vs 普通最小二乘（Ordinary Least Square, OLS）回归&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.4525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquJHcasjaM2L6EfcmNArbJZGiaL8tQzBqkwo1l4rbb3lH2KRKLVlVFuvg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;左：线性关系b/w X1和Y，残差的方差恒定。右：线性关系b/w X2和Y，但Y的方差随着X2增加而变大（异方差）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.4525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquujCpkT2Qx2A8HvibBVjbtC5yvhDQeSyhgzRavLMeh5x3hcOhiaKPC72Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;橙线表示两种情况下的OLS估计&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.7464788732394366&quot; data-type=&quot;png&quot; data-w=&quot;568&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquVzstKrVfnOibicvY1BQBwPqoqk5WgeIuvSt5SQLSghn6AtsibAkQcfXzw/640?wx_fmt=png&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Quantile回归：虚线表示基于0.05和0.95 分位数损失函数的回归估计&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如上所示的Quantile回归代码在下面这个notebook中。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;地址：&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09_Quantile_Regression.ipynb&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;了解Quantile Loss函数&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;基于Quantile回归的目的是，在给定预测变量的某些值时，估计因变量的条件“分位数”。Quantile Loss实际上只是MAE的扩展形式（当分位数是第50个百分位时，Quantile Loss退化为MAE）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Quantile Loss的思想是根据我们是打算给正误差还是负误差更多的值来选择分位数数值。损失函数根据所选quantile (γ)的值对高估和低估的预测值给予不同的惩罚值。举个例子，γ= 0.25的Quantile Loss函数给高估的预测值更多的惩罚，并试图使预测值略低于中位数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.13807531380753138&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquZbv72jVjocwNFC8FHV9fic8EEbqWFNRLBfIDcCwEh8oXGELve8Tnkbg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;717&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;γ 是给定的分位数，其值介于0和1之间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7142857142857143&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquFC5OKMicPfvP9jwqLNMCApYnavRXFpRn2QxVhaXxibs6qFGRN5gJ8fDg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Quantile Loss（Y轴）与预测值（X轴）关系图。真值为Y= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们也可以使用这个损失函数来计算神经网络或基于树的模型的预测区间。下图是sklearn实现的梯度提升树回归。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquA8JafWRFGibIA96IDFWcKeqTWwpbEiboCYYKd9BZZw4HU1PLias5malqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;使用Quantile Loss的预测区间（梯度提升回归）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上图显示的是sklearn库的GradientBoostingRegression中的quantile loss函数计算的90％预测区间。上限的计算使用了γ = 0.95，下限则是使用了γ = 0.05。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;▌&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;比较研究&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;“Gradient boosting machines, a tutorial”中提供了一个很好的比较研究。为了演示上述所有的损失函数的性质，研究人员创造了一个人工数据集，数据集从sinc(x)函数中采样，其中加入了两种人造模拟噪声：高斯噪声分量和脉冲噪声分量。脉冲噪声项是用来展示结果的鲁棒效果的。以下是使用不同损失函数来拟合GBM（Gradient Boosting Machine, 梯度提升回归）的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  ﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.425&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9Fqu16eo4ia9fCPNia0Yo0XovgZU0Y1ibooaFQHq15Eiciaaib0ywGianlBwhuBPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;连续损失函数：（A）MSE损失函数; （B）MAE损失函数; （C）Huber损失函数; （D）Quantile损失函数。用有噪声的sinc(x)数据来拟合平滑GBM的示例：（E）原始sinc(x)函数; （F）以MSE和MAE为损失拟合的平滑GBM; （G）以Huber Loss拟合的平滑GBM， = {4,2,1}; （H）以Quantile Loss拟合的平滑GBM。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;模拟实验中的一些观察结果：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;一张图画出所有损失函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.65&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquFEmVhlvKbbicSJkWIKVaI6dbpQYoOYyGIV8dW8kIOI3MSibZOt63ib7fA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;原文链接：&lt;br/&gt;https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 人工智能头条&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 04 Jul 2018 01:46:28 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/cD8l43kowg</dc:identifier>
</item>
</channel>
</rss>