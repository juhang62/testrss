<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>怎样用数学找到一颗丢失的氢弹？</title>
<link>http://www.jintiankansha.me/t/zjWumyV2sQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/zjWumyV2sQ</guid>
<description>&lt;section class=&quot;xmt-style-block&quot; data-style-type=&quot;2&quot; data-tools=&quot;&amp;#x65B0;&amp;#x5A92;&amp;#x4F53;&amp;#x6392;&amp;#x7248;&quot; data-id=&quot;12849&quot;&gt;&lt;section class=&quot;KolEditor&quot; readability=&quot;2&quot;&gt;&lt;section readability=&quot;4&quot;&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;“怎样用数学找到一颗丢失的氢弹？”&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;——贝叶斯定理在搜索失踪物品方面的几个小故事&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;冷战期间，在西班牙上空曾发生过一次令人难以置信的事故：一场原本是例行公事的飞行活动，最终却造成两架飞机坠毁，七名机组人员死亡，整整一个村庄受到污染，更糟糕的是，一颗氢弹掉到海里，丢了。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;一千多名美国和西班牙人员展开了搜索和清理工作，还出动了十几架飞机、近三十艘美国海军的舰艇和五艘潜艇。整个行动花费超过1.2亿美元，造成的外交影响更是难以估量。而且颇具黑色幽默意味的是，18世纪曾有人提出过一个不起眼的数学定理，是关于如何在干草堆中寻找一根真正的针，当时没人认为这个定理有什么实用性，可在这次丢氢弹的事件中，这个定理却提供了无比实用的解决方案。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;让我们来看看整个事件的始末：1966年1月17日上午10点左右，驻扎在美国北卡罗来纳州的第31轰炸中队麾下的两架B-52G轰炸机正在西班牙卡塔赫纳西南方的海面上接近两架KC-135空中加油机，准备接受空中加油。这两架轰炸机每架都携带有四颗爆炸当量为150万吨梯恩梯的B-28型氢弹。此次带弹飞行代号“铬圆顶行动”，是美国核威慑任务的一部分，旨在让核武装轰炸机飞到苏联的家门口展示力量。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4636363636363636&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1tCSic1XeUVyJuVpudloCAz0x0qB1g9VCViccibUOXuOJddTg0c7nicOcpg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;550&quot; height=&quot;191.25pt&quot; width=&quot;412.5pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;正在接受KC-135空中加油机加油的B-52轰炸机。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;意外就在这个时候发生了：有一架B-52轰炸机在接近加油机时飞得太快了，以至于后者没有收到告警信号，结果这两架飞机在九千多米高的天上相撞。空中加油机的加油杆撕裂了这架B-52轰炸机的左翼，机上的七名机组人员中，有三人在事故中丧生。机翼断裂产生的火星引燃了机上的燃油，把整架加油机炸成了一团耀眼的火球。加油机上所有的四名机组人员当场死亡，一百多吨燃烧的飞机残骸落在了地中海附近一个名叫帕洛马雷斯的西班牙小村庄里。与飞机残骸一同落在地上的，还有那四颗氢弹中的三颗。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;不到24小时，美国空军的一支事故处理小组便从马德里附近的托雷洪空军基地赶到了现场。来自洛斯阿拉莫斯国家实验室和桑迪亚武器实验室以及空军后勤部队的专家们纷纷赶往这个不起眼的西班牙小乡村。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;搜索小组在一天之内就发现了三颗落在地上的氢弹：一颗落在了较软的土坡上，其外壳相对完整；其余两颗氢弹内的高爆炸药（用于产生爆轰波压缩核物质）在撞击时被引爆，在干燥的土壤中炸出了一个直径30多米的大坑，并把里面的放射性元素钚、铀和氚等洒落得到处都是。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7741176470588236&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1xGWyCsNF9VQZn9ZbLAQzDddfndxmRmdUOwCI8weFOiamPG8vuNziaPkw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;425&quot; height=&quot;246.75pt&quot; width=&quot;318.75pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;回收的一枚B-28型氢弹。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;按照惯例，必须找到下落不明的第四颗氢弹，然而，帕洛马雷斯村所在的阿尔梅里亚省拥有上千年的采矿业，频繁的人类活动打出了无数的矿井，原本干燥平坦的土壤被无数大坑和洼地弄得支离破碎——这些景观甚至因在那里拍摄过美国西部牛仔片而闻名。这使得对第四颗氢弹的搜索变得困难无比。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;几个星期内，美军和西班牙警察用辐射探测器对该地区进行了仔细的搜查，但未能找到第四颗氢弹。这时有目击者报告说，他看见有物体挂在降落伞下落入了大海。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;早在事故发生8小时后，美国海军便下令让一支舰队驶往西班牙海岸。事故发生5天后，美国空军正式请求海军帮助寻找失踪的氢弹。为了完成这次任务，美国海军甚至在其驻地聘请了一名“神童”。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;xmt-style-block&quot; data-style-type=&quot;5&quot; data-tools=&quot;&amp;#x65B0;&amp;#x5A92;&amp;#x4F53;&amp;#x6392;&amp;#x7248;&quot; data-id=&quot;1650104&quot;&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;59692&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;strong&gt;&lt;span&gt;管用的“神童”&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;这位“神童”名叫约翰·皮纳·克雷文，第二次世界大战期间，他曾在加州理工学院和衣阿华大学学习工程和水利学&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;当他作为一名文职科学家回到海军服役后，他解决了美国海军“鹦鹉螺”号核潜艇的一大结构性问题，还审查了“北极星”潜射弹道导弹项目。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;1963年，美国海军遇上了一件大事——“长尾鲨”号核潜艇在潜航期间失踪。海军方面派克雷文去负责深海救援和打捞研究工作。克雷文当时是海军特别项目办公室的负责人，他出色地完成了工作。三年后，他不得不再次上阵，以找到这颗失踪的氢弹，而且要快。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;苏联人肯定会竭尽全力地找到这颗氢弹，白宫方面也在施加压力。林登·约翰逊总统不相信某些政客做出的保证，即这颗氢弹永远消失在了大洋里，任何人也找不到。但是，在数百平方千米的海底图像中找到一艘皮划艇大小的物体看上去几乎是不可能完成的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3611111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1ickyXGXtNIBPdrmpVg25OGC7SWWyAawgWe8xibgFrtnicTvBHXz5uHVFg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;684&quot; height=&quot;149.95pt&quot; width=&quot;415.3pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;一名美国海军的潜水员正在地中海海底回收飞机残骸，照片来自美联社。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;当时，帕洛马雷斯村有位名叫弗朗西斯科·西莫·奥特斯的渔夫，他曾看到有个带着降落伞的物体从天而降，还指出了坠落的地方。不过，美国海军的专家基于他们自己对氢弹下降过程的计算，驳回了这位渔夫的说法。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;经过潜水员和声呐为期数周却无果的海底搜索之后，克雷文转向神圣的数学领域寻求帮助。令很多人没有想到的是，一个不起眼的、诞生于250多年前的概率理论居然让人们看到了找到氢弹的希望。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;在一本可追溯到18世纪60年代的未发表手稿中，一位英国皇家学会会员兼统计学家托马斯·贝叶斯首先提出了一种理论，这种理论后来还以贝叶斯的名字被命名为“贝叶斯定理”。简单来说，贝叶斯定理在数学上描述了如何“根据不确定性信息做出推理和决策”，即解决条件概率推理问题。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6419270833333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1RnwJUnToesoibbiaBckugaRRx0pX8RP6AQS5HWhonzRLxu6qjiabPqGsQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;768&quot; height=&quot;266.6pt&quot; width=&quot;415.3pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;剑桥大学用霓虹灯呈现出的贝叶斯定理公式。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;克雷文意识到，贝叶斯定理可以帮助改善搜索团队关于失踪氢弹所在位置的观念。他首先调来了帕洛马雷斯附近海底的详细地图，然后要求他手下的打捞和搜索专家对氢弹坠落期间可能发生的每一件事都计算概率。例如，氢弹有两个降落伞，在坠落过程中，一个伞打开的概率是多少？两个都打开的概率是多少？两个都未打开的概率是多少？氢弹直接落入水中的概率是多少？如果它以某个特定的角度下落会如何？克雷文的团队探索了数百种可能，并计算了每种可能发生的概率。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;xmt-style-block&quot; data-style-type=&quot;5&quot; data-tools=&quot;&amp;#x65B0;&amp;#x5A92;&amp;#x4F53;&amp;#x6392;&amp;#x7248;&quot; data-id=&quot;1650104&quot;&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;59692&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;strong&gt;&lt;span&gt;量化的观念&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;根据克雷文团队计算出的概率，这颗氢弹有可能会落在远离海岸的许多不同地方。然后，克雷文手下的数学家们根据最初的一轮“猜测”和每个指定位置的概率，计算出了每个指定位置有落弹的可能性。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;从本质上讲，数学家根据他们算出的情况，量化了他们对氢弹落点的猜测。最终，他们将对氢弹落点的猜测反映在了一张海底图像上——这张“概率图”显示了最有希望搜索到氢弹丢失的位置，但这些位置与传统搜索技术所认定的位置有极大的差别。数学家们的这张图表明，这颗氢弹的落点远不是在飞机残骸附近。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;美国海军派遣科研潜艇“阿尔文”号和“阿鲁米纳特”号检查了相关地点，但它们在搜索一番后无功而返。克雷文团队根据新的搜索信息重新计算了他们的概率。此时，距离氢弹坠落已经过去相当长一段时间了。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;白宫方面在收到克雷文的最新报告后，约翰逊要求派一群“真正的专家”解决这个问题。但是，在仔细审查了克雷文的报告之后，麻省理工学院和康奈尔大学的一个专家小组一致认为，克雷文所用的这种“奇怪的方法”是最好的方法。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;与此同时，有人开始重新审视渔夫奥特斯的说法。这位渔夫被邀请到了美国海军的“品尼高”号扫雷舰上，并指引着这艘船驶向一片声呐曾收到过可能是氢弹信号的海域。这片海域位于克雷文重新计算的最新海图上的一处高概率地点中。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;“阿尔文”号潜艇下潜到了2550英尺（约686米）深的海底，发现了一个覆盖在圆柱形金属物体上的降落伞。这艘潜艇试图抓住这个物体，但失败了，氢弹滑向了更深处。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;三周后，一艘早期型号的远程遥控潜水器重新找到了这颗氢弹，但却不小心将自身与这颗氢弹缠在了一起。冒着巨大的风险，遥控人员操纵着潜水器及缠绕其上的氢弹一起返回了海面。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7526041666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1Mny3TPohcptErSmkugee5icqkeBrdia5dJqrpvFlEMFvbsOyySFEicx3A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;768&quot; height=&quot;312.55pt&quot; width=&quot;415.3pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;1966年4月8日，美国海军的水兵正在检查从帕洛马雷斯外海回收的氢弹。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;在“帕洛马雷斯事件”发生两年后，克雷文团队再次应用贝叶斯定理找到了另一件丢失的物品——美国海军的“天蝎”号核潜艇。这艘潜艇约在1968年5月21日前后沉没在了亚速尔群岛外海，艇上人员全部遇难。再一次地，贝叶斯定理这一“数学魔法”向世人证实了其巨大的威力。不过，与上次找氢弹时的“眼见为实”不同，这次“找潜艇”是以一种“耳听为实”的方式证实的：水下听音器记录到了潜艇被压碎的声音，因为这艘潜艇的下潜深度超过了它所能承受的极限。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6972972972972973&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1ySjB8DIiaULWOT0CKoDgCc5GEx87dqJibXibyUU0wxTKb3snpiaJlt60gA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;740&quot; height=&quot;289.6pt&quot; width=&quot;415.3pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;沉在海底的“天蝎”号核潜艇的残骸。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;xmt-style-block&quot; data-style-type=&quot;5&quot; data-tools=&quot;&amp;#x65B0;&amp;#x5A92;&amp;#x4F53;&amp;#x6392;&amp;#x7248;&quot; data-id=&quot;1650104&quot;&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;59692&quot;&gt;&lt;section&gt;&lt;p&gt;&lt;span&gt;&lt;strong class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;strong&gt;&lt;span&gt;有用的“魔法”&lt;/span&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;在接下来的几十年里，克雷文的这种“数学魔法”一次又一次地证明了自己。例如，2009年，从里约热内卢飞往巴黎的法航447号航班在大西洋中部坠毁，坠毁处所在的海底距离海面有3000米之多。法国调查人员搜查了两年都没有找到残骸。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;最终，法国调查人员求助于美国密特隆咨询公司。密特隆公司将克雷文的方法应用于当时的整个搜索工作，并分别就事件、场景和位置计算了概率。密特隆公司的分析师获取了有关飞行动力学、飞机性能、当地风和洋流的数据，并分别对其计算了概率。然后，他们用之前获得的法航447号航班的搜索数据重复了这一步骤，并使用贝叶斯定理来更新他们对坠机发生位置的猜测。不出所料，新的概率将调查人员指引到了之前忽略的位置上。一周之后，搜索小组从水深约4000米的海底回收了飞机的黑匣子。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7135416666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pSqFmkOFya1FH5mzScnlbLUe294G0Qa1XOoS7Q7rKIQTGnCCE8gP6umWTEHYLt22Mr6PpeSbC2xMicD9SGFZ0oQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;768&quot; height=&quot;296.35pt&quot; width=&quot;415.3pt&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; center=&quot;&quot;&gt;&lt;span&gt;打捞出水的法航447号航班的残骸。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;饶是如此，基于贝叶斯定理的搜索需要至少提供一些“较好”的数据才能应用。以目前已终止寻找的马航370号航班为例，由于缺少数据，因此存在各种可能性——也就是说，几乎任何情况都有可能发生，可我们总不能真的认为是外星人绑架或黑洞吸引导致的吧！&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot;&gt;&lt;span&gt;在印度洋这片世界上最知之甚少的海域中，没有目击者，且几乎没有碎片和较确定的搜索区域，在这种情况下，恐怕就连克雷文这样的数学“神童”也无从下手。但是我们相信，五十多年前丢失在帕洛马雷斯外海的那颗氢弹曾被很多人认为是找不到的，但最终凭借数学这一强大的科学工具而被找到了，那么随着概率论等数学科学的发展，人们终有一天也会解开包括马航370号航班在内的诸多“失踪之谜”。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;- - End - -&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;更多精彩：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483829&amp;amp;idx=1&amp;amp;sn=470981938a14e177edd00eb302004723&amp;amp;chksm=ebe9cb08dc9e421e49337daa61a99811cb6b57ff65ff5d1ba22da0bb4b53e05bee8698384b9d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;重返数学史的黄金时代，由数学推动诞生的人工智能，一部人类智慧形成的历史&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483829&amp;amp;idx=2&amp;amp;sn=b0cf8da87ea8eacecefbb74e5eee38d7&amp;amp;chksm=ebe9cb08dc9e421ee7986ac38cd383621494d3ce3f952be01c563d49e8aef682b98c4c8e420a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;唯一能让马云安枕无忧的男人-吴翰清&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483821&amp;amp;idx=2&amp;amp;sn=8a2990537233e8d671aac0198e6f94b1&amp;amp;chksm=ebe9cb10dc9e420606f2598a808fe1bd30770d2a922749593afd59c5e30084c50b7cf520ceb1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;数学界的花木兰——苏菲﹒热尔曼&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  ☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483821&amp;amp;idx=1&amp;amp;sn=41ad433db38f618cef266b862d51d4ed&amp;amp;chksm=ebe9cb10dc9e4206f746d8a07e89d9a409b64220ee2852bc08d9cb40c445e0141ada0e11e1ef&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;艺术中蕴含的数学原理&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483858&amp;amp;idx=2&amp;amp;sn=7791d8b561c923c5ca0500569db387ab&amp;amp;chksm=ebe9cb6fdc9e4279ab80ec0f3427ba2008e1dfcc4c16315d09cc4aa1c31dd7171247dac339c8&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;湖南打工妹逆袭成为身价5亿“网络第一红娘”&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483858&amp;amp;idx=1&amp;amp;sn=2a2e02748a1a7c4512a3488d3e8dbc8f&amp;amp;chksm=ebe9cb6fdc9e4279f9a4e377688579d811a60786b6bcbb0e30a5eadc0546a7d6d6405082734e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;缅怀中国“布鞋院士”李小文&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 24 Jan 2019 15:16:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/zjWumyV2sQ</dc:identifier>
</item>
<item>
<title>全球“最有钱”国家宣布：美元不够 我自己印钱花</title>
<link>http://www.jintiankansha.me/t/CxQ4viGr4h</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/CxQ4viGr4h</guid>
<description>&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-ratio=&quot;0.060382916053019146&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkyYNr2mLLFgV8rAcf8TKrhVVxPz4sJSdjDum3ia43TqSj6sPicZVQ5XqpfDhulibnGRdeLqAhyHf8TWw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;679&quot; data-width=&quot;100%&quot;/&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;在任何一个成熟的经济体，如果每年的通胀超过6%，恐怕飞涨的物价早已让人怨声载道了&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;然而在非洲南部的津巴布韦，通货膨胀的概念，可能是价格标签上的小数点一年向右移动6位。。。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;比如在2008年津巴布韦恶性通胀最“巅峰”时期，通胀率曾达到每天98%，也就是说居民们早上一睁开眼睛，生活用品的价格就比前一天翻了一倍。。。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.49636363636363634&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nznM6jOGbs00TYeibJkc7guXq2uBUdsH2XX8wGtMn8hkdCvaY3yG6ZEQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot;/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;图片来源：津巴布韦储备银行 （public domain）&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而全球迄今为止面值最大的纸钞，正是津巴布韦在那段时间发行的100万亿大钞（即1后面带14个0），而这张钱竟然还不够买一个面包。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对于当时的津巴布韦人来说，最不缺的就是“钱”了，老百姓个个都是“兆亿富翁”。津巴布韦用了26年时间，将货币价值贬到几乎归零，换算过来只相当于最初的200，000，000，000，000，000，000，000，000，000，000，000分之一！（2乘以10的35次方）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0238095238095237&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32n3Icficz2eTLibF1P7t6UNKC7QY3NkascMjUO3pia8D3NACgLdLmfB56cg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;756&quot;/&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;津巴布韦人推着一车钞票买菜&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2009年中，津巴布韦无奈之下废除了本国货币，开始进入美元化时代。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;靠着美元作锚，津巴布韦在最近十年间基本稳住了通胀，GDP也开始了正增长。不过随着美联储加息回收美元流动性，津巴布韦最近又碰到了新的难题：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;美元现金的稀缺，导致美元在黑市上溢价几倍，进口成本猛增。而对于这个从牙签到矿泉水瓶都要进口的国家来说，物价又开始了新一轮飞涨。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了解决这一矛盾，津巴布韦财长于11日表示，该国将在今年再度发行本国货币！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt; 时隔10年重启本国货币 &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;据路透社等报道，1月11日，津巴布韦财政部长穆苏利·恩库贝在首都哈拉雷一次会议上表示，该国正在筹备发行新的本国货币，预期需耗时“几个月，不必几年”。当在场代表询问具体时间表时，他给出了“12个月内”的回答。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;津巴布韦央行数据显示，该国的外汇储备现在只能满足不到两周的进口需求。据统计，津巴布韦全国的账户余额大约为100亿美元，但央行实际掌握的美元现钞不到4亿美元。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1394849785407726&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nSgLX5rVsn7U7iafd8XaRGzaQbk3MzlHU5QLBL10xm0oykjPGqiccCFTw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;466&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对美元现金的“饥渴”，使得美元现钞在黑市上溢价370%。上周六（12日），津巴布韦的燃油价格突然从1.32美元一升暴涨到3.31美元一升，但如果外国游客能用美元现金付费的话则价格保持不变。据当地网友爆料，津巴布韦某加油站由于宣布只接受美元现金，引发了排队群众的愤怒。津巴布韦很多工厂由于进口原材料的成本暴涨，不得不停工歇业。该国著名的食用油和肥皂制造商Olivine上周六表示，由于拖欠外国供应商1100万美元无法还清，该公司暂停生产并安排工人无限期休假。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;某国有大行负责对接津巴布韦业务的业内人士对每日经济新闻（微信号：nbdnews）记者表示，最近几年在津巴布韦市面上已经很少看的到纸钞了，大多数交易都是通过电子支付完成。如果想要从自己的电子账户里提取美元现金，则需要支付一笔手续费。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nYf3aMkzHicSkfbZ8uQaVEuhNz6qqQnicOZUAJV8BiaqHGQOSVic6AlEY7Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;732&quot;/&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;津巴布韦瀑布风光&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;记者注意到，除美国外，全球共有25个国家或地区将美元视为官方货币，意味着这些社会只能被动接受美国货币政策带来的冲击。随着美联储加息的脚步，美元流动性被收回美国国内；而且此前因为经济制裁等因素，很多美元资金无法进入津巴布韦，使得津巴布韦极度缺乏美元现金。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;通胀之苦：牛奶鸡蛋每小时都在涨价&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对于津巴布韦的普通百姓来说，他们未必会怀着喜悦的心情迎接本国新货币的发行，因为过去恶性通胀的回忆实在太过惨痛。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2008年，津巴布韦创造了和平年代人类货币史上最严重的通胀，全年物价上涨了约50亿倍（是的，你没有看错）。2009年2月，津巴布韦被迫宣布货币改革，直接将货币面值删除12个0，即1万亿第四代津元才抵得上1个第三代津元。即使这样，津巴布韦当时还是要推出100万亿巨钞，如此惊人面值的钞票也在发行4个月后就“不堪重任”，退出了流通。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当地人曾对纽约时报回忆起那段岁月，“鸡蛋、蔬菜、水果、牛奶每个小时都要涨价，极端的时候甚至几分钟涨一次。”由于本国货币还没有纸张值钱，当地居民甚至将手里的巨额钞票当做纪念品卖给外国游客，换来的美元要远远超过货币的面值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nEKByicn3YKiabScyTbFlXiaibS5XyRMsxXsQVb88OQm1emqXqyYNmP44Mw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;480&quot;/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;纸板上写着“挨饿的亿万富翁”&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;其实，在上世纪80年代，津巴布韦元甚至比美元还要值钱！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作为英国前殖民地，资源富饶的津巴布韦在独立之初一度风光无限，首都哈拉雷的国际知名度在当时甚至远超南非的约翰内斯堡。1983年，第一代津元与英镑1：1挂钩，只需要0.678津元就能兑换1美元。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而到了2009年2月津巴布韦货币改革时，津巴布韦元的币值只相当于最初的2乘以10的35次方分之一了！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上述国有大行受访人士对每日经济新闻（微信号：nbdnews）记者表示，恶性通胀当时极大地损害了津巴布韦普通平民的利益。因为如此严重的通胀几乎相当于抹平了一切债务，而有能力借债的只有少数富人，大部分普通人只有微薄的银行存款，资产价值被通胀吞噬了很多。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;摆脱美元，未来将拥抱人民币？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2009年1月29日，津巴布韦宣布允许当地人使用任何其他国家的货币开展业务。2009年6月30日，津巴布韦本国货币退出流通。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;出于保值需求，当地人将能找得到的外国货币都当做了硬通货，其中包括美元、英镑、南非兰特以及博茨瓦纳普拉等世界主流储备货币以及非洲南部流通较广的币种。到了2014年1月30日，津巴布韦央行又宣布接受人民币、日元、澳元和印度卢比作为该国的流通货币，成为世所罕见的9种外国货币同时流通的国家。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7506024096385542&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nPXS56rsH7mtoNo2CmQ4XFKNOicD2G6vAunnPUuYLicL5eWPNYnBdibR5A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;830&quot;/&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;津巴布韦首都附近&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了推进美元的流通，津巴布韦央行也作出过努力。2016年11月，津巴布韦央行在非洲进出口银行提供的担保下，推出价值2亿美元的“债券货币”（bond note）。津巴布韦央行规定，这种“代币”与美元等值，以增加市面上的美元流动性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不过由于“债券货币”发行规模小，总体而言对于缓解美元现金紧缺助益不大。上述受访者对记者说道，&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;“债券货币的流动性还是比美元差一些。不过也有当地人在黑市上拿人民币找到中国老板，要换债券货币的。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如今，津巴布韦的美元化进程似乎走到了拐点，那未来人民币有机会成为这里的主要储备货币之一吗？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.36823104693140796&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxgSWdsPWo5oCn2fXLhz32nib2RqficJDUcL3l9UfGVF4v0atibCW4rsHAwIjD7bZKhGibOUibxPleWcVQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;831&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;据我国外交部网站援引埃菲社报道，2018年5月29日，津巴布韦央行行长约翰·曼古迪亚在首都哈拉雷召开的南部和东部非洲国家央行行长会议上建议将人民币作为储备货币。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;“出席会议的大部分国家都得到了中国的贷款或者援助，用人民币还款的话很有经济意义”，曼古迪亚说。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;报道称，曼古迪亚认为，人民币在中国与非洲的贸易中已经成为“通用外币”，制定政策的人必须注意到非洲的发展，要拥抱人民币。报道表示，中国是津巴布韦的主要贸易伙伴之一，也是该国自2000年以来的最大投资来源。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;————&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;编辑 ∑ Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源：每日经济新闻&lt;/span&gt;&lt;/p&gt;
&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section readability=&quot;2.7516891891892&quot;&gt;&lt;section readability=&quot;5.5033783783784&quot;&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; display:=&quot;&quot; inline-block=&quot;&quot; top=&quot;&quot; solid=&quot;&quot; overflow-wrap:=&quot;&quot; break-word=&quot;&quot; important=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section readability=&quot;1.5&quot;&gt;&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;&lt;span&gt;微信公众号“算法数学之美”，由算法与数学之美团队打造的另一个公众号，欢迎大家扫码关注！&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; wenquanyi=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; helvetica=&quot;&quot; neue=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; text-align:=&quot;&quot; justify=&quot;&quot; em=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkyjQVC3BFiaFKQ4DJqz2xhrwkzeCPbjQdnnG8678fRf1sxc2ZQtvtVib2dqWUkeopYtmgckINoOoGoQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;258&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;更多精彩：&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483726&amp;amp;idx=1&amp;amp;sn=e5e008fb68a7d837546d0ac5b5438042&amp;amp;chksm=ebe9cbf3dc9e42e5d625b2da6b9b3866dff9f08d442d8106f4cbf035d8602e1fdda86eec6476&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;如何向5岁小孩解释什么是支持向量机（SVM）？&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483726&amp;amp;idx=2&amp;amp;sn=18272b7eaa172794b51c30d0a2dd9c48&amp;amp;chksm=ebe9cbf3dc9e42e5ddf9a189822a2fa099543a631ad63a1d6ed0158b51c76212eb65ebbfe71b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;自然底数e的意义是什么？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483715&amp;amp;idx=1&amp;amp;sn=9069dadf4bbce2aa34bd64b85a69dcee&amp;amp;chksm=ebe9cbfedc9e42e81c27d72da15c0dbf848e505946f231051b8b4033d0941bc6f51cef32790e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;☞&lt;/span&gt;费马大定理，集惊险与武侠于一体&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483715&amp;amp;idx=2&amp;amp;sn=55a16f37c89b27994b263e0dc9837561&amp;amp;chksm=ebe9cbfedc9e42e842deb581ea62b750cedd839abd58c2db3261bf9fbcd172a2cf18512e4d2d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;简单的解释，让你秒懂“最优化” 问题&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483676&amp;amp;idx=1&amp;amp;sn=2366a39bca3ef42a6e868e91ea718813&amp;amp;chksm=ebe9cba1dc9e42b70c7e147b9e43828c1a7c68401f442890471a06e5cf0704437f9813ca0e0f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;☞&lt;/span&gt;一分钟看懂一维空间到十维空间&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483661&amp;amp;idx=1&amp;amp;sn=d822666a054ba70b37dfb06d14c60f3a&amp;amp;chksm=ebe9cbb0dc9e42a6c476f7f81095b772aa45d960bf516f60c5b2e1155c9093696222cea0a83d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;本科、硕士和博士到底有什么区别？&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=1&amp;amp;sn=7d0d05c78cd01df91495f1d14609cbce&amp;amp;chksm=ebe9cbbbdc9e42add13cfe99f3383745fa5c059df705a3a9e28644d073dff804569af94970e3&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;小波变换通俗解释&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=4&amp;amp;sn=ce88086b650c601bdbf57ecfe5a490a1&amp;amp;chksm=ebe9cbbbdc9e42adfaf0e4ee644d254835c830ef47663315b70a39a2b47e6a7cf10d0826b88d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;微积分必背公式&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=3&amp;amp;sn=ceaec6043bb0e8a851033482f8f572bf&amp;amp;chksm=ebe9cbbbdc9e42ad30fd38383cf1caa609ac6e81964da17277f8e2a7f17a933cd11e0f3840c8&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;影响计算机算法世界的十位大师&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=2&amp;amp;sn=bf439d56bc7d42083708fa76434a6025&amp;amp;chksm=ebe9cbbbdc9e42ad9e5f2b3c1952e620e0e3d4452aae25b611e7e54be8678b0d80e002e7be6d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;数据挖掘之七种常用的方法&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 24 Jan 2019 15:16:53 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/CxQ4viGr4h</dc:identifier>
</item>
<item>
<title>直白介绍卷积神经网络（CNN）</title>
<link>http://www.jintiankansha.me/t/rtUFGswJNY</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/rtUFGswJNY</guid>
<description>&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt; 什么是卷积神经网络，它为何重要？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积神经网络（也称作 &lt;strong&gt;ConvNets&lt;/strong&gt; 或 &lt;strong&gt;CNN&lt;/strong&gt;）是神经网络的一种，它在图像识别和分类等领域已被证明非常有效。 卷积神经网络除了为机器人和自动驾驶汽车的视觉助力之外，还可以成功识别人脸，物体和交通标志。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5037513397642015&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3MuhEAb6yS9dfevx5BBmibyNL0Nt6MzcQ59UscQIPhxglIroDYmRXq4Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;933&quot;/&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;图1&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;如&lt;strong&gt;图1&lt;/strong&gt;所示，卷积神经网络能够识别图片的场景并且提供相关标题（“足球运动员正在踢足球”），&lt;strong&gt;图2&lt;/strong&gt;则是利用卷积神经网络识别日常物品、人类和动物的例子。最近，卷积神经网络在一些自然语言处理任务（如语句分类）中也发挥了很大作用。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;strong&gt;&lt;img class=&quot;aligncenter&quot; data-ratio=&quot;0.37254901960784315&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3fmJCiaIoH5RMn64cDxp9d7DkiacToX7f70otvSBRt0bHlWqCYjbsWPHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1326&quot; height=&quot;258&quot; width=&quot;693&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; center=&quot;&quot;&gt;&lt;strong&gt;&lt;span&gt;图2&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;因此，卷积神经网络是当今大多数机器学习实践者的重要工具。但是，理解卷积神经网络并开始尝试运用着实是一个痛苦的过程。本文的主要目的是了解卷积神经网络如何处理图像。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;对于刚接触神经网络的人，我建议大家先阅读这篇关于多层感知机的简短教程 ，了解其工作原理之后再继续阅读本文。多层感知机即本文中的“完全连接层”。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; LeNet 框架（20世纪90年代）&lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;LeNet 是最早推动深度学习领域发展的卷积神经网络之一。这项由 Yann LeCun 完成的开创性工作自1988年以来多次成功迭代之后被命名为 LeNet5。当时 LeNet 框架主要用于字符识别任务，例如阅读邮政编码，数字等。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;接下来，我们将直观地了解 LeNet 框架如何学习识别图像。 近年来有人提出了几种基于 LeNet 改进的新框架，但是它们的基本思路与 LeNet 并无差别，如果您清楚地理解了 LeNet，那么对这些新的框架理解起来就相对容易很多。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23850267379679144&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3DzcT2JmcebsR0ofIsibMz1QqxX7TSAPcZEicQnusskqqC18znicGJmZcA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1870&quot; height=&quot;109&quot; width=&quot;456&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图3: 一个简单的卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;3&lt;/strong&gt;中的卷积神经网络在结构上与原始的 LeNet 类似，并将输入图像分为四类：狗，猫，船或鸟（原始的 LeNet 主要用于字符识别任务）。 从上图可以看出，接收船只图像作为输入时，神经网络在四个类别中正确地给船只分配了最高概率值（0.94）。输出层中所有概率的总和应该是1（之后会做解释）。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;3 &lt;/strong&gt;的卷积神经网络中有四个主要操作：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;-1&quot;&gt;&lt;li&gt;
&lt;p&gt;&lt;span&gt;卷积&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;非线性变换（ReLU）&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span&gt;池化或子采样&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;分类（完全连接层）&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;这些操作是所有卷积神经网络的基本组成部分，因此了解它们的工作原理是理解卷积神经网络的重要步骤。下面我们将尝试直观地理解每个操作。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 一张图片就是一个由像素值组成的矩阵 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;实质上，每张图片都可以表示为由像素值组成的矩阵。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9982788296041308&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO30pHwunom79x1myepHI8ws6a0sgzuVfxa2tjhicia3ib8RVd8xia0zcdCsg/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;581&quot; height=&quot;580&quot; width=&quot;581&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图4: 每张图片就是一个像素矩阵&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;通道（channel）是一个传统术语，指图像的一个特定成分。标准数码相机拍摄的照片具有三个通道——红，绿和蓝——你可以将它们想象为三个堆叠在一起的二维矩阵（每种颜色一个），每个矩阵的像素值都在0到255之间。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;而灰度图像只有一个通道。 鉴于本文的科普目的，我们只考虑灰度图像，即一个代表图像的二维矩阵。矩阵中每个像素值的范围在0到255之间——0表示黑色，255表示白色。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;卷积 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积神经网络的名字来源于“卷积”运算。在卷积神经网络中，卷积的主要目的是从输入图像中提取特征。通过使用输入数据中的小方块来学习图像特征，卷积保留了像素间的空间关系。我们在这里不会介绍卷积的数学推导，但会尝试理解它是如何处理图像的。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;正如前文所说，每个图像可以被看做像素值矩阵。考虑一个像素值仅为0和1的5 × 5大小的图像（注意，对于灰度图像，像素值范围从0到255，下面的绿色矩阵是像素值仅为0和1的特殊情况）：&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;alignnone aligncenter&quot; data-ratio=&quot;0.9061224489795918&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO30uyicKkrCiccGwL9m05Ffw3WOib1mLZcMg5HVssqDoeUlUPJuv4cfSL3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;490&quot; height=&quot;444&quot; width=&quot;490&quot;/&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;另外，考虑另一个 3×3 矩阵，如下图所示：&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;aligncenter&quot; data-ratio=&quot;0.85625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3htib0aPhmicOaMibJSHlwIjWDgM0YY9icnL7T2MwM1JjhrrZHXkyDk50jA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;320&quot; height=&quot;274&quot; width=&quot;320&quot;/&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;上述5 x 5图像和3 x 3矩阵的卷积计算过程如&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;5&lt;/strong&gt;中的动画所示：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7300380228136882&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3bU6u542rHFRWNJHNsWUzRUJSR29eMUb5XBuW0S37aXFWLahGicF8iaPQ/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;526&quot; height=&quot;384&quot; width=&quot;526&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图5：卷积操作。输出矩阵称作“卷积特征”或“特征映射”&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;我们来花点时间理解一下上述计算是如何完成的。将橙色矩阵在原始图像（绿色）上以每次1个像素的速率（也称为“步幅”）移动，对于每个位置，计算两个矩阵相对元素的乘积并相加，输出一个整数并作为最终输出矩阵（粉色）的一个元素。注意，3 × 3矩阵每个步幅仅能“看到”输入图像的一部分。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;在卷积神经网路的术语中，这个3 × 3矩阵被称为“&lt;strong&gt;过滤器&lt;/strong&gt;”或“核”或“特征探测器”，通过在图像上移动过滤器并计算点积得到的矩阵被称为“卷积特征”或“激活映射”或“&lt;strong&gt;特征映射&lt;/strong&gt;”。重要的是要注意，过滤器的作用就是原始输入图像的特征检测器。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;从上面的动画可以明显看出，对于同一张输入图像，不同的过滤器矩阵将会产生不同的特征映射。例如，考虑如下输入图像：&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;alignnone&quot; data-ratio=&quot;0.9879518072289156&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO34YA3j0ljnybZvaAJBdr0frW6hJYnn3RcdDsdCB6dHZ1Zuqh8licrVQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;166&quot; height=&quot;164&quot; width=&quot;166&quot;/&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;在下表中，我们可以看到上图在不同过滤器下卷积的效果。如图所示，只需在卷积运算前改变过滤器矩阵的数值就可以执行边缘检测，锐化和模糊等不同操作 [8] —— 这意味着不同的过滤器可以检测图像的不同特征，例如边缘， 曲线等。更多此类示例可在 这里 8.2.4节中找到。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; start=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;img class=&quot;alignnone&quot; data-ratio=&quot;1.6443768996960486&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO30QFx9rmPIeQRM4RqtW34cJTN5saP4ARW0ljHcMjHUMD05h1Adhxj0Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;658&quot; height=&quot;1082&quot; width=&quot;658&quot;/&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;另一个理解卷积操作的好方法可以参考下面&lt;strong&gt;文字：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;一个过滤器（红色边框）在输入图像上移动（卷积操作）以生成特征映射。在同一张图像上，另一个过滤器（绿色边框）的卷积生成了不同的特征图，如图所示。需要注意到，卷积操作捕获原始图像中的局部依赖关系很重要。还要注意这两个不同的过滤器如何从同一张原始图像得到不同的特征图。请记住，以上图像和两个过滤器只是数值矩阵。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;实际上，卷积神经网络在训练过程中会自己&lt;em&gt;学习&lt;/em&gt;这些过滤器的值（尽管在训练过程之前我们仍需要指定诸如过滤器数目、大小，网络框架等参数）。我们拥有的过滤器数目越多，提取的图像特征就越多，我们的网络在识别新图像时效果就会越好。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;特征映射（卷积特征）的大小由我们在执行卷积步骤之前需要决定的三个参数[4]控制：&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;-0.5&quot;&gt;&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;深度：&lt;/strong&gt;深度对应于我们用于卷积运算的过滤器数量。在&lt;strong&gt;图6&lt;/strong&gt;所示的网络中，我们使用三个不同的过滤器对初始的船图像进行卷积，从而生成三个不同的特征图。可以将这三个特征地图视为堆叠的二维矩阵，因此，特征映射的“深度”为3。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5015974440894568&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3slibM6gf9SWgmiafFnGN9P1iaS9JoWDb6c4hiaoLuV7GOFibWlzwHcIsX8g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1252&quot; height=&quot;258&quot; width=&quot;515&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图6&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;步幅：&lt;/strong&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  步幅是我们在输入矩阵上移动一次过滤器矩阵的像素数量。当步幅为1时，我们一次将过滤器移动1个像素。当步幅为2时，过滤器每次移动2个像素。步幅越大，生成的特征映射越小。&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;零填充：&lt;/strong&gt;有时，将输入矩阵边界用零来填充会很方便，这样我们可以将过滤器应用于输入图像矩阵的边界元素。零填充一个很好的特性是它允许我们控制特征映射的大小。添加零填充也称为&lt;em&gt;宽卷积&lt;/em&gt;，而不使用零填充是为&lt;em&gt;窄卷积&lt;/em&gt;。 这在[14]中有清楚的解释。&lt;/span&gt;&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 非线性部分介绍（ReLU）&lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;如上文&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;3&lt;/strong&gt;所示，每次卷积之后，都进行了另一项称为 ReLU 的操作。ReLU 全称为修正线性单元（Rectified Linear Units），是一种非线性操作。 其输出如下图所示：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3137614678899083&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3gkGKZvaaTib9mlx9HoG1JwSBWOwDicplTiauYvtfUx2oBia5mV86a6Pcicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1090&quot; height=&quot;190&quot; width=&quot;607&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图7: ReLU 函数&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;ReLU 是一个针对元素的操作（应用于每个像素），并将特征映射中的所有负像素值替换为零。ReLU 的目的是在卷积神经网络中引入非线性因素，因为在实际生活中我们想要用神经网络学习的数据大多数都是非线性的（卷积是一个线性运算 —— 按元素进行矩阵乘法和加法，所以我们希望通过引入 ReLU 这样的非线性函数来解决非线性问题）。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;从&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;8&lt;/strong&gt;可以很清楚地理解 ReLU 操作。它展示了将 ReLU 作用于某个特征映射得到的结果。这里的输出特征映射也被称为“修正”特征映射。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3745819397993311&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3QmM1ZZVdJ3wHWiaMkEDKQDqfjJBQ4v2EfEktpI8Sd6EzSYL5dlgC4nw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1794&quot; height=&quot;241&quot; width=&quot;643&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图8: ReLU 操作&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;其他非线性函数诸如 &lt;strong&gt;tanh&lt;/strong&gt; 或 &lt;strong&gt;sigmoid&lt;/strong&gt; 也可以用来代替 ReLU，但是在大多数情况下，ReLU 的表现更好。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 池化 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;空间池化（也称为子采样或下采样）可降低每个特征映射的维度，并保留最重要的信息。空间池化有几种不同的方式：最大值，平均值，求和等。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;在最大池化的情况下，我们定义一个空间邻域（例如，一个2 × 2窗口），并取修正特征映射在该窗口内最大的元素。当然我们也可以取该窗口内所有元素的平均值（平均池化）或所有元素的总和。在实际运用中，最大池化的表现更好。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;9&lt;/strong&gt;展示了通过2 × 2窗口在修正特征映射（卷积+ ReLU 操作后得到）上应用最大池化操作的示例。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8526863084922011&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO391GfmyvpmYVlLExQma4ggWJJS9JnCNWPygibHD1YueUNAXI4DT3XQUQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1154&quot; height=&quot;984&quot; width=&quot;1154&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图9: 最大池化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;我们将2 x 2窗口移动2个单元格（也称为“步幅”），并取每个区域中的最大值。如&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;9&lt;/strong&gt;所示，这样就降低了特征映射的维度。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;在&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;10&lt;/strong&gt;所示的网络中，池化操作分别应用于每个特征映射（因此，我们从三个输入映射中得到了三个输出映射）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5471264367816092&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3ZAp2Qq3jiaTbHrUia3I4e1aYDiaPxSOibWdlpibhuYy26T06yKteYiaV2S5w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;870&quot; height=&quot;476&quot; width=&quot;870&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图10: 在修正特征映射上应用池化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;11&lt;/strong&gt;展示了我们对&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;8&lt;/strong&gt;中经过 ReLU 操作之后得到的修正特征映射应用池化之后的效果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.42630744849445323&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3ic1v6Zia0aa3cVDXKKoNUFWic1zQqTB68cmUzu0LacLZNV1yKuric4Jiarw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1262&quot; height=&quot;239&quot; width=&quot;560&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图11: 池化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;池化的作用是逐步减少输入的空间大小[4]。具体来说有以下四点：&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;2.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;使输入（特征维度）更小，更易于管理&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;减少网络中的参数和运算次数，因此可以控制过拟合 [4]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;使网络对输入图像微小的变换、失真和平移更加稳健（输入图片小幅度的失真不会改池化的输出结果 —— 因为我们取了邻域的最大值/平均值）。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;可以得到尺度几乎不变的图像（确切的术语是“等变”）。这是非常有用的，这样无论图片中的物体位于何处，我们都可以检测到，（详情参阅[18]和[19]）。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;至此…&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.23756345177664975&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3Ria7plichOKkiaJ4hbAc4zfibfSa89t9kHWzJJGp5BrX6YO8ichXunncQyQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1970&quot; height=&quot;113&quot; width=&quot;475&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图12&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;目前为止，我们已经了解了卷积，ReLU 和池化的工作原理。这些是卷积神经网络的基本组成部分，理解这一点很重要。如&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;13&lt;/strong&gt;所示，我们有两个由卷积，ReLU 和 Pooling 组成的中间层 —— 第二个卷积层使用六个过滤器对第一层的输出执行卷积，生成六个特征映射。然后将 ReLU 分别应用于这六个特征映射。接着，我们对六个修正特征映射分别执行最大池化操作。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;这两个中间层的作用都是从图像中提取有用的特征，在网络中引入非线性因素，同时对特征降维并使其在尺度和平移上等变[18]。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;第二个池化层的输出即完全连接层的输入，我们将在下一节讨论。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 完全连接层 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;完全连接层是一个传统的多层感知器，它在输出层使用 softmax 激活函数（也可以使用其他分类器，比如 SVM，但在本文只用到了 softmax）。“完全连接”这个术语意味着前一层中的每个神经元都连接到下一层的每个神经元。 如果对多层感知器不甚了解，我建议您阅读这篇文章。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积层和池化层的输出代表了输入图像的高级特征。完全连接层的目的是利用这些基于训练数据集得到的特征，将输入图像分为不同的类。例如，我们要执行的图像分类任务有四个可能的输出，如&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;13&lt;/strong&gt;所示（请注意，图14没有展示出完全连接层中节点之间的连接）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3132328308207705&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3udyjp6XMc8nKsGibQaMzicvPDFKfeEoPWrgFhFRpgo5wS60LEbyiapAQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1194&quot; height=&quot;150&quot; width=&quot;479&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图13: 完全连接层——每个节点都与相邻层的其他节点连接&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;除分类之外，添加完全连接层也是一个（通常来说）比较简单的学习这些特征非线性组合的方式。卷积层和池化层得到的大部分特征对分类的效果可能也不错，但这些特征的组合可能会更好[11]。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;完全连接层的输出概率之和为1。这是因为我们在完全连接层的输出层使用了 softmax 激活函数。Softmax 函数取任意实数向量作为输入，并将其压缩到数值在0到1之间，总和为1的向量。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 正式开始——使用反向传播进行训练 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;如上所述，卷积+池化层用来从输入图像提取特征，完全连接层用来做分类器。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;注意，在&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;14&lt;/strong&gt;中，由于输入图像是船，对于船类目标概率为1，其他三个类为0&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;-0.5&quot;&gt;&lt;li&gt;
&lt;p&gt;&lt;span&gt;输入图像 = 船&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;目标向量 = [0, 0, 1, 0]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3516949152542373&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3UvkfEgjRLyxg4LqW3iarTiaOdaIRaPCBBXOZxhs1YzsiaSPXgPjicMJJKg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1888&quot; height=&quot;183&quot; width=&quot;521&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;图14：训练卷积神经网络&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积网络的整体训练过程概括如下：&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;16&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;步骤1：用随机值初始化所有过滤器和参数/权重&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;步骤2：神经网络将训练图像作为输入，经过前向传播步骤（卷积，ReLU 和池化操作以在完全连接层中的前向传播），得到每个类的输出概率。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;假设上面船只图像的输出概率是 [0.2,0.4,0.1,0.3]&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;由于权重是随机分配给第一个训练样本，因此输出概率也是随机的。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;步骤3：计算输出层的总误差（对所有4个类进行求和）&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;总误差&lt;/strong&gt;&lt;strong&gt;=∑ ½(目标概率 – 输出概率)²&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;步骤4：使用反向传播计算网络中所有权重的误差&lt;em&gt;梯度&lt;/em&gt;，并使用&lt;em&gt;梯度下降&lt;/em&gt;更新所有过滤器值/权重和参数值，以最小化输出误差。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;根据权重对总误差的贡献对其进行调整。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;当再次输入相同的图像时，输出概率可能就变成了 [0.1,0.1,0.7,0.1]，这更接近目标向量 [0,0,1,0]。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;这意味着网络已经&lt;em&gt;学会了&lt;/em&gt;如何通过调整其权重/过滤器并减少输出误差的方式对特定图像进行正确分类。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;过滤器数量、大小，网络结构等参数在步骤1之前都已经固定，并且在训练过程中不会改变 —— 只会更新滤器矩阵和连接权值。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;步骤5：对训练集中的所有图像重复步骤2-4。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;通过以上步骤就可以&lt;em&gt;训练&lt;/em&gt;出卷积神经网络 —— 这实际上意味着卷积神经网络中的所有权重和参数都已经过优化，可以对训练集中的图像进行正确分类。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;当我们给卷积神经网络中输入一个新的（未见过的）图像时，网络会执行前向传播步骤并输出每个类的概率（对于新图像，计算输出概率所用的权重是之前优化过，并能够对训练集完全正确分类的）。如果我们的训练集足够大，神经网络会有很好的泛化能力（但愿如此）并将新图片分到正确的类里。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;注1&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;为了给大家提供一个直观的训练过程，上述步骤已经简化了很多，并且忽略了数学推导过程。如果想要数学推导以及对卷积神经网络透彻的理解，请参阅 [4] 和 [12]。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;注&lt;/strong&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;上面的例子中，我们使用了两组交替的卷积和池化层。但请注意，这些操作可以在一个卷积神经网络中重复执行多次。实际上，现在效果最好的一些卷积神经网络都包含几十个卷积和池化层！ 另外，每个卷积层之后的池化层不是必需的。从下面的&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;15&lt;/strong&gt;中可以看出，在进行池化操作之前，我们可以连续进行多个卷积 + ReLU 操作。另外请注意图16卷积神经网络的每一层是如何展示的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4854614412136536&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3vbRukbkSlwdMFZ1yViazjdN3IMLBXX3f8jlZtLz4WDXt8EvxLP6c1og/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1582&quot; height=&quot;272&quot; width=&quot;561&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图15&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 卷积神经网络的可视化 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;一般来说，卷积步骤越多，神经网络能够学习识别的特征就更复杂。例如，在图像分类中，卷积神经网络在第一层可能会学习检测原始像素的边缘，然后在第二层利用这些边缘检测简单形状，然后在更高级的层用这些形状来检测高级特征，例如面部形状 [14]。&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;16&lt;/strong&gt;演示了这个过程 —— 这些特征是使用卷积深度信念网络学习的，这张图片只是为了演示思路（这只是一个例子：实际上卷积过滤器识别出来的对象可能对人来说并没有什么意义）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0547112462006079&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO30gbHarksicy52IXRZcN8rns3DvBt2Kgqsae4jFOv6pvJ3jibsx0q9DfA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;658&quot; height=&quot;694&quot; width=&quot;658&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图16: 卷积深度信念网络学习特征&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;Adam Harley 创建了一个基于 MNIST 手写数字数据集 [13]训练卷积神经网络的可视化。我强烈推荐大家 使用它来了解卷积神经网络的工作细节。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;我们在下图中可以看到神经网络对于输入数字“8”的具体操作细节。请注意，&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;18&lt;/strong&gt;中并未单独显示ReLU操作。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5663900414937759&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3GgqocnYfgTjk5y2llJMCnPrMN6sic27S0kfourIdsYTczfyrFmEt8aw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1928&quot; height=&quot;314&quot; width=&quot;555&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图17：基于手写数字训练卷积神经网络的可视化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;输入图像包含 1024 个像素点（32 × 32 图像），第一个卷积层（卷积层1）由六个不同的5 × 5（步幅为1）过滤器与输入图像卷积而成。如图所示，使用六个不同的过滤器得到深度为六的特征映射。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积层1之后是池化层1，它在卷积层1中的六个特征映射上分别进行2 × 2最大池化（步幅为2）。将鼠标指针移动到池化层的任意像素上，可以观察到它来自于2 x 2网格在前一个卷积层中的作用（如&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;18&lt;/strong&gt;所示）。注意到2 x 2网格中具有最大值（最亮的那个）的像素点会被映射到池化层。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2389937106918239&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3N6LWmmejPgM754KlRQficuykwTZDSWPYribjUPJwwUbT0dkribsTR77nA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1590&quot; height=&quot;119&quot; width=&quot;498&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图18：池化操作可视化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;池化层1之后是十六个执行卷积操作的5 × 5（步幅为1）卷积过滤器。然后是执行2 × 2最大池化（步幅为2）的池化层2。 这两层的作用与上述相同。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;然后有三个完全连接（FC）层：&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;0&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;第一个FC层中有120个神经元&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;第二个FC层中有100个神经元&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;第三个FC层中的10个神经元对应于10个数字 —— 也称为输出层&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;注意，在&lt;strong&gt;图&lt;/strong&gt;&lt;strong&gt;19&lt;/strong&gt;中，输出层的10个节点每一个都连接到第二个完全连接层中的全部100个节点（因此称为完全连接）。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;另外，注意为什么输出层中唯一明亮的节点是’8’ —— 这意味着神经网络对我们的手写数字进行了正确分类（节点亮度越高表示它的输出更高，即8在所有数字中具有最高的概率）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2072336265884653&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkzicUxx70YgJvbk0vEk6ZeO3NlmxYCpVCBDSBBeGqbiaY2jNbZ6lvQFqJtg8pvRjVvEXgOeUdSTjgvg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;2046&quot; height=&quot;102&quot; width=&quot;491&quot;/&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;图19：完全连接层可视化&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;该可视化系统的 3D 版本在此。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 其他卷积神经网络框架 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;卷积神经网络始于20世纪90年代初。我们已经讨论了LeNet，它是最早的卷积神经网络之一。下面列出了其他一些有影响力的神经网络框架 [3] [4]。&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;12.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;LeNet (20&lt;/strong&gt;&lt;strong&gt;世纪90&lt;/strong&gt;&lt;strong&gt;年代)&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;本文已详述。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;20&lt;/strong&gt;&lt;strong&gt;世纪&lt;/strong&gt;&lt;strong&gt;90&lt;/strong&gt;&lt;strong&gt;年代到&lt;/strong&gt;&lt;strong&gt;2012&lt;/strong&gt;&lt;strong&gt;年：&lt;/strong&gt;从20世纪90年代后期到2010年初，卷积神经网络正处于孵化期。随着越来越多的数据和计算能力的提升，卷积神经网络可以解决的任务变得越来越有趣。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;AlexNet&lt;/strong&gt;&lt;strong&gt;（2012&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt; – 2012年，Alex Krizhevsky（和其他人）发布了 AlexNet，它是提升了深度和广度版本的 LeNet，并在2012年以巨大优势赢得了 ImageNet 大规模视觉识别挑战赛（ILSVRC）。这是基于之前方法的重大突破，目前 CNN 的广泛应用都要归功于 AlexNet。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;ZF Net&lt;/strong&gt;&lt;strong&gt;（2013&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt; – 2013年 ILSVRC 获奖者来自 Matthew Zeiler 和 Rob Fergus 的卷积网络。它被称为 ZFNet（Zeiler 和 Fergus Net 的简称）。它在 AlexNet 的基础上通过调整网络框架超参数对其进行了改进。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;GoogLeNet&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;2014&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt; – 2014年 ILSVRC 获奖者是 Google 的 Szegedy 等人的卷积网络。其主要贡献是开发了一个初始模块，该模块大大减少了网络中的参数数量（4M，而 AlexNet 有60M）。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;VGGNet&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;2014&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt; – 2014年 ILSVRC 亚军是名为 VGGNet 的网络。其主要贡献在于证明了网络深度（层数）是影响性能的关键因素。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;ResNets&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;2015&lt;/strong&gt;&lt;strong&gt;）&lt;/strong&gt; – 何凯明（和其他人）开发的残差网络是2015年 ILSVRC 的冠军。ResNets 是迄今为止最先进的卷积神经网络模型，并且是大家在实践中使用卷积神经网络的默认选择（截至2016年5月）。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;DenseNet&lt;/strong&gt;&lt;strong&gt;（&lt;/strong&gt;&lt;strong&gt;2016&lt;/strong&gt;&lt;strong&gt;年&lt;/strong&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;strong&gt;月）&lt;/strong&gt; – 最近由黄高等人发表，密集连接卷积网络的每一层都以前馈方式直接连接到其他层。 DenseNet 已经在五项竞争激烈的对象识别基准测试任务中证明自己比之前最先进的框架有了显着的改进。具体实现请参考这个网址。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt; 结论 &lt;/span&gt;&lt;/h2&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;本文中，我尝试着用一些简单的术语解释卷积神经网络背后的主要概念，同时简化/略过了几个细节部分，但我希望这篇文章能够让你直观地理解其工作原理。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;本文最初是受 Denny Britz 《理解卷积神经网络在自然语言处理上的运用》这篇文章的启发（推荐阅读），文中的许多解释是基于这篇文章的。为了更深入地理解其中一些概念，我鼓励您阅读斯坦福大学卷积神经网络课程的笔记以及一下参考资料中提到的其他很棒的资源。如果您对上述概念的理解遇到任何问题/建议，请随时在下面留言。&lt;/span&gt;&lt;/p&gt;
&lt;p microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;文中所使用的所有图像和动画均属于其各自的作者，陈列如下。&lt;/span&gt;&lt;/p&gt;
&lt;h2 microsoft=&quot;&quot; yahei=&quot;&quot; pro=&quot;&quot; lato=&quot;&quot; neue=&quot;&quot; helvetica=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; rgb=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot;&gt;&lt;span&gt;参考&lt;/span&gt;&lt;/h2&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;28.180574098798&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;karpathy/neuraltalk2: Efficient Image Captioning code in Torch, Examples&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  Shaoqing Ren, et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, 2015, arXiv:1506.01497&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;Neural Network Architectures, Eugenio Culurciello’s blog&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;CS231n Convolutional Neural Networks for Visual Recognition, Stanford&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;span&gt;Clarifai/Technology&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;Feature extraction using convolution, Stanford&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;Wikipedia article on Kernel (image processing)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;Deep Learning Methods for Vision, CVPR 2012 Tutorial&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;Neural Networks by Rob Fergus, Machine Learning Summer School 2015&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;What do the fully connected layers do in CNNs?&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;Convolutional Neural Networks, Andrew Gibiansky&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;span&gt;W. Harley, “An Interactive Node-Link Visualization of Convolutional Neural Networks,” in ISVC, pages 867-877, 2015 (link). Demo&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;Understanding Convolutional Neural Networks for NLP&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;Backpropagation in Convolutional Neural Networks&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner&quot; s-guide-to-understanding-convolutional-neural-networks-part-2=&quot;&quot;&gt;A Beginner’s Guide To Understanding Convolutional Neural Networks&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;span&gt;Vincent Dumoulin, et al, “A guide to convolution arithmetic for deep learning”, 2015, arXiv:1603.07285&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;What is the difference between deep learning and usual machine learning?&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;How is a convolutional neural network able to learn invariant features?&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;A Taxonomy of Deep Convolutional Neural Nets for Computer Vision&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;Honglak Lee, et a&lt;/span&gt;&lt;span&gt;l, “Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations” (link)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;来&lt;/span&gt;&lt;span&gt;源：伯乐在线&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;编辑 &lt;span&gt;∑ Gemini&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section readability=&quot;2.7622259696459&quot;&gt;&lt;section readability=&quot;5.5244519392917&quot;&gt;&lt;section helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; display:=&quot;&quot; inline-block=&quot;&quot; top=&quot;&quot; solid=&quot;&quot; overflow-wrap:=&quot;&quot; break-word=&quot;&quot; important=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section readability=&quot;1.5&quot;&gt;&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;微信公众号“算法数学之美”，由算法与数学之美团队打造的另一个公众号，欢迎大家扫码关注！&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; wenquanyi=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; helvetica=&quot;&quot; neue=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; text-align:=&quot;&quot; justify=&quot;&quot; em=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkyjQVC3BFiaFKQ4DJqz2xhrwkzeCPbjQdnnG8678fRf1sxc2ZQtvtVib2dqWUkeopYtmgckINoOoGoQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;258&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;更多精彩：&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483726&amp;amp;idx=1&amp;amp;sn=e5e008fb68a7d837546d0ac5b5438042&amp;amp;chksm=ebe9cbf3dc9e42e5d625b2da6b9b3866dff9f08d442d8106f4cbf035d8602e1fdda86eec6476&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;如何向5岁小孩解释什么是支持向量机（SVM）？&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483726&amp;amp;idx=2&amp;amp;sn=18272b7eaa172794b51c30d0a2dd9c48&amp;amp;chksm=ebe9cbf3dc9e42e5ddf9a189822a2fa099543a631ad63a1d6ed0158b51c76212eb65ebbfe71b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;自然底数e的意义是什么？&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483715&amp;amp;idx=1&amp;amp;sn=9069dadf4bbce2aa34bd64b85a69dcee&amp;amp;chksm=ebe9cbfedc9e42e81c27d72da15c0dbf848e505946f231051b8b4033d0941bc6f51cef32790e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;☞&lt;/span&gt;费马大定理，集惊险与武侠于一体&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483715&amp;amp;idx=2&amp;amp;sn=55a16f37c89b27994b263e0dc9837561&amp;amp;chksm=ebe9cbfedc9e42e842deb581ea62b750cedd839abd58c2db3261bf9fbcd172a2cf18512e4d2d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;简单的解释，让你秒懂“最优化” 问题&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483676&amp;amp;idx=1&amp;amp;sn=2366a39bca3ef42a6e868e91ea718813&amp;amp;chksm=ebe9cba1dc9e42b70c7e147b9e43828c1a7c68401f442890471a06e5cf0704437f9813ca0e0f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;&lt;span&gt;☞&lt;/span&gt;一分钟看懂一维空间到十维空间&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞ &lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483661&amp;amp;idx=1&amp;amp;sn=d822666a054ba70b37dfb06d14c60f3a&amp;amp;chksm=ebe9cbb0dc9e42a6c476f7f81095b772aa45d960bf516f60c5b2e1155c9093696222cea0a83d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;本科、硕士和博士到底有什么区别？&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=1&amp;amp;sn=7d0d05c78cd01df91495f1d14609cbce&amp;amp;chksm=ebe9cbbbdc9e42add13cfe99f3383745fa5c059df705a3a9e28644d073dff804569af94970e3&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;小波变换通俗解释&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=4&amp;amp;sn=ce88086b650c601bdbf57ecfe5a490a1&amp;amp;chksm=ebe9cbbbdc9e42adfaf0e4ee644d254835c830ef47663315b70a39a2b47e6a7cf10d0826b88d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;微积分必背公式&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=3&amp;amp;sn=ceaec6043bb0e8a851033482f8f572bf&amp;amp;chksm=ebe9cbbbdc9e42ad30fd38383cf1caa609ac6e81964da17277f8e2a7f17a933cd11e0f3840c8&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;影响计算机算法世界的十位大师&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;☞&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzI4NTY3OTU3MA==&amp;amp;mid=2247483654&amp;amp;idx=2&amp;amp;sn=bf439d56bc7d42083708fa76434a6025&amp;amp;chksm=ebe9cbbbdc9e42ad9e5f2b3c1952e620e0e3d4452aae25b611e7e54be8678b0d80e002e7be6d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot; data-linktype=&quot;2&quot;&gt;数据挖掘之七种常用的方法&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;

&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 23 Jan 2019 14:31:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/rtUFGswJNY</dc:identifier>
</item>
</channel>
</rss>